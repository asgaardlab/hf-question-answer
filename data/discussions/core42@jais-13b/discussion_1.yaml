!!python/object:huggingface_hub.community.DiscussionWithDetails
author: decodingdatascience
conflicting_files: null
created_at: 2023-08-31 03:20:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
      fullname: Mohammad Arshad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: decodingdatascience
      type: user
    createdAt: '2023-08-31T04:20:56.000Z'
    data:
      edited: false
      editors:
      - decodingdatascience
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5872582793235779
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
          fullname: Mohammad Arshad
          isHf: false
          isPro: false
          name: decodingdatascience
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/IL68HhPXz_RJ-OQIGemiz.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/IL68HhPXz_RJ-OQIGemiz.png"></a></p>

          <p>Even though i got access</p>

          '
        raw: "\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/IL68HhPXz_RJ-OQIGemiz.png)\r\
          \n\r\nEven though i got access"
        updatedAt: '2023-08-31T04:20:56.181Z'
      numEdits: 0
      reactions: []
    id: 64f015281df52444a505d364
    type: comment
  author: decodingdatascience
  content: "\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/IL68HhPXz_RJ-OQIGemiz.png)\r\
    \n\r\nEven though i got access"
  created_at: 2023-08-31 03:20:56+00:00
  edited: false
  hidden: false
  id: 64f015281df52444a505d364
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
      fullname: samta kamboj
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: samta-kamboj
      type: user
    createdAt: '2023-08-31T05:08:32.000Z'
    data:
      edited: false
      editors:
      - samta-kamboj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9536714553833008
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
          fullname: samta kamboj
          isHf: false
          isPro: false
          name: samta-kamboj
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;decodingdatascience&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/decodingdatascience\"\
          >@<span class=\"underline\">decodingdatascience</span></a></span>\n\n\t\
          </span></span> you may try to access it now.</p>\n"
        raw: '@decodingdatascience you may try to access it now.'
        updatedAt: '2023-08-31T05:08:32.595Z'
      numEdits: 0
      reactions: []
    id: 64f0205050dae59768e8f73f
    type: comment
  author: samta-kamboj
  content: '@decodingdatascience you may try to access it now.'
  created_at: 2023-08-31 04:08:32+00:00
  edited: false
  hidden: false
  id: 64f0205050dae59768e8f73f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
      fullname: Mohammad Arshad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: decodingdatascience
      type: user
    createdAt: '2023-08-31T05:11:35.000Z'
    data:
      edited: false
      editors:
      - decodingdatascience
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37618017196655273
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
          fullname: Mohammad Arshad
          isHf: false
          isPro: false
          name: decodingdatascience
          type: user
        html: '<p>Thanks Samta Kamboj</p>

          '
        raw: Thanks Samta Kamboj
        updatedAt: '2023-08-31T05:11:35.290Z'
      numEdits: 0
      reactions: []
    id: 64f02107682edd520a48512b
    type: comment
  author: decodingdatascience
  content: Thanks Samta Kamboj
  created_at: 2023-08-31 04:11:35+00:00
  edited: false
  hidden: false
  id: 64f02107682edd520a48512b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
      fullname: Mohammad Arshad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: decodingdatascience
      type: user
    createdAt: '2023-08-31T05:16:22.000Z'
    data:
      edited: false
      editors:
      - decodingdatascience
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7226696014404297
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
          fullname: Mohammad Arshad
          isHf: false
          isPro: false
          name: decodingdatascience
          type: user
        html: '<p>i am using google colab ,I  m using accelerate as well m Still issue
          am I doing something wrong<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/4s3h4M30PjCG6rx-gohaO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/4s3h4M30PjCG6rx-gohaO.png"></a></p>

          '
        raw: 'i am using google colab ,I  m using accelerate as well m Still issue
          am I doing something wrong

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/4s3h4M30PjCG6rx-gohaO.png)

          '
        updatedAt: '2023-08-31T05:16:22.396Z'
      numEdits: 0
      reactions: []
    id: 64f02226b0995e6b498aed3f
    type: comment
  author: decodingdatascience
  content: 'i am using google colab ,I  m using accelerate as well m Still issue am
    I doing something wrong

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/4s3h4M30PjCG6rx-gohaO.png)

    '
  created_at: 2023-08-31 04:16:22+00:00
  edited: false
  hidden: false
  id: 64f02226b0995e6b498aed3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
      fullname: samta kamboj
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: samta-kamboj
      type: user
    createdAt: '2023-08-31T05:40:26.000Z'
    data:
      edited: false
      editors:
      - samta-kamboj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6752375960350037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
          fullname: samta kamboj
          isHf: false
          isPro: false
          name: samta-kamboj
          type: user
        html: '<p>Restart your notebook , install accelerate before importing transformers.  This
          may resolve the issue.<br>The order should be : </p>

          <ul>

          <li>pip install accelerate</li>

          <li>from transformers import AutoTokenizer, AutoModelForCausalLM</li>

          </ul>

          '
        raw: "Restart your notebook , install accelerate before importing transformers.\
          \  This may resolve the issue.\nThe order should be : \n- pip install accelerate\n\
          - from transformers import AutoTokenizer, AutoModelForCausalLM\n"
        updatedAt: '2023-08-31T05:40:26.255Z'
      numEdits: 0
      reactions: []
    id: 64f027ca7ca2e4871b4691df
    type: comment
  author: samta-kamboj
  content: "Restart your notebook , install accelerate before importing transformers.\
    \  This may resolve the issue.\nThe order should be : \n- pip install accelerate\n\
    - from transformers import AutoTokenizer, AutoModelForCausalLM\n"
  created_at: 2023-08-31 04:40:26+00:00
  edited: false
  hidden: false
  id: 64f027ca7ca2e4871b4691df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
      fullname: Mohammad Arshad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: decodingdatascience
      type: user
    createdAt: '2023-08-31T05:58:37.000Z'
    data:
      edited: false
      editors:
      - decodingdatascience
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8712421655654907
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
          fullname: Mohammad Arshad
          isHf: false
          isPro: false
          name: decodingdatascience
          type: user
        html: '<p>Thanks Samta , will restart again and let you know if working, thanks
          for prompt reply</p>

          '
        raw: Thanks Samta , will restart again and let you know if working, thanks
          for prompt reply
        updatedAt: '2023-08-31T05:58:37.209Z'
      numEdits: 0
      reactions: []
    id: 64f02c0da6a978156aafd973
    type: comment
  author: decodingdatascience
  content: Thanks Samta , will restart again and let you know if working, thanks for
    prompt reply
  created_at: 2023-08-31 04:58:37+00:00
  edited: false
  hidden: false
  id: 64f02c0da6a978156aafd973
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
      fullname: Mohammad Arshad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: decodingdatascience
      type: user
    createdAt: '2023-08-31T06:13:37.000Z'
    data:
      edited: false
      editors:
      - decodingdatascience
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5390706062316895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WpGrRaP7cS5kx1kGZIq5d.jpeg?w=200&h=200&f=face
          fullname: Mohammad Arshad
          isHf: false
          isPro: false
          name: decodingdatascience
          type: user
        html: '<p>Still same error , will try in sagemaker latter , thanks Samta<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/kPCpAGT2xvdb5L9oRZ1eV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/kPCpAGT2xvdb5L9oRZ1eV.png"></a></p>

          '
        raw: "Still same error , will try in sagemaker latter , thanks Samta \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/kPCpAGT2xvdb5L9oRZ1eV.png)\n"
        updatedAt: '2023-08-31T06:13:37.194Z'
      numEdits: 0
      reactions: []
    id: 64f02f91bae933568b842e72
    type: comment
  author: decodingdatascience
  content: "Still same error , will try in sagemaker latter , thanks Samta \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476ff295c48af2f1394ec62/kPCpAGT2xvdb5L9oRZ1eV.png)\n"
  created_at: 2023-08-31 05:13:37+00:00
  edited: false
  hidden: false
  id: 64f02f91bae933568b842e72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0638d5c65ccf0ba571f4d052d02d667f.svg
      fullname: s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ahmes91
      type: user
    createdAt: '2023-09-01T18:04:55.000Z'
    data:
      edited: false
      editors:
      - Ahmes91
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9701278209686279
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0638d5c65ccf0ba571f4d052d02d667f.svg
          fullname: s
          isHf: false
          isPro: false
          name: Ahmes91
          type: user
        html: '<p>I was getting errors using it with lower end gpus, got it working
          on 48gb GPU</p>

          '
        raw: I was getting errors using it with lower end gpus, got it working on
          48gb GPU
        updatedAt: '2023-09-01T18:04:55.745Z'
      numEdits: 0
      reactions: []
    id: 64f227c783dc7806ae3c6463
    type: comment
  author: Ahmes91
  content: I was getting errors using it with lower end gpus, got it working on 48gb
    GPU
  created_at: 2023-09-01 17:04:55+00:00
  edited: false
  hidden: false
  id: 64f227c783dc7806ae3c6463
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/046c7096cf88f50f34516e7da1b18a0a.svg
      fullname: Osama Mohammed Afzal
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: oafzal
      type: user
    createdAt: '2023-09-01T18:25:20.000Z'
    data:
      edited: false
      editors:
      - oafzal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8912591338157654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/046c7096cf88f50f34516e7da1b18a0a.svg
          fullname: Osama Mohammed Afzal
          isHf: false
          isPro: false
          name: oafzal
          type: user
        html: '<p>You should be able to load it on a smaller V100 (32GB) or A100 (40GB)
          GPU by using <code>bfloat16</code> precision. You can achieve this by adding
          the dtype argument to the method. Additionally, you can further reduce the
          memory requirement to 13GB (1 x T4) by using <code>int8</code> precision
          or <code>4 bits</code> precision with the help of bits-and-bytes library,
          but be aware that this may lead to degradation in quality. We have not tested
          that yet.</p>

          '
        raw: You should be able to load it on a smaller V100 (32GB) or A100 (40GB)
          GPU by using `bfloat16` precision. You can achieve this by adding the dtype
          argument to the method. Additionally, you can further reduce the memory
          requirement to 13GB (1 x T4) by using `int8` precision or `4 bits` precision
          with the help of bits-and-bytes library, but be aware that this may lead
          to degradation in quality. We have not tested that yet.
        updatedAt: '2023-09-01T18:25:20.455Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - zainmujahid
    id: 64f22c9074f9109c42ccfe9a
    type: comment
  author: oafzal
  content: You should be able to load it on a smaller V100 (32GB) or A100 (40GB) GPU
    by using `bfloat16` precision. You can achieve this by adding the dtype argument
    to the method. Additionally, you can further reduce the memory requirement to
    13GB (1 x T4) by using `int8` precision or `4 bits` precision with the help of
    bits-and-bytes library, but be aware that this may lead to degradation in quality.
    We have not tested that yet.
  created_at: 2023-09-01 17:25:20+00:00
  edited: false
  hidden: false
  id: 64f22c9074f9109c42ccfe9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0638d5c65ccf0ba571f4d052d02d667f.svg
      fullname: s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ahmes91
      type: user
    createdAt: '2023-09-01T19:34:33.000Z'
    data:
      edited: true
      editors:
      - Ahmes91
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6231120228767395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0638d5c65ccf0ba571f4d052d02d667f.svg
          fullname: s
          isHf: false
          isPro: false
          name: Ahmes91
          type: user
        html: '<blockquote>

          <p>You should be able to load it on a smaller V100 (32GB) or A100 (40GB)
          GPU by using <code>bfloat16</code> precision. You can achieve this by adding
          the dtype argument to the method. Additionally, you can further reduce the
          memory requirement to 13GB (1 x T4) by using <code>int8</code> precision
          or <code>4 bits</code> precision with the help of bits-and-bytes library,
          but be aware that this may lead to degradation in quality. We have not tested
          that yet.</p>

          </blockquote>

          <p>thanks! I just did, with int8 the model was setting at around 21gb, in
          my limited tests there is no difference in the quailty of the responce<br>+---------------------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:
          12.2     |<br>|-----------------------------------------+----------------------+----------------------+<br>|
          GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile
          Uncorr. ECC |<br>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage
          | GPU-Util  Compute M. |<br>|                                         |                      |               MIG
          M. |<br>|=========================================+======================+======================|<br>|   0  NVIDIA
          A10G                    On  | 00000000:00:1E.0 Off |                    0
          |<br>|  0%   36C    P0              71W / 300W |  21192MiB / 23028MiB |      9%      Default
          |<br>|                                         |                      |                  N/A
          |<br>+-----------------------------------------+----------------------+----------------------+</p>

          <p>+---------------------------------------------------------------------------------------+<br>|
          Processes:                                                                            |<br>|  GPU   GI   CI        PID   Type   Process
          name                            GPU Memory |<br>|        ID   ID                                                             Usage      |<br>|=======================================================================================|<br>|    0   N/A  N/A      2547      C   /usr/bin/python3                          21184MiB
          |</p>

          '
        raw: "> You should be able to load it on a smaller V100 (32GB) or A100 (40GB)\
          \ GPU by using `bfloat16` precision. You can achieve this by adding the\
          \ dtype argument to the method. Additionally, you can further reduce the\
          \ memory requirement to 13GB (1 x T4) by using `int8` precision or `4 bits`\
          \ precision with the help of bits-and-bytes library, but be aware that this\
          \ may lead to degradation in quality. We have not tested that yet.\n\nthanks!\
          \ I just did, with int8 the model was setting at around 21gb, in my limited\
          \ tests there is no difference in the quailty of the responce \n+---------------------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:\
          \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |     \
          \               0 |\n|  0%   36C    P0              71W / 300W |  21192MiB\
          \ / 23028MiB |      9%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \                   \n+---------------------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \                 |\n|  GPU   GI   CI        PID   Type   Process name \
          \                           GPU Memory |\n|        ID   ID             \
          \                                                Usage      |\n|=======================================================================================|\n\
          |    0   N/A  N/A      2547      C   /usr/bin/python3                  \
          \        21184MiB |\n\n\n"
        updatedAt: '2023-09-01T19:35:00.284Z'
      numEdits: 1
      reactions: []
    id: 64f23cc95f94b7dd9ba56912
    type: comment
  author: Ahmes91
  content: "> You should be able to load it on a smaller V100 (32GB) or A100 (40GB)\
    \ GPU by using `bfloat16` precision. You can achieve this by adding the dtype\
    \ argument to the method. Additionally, you can further reduce the memory requirement\
    \ to 13GB (1 x T4) by using `int8` precision or `4 bits` precision with the help\
    \ of bits-and-bytes library, but be aware that this may lead to degradation in\
    \ quality. We have not tested that yet.\n\nthanks! I just did, with int8 the model\
    \ was setting at around 21gb, in my limited tests there is no difference in the\
    \ quailty of the responce \n+---------------------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:\
    \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr.\
    \ ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util\
    \  Compute M. |\n|                                         |                 \
    \     |               MIG M. |\n|=========================================+======================+======================|\n\
    |   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |           \
    \         0 |\n|  0%   36C    P0              71W / 300W |  21192MiB / 23028MiB\
    \ |      9%      Default |\n|                                         |      \
    \                |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
    \                                                                            \
    \             \n+---------------------------------------------------------------------------------------+\n\
    | Processes:                                                                 \
    \           |\n|  GPU   GI   CI        PID   Type   Process name             \
    \               GPU Memory |\n|        ID   ID                               \
    \                              Usage      |\n|=======================================================================================|\n\
    |    0   N/A  N/A      2547      C   /usr/bin/python3                        \
    \  21184MiB |\n\n\n"
  created_at: 2023-09-01 18:34:33+00:00
  edited: true
  hidden: false
  id: 64f23cc95f94b7dd9ba56912
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e0a0fc37f7236cd9b7587fbe65ab79f.svg
      fullname: Imad Choucair
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imadch1
      type: user
    createdAt: '2023-09-02T15:12:50.000Z'
    data:
      edited: false
      editors:
      - imadch1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5752226710319519
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e0a0fc37f7236cd9b7587fbe65ab79f.svg
          fullname: Imad Choucair
          isHf: false
          isPro: false
          name: imadch1
          type: user
        html: '<h1 id="update-the-model-to-add-the-offload-folder">Update the model
          to add the offload folder.</h1>

          <p>model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto",
          offload_folder="offload", offload_state_dict = False, trust_remote_code=True)</p>

          <h1 id="also-add">also add</h1>

          <p>model.to(device)</p>

          '
        raw: "# Update the model to add the offload folder. \n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ device_map=\"auto\", offload_folder=\"offload\", offload_state_dict =\
          \ False, trust_remote_code=True)\n # also add\n\nmodel.to(device)\n"
        updatedAt: '2023-09-02T15:12:50.497Z'
      numEdits: 0
      reactions: []
    id: 64f350f26c0c0d44036e94ca
    type: comment
  author: imadch1
  content: "# Update the model to add the offload folder. \n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ device_map=\"auto\", offload_folder=\"offload\", offload_state_dict = False,\
    \ trust_remote_code=True)\n # also add\n\nmodel.to(device)\n"
  created_at: 2023-09-02 14:12:50+00:00
  edited: false
  hidden: false
  id: 64f350f26c0c0d44036e94ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
      fullname: samta kamboj
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: samta-kamboj
      type: user
    createdAt: '2023-09-04T10:07:18.000Z'
    data:
      status: closed
    id: 64f5ac56a539ca3ba2bef91f
    type: status-change
  author: samta-kamboj
  created_at: 2023-09-04 09:07:18+00:00
  id: 64f5ac56a539ca3ba2bef91f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/246abaaa9c6a18b7859d320d99229933.svg
      fullname: mohammad faris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: faris98
      type: user
    createdAt: '2023-09-04T12:22:20.000Z'
    data:
      edited: false
      editors:
      - faris98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5624431371688843
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/246abaaa9c6a18b7859d320d99229933.svg
          fullname: mohammad faris
          isHf: false
          isPro: false
          name: faris98
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;oafzal&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/oafzal\">@<span class=\"\
          underline\">oafzal</span></a></span>\n\n\t</span></span>  <span data-props=\"\
          {&quot;user&quot;:&quot;Ahmes91&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Ahmes91\">@<span class=\"underline\">Ahmes91</span></a></span>\n\
          \n\t</span></span> Where can I add the lower precision in the model? Are\
          \ there any parameters?</p>\n"
        raw: '@oafzal  @Ahmes91 Where can I add the lower precision in the model?
          Are there any parameters?'
        updatedAt: '2023-09-04T12:22:20.904Z'
      numEdits: 0
      reactions: []
    id: 64f5cbfc9bd32ae6c8323c8d
    type: comment
  author: faris98
  content: '@oafzal  @Ahmes91 Where can I add the lower precision in the model? Are
    there any parameters?'
  created_at: 2023-09-04 11:22:20+00:00
  edited: false
  hidden: false
  id: 64f5cbfc9bd32ae6c8323c8d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: core42/jais-13b
repo_type: model
status: closed
target_branch: null
title: Not able to run the LLM Jais
