!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mikeytrw
conflicting_files: null
created_at: 2023-12-11 17:18:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
      fullname: Mike Wharton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeytrw
      type: user
    createdAt: '2023-12-11T17:18:41.000Z'
    data:
      edited: false
      editors:
      - mikeytrw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7706261277198792
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
          fullname: Mike Wharton
          isHf: false
          isPro: false
          name: mikeytrw
          type: user
        html: '<p>I''m trying to run this on my 3090, the model loads just fine but
          I get the below error whether I use pipielines or transformer implementation:</p>

          <p>RuntimeError: The size of tensor a (616) must match the size of tensor
          b (1231) at non-singleton dimension 3</p>

          '
        raw: "I'm trying to run this on my 3090, the model loads just fine but I get\
          \ the below error whether I use pipielines or transformer implementation:\r\
          \n\r\nRuntimeError: The size of tensor a (616) must match the size of tensor\
          \ b (1231) at non-singleton dimension 3"
        updatedAt: '2023-12-11T17:18:41.727Z'
      numEdits: 0
      reactions: []
    id: 65774471a1688debf1969afa
    type: comment
  author: mikeytrw
  content: "I'm trying to run this on my 3090, the model loads just fine but I get\
    \ the below error whether I use pipielines or transformer implementation:\r\n\r\
    \nRuntimeError: The size of tensor a (616) must match the size of tensor b (1231)\
    \ at non-singleton dimension 3"
  created_at: 2023-12-11 17:18:41+00:00
  edited: false
  hidden: false
  id: 65774471a1688debf1969afa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-11T18:11:51.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8131353259086609
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;mikeytrw&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mikeytrw\"\
          >@<span class=\"underline\">mikeytrw</span></a></span>\n\n\t</span></span>\
          \ we recently fixed a similar issue on transformers, can you try to use\
          \ <code>transformers==4.36.0</code> ? <code>pip install -U transformers</code></p>\n"
        raw: hi @mikeytrw we recently fixed a similar issue on transformers, can you
          try to use `transformers==4.36.0` ? `pip install -U transformers`
        updatedAt: '2023-12-11T18:11:51.523Z'
      numEdits: 0
      reactions: []
    id: 657750e78c1400ddda1e5749
    type: comment
  author: ybelkada
  content: hi @mikeytrw we recently fixed a similar issue on transformers, can you
    try to use `transformers==4.36.0` ? `pip install -U transformers`
  created_at: 2023-12-11 18:11:51+00:00
  edited: false
  hidden: false
  id: 657750e78c1400ddda1e5749
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
      fullname: Mike Wharton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeytrw
      type: user
    createdAt: '2023-12-11T18:17:13.000Z'
    data:
      edited: false
      editors:
      - mikeytrw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7753026485443115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
          fullname: Mike Wharton
          isHf: false
          isPro: false
          name: mikeytrw
          type: user
        html: '<p>Hey, thanks for the quick response. I''m using transformers 4.36.0.dev0</p>

          '
        raw: Hey, thanks for the quick response. I'm using transformers 4.36.0.dev0
        updatedAt: '2023-12-11T18:17:13.462Z'
      numEdits: 0
      reactions: []
    id: 6577522917b474ae6b577bfa
    type: comment
  author: mikeytrw
  content: Hey, thanks for the quick response. I'm using transformers 4.36.0.dev0
  created_at: 2023-12-11 18:17:13+00:00
  edited: false
  hidden: false
  id: 6577522917b474ae6b577bfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
      fullname: Mike Wharton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeytrw
      type: user
    createdAt: '2023-12-11T18:52:30.000Z'
    data:
      edited: false
      editors:
      - mikeytrw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3365217447280884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
          fullname: Mike Wharton
          isHf: false
          isPro: false
          name: mikeytrw
          type: user
        html: '<p>Here is the full output, the tensor sizes change if I change the
          input image:</p>

          <hr>

          <p>RuntimeError                              Traceback (most recent call
          last)<br>Cell In[7], line 7<br>      5 inputs = processor(prompt, raw_image,
          return_tensors=''pt'').to(0, torch.float16)<br>      6 #inputs = processor(prompt,
          return_tensors=''pt'').to(0, torch.float16)<br>----&gt; 7 output = model.generate(**inputs,
          max_new_tokens=200, do_sample=False)<br>      8 print(processor.decode(output[0][2:],
          skip_special_tokens=True))</p>

          <p>File ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,
          in context_decorator..decorate_context(*args, **kwargs)<br>    112 @functools.wraps(func)<br>    113
          def decorate_context(*args, **kwargs):<br>    114     with ctx_factory():<br>--&gt;
          115         return func(*args, **kwargs)</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1718,
          in GenerationMixin.generate(self, inputs, generation_config, logits_processor,
          stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,
          streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)<br>   1701     return
          self.assisted_decoding(<br>   1702         input_ids,<br>   1703         assistant_model=assistant_model,<br>   (...)<br>   1714         **model_kwargs,<br>   1715     )<br>   1716
          if generation_mode == GenerationMode.GREEDY_SEARCH:<br>   1717     # 11.
          run greedy search<br>-&gt; 1718     return self.greedy_search(<br>   1719         input_ids,<br>   1720         logits_processor=logits_processor,<br>   1721         stopping_criteria=stopping_criteria,<br>   1722         pad_token_id=generation_config.pad_token_id,<br>   1723         eos_token_id=generation_config.eos_token_id,<br>   1724         output_scores=generation_config.output_scores,<br>   1725         return_dict_in_generate=generation_config.return_dict_in_generate,<br>   1726         synced_gpus=synced_gpus,<br>   1727         streamer=streamer,<br>   1728         **model_kwargs,<br>   1729     )<br>   1731
          elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:<br>   1732     if
          not model_kwargs["use_cache"]:</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2579,
          in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria,
          max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,
          output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)<br>   2576
          model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)<br>   2578
          # forward pass to get next token<br>-&gt; 2579 outputs = self(<br>   2580     **model_inputs,<br>   2581     return_dict=True,<br>   2582     output_attentions=output_attentions,<br>   2583     output_hidden_states=output_hidden_states,<br>   2584
          )<br>   2586 if synced_gpus and this_peer_finished:<br>   2587     continue  #
          don''t waste resources running the code we don''t need</p>

          <p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,
          in Module._wrapped_call_impl(self, *args, **kwargs)<br>   1516     return
          self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]<br>   1517
          else:<br>-&gt; 1518     return self._call_impl(*args, **kwargs)</p>

          <p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,
          in Module._call_impl(self, *args, **kwargs)<br>   1522 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1523 # this function,
          and just call forward.<br>   1524 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1525         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1526         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1527     return
          forward_call(*args, **kwargs)<br>   1529 try:<br>   1530     result = None</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py:433,
          in LlavaForConditionalGeneration.forward(self, input_ids, pixel_values,
          attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer,
          vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states,
          return_dict)<br>    430             attention_mask = torch.cat((attention_mask,
          extended_attention_mask), dim=1)<br>    431             position_ids = torch.sum(attention_mask,
          dim=1).unsqueeze(-1) - 1<br>--&gt; 433 outputs = self.language_model(<br>    434     attention_mask=attention_mask,<br>    435     position_ids=position_ids,<br>    436     past_key_values=past_key_values,<br>    437     inputs_embeds=inputs_embeds,<br>    438     use_cache=use_cache,<br>    439     output_attentions=output_attentions,<br>    440     output_hidden_states=output_hidden_states,<br>    441     return_dict=return_dict,<br>    442
          )<br>    444 logits = outputs[0]<br>    446 loss = None</p>

          <p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,
          in Module._wrapped_call_impl(self, *args, **kwargs)<br>   1516     return
          self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]<br>   1517
          else:<br>-&gt; 1518     return self._call_impl(*args, **kwargs)</p>

          <p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,
          in Module._call_impl(self, *args, **kwargs)<br>   1522 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1523 # this function,
          and just call forward.<br>   1524 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1525         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1526         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1527     return
          forward_call(*args, **kwargs)<br>   1529 try:<br>   1530     result = None</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1046,
          in MistralForCausalLM.forward(self, input_ids, attention_mask, position_ids,
          past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,
          return_dict)<br>   1043 return_dict = return_dict if return_dict is not
          None else self.config.use_return_dict<br>   1045 # decoder outputs consists
          of (dec_features, layer_state, dec_hidden, dec_attn)<br>-&gt; 1046 outputs
          = self.model(<br>   1047     input_ids=input_ids,<br>   1048     attention_mask=attention_mask,<br>   1049     position_ids=position_ids,<br>   1050     past_key_values=past_key_values,<br>   1051     inputs_embeds=inputs_embeds,<br>   1052     use_cache=use_cache,<br>   1053     output_attentions=output_attentions,<br>   1054     output_hidden_states=output_hidden_states,<br>   1055     return_dict=return_dict,<br>   1056
          )<br>   1058 hidden_states = outputs[0]<br>   1059 logits = self.lm_head(hidden_states)</p>

          <p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,
          in Module._wrapped_call_impl(self, *args, **kwargs)<br>   1516     return
          self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]<br>   1517
          else:<br>-&gt; 1518     return self._call_impl(*args, **kwargs)</p>

          <p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,
          in Module._call_impl(self, *args, **kwargs)<br>   1522 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1523 # this function,
          and just call forward.<br>   1524 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1525         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1526         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1527     return
          forward_call(*args, **kwargs)<br>   1529 try:<br>   1530     result = None</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:894,
          in MistralModel.forward(self, input_ids, attention_mask, position_ids, past_key_values,
          inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)<br>    891     attention_mask
          = attention_mask if (attention_mask is not None and 0 in attention_mask)
          else None<br>    892 else:<br>    893     # 4d mask is passed through the
          layers<br>--&gt; 894     attention_mask = _prepare_4d_causal_attention_mask(<br>    895         attention_mask,<br>    896         (batch_size,
          seq_length),<br>    897         inputs_embeds,<br>    898         past_key_values_length,<br>    899         sliding_window=self.config.sliding_window,<br>    900     )<br>    902
          hidden_states = inputs_embeds<br>    904 if self.gradient_checkpointing
          and self.training:</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:217,
          in _prepare_4d_causal_attention_mask(attention_mask, input_shape, inputs_embeds,
          past_key_values_length, sliding_window)<br>    215 # 4d mask is passed through
          the layers<br>    216 if attention_mask is not None:<br>--&gt; 217     attention_mask
          = attn_mask_converter.to_4d(<br>    218         attention_mask, input_shape[-1],
          key_value_length=key_value_length, dtype=inputs_embeds.dtype<br>    219     )<br>    220
          else:<br>    221     attention_mask = attn_mask_converter.to_causal_4d(<br>    222         input_shape[0],
          input_shape[-1], key_value_length, dtype=inputs_embeds.dtype, device=inputs_embeds.device<br>    223     )</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:136,
          in AttentionMaskConverter.to_4d(self, attention_mask_2d, query_length, dtype,
          key_value_length)<br>    132 expanded_attn_mask = self._expand_mask(attention_mask_2d,
          dtype, tgt_len=input_shape[-1]).to(<br>    133     attention_mask_2d.device<br>    134
          )<br>    135 if causal_4d_mask is not None:<br>--&gt; 136     expanded_attn_mask
          = causal_4d_mask.masked_fill(expanded_attn_mask.bool(), torch.finfo(dtype).min)<br>    138
          # expanded_attn_mask + causal_4d_mask can cause some overflow<br>    139
          expanded_4d_mask = expanded_attn_mask</p>

          <p>RuntimeError: The size of tensor a (595) must match the size of tensor
          b (1189) at non-singleton dimension 3</p>

          '
        raw: "Here is the full output, the tensor sizes change if I change the input\
          \ image:\n\n\n---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Cell In[7], line 7\n      5 inputs = processor(prompt, raw_image, return_tensors='pt').to(0,\
          \ torch.float16)\n      6 #inputs = processor(prompt, return_tensors='pt').to(0,\
          \ torch.float16)\n----> 7 output = model.generate(**inputs, max_new_tokens=200,\
          \ do_sample=False)\n      8 print(processor.decode(output[0][2:], skip_special_tokens=True))\n\
          \nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    112\
          \ @functools.wraps(func)\n    113 def decorate_context(*args, **kwargs):\n\
          \    114     with ctx_factory():\n--> 115         return func(*args, **kwargs)\n\
          \nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1718,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n\
          \   1701     return self.assisted_decoding(\n   1702         input_ids,\n\
          \   1703         assistant_model=assistant_model,\n   (...)\n   1714   \
          \      **model_kwargs,\n   1715     )\n   1716 if generation_mode == GenerationMode.GREEDY_SEARCH:\n\
          \   1717     # 11. run greedy search\n-> 1718     return self.greedy_search(\n\
          \   1719         input_ids,\n   1720         logits_processor=logits_processor,\n\
          \   1721         stopping_criteria=stopping_criteria,\n   1722         pad_token_id=generation_config.pad_token_id,\n\
          \   1723         eos_token_id=generation_config.eos_token_id,\n   1724 \
          \        output_scores=generation_config.output_scores,\n   1725       \
          \  return_dict_in_generate=generation_config.return_dict_in_generate,\n\
          \   1726         synced_gpus=synced_gpus,\n   1727         streamer=streamer,\n\
          \   1728         **model_kwargs,\n   1729     )\n   1731 elif generation_mode\
          \ == GenerationMode.CONTRASTIVE_SEARCH:\n   1732     if not model_kwargs[\"\
          use_cache\"]:\n\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2579,\
          \ in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria,\
          \ max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
          \ output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\n\
          \   2576 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\
          \   2578 # forward pass to get next token\n-> 2579 outputs = self(\n   2580\
          \     **model_inputs,\n   2581     return_dict=True,\n   2582     output_attentions=output_attentions,\n\
          \   2583     output_hidden_states=output_hidden_states,\n   2584 )\n   2586\
          \ if synced_gpus and this_peer_finished:\n   2587     continue  # don't\
          \ waste resources running the code we don't need\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,\
          \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return\
          \ self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1517\
          \ else:\n-> 1518     return self._call_impl(*args, **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1523 # this function,\
          \ and just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1526        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1527     return\
          \ forward_call(*args, **kwargs)\n   1529 try:\n   1530     result = None\n\
          \nFile ~/.local/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py:433,\
          \ in LlavaForConditionalGeneration.forward(self, input_ids, pixel_values,\
          \ attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer,\
          \ vision_feature_select_strategy, labels, use_cache, output_attentions,\
          \ output_hidden_states, return_dict)\n    430             attention_mask\
          \ = torch.cat((attention_mask, extended_attention_mask), dim=1)\n    431\
          \             position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1)\
          \ - 1\n--> 433 outputs = self.language_model(\n    434     attention_mask=attention_mask,\n\
          \    435     position_ids=position_ids,\n    436     past_key_values=past_key_values,\n\
          \    437     inputs_embeds=inputs_embeds,\n    438     use_cache=use_cache,\n\
          \    439     output_attentions=output_attentions,\n    440     output_hidden_states=output_hidden_states,\n\
          \    441     return_dict=return_dict,\n    442 )\n    444 logits = outputs[0]\n\
          \    446 loss = None\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,\
          \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return\
          \ self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1517\
          \ else:\n-> 1518     return self._call_impl(*args, **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1523 # this function,\
          \ and just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1526        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1527     return\
          \ forward_call(*args, **kwargs)\n   1529 try:\n   1530     result = None\n\
          \nFile ~/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1046,\
          \ in MistralForCausalLM.forward(self, input_ids, attention_mask, position_ids,\
          \ past_key_values, inputs_embeds, labels, use_cache, output_attentions,\
          \ output_hidden_states, return_dict)\n   1043 return_dict = return_dict\
          \ if return_dict is not None else self.config.use_return_dict\n   1045 #\
          \ decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n\
          -> 1046 outputs = self.model(\n   1047     input_ids=input_ids,\n   1048\
          \     attention_mask=attention_mask,\n   1049     position_ids=position_ids,\n\
          \   1050     past_key_values=past_key_values,\n   1051     inputs_embeds=inputs_embeds,\n\
          \   1052     use_cache=use_cache,\n   1053     output_attentions=output_attentions,\n\
          \   1054     output_hidden_states=output_hidden_states,\n   1055     return_dict=return_dict,\n\
          \   1056 )\n   1058 hidden_states = outputs[0]\n   1059 logits = self.lm_head(hidden_states)\n\
          \nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,\
          \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return\
          \ self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1517\
          \ else:\n-> 1518     return self._call_impl(*args, **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1523 # this function,\
          \ and just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1526        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1527     return\
          \ forward_call(*args, **kwargs)\n   1529 try:\n   1530     result = None\n\
          \nFile ~/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:894,\
          \ in MistralModel.forward(self, input_ids, attention_mask, position_ids,\
          \ past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)\n    891     attention_mask = attention_mask if (attention_mask\
          \ is not None and 0 in attention_mask) else None\n    892 else:\n    893\
          \     # 4d mask is passed through the layers\n--> 894     attention_mask\
          \ = _prepare_4d_causal_attention_mask(\n    895         attention_mask,\n\
          \    896         (batch_size, seq_length),\n    897         inputs_embeds,\n\
          \    898         past_key_values_length,\n    899         sliding_window=self.config.sliding_window,\n\
          \    900     )\n    902 hidden_states = inputs_embeds\n    904 if self.gradient_checkpointing\
          \ and self.training:\n\nFile ~/.local/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:217,\
          \ in _prepare_4d_causal_attention_mask(attention_mask, input_shape, inputs_embeds,\
          \ past_key_values_length, sliding_window)\n    215 # 4d mask is passed through\
          \ the layers\n    216 if attention_mask is not None:\n--> 217     attention_mask\
          \ = attn_mask_converter.to_4d(\n    218         attention_mask, input_shape[-1],\
          \ key_value_length=key_value_length, dtype=inputs_embeds.dtype\n    219\
          \     )\n    220 else:\n    221     attention_mask = attn_mask_converter.to_causal_4d(\n\
          \    222         input_shape[0], input_shape[-1], key_value_length, dtype=inputs_embeds.dtype,\
          \ device=inputs_embeds.device\n    223     )\n\nFile ~/.local/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:136,\
          \ in AttentionMaskConverter.to_4d(self, attention_mask_2d, query_length,\
          \ dtype, key_value_length)\n    132 expanded_attn_mask = self._expand_mask(attention_mask_2d,\
          \ dtype, tgt_len=input_shape[-1]).to(\n    133     attention_mask_2d.device\n\
          \    134 )\n    135 if causal_4d_mask is not None:\n--> 136     expanded_attn_mask\
          \ = causal_4d_mask.masked_fill(expanded_attn_mask.bool(), torch.finfo(dtype).min)\n\
          \    138 # expanded_attn_mask + causal_4d_mask can cause some overflow\n\
          \    139 expanded_4d_mask = expanded_attn_mask\n\nRuntimeError: The size\
          \ of tensor a (595) must match the size of tensor b (1189) at non-singleton\
          \ dimension 3"
        updatedAt: '2023-12-11T18:52:30.620Z'
      numEdits: 0
      reactions: []
    id: 65775a6e256953fd7640af23
    type: comment
  author: mikeytrw
  content: "Here is the full output, the tensor sizes change if I change the input\
    \ image:\n\n\n---------------------------------------------------------------------------\n\
    RuntimeError                              Traceback (most recent call last)\n\
    Cell In[7], line 7\n      5 inputs = processor(prompt, raw_image, return_tensors='pt').to(0,\
    \ torch.float16)\n      6 #inputs = processor(prompt, return_tensors='pt').to(0,\
    \ torch.float16)\n----> 7 output = model.generate(**inputs, max_new_tokens=200,\
    \ do_sample=False)\n      8 print(processor.decode(output[0][2:], skip_special_tokens=True))\n\
    \nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in\
    \ context_decorator.<locals>.decorate_context(*args, **kwargs)\n    112 @functools.wraps(func)\n\
    \    113 def decorate_context(*args, **kwargs):\n    114     with ctx_factory():\n\
    --> 115         return func(*args, **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1718,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n   1701    \
    \ return self.assisted_decoding(\n   1702         input_ids,\n   1703        \
    \ assistant_model=assistant_model,\n   (...)\n   1714         **model_kwargs,\n\
    \   1715     )\n   1716 if generation_mode == GenerationMode.GREEDY_SEARCH:\n\
    \   1717     # 11. run greedy search\n-> 1718     return self.greedy_search(\n\
    \   1719         input_ids,\n   1720         logits_processor=logits_processor,\n\
    \   1721         stopping_criteria=stopping_criteria,\n   1722         pad_token_id=generation_config.pad_token_id,\n\
    \   1723         eos_token_id=generation_config.eos_token_id,\n   1724       \
    \  output_scores=generation_config.output_scores,\n   1725         return_dict_in_generate=generation_config.return_dict_in_generate,\n\
    \   1726         synced_gpus=synced_gpus,\n   1727         streamer=streamer,\n\
    \   1728         **model_kwargs,\n   1729     )\n   1731 elif generation_mode\
    \ == GenerationMode.CONTRASTIVE_SEARCH:\n   1732     if not model_kwargs[\"use_cache\"\
    ]:\n\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2579,\
    \ in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria,\
    \ max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
    \ output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\n\
    \   2576 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\
    \   2578 # forward pass to get next token\n-> 2579 outputs = self(\n   2580  \
    \   **model_inputs,\n   2581     return_dict=True,\n   2582     output_attentions=output_attentions,\n\
    \   2583     output_hidden_states=output_hidden_states,\n   2584 )\n   2586 if\
    \ synced_gpus and this_peer_finished:\n   2587     continue  # don't waste resources\
    \ running the code we don't need\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,\
    \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return self._compiled_call_impl(*args,\
    \ **kwargs)  # type: ignore[misc]\n   1517 else:\n-> 1518     return self._call_impl(*args,\
    \ **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1523 # this function, and\
    \ just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1527     return forward_call(*args, **kwargs)\n   1529 try:\n   1530     result\
    \ = None\n\nFile ~/.local/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py:433,\
    \ in LlavaForConditionalGeneration.forward(self, input_ids, pixel_values, attention_mask,\
    \ position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy,\
    \ labels, use_cache, output_attentions, output_hidden_states, return_dict)\n \
    \   430             attention_mask = torch.cat((attention_mask, extended_attention_mask),\
    \ dim=1)\n    431             position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1)\
    \ - 1\n--> 433 outputs = self.language_model(\n    434     attention_mask=attention_mask,\n\
    \    435     position_ids=position_ids,\n    436     past_key_values=past_key_values,\n\
    \    437     inputs_embeds=inputs_embeds,\n    438     use_cache=use_cache,\n\
    \    439     output_attentions=output_attentions,\n    440     output_hidden_states=output_hidden_states,\n\
    \    441     return_dict=return_dict,\n    442 )\n    444 logits = outputs[0]\n\
    \    446 loss = None\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,\
    \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return self._compiled_call_impl(*args,\
    \ **kwargs)  # type: ignore[misc]\n   1517 else:\n-> 1518     return self._call_impl(*args,\
    \ **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1523 # this function, and\
    \ just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1527     return forward_call(*args, **kwargs)\n   1529 try:\n   1530     result\
    \ = None\n\nFile ~/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1046,\
    \ in MistralForCausalLM.forward(self, input_ids, attention_mask, position_ids,\
    \ past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,\
    \ return_dict)\n   1043 return_dict = return_dict if return_dict is not None else\
    \ self.config.use_return_dict\n   1045 # decoder outputs consists of (dec_features,\
    \ layer_state, dec_hidden, dec_attn)\n-> 1046 outputs = self.model(\n   1047 \
    \    input_ids=input_ids,\n   1048     attention_mask=attention_mask,\n   1049\
    \     position_ids=position_ids,\n   1050     past_key_values=past_key_values,\n\
    \   1051     inputs_embeds=inputs_embeds,\n   1052     use_cache=use_cache,\n\
    \   1053     output_attentions=output_attentions,\n   1054     output_hidden_states=output_hidden_states,\n\
    \   1055     return_dict=return_dict,\n   1056 )\n   1058 hidden_states = outputs[0]\n\
    \   1059 logits = self.lm_head(hidden_states)\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518,\
    \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return self._compiled_call_impl(*args,\
    \ **kwargs)  # type: ignore[misc]\n   1517 else:\n-> 1518     return self._call_impl(*args,\
    \ **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1523 # this function, and\
    \ just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1527     return forward_call(*args, **kwargs)\n   1529 try:\n   1530     result\
    \ = None\n\nFile ~/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:894,\
    \ in MistralModel.forward(self, input_ids, attention_mask, position_ids, past_key_values,\
    \ inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\n\
    \    891     attention_mask = attention_mask if (attention_mask is not None and\
    \ 0 in attention_mask) else None\n    892 else:\n    893     # 4d mask is passed\
    \ through the layers\n--> 894     attention_mask = _prepare_4d_causal_attention_mask(\n\
    \    895         attention_mask,\n    896         (batch_size, seq_length),\n\
    \    897         inputs_embeds,\n    898         past_key_values_length,\n   \
    \ 899         sliding_window=self.config.sliding_window,\n    900     )\n    902\
    \ hidden_states = inputs_embeds\n    904 if self.gradient_checkpointing and self.training:\n\
    \nFile ~/.local/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:217,\
    \ in _prepare_4d_causal_attention_mask(attention_mask, input_shape, inputs_embeds,\
    \ past_key_values_length, sliding_window)\n    215 # 4d mask is passed through\
    \ the layers\n    216 if attention_mask is not None:\n--> 217     attention_mask\
    \ = attn_mask_converter.to_4d(\n    218         attention_mask, input_shape[-1],\
    \ key_value_length=key_value_length, dtype=inputs_embeds.dtype\n    219     )\n\
    \    220 else:\n    221     attention_mask = attn_mask_converter.to_causal_4d(\n\
    \    222         input_shape[0], input_shape[-1], key_value_length, dtype=inputs_embeds.dtype,\
    \ device=inputs_embeds.device\n    223     )\n\nFile ~/.local/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:136,\
    \ in AttentionMaskConverter.to_4d(self, attention_mask_2d, query_length, dtype,\
    \ key_value_length)\n    132 expanded_attn_mask = self._expand_mask(attention_mask_2d,\
    \ dtype, tgt_len=input_shape[-1]).to(\n    133     attention_mask_2d.device\n\
    \    134 )\n    135 if causal_4d_mask is not None:\n--> 136     expanded_attn_mask\
    \ = causal_4d_mask.masked_fill(expanded_attn_mask.bool(), torch.finfo(dtype).min)\n\
    \    138 # expanded_attn_mask + causal_4d_mask can cause some overflow\n    139\
    \ expanded_4d_mask = expanded_attn_mask\n\nRuntimeError: The size of tensor a\
    \ (595) must match the size of tensor b (1189) at non-singleton dimension 3"
  created_at: 2023-12-11 18:52:30+00:00
  edited: false
  hidden: false
  id: 65775a6e256953fd7640af23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-11T18:57:24.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9922555088996887
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Attention mask seems to be wrong, but I think we fixed this on main</p>

          '
        raw: Attention mask seems to be wrong, but I think we fixed this on main
        updatedAt: '2023-12-11T18:57:24.509Z'
      numEdits: 0
      reactions: []
    id: 65775b94ee80150bcd188dea
    type: comment
  author: ArthurZ
  content: Attention mask seems to be wrong, but I think we fixed this on main
  created_at: 2023-12-11 18:57:24+00:00
  edited: false
  hidden: false
  id: 65775b94ee80150bcd188dea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-11T19:04:38.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9012632966041565
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mikeytrw&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mikeytrw\">@<span class=\"\
          underline\">mikeytrw</span></a></span>\n\n\t</span></span> the fix should\
          \ be addressed recently in a PR by <span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\"\
          >@<span class=\"underline\">ArthurZ</span></a></span>\n\n\t</span></span>\
          \ , can you try to uninstall transformers and re-install it? <code>pip uninstall\
          \ transformers &amp;&amp; pip install -U transformers</code></p>\n"
        raw: '@mikeytrw the fix should be addressed recently in a PR by @ArthurZ ,
          can you try to uninstall transformers and re-install it? `pip uninstall
          transformers && pip install -U transformers`'
        updatedAt: '2023-12-11T19:04:38.361Z'
      numEdits: 0
      reactions: []
    id: 65775d464bf0dee5d7681f04
    type: comment
  author: ybelkada
  content: '@mikeytrw the fix should be addressed recently in a PR by @ArthurZ , can
    you try to uninstall transformers and re-install it? `pip uninstall transformers
    && pip install -U transformers`'
  created_at: 2023-12-11 19:04:38+00:00
  edited: false
  hidden: false
  id: 65775d464bf0dee5d7681f04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
      fullname: Mike Wharton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeytrw
      type: user
    createdAt: '2023-12-11T19:58:22.000Z'
    data:
      edited: false
      editors:
      - mikeytrw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9962554574012756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
          fullname: Mike Wharton
          isHf: false
          isPro: false
          name: mikeytrw
          type: user
        html: '<p>it''s now working! thanks</p>

          '
        raw: it's now working! thanks
        updatedAt: '2023-12-11T19:58:22.170Z'
      numEdits: 0
      reactions: []
    id: 657769de7f5da1deb652df72
    type: comment
  author: mikeytrw
  content: it's now working! thanks
  created_at: 2023-12-11 19:58:22+00:00
  edited: false
  hidden: false
  id: 657769de7f5da1deb652df72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-11T20:53:28.000Z'
    data:
      status: closed
    id: 657776c873080b490ca98944
    type: status-change
  author: ArthurZ
  created_at: 2023-12-11 20:53:28+00:00
  id: 657776c873080b490ca98944
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: llava-hf/bakLlava-v1-hf
repo_type: model
status: closed
target_branch: null
title: 'Inference error: The size of tensor a (616) must match the size of tensor
  b (1231) at non-singleton dimension 3'
