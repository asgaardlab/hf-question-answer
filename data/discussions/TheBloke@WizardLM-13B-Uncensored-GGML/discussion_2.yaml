!!python/object:huggingface_hub.community.DiscussionWithDetails
author: webslug
conflicting_files: null
created_at: 2023-06-22 05:13:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/100a08fb1d2d9294880437fda47c9f93.svg
      fullname: Tim Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: webslug
      type: user
    createdAt: '2023-06-22T06:13:36.000Z'
    data:
      edited: true
      editors:
      - webslug
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9152638912200928
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/100a08fb1d2d9294880437fda47c9f93.svg
          fullname: Tim Smith
          isHf: false
          isPro: false
          name: webslug
          type: user
        html: '<p>Hi will this model run in oobabooga on a GTX 3060 with 12 GB of
          VRAM?</p>

          <p>I have 16GB of DDR 4 RAM.  Any help is appreciated.  Thanks.</p>

          <p>Specs: I7 6700<br>RAM: 16GB DDR 4<br>GPU: GTX 3060 12 GB VRAM</p>

          '
        raw: 'Hi will this model run in oobabooga on a GTX 3060 with 12 GB of VRAM?


          I have 16GB of DDR 4 RAM.  Any help is appreciated.  Thanks.


          Specs: I7 6700

          RAM: 16GB DDR 4

          GPU: GTX 3060 12 GB VRAM'
        updatedAt: '2023-06-22T06:14:07.654Z'
      numEdits: 1
      reactions: []
    id: 6493e69014b2e2d47e4f2d8e
    type: comment
  author: webslug
  content: 'Hi will this model run in oobabooga on a GTX 3060 with 12 GB of VRAM?


    I have 16GB of DDR 4 RAM.  Any help is appreciated.  Thanks.


    Specs: I7 6700

    RAM: 16GB DDR 4

    GPU: GTX 3060 12 GB VRAM'
  created_at: 2023-06-22 05:13:36+00:00
  edited: true
  hidden: false
  id: 6493e69014b2e2d47e4f2d8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T08:25:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8635630011558533
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes it will. I''d recommend using text-generation-webui with the
          new ExLlama loader, which will use less than 10GB VRAM.</p>

          <p>If you can''t use ExLlama for any reason, then you can use the default
          AutoGPTQ loader, but should tick the box that says <code>no_inject_fused_attention</code>
          - this will reduce VRAM usage so it won''t exceed 12GB. Without this, a
          13B model loaded with AutoGPTQ will out-of-memory on a 12GB card.</p>

          '
        raw: 'Yes it will. I''d recommend using text-generation-webui with the new
          ExLlama loader, which will use less than 10GB VRAM.


          If you can''t use ExLlama for any reason, then you can use the default AutoGPTQ
          loader, but should tick the box that says `no_inject_fused_attention` -
          this will reduce VRAM usage so it won''t exceed 12GB. Without this, a 13B
          model loaded with AutoGPTQ will out-of-memory on a 12GB card.'
        updatedAt: '2023-06-22T08:25:51.260Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - webslug
    id: 6494058f3918c61955d20207
    type: comment
  author: TheBloke
  content: 'Yes it will. I''d recommend using text-generation-webui with the new ExLlama
    loader, which will use less than 10GB VRAM.


    If you can''t use ExLlama for any reason, then you can use the default AutoGPTQ
    loader, but should tick the box that says `no_inject_fused_attention` - this will
    reduce VRAM usage so it won''t exceed 12GB. Without this, a 13B model loaded with
    AutoGPTQ will out-of-memory on a 12GB card.'
  created_at: 2023-06-22 07:25:51+00:00
  edited: false
  hidden: false
  id: 6494058f3918c61955d20207
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardLM-13B-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: Will this work on my 3060 12GB?
