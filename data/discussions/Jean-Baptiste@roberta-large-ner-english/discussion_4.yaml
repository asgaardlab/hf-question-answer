!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArthurParkerhouse
conflicting_files: null
created_at: 2022-12-09 16:23:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
      fullname: Parkerhouse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurParkerhouse
      type: user
    createdAt: '2022-12-09T16:23:27.000Z'
    data:
      edited: false
      editors:
      - ArthurParkerhouse
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
          fullname: Parkerhouse
          isHf: false
          isPro: false
          name: ArthurParkerhouse
          type: user
        html: '<p>Is there any way to use this on text that is longer that 512 words?
          I''m attempting to use it in a google Collab, but it only seems to be reading
          a small portion of the text file I''m giving to it. I''m trying to get all
          of the characters names, locations and orgs from a somewhat short novel
          which I split into 10,000 words per text file. </p>

          '
        raw: 'Is there any way to use this on text that is longer that 512 words?
          I''m attempting to use it in a google Collab, but it only seems to be reading
          a small portion of the text file I''m giving to it. I''m trying to get all
          of the characters names, locations and orgs from a somewhat short novel
          which I split into 10,000 words per text file. '
        updatedAt: '2022-12-09T16:23:27.355Z'
      numEdits: 0
      reactions: []
    id: 639360ff9694ec0d025ef929
    type: comment
  author: ArthurParkerhouse
  content: 'Is there any way to use this on text that is longer that 512 words? I''m
    attempting to use it in a google Collab, but it only seems to be reading a small
    portion of the text file I''m giving to it. I''m trying to get all of the characters
    names, locations and orgs from a somewhat short novel which I split into 10,000
    words per text file. '
  created_at: 2022-12-09 16:23:27+00:00
  edited: false
  hidden: false
  id: 639360ff9694ec0d025ef929
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/82296b9e5748bd60b88b608f46311db2.svg
      fullname: JB Polle
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Jean-Baptiste
      type: user
    createdAt: '2022-12-10T03:30:53.000Z'
    data:
      edited: false
      editors:
      - Jean-Baptiste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/82296b9e5748bd60b88b608f46311db2.svg
          fullname: JB Polle
          isHf: false
          isPro: false
          name: Jean-Baptiste
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;ArthurParkerhouse&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ArthurParkerhouse\"\
          >@<span class=\"underline\">ArthurParkerhouse</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>No there is no way to use this model on more than 512 tokens.\
          \ This is linked to the architecture of the Roberta model which is defined\
          \ like that. You might find some model that can accept more token but there\
          \ will always be a limit. I would recommend that you split your text in\
          \ shorter text. I usually split my text by sentence before sending it in\
          \ the model. You can keep track of the position of each sentence if you\
          \ need to have the position of the entity in the original text. I hope that\
          \ it helps.</p>\n<p>Thanks,<br>Jean-Baptiste</p>\n"
        raw: 'Hello @ArthurParkerhouse ,


          No there is no way to use this model on more than 512 tokens. This is linked
          to the architecture of the Roberta model which is defined like that. You
          might find some model that can accept more token but there will always be
          a limit. I would recommend that you split your text in shorter text. I usually
          split my text by sentence before sending it in the model. You can keep track
          of the position of each sentence if you need to have the position of the
          entity in the original text. I hope that it helps.


          Thanks,

          Jean-Baptiste'
        updatedAt: '2022-12-10T03:30:53.158Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurParkerhouse
    id: 6393fd6d18601a2770780737
    type: comment
  author: Jean-Baptiste
  content: 'Hello @ArthurParkerhouse ,


    No there is no way to use this model on more than 512 tokens. This is linked to
    the architecture of the Roberta model which is defined like that. You might find
    some model that can accept more token but there will always be a limit. I would
    recommend that you split your text in shorter text. I usually split my text by
    sentence before sending it in the model. You can keep track of the position of
    each sentence if you need to have the position of the entity in the original text.
    I hope that it helps.


    Thanks,

    Jean-Baptiste'
  created_at: 2022-12-10 03:30:53+00:00
  edited: false
  hidden: false
  id: 6393fd6d18601a2770780737
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Jean-Baptiste/roberta-large-ner-english
repo_type: model
status: open
target_branch: null
title: Hello,
