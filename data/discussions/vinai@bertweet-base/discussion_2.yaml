!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fernandopc
conflicting_files: null
created_at: 2022-09-10 17:36:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eff591c4fc0d8881a18f057088998a1f.svg
      fullname: Fernando Pereira Carneiro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fernandopc
      type: user
    createdAt: '2022-09-10T18:36:14.000Z'
    data:
      edited: false
      editors:
      - fernandopc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eff591c4fc0d8881a18f057088998a1f.svg
          fullname: Fernando Pereira Carneiro
          isHf: false
          isPro: false
          name: fernandopc
          type: user
        html: '<p>Hi, I have trained a LM for tweets in Portuguese following most
          of your work here. I am stuck now in a issue that I think you guys authors
          of BERTweet might help.</p>

          <p>Well, to adapt the BERTweetTokenizer for Portuguese we cloned the entire
          BERTweetTokenizer code and changed only one simple line of it:</p>

          <p><strong>From:</strong> </p>

          <pre><code class="language-python"><span class="hljs-keyword">return</span>
          self.demojizer(token)

          </code></pre>

          <p><strong>To:</strong> </p>

          <pre><code class="language-python"><span class="hljs-keyword">return</span>
          self.demojizer(token, language=<span class="hljs-string">''pt''</span>)

          </code></pre>

          <p>We were able to train the Tokenizer and the LM from scratch with no problem.
          The only inconvenient is that we could not instantiate it from AutoTokenizer
          but it worked from <code>BERTweetBRTokenizer.from_pretrained</code> method
          while we conducted experiments locally. However, now we want to finally
          share the model and also tokenizer to the world through Hugging Face and
          publish the work. Then, allowing users to instatiate from AutoModel and
          AutoTokenizer classes.</p>

          <p>We created the the repo and procedures to upload the model and tokenizer
          to hugging face following documentation. We were able to instantiate the
          model using AutoModel but did not manage to do the same for tokenizer. </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1662834182962-6184a16b8aaadc9253c2dfaa.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1662834182962-6184a16b8aaadc9253c2dfaa.png"></a></p>

          <p>It is clear that BERTweetBRTokenizer was not found and I was able to
          work around this locally but in order to make it truly available to the
          comunity I wanted to work exactly the same every other models like BERTweet
          using Auto* classes. As you use very similar approach and are the base of
          my work for Portuguese I was wondering what you guys have done to make that
          works from AutoTokenizer. Can you give me a tip on that?</p>

          <p>Additionally what you would do if you needed to extend BERTweet to a
          different language? Would you do the same we did here creating a whole new
          .py file for a specific tokenizer language but actually changing on single
          line? (in fact, only passing a different parameter value)?</p>

          <p>I appreciate in advance your help.<br>Fernando.</p>

          '
        raw: "Hi, I have trained a LM for tweets in Portuguese following most of your\
          \ work here. I am stuck now in a issue that I think you guys authors of\
          \ BERTweet might help.\r\n\r\n\r\n\r\nWell, to adapt the BERTweetTokenizer\
          \ for Portuguese we cloned the entire BERTweetTokenizer code and changed\
          \ only one simple line of it:\r\n\r\n**From:** \r\n```python\r\nreturn self.demojizer(token)\r\
          \n```\r\n\r\n**To:** \r\n```python\r\nreturn self.demojizer(token, language='pt')\r\
          \n```\r\n\r\nWe were able to train the Tokenizer and the LM from scratch\
          \ with no problem. The only inconvenient is that we could not instantiate\
          \ it from AutoTokenizer but it worked from `BERTweetBRTokenizer.from_pretrained`\
          \ method while we conducted experiments locally. However, now we want to\
          \ finally share the model and also tokenizer to the world through Hugging\
          \ Face and publish the work. Then, allowing users to instatiate from AutoModel\
          \ and AutoTokenizer classes.\r\n\r\nWe created the the repo and procedures\
          \ to upload the model and tokenizer to hugging face following documentation.\
          \ We were able to instantiate the model using AutoModel but did not manage\
          \ to do the same for tokenizer. \r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1662834182962-6184a16b8aaadc9253c2dfaa.png)\r\
          \n\r\nIt is clear that BERTweetBRTokenizer was not found and I was able\
          \ to work around this locally but in order to make it truly available to\
          \ the comunity I wanted to work exactly the same every other models like\
          \ BERTweet using Auto* classes. As you use very similar approach and are\
          \ the base of my work for Portuguese I was wondering what you guys have\
          \ done to make that works from AutoTokenizer. Can you give me a tip on that?\r\
          \n\r\nAdditionally what you would do if you needed to extend BERTweet to\
          \ a different language? Would you do the same we did here creating a whole\
          \ new .py file for a specific tokenizer language but actually changing on\
          \ single line? (in fact, only passing a different parameter value)?\r\n\r\
          \nI appreciate in advance your help.\r\nFernando."
        updatedAt: '2022-09-10T18:36:14.778Z'
      numEdits: 0
      reactions: []
    id: 631cd91ea31a0a462ca2384f
    type: comment
  author: fernandopc
  content: "Hi, I have trained a LM for tweets in Portuguese following most of your\
    \ work here. I am stuck now in a issue that I think you guys authors of BERTweet\
    \ might help.\r\n\r\n\r\n\r\nWell, to adapt the BERTweetTokenizer for Portuguese\
    \ we cloned the entire BERTweetTokenizer code and changed only one simple line\
    \ of it:\r\n\r\n**From:** \r\n```python\r\nreturn self.demojizer(token)\r\n```\r\
    \n\r\n**To:** \r\n```python\r\nreturn self.demojizer(token, language='pt')\r\n\
    ```\r\n\r\nWe were able to train the Tokenizer and the LM from scratch with no\
    \ problem. The only inconvenient is that we could not instantiate it from AutoTokenizer\
    \ but it worked from `BERTweetBRTokenizer.from_pretrained` method while we conducted\
    \ experiments locally. However, now we want to finally share the model and also\
    \ tokenizer to the world through Hugging Face and publish the work. Then, allowing\
    \ users to instatiate from AutoModel and AutoTokenizer classes.\r\n\r\nWe created\
    \ the the repo and procedures to upload the model and tokenizer to hugging face\
    \ following documentation. We were able to instantiate the model using AutoModel\
    \ but did not manage to do the same for tokenizer. \r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1662834182962-6184a16b8aaadc9253c2dfaa.png)\r\
    \n\r\nIt is clear that BERTweetBRTokenizer was not found and I was able to work\
    \ around this locally but in order to make it truly available to the comunity\
    \ I wanted to work exactly the same every other models like BERTweet using Auto*\
    \ classes. As you use very similar approach and are the base of my work for Portuguese\
    \ I was wondering what you guys have done to make that works from AutoTokenizer.\
    \ Can you give me a tip on that?\r\n\r\nAdditionally what you would do if you\
    \ needed to extend BERTweet to a different language? Would you do the same we\
    \ did here creating a whole new .py file for a specific tokenizer language but\
    \ actually changing on single line? (in fact, only passing a different parameter\
    \ value)?\r\n\r\nI appreciate in advance your help.\r\nFernando."
  created_at: 2022-09-10 17:36:14+00:00
  edited: false
  hidden: false
  id: 631cd91ea31a0a462ca2384f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
      fullname: Dat Quoc Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dqnguyen
      type: user
    createdAt: '2022-09-12T03:20:52.000Z'
    data:
      edited: false
      editors:
      - dqnguyen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
          fullname: Dat Quoc Nguyen
          isHf: false
          isPro: false
          name: dqnguyen
          type: user
        html: "<p>Did you include BertweetBRTokenizer into \"tokenization_auto.py\"\
          ?<br>For a different language, e.g. Portuguese, I would create a new tokenizer\
          \ for that language, i.e. including a new vocabulary of BPE subword tokens.\
          \ At the moment, as far as I understand, you're reusing the BertweetTokenizer's\
          \ vocabulary that is specified for English.<br>OR, you can simply reuse\
          \ the Tokenizer from \"XLM-T: Multilingual Language Models in Twitter for\
          \ Sentiment Analysis and Beyond\" without changing anything. \n  </p>\n"
        raw: "Did you include BertweetBRTokenizer into \"tokenization_auto.py\"?\n\
          For a different language, e.g. Portuguese, I would create a new tokenizer\
          \ for that language, i.e. including a new vocabulary of BPE subword tokens.\
          \ At the moment, as far as I understand, you're reusing the BertweetTokenizer's\
          \ vocabulary that is specified for English.\nOR, you can simply reuse the\
          \ Tokenizer from \"XLM-T: Multilingual Language Models in Twitter for Sentiment\
          \ Analysis and Beyond\" without changing anything. \n  "
        updatedAt: '2022-09-12T03:20:52.751Z'
      numEdits: 0
      reactions: []
      relatedEventId: 631ea5947beada30465faf98
    id: 631ea5947beada30465faf97
    type: comment
  author: dqnguyen
  content: "Did you include BertweetBRTokenizer into \"tokenization_auto.py\"?\nFor\
    \ a different language, e.g. Portuguese, I would create a new tokenizer for that\
    \ language, i.e. including a new vocabulary of BPE subword tokens. At the moment,\
    \ as far as I understand, you're reusing the BertweetTokenizer's vocabulary that\
    \ is specified for English.\nOR, you can simply reuse the Tokenizer from \"XLM-T:\
    \ Multilingual Language Models in Twitter for Sentiment Analysis and Beyond\"\
    \ without changing anything. \n  "
  created_at: 2022-09-12 02:20:52+00:00
  edited: false
  hidden: false
  id: 631ea5947beada30465faf97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
      fullname: Dat Quoc Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dqnguyen
      type: user
    createdAt: '2022-09-12T03:20:52.000Z'
    data:
      status: closed
    id: 631ea5947beada30465faf98
    type: status-change
  author: dqnguyen
  created_at: 2022-09-12 02:20:52+00:00
  id: 631ea5947beada30465faf98
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: vinai/bertweet-base
repo_type: model
status: closed
target_branch: null
title: Specializing BERTweet for different languages.
