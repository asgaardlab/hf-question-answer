!!python/object:huggingface_hub.community.DiscussionWithDetails
author: clown134
conflicting_files: null
created_at: 2023-10-18 21:38:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
      fullname: katie barber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clown134
      type: user
    createdAt: '2023-10-18T22:38:37.000Z'
    data:
      edited: false
      editors:
      - clown134
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8889926671981812
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
          fullname: katie barber
          isHf: false
          isPro: false
          name: clown134
          type: user
        html: '<p>im not too sure if its possible, but i think id love to try these
          mlewd models out, but my 1060 only has 6gb of vram and has a hard time handling
          such a large model.<br>if possible id love a 7b version of this or the chat
          version of this model.<br>please and thank you!<br>currently im using MistRP-AirOrca-7B.q5_k_m.gguf
          and love it</p>

          '
        raw: "im not too sure if its possible, but i think id love to try these mlewd\
          \ models out, but my 1060 only has 6gb of vram and has a hard time handling\
          \ such a large model.\r\nif possible id love a 7b version of this or the\
          \ chat version of this model.\r\nplease and thank you!\r\ncurrently im using\
          \ MistRP-AirOrca-7B.q5_k_m.gguf and love it"
        updatedAt: '2023-10-18T22:38:37.178Z'
      numEdits: 0
      reactions: []
    id: 65305e6d944086d1d507ca62
    type: comment
  author: clown134
  content: "im not too sure if its possible, but i think id love to try these mlewd\
    \ models out, but my 1060 only has 6gb of vram and has a hard time handling such\
    \ a large model.\r\nif possible id love a 7b version of this or the chat version\
    \ of this model.\r\nplease and thank you!\r\ncurrently im using MistRP-AirOrca-7B.q5_k_m.gguf\
    \ and love it"
  created_at: 2023-10-18 21:38:37+00:00
  edited: false
  hidden: false
  id: 65305e6d944086d1d507ca62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4beb36c1c1be65ba151dc95d30d6963.svg
      fullname: Nepost Szlyak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d7dd0d
      type: user
    createdAt: '2023-11-18T17:40:27.000Z'
    data:
      edited: false
      editors:
      - d7dd0d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8976044058799744
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4beb36c1c1be65ba151dc95d30d6963.svg
          fullname: Nepost Szlyak
          isHf: false
          isPro: false
          name: d7dd0d
          type: user
        html: '<p>Well, since you''re already familiar with GGUF, why not give <a
          href="https://huggingface.co/Undi95/MLewd-v2.4-13B-GGUF">Undi95/MLewd-v2.4-13B-GGUF</a>
          a try? I''m pretty sure it performs better than any 7b version of this model.</p>

          '
        raw: Well, since you're already familiar with GGUF, why not give [Undi95/MLewd-v2.4-13B-GGUF](https://huggingface.co/Undi95/MLewd-v2.4-13B-GGUF)
          a try? I'm pretty sure it performs better than any 7b version of this model.
        updatedAt: '2023-11-18T17:40:27.920Z'
      numEdits: 0
      reactions: []
    id: 6558f70bc619b485313c0eb8
    type: comment
  author: d7dd0d
  content: Well, since you're already familiar with GGUF, why not give [Undi95/MLewd-v2.4-13B-GGUF](https://huggingface.co/Undi95/MLewd-v2.4-13B-GGUF)
    a try? I'm pretty sure it performs better than any 7b version of this model.
  created_at: 2023-11-18 17:40:27+00:00
  edited: false
  hidden: false
  id: 6558f70bc619b485313c0eb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-18T17:57:28.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9839694499969482
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Hello, I completely forgot this, I can''t shrink down the model
          to 7B because all ressource used was 13B. Sorry!</p>

          '
        raw: Hello, I completely forgot this, I can't shrink down the model to 7B
          because all ressource used was 13B. Sorry!
        updatedAt: '2023-11-18T17:57:28.754Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - d7dd0d
    id: 6558fb084cd8d44865a9218c
    type: comment
  author: Undi95
  content: Hello, I completely forgot this, I can't shrink down the model to 7B because
    all ressource used was 13B. Sorry!
  created_at: 2023-11-18 17:57:28+00:00
  edited: false
  hidden: false
  id: 6558fb084cd8d44865a9218c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
      fullname: Matthew Andrews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueNipples
      type: user
    createdAt: '2023-11-19T13:46:59.000Z'
    data:
      edited: true
      editors:
      - BlueNipples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8771716356277466
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
          fullname: Matthew Andrews
          isHf: false
          isPro: false
          name: BlueNipples
          type: user
        html: '<blockquote>

          <p>im not too sure if its possible, but i think id love to try these mlewd
          models out, but my 1060 only has 6gb of vram and has a hard time handling
          such a large model.<br>if possible id love a 7b version of this or the chat
          version of this model.<br>please and thank you!<br>currently im using MistRP-AirOrca-7B.q5_k_m.gguf
          and love it</p>

          </blockquote>

          <p>Try Undi95''s toppy. It''s <em>basically</em> mlewd for 7b (decent prose,
          pretty dirty). You could potentially use a 2k gguf of a 13b too, but mistral
          7b models fit sweet into vram because of the sliding window (reduces context
          size ram use a lot)</p>

          '
        raw: '> im not too sure if its possible, but i think id love to try these
          mlewd models out, but my 1060 only has 6gb of vram and has a hard time handling
          such a large model.

          > if possible id love a 7b version of this or the chat version of this model.

          > please and thank you!

          > currently im using MistRP-AirOrca-7B.q5_k_m.gguf and love it


          Try Undi95''s toppy. It''s _basically_ mlewd for 7b (decent prose, pretty
          dirty). You could potentially use a 2k gguf of a 13b too, but mistral 7b
          models fit sweet into vram because of the sliding window (reduces context
          size ram use a lot)'
        updatedAt: '2023-11-19T13:48:15.645Z'
      numEdits: 1
      reactions: []
    id: 655a11d3b158b94c3ac4e576
    type: comment
  author: BlueNipples
  content: '> im not too sure if its possible, but i think id love to try these mlewd
    models out, but my 1060 only has 6gb of vram and has a hard time handling such
    a large model.

    > if possible id love a 7b version of this or the chat version of this model.

    > please and thank you!

    > currently im using MistRP-AirOrca-7B.q5_k_m.gguf and love it


    Try Undi95''s toppy. It''s _basically_ mlewd for 7b (decent prose, pretty dirty).
    You could potentially use a 2k gguf of a 13b too, but mistral 7b models fit sweet
    into vram because of the sliding window (reduces context size ram use a lot)'
  created_at: 2023-11-19 13:46:59+00:00
  edited: true
  hidden: false
  id: 655a11d3b158b94c3ac4e576
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
      fullname: katie barber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clown134
      type: user
    createdAt: '2023-11-20T02:56:06.000Z'
    data:
      edited: false
      editors:
      - clown134
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.985912561416626
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
          fullname: katie barber
          isHf: false
          isPro: false
          name: clown134
          type: user
        html: '<p>thanks. toppy is pretty good. it JUST fits into my vram and response
          extremely fast, faster than i can type certainly. i have 6gb vram so 13b
          models are extremely slow, but i could see them being useful if it was for
          something i needed coherency on</p>

          '
        raw: thanks. toppy is pretty good. it JUST fits into my vram and response
          extremely fast, faster than i can type certainly. i have 6gb vram so 13b
          models are extremely slow, but i could see them being useful if it was for
          something i needed coherency on
        updatedAt: '2023-11-20T02:56:06.410Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BlueNipples
    id: 655acac67f246643398bae80
    type: comment
  author: clown134
  content: thanks. toppy is pretty good. it JUST fits into my vram and response extremely
    fast, faster than i can type certainly. i have 6gb vram so 13b models are extremely
    slow, but i could see them being useful if it was for something i needed coherency
    on
  created_at: 2023-11-20 02:56:06+00:00
  edited: false
  hidden: false
  id: 655acac67f246643398bae80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Undi95/MLewd-v2.4-13B
repo_type: model
status: open
target_branch: null
title: any possibility of getting a 7b version of this somehow?
