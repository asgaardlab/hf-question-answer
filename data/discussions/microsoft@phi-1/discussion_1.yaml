!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gardner
conflicting_files: null
created_at: 2023-09-12 10:29:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2023-09-12T11:29:39.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950044572353363
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: '<p>I really enjoyed the paper. Thanks for publishing these weights.
          Are there any code examples of fine tuning phi-1?</p>

          <p>Cheers</p>

          '
        raw: "I really enjoyed the paper. Thanks for publishing these weights. Are\
          \ there any code examples of fine tuning phi-1?\r\n\r\nCheers"
        updatedAt: '2023-09-12T11:29:39.492Z'
      numEdits: 0
      reactions: []
    id: 65004ba3f4ab53ce6d99bdbe
    type: comment
  author: gardner
  content: "I really enjoyed the paper. Thanks for publishing these weights. Are there\
    \ any code examples of fine tuning phi-1?\r\n\r\nCheers"
  created_at: 2023-09-12 10:29:39+00:00
  edited: false
  hidden: false
  id: 65004ba3f4ab53ce6d99bdbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-12T16:48:34.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5233068466186523
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;gardner&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gardner\"\
          >@<span class=\"underline\">gardner</span></a></span>\n\n\t</span></span>!\
          \ I hope everything is going well with you.</p>\n<p>You can use the following\
          \ snippet to fine-tune the model:</p>\n<pre><code>import torch\nfrom datasets\
          \ import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
          \ Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\", trust_remote_code=True,\
          \ torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\"\
          , trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer.pad_token = tokenizer.eos_token\n\
          \ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\"\
          )\ndataset = dataset.map(lambda x: tokenizer(x[\"text\"], return_tensors=\"\
          pt\", padding=\"max_length\", truncation=True), batched=True)\ndata_collator\
          \ = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\
          training_args = TrainingArguments(\"tmp\", max_steps=1, per_device_train_batch_size=1)\n\
          trainer = Trainer(model, args=training_args, train_dataset=dataset, data_collator=data_collator)\n\
          \ntrainer.train()\n</code></pre>\n<p>But please be aware that this is only\
          \ an example, because the model still does not support <code>attention_mask</code>\
          \ / padding. You will need to create a contiguous dataset that provides\
          \ sequences with full length.</p>\n<p>Regards,<br>Gustavo.</p>\n"
        raw: 'Hello @gardner! I hope everything is going well with you.


          You can use the following snippet to fine-tune the model:


          ```

          import torch

          from datasets import load_dataset

          from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,
          DataCollatorForLanguageModeling


          model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1", trust_remote_code=True,
          torch_dtype="auto")

          tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1", trust_remote_code=True,
          torch_dtype="auto")

          tokenizer.pad_token = tokenizer.eos_token


          dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1%]")

          dataset = dataset.map(lambda x: tokenizer(x["text"], return_tensors="pt",
          padding="max_length", truncation=True), batched=True)

          data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)


          training_args = TrainingArguments("tmp", max_steps=1, per_device_train_batch_size=1)

          trainer = Trainer(model, args=training_args, train_dataset=dataset, data_collator=data_collator)


          trainer.train()

          ```


          But please be aware that this is only an example, because the model still
          does not support `attention_mask` / padding. You will need to create a contiguous
          dataset that provides sequences with full length.


          Regards,

          Gustavo.'
        updatedAt: '2023-09-12T16:48:34.008Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - gardner
        - susnato
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nulltella
    id: 65009662f4ab53ce6da54651
    type: comment
  author: gugarosa
  content: 'Hello @gardner! I hope everything is going well with you.


    You can use the following snippet to fine-tune the model:


    ```

    import torch

    from datasets import load_dataset

    from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,
    DataCollatorForLanguageModeling


    model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1", trust_remote_code=True,
    torch_dtype="auto")

    tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1", trust_remote_code=True,
    torch_dtype="auto")

    tokenizer.pad_token = tokenizer.eos_token


    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1%]")

    dataset = dataset.map(lambda x: tokenizer(x["text"], return_tensors="pt", padding="max_length",
    truncation=True), batched=True)

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)


    training_args = TrainingArguments("tmp", max_steps=1, per_device_train_batch_size=1)

    trainer = Trainer(model, args=training_args, train_dataset=dataset, data_collator=data_collator)


    trainer.train()

    ```


    But please be aware that this is only an example, because the model still does
    not support `attention_mask` / padding. You will need to create a contiguous dataset
    that provides sequences with full length.


    Regards,

    Gustavo.'
  created_at: 2023-09-12 15:48:34+00:00
  edited: false
  hidden: false
  id: 65009662f4ab53ce6da54651
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2023-09-12T21:58:46.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.16054198145866394
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ !</p>\n"
        raw: Thank you @gugarosa !
        updatedAt: '2023-09-12T21:58:46.900Z'
      numEdits: 0
      reactions: []
    id: 6500df162233e7d7f5653f0b
    type: comment
  author: gardner
  content: Thank you @gugarosa !
  created_at: 2023-09-12 20:58:46+00:00
  edited: false
  hidden: false
  id: 6500df162233e7d7f5653f0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-15T22:50:29.000Z'
    data:
      status: closed
    id: 6504dfb5d3219dc63c5928aa
    type: status-change
  author: gugarosa
  created_at: 2023-09-15 21:50:29+00:00
  id: 6504dfb5d3219dc63c5928aa
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7dea90e85e25f7b7138bd2055ff5eab2.svg
      fullname: Xinyi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xinyi-wang
      type: user
    createdAt: '2023-09-23T16:36:39.000Z'
    data:
      edited: false
      editors:
      - xinyi-wang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.540325403213501
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7dea90e85e25f7b7138bd2055ff5eab2.svg
          fullname: Xinyi Wang
          isHf: false
          isPro: false
          name: xinyi-wang
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ thanks for the example training code! Just wanted to clarify, what do\
          \ you mean by \"create a contiguous dataset that provides sequences with\
          \ full length\"? Because when I tried to fine tune phi1 with a similar training\
          \ code, I got error like this:</p>\n<pre><code>Traceback (most recent call\
          \ last):\n  File \"/home/t-xinyiwang/reasoning-tuning/train.py\", line 427,\
          \ in &lt;module&gt;\n    train()\n  File \"/home/t-xinyiwang/reasoning-tuning/train.py\"\
          , line 413, in train\n    trainer.train()\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1553, in train\n    return inner_training_loop(\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1835, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
          \ inputs)\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2679, in training_step\n    loss = self.compute_loss(model, inputs)\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2704, in compute_loss\n    outputs = model(**inputs)\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1148, in forward\n    self._sync_buffers()\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1748, in _sync_buffers\n    self._sync_module_buffers(authoritative_rank)\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1752, in _sync_module_buffers\n    self._default_broadcast_coalesced(\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1775, in _default_broadcast_coalesced\n    self._distributed_broadcast_coalesced(\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1689, in _distributed_broadcast_coalesced\n    dist._broadcast_coalesced(\n\
          RuntimeError: Tensors must be CUDA and dense\n</code></pre>\n<p>Thank you</p>\n"
        raw: "Hi @gugarosa thanks for the example training code! Just wanted to clarify,\
          \ what do you mean by \"create a contiguous dataset that provides sequences\
          \ with full length\"? Because when I tried to fine tune phi1 with a similar\
          \ training code, I got error like this:\n\n```\nTraceback (most recent call\
          \ last):\n  File \"/home/t-xinyiwang/reasoning-tuning/train.py\", line 427,\
          \ in <module>\n    train()\n  File \"/home/t-xinyiwang/reasoning-tuning/train.py\"\
          , line 413, in train\n    trainer.train()\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1553, in train\n    return inner_training_loop(\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1835, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
          \ inputs)\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2679, in training_step\n    loss = self.compute_loss(model, inputs)\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2704, in compute_loss\n    outputs = model(**inputs)\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1148, in forward\n    self._sync_buffers()\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1748, in _sync_buffers\n    self._sync_module_buffers(authoritative_rank)\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1752, in _sync_module_buffers\n    self._default_broadcast_coalesced(\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1775, in _default_broadcast_coalesced\n    self._distributed_broadcast_coalesced(\n\
          \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1689, in _distributed_broadcast_coalesced\n    dist._broadcast_coalesced(\n\
          RuntimeError: Tensors must be CUDA and dense\n```\n\nThank you"
        updatedAt: '2023-09-23T16:36:39.488Z'
      numEdits: 0
      reactions: []
    id: 650f1417671604f140d962f5
    type: comment
  author: xinyi-wang
  content: "Hi @gugarosa thanks for the example training code! Just wanted to clarify,\
    \ what do you mean by \"create a contiguous dataset that provides sequences with\
    \ full length\"? Because when I tried to fine tune phi1 with a similar training\
    \ code, I got error like this:\n\n```\nTraceback (most recent call last):\n  File\
    \ \"/home/t-xinyiwang/reasoning-tuning/train.py\", line 427, in <module>\n   \
    \ train()\n  File \"/home/t-xinyiwang/reasoning-tuning/train.py\", line 413, in\
    \ train\n    trainer.train()\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1553, in train\n    return inner_training_loop(\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1835, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
    \ inputs)\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 2679, in training_step\n    loss = self.compute_loss(model, inputs)\n \
    \ File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 2704, in compute_loss\n    outputs = model(**inputs)\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1148, in forward\n    self._sync_buffers()\n  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1748, in _sync_buffers\n    self._sync_module_buffers(authoritative_rank)\n\
    \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1752, in _sync_module_buffers\n    self._default_broadcast_coalesced(\n\
    \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1775, in _default_broadcast_coalesced\n    self._distributed_broadcast_coalesced(\n\
    \  File \"/home/t-xinyiwang/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1689, in _distributed_broadcast_coalesced\n    dist._broadcast_coalesced(\n\
    RuntimeError: Tensors must be CUDA and dense\n```\n\nThank you"
  created_at: 2023-09-23 15:36:39+00:00
  edited: false
  hidden: false
  id: 650f1417671604f140d962f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2023-10-07T07:44:13.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8725144267082214
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;xinyi-wang&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xinyi-wang\">@<span class=\"\
          underline\">xinyi-wang</span></a></span>\n\n\t</span></span>  The error\
          \ message you provided is quite detailed, and it gives us some insight into\
          \ what might be going wrong.</p>\n<p>The last error in the traceback, <code>RuntimeError:\
          \ Tensors must be CUDA and dense</code>, suggests that the model or the\
          \ data tensors are either not on the CUDA device (i.e., the GPU) or are\
          \ not in the expected format (dense).</p>\n<p>Here are some steps to troubleshoot\
          \ and potentially resolve the issue:</p>\n<ol>\n<li><p><strong>Ensure the\
          \ model is on the CUDA device:</strong> Before starting training, ensure\
          \ that you've moved the model to the CUDA device using:</p>\n<pre><code\
          \ class=\"language-python\">model = model.cuda()\n</code></pre>\n</li>\n\
          <li><p><strong>Ensure the data tensors are on the CUDA device:</strong>\
          \ Before passing data tensors to the model, ensure they are on the CUDA\
          \ device:</p>\n<pre><code class=\"language-python\">input_tensor = input_tensor.cuda()\n\
          </code></pre>\n</li>\n<li><p><strong>Check for Sparse Tensors:</strong>\
          \ The error suggests that the tensors should be dense. If you're using sparse\
          \ tensors for any reason, you'll need to convert them to dense format before\
          \ passing them to the model. If you aren't explicitly using sparse tensors,\
          \ then this might not be the issue.</p>\n</li>\n<li><p><strong>Using Distributed\
          \ Training:</strong> Since the traceback also includes references to distributed\
          \ training, ensure that you've properly initialized the distributed environment.\
          \ If you're using <code>torch.nn.parallel.DistributedDataParallel</code>,\
          \ make sure you've set up the environment correctly with <code>torch.distributed.init_process_group</code>.</p>\n\
          </li>\n<li><p><strong>Check GPU Memory:</strong> Ensure that your GPU has\
          \ enough memory to hold the model and the data. If the GPU memory is full,\
          \ it might not allow new tensors to be allocated on it.</p>\n</li>\n<li><p><strong>Update\
          \ Libraries:</strong> Sometimes, errors can be due to compatibility issues\
          \ or bugs in libraries. Ensure that you're using compatible versions of\
          \ PyTorch and Transformers. If possible, try updating both libraries to\
          \ the latest versions.</p>\n</li>\n<li><p><strong>Inspect the Training Code:</strong>\
          \ Review the training loop, data loading, and model creation to ensure there's\
          \ no part of the code accidentally converting tensors to CPU or changing\
          \ their format.</p>\n</li>\n</ol>\n<p>Can you share the training code?</p>\n"
        raw: "@xinyi-wang  The error message you provided is quite detailed, and it\
          \ gives us some insight into what might be going wrong.\n\nThe last error\
          \ in the traceback, `RuntimeError: Tensors must be CUDA and dense`, suggests\
          \ that the model or the data tensors are either not on the CUDA device (i.e.,\
          \ the GPU) or are not in the expected format (dense).\n\nHere are some steps\
          \ to troubleshoot and potentially resolve the issue:\n\n1. **Ensure the\
          \ model is on the CUDA device:** Before starting training, ensure that you've\
          \ moved the model to the CUDA device using:\n   ```python\n   model = model.cuda()\n\
          \   ```\n\n2. **Ensure the data tensors are on the CUDA device:** Before\
          \ passing data tensors to the model, ensure they are on the CUDA device:\n\
          \   ```python\n   input_tensor = input_tensor.cuda()\n   ```\n\n3. **Check\
          \ for Sparse Tensors:** The error suggests that the tensors should be dense.\
          \ If you're using sparse tensors for any reason, you'll need to convert\
          \ them to dense format before passing them to the model. If you aren't explicitly\
          \ using sparse tensors, then this might not be the issue.\n\n4. **Using\
          \ Distributed Training:** Since the traceback also includes references to\
          \ distributed training, ensure that you've properly initialized the distributed\
          \ environment. If you're using `torch.nn.parallel.DistributedDataParallel`,\
          \ make sure you've set up the environment correctly with `torch.distributed.init_process_group`.\n\
          \n5. **Check GPU Memory:** Ensure that your GPU has enough memory to hold\
          \ the model and the data. If the GPU memory is full, it might not allow\
          \ new tensors to be allocated on it.\n\n6. **Update Libraries:** Sometimes,\
          \ errors can be due to compatibility issues or bugs in libraries. Ensure\
          \ that you're using compatible versions of PyTorch and Transformers. If\
          \ possible, try updating both libraries to the latest versions.\n\n7. **Inspect\
          \ the Training Code:** Review the training loop, data loading, and model\
          \ creation to ensure there's no part of the code accidentally converting\
          \ tensors to CPU or changing their format.\n\nCan you share the training\
          \ code?"
        updatedAt: '2023-10-07T07:44:13.632Z'
      numEdits: 0
      reactions: []
    id: 65210c4dbafd014bf685501e
    type: comment
  author: gardner
  content: "@xinyi-wang  The error message you provided is quite detailed, and it\
    \ gives us some insight into what might be going wrong.\n\nThe last error in the\
    \ traceback, `RuntimeError: Tensors must be CUDA and dense`, suggests that the\
    \ model or the data tensors are either not on the CUDA device (i.e., the GPU)\
    \ or are not in the expected format (dense).\n\nHere are some steps to troubleshoot\
    \ and potentially resolve the issue:\n\n1. **Ensure the model is on the CUDA device:**\
    \ Before starting training, ensure that you've moved the model to the CUDA device\
    \ using:\n   ```python\n   model = model.cuda()\n   ```\n\n2. **Ensure the data\
    \ tensors are on the CUDA device:** Before passing data tensors to the model,\
    \ ensure they are on the CUDA device:\n   ```python\n   input_tensor = input_tensor.cuda()\n\
    \   ```\n\n3. **Check for Sparse Tensors:** The error suggests that the tensors\
    \ should be dense. If you're using sparse tensors for any reason, you'll need\
    \ to convert them to dense format before passing them to the model. If you aren't\
    \ explicitly using sparse tensors, then this might not be the issue.\n\n4. **Using\
    \ Distributed Training:** Since the traceback also includes references to distributed\
    \ training, ensure that you've properly initialized the distributed environment.\
    \ If you're using `torch.nn.parallel.DistributedDataParallel`, make sure you've\
    \ set up the environment correctly with `torch.distributed.init_process_group`.\n\
    \n5. **Check GPU Memory:** Ensure that your GPU has enough memory to hold the\
    \ model and the data. If the GPU memory is full, it might not allow new tensors\
    \ to be allocated on it.\n\n6. **Update Libraries:** Sometimes, errors can be\
    \ due to compatibility issues or bugs in libraries. Ensure that you're using compatible\
    \ versions of PyTorch and Transformers. If possible, try updating both libraries\
    \ to the latest versions.\n\n7. **Inspect the Training Code:** Review the training\
    \ loop, data loading, and model creation to ensure there's no part of the code\
    \ accidentally converting tensors to CPU or changing their format.\n\nCan you\
    \ share the training code?"
  created_at: 2023-10-07 06:44:13+00:00
  edited: false
  hidden: false
  id: 65210c4dbafd014bf685501e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7dea90e85e25f7b7138bd2055ff5eab2.svg
      fullname: Xinyi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xinyi-wang
      type: user
    createdAt: '2023-11-02T17:37:40.000Z'
    data:
      edited: false
      editors:
      - xinyi-wang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619584679603577
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7dea90e85e25f7b7138bd2055ff5eab2.svg
          fullname: Xinyi Wang
          isHf: false
          isPro: false
          name: xinyi-wang
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;gardner&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gardner\"\
          >@<span class=\"underline\">gardner</span></a></span>\n\n\t</span></span>\
          \ for the detailed reply! We have identified the issue, which is caused\
          \ by distributed training. Since we are only using one GPU, we just removed\
          \ the <code>torchrun</code> and everything works fine. </p>\n"
        raw: 'Thank you @gardner for the detailed reply! We have identified the issue,
          which is caused by distributed training. Since we are only using one GPU,
          we just removed the `torchrun` and everything works fine. '
        updatedAt: '2023-11-02T17:37:40.334Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - gardner
    id: 6543de64ee7bbb595295d18a
    type: comment
  author: xinyi-wang
  content: 'Thank you @gardner for the detailed reply! We have identified the issue,
    which is caused by distributed training. Since we are only using one GPU, we just
    removed the `torchrun` and everything works fine. '
  created_at: 2023-11-02 16:37:40+00:00
  edited: false
  hidden: false
  id: 6543de64ee7bbb595295d18a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: microsoft/phi-1
repo_type: model
status: closed
target_branch: null
title: Fine tuning example
