!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ParkerBurchett
conflicting_files: null
created_at: 2022-10-21 15:25:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
      fullname: Parker Burchett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParkerBurchett
      type: user
    createdAt: '2022-10-21T16:25:36.000Z'
    data:
      edited: false
      editors:
      - ParkerBurchett
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
          fullname: Parker Burchett
          isHf: false
          isPro: false
          name: ParkerBurchett
          type: user
        html: '<p>Has anybody checked how often the model gets stuck in loops? </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1666369397443-6312ab115beb528b5c14190b.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1666369397443-6312ab115beb528b5c14190b.png"></a></p>

          '
        raw: "Has anybody checked how often the model gets stuck in loops? \r\n\r\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1666369397443-6312ab115beb528b5c14190b.png)\r\
          \n"
        updatedAt: '2022-10-21T16:25:36.392Z'
      numEdits: 0
      reactions: []
    id: 6352c800a58cfef322d3f838
    type: comment
  author: ParkerBurchett
  content: "Has anybody checked how often the model gets stuck in loops? \r\n\r\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1666369397443-6312ab115beb528b5c14190b.png)\r\
    \n"
  created_at: 2022-10-21 15:25:36+00:00
  edited: false
  hidden: false
  id: 6352c800a58cfef322d3f838
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
      fullname: Manuel Romero
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: mrm8488
      type: user
    createdAt: '2022-10-21T16:41:11.000Z'
    data:
      edited: false
      editors:
      - mrm8488
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
          fullname: Manuel Romero
          isHf: false
          isPro: true
          name: mrm8488
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;ParkerBurchett&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ParkerBurchett\"\
          >@<span class=\"underline\">ParkerBurchett</span></a></span>\n\n\t</span></span>.\
          \ In the inference widget it doesn't seem to work too fine. I didn't tested\
          \ it there because it is slowly. Can you test in a Colab? I will add the\
          \ recommend generation params</p>\n"
        raw: Hi, @ParkerBurchett. In the inference widget it doesn't seem to work
          too fine. I didn't tested it there because it is slowly. Can you test in
          a Colab? I will add the recommend generation params
        updatedAt: '2022-10-21T16:41:11.278Z'
      numEdits: 0
      reactions: []
    id: 6352cba74523bcc31156d93f
    type: comment
  author: mrm8488
  content: Hi, @ParkerBurchett. In the inference widget it doesn't seem to work too
    fine. I didn't tested it there because it is slowly. Can you test in a Colab?
    I will add the recommend generation params
  created_at: 2022-10-21 15:41:11+00:00
  edited: false
  hidden: false
  id: 6352cba74523bcc31156d93f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
      fullname: Parker Burchett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParkerBurchett
      type: user
    createdAt: '2022-10-21T17:04:48.000Z'
    data:
      edited: true
      editors:
      - ParkerBurchett
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
          fullname: Parker Burchett
          isHf: false
          isPro: false
          name: ParkerBurchett
          type: user
        html: '<p>Sure I got it up in Colab. It''s still in a loop<br><a rel="nofollow"
          href="https://colab.research.google.com/drive/1YcH40MhJfYiFd39Q361SC6cauq2YFx9v?usp=sharing">https://colab.research.google.com/drive/1YcH40MhJfYiFd39Q361SC6cauq2YFx9v?usp=sharing</a></p>

          <pre><code>prompt = f"McDonald''s hamburger promotion on a red billboard,
          white lettering, "

          input_ids = tokenizer(prompt, return_tensors="pt").to(''cuda'')

          sample = model.generate(**input_ids, max_new_tokens=50)

          tokenizer.decode(sample[0])



          McDonald''s hamburger promotion on a red billboard, white lettering,  digital
          billboard, digital billboard, digital billboard, digital billboard, digital
          billboard, digital billboard, digital billboard, digital billboard, digital
          billboard, digital billboard, digital billboard, digital billboard, digital
          bill

          </code></pre>

          '
        raw: 'Sure I got it up in Colab. It''s still in a loop

          https://colab.research.google.com/drive/1YcH40MhJfYiFd39Q361SC6cauq2YFx9v?usp=sharing

          ```

          prompt = f"McDonald''s hamburger promotion on a red billboard, white lettering,
          "

          input_ids = tokenizer(prompt, return_tensors="pt").to(''cuda'')

          sample = model.generate(**input_ids, max_new_tokens=50)

          tokenizer.decode(sample[0])



          McDonald''s hamburger promotion on a red billboard, white lettering,  digital
          billboard, digital billboard, digital billboard, digital billboard, digital
          billboard, digital billboard, digital billboard, digital billboard, digital
          billboard, digital billboard, digital billboard, digital billboard, digital
          bill

          ```'
        updatedAt: '2022-10-21T17:06:01.309Z'
      numEdits: 1
      reactions: []
    id: 6352d130fc9b7ad0a53d618e
    type: comment
  author: ParkerBurchett
  content: 'Sure I got it up in Colab. It''s still in a loop

    https://colab.research.google.com/drive/1YcH40MhJfYiFd39Q361SC6cauq2YFx9v?usp=sharing

    ```

    prompt = f"McDonald''s hamburger promotion on a red billboard, white lettering,
    "

    input_ids = tokenizer(prompt, return_tensors="pt").to(''cuda'')

    sample = model.generate(**input_ids, max_new_tokens=50)

    tokenizer.decode(sample[0])



    McDonald''s hamburger promotion on a red billboard, white lettering,  digital
    billboard, digital billboard, digital billboard, digital billboard, digital billboard,
    digital billboard, digital billboard, digital billboard, digital billboard, digital
    billboard, digital billboard, digital billboard, digital bill

    ```'
  created_at: 2022-10-21 16:04:48+00:00
  edited: true
  hidden: false
  id: 6352d130fc9b7ad0a53d618e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
      fullname: Parker Burchett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParkerBurchett
      type: user
    createdAt: '2022-10-22T16:08:53.000Z'
    data:
      edited: false
      editors:
      - ParkerBurchett
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
          fullname: Parker Burchett
          isHf: false
          isPro: false
          name: ParkerBurchett
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mrm8488&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mrm8488\">@<span class=\"\
          underline\">mrm8488</span></a></span>\n\n\t</span></span> which are the\
          \ generation params you are using?</p>\n"
        raw: '@mrm8488 which are the generation params you are using?'
        updatedAt: '2022-10-22T16:08:53.766Z'
      numEdits: 0
      reactions: []
    id: 6354159503d16ed3bebc1839
    type: comment
  author: ParkerBurchett
  content: '@mrm8488 which are the generation params you are using?'
  created_at: 2022-10-22 15:08:53+00:00
  edited: false
  hidden: false
  id: 6354159503d16ed3bebc1839
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58057edaf14efc22382f81b74bf602e3.svg
      fullname: Antoine Pultier
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: undefined2
      type: user
    createdAt: '2022-10-26T06:51:54.000Z'
    data:
      edited: false
      editors:
      - undefined2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58057edaf14efc22382f81b74bf602e3.svg
          fullname: Antoine Pultier
          isHf: false
          isPro: false
          name: undefined2
          type: user
        html: '<p>I had the same issue using the default parameters but I put a high
          <code>repetition_penalty</code> to fix it.</p>

          '
        raw: I had the same issue using the default parameters but I put a high `repetition_penalty`
          to fix it.
        updatedAt: '2022-10-26T06:51:54.329Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mrm8488
        - ParkerBurchett
    id: 6358d90ae81ff524cc6e9684
    type: comment
  author: undefined2
  content: I had the same issue using the default parameters but I put a high `repetition_penalty`
    to fix it.
  created_at: 2022-10-26 05:51:54+00:00
  edited: false
  hidden: false
  id: 6358d90ae81ff524cc6e9684
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
      fullname: Manuel Romero
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: mrm8488
      type: user
    createdAt: '2022-10-26T18:31:11.000Z'
    data:
      edited: false
      editors:
      - mrm8488
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
          fullname: Manuel Romero
          isHf: false
          isPro: true
          name: mrm8488
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;ParkerBurchett&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ParkerBurchett\"\
          >@<span class=\"underline\">ParkerBurchett</span></a></span>\n\n\t</span></span>!\
          \ Try this: <a href=\"https://huggingface.co/mrm8488/bloom-560m-finetuned-sd-prompts?text=%3Cs%3EPrompt%3A+a+powerful+energy+woman%2C+by+alexander+fedosav#example-of-usage\"\
          >https://huggingface.co/mrm8488/bloom-560m-finetuned-sd-prompts?text=%3Cs%3EPrompt%3A+a+powerful+energy+woman%2C+by+alexander+fedosav#example-of-usage</a></p>\n"
        raw: 'Hi, @ParkerBurchett! Try this: https://huggingface.co/mrm8488/bloom-560m-finetuned-sd-prompts?text=%3Cs%3EPrompt%3A+a+powerful+energy+woman%2C+by+alexander+fedosav#example-of-usage'
        updatedAt: '2022-10-26T18:31:11.914Z'
      numEdits: 0
      reactions: []
    id: 63597cef3568d26d3f0ab78c
    type: comment
  author: mrm8488
  content: 'Hi, @ParkerBurchett! Try this: https://huggingface.co/mrm8488/bloom-560m-finetuned-sd-prompts?text=%3Cs%3EPrompt%3A+a+powerful+energy+woman%2C+by+alexander+fedosav#example-of-usage'
  created_at: 2022-10-26 17:31:11+00:00
  edited: false
  hidden: false
  id: 63597cef3568d26d3f0ab78c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
      fullname: Parker Burchett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParkerBurchett
      type: user
    createdAt: '2022-10-27T05:09:32.000Z'
    data:
      edited: false
      editors:
      - ParkerBurchett
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
          fullname: Parker Burchett
          isHf: false
          isPro: false
          name: ParkerBurchett
          type: user
        html: "<p>In Colab it still gets stuck in a loop</p>\n<pre><code>import torch\n\
          from transformers import BloomTokenizerFast, BloomForCausalLM\n\ndevice\
          \ = 'cuda' if torch.cuda.is_available() else 'cpu'\nckpt = 'mrm8488/bloom-560m-finetuned-sd-prompts'\
          \ \n\ntokenizer = BloomTokenizerFast.from_pretrained(ckpt)\nmodel = BloomForCausalLM.from_pretrained(ckpt).to(device)\n\
          \ndef generate_prompt(text):\n    torch.cuda.empty_cache()\n    inputs =\
          \ tokenizer(text, return_tensors='pt')\n    input_ids = inputs.input_ids.to(device)\n\
          \    attention_mask = inputs.attention_mask.to(device)\n    output = model.generate(input_ids,\
          \ attention_mask=attention_mask, max_length=512, eos_token_id=tokenizer.eos_token_id)\n\
          \n    return tokenizer.decode(output[0], skip_special_tokens=False)\n  \
          \  \ntext = \"&lt;s&gt;Prompt: pikachu dinning in the eiffel tower\"\ntext2\
          \ = f\"&lt;s&gt;Prompt: McDonald's hamburger promotion on a red billboard,\
          \ white lettering, \"\n\ngenerate_prompt(text2)\n</code></pre>\n<p>Returns</p>\n\
          <pre><code>&lt;s&gt;Prompt: McDonald's hamburger promotion on a red billboard,\
          \ white lettering,  digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital billboard, digital billboard, digital billboard, digital billboard,\
          \ digital bil\n</code></pre>\n"
        raw: "In Colab it still gets stuck in a loop\n```\nimport torch\nfrom transformers\
          \ import BloomTokenizerFast, BloomForCausalLM\n\ndevice = 'cuda' if torch.cuda.is_available()\
          \ else 'cpu'\nckpt = 'mrm8488/bloom-560m-finetuned-sd-prompts' \n\ntokenizer\
          \ = BloomTokenizerFast.from_pretrained(ckpt)\nmodel = BloomForCausalLM.from_pretrained(ckpt).to(device)\n\
          \ndef generate_prompt(text):\n    torch.cuda.empty_cache()\n    inputs =\
          \ tokenizer(text, return_tensors='pt')\n    input_ids = inputs.input_ids.to(device)\n\
          \    attention_mask = inputs.attention_mask.to(device)\n    output = model.generate(input_ids,\
          \ attention_mask=attention_mask, max_length=512, eos_token_id=tokenizer.eos_token_id)\n\
          \n    return tokenizer.decode(output[0], skip_special_tokens=False)\n  \
          \  \ntext = \"<s>Prompt: pikachu dinning in the eiffel tower\"\ntext2 =\
          \ f\"<s>Prompt: McDonald's hamburger promotion on a red billboard, white\
          \ lettering, \"\n\ngenerate_prompt(text2)\n```\nReturns\n```\n<s>Prompt:\
          \ McDonald's hamburger promotion on a red billboard, white lettering,  digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital billboard, digital\
          \ billboard, digital billboard, digital billboard, digital bil\n```"
        updatedAt: '2022-10-27T05:09:32.099Z'
      numEdits: 0
      reactions: []
    id: 635a128c0cd44992264518ba
    type: comment
  author: ParkerBurchett
  content: "In Colab it still gets stuck in a loop\n```\nimport torch\nfrom transformers\
    \ import BloomTokenizerFast, BloomForCausalLM\n\ndevice = 'cuda' if torch.cuda.is_available()\
    \ else 'cpu'\nckpt = 'mrm8488/bloom-560m-finetuned-sd-prompts' \n\ntokenizer =\
    \ BloomTokenizerFast.from_pretrained(ckpt)\nmodel = BloomForCausalLM.from_pretrained(ckpt).to(device)\n\
    \ndef generate_prompt(text):\n    torch.cuda.empty_cache()\n    inputs = tokenizer(text,\
    \ return_tensors='pt')\n    input_ids = inputs.input_ids.to(device)\n    attention_mask\
    \ = inputs.attention_mask.to(device)\n    output = model.generate(input_ids, attention_mask=attention_mask,\
    \ max_length=512, eos_token_id=tokenizer.eos_token_id)\n\n    return tokenizer.decode(output[0],\
    \ skip_special_tokens=False)\n    \ntext = \"<s>Prompt: pikachu dinning in the\
    \ eiffel tower\"\ntext2 = f\"<s>Prompt: McDonald's hamburger promotion on a red\
    \ billboard, white lettering, \"\n\ngenerate_prompt(text2)\n```\nReturns\n```\n\
    <s>Prompt: McDonald's hamburger promotion on a red billboard, white lettering,\
    \  digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital billboard,\
    \ digital billboard, digital billboard, digital billboard, digital bil\n```"
  created_at: 2022-10-27 04:09:32+00:00
  edited: false
  hidden: false
  id: 635a128c0cd44992264518ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
      fullname: Parker Burchett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParkerBurchett
      type: user
    createdAt: '2022-10-27T05:10:39.000Z'
    data:
      edited: false
      editors:
      - ParkerBurchett
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
          fullname: Parker Burchett
          isHf: false
          isPro: false
          name: ParkerBurchett
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;undefined2&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/undefined2\">@<span class=\"\
          underline\">undefined2</span></a></span>\n\n\t</span></span> changing the\
          \ repetition_penalty to 1.05 worked for me too. </p>\n<pre><code>def generate_prompt(text):\n\
          \    inputs = tokenizer(text, return_tensors='pt')\n    input_ids = inputs.input_ids.to(device)\n\
          \    attention_mask = inputs.attention_mask.to(device)\n    output = model.generate(input_ids,\
          \ attention_mask=attention_mask, repetition_penalty=1.05, max_length=512,\
          \ eos_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(output[0],\
          \ skip_special_tokens=False)\n    \n\ntext2 = f\"&lt;s&gt;Prompt: McDonald's\
          \ hamburger promotion on a red billboard, white lettering,\"\ngenerate_prompt(text2)\n\
          </code></pre>\n<pre><code>&lt;s&gt;Prompt: McDonald's hamburger promotion\
          \ on a red billboard, white lettering, advertisement with posters and flyrets\
          \ in the style of artgerm.&lt;/s&gt;\n</code></pre>\n"
        raw: "@undefined2 changing the repetition_penalty to 1.05 worked for me too.\
          \ \n\n```\ndef generate_prompt(text):\n    inputs = tokenizer(text, return_tensors='pt')\n\
          \    input_ids = inputs.input_ids.to(device)\n    attention_mask = inputs.attention_mask.to(device)\n\
          \    output = model.generate(input_ids, attention_mask=attention_mask, repetition_penalty=1.05,\
          \ max_length=512, eos_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(output[0],\
          \ skip_special_tokens=False)\n    \n\ntext2 = f\"<s>Prompt: McDonald's hamburger\
          \ promotion on a red billboard, white lettering,\"\ngenerate_prompt(text2)\n\
          ```\n\n```\n<s>Prompt: McDonald's hamburger promotion on a red billboard,\
          \ white lettering, advertisement with posters and flyrets in the style of\
          \ artgerm.</s>\n```"
        updatedAt: '2022-10-27T05:10:39.982Z'
      numEdits: 0
      reactions: []
    id: 635a12cfd72fc0539e780e6b
    type: comment
  author: ParkerBurchett
  content: "@undefined2 changing the repetition_penalty to 1.05 worked for me too.\
    \ \n\n```\ndef generate_prompt(text):\n    inputs = tokenizer(text, return_tensors='pt')\n\
    \    input_ids = inputs.input_ids.to(device)\n    attention_mask = inputs.attention_mask.to(device)\n\
    \    output = model.generate(input_ids, attention_mask=attention_mask, repetition_penalty=1.05,\
    \ max_length=512, eos_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(output[0],\
    \ skip_special_tokens=False)\n    \n\ntext2 = f\"<s>Prompt: McDonald's hamburger\
    \ promotion on a red billboard, white lettering,\"\ngenerate_prompt(text2)\n```\n\
    \n```\n<s>Prompt: McDonald's hamburger promotion on a red billboard, white lettering,\
    \ advertisement with posters and flyrets in the style of artgerm.</s>\n```"
  created_at: 2022-10-27 04:10:39+00:00
  edited: false
  hidden: false
  id: 635a12cfd72fc0539e780e6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
      fullname: Parker Burchett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParkerBurchett
      type: user
    createdAt: '2022-10-27T05:11:43.000Z'
    data:
      edited: false
      editors:
      - ParkerBurchett
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2c6b82c8b7cf267e8dfcb3bae2db7ce.svg
          fullname: Parker Burchett
          isHf: false
          isPro: false
          name: ParkerBurchett
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mrm8488&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mrm8488\">@<span class=\"\
          underline\">mrm8488</span></a></span>\n\n\t</span></span> Maybe change the\
          \ default <code>repetition_penalty</code> to slightly over 1 in the example\
          \ code?</p>\n"
        raw: '@mrm8488 Maybe change the default `repetition_penalty` to slightly over
          1 in the example code?'
        updatedAt: '2022-10-27T05:11:43.035Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mrm8488
    id: 635a130f7523ad3468070f75
    type: comment
  author: ParkerBurchett
  content: '@mrm8488 Maybe change the default `repetition_penalty` to slightly over
    1 in the example code?'
  created_at: 2022-10-27 04:11:43+00:00
  edited: false
  hidden: false
  id: 635a130f7523ad3468070f75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9170e34512cd21e324dd42a0d2ecbb6.svg
      fullname: Tran Thanh Trung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TrungTran
      type: user
    createdAt: '2022-11-07T09:52:50.000Z'
    data:
      edited: false
      editors:
      - TrungTran
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9170e34512cd21e324dd42a0d2ecbb6.svg
          fullname: Tran Thanh Trung
          isHf: false
          isPro: false
          name: TrungTran
          type: user
        html: '<p>I got the problem when running the code, it generates the exactly
          same sentence every time. What should I do?</p>

          '
        raw: I got the problem when running the code, it generates the exactly same
          sentence every time. What should I do?
        updatedAt: '2022-11-07T09:52:50.060Z'
      numEdits: 0
      reactions: []
    id: 6368d57279d915ed0e20c134
    type: comment
  author: TrungTran
  content: I got the problem when running the code, it generates the exactly same
    sentence every time. What should I do?
  created_at: 2022-11-07 09:52:50+00:00
  edited: false
  hidden: false
  id: 6368d57279d915ed0e20c134
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mrm8488/bloom-560m-finetuned-sd-prompts
repo_type: model
status: open
target_branch: null
title: Completion stuck in a loop
