!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cooee-ashutosh
conflicting_files: null
created_at: 2023-09-23 04:40:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a481c9cd57fca7ae73072a33b23c5ca7.svg
      fullname: Ashutosh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cooee-ashutosh
      type: user
    createdAt: '2023-09-23T05:40:05.000Z'
    data:
      edited: true
      editors:
      - cooee-ashutosh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581383466720581
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a481c9cd57fca7ae73072a33b23c5ca7.svg
          fullname: Ashutosh
          isHf: false
          isPro: false
          name: cooee-ashutosh
          type: user
        html: '<p>Hi, thanks for the amazing paper.<br>(I know this is different from
          what the paper is about, but still) Is this novel approach just for remembering
          large context of input or can it work on multi level inputs, for example
          is there a benchmarking on remembering context of more than 100 back and
          forth conversation ?<br>Although increasing context size does somewhat help
          in remembering context for longer time frame, but there is no clear evidence
          of this as far as I know.<br>I would love to know more about this cuz I
          am struggling to find a solution for this. Any input is appreciated.<br>Thanks.</p>

          '
        raw: 'Hi, thanks for the amazing paper.

          (I know this is different from what the paper is about, but still) Is this
          novel approach just for remembering large context of input or can it work
          on multi level inputs, for example is there a benchmarking on remembering
          context of more than 100 back and forth conversation ?

          Although increasing context size does somewhat help in remembering context
          for longer time frame, but there is no clear evidence of this as far as
          I know.

          I would love to know more about this cuz I am struggling to find a solution
          for this. Any input is appreciated.

          Thanks.'
        updatedAt: '2023-09-23T05:42:16.200Z'
      numEdits: 1
      reactions: []
    id: 650e7a359d93eec4a7295610
    type: comment
  author: cooee-ashutosh
  content: 'Hi, thanks for the amazing paper.

    (I know this is different from what the paper is about, but still) Is this novel
    approach just for remembering large context of input or can it work on multi level
    inputs, for example is there a benchmarking on remembering context of more than
    100 back and forth conversation ?

    Although increasing context size does somewhat help in remembering context for
    longer time frame, but there is no clear evidence of this as far as I know.

    I would love to know more about this cuz I am struggling to find a solution for
    this. Any input is appreciated.

    Thanks.'
  created_at: 2023-09-23 04:40:05+00:00
  edited: true
  hidden: false
  id: 650e7a359d93eec4a7295610
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
      fullname: YukangChen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yukang
      type: user
    createdAt: '2023-09-23T05:48:38.000Z'
    data:
      edited: false
      editors:
      - Yukang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9186624884605408
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
          fullname: YukangChen
          isHf: false
          isPro: false
          name: Yukang
          type: user
        html: '<p>Hi,</p>

          <p>Thanks for your good question. I think the current version can not support
          multiple conversation well. Because during supervised fine-tuning, we used
          the prompt format like this</p>

          <p>Below is a paper. Memorize the content and answer my question after the
          paper. {paper_content} \n Now the material ends. {question}</p>

          <p>It follows well if asked in this prompt format. But conversation might
          not work well.</p>

          <p>Regards,<br>Yukang Chen</p>

          '
        raw: 'Hi,


          Thanks for your good question. I think the current version can not support
          multiple conversation well. Because during supervised fine-tuning, we used
          the prompt format like this


          Below is a paper. Memorize the content and answer my question after the
          paper. {paper_content} \n Now the material ends. {question}


          It follows well if asked in this prompt format. But conversation might not
          work well.



          Regards,

          Yukang Chen'
        updatedAt: '2023-09-23T05:48:38.055Z'
      numEdits: 0
      reactions: []
    id: 650e7c36b63668f44805e145
    type: comment
  author: Yukang
  content: 'Hi,


    Thanks for your good question. I think the current version can not support multiple
    conversation well. Because during supervised fine-tuning, we used the prompt format
    like this


    Below is a paper. Memorize the content and answer my question after the paper.
    {paper_content} \n Now the material ends. {question}


    It follows well if asked in this prompt format. But conversation might not work
    well.



    Regards,

    Yukang Chen'
  created_at: 2023-09-23 04:48:38+00:00
  edited: false
  hidden: false
  id: 650e7c36b63668f44805e145
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8d710e0de551cd2bf545cc31fcaf099d.svg
      fullname: Shengju Qian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thesouthfrog
      type: user
    createdAt: '2023-09-23T16:26:42.000Z'
    data:
      edited: false
      editors:
      - thesouthfrog
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.963901937007904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8d710e0de551cd2bf545cc31fcaf099d.svg
          fullname: Shengju Qian
          isHf: false
          isPro: false
          name: thesouthfrog
          type: user
        html: '<p>Adding to Yukang''s context here:</p>

          <p>The SFT model we trained here is somewhat like the "long-context version"
          Alpaca, with QA from books and papers. Our experience is that it does remember
          some detailed contents such as the role and the plot.</p>

          <p>We also observed that the data used in SFT influenced the model''s behavior
          significantly.  For the case you''ve mentioned, we believe some SFT data
          like extremely-long conversation data are really needed if we want the chat
          model to remember the history of so many rounds of conversations. Say, if
          there is a high-quality long-conversation version of GPT4ALL, etc, we believe
          training a chatbot with good memory won''t be too difficult. However, we
          are still looking at how to collect such long-round chat data, which is
          quite challenging.</p>

          '
        raw: 'Adding to Yukang''s context here:


          The SFT model we trained here is somewhat like the "long-context version"
          Alpaca, with QA from books and papers. Our experience is that it does remember
          some detailed contents such as the role and the plot.


          We also observed that the data used in SFT influenced the model''s behavior
          significantly.  For the case you''ve mentioned, we believe some SFT data
          like extremely-long conversation data are really needed if we want the chat
          model to remember the history of so many rounds of conversations. Say, if
          there is a high-quality long-conversation version of GPT4ALL, etc, we believe
          training a chatbot with good memory won''t be too difficult. However, we
          are still looking at how to collect such long-round chat data, which is
          quite challenging.'
        updatedAt: '2023-09-23T16:26:42.063Z'
      numEdits: 0
      reactions: []
    id: 650f11c25877b1c077107e55
    type: comment
  author: thesouthfrog
  content: 'Adding to Yukang''s context here:


    The SFT model we trained here is somewhat like the "long-context version" Alpaca,
    with QA from books and papers. Our experience is that it does remember some detailed
    contents such as the role and the plot.


    We also observed that the data used in SFT influenced the model''s behavior significantly.  For
    the case you''ve mentioned, we believe some SFT data like extremely-long conversation
    data are really needed if we want the chat model to remember the history of so
    many rounds of conversations. Say, if there is a high-quality long-conversation
    version of GPT4ALL, etc, we believe training a chatbot with good memory won''t
    be too difficult. However, we are still looking at how to collect such long-round
    chat data, which is quite challenging.'
  created_at: 2023-09-23 15:26:42+00:00
  edited: false
  hidden: false
  id: 650f11c25877b1c077107e55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-09-23T18:09:30.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8210078477859497
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Do you plan on releasing the LongQA dataset Yukang? Thanks for your
          contributions!</p>

          '
        raw: Do you plan on releasing the LongQA dataset Yukang? Thanks for your contributions!
        updatedAt: '2023-09-23T18:09:30.444Z'
      numEdits: 0
      reactions: []
    id: 650f29da469c325dc4ab8528
    type: comment
  author: grimulkan
  content: Do you plan on releasing the LongQA dataset Yukang? Thanks for your contributions!
  created_at: 2023-09-23 17:09:30+00:00
  edited: false
  hidden: false
  id: 650f29da469c325dc4ab8528
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
      fullname: YukangChen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yukang
      type: user
    createdAt: '2023-09-24T03:03:28.000Z'
    data:
      edited: false
      editors:
      - Yukang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.959679365158081
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
          fullname: YukangChen
          isHf: false
          isPro: false
          name: Yukang
          type: user
        html: '<p>Yes. It takes some steps for releasing. We are busy on the ICLR
          submission deadline (Sep 28). We plan to release it one or two weeks after
          the deadline. Thanks for your patience.</p>

          '
        raw: Yes. It takes some steps for releasing. We are busy on the ICLR submission
          deadline (Sep 28). We plan to release it one or two weeks after the deadline.
          Thanks for your patience.
        updatedAt: '2023-09-24T03:03:28.252Z'
      numEdits: 0
      reactions: []
    id: 650fa700dba31194cb526631
    type: comment
  author: Yukang
  content: Yes. It takes some steps for releasing. We are busy on the ICLR submission
    deadline (Sep 28). We plan to release it one or two weeks after the deadline.
    Thanks for your patience.
  created_at: 2023-09-24 02:03:28+00:00
  edited: false
  hidden: false
  id: 650fa700dba31194cb526631
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
      fullname: YukangChen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yukang
      type: user
    createdAt: '2023-10-09T06:31:49.000Z'
    data:
      edited: false
      editors:
      - Yukang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8176893591880798
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
          fullname: YukangChen
          isHf: false
          isPro: false
          name: Yukang
          type: user
        html: '<p>Hi,</p>

          <p>We have release our data for long instruction following, LongAlpaca-12k,
          and the update models, LongAlpaca-7B/13B/70B. They are available in the
          following links. These models should be much better than the original SFT
          models. We use the alpaca prompt format it is more general than what we
          used previously.</p>

          <p><a href="https://huggingface.co/datasets/Yukang/LongAlpaca-12k">https://huggingface.co/datasets/Yukang/LongAlpaca-12k</a><br><a
          href="https://huggingface.co/Yukang/LongAlpaca-7B">https://huggingface.co/Yukang/LongAlpaca-7B</a><br><a
          href="https://huggingface.co/Yukang/LongAlpaca-13B">https://huggingface.co/Yukang/LongAlpaca-13B</a><br><a
          href="https://huggingface.co/Yukang/LongAlpaca-70B-lora">https://huggingface.co/Yukang/LongAlpaca-70B-lora</a></p>

          <p>Regards,<br>Yukang Chen</p>

          '
        raw: 'Hi,


          We have release our data for long instruction following, LongAlpaca-12k,
          and the update models, LongAlpaca-7B/13B/70B. They are available in the
          following links. These models should be much better than the original SFT
          models. We use the alpaca prompt format it is more general than what we
          used previously.


          https://huggingface.co/datasets/Yukang/LongAlpaca-12k

          https://huggingface.co/Yukang/LongAlpaca-7B

          https://huggingface.co/Yukang/LongAlpaca-13B

          https://huggingface.co/Yukang/LongAlpaca-70B-lora


          Regards,

          Yukang Chen'
        updatedAt: '2023-10-09T06:31:49.907Z'
      numEdits: 0
      reactions: []
    id: 65239e558fc488499202c748
    type: comment
  author: Yukang
  content: 'Hi,


    We have release our data for long instruction following, LongAlpaca-12k, and the
    update models, LongAlpaca-7B/13B/70B. They are available in the following links.
    These models should be much better than the original SFT models. We use the alpaca
    prompt format it is more general than what we used previously.


    https://huggingface.co/datasets/Yukang/LongAlpaca-12k

    https://huggingface.co/Yukang/LongAlpaca-7B

    https://huggingface.co/Yukang/LongAlpaca-13B

    https://huggingface.co/Yukang/LongAlpaca-70B-lora


    Regards,

    Yukang Chen'
  created_at: 2023-10-09 05:31:49+00:00
  edited: false
  hidden: false
  id: 65239e558fc488499202c748
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Yukang/Llama-2-13b-chat-longlora-32k-sft
repo_type: model
status: open
target_branch: null
title: Evaluation of long sequence of conversation
