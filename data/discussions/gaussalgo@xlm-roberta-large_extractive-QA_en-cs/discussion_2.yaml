!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Puresoul
conflicting_files: null
created_at: 2023-07-30 10:13:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e54fb0c942579b29ff6b98bfe262a15.svg
      fullname: Pure soul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Puresoul
      type: user
    createdAt: '2023-07-30T11:13:47.000Z'
    data:
      edited: false
      editors:
      - Puresoul
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9747986793518066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e54fb0c942579b29ff6b98bfe262a15.svg
          fullname: Pure soul
          isHf: false
          isPro: false
          name: Puresoul
          type: user
        html: '<p>Let''s say I have a document of context which I am going to use
          as context for everything.</p>

          <p>I do not want to create embeddings for context for every time, but to
          store and use them again and again. How can I acheive this. Sorry if this
          is a dumb question.</p>

          <p>Thanks</p>

          '
        raw: "Let's say I have a document of context which I am going to use as context\
          \ for everything.\r\n\r\nI do not want to create embeddings for context\
          \ for every time, but to store and use them again and again. How can I acheive\
          \ this. Sorry if this is a dumb question.\r\n\r\nThanks"
        updatedAt: '2023-07-30T11:13:47.754Z'
      numEdits: 0
      reactions: []
    id: 64c645eb0f9144cb38bfb04c
    type: comment
  author: Puresoul
  content: "Let's say I have a document of context which I am going to use as context\
    \ for everything.\r\n\r\nI do not want to create embeddings for context for every\
    \ time, but to store and use them again and again. How can I acheive this. Sorry\
    \ if this is a dumb question.\r\n\r\nThanks"
  created_at: 2023-07-30 10:13:47+00:00
  edited: false
  hidden: false
  id: 64c645eb0f9144cb38bfb04c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659956308483-62f0c5bca594daba4a55e793.jpeg?w=200&h=200&f=face
      fullname: "Michal \u0160tef\xE1nik"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: michal-stefanik
      type: user
    createdAt: '2023-08-01T14:46:05.000Z'
    data:
      edited: false
      editors:
      - michal-stefanik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.893876314163208
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659956308483-62f0c5bca594daba4a55e793.jpeg?w=200&h=200&f=face
          fullname: "Michal \u0160tef\xE1nik"
          isHf: false
          isPro: false
          name: michal-stefanik
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Puresoul&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Puresoul\"\
          >@<span class=\"underline\">Puresoul</span></a></span>\n\n\t</span></span>\
          \ ,<br>If you are referring to this model, note that this is an extractive\
          \ QA model, that consumes an input containing both context and your question.\
          \ Internally, the transformer's token representations of both the context\
          \ and question are dependent on the whole input, so it's not easy to disentangle\
          \ the representation of context from the output of an extractive QA model.</p>\n\
          <p>If you want to pre-encode the contexts with transformers, you need to\
          \ choose a rather different approach through dense encoders, commonly used\
          \ in search applications. There, you can train a transformer, e.g. to produce\
          \ similar embedding for question and context that answers the question.\
          \ <a rel=\"nofollow\" href=\"https://www.sbert.net/docs/quickstart.html\"\
          >Sentence transformers documentation</a> can show you how to do that, but\
          \ feel free to ask questions if you like.</p>\n"
        raw: 'Hi @Puresoul ,

          If you are referring to this model, note that this is an extractive QA model,
          that consumes an input containing both context and your question. Internally,
          the transformer''s token representations of both the context and question
          are dependent on the whole input, so it''s not easy to disentangle the representation
          of context from the output of an extractive QA model.


          If you want to pre-encode the contexts with transformers, you need to choose
          a rather different approach through dense encoders, commonly used in search
          applications. There, you can train a transformer, e.g. to produce similar
          embedding for question and context that answers the question. [Sentence
          transformers documentation](https://www.sbert.net/docs/quickstart.html)
          can show you how to do that, but feel free to ask questions if you like.'
        updatedAt: '2023-08-01T14:46:05.795Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Puresoul
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Puresoul
    id: 64c91aad8d2d187c24ba7888
    type: comment
  author: michal-stefanik
  content: 'Hi @Puresoul ,

    If you are referring to this model, note that this is an extractive QA model,
    that consumes an input containing both context and your question. Internally,
    the transformer''s token representations of both the context and question are
    dependent on the whole input, so it''s not easy to disentangle the representation
    of context from the output of an extractive QA model.


    If you want to pre-encode the contexts with transformers, you need to choose a
    rather different approach through dense encoders, commonly used in search applications.
    There, you can train a transformer, e.g. to produce similar embedding for question
    and context that answers the question. [Sentence transformers documentation](https://www.sbert.net/docs/quickstart.html)
    can show you how to do that, but feel free to ask questions if you like.'
  created_at: 2023-08-01 13:46:05+00:00
  edited: false
  hidden: false
  id: 64c91aad8d2d187c24ba7888
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e54fb0c942579b29ff6b98bfe262a15.svg
      fullname: Pure soul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Puresoul
      type: user
    createdAt: '2023-08-02T06:57:18.000Z'
    data:
      edited: false
      editors:
      - Puresoul
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592081904411316
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e54fb0c942579b29ff6b98bfe262a15.svg
          fullname: Pure soul
          isHf: false
          isPro: false
          name: Puresoul
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;michal-stefanik&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/michal-stefanik\"\
          >@<span class=\"underline\">michal-stefanik</span></a></span>\n\n\t</span></span>\
          \ Thank you so much for  your inputs, I'll try that way.</p>\n"
        raw: '@michal-stefanik Thank you so much for  your inputs, I''ll try that
          way.'
        updatedAt: '2023-08-02T06:57:18.365Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - michal-stefanik
    id: 64c9fe4eeabc2bbc15bbb691
    type: comment
  author: Puresoul
  content: '@michal-stefanik Thank you so much for  your inputs, I''ll try that way.'
  created_at: 2023-08-02 05:57:18+00:00
  edited: false
  hidden: false
  id: 64c9fe4eeabc2bbc15bbb691
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: gaussalgo/xlm-roberta-large_extractive-QA_en-cs
repo_type: model
status: open
target_branch: null
title: How to avoid creation of embeddings for context
