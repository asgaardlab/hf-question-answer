!!python/object:huggingface_hub.community.DiscussionWithDetails
author: changgeli
conflicting_files: null
created_at: 2023-11-01 02:18:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
      fullname: changgeli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: changgeli
      type: user
    createdAt: '2023-11-01T03:18:35.000Z'
    data:
      edited: false
      editors:
      - changgeli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6850782632827759
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
          fullname: changgeli
          isHf: false
          isPro: false
          name: changgeli
          type: user
        html: '<p>My output:"The image features two plates of food, each containing
          different types of food. One plate contains fish, carrots, and potatoes,
          while the other plate contains fish, lemon, and chicken."</p>

          <p>The blog example:"fish, carrots"</p>

          <p>Is there something wrong with the way I''m using it?<br>''''''<br>text_prompt
          = "What type of foods are in the image?\n"<br>image_path = "pictures/fish_and_carrots.png"  #
          <a href="https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/chart.png">https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/chart.png</a><br>image_pil
          = Image.open(image_path)</p>

          <p>model_inputs = processor(text=text_prompt, images=[image_pil], device="cuda:0")<br>for
          k, v in model_inputs.items():<br>   model_inputs[k] = v.to("cuda:0")</p>

          <p>generation_output = model.generate(**model_inputs, max_new_tokens=50)<br>generation_text
          = processor.batch_decode(generation_output[:, -50:], skip_special_tokens=True)<br>print("generation_text:",
          generation_text)<br>''''''</p>

          '
        raw: "My output:\"The image features two plates of food, each containing different\
          \ types of food. One plate contains fish, carrots, and potatoes, while the\
          \ other plate contains fish, lemon, and chicken.\"\r\n\r\nThe blog example:\"\
          fish, carrots\"\r\n\r\n\r\n\r\nIs there something wrong with the way I'm\
          \ using it?\r\n'''\r\ntext_prompt = \"What type of foods are in the image?\\\
          n\"\r\nimage_path = \"pictures/fish_and_carrots.png\"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/chart.png\r\
          \nimage_pil = Image.open(image_path)\r\n\r\nmodel_inputs = processor(text=text_prompt,\
          \ images=[image_pil], device=\"cuda:0\")\r\nfor k, v in model_inputs.items():\r\
          \n   model_inputs[k] = v.to(\"cuda:0\")\r\n\r\ngeneration_output = model.generate(**model_inputs,\
          \ max_new_tokens=50)\r\ngeneration_text = processor.batch_decode(generation_output[:,\
          \ -50:], skip_special_tokens=True)\r\nprint(\"generation_text:\", generation_text)\r\
          \n'''\r\n"
        updatedAt: '2023-11-01T03:18:35.832Z'
      numEdits: 0
      reactions: []
    id: 6541c38b72a0eadc6951030a
    type: comment
  author: changgeli
  content: "My output:\"The image features two plates of food, each containing different\
    \ types of food. One plate contains fish, carrots, and potatoes, while the other\
    \ plate contains fish, lemon, and chicken.\"\r\n\r\nThe blog example:\"fish, carrots\"\
    \r\n\r\n\r\n\r\nIs there something wrong with the way I'm using it?\r\n'''\r\n\
    text_prompt = \"What type of foods are in the image?\\n\"\r\nimage_path = \"pictures/fish_and_carrots.png\"\
    \  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/chart.png\r\nimage_pil\
    \ = Image.open(image_path)\r\n\r\nmodel_inputs = processor(text=text_prompt, images=[image_pil],\
    \ device=\"cuda:0\")\r\nfor k, v in model_inputs.items():\r\n   model_inputs[k]\
    \ = v.to(\"cuda:0\")\r\n\r\ngeneration_output = model.generate(**model_inputs,\
    \ max_new_tokens=50)\r\ngeneration_text = processor.batch_decode(generation_output[:,\
    \ -50:], skip_special_tokens=True)\r\nprint(\"generation_text:\", generation_text)\r\
    \n'''\r\n"
  created_at: 2023-11-01 02:18:35+00:00
  edited: false
  hidden: false
  id: 6541c38b72a0eadc6951030a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-11-03T08:39:18.000Z'
    data:
      edited: true
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7245193719863892
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;changgeli&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/changgeli\"\
          >@<span class=\"underline\">changgeli</span></a></span>\n\n\t</span></span>\
          \ ! With the new transformers release, fuyu-8b's processor has been updated\
          \ and we've also got more information on prompt structure. You have to prompt\
          \ it to answer in a VQA fashion.<br>here, try this:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> PIL <span class=\"\
          hljs-keyword\">import</span> Image\n<span class=\"hljs-keyword\">import</span>\
          \ requests\n<span class=\"hljs-keyword\">import</span> io\n<span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ FuyuForCausalLM, FuyuProcessor\n\n\npretrained_path = <span class=\"hljs-string\"\
          >\"adept/fuyu-8b\"</span>\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=<span\
          \ class=\"hljs-string\">'auto'</span>)\n\n\ntext_prompt = <span class=\"\
          hljs-string\">\"Answer the following VQAv2 question based on the image:\
          \ What type of foods are in the image?\"</span>\nfish_image_url = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
          </span>\nfish_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(fish_image_url).content))\n\
          model_inputs = processor(text=text_prompt, images=fish_image_pil).to(<span\
          \ class=\"hljs-string\">'cuda'</span>)\n\n\nmodel_outputs = processor.batch_decode(model.generate(\
          \ **model_inputs, max_new_tokens=<span class=\"hljs-number\">10</span>)[:,\
          \ -<span class=\"hljs-number\">10</span>:], skip_special_tokens=<span class=\"\
          hljs-literal\">True</span>)[<span class=\"hljs-number\">0</span>]\nprediction\
          \ = model_outputs.split(<span class=\"hljs-string\">'\\x04 '</span>, <span\
          \ class=\"hljs-number\">1</span>)[<span class=\"hljs-number\">1</span>]\
          \ <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'\\\
          x04'</span> <span class=\"hljs-keyword\">in</span> model_outputs <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-string\">''</span>\n</code></pre>\n\
          <p>You should see \"fish, carrots, lemon\" as a prediction.</p>\n"
        raw: "Hi @changgeli ! With the new transformers release, fuyu-8b's processor\
          \ has been updated and we've also got more information on prompt structure.\
          \ You have to prompt it to answer in a VQA fashion. \nhere, try this:\n\
          \ \n```python\nfrom PIL import Image\nimport requests\nimport io\nfrom transformers\
          \ import FuyuForCausalLM, FuyuProcessor\n\n\npretrained_path = \"adept/fuyu-8b\"\
          \nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\nmodel = FuyuForCausalLM.from_pretrained(pretrained_path,\
          \ device_map='auto')\n\n\ntext_prompt = \"Answer the following VQAv2 question\
          \ based on the image: What type of foods are in the image?\"\nfish_image_url\
          \ = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
          \nfish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))\n\
          model_inputs = processor(text=text_prompt, images=fish_image_pil).to('cuda')\n\
          \n\nmodel_outputs = processor.batch_decode(model.generate( **model_inputs,\
          \ max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]\nprediction\
          \ = model_outputs.split('\\x04 ', 1)[1] if '\\x04' in model_outputs else\
          \ ''\n\n```\nYou should see \"fish, carrots, lemon\" as a prediction.\n"
        updatedAt: '2023-11-03T08:47:28.907Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - changgeli
    id: 6544b1b647fa58e9bb666461
    type: comment
  author: Molbap
  content: "Hi @changgeli ! With the new transformers release, fuyu-8b's processor\
    \ has been updated and we've also got more information on prompt structure. You\
    \ have to prompt it to answer in a VQA fashion. \nhere, try this:\n \n```python\n\
    from PIL import Image\nimport requests\nimport io\nfrom transformers import FuyuForCausalLM,\
    \ FuyuProcessor\n\n\npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
    model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\n\
    \ntext_prompt = \"Answer the following VQAv2 question based on the image: What\
    \ type of foods are in the image?\"\nfish_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
    \nfish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))\n\
    model_inputs = processor(text=text_prompt, images=fish_image_pil).to('cuda')\n\
    \n\nmodel_outputs = processor.batch_decode(model.generate( **model_inputs, max_new_tokens=10)[:,\
    \ -10:], skip_special_tokens=True)[0]\nprediction = model_outputs.split('\\x04\
    \ ', 1)[1] if '\\x04' in model_outputs else ''\n\n```\nYou should see \"fish,\
    \ carrots, lemon\" as a prediction.\n"
  created_at: 2023-11-03 07:39:18+00:00
  edited: true
  hidden: false
  id: 6544b1b647fa58e9bb666461
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
      fullname: changgeli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: changgeli
      type: user
    createdAt: '2023-11-03T09:17:46.000Z'
    data:
      edited: false
      editors:
      - changgeli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8345190286636353
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
          fullname: changgeli
          isHf: false
          isPro: false
          name: changgeli
          type: user
        html: '<p>Thank you for the comprehensive reply!  Are there any recommended
          prompts to use for other datasets or tasks?</p>

          '
        raw: Thank you for the comprehensive reply!  Are there any recommended prompts
          to use for other datasets or tasks?
        updatedAt: '2023-11-03T09:17:46.545Z'
      numEdits: 0
      reactions: []
    id: 6544baba893aec5da9736ac2
    type: comment
  author: changgeli
  content: Thank you for the comprehensive reply!  Are there any recommended prompts
    to use for other datasets or tasks?
  created_at: 2023-11-03 08:17:46+00:00
  edited: false
  hidden: false
  id: 6544baba893aec5da9736ac2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-11-03T10:05:26.000Z'
    data:
      edited: false
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7668130397796631
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: "<p>You're welcome :) that I know of, to best match the blog post examples,\
          \ you can try \"Answer the following DocVQA question based on the image.\"\
          \ or the one I shared above, seems to match well!</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> PIL <span class=\"\
          hljs-keyword\">import</span> Image\n<span class=\"hljs-keyword\">import</span>\
          \ requests\n<span class=\"hljs-keyword\">import</span> io\n<span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ FuyuForCausalLM, FuyuProcessor\n\npretrained_path = <span class=\"hljs-string\"\
          >\"adept/fuyu-8b\"</span>\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=<span\
          \ class=\"hljs-string\">'auto'</span>)\n\n\ntext_prompt = <span class=\"\
          hljs-string\">\"Answer the following DocVQA question based on the image.\
          \ \\n Which is the metro in California that has a good job Outlook?\"</span>\n\
          jobs_image_url = <span class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png\"\
          </span>\njobs_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(jobs_image_url).content))\n\
          \nsecond_text_prompt = <span class=\"hljs-string\">\"Answer the following\
          \ DocVQA question based on the image. \\n What if the maximum male life\
          \ expectancy?\"</span>\nchart_image_url = <span class=\"hljs-string\">\"\
          https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png\"\
          </span>\nchart_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(chart_image_url).content))\n\
          \nthird_text_prompt = <span class=\"hljs-string\">\"Answer the following\
          \ DocVQA question based on the image. \\n What sport is that?\"</span>\n\
          skate_image_url = <span class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png\"\
          </span>\nskate_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(skate_image_url).content))\n\
          \nfourth_text_prompt = <span class=\"hljs-string\">\"Answer the following\
          \ DocVQA question based on the image. \\n What was the fair amount of paid\
          \ vacation days in the United Kingdom?\"</span>\nvacations_image_url = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png\"\
          </span>\nvacations_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(vacations_image_url).content)).convert(<span\
          \ class=\"hljs-string\">'RGB'</span>)\n\nfifth_text_prompt = <span class=\"\
          hljs-string\">\"Answer the following VQAv2 question based on the image:\
          \ What type of foods are in the image?\"</span>\nfish_image_url = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
          </span>\nfish_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(fish_image_url).content))\n\
          \n\ntexts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt,\
          \ fifth_text_prompt]\nimages = [jobs_image_pil, chart_image_pil, skate_image_pil,\
          \ vacations_image_pil, fish_image_pil]\n\nmodel_inputs = processor(text=texts,\
          \ images=images).to(<span class=\"hljs-string\">'cuda'</span>)\n\n\nmodel_outputs\
          \ = processor.batch_decode(model.generate(\n    **model_inputs, max_new_tokens=<span\
          \ class=\"hljs-number\">10</span>)[:, -<span class=\"hljs-number\">10</span>:],\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n\nground_truths\
          \ = [<span class=\"hljs-string\">'Los Angeles'</span>, <span class=\"hljs-string\"\
          >'80.7'</span>, <span class=\"hljs-string\">'skateboarding'</span>, <span\
          \ class=\"hljs-string\">'28'</span>, <span class=\"hljs-string\">'fish,\
          \ carrots, lemon'</span>]\n\n\n<span class=\"hljs-keyword\">for</span> ground_truth,\
          \ model_output <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >zip</span>(ground_truths, model_outputs):\n    prediction = model_output.split(<span\
          \ class=\"hljs-string\">'\\x04 '</span>, <span class=\"hljs-number\">1</span>)[<span\
          \ class=\"hljs-number\">1</span>] <span class=\"hljs-keyword\">if</span>\
          \ <span class=\"hljs-string\">'\\x04'</span> <span class=\"hljs-keyword\"\
          >in</span> model_output <span class=\"hljs-keyword\">else</span> <span class=\"\
          hljs-string\">''</span>\n    <span class=\"hljs-keyword\">assert</span>\
          \ (ground_truth == prediction)\n</code></pre>\n"
        raw: "You're welcome :) that I know of, to best match the blog post examples,\
          \ you can try \"Answer the following DocVQA question based on the image.\"\
          \ or the one I shared above, seems to match well!\n\n```python\nfrom PIL\
          \ import Image\nimport requests\nimport io\nfrom transformers import FuyuForCausalLM,\
          \ FuyuProcessor\n\npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\
          \n\ntext_prompt = \"Answer the following DocVQA question based on the image.\
          \ \\n Which is the metro in California that has a good job Outlook?\"\n\
          jobs_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png\"\
          \njobs_image_pil = Image.open(io.BytesIO(requests.get(jobs_image_url).content))\n\
          \nsecond_text_prompt = \"Answer the following DocVQA question based on the\
          \ image. \\n What if the maximum male life expectancy?\"\nchart_image_url\
          \ = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png\"\
          \nchart_image_pil = Image.open(io.BytesIO(requests.get(chart_image_url).content))\n\
          \nthird_text_prompt = \"Answer the following DocVQA question based on the\
          \ image. \\n What sport is that?\"\nskate_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png\"\
          \nskate_image_pil = Image.open(io.BytesIO(requests.get(skate_image_url).content))\n\
          \nfourth_text_prompt = \"Answer the following DocVQA question based on the\
          \ image. \\n What was the fair amount of paid vacation days in the United\
          \ Kingdom?\"\nvacations_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png\"\
          \nvacations_image_pil = Image.open(io.BytesIO(requests.get(vacations_image_url).content)).convert('RGB')\n\
          \nfifth_text_prompt = \"Answer the following VQAv2 question based on the\
          \ image: What type of foods are in the image?\"\nfish_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
          \nfish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))\n\
          \n\ntexts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt,\
          \ fifth_text_prompt]\nimages = [jobs_image_pil, chart_image_pil, skate_image_pil,\
          \ vacations_image_pil, fish_image_pil]\n\nmodel_inputs = processor(text=texts,\
          \ images=images).to('cuda')\n\n\nmodel_outputs = processor.batch_decode(model.generate(\n\
          \    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)\n\
          \nground_truths = ['Los Angeles', '80.7', 'skateboarding', '28', 'fish,\
          \ carrots, lemon']\n\n\nfor ground_truth, model_output in zip(ground_truths,\
          \ model_outputs):\n    prediction = model_output.split('\\x04 ', 1)[1] if\
          \ '\\x04' in model_output else ''\n    assert (ground_truth == prediction)\n\
          \n```"
        updatedAt: '2023-11-03T10:05:26.997Z'
      numEdits: 0
      reactions: []
    id: 6544c5e6ee7bbb5952bdebfb
    type: comment
  author: Molbap
  content: "You're welcome :) that I know of, to best match the blog post examples,\
    \ you can try \"Answer the following DocVQA question based on the image.\" or\
    \ the one I shared above, seems to match well!\n\n```python\nfrom PIL import Image\n\
    import requests\nimport io\nfrom transformers import FuyuForCausalLM, FuyuProcessor\n\
    \npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
    model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\n\
    \ntext_prompt = \"Answer the following DocVQA question based on the image. \\\
    n Which is the metro in California that has a good job Outlook?\"\njobs_image_url\
    \ = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png\"\
    \njobs_image_pil = Image.open(io.BytesIO(requests.get(jobs_image_url).content))\n\
    \nsecond_text_prompt = \"Answer the following DocVQA question based on the image.\
    \ \\n What if the maximum male life expectancy?\"\nchart_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png\"\
    \nchart_image_pil = Image.open(io.BytesIO(requests.get(chart_image_url).content))\n\
    \nthird_text_prompt = \"Answer the following DocVQA question based on the image.\
    \ \\n What sport is that?\"\nskate_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png\"\
    \nskate_image_pil = Image.open(io.BytesIO(requests.get(skate_image_url).content))\n\
    \nfourth_text_prompt = \"Answer the following DocVQA question based on the image.\
    \ \\n What was the fair amount of paid vacation days in the United Kingdom?\"\n\
    vacations_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png\"\
    \nvacations_image_pil = Image.open(io.BytesIO(requests.get(vacations_image_url).content)).convert('RGB')\n\
    \nfifth_text_prompt = \"Answer the following VQAv2 question based on the image:\
    \ What type of foods are in the image?\"\nfish_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
    \nfish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))\n\
    \n\ntexts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt,\
    \ fifth_text_prompt]\nimages = [jobs_image_pil, chart_image_pil, skate_image_pil,\
    \ vacations_image_pil, fish_image_pil]\n\nmodel_inputs = processor(text=texts,\
    \ images=images).to('cuda')\n\n\nmodel_outputs = processor.batch_decode(model.generate(\n\
    \    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)\n\n\
    ground_truths = ['Los Angeles', '80.7', 'skateboarding', '28', 'fish, carrots,\
    \ lemon']\n\n\nfor ground_truth, model_output in zip(ground_truths, model_outputs):\n\
    \    prediction = model_output.split('\\x04 ', 1)[1] if '\\x04' in model_output\
    \ else ''\n    assert (ground_truth == prediction)\n\n```"
  created_at: 2023-11-03 09:05:26+00:00
  edited: false
  hidden: false
  id: 6544c5e6ee7bbb5952bdebfb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-11-03T11:09:05.000Z'
    data:
      edited: false
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5819573998451233
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;changgeli&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/changgeli\">@<span class=\"\
          underline\">changgeli</span></a></span>\n\n\t</span></span> And I forgot\
          \ to include bounding box localisation and guided OCR: for that, you can\
          \ do</p>\n<pre><code class=\"language-python\">\n<span class=\"hljs-comment\"\
          ># bbox contents prediction (OCR)</span>\n\npretrained_path = <span class=\"\
          hljs-string\">\"adept/fuyu-8b\"</span>\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=<span\
          \ class=\"hljs-string\">'auto'</span>)\n\n\nbbox_prompt = <span class=\"\
          hljs-string\">\"When presented with a box, perform OCR to extract text contained\
          \ within it. If provided with text, generate the corresponding bounding\
          \ box.\\\\n&lt;box&gt;388, 428, 404, 488&lt;/box&gt;\"</span>\nbbox_image_url\
          \ = <span class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
          </span>\nbbox_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(bbox_image_url).content))\n\
          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to(<span\
          \ class=\"hljs-string\">'cuda'</span>)\n\n\nmodel_outputs = processor.batch_decode(model.generate(\n\
          \    **model_inputs, max_new_tokens=<span class=\"hljs-number\">10</span>)[:,\
          \ -<span class=\"hljs-number\">10</span>:], skip_special_tokens=<span class=\"\
          hljs-literal\">True</span>)[<span class=\"hljs-number\">0</span>]\nprediction\
          \ = model_outputs.split(<span class=\"hljs-string\">'\\x04'</span>, <span\
          \ class=\"hljs-number\">1</span>)[<span class=\"hljs-number\">1</span>]\
          \ <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'\\\
          x04'</span> <span class=\"hljs-keyword\">in</span> model_outputs <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-string\">''</span>\n\n<span\
          \ class=\"hljs-comment\"># bbox localisation from text</span>\n\npretrained_path\
          \ = <span class=\"hljs-string\">\"adept/fuyu-8b\"</span>\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=<span\
          \ class=\"hljs-string\">'auto'</span>)\n\n\nbbox_prompt = <span class=\"\
          hljs-string\">\"When presented with a box, perform OCR to extract text contained\
          \ within it. If provided with text, generate the corresponding bounding\
          \ box.\\\\n Williams\"</span>\nbbox_image_url = <span class=\"hljs-string\"\
          >\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
          </span>\nbbox_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(bbox_image_url).content))\n\
          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to(<span\
          \ class=\"hljs-string\">'cuda'</span>)\n\noutputs = model.generate(**model_inputs,\
          \ max_new_tokens=<span class=\"hljs-number\">10</span>)\npost_processed_bbox_tokens\
          \ = processor.post_process_box_coordinates(outputs)[<span class=\"hljs-number\"\
          >0</span>]\nmodel_outputs = processor.decode(post_processed_bbox_tokens,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\nprediction\
          \ = model_outputs.split(<span class=\"hljs-string\">'\\x04'</span>, <span\
          \ class=\"hljs-number\">1</span>)[<span class=\"hljs-number\">1</span>]\
          \ <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'\\\
          x04'</span> <span class=\"hljs-keyword\">in</span> model_outputs <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-string\">''</span>\n</code></pre>\n"
        raw: "@changgeli And I forgot to include bounding box localisation and guided\
          \ OCR: for that, you can do\n\n```python\n\n# bbox contents prediction (OCR)\n\
          \npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\
          \n\nbbox_prompt = \"When presented with a box, perform OCR to extract text\
          \ contained within it. If provided with text, generate the corresponding\
          \ bounding box.\\\\n<box>388, 428, 404, 488</box>\"\nbbox_image_url = \"\
          https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
          \nbbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))\n\
          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')\n\
          \n\nmodel_outputs = processor.batch_decode(model.generate(\n    **model_inputs,\
          \ max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]\nprediction\
          \ = model_outputs.split('\\x04', 1)[1] if '\\x04' in model_outputs else\
          \ ''\n\n# bbox localisation from text\n\npretrained_path = \"adept/fuyu-8b\"\
          \nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\nmodel = FuyuForCausalLM.from_pretrained(pretrained_path,\
          \ device_map='auto')\n\n\nbbox_prompt = \"When presented with a box, perform\
          \ OCR to extract text contained within it. If provided with text, generate\
          \ the corresponding bounding box.\\\\n Williams\"\nbbox_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
          \nbbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))\n\
          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')\n\
          \noutputs = model.generate(**model_inputs, max_new_tokens=10)\npost_processed_bbox_tokens\
          \ = processor.post_process_box_coordinates(outputs)[0]\nmodel_outputs =\
          \ processor.decode(post_processed_bbox_tokens, skip_special_tokens=True)\n\
          prediction = model_outputs.split('\\x04', 1)[1] if '\\x04' in model_outputs\
          \ else ''\n\n```"
        updatedAt: '2023-11-03T11:09:05.947Z'
      numEdits: 0
      reactions: []
    id: 6544d4d14200ae379eaafd33
    type: comment
  author: Molbap
  content: "@changgeli And I forgot to include bounding box localisation and guided\
    \ OCR: for that, you can do\n\n```python\n\n# bbox contents prediction (OCR)\n\
    \npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
    model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\n\
    \nbbox_prompt = \"When presented with a box, perform OCR to extract text contained\
    \ within it. If provided with text, generate the corresponding bounding box.\\\
    \\n<box>388, 428, 404, 488</box>\"\nbbox_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
    \nbbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))\n\
    model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')\n\
    \n\nmodel_outputs = processor.batch_decode(model.generate(\n    **model_inputs,\
    \ max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]\nprediction = model_outputs.split('\\\
    x04', 1)[1] if '\\x04' in model_outputs else ''\n\n# bbox localisation from text\n\
    \npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
    model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\n\
    \nbbox_prompt = \"When presented with a box, perform OCR to extract text contained\
    \ within it. If provided with text, generate the corresponding bounding box.\\\
    \\n Williams\"\nbbox_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
    \nbbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))\n\
    model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')\n\
    \noutputs = model.generate(**model_inputs, max_new_tokens=10)\npost_processed_bbox_tokens\
    \ = processor.post_process_box_coordinates(outputs)[0]\nmodel_outputs = processor.decode(post_processed_bbox_tokens,\
    \ skip_special_tokens=True)\nprediction = model_outputs.split('\\x04', 1)[1] if\
    \ '\\x04' in model_outputs else ''\n\n```"
  created_at: 2023-11-03 10:09:05+00:00
  edited: false
  hidden: false
  id: 6544d4d14200ae379eaafd33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
      fullname: changgeli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: changgeli
      type: user
    createdAt: '2023-11-05T08:25:13.000Z'
    data:
      edited: true
      editors:
      - changgeli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8813856840133667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
          fullname: changgeli
          isHf: false
          isPro: false
          name: changgeli
          type: user
        html: '<p>The last example "bbox localisation from text" seems to not output
          the ''\x04'' symbol, and cannot be parsed.</p>

          '
        raw: 'The last example "bbox localisation from text" seems to not output the
          ''\x04'' symbol, and cannot be parsed.

          '
        updatedAt: '2023-11-05T08:25:41.744Z'
      numEdits: 1
      reactions: []
    id: 654751694d1931dc9300d7cc
    type: comment
  author: changgeli
  content: 'The last example "bbox localisation from text" seems to not output the
    ''\x04'' symbol, and cannot be parsed.

    '
  created_at: 2023-11-05 07:25:13+00:00
  edited: true
  hidden: false
  id: 654751694d1931dc9300d7cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-11-05T20:41:27.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7017077803611755
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;changgeli&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/changgeli\"\
          >@<span class=\"underline\">changgeli</span></a></span>\n\n\t</span></span>!\
          \ Are you using the same input image and prompt? Can you verify what's the\
          \ decoded result before the<code>split</code>?</p>\n"
        raw: Hello @changgeli! Are you using the same input image and prompt? Can
          you verify what's the decoded result before the`split`?
        updatedAt: '2023-11-05T20:41:27.195Z'
      numEdits: 0
      reactions: []
    id: 6547fdf7b6a8bd2c9070f53a
    type: comment
  author: pcuenq
  content: Hello @changgeli! Are you using the same input image and prompt? Can you
    verify what's the decoded result before the`split`?
  created_at: 2023-11-05 20:41:27+00:00
  edited: false
  hidden: false
  id: 6547fdf7b6a8bd2c9070f53a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
      fullname: changgeli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: changgeli
      type: user
    createdAt: '2023-11-07T08:35:41.000Z'
    data:
      edited: true
      editors:
      - changgeli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7596017718315125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/113f1f85d1b501d462578cb2162b47bf.svg
          fullname: changgeli
          isHf: false
          isPro: false
          name: changgeli
          type: user
        html: '<p>Thank you for reply!  The symbol <code>''\x04''</code>  can be output.
          I used <code>''\x04 ''</code> instead of <code>''\x04''</code> as a delimiter,
          so the parsing failed.<br>My question is, why do some predictions start
          with a space, while others do not?</p>

          <pre><code>the decoded result of bbox localisation from text:

          ...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt; When presented with a
          box, perform OCR to extract text contained within it. If provided with text,
          generate the corresponding bounding box.\\n Williams\x04&lt;box&gt;388,
          428, 404, 900&lt;/box&gt;''


          the decoded result of bbox contents prediction (OCR):

          ...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt; When presented with a
          box, perform OCR to extract text contained within it. If provided with text,
          generate the corresponding bounding box.\\n&lt;box&gt;388, 428, 404, 488&lt;/box&gt;\x04
          Williams''

          </code></pre>

          '
        raw: 'Thank you for reply!  The symbol `''\x04''`  can be output. I used `''\x04
          ''` instead of `''\x04''` as a delimiter, so the parsing failed.

          My question is, why do some predictions start with a space, while others
          do not?


          ```

          the decoded result of bbox localisation from text:

          ...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s> When presented with a box, perform
          OCR to extract text contained within it. If provided with text, generate
          the corresponding bounding box.\\n Williams\x04<box>388, 428, 404, 900</box>''


          the decoded result of bbox contents prediction (OCR):

          ...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s> When presented with a box, perform
          OCR to extract text contained within it. If provided with text, generate
          the corresponding bounding box.\\n<box>388, 428, 404, 488</box>\x04 Williams''

          ```'
        updatedAt: '2023-11-07T08:40:38.902Z'
      numEdits: 1
      reactions: []
    id: 6549f6dd1c5192d98185f580
    type: comment
  author: changgeli
  content: 'Thank you for reply!  The symbol `''\x04''`  can be output. I used `''\x04
    ''` instead of `''\x04''` as a delimiter, so the parsing failed.

    My question is, why do some predictions start with a space, while others do not?


    ```

    the decoded result of bbox localisation from text:

    ...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s> When presented with a box, perform
    OCR to extract text contained within it. If provided with text, generate the corresponding
    bounding box.\\n Williams\x04<box>388, 428, 404, 900</box>''


    the decoded result of bbox contents prediction (OCR):

    ...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s> When presented with a box, perform
    OCR to extract text contained within it. If provided with text, generate the corresponding
    bounding box.\\n<box>388, 428, 404, 488</box>\x04 Williams''

    ```'
  created_at: 2023-11-07 08:35:41+00:00
  edited: true
  hidden: false
  id: 6549f6dd1c5192d98185f580
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-11-07T15:02:13.000Z'
    data:
      edited: false
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9346816539764404
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: '<p>Did you try tweaking the generation parameters? I think we mostly
          tried greedy decoding, sampling might change what you encounter. </p>

          '
        raw: 'Did you try tweaking the generation parameters? I think we mostly tried
          greedy decoding, sampling might change what you encounter. '
        updatedAt: '2023-11-07T15:02:13.863Z'
      numEdits: 0
      reactions: []
    id: 654a5175589f50f1b1a1e8fc
    type: comment
  author: Molbap
  content: 'Did you try tweaking the generation parameters? I think we mostly tried
    greedy decoding, sampling might change what you encounter. '
  created_at: 2023-11-07 15:02:13+00:00
  edited: false
  hidden: false
  id: 654a5175589f50f1b1a1e8fc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: For the vqav2 data set example "fish and carrot", why does the model output
  a sentence instead of a phrase?
