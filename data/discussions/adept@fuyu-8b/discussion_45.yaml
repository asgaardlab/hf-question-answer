!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VatsaDev
conflicting_files: null
created_at: 2023-11-07 03:07:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
      fullname: Vatsa Pandey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VatsaDev
      type: user
    createdAt: '2023-11-07T03:07:05.000Z'
    data:
      edited: false
      editors:
      - VatsaDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9066680073738098
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
          fullname: Vatsa Pandey
          isHf: false
          isPro: false
          name: VatsaDev
          type: user
        html: '<p>The Q above, because from what im seeing, you take an image, split
          it into rows, and give that to the model, and it supossably has no real
          difference from permission 8b. Like how are the images going in? From what
          I can tell, youre not making image embeddings, so hows the model understanding
          images?</p>

          '
        raw: The Q above, because from what im seeing, you take an image, split it
          into rows, and give that to the model, and it supossably has no real difference
          from permission 8b. Like how are the images going in? From what I can tell,
          youre not making image embeddings, so hows the model understanding images?
        updatedAt: '2023-11-07T03:07:05.157Z'
      numEdits: 0
      reactions: []
    id: 6549a9d98fde27109bcb3f79
    type: comment
  author: VatsaDev
  content: The Q above, because from what im seeing, you take an image, split it into
    rows, and give that to the model, and it supossably has no real difference from
    permission 8b. Like how are the images going in? From what I can tell, youre not
    making image embeddings, so hows the model understanding images?
  created_at: 2023-11-07 03:07:05+00:00
  edited: false
  hidden: false
  id: 6549a9d98fde27109bcb3f79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-11-07T09:54:50.000Z'
    data:
      edited: false
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8963804244995117
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;VatsaDev&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/VatsaDev\"\
          >@<span class=\"underline\">VatsaDev</span></a></span>\n\n\t</span></span>\
          \ , not sure I understand your question exactly but the model does have\
          \ a vision layer. It is simply linear, but it does create an embedding vector\
          \ of required dimension from each patch. Then as you said the embeddings\
          \ are combined with the text embeddings from the prompt tokens and fed into\
          \ a Persimmon-8b like architecture.</p>\n<p>I recommend inspecting the modeling\
          \ code here to get a better sense of what the model is doing: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/blob/9beb2737d758160e845b66742a0c01201e38007f/src/transformers/models/fuyu/modeling_fuyu.py#L154C1-L158C10\"\
          >https://github.com/huggingface/transformers/blob/9beb2737d758160e845b66742a0c01201e38007f/src/transformers/models/fuyu/modeling_fuyu.py#L154C1-L158C10</a></p>\n"
        raw: 'Hi @VatsaDev , not sure I understand your question exactly but the model
          does have a vision layer. It is simply linear, but it does create an embedding
          vector of required dimension from each patch. Then as you said the embeddings
          are combined with the text embeddings from the prompt tokens and fed into
          a Persimmon-8b like architecture.


          I recommend inspecting the modeling code here to get a better sense of what
          the model is doing: https://github.com/huggingface/transformers/blob/9beb2737d758160e845b66742a0c01201e38007f/src/transformers/models/fuyu/modeling_fuyu.py#L154C1-L158C10

          '
        updatedAt: '2023-11-07T09:54:50.253Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - VatsaDev
        - darknoon
    id: 654a096a5d2f6ae0a8b0bce4
    type: comment
  author: Molbap
  content: 'Hi @VatsaDev , not sure I understand your question exactly but the model
    does have a vision layer. It is simply linear, but it does create an embedding
    vector of required dimension from each patch. Then as you said the embeddings
    are combined with the text embeddings from the prompt tokens and fed into a Persimmon-8b
    like architecture.


    I recommend inspecting the modeling code here to get a better sense of what the
    model is doing: https://github.com/huggingface/transformers/blob/9beb2737d758160e845b66742a0c01201e38007f/src/transformers/models/fuyu/modeling_fuyu.py#L154C1-L158C10

    '
  created_at: 2023-11-07 09:54:50+00:00
  edited: false
  hidden: false
  id: 654a096a5d2f6ae0a8b0bce4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
      fullname: Vatsa Pandey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VatsaDev
      type: user
    createdAt: '2023-11-07T16:02:37.000Z'
    data:
      edited: false
      editors:
      - VatsaDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8964388370513916
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
          fullname: Vatsa Pandey
          isHf: false
          isPro: false
          name: VatsaDev
          type: user
        html: '<p>ok, so your visual layer is turning images to embeddings through
          an <code>nn.linear</code> class?</p>

          <p>Did you really have to train it, or does image to embedding just work?</p>

          <p>Also, Im sorry if this is too much, but im new to pytorch, learning it,
          could you give me code example of  image -&gt; embedding -&gt; image?</p>

          '
        raw: 'ok, so your visual layer is turning images to embeddings through an
          `nn.linear` class?


          Did you really have to train it, or does image to embedding just work?


          Also, Im sorry if this is too much, but im new to pytorch, learning it,
          could you give me code example of  image -> embedding -> image?'
        updatedAt: '2023-11-07T16:02:37.320Z'
      numEdits: 0
      reactions: []
    id: 654a5f9da2de0e36e43eb01a
    type: comment
  author: VatsaDev
  content: 'ok, so your visual layer is turning images to embeddings through an `nn.linear`
    class?


    Did you really have to train it, or does image to embedding just work?


    Also, Im sorry if this is too much, but im new to pytorch, learning it, could
    you give me code example of  image -> embedding -> image?'
  created_at: 2023-11-07 16:02:37+00:00
  edited: false
  hidden: false
  id: 654a5f9da2de0e36e43eb01a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/81c1183b7a1b3dc9e4f8e95b63372216.svg
      fullname: Xhafer Epiroti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bn22
      type: user
    createdAt: '2023-11-10T13:44:44.000Z'
    data:
      edited: false
      editors:
      - bn22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9023631811141968
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/81c1183b7a1b3dc9e4f8e95b63372216.svg
          fullname: Xhafer Epiroti
          isHf: false
          isPro: false
          name: bn22
          type: user
        html: '<blockquote>

          <p>ok, so your visual layer is turning images to embeddings through an <code>nn.linear</code>
          class?</p>

          <p>Did you really have to train it, or does image to embedding just work?</p>

          <p>Also, Im sorry if this is too much, but im new to pytorch, learning it,
          could you give me code example of  image -&gt; embedding -&gt; image?</p>

          </blockquote>

          <p>The linear layer has to be trained.</p>

          '
        raw: "> ok, so your visual layer is turning images to embeddings through an\
          \ `nn.linear` class?\n> \n> Did you really have to train it, or does image\
          \ to embedding just work?\n> \n> Also, Im sorry if this is too much, but\
          \ im new to pytorch, learning it, could you give me code example of  image\
          \ -> embedding -> image?\n\nThe linear layer has to be trained."
        updatedAt: '2023-11-10T13:44:44.588Z'
      numEdits: 0
      reactions: []
    id: 654e33ccd423b4e0ef833532
    type: comment
  author: bn22
  content: "> ok, so your visual layer is turning images to embeddings through an\
    \ `nn.linear` class?\n> \n> Did you really have to train it, or does image to\
    \ embedding just work?\n> \n> Also, Im sorry if this is too much, but im new to\
    \ pytorch, learning it, could you give me code example of  image -> embedding\
    \ -> image?\n\nThe linear layer has to be trained."
  created_at: 2023-11-10 13:44:44+00:00
  edited: false
  hidden: false
  id: 654e33ccd423b4e0ef833532
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 45
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: How does the Fuyu model Get images?
