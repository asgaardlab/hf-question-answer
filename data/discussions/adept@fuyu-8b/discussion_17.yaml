!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sagar-kris
conflicting_files: null
created_at: 2023-10-20 06:08:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce90e5fc66f2d6d5f67bdf4c1e3d3479.svg
      fullname: Sagar Krishnaraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sagar-kris
      type: user
    createdAt: '2023-10-20T07:08:42.000Z'
    data:
      edited: true
      editors:
      - sagar-kris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7692496180534363
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce90e5fc66f2d6d5f67bdf4c1e3d3479.svg
          fullname: Sagar Krishnaraj
          isHf: false
          isPro: false
          name: sagar-kris
          type: user
        html: '<p>wondering if there''s any way to run this model on my Macbook pro
          M1. the <code>"cuda:0"</code> causes all sorts of issues, even after setting
          <code>device=torch.float16</code>.</p>

          '
        raw: wondering if there's any way to run this model on my Macbook pro M1.
          the `"cuda:0"` causes all sorts of issues, even after setting `device=torch.float16`.
        updatedAt: '2023-10-20T07:09:48.828Z'
      numEdits: 1
      reactions: []
    id: 6532277ad477c43e79a87126
    type: comment
  author: sagar-kris
  content: wondering if there's any way to run this model on my Macbook pro M1. the
    `"cuda:0"` causes all sorts of issues, even after setting `device=torch.float16`.
  created_at: 2023-10-20 06:08:42+00:00
  edited: true
  hidden: false
  id: 6532277ad477c43e79a87126
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-10-20T08:31:10.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9350871443748474
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: "<p>The device you should use is either \u201Ccpu\u201D or \u201Cmps\u201D\
          \ for Mac </p>\n"
        raw: "The device you should use is either \u201Ccpu\u201D or \u201Cmps\u201D\
          \ for Mac "
        updatedAt: '2023-10-20T08:31:10.552Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - ntcho13
        - wong2
        - dduflyme
        - pcuenq
    id: 65323aceef70d3555f4bfdb4
    type: comment
  author: ArthurZ
  content: "The device you should use is either \u201Ccpu\u201D or \u201Cmps\u201D\
    \ for Mac "
  created_at: 2023-10-20 07:31:10+00:00
  edited: false
  hidden: false
  id: 65323aceef70d3555f4bfdb4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636028075422-601abe651d780db54a93ea8f.jpeg?w=200&h=200&f=face
      fullname: Louis Beaumont
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: louis030195
      type: user
    createdAt: '2023-10-28T20:53:37.000Z'
    data:
      edited: false
      editors:
      - louis030195
      hidden: false
      identifiedLanguage:
        language: sah
        probability: 0.3222160339355469
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636028075422-601abe651d780db54a93ea8f.jpeg?w=200&h=200&f=face
          fullname: Louis Beaumont
          isHf: false
          isPro: false
          name: louis030195
          type: user
        html: "<p>Anyone managed to run it? </p>\n<pre><code>Using device: mps\nDownloading\
          \ (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 768/768 [00:00&lt;00:00,\
          \ 1.59MB/s]\nDownloading (\u2026)fetensors.index.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58.2k/58.2k\
          \ [00:00&lt;00:00, 800kB/s]\nDownloading (\u2026)of-00002.safetensors: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.93G/9.93G\
          \ [06:55&lt;00:00, 23.9MB/s]\nDownloading (\u2026)of-00002.safetensors:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 8.88G/8.88G [08:00&lt;00:00, 18.5MB/s]\nDownloading shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [14:56&lt;00:00,\
          \ 448.48s/it]\nLoading checkpoint shards:   0%|                        \
          \                                                                      \
          \                                      | 0/2 [00:00&lt;?, ?it/s]Killed:\
          \ 9\n/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224:\
          \ UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects\
          \ to clean up at shutdown\n  warnings.warn('resource_tracker: There appear\
          \ to be %d '\n</code></pre>\n<p>Macbook pro m2 32 gb ram - not enough?</p>\n"
        raw: "Anyone managed to run it? \n\n```\nUsing device: mps\nDownloading (\u2026\
          )lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 768/768 [00:00<00:00, 1.59MB/s]\n\
          Downloading (\u2026)fetensors.index.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58.2k/58.2k [00:00<00:00, 800kB/s]\n\
          Downloading (\u2026)of-00002.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 9.93G/9.93G [06:55<00:00, 23.9MB/s]\n\
          Downloading (\u2026)of-00002.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 8.88G/8.88G [08:00<00:00, 18.5MB/s]\n\
          Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 2/2 [14:56<00:00, 448.48s/it]\nLoading checkpoint shards:\
          \   0%|                                                                \
          \                                                                    | 0/2\
          \ [00:00<?, ?it/s]Killed: 9\n/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224:\
          \ UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects\
          \ to clean up at shutdown\n  warnings.warn('resource_tracker: There appear\
          \ to be %d '\n\n```\n\nMacbook pro m2 32 gb ram - not enough?"
        updatedAt: '2023-10-28T20:53:37.946Z'
      numEdits: 0
      reactions: []
    id: 653d74d1c2307cc448747e03
    type: comment
  author: louis030195
  content: "Anyone managed to run it? \n\n```\nUsing device: mps\nDownloading (\u2026\
    )lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 768/768 [00:00<00:00,\
    \ 1.59MB/s]\nDownloading (\u2026)fetensors.index.json: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 58.2k/58.2k [00:00<00:00, 800kB/s]\nDownloading (\u2026)of-00002.safetensors:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 9.93G/9.93G [06:55<00:00, 23.9MB/s]\nDownloading (\u2026\
    )of-00002.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.88G/8.88G [08:00<00:00, 18.5MB/s]\n\
    Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 2/2 [14:56<00:00, 448.48s/it]\nLoading checkpoint shards:\
    \   0%|                                                                      \
    \                                                              | 0/2 [00:00<?,\
    \ ?it/s]Killed: 9\n/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224:\
    \ UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects\
    \ to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to\
    \ be %d '\n\n```\n\nMacbook pro m2 32 gb ram - not enough?"
  created_at: 2023-10-28 19:53:37+00:00
  edited: false
  hidden: false
  id: 653d74d1c2307cc448747e03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-10-29T19:18:56.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4686409533023834
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: '<p>The following works for me. The Python process seems to take ~21
          GB when using <code>float16</code> in my M1.</p>

          <pre><code class="language-py"><span class="hljs-keyword">import</span>
          requests

          <span class="hljs-keyword">import</span> torch

          <span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span>
          Image

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoTokenizer, FuyuForCausalLM, FuyuImageProcessor, FuyuProcessor


          device = <span class="hljs-string">"mps"</span>


          <span class="hljs-comment"># Metal supports `bfloat16` in Sonoma, but it
          still doesn''t work</span>

          dtype = torch.bfloat16 <span class="hljs-keyword">if</span> device != <span
          class="hljs-string">"mps"</span> <span class="hljs-keyword">else</span>
          torch.float16


          model_id = <span class="hljs-string">"adept/fuyu-8b"</span>

          tokenizer = AutoTokenizer.from_pretrained(model_id)

          model = FuyuForCausalLM.from_pretrained(model_id, device_map=device, torch_dtype=dtype)

          processor = FuyuProcessor(image_processor=FuyuImageProcessor(), tokenizer=tokenizer)


          prompt = <span class="hljs-string">"Generate a coco-style caption.\n"</span>

          url = <span class="hljs-string">"https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png"</span>


          image = Image.<span class="hljs-built_in">open</span>(requests.get(url,
          stream=<span class="hljs-literal">True</span>).raw)


          model_inputs = processor(text=prompt, images=[image])

          model_inputs = {k: v.to(dtype=dtype <span class="hljs-keyword">if</span>
          torch.is_floating_point(v) <span class="hljs-keyword">else</span> v.dtype,
          device=device) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span>
          model_inputs.items()}

          prompt_len = model_inputs[<span class="hljs-string">"input_ids"</span>].shape[-<span
          class="hljs-number">1</span>]


          generation_output = model.generate(**model_inputs, max_new_tokens=<span
          class="hljs-number">10</span>)

          <span class="hljs-built_in">print</span>(tokenizer.decode(generation_output[<span
          class="hljs-number">0</span>][prompt_len:], skip_special_tokens=<span class="hljs-literal">True</span>))

          </code></pre>

          '
        raw: 'The following works for me. The Python process seems to take ~21 GB
          when using `float16` in my M1.


          ```py

          import requests

          import torch

          from PIL import Image

          from transformers import AutoTokenizer, FuyuForCausalLM, FuyuImageProcessor,
          FuyuProcessor


          device = "mps"


          # Metal supports `bfloat16` in Sonoma, but it still doesn''t work

          dtype = torch.bfloat16 if device != "mps" else torch.float16


          model_id = "adept/fuyu-8b"

          tokenizer = AutoTokenizer.from_pretrained(model_id)

          model = FuyuForCausalLM.from_pretrained(model_id, device_map=device, torch_dtype=dtype)

          processor = FuyuProcessor(image_processor=FuyuImageProcessor(), tokenizer=tokenizer)


          prompt = "Generate a coco-style caption.\n"

          url = "https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png"


          image = Image.open(requests.get(url, stream=True).raw)


          model_inputs = processor(text=prompt, images=[image])

          model_inputs = {k: v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype,
          device=device) for k,v in model_inputs.items()}

          prompt_len = model_inputs["input_ids"].shape[-1]


          generation_output = model.generate(**model_inputs, max_new_tokens=10)

          print(tokenizer.decode(generation_output[0][prompt_len:], skip_special_tokens=True))

          ```'
        updatedAt: '2023-10-29T19:18:56.996Z'
      numEdits: 0
      reactions: []
    id: 653eb0203ea696d46372d20b
    type: comment
  author: pcuenq
  content: 'The following works for me. The Python process seems to take ~21 GB when
    using `float16` in my M1.


    ```py

    import requests

    import torch

    from PIL import Image

    from transformers import AutoTokenizer, FuyuForCausalLM, FuyuImageProcessor, FuyuProcessor


    device = "mps"


    # Metal supports `bfloat16` in Sonoma, but it still doesn''t work

    dtype = torch.bfloat16 if device != "mps" else torch.float16


    model_id = "adept/fuyu-8b"

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    model = FuyuForCausalLM.from_pretrained(model_id, device_map=device, torch_dtype=dtype)

    processor = FuyuProcessor(image_processor=FuyuImageProcessor(), tokenizer=tokenizer)


    prompt = "Generate a coco-style caption.\n"

    url = "https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png"


    image = Image.open(requests.get(url, stream=True).raw)


    model_inputs = processor(text=prompt, images=[image])

    model_inputs = {k: v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype,
    device=device) for k,v in model_inputs.items()}

    prompt_len = model_inputs["input_ids"].shape[-1]


    generation_output = model.generate(**model_inputs, max_new_tokens=10)

    print(tokenizer.decode(generation_output[0][prompt_len:], skip_special_tokens=True))

    ```'
  created_at: 2023-10-29 18:18:56+00:00
  edited: false
  hidden: false
  id: 653eb0203ea696d46372d20b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a207b84351dda6adc70161fc9d77dc2b.svg
      fullname: faraday
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: faraday
      type: user
    createdAt: '2023-12-01T07:04:24.000Z'
    data:
      edited: true
      editors:
      - faraday
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9424397349357605
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a207b84351dda6adc70161fc9d77dc2b.svg
          fullname: faraday
          isHf: false
          isPro: false
          name: faraday
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pcuenq\">@<span class=\"\
          underline\">pcuenq</span></a></span>\n\n\t</span></span> Which transformers\
          \ version is this running with? I tried and when you're doing v.to(dtype=...)\
          \ , v is a list of tensors. In order for that call to work v must have been\
          \ a tensor.</p>\n<p>model_inputs = processor(text=prompt, images=[image])<br>for\
          \ k, v in model_inputs.items():<br>    if isinstance(v, list):<br>     \
          \   model_inputs[k] = [item.to(dtype=dtype if torch.is_floating_point(item)\
          \ else item.dtype, device=device) for item in v]<br>    else:<br>      \
          \  model_inputs[k] = v.to(dtype=dtype if torch.is_floating_point(v) else\
          \ v.dtype, device=device)</p>\n<p>Works like this. The else is not needed\
          \ on my part but I wanted to keep the original case as well.</p>\n"
        raw: "@pcuenq Which transformers version is this running with? I tried and\
          \ when you're doing v.to(dtype=...) , v is a list of tensors. In order for\
          \ that call to work v must have been a tensor.\n\nmodel_inputs = processor(text=prompt,\
          \ images=[image])\nfor k, v in model_inputs.items():\n    if isinstance(v,\
          \ list):\n        model_inputs[k] = [item.to(dtype=dtype if torch.is_floating_point(item)\
          \ else item.dtype, device=device) for item in v]\n    else:\n        model_inputs[k]\
          \ = v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype, device=device)\n\
          \nWorks like this. The else is not needed on my part but I wanted to keep\
          \ the original case as well.\n"
        updatedAt: '2023-12-01T07:09:58.253Z'
      numEdits: 2
      reactions: []
    id: 65698578da84dcf5973a28ae
    type: comment
  author: faraday
  content: "@pcuenq Which transformers version is this running with? I tried and when\
    \ you're doing v.to(dtype=...) , v is a list of tensors. In order for that call\
    \ to work v must have been a tensor.\n\nmodel_inputs = processor(text=prompt,\
    \ images=[image])\nfor k, v in model_inputs.items():\n    if isinstance(v, list):\n\
    \        model_inputs[k] = [item.to(dtype=dtype if torch.is_floating_point(item)\
    \ else item.dtype, device=device) for item in v]\n    else:\n        model_inputs[k]\
    \ = v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype, device=device)\n\
    \nWorks like this. The else is not needed on my part but I wanted to keep the\
    \ original case as well.\n"
  created_at: 2023-12-01 07:04:24+00:00
  edited: true
  hidden: false
  id: 65698578da84dcf5973a28ae
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: Run on MBP M1
