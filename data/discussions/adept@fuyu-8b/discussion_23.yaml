!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Techie5879
conflicting_files: null
created_at: 2023-10-23 11:04:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46567ff1323dfc9df9d624de1339a75b.svg
      fullname: Aritra Bandyopadhyay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Techie5879
      type: user
    createdAt: '2023-10-23T12:04:25.000Z'
    data:
      edited: false
      editors:
      - Techie5879
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6946330666542053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46567ff1323dfc9df9d624de1339a75b.svg
          fullname: Aritra Bandyopadhyay
          isHf: false
          isPro: false
          name: Techie5879
          type: user
        html: "<p>I'm trying to use the following code snippet to load the model on\
          \ a multi-gpu setup (NVIDIA TESLA T4 x4)</p>\n<pre><code>from transformers\
          \ import FuyuProcessor, FuyuForCausalLM\nfrom PIL import Image\n\n# load\
          \ model and processor\nmodel_id = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(model_id)\n\
          model = FuyuForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\
          \n# prepare inputs for the model\ntext_prompt = \"Generate a coco-style\
          \ caption.\\n\"\nimage_path = \"bus.png\"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/bus.png\n\
          image = Image.open(image_path)\n\ninputs = processor(text=text_prompt, images=image,\
          \ return_tensors=\"pt\")\nfor k, v in inputs.items():\n    inputs[k] = v.to(\"\
          cuda\")\n\n# autoregressively generate text\ngeneration_output = model.generate(**inputs,\
          \ max_new_tokens=7)\ngeneration_text = processor.batch_decode(generation_output[:,\
          \ -7:], skip_special_tokens=True)\nassert generation_text == ['A bus parked\
          \ on the side of a road.']\n</code></pre>\n<p>This doesn't seem to work,\
          \ and the generation process returns an error that \"indices should be either\
          \ on cpu or on the same device as the indexed tensor\"</p>\n<p>Is there\
          \ any fix to this, or do I need to use a custom device map?</p>\n"
        raw: "I'm trying to use the following code snippet to load the model on a\
          \ multi-gpu setup (NVIDIA TESLA T4 x4)\r\n```\r\nfrom transformers import\
          \ FuyuProcessor, FuyuForCausalLM\r\nfrom PIL import Image\r\n\r\n# load\
          \ model and processor\r\nmodel_id = \"adept/fuyu-8b\"\r\nprocessor = FuyuProcessor.from_pretrained(model_id)\r\
          \nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map=\"auto\"\
          )\r\n\r\n# prepare inputs for the model\r\ntext_prompt = \"Generate a coco-style\
          \ caption.\\n\"\r\nimage_path = \"bus.png\"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/bus.png\r\
          \nimage = Image.open(image_path)\r\n\r\ninputs = processor(text=text_prompt,\
          \ images=image, return_tensors=\"pt\")\r\nfor k, v in inputs.items():\r\n\
          \    inputs[k] = v.to(\"cuda\")\r\n\r\n# autoregressively generate text\r\
          \ngeneration_output = model.generate(**inputs, max_new_tokens=7)\r\ngeneration_text\
          \ = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)\r\
          \nassert generation_text == ['A bus parked on the side of a road.']\r\n\
          ```\r\n\r\nThis doesn't seem to work, and the generation process returns\
          \ an error that \"indices should be either on cpu or on the same device\
          \ as the indexed tensor\"\r\n\r\nIs there any fix to this, or do I need\
          \ to use a custom device map?"
        updatedAt: '2023-10-23T12:04:25.727Z'
      numEdits: 0
      reactions: []
    id: 65366149910b844786d3792c
    type: comment
  author: Techie5879
  content: "I'm trying to use the following code snippet to load the model on a multi-gpu\
    \ setup (NVIDIA TESLA T4 x4)\r\n```\r\nfrom transformers import FuyuProcessor,\
    \ FuyuForCausalLM\r\nfrom PIL import Image\r\n\r\n# load model and processor\r\
    \nmodel_id = \"adept/fuyu-8b\"\r\nprocessor = FuyuProcessor.from_pretrained(model_id)\r\
    \nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map=\"auto\")\r\n\r\
    \n# prepare inputs for the model\r\ntext_prompt = \"Generate a coco-style caption.\\\
    n\"\r\nimage_path = \"bus.png\"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/bus.png\r\
    \nimage = Image.open(image_path)\r\n\r\ninputs = processor(text=text_prompt, images=image,\
    \ return_tensors=\"pt\")\r\nfor k, v in inputs.items():\r\n    inputs[k] = v.to(\"\
    cuda\")\r\n\r\n# autoregressively generate text\r\ngeneration_output = model.generate(**inputs,\
    \ max_new_tokens=7)\r\ngeneration_text = processor.batch_decode(generation_output[:,\
    \ -7:], skip_special_tokens=True)\r\nassert generation_text == ['A bus parked\
    \ on the side of a road.']\r\n```\r\n\r\nThis doesn't seem to work, and the generation\
    \ process returns an error that \"indices should be either on cpu or on the same\
    \ device as the indexed tensor\"\r\n\r\nIs there any fix to this, or do I need\
    \ to use a custom device map?"
  created_at: 2023-10-23 11:04:25+00:00
  edited: false
  hidden: false
  id: 65366149910b844786d3792c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-10-24T10:16:50.000Z'
    data:
      edited: false
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9280049204826355
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: '<p>Hey, the PR <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/27007">https://github.com/huggingface/transformers/pull/27007</a>
          aims at improving the image processor, right now device_map auto on multi
          gpu indeed seems to have issues, will be fixed there! For now you have to
          manually set your devices.</p>

          '
        raw: Hey, the PR https://github.com/huggingface/transformers/pull/27007 aims
          at improving the image processor, right now device_map auto on multi gpu
          indeed seems to have issues, will be fixed there! For now you have to manually
          set your devices.
        updatedAt: '2023-10-24T10:16:50.624Z'
      numEdits: 0
      reactions: []
    id: 6537999272f105eef03c93d8
    type: comment
  author: Molbap
  content: Hey, the PR https://github.com/huggingface/transformers/pull/27007 aims
    at improving the image processor, right now device_map auto on multi gpu indeed
    seems to have issues, will be fixed there! For now you have to manually set your
    devices.
  created_at: 2023-10-24 09:16:50+00:00
  edited: false
  hidden: false
  id: 6537999272f105eef03c93d8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: Loading the model on multi-gpu setup?
