!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gpantalos
conflicting_files: null
created_at: 2023-10-19 17:23:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21d5cf8993b12311b62b2fe82a05ae2d.svg
      fullname: Georges Pantalos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gpantalos
      type: user
    createdAt: '2023-10-19T18:23:14.000Z'
    data:
      edited: false
      editors:
      - gpantalos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43711140751838684
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21d5cf8993b12311b62b2fe82a05ae2d.svg
          fullname: Georges Pantalos
          isHf: false
          isPro: false
          name: gpantalos
          type: user
        html: '<p>I get this bug when deploying as is: </p>

          <pre><code>2023/10/19 20:07:32 ~ INFO | Using device GPU 2023/10/19 20:07:32
          ~ INFO | No custom pipeline found at /repository/handler.py 2023/10/19 20:07:32
          ~ 2023-10-19 18:07:32,283 | INFO | Initializing model from directory:/repository
          2023/10/19 20:07:32 ~ KeyError: ''fuyu'' 2023/10/19 20:07:32 ~ File "/app/huggingface_inference_toolkit/utils.py",
          line 261, in get_pipeline 2023/10/19 20:07:32 ~ self.pipeline = get_pipeline(model_dir=model_dir,
          task=task) 2023/10/19 20:07:32 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 682, in startup 2023/10/19 20:07:32 ~ Application startup failed. Exiting.
          2023/10/19 20:07:32 ~ raise KeyError(key) 2023/10/19 20:07:32 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py",
          line 710, in __getitem__ 2023/10/19 20:07:32 ~ config_class = CONFIG_MAPPING[config_dict["model_type"]]
          2023/10/19 20:07:32 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py",
          line 998, in from_pretrained 2023/10/19 20:07:32 ~ config = AutoConfig.from_pretrained(model,
          _from_pipeline=task, **hub_kwargs, **model_kwargs) 2023/10/19 20:07:32 ~
          File "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/__init__.py",
          line 705, in pipeline 2023/10/19 20:07:32 ~ File "/app/huggingface_inference_toolkit/handler.py",
          line 45, in get_inference_handler_either_custom_or_default_handler 2023/10/19
          20:07:32 ~ File "/app/webservice_starlette.py", line 57, in some_startup_task
          2023/10/19 20:07:32 ~ await handler() 2023/10/19 20:07:32 ~ await self._router.startup()
          2023/10/19 20:07:32 ~ async with self.lifespan_context(app) as maybe_state:
          2023/10/19 20:07:32 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 705, in lifespan 2023/10/19 20:07:32 ~ Traceback (most recent call
          last): 2023/10/19 20:07:32 ~ return HuggingFaceHandler(model_dir=model_dir,
          task=task) 2023/10/19 20:07:32 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 584, in __aenter__ pqshc 2023-10-19T18:07:32.285Z  2023/10/19 20:07:32
          ~ hf_pipeline = pipeline(task=task, model=model_dir, device=device, **kwargs)
          2023/10/19 20:07:32 ~ File "/app/huggingface_inference_toolkit/handler.py",
          line 17, in __init__ 2023/10/19 20:07:32 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,
          task=HF_TASK)

          </code></pre>

          <p>could you upload a step by step tutorial for deploying to inference endpoints?<br>Thanks</p>

          '
        raw: "I get this bug when deploying as is: \r\n\r\n```\r\n2023/10/19 20:07:32\
          \ ~ INFO | Using device GPU 2023/10/19 20:07:32 ~ INFO | No custom pipeline\
          \ found at /repository/handler.py 2023/10/19 20:07:32 ~ 2023-10-19 18:07:32,283\
          \ | INFO | Initializing model from directory:/repository 2023/10/19 20:07:32\
          \ ~ KeyError: 'fuyu' 2023/10/19 20:07:32 ~ File \"/app/huggingface_inference_toolkit/utils.py\"\
          , line 261, in get_pipeline 2023/10/19 20:07:32 ~ self.pipeline = get_pipeline(model_dir=model_dir,\
          \ task=task) 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
          , line 682, in startup 2023/10/19 20:07:32 ~ Application startup failed.\
          \ Exiting. 2023/10/19 20:07:32 ~ raise KeyError(key) 2023/10/19 20:07:32\
          \ ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 710, in __getitem__ 2023/10/19 20:07:32 ~ config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]] 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 998, in from_pretrained 2023/10/19 20:07:32 ~ config = AutoConfig.from_pretrained(model,\
          \ _from_pipeline=task, **hub_kwargs, **model_kwargs) 2023/10/19 20:07:32\
          \ ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
          , line 705, in pipeline 2023/10/19 20:07:32 ~ File \"/app/huggingface_inference_toolkit/handler.py\"\
          , line 45, in get_inference_handler_either_custom_or_default_handler 2023/10/19\
          \ 20:07:32 ~ File \"/app/webservice_starlette.py\", line 57, in some_startup_task\
          \ 2023/10/19 20:07:32 ~ await handler() 2023/10/19 20:07:32 ~ await self._router.startup()\
          \ 2023/10/19 20:07:32 ~ async with self.lifespan_context(app) as maybe_state:\
          \ 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
          , line 705, in lifespan 2023/10/19 20:07:32 ~ Traceback (most recent call\
          \ last): 2023/10/19 20:07:32 ~ return HuggingFaceHandler(model_dir=model_dir,\
          \ task=task) 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
          , line 584, in __aenter__ pqshc 2023-10-19T18:07:32.285Z  2023/10/19 20:07:32\
          \ ~ hf_pipeline = pipeline(task=task, model=model_dir, device=device, **kwargs)\
          \ 2023/10/19 20:07:32 ~ File \"/app/huggingface_inference_toolkit/handler.py\"\
          , line 17, in __init__ 2023/10/19 20:07:32 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,\
          \ task=HF_TASK)\r\n```\r\n\r\ncould you upload a step by step tutorial for\
          \ deploying to inference endpoints?\r\nThanks"
        updatedAt: '2023-10-19T18:23:14.508Z'
      numEdits: 0
      reactions: []
    id: 653174125c959f0f95c0afb1
    type: comment
  author: gpantalos
  content: "I get this bug when deploying as is: \r\n\r\n```\r\n2023/10/19 20:07:32\
    \ ~ INFO | Using device GPU 2023/10/19 20:07:32 ~ INFO | No custom pipeline found\
    \ at /repository/handler.py 2023/10/19 20:07:32 ~ 2023-10-19 18:07:32,283 | INFO\
    \ | Initializing model from directory:/repository 2023/10/19 20:07:32 ~ KeyError:\
    \ 'fuyu' 2023/10/19 20:07:32 ~ File \"/app/huggingface_inference_toolkit/utils.py\"\
    , line 261, in get_pipeline 2023/10/19 20:07:32 ~ self.pipeline = get_pipeline(model_dir=model_dir,\
    \ task=task) 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
    , line 682, in startup 2023/10/19 20:07:32 ~ Application startup failed. Exiting.\
    \ 2023/10/19 20:07:32 ~ raise KeyError(key) 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 710, in __getitem__ 2023/10/19 20:07:32 ~ config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]] 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 998, in from_pretrained 2023/10/19 20:07:32 ~ config = AutoConfig.from_pretrained(model,\
    \ _from_pipeline=task, **hub_kwargs, **model_kwargs) 2023/10/19 20:07:32 ~ File\
    \ \"/opt/conda/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
    , line 705, in pipeline 2023/10/19 20:07:32 ~ File \"/app/huggingface_inference_toolkit/handler.py\"\
    , line 45, in get_inference_handler_either_custom_or_default_handler 2023/10/19\
    \ 20:07:32 ~ File \"/app/webservice_starlette.py\", line 57, in some_startup_task\
    \ 2023/10/19 20:07:32 ~ await handler() 2023/10/19 20:07:32 ~ await self._router.startup()\
    \ 2023/10/19 20:07:32 ~ async with self.lifespan_context(app) as maybe_state:\
    \ 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
    , line 705, in lifespan 2023/10/19 20:07:32 ~ Traceback (most recent call last):\
    \ 2023/10/19 20:07:32 ~ return HuggingFaceHandler(model_dir=model_dir, task=task)\
    \ 2023/10/19 20:07:32 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
    , line 584, in __aenter__ pqshc 2023-10-19T18:07:32.285Z  2023/10/19 20:07:32\
    \ ~ hf_pipeline = pipeline(task=task, model=model_dir, device=device, **kwargs)\
    \ 2023/10/19 20:07:32 ~ File \"/app/huggingface_inference_toolkit/handler.py\"\
    , line 17, in __init__ 2023/10/19 20:07:32 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,\
    \ task=HF_TASK)\r\n```\r\n\r\ncould you upload a step by step tutorial for deploying\
    \ to inference endpoints?\r\nThanks"
  created_at: 2023-10-19 17:23:14+00:00
  edited: false
  hidden: false
  id: 653174125c959f0f95c0afb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-10-20T10:13:14.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957792341709137
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Hey! That is expected it''s not supported in inference endpoint
          <em>yet</em> </p>

          '
        raw: 'Hey! That is expected it''s not supported in inference endpoint *yet* '
        updatedAt: '2023-10-20T10:13:14.115Z'
      numEdits: 0
      reactions: []
    id: 653252ba548eae0c990e16b4
    type: comment
  author: ArthurZ
  content: 'Hey! That is expected it''s not supported in inference endpoint *yet* '
  created_at: 2023-10-20 09:13:14+00:00
  edited: false
  hidden: false
  id: 653252ba548eae0c990e16b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21d5cf8993b12311b62b2fe82a05ae2d.svg
      fullname: Georges Pantalos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gpantalos
      type: user
    createdAt: '2023-10-22T23:01:58.000Z'
    data:
      edited: false
      editors:
      - gpantalos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9053104519844055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21d5cf8993b12311b62b2fe82a05ae2d.svg
          fullname: Georges Pantalos
          isHf: false
          isPro: false
          name: gpantalos
          type: user
        html: '<p>Ok thanks, please let me know when they are.</p>

          '
        raw: Ok thanks, please let me know when they are.
        updatedAt: '2023-10-22T23:01:58.209Z'
      numEdits: 0
      reactions: []
    id: 6535a9e6e778506c5b3d2619
    type: comment
  author: gpantalos
  content: Ok thanks, please let me know when they are.
  created_at: 2023-10-22 22:01:58+00:00
  edited: false
  hidden: false
  id: 6535a9e6e778506c5b3d2619
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: Bug when deploying to Inference Endpoints
