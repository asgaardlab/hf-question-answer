!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nyandwi
conflicting_files: null
created_at: 2023-11-14 12:30:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654168730944-noauth.png?w=200&h=200&f=face
      fullname: Jean de Dieu Nyandwi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nyandwi
      type: user
    createdAt: '2023-11-14T12:30:39.000Z'
    data:
      edited: false
      editors:
      - Nyandwi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7559365034103394
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654168730944-noauth.png?w=200&h=200&f=face
          fullname: Jean de Dieu Nyandwi
          isHf: false
          isPro: false
          name: Nyandwi
          type: user
        html: '<p>Hello, thanks for this amazing visual language model.</p>

          <p>I am having memory issues while forwarding the inputs to the model. The
          generate functionality works fine and I can run it multiple times. But when
          trying to get the logits with <code>model(**inputs)</code>, I run out of
          memory. I have 48GB GPU RAM which is reasonably enough according to other
          discussions about devices. Is there something I am missing?</p>

          <pre><code class="language-python">model_id = <span class="hljs-string">"adept/fuyu-8b"</span>

          processor = FuyuProcessor.from_pretrained(model_id)

          model = FuyuForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">"auto"</span>,
          torch_dtype=torch.float16)


          inputs = processor(text=prompt, images=sample_im_1, return_tensors=<span
          class="hljs-string">"pt"</span>).to(<span class="hljs-string">"cuda:0"</span>)

          outputs =  model(**inputs)

          </code></pre>

          <p>Thanks!</p>

          '
        raw: "Hello, thanks for this amazing visual language model.\r\n\r\nI am having\
          \ memory issues while forwarding the inputs to the model. The generate functionality\
          \ works fine and I can run it multiple times. But when trying to get the\
          \ logits with `model(**inputs)`, I run out of memory. I have 48GB GPU RAM\
          \ which is reasonably enough according to other discussions about devices.\
          \ Is there something I am missing?\r\n\r\n```python\r\nmodel_id = \"adept/fuyu-8b\"\
          \r\nprocessor = FuyuProcessor.from_pretrained(model_id)\r\nmodel = FuyuForCausalLM.from_pretrained(model_id,\
          \ device_map=\"auto\", torch_dtype=torch.float16)\r\n\r\ninputs = processor(text=prompt,\
          \ images=sample_im_1, return_tensors=\"pt\").to(\"cuda:0\")\r\noutputs =\
          \  model(**inputs)\r\n```\r\n\r\nThanks!"
        updatedAt: '2023-11-14T12:30:39.946Z'
      numEdits: 0
      reactions: []
    id: 6553686f0c87b30938c24a05
    type: comment
  author: Nyandwi
  content: "Hello, thanks for this amazing visual language model.\r\n\r\nI am having\
    \ memory issues while forwarding the inputs to the model. The generate functionality\
    \ works fine and I can run it multiple times. But when trying to get the logits\
    \ with `model(**inputs)`, I run out of memory. I have 48GB GPU RAM which is reasonably\
    \ enough according to other discussions about devices. Is there something I am\
    \ missing?\r\n\r\n```python\r\nmodel_id = \"adept/fuyu-8b\"\r\nprocessor = FuyuProcessor.from_pretrained(model_id)\r\
    \nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\r\
    \n\r\ninputs = processor(text=prompt, images=sample_im_1, return_tensors=\"pt\"\
    ).to(\"cuda:0\")\r\noutputs =  model(**inputs)\r\n```\r\n\r\nThanks!"
  created_at: 2023-11-14 12:30:39+00:00
  edited: false
  hidden: false
  id: 6553686f0c87b30938c24a05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-11-15T12:05:07.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.940075695514679
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;Nyandwi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Nyandwi\"\
          >@<span class=\"underline\">Nyandwi</span></a></span>\n\n\t</span></span>!\
          \ What image sizes are you working with? Perhaps you could downscale them\
          \ to a max height of <code>1080</code> before processing? 48 GB of GPU RAM\
          \ should be plenty for the model.</p>\n"
        raw: Hello @Nyandwi! What image sizes are you working with? Perhaps you could
          downscale them to a max height of `1080` before processing? 48 GB of GPU
          RAM should be plenty for the model.
        updatedAt: '2023-11-15T12:05:07.500Z'
      numEdits: 0
      reactions: []
    id: 6554b3f3d7b239fd39cea23f
    type: comment
  author: pcuenq
  content: Hello @Nyandwi! What image sizes are you working with? Perhaps you could
    downscale them to a max height of `1080` before processing? 48 GB of GPU RAM should
    be plenty for the model.
  created_at: 2023-11-15 12:05:07+00:00
  edited: false
  hidden: false
  id: 6554b3f3d7b239fd39cea23f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654168730944-noauth.png?w=200&h=200&f=face
      fullname: Jean de Dieu Nyandwi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nyandwi
      type: user
    createdAt: '2023-11-16T09:34:42.000Z'
    data:
      edited: false
      editors:
      - Nyandwi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.953773558139801
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654168730944-noauth.png?w=200&h=200&f=face
          fullname: Jean de Dieu Nyandwi
          isHf: false
          isPro: false
          name: Nyandwi
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pcuenq\">@<span class=\"\
          underline\">pcuenq</span></a></span>\n\n\t</span></span>. Thanks for your\
          \ reply, really appreciate the support. The images have variable size, but\
          \ some of them are probably greater than 1080. I will ensure the images\
          \ are resized to that resolution and see if that solve the problem.</p>\n"
        raw: Hi, @pcuenq. Thanks for your reply, really appreciate the support. The
          images have variable size, but some of them are probably greater than 1080.
          I will ensure the images are resized to that resolution and see if that
          solve the problem.
        updatedAt: '2023-11-16T09:34:42.504Z'
      numEdits: 0
      reactions: []
    id: 6555e232c9d297462257f5bf
    type: comment
  author: Nyandwi
  content: Hi, @pcuenq. Thanks for your reply, really appreciate the support. The
    images have variable size, but some of them are probably greater than 1080. I
    will ensure the images are resized to that resolution and see if that solve the
    problem.
  created_at: 2023-11-16 09:34:42+00:00
  edited: false
  hidden: false
  id: 6555e232c9d297462257f5bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 49
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: Memory Spikes while Getting Model Logits
