!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ludeksvoboda
conflicting_files: null
created_at: 2023-10-29 19:23:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
      fullname: "Lud\u011Bk Svoboda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludeksvoboda
      type: user
    createdAt: '2023-10-29T20:23:12.000Z'
    data:
      edited: false
      editors:
      - ludeksvoboda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.970645546913147
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
          fullname: "Lud\u011Bk Svoboda"
          isHf: false
          isPro: false
          name: ludeksvoboda
          type: user
        html: '<p>Hi,<br>as I understand the released model is not capable of the
          OCR, bbox_to_text and text_to_bbox, correct?<br>Are there any resources
          as how to go about finetuning the model for this?<br>Nice work and thank
          you!</p>

          '
        raw: "Hi,\r\nas I understand the released model is not capable of the OCR,\
          \ bbox_to_text and text_to_bbox, correct?\r\nAre there any resources as\
          \ how to go about finetuning the model for this?\r\nNice work and thank\
          \ you!"
        updatedAt: '2023-10-29T20:23:12.668Z'
      numEdits: 0
      reactions: []
    id: 653ebf30c2307cc44898fabe
    type: comment
  author: ludeksvoboda
  content: "Hi,\r\nas I understand the released model is not capable of the OCR, bbox_to_text\
    \ and text_to_bbox, correct?\r\nAre there any resources as how to go about finetuning\
    \ the model for this?\r\nNice work and thank you!"
  created_at: 2023-10-29 19:23:12+00:00
  edited: false
  hidden: false
  id: 653ebf30c2307cc44898fabe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-11-03T08:50:15.000Z'
    data:
      edited: false
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.634537935256958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ludeksvoboda&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ludeksvoboda\"\
          >@<span class=\"underline\">ludeksvoboda</span></a></span>\n\n\t</span></span>\
          \ , with the recent transformers release (run a <code>pip install --upgrade\
          \ transformers</code>) the model should! Given bbox coordinates, it will\
          \ perform OCR within that bbox. </p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\"\
          >import</span> Image\n<span class=\"hljs-keyword\">import</span> requests\n\
          <span class=\"hljs-keyword\">import</span> io\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> FuyuForCausalLM,\
          \ FuyuProcessor\n\npretrained_path = <span class=\"hljs-string\">\"adept/fuyu-8b\"\
          </span>\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\nmodel\
          \ = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=<span class=\"\
          hljs-string\">'auto'</span>)\n\n\nbbox_prompt = <span class=\"hljs-string\"\
          >\"When presented with a box, perform OCR to extract text contained within\
          \ it. If provided with text, generate the corresponding bounding box.\\\\\
          n&lt;box&gt;388, 428, 404, 488&lt;/box&gt;\"</span>\nbbox_image_url = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
          </span>\nbbox_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(bbox_image_url).content))\n\
          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to(<span\
          \ class=\"hljs-string\">'cuda'</span>)\n\n\nmodel_outputs = processor.batch_decode(model.generate(\n\
          \    **model_inputs, max_new_tokens=<span class=\"hljs-number\">10</span>)[:,\
          \ -<span class=\"hljs-number\">10</span>:], skip_special_tokens=<span class=\"\
          hljs-literal\">True</span>)[<span class=\"hljs-number\">0</span>]\nprediction\
          \ = model_outputs.split(<span class=\"hljs-string\">'\\x04 '</span>, <span\
          \ class=\"hljs-number\">1</span>)[<span class=\"hljs-number\">1</span>]\
          \ <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'\\\
          x04'</span> <span class=\"hljs-keyword\">in</span> model_outputs <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-string\">''</span>\n</code></pre>\n\
          <p>This should output <code>Williams</code>, the text contained within coordinates.\
          \ text_to_bbox should work as well, with <code>processor.post_process_box_coordinates</code>.\
          \  Have fun!</p>\n"
        raw: "Hi @ludeksvoboda , with the recent transformers release (run a `pip\
          \ install --upgrade transformers`) the model should! Given bbox coordinates,\
          \ it will perform OCR within that bbox. \n```python\nfrom PIL import Image\n\
          import requests\nimport io\nfrom transformers import FuyuForCausalLM, FuyuProcessor\n\
          \npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\
          \n\nbbox_prompt = \"When presented with a box, perform OCR to extract text\
          \ contained within it. If provided with text, generate the corresponding\
          \ bounding box.\\\\n<box>388, 428, 404, 488</box>\"\nbbox_image_url = \"\
          https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
          \nbbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))\n\
          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')\n\
          \n\nmodel_outputs = processor.batch_decode(model.generate(\n    **model_inputs,\
          \ max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]\nprediction\
          \ = model_outputs.split('\\x04 ', 1)[1] if '\\x04' in model_outputs else\
          \ ''\n```\nThis should output `Williams`, the text contained within coordinates.\
          \ text_to_bbox should work as well, with `processor.post_process_box_coordinates`.\
          \  Have fun!"
        updatedAt: '2023-11-03T08:50:15.602Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - ludeksvoboda
    id: 6544b4476f7fb5b3548cebd8
    type: comment
  author: Molbap
  content: "Hi @ludeksvoboda , with the recent transformers release (run a `pip install\
    \ --upgrade transformers`) the model should! Given bbox coordinates, it will perform\
    \ OCR within that bbox. \n```python\nfrom PIL import Image\nimport requests\n\
    import io\nfrom transformers import FuyuForCausalLM, FuyuProcessor\n\npretrained_path\
    \ = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
    model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\n\
    \nbbox_prompt = \"When presented with a box, perform OCR to extract text contained\
    \ within it. If provided with text, generate the corresponding bounding box.\\\
    \\n<box>388, 428, 404, 488</box>\"\nbbox_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg\"\
    \nbbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))\n\
    model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')\n\
    \n\nmodel_outputs = processor.batch_decode(model.generate(\n    **model_inputs,\
    \ max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]\nprediction = model_outputs.split('\\\
    x04 ', 1)[1] if '\\x04' in model_outputs else ''\n```\nThis should output `Williams`,\
    \ the text contained within coordinates. text_to_bbox should work as well, with\
    \ `processor.post_process_box_coordinates`.  Have fun!"
  created_at: 2023-11-03 07:50:15+00:00
  edited: false
  hidden: false
  id: 6544b4476f7fb5b3548cebd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
      fullname: "Lud\u011Bk Svoboda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludeksvoboda
      type: user
    createdAt: '2023-11-03T08:55:01.000Z'
    data:
      edited: false
      editors:
      - ludeksvoboda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8764714598655701
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
          fullname: "Lud\u011Bk Svoboda"
          isHf: false
          isPro: false
          name: ludeksvoboda
          type: user
        html: "<p>That is absolutely awesome <span data-props=\"{&quot;user&quot;:&quot;Molbap&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Molbap\"\
          >@<span class=\"underline\">Molbap</span></a></span>\n\n\t</span></span>\
          \ !<br>Thank you for the comprehensive reply!</p>\n"
        raw: 'That is absolutely awesome @Molbap !

          Thank you for the comprehensive reply!'
        updatedAt: '2023-11-03T08:55:01.898Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Molbap
    id: 6544b565fb337c80cdc7386e
    type: comment
  author: ludeksvoboda
  content: 'That is absolutely awesome @Molbap !

    Thank you for the comprehensive reply!'
  created_at: 2023-11-03 07:55:01+00:00
  edited: false
  hidden: false
  id: 6544b565fb337c80cdc7386e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a3182996bd41b526dcbfa8687d91963.svg
      fullname: Kanzhi Cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cckevinn
      type: user
    createdAt: '2023-11-05T16:23:15.000Z'
    data:
      edited: false
      editors:
      - cckevinn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6219093203544617
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a3182996bd41b526dcbfa8687d91963.svg
          fullname: Kanzhi Cheng
          isHf: false
          isPro: false
          name: cckevinn
          type: user
        html: '<p>Hi, nice work!<br>I wonder how to use text_to_bbox to locate items.
          I try:</p>

          <pre><code>bbox_prompt = "When presented with a box, perform OCR to extract
          text contained within it. If provided with text, generate the corresponding
          bounding box.\\n 561 Dillman"

          bbox_image_url = "https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg"

          bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))

          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to(''cuda'')


          model_outputs = model.generate(**model_inputs, max_new_tokens=20)[:, -20:]

          model_outputs = processor.post_process_box_coordinates(model_outputs)

          model_outputs = processor.batch_decode(model_outputs, skip_special_tokens=True)[0]

          print(model_outputs)

          </code></pre>

          <p>And it outputs <code>text, generate the corresponding bounding box.\n
          Williams&lt;box&gt;388, 428, 404, 900&lt;/box&gt;</code>, is this the right
          way to use it?</p>

          '
        raw: 'Hi, nice work!

          I wonder how to use text_to_bbox to locate items. I try:

          ```

          bbox_prompt = "When presented with a box, perform OCR to extract text contained
          within it. If provided with text, generate the corresponding bounding box.\\n
          561 Dillman"

          bbox_image_url = "https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg"

          bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))

          model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to(''cuda'')


          model_outputs = model.generate(**model_inputs, max_new_tokens=20)[:, -20:]

          model_outputs = processor.post_process_box_coordinates(model_outputs)

          model_outputs = processor.batch_decode(model_outputs, skip_special_tokens=True)[0]

          print(model_outputs)

          ```

          And it outputs `text, generate the corresponding bounding box.\n Williams<box>388,
          428, 404, 900</box>`, is this the right way to use it?'
        updatedAt: '2023-11-05T16:23:15.509Z'
      numEdits: 0
      reactions: []
    id: 6547c17308568852407a0a67
    type: comment
  author: cckevinn
  content: 'Hi, nice work!

    I wonder how to use text_to_bbox to locate items. I try:

    ```

    bbox_prompt = "When presented with a box, perform OCR to extract text contained
    within it. If provided with text, generate the corresponding bounding box.\\n
    561 Dillman"

    bbox_image_url = "https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg"

    bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))

    model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to(''cuda'')


    model_outputs = model.generate(**model_inputs, max_new_tokens=20)[:, -20:]

    model_outputs = processor.post_process_box_coordinates(model_outputs)

    model_outputs = processor.batch_decode(model_outputs, skip_special_tokens=True)[0]

    print(model_outputs)

    ```

    And it outputs `text, generate the corresponding bounding box.\n Williams<box>388,
    428, 404, 900</box>`, is this the right way to use it?'
  created_at: 2023-11-05 16:23:15+00:00
  edited: false
  hidden: false
  id: 6547c17308568852407a0a67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
      fullname: "Lud\u011Bk Svoboda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludeksvoboda
      type: user
    createdAt: '2023-11-05T19:59:55.000Z'
    data:
      edited: true
      editors:
      - ludeksvoboda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9426825642585754
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
          fullname: "Lud\u011Bk Svoboda"
          isHf: false
          isPro: false
          name: ludeksvoboda
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cckevinn&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cckevinn\">@<span class=\"\
          underline\">cckevinn</span></a></span>\n\n\t</span></span> try to have a\
          \ look at this <a href=\"https://huggingface.co/adept/fuyu-8b/discussions/38\"\
          >https://huggingface.co/adept/fuyu-8b/discussions/38</a> , but essentially\
          \ you have it correct I think.<br>I have tried the linked solution and it\
          \ works somewhat on the resized image (to 1/2 of the original size), very\
          \ likely it does even better on the fullsized image. Tried also to crop\
          \ the test image so it contains only the part filled with text (removed\
          \ the white space on both sides) and it fails to generate any bbox, I either\
          \ get empty string or part of some text. I think the model has problems\
          \ with different image sizes.<br>Only thing I had to tinker with was permuting\
          \ the coordinates for ploting.</p>\n<pre><code>def permute_bbox(bbox):\n\
          \    return (bbox[1], bbox[0], bbox[3], bbox[2])\n\ndef plot_bbox(img, bbox):\n\
          \    \"\"\"simplest way to plot bounding box on the image\"\"\"\n    if\
          \ isinstance(img, np.ndarray):\n        img = Image.fromarray(img)\n   \
          \ draw = ImageDraw.Draw(img)\n    draw.rectangle(bbox, outline='red')\n\
          \    return img\n</code></pre>\n"
        raw: "@cckevinn try to have a look at this https://huggingface.co/adept/fuyu-8b/discussions/38\
          \ , but essentially you have it correct I think.\nI have tried the linked\
          \ solution and it works somewhat on the resized image (to 1/2 of the original\
          \ size), very likely it does even better on the fullsized image. Tried also\
          \ to crop the test image so it contains only the part filled with text (removed\
          \ the white space on both sides) and it fails to generate any bbox, I either\
          \ get empty string or part of some text. I think the model has problems\
          \ with different image sizes.\nOnly thing I had to tinker with was permuting\
          \ the coordinates for ploting.\n```\ndef permute_bbox(bbox):\n    return\
          \ (bbox[1], bbox[0], bbox[3], bbox[2])\n\ndef plot_bbox(img, bbox):\n  \
          \  \"\"\"simplest way to plot bounding box on the image\"\"\"\n    if isinstance(img,\
          \ np.ndarray):\n        img = Image.fromarray(img)\n    draw = ImageDraw.Draw(img)\n\
          \    draw.rectangle(bbox, outline='red')\n    return img\n```"
        updatedAt: '2023-11-05T20:38:24.408Z'
      numEdits: 3
      reactions: []
    id: 6547f43b7e0f913218463ac2
    type: comment
  author: ludeksvoboda
  content: "@cckevinn try to have a look at this https://huggingface.co/adept/fuyu-8b/discussions/38\
    \ , but essentially you have it correct I think.\nI have tried the linked solution\
    \ and it works somewhat on the resized image (to 1/2 of the original size), very\
    \ likely it does even better on the fullsized image. Tried also to crop the test\
    \ image so it contains only the part filled with text (removed the white space\
    \ on both sides) and it fails to generate any bbox, I either get empty string\
    \ or part of some text. I think the model has problems with different image sizes.\n\
    Only thing I had to tinker with was permuting the coordinates for ploting.\n```\n\
    def permute_bbox(bbox):\n    return (bbox[1], bbox[0], bbox[3], bbox[2])\n\ndef\
    \ plot_bbox(img, bbox):\n    \"\"\"simplest way to plot bounding box on the image\"\
    \"\"\n    if isinstance(img, np.ndarray):\n        img = Image.fromarray(img)\n\
    \    draw = ImageDraw.Draw(img)\n    draw.rectangle(bbox, outline='red')\n   \
    \ return img\n```"
  created_at: 2023-11-05 19:59:55+00:00
  edited: true
  hidden: false
  id: 6547f43b7e0f913218463ac2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-11-05T20:49:13.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.929786741733551
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: '<p>The bounding box tasks are very sensitive to input resolution, because
          the model was trained on screenshots with a height of <code>1080</code>
          and not fine-tuned on other content. The best way to use these features
          is to scale your input image so its height is close to <code>1080</code>.
          If the image is smaller, then padding to <code>1920x1080</code> works well.</p>

          <p>This is the strategy used in the demo for this task, these are the lines
          that rescale and pad so the input to the model is always <code>1920x1080</code>:
          <a href="https://huggingface.co/spaces/adept/fuyu-8b-demo/blob/main/app.py#L71-L72">https://huggingface.co/spaces/adept/fuyu-8b-demo/blob/main/app.py#L71-L72</a></p>

          '
        raw: 'The bounding box tasks are very sensitive to input resolution, because
          the model was trained on screenshots with a height of `1080` and not fine-tuned
          on other content. The best way to use these features is to scale your input
          image so its height is close to `1080`. If the image is smaller, then padding
          to `1920x1080` works well.


          This is the strategy used in the demo for this task, these are the lines
          that rescale and pad so the input to the model is always `1920x1080`: https://huggingface.co/spaces/adept/fuyu-8b-demo/blob/main/app.py#L71-L72

          '
        updatedAt: '2023-11-05T20:49:13.994Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - ludeksvoboda
        - cckevinn
        - kldsaid
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cckevinn
    id: 6547ffc9e70ffa3c0734985f
    type: comment
  author: pcuenq
  content: 'The bounding box tasks are very sensitive to input resolution, because
    the model was trained on screenshots with a height of `1080` and not fine-tuned
    on other content. The best way to use these features is to scale your input image
    so its height is close to `1080`. If the image is smaller, then padding to `1920x1080`
    works well.


    This is the strategy used in the demo for this task, these are the lines that
    rescale and pad so the input to the model is always `1920x1080`: https://huggingface.co/spaces/adept/fuyu-8b-demo/blob/main/app.py#L71-L72

    '
  created_at: 2023-11-05 20:49:13+00:00
  edited: false
  hidden: false
  id: 6547ffc9e70ffa3c0734985f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
      fullname: "Lud\u011Bk Svoboda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludeksvoboda
      type: user
    createdAt: '2023-11-05T23:11:25.000Z'
    data:
      edited: false
      editors:
      - ludeksvoboda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41032254695892334
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
          fullname: "Lud\u011Bk Svoboda"
          isHf: false
          isPro: false
          name: ludeksvoboda
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pcuenq\">@<span class=\"\
          underline\">pcuenq</span></a></span>\n\n\t</span></span> Oh, thank you for\
          \ the clarification!</p>\n"
        raw: '@pcuenq Oh, thank you for the clarification!'
        updatedAt: '2023-11-05T23:11:25.346Z'
      numEdits: 0
      reactions: []
    id: 6548211d9d940c4088239675
    type: comment
  author: ludeksvoboda
  content: '@pcuenq Oh, thank you for the clarification!'
  created_at: 2023-11-05 23:11:25+00:00
  edited: false
  hidden: false
  id: 6548211d9d940c4088239675
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: Released capabilities
