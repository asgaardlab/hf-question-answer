!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joanfihu
conflicting_files: null
created_at: 2023-10-18 21:02:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aec45d4d4aaf5e3bc683548f10a3a5d8.svg
      fullname: Joan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joanfihu
      type: user
    createdAt: '2023-10-18T22:02:35.000Z'
    data:
      edited: false
      editors:
      - joanfihu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9746183156967163
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aec45d4d4aaf5e3bc683548f10a3a5d8.svg
          fullname: Joan
          isHf: false
          isPro: false
          name: joanfihu
          type: user
        html: '<p>Hello there, good work with this model. What are the system requirements
          for this model? I don''t seem to be able to fit it in a 24GB GPU card</p>

          '
        raw: Hello there, good work with this model. What are the system requirements
          for this model? I don't seem to be able to fit it in a 24GB GPU card
        updatedAt: '2023-10-18T22:02:35.695Z'
      numEdits: 0
      reactions: []
    id: 653055fba6a6f2be6f1ec3bf
    type: comment
  author: joanfihu
  content: Hello there, good work with this model. What are the system requirements
    for this model? I don't seem to be able to fit it in a 24GB GPU card
  created_at: 2023-10-18 21:02:35+00:00
  edited: false
  hidden: false
  id: 653055fba6a6f2be6f1ec3bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-10-19T00:33:46.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.28055065870285034
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;joanfihu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/joanfihu\"\
          >@<span class=\"underline\">joanfihu</span></a></span>\n\n\t</span></span>,\
          \ it works in a 24GB GPU using <code>bfloat16</code> :) </p>\n"
        raw: 'Hi @joanfihu, it works in a 24GB GPU using `bfloat16` :) '
        updatedAt: '2023-10-19T00:33:46.911Z'
      numEdits: 0
      reactions: []
    id: 6530796a6072a8d5c7b697c8
    type: comment
  author: pcuenq
  content: 'Hi @joanfihu, it works in a 24GB GPU using `bfloat16` :) '
  created_at: 2023-10-18 23:33:46+00:00
  edited: false
  hidden: false
  id: 6530796a6072a8d5c7b697c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
      fullname: Lino Valdovinos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: latent-variable
      type: user
    createdAt: '2023-10-19T04:04:47.000Z'
    data:
      edited: false
      editors:
      - latent-variable
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5474391579627991
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
          fullname: Lino Valdovinos
          isHf: false
          isPro: false
          name: latent-variable
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pcuenq\">@<span class=\"\
          underline\">pcuenq</span></a></span>\n\n\t</span></span> Can you tell me\
          \ how you modified the code to use bfloat16?</p>\n"
        raw: '@pcuenq Can you tell me how you modified the code to use bfloat16?'
        updatedAt: '2023-10-19T04:04:47.899Z'
      numEdits: 0
      reactions: []
    id: 6530aadf8e4767ecfe29019d
    type: comment
  author: latent-variable
  content: '@pcuenq Can you tell me how you modified the code to use bfloat16?'
  created_at: 2023-10-19 03:04:47+00:00
  edited: false
  hidden: false
  id: 6530aadf8e4767ecfe29019d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
      fullname: Lino Valdovinos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: latent-variable
      type: user
    createdAt: '2023-10-19T04:11:23.000Z'
    data:
      edited: false
      editors:
      - latent-variable
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5551451444625854
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
          fullname: Lino Valdovinos
          isHf: false
          isPro: false
          name: latent-variable
          type: user
        html: '<p>For anyone else wondering this,<br>Just update the following line:</p>

          <p>model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map="cuda:0",
          torch_dtype=torch.float16)</p>

          '
        raw: "For anyone else wondering this, \nJust update the following line:\n\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=\"cuda:0\"\
          , torch_dtype=torch.float16)\n"
        updatedAt: '2023-10-19T04:11:23.993Z'
      numEdits: 0
      reactions:
      - count: 10
        reaction: "\U0001F44D"
        users:
        - pcuenq
        - initmethod
        - balisujohn
        - joanfihu
        - rexzen
        - Colderthanice
        - layoric
        - cys-mnl
        - Lotharian
        - whpyjnqd
    id: 6530ac6bf68c6e013d06e3bf
    type: comment
  author: latent-variable
  content: "For anyone else wondering this, \nJust update the following line:\n\n\
    model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=\"cuda:0\"\
    , torch_dtype=torch.float16)\n"
  created_at: 2023-10-19 03:11:23+00:00
  edited: false
  hidden: false
  id: 6530ac6bf68c6e013d06e3bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b55f6cf431881d56bc999a35d8e4cbf7.svg
      fullname: John Balis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: balisujohn
      type: user
    createdAt: '2023-10-21T07:53:03.000Z'
    data:
      edited: false
      editors:
      - balisujohn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9537113904953003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b55f6cf431881d56bc999a35d8e4cbf7.svg
          fullname: John Balis
          isHf: false
          isPro: false
          name: balisujohn
          type: user
        html: '<p>Is there a way to get this to work on CPU with under 25Gigs of ram?  when
          I tried to set it to float 16, get the following error:</p>

          <p>"RuntimeError: "addmm_impl_cpu_" not implemented for ''Half''"</p>

          <p>It seems like float16 is not supported for some operations on CPU, so
          low precision CPU mode might need some actual tweaks to the code. </p>

          <p>The reason I think this would be nice if possible is because many people
          don''t have more than 32 gigs of ram or a graphics card with 20+ gigs of
          vram. </p>

          '
        raw: "Is there a way to get this to work on CPU with under 25Gigs of ram?\
          \  when I tried to set it to float 16, get the following error:\n\n\"RuntimeError:\
          \ \"addmm_impl_cpu_\" not implemented for 'Half'\"\n\nIt seems like float16\
          \ is not supported for some operations on CPU, so low precision CPU mode\
          \ might need some actual tweaks to the code. \n\nThe reason I think this\
          \ would be nice if possible is because many people don't have more than\
          \ 32 gigs of ram or a graphics card with 20+ gigs of vram. "
        updatedAt: '2023-10-21T07:53:03.105Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Sysking
    id: 6533835fdc1c6f1ed7dfb363
    type: comment
  author: balisujohn
  content: "Is there a way to get this to work on CPU with under 25Gigs of ram?  when\
    \ I tried to set it to float 16, get the following error:\n\n\"RuntimeError: \"\
    addmm_impl_cpu_\" not implemented for 'Half'\"\n\nIt seems like float16 is not\
    \ supported for some operations on CPU, so low precision CPU mode might need some\
    \ actual tweaks to the code. \n\nThe reason I think this would be nice if possible\
    \ is because many people don't have more than 32 gigs of ram or a graphics card\
    \ with 20+ gigs of vram. "
  created_at: 2023-10-21 06:53:03+00:00
  edited: false
  hidden: false
  id: 6533835fdc1c6f1ed7dfb363
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-22T15:52:13.000Z'
    data:
      edited: false
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8991608619689941
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<blockquote>

          <p>model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map="cuda:0",
          torch_dtype=torch.float16)</p>

          </blockquote>

          <p>Maybe debatable, but have read that bfloat16 is recommended over float16
          because of its structure. So: torch_dtype=torch.bfloat16<br>I tested both
          and model is loading faster in bf16 (6s vs 10s for f16), but inference is
          faster with f16 (1.22 vs 1.70 for bf16). Can''t assess quality, but people
          saying bf16 is better.</p>

          '
        raw: '> model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map="cuda:0",
          torch_dtype=torch.float16)


          Maybe debatable, but have read that bfloat16 is recommended over float16
          because of its structure. So: torch_dtype=torch.bfloat16

          I tested both and model is loading faster in bf16 (6s vs 10s for f16), but
          inference is faster with f16 (1.22 vs 1.70 for bf16). Can''t assess quality,
          but people saying bf16 is better.'
        updatedAt: '2023-10-22T15:52:13.045Z'
      numEdits: 0
      reactions: []
    id: 6535452ddea545ecdaa8cbee
    type: comment
  author: Neman
  content: '> model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map="cuda:0",
    torch_dtype=torch.float16)


    Maybe debatable, but have read that bfloat16 is recommended over float16 because
    of its structure. So: torch_dtype=torch.bfloat16

    I tested both and model is loading faster in bf16 (6s vs 10s for f16), but inference
    is faster with f16 (1.22 vs 1.70 for bf16). Can''t assess quality, but people
    saying bf16 is better.'
  created_at: 2023-10-22 14:52:13+00:00
  edited: false
  hidden: false
  id: 6535452ddea545ecdaa8cbee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d479aad0ffa2cee9619972d5ecb0f4a1.svg
      fullname: Green
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Colderthanice
      type: user
    createdAt: '2023-10-23T01:38:52.000Z'
    data:
      edited: false
      editors:
      - Colderthanice
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8678642511367798
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d479aad0ffa2cee9619972d5ecb0f4a1.svg
          fullname: Green
          isHf: false
          isPro: false
          name: Colderthanice
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;latent-variable&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/latent-variable\"\
          >@<span class=\"underline\">latent-variable</span></a></span>\n\n\t</span></span><br>Thanks,\
          \ it worked!</p>\n"
        raw: "@latent-variable \nThanks, it worked!"
        updatedAt: '2023-10-23T01:38:52.248Z'
      numEdits: 0
      reactions: []
    id: 6535ceace983fb23fa64d0fc
    type: comment
  author: Colderthanice
  content: "@latent-variable \nThanks, it worked!"
  created_at: 2023-10-23 00:38:52+00:00
  edited: false
  hidden: false
  id: 6535ceace983fb23fa64d0fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a65eb593f7e383740d9d2/Kh6Tf2pvIRQi0Rrx1LiAL.png?w=200&h=200&f=face
      fullname: Darren Reid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: layoric
      type: user
    createdAt: '2023-10-23T11:33:12.000Z'
    data:
      edited: true
      editors:
      - layoric
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7090447545051575
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a65eb593f7e383740d9d2/Kh6Tf2pvIRQi0Rrx1LiAL.png?w=200&h=200&f=face
          fullname: Darren Reid
          isHf: false
          isPro: true
          name: layoric
          type: user
        html: "<p>I have found when using multiple GPUs, this model requires above\
          \ 32GB of VRAM. I have 3x A4000 16GBs (a weird setup I know), and only just\
          \ manage to fit within their limit when using a custom <code>device_map</code>.\
          \ I hit a few issues related to which layers were required to be on the\
          \ same GPU, so if it helps others, the following layers need to be listed\
          \ on the <code>device_map</code> on the same device:</p>\n<pre><code>device_map\
          \ = {\n    'language_model.model.embed_tokens.weight': 'cuda:0',\n    'language_model.lm_head.weight':'cuda:0',\n\
          \    'language_model.model.final_layernorm.bias':'cuda:0',\n    'language_model.model.final_layernorm.weight':'cuda:0',\n\
          \    'vision_embed_tokens.bias':'cuda:0',\n    'vision_embed_tokens.weight':'cuda:0',\n\
          \   # rest of the layers shared among devices..\n}\n</code></pre>\n<p>My\
          \ usage using <code>torch_dtype=torch.bfloat16</code> used more like 40GB,\
          \ though I might be doing something else wrong. Eg my code is:</p>\n<pre><code>model_id\
          \ = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(model_id,torch_dtype=torch.bfloat16)\n\
          model = FuyuForCausalLM.from_pretrained(model_id, device_map=device_map)\
          \ # torch_dtype=torch.bfloat16 should be here\n</code></pre>\n<p><del>Any\
          \ insights would be appreciated</del>.  Above I used the <code>torch_dtype=torch.bfloat16</code>\
          \ on the wrong line \U0001F926\u200D\u2642\uFE0F, never mind me.</p>\n<p>Thanks\
          \ for releasing such an interesting model! \U0001F44F</p>\n"
        raw: "I have found when using multiple GPUs, this model requires above 32GB\
          \ of VRAM. I have 3x A4000 16GBs (a weird setup I know), and only just manage\
          \ to fit within their limit when using a custom `device_map`. I hit a few\
          \ issues related to which layers were required to be on the same GPU, so\
          \ if it helps others, the following layers need to be listed on the `device_map`\
          \ on the same device:\n\n```\ndevice_map = {\n    'language_model.model.embed_tokens.weight':\
          \ 'cuda:0',\n    'language_model.lm_head.weight':'cuda:0',\n    'language_model.model.final_layernorm.bias':'cuda:0',\n\
          \    'language_model.model.final_layernorm.weight':'cuda:0',\n    'vision_embed_tokens.bias':'cuda:0',\n\
          \    'vision_embed_tokens.weight':'cuda:0',\n   # rest of the layers shared\
          \ among devices..\n}\n```\n\nMy usage using `torch_dtype=torch.bfloat16`\
          \ used more like 40GB, though I might be doing something else wrong. Eg\
          \ my code is:\n\n```\nmodel_id = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(model_id,torch_dtype=torch.bfloat16)\n\
          model = FuyuForCausalLM.from_pretrained(model_id, device_map=device_map)\
          \ # torch_dtype=torch.bfloat16 should be here\n```\n\n~~Any insights would\
          \ be appreciated~~.  Above I used the `torch_dtype=torch.bfloat16` on the\
          \ wrong line \U0001F926\u200D\u2642\uFE0F, never mind me.\n\nThanks for\
          \ releasing such an interesting model! \U0001F44F"
        updatedAt: '2023-10-23T22:44:14.128Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurZ
    id: 653659f83da0ff3c70d0293e
    type: comment
  author: layoric
  content: "I have found when using multiple GPUs, this model requires above 32GB\
    \ of VRAM. I have 3x A4000 16GBs (a weird setup I know), and only just manage\
    \ to fit within their limit when using a custom `device_map`. I hit a few issues\
    \ related to which layers were required to be on the same GPU, so if it helps\
    \ others, the following layers need to be listed on the `device_map` on the same\
    \ device:\n\n```\ndevice_map = {\n    'language_model.model.embed_tokens.weight':\
    \ 'cuda:0',\n    'language_model.lm_head.weight':'cuda:0',\n    'language_model.model.final_layernorm.bias':'cuda:0',\n\
    \    'language_model.model.final_layernorm.weight':'cuda:0',\n    'vision_embed_tokens.bias':'cuda:0',\n\
    \    'vision_embed_tokens.weight':'cuda:0',\n   # rest of the layers shared among\
    \ devices..\n}\n```\n\nMy usage using `torch_dtype=torch.bfloat16` used more like\
    \ 40GB, though I might be doing something else wrong. Eg my code is:\n\n```\n\
    model_id = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(model_id,torch_dtype=torch.bfloat16)\n\
    model = FuyuForCausalLM.from_pretrained(model_id, device_map=device_map) # torch_dtype=torch.bfloat16\
    \ should be here\n```\n\n~~Any insights would be appreciated~~.  Above I used\
    \ the `torch_dtype=torch.bfloat16` on the wrong line \U0001F926\u200D\u2642\uFE0F\
    , never mind me.\n\nThanks for releasing such an interesting model! \U0001F44F"
  created_at: 2023-10-23 10:33:12+00:00
  edited: true
  hidden: false
  id: 653659f83da0ff3c70d0293e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-10-25T13:39:29.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9249913096427917
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Thanks for sharing you experiences! </p>

          '
        raw: 'Thanks for sharing you experiences! '
        updatedAt: '2023-10-25T13:39:29.195Z'
      numEdits: 0
      reactions: []
    id: 65391a91ee5888edef6e2cfb
    type: comment
  author: ArthurZ
  content: 'Thanks for sharing you experiences! '
  created_at: 2023-10-25 12:39:29+00:00
  edited: false
  hidden: false
  id: 65391a91ee5888edef6e2cfb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
      fullname: "Lud\u011Bk Svoboda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludeksvoboda
      type: user
    createdAt: '2023-11-05T19:57:43.000Z'
    data:
      edited: true
      editors:
      - ludeksvoboda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9388378858566284
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59b93a634f3f07258e69481f432eba7.svg
          fullname: "Lud\u011Bk Svoboda"
          isHf: false
          isPro: false
          name: ludeksvoboda
          type: user
        html: '<p>I have run it on two rtx3060, each has 12GB. Needs custom device
          map and FP16. Can process only images in ~500x1000 maximum resolution though.</p>

          '
        raw: I have run it on two rtx3060, each has 12GB. Needs custom device map
          and FP16. Can process only images in ~500x1000 maximum resolution though.
        updatedAt: '2023-11-05T19:58:20.214Z'
      numEdits: 1
      reactions: []
    id: 6547f3b733972a012f82da40
    type: comment
  author: ludeksvoboda
  content: I have run it on two rtx3060, each has 12GB. Needs custom device map and
    FP16. Can process only images in ~500x1000 maximum resolution though.
  created_at: 2023-11-05 19:57:43+00:00
  edited: true
  hidden: false
  id: 6547f3b733972a012f82da40
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: What are the memory requirements for running the model?
