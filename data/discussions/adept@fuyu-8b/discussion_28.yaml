!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YuntaoChen
conflicting_files: null
created_at: 2023-10-24 05:49:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/857b0b4d115aa5ab2f143e60b0e4edc6.svg
      fullname: Yuntao Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YuntaoChen
      type: user
    createdAt: '2023-10-24T06:49:01.000Z'
    data:
      edited: false
      editors:
      - YuntaoChen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7220523953437805
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/857b0b4d115aa5ab2f143e60b0e4edc6.svg
          fullname: Yuntao Chen
          isHf: false
          isPro: false
          name: YuntaoChen
          type: user
        html: "<p>My inference code is list below.<br>Fuyu-8b gives reasonable results\
          \ for input text &amp; image prompts examples on the huggingface page, but\
          \ outputs non-sense for anything else.<br>See  \"life expectancy case v2\"\
          \ and \"life expectancy cas v3\" for results of just slightly modifying\
          \ the text prompts.<br>See \"big ben coco caption case\" and other cases\
          \ below for text prompts and image prompts used by the offical blog.</p>\n\
          <p>I'm wondering if this is the expected behavior of fuyu-8b or I made some\
          \ mistake in the inference code part.</p>\n<pre><code>import torch\nfrom\
          \ transformers import FuyuProcessor, FuyuForCausalLM\nfrom PIL import Image\n\
          \n# load model and processor\nmodel_id = \"adept/fuyu-8b\"\nprocessor =\
          \ FuyuProcessor.from_pretrained(model_id)\nmodel = FuyuForCausalLM.from_pretrained(model_id,\
          \ device_map=\"cuda:0\", torch_dtype=torch.float16)\n\n\ndef inference_lmm(text_prompt,\
          \ image_path):\n    image = Image.open(image_path)\n\n    inputs = processor(text=text_prompt,\
          \ images=image, return_tensors=\"pt\")\n    for k, v in inputs.items():\n\
          \        inputs[k] = v.to(\"cuda:0\")\n\n    # autoregressively generate\
          \ text\n    generation_output = model.generate(**inputs, max_new_tokens=50)\n\
          \    generation_text = processor.batch_decode(generation_output[:, -50:],\
          \ skip_special_tokens=True)\n    print(f\"input text: {text_prompt}\")\n\
          \    print(f\"generated text: {generation_text}\")\n\n\n# coco caption case\n\
          text_prompt = \"Generate a coco-style caption.\\n\"\nimage_path = \"bus.png\"\
          \ninference_lmm(text_prompt, image_path)\n\n# bus color case\ntext_prompt\
          \ = \"What color is the bus?\\n\"\nimage_path = \"bus.png\"\ninference_lmm(text_prompt,\
          \ image_path)\n\n# life expectancy case\ntext_prompt = \"What is the highest\
          \ life expectancy at birth of male?\\n\"\nimage_path = \"chart.png\"\ninference_lmm(text_prompt,\
          \ image_path)\n\n# life expectancy case v2\ntext_prompt = \"What is the\
          \ highest life expectancy at birth of female?\\n\"\nimage_path = \"chart.png\"\
          \ninference_lmm(text_prompt, image_path)\n\n# life expectancy case v3\n\
          text_prompt = \"What is the lowest life expectancy at birth of male?\\n\"\
          \nimage_path = \"chart.png\"\ninference_lmm(text_prompt, image_path)\n\n\
          # big ben coco caption case\ntext_prompt = \"Generate a coco-style caption.\\\
          n\"\nimage_path = \"big_ben.png\"\ninference_lmm(text_prompt, image_path)\n\
          \n# hbo case\ntext_prompt = \"Aidan Gillen acted in how many series?\"\n\
          image_path = \"hbo.png\"\ninference_lmm(text_prompt, image_path)\n\n# twitter_graph\
          \ case\ntext_prompt = \"Find missing data of the sequence 24, _ ,32, 33,\
          \ 42?\"\nimage_path = \"twitter_graph.png\"\ninference_lmm(text_prompt,\
          \ image_path)\n\n# vacation_days case\ntext_prompt = \"What was the fair\
          \ amount of paid vacation days in the UK?\"\nimage_path = \"vacation_days.png\"\
          \ninference_lmm(text_prompt, image_path)\n\n# job case\ntext_prompt = \"\
          Which is the metro in California that has a good job Outlook?\"\nimage_path\
          \ = \"job.png\"\ninference_lmm(text_prompt, image_path)\n\n# pdf case\n\
          text_prompt = \"What was the pack spinner capacity?\"\nimage_path = \"pdf.png\"\
          \ninference_lmm(text_prompt, image_path)\n\n# leaf_shapes case\ntext_prompt\
          \ = \"What letter does a keel-shaped cross-section look like?\"\nimage_path\
          \ = \"leaf_shapes.png\"\ninference_lmm(text_prompt, image_path)\n\n# red_tree_vole\
          \ case\ntext_prompt = \"If in the food web shown in the diagram, Douglas\
          \ fir tree needles are absent, which organism would starve?\"\nimage_path\
          \ = \"red_tree_vole.png\"\ninference_lmm(text_prompt, image_path)\n</code></pre>\n\
          <pre><code>input text: Generate a coco-style caption.\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ Generate a coco-style caption.\\n\\x04 A bus parked on the side of a road.']\n\
          \ninput text: What color is the bus?\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ What color is the bus?\\n\\x04 The bus is blue.\\n']\n\ninput text: What\
          \ is the highest life expectancy at birth of male?\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ What is the highest life expectancy at birth of male?\\n\\x04 The life\
          \ expectancy at birth of males in 2018 is 80.7.\\n']\n\ninput text: What\
          \ is the highest life expectancy at birth of female?\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ What is the highest life expectancy at birth of female?\\n\\x04 The life\
          \ expectancy at birth of female is 80.2.\\n']\n\ninput text: What is the\
          \ lowest life expectancy at birth of male?\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ What is the lowest life expectancy at birth of male?\\n\\x04 The life\
          \ expectancy at birth of males in 2018 is 80.7.\\n']\n\ninput text: Generate\
          \ a coco-style caption.\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ Generate a coco-style caption.\\n\\x04 The city is lit up at night with\
          \ traffic and lights.']\n\ninput text: Aidan Gillen acted in how many series?\n\
          generated text: ['|NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ Aidan Gillen acted in how many series?\\x04 3']\n\ninput text: Find missing\
          \ data of the sequence 24, _ ,32, 33, 42?\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ Find missing data of the sequence 24, _ ,32, 33, 42?\\x04 32, 33, 42']\n\
          \ninput text: What was the fair amount of paid vacation days in the UK?\n\
          generated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ What was the fair amount of paid vacation days in the UK?\\x04 How many\
          \ days did the UK spend in paid vacation in 2019?']\n\ninput text: Which\
          \ is the metro in California that has a good job Outlook?\ngenerated text:\
          \ ['\"PCO Configurations\" \"Time Savers, New Orleans - 1977\" \"Results\"\
          \ \"A Single PCD with a capacity of 104 packs was used in the test.\" \"\
          Results\" \"Sales of brand style']\n\ninput text: What was the pack spinner\
          \ capacity?\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ What was the pack spinner capacity?\\x04 Each display was loaded with\
          \ various styles of vantage, more and/or NOW.']\n\ninput text: What letter\
          \ does a keel-shaped cross-section look like?\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|&lt;s&gt;\
          \ What letter does a keel-shaped cross-section look like?\\x04 B']\n\ninput\
          \ text: If in the food web shown in the diagram, Douglas fir tree needles\
          \ are absent, which organism would starve?\ngenerated text: ['Black-tailed\
          \ Deer.\\n\\nThe food web in the image shows deer, carpenter ants, fern,\
          \ lichen, cougars, and mountain beaver as food sources. If the deer were\
          \ to die, the carpenter ants would']\n</code></pre>\n"
        raw: "My inference code is list below. \r\nFuyu-8b gives reasonable results\
          \ for input text & image prompts examples on the huggingface page, but outputs\
          \ non-sense for anything else. \r\nSee  \"life expectancy case v2\" and\
          \ \"life expectancy cas v3\" for results of just slightly modifying the\
          \ text prompts. \r\nSee \"big ben coco caption case\" and other cases below\
          \ for text prompts and image prompts used by the offical blog.\r\n\r\nI'm\
          \ wondering if this is the expected behavior of fuyu-8b or I made some mistake\
          \ in the inference code part.\r\n\r\n```\r\nimport torch\r\nfrom transformers\
          \ import FuyuProcessor, FuyuForCausalLM\r\nfrom PIL import Image\r\n\r\n\
          # load model and processor\r\nmodel_id = \"adept/fuyu-8b\"\r\nprocessor\
          \ = FuyuProcessor.from_pretrained(model_id)\r\nmodel = FuyuForCausalLM.from_pretrained(model_id,\
          \ device_map=\"cuda:0\", torch_dtype=torch.float16)\r\n\r\n\r\ndef inference_lmm(text_prompt,\
          \ image_path):\r\n    image = Image.open(image_path)\r\n\r\n    inputs =\
          \ processor(text=text_prompt, images=image, return_tensors=\"pt\")\r\n \
          \   for k, v in inputs.items():\r\n        inputs[k] = v.to(\"cuda:0\")\r\
          \n\r\n    # autoregressively generate text\r\n    generation_output = model.generate(**inputs,\
          \ max_new_tokens=50)\r\n    generation_text = processor.batch_decode(generation_output[:,\
          \ -50:], skip_special_tokens=True)\r\n    print(f\"input text: {text_prompt}\"\
          )\r\n    print(f\"generated text: {generation_text}\")\r\n\r\n\r\n# coco\
          \ caption case\r\ntext_prompt = \"Generate a coco-style caption.\\n\"\r\n\
          image_path = \"bus.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\
          \n# bus color case\r\ntext_prompt = \"What color is the bus?\\n\"\r\nimage_path\
          \ = \"bus.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\n# life\
          \ expectancy case\r\ntext_prompt = \"What is the highest life expectancy\
          \ at birth of male?\\n\"\r\nimage_path = \"chart.png\"\r\ninference_lmm(text_prompt,\
          \ image_path)\r\n\r\n# life expectancy case v2\r\ntext_prompt = \"What is\
          \ the highest life expectancy at birth of female?\\n\"\r\nimage_path = \"\
          chart.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\n# life expectancy\
          \ case v3\r\ntext_prompt = \"What is the lowest life expectancy at birth\
          \ of male?\\n\"\r\nimage_path = \"chart.png\"\r\ninference_lmm(text_prompt,\
          \ image_path)\r\n\r\n# big ben coco caption case\r\ntext_prompt = \"Generate\
          \ a coco-style caption.\\n\"\r\nimage_path = \"big_ben.png\"\r\ninference_lmm(text_prompt,\
          \ image_path)\r\n\r\n# hbo case\r\ntext_prompt = \"Aidan Gillen acted in\
          \ how many series?\"\r\nimage_path = \"hbo.png\"\r\ninference_lmm(text_prompt,\
          \ image_path)\r\n\r\n# twitter_graph case\r\ntext_prompt = \"Find missing\
          \ data of the sequence 24, _ ,32, 33, 42?\"\r\nimage_path = \"twitter_graph.png\"\
          \r\ninference_lmm(text_prompt, image_path)\r\n\r\n# vacation_days case\r\
          \ntext_prompt = \"What was the fair amount of paid vacation days in the\
          \ UK?\"\r\nimage_path = \"vacation_days.png\"\r\ninference_lmm(text_prompt,\
          \ image_path)\r\n\r\n# job case\r\ntext_prompt = \"Which is the metro in\
          \ California that has a good job Outlook?\"\r\nimage_path = \"job.png\"\r\
          \ninference_lmm(text_prompt, image_path)\r\n\r\n# pdf case\r\ntext_prompt\
          \ = \"What was the pack spinner capacity?\"\r\nimage_path = \"pdf.png\"\r\
          \ninference_lmm(text_prompt, image_path)\r\n\r\n# leaf_shapes case\r\ntext_prompt\
          \ = \"What letter does a keel-shaped cross-section look like?\"\r\nimage_path\
          \ = \"leaf_shapes.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\n\
          # red_tree_vole case\r\ntext_prompt = \"If in the food web shown in the\
          \ diagram, Douglas fir tree needles are absent, which organism would starve?\"\
          \r\nimage_path = \"red_tree_vole.png\"\r\ninference_lmm(text_prompt, image_path)\r\
          \n```\r\n\r\n\r\n```\r\ninput text: Generate a coco-style caption.\r\ngenerated\
          \ text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ Generate a coco-style caption.\\n\\x04 A bus parked on the side of a road.']\r\
          \n\r\ninput text: What color is the bus?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ What color is the bus?\\n\\x04 The bus is blue.\\n']\r\n\r\ninput text:\
          \ What is the highest life expectancy at birth of male?\r\ngenerated text:\
          \ ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ What is the highest life expectancy at birth of male?\\n\\x04 The life\
          \ expectancy at birth of males in 2018 is 80.7.\\n']\r\n\r\ninput text:\
          \ What is the highest life expectancy at birth of female?\r\ngenerated text:\
          \ ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ What is the highest life expectancy at birth of female?\\n\\x04 The life\
          \ expectancy at birth of female is 80.2.\\n']\r\n\r\ninput text: What is\
          \ the lowest life expectancy at birth of male?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ What is the lowest life expectancy at birth of male?\\n\\x04 The life\
          \ expectancy at birth of males in 2018 is 80.7.\\n']\r\n\r\ninput text:\
          \ Generate a coco-style caption.\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ Generate a coco-style caption.\\n\\x04 The city is lit up at night with\
          \ traffic and lights.']\r\n\r\ninput text: Aidan Gillen acted in how many\
          \ series?\r\ngenerated text: ['|NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ Aidan Gillen acted in how many series?\\x04 3']\r\n\r\ninput text: Find\
          \ missing data of the sequence 24, _ ,32, 33, 42?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ Find missing data of the sequence 24, _ ,32, 33, 42?\\x04 32, 33, 42']\r\
          \n\r\ninput text: What was the fair amount of paid vacation days in the\
          \ UK?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ What was the fair amount of paid vacation days in the UK?\\x04 How many\
          \ days did the UK spend in paid vacation in 2019?']\r\n\r\ninput text: Which\
          \ is the metro in California that has a good job Outlook?\r\ngenerated text:\
          \ ['\"PCO Configurations\" \"Time Savers, New Orleans - 1977\" \"Results\"\
          \ \"A Single PCD with a capacity of 104 packs was used in the test.\" \"\
          Results\" \"Sales of brand style']\r\n\r\ninput text: What was the pack\
          \ spinner capacity?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ What was the pack spinner capacity?\\x04 Each display was loaded with\
          \ various styles of vantage, more and/or NOW.']\r\n\r\ninput text: What\
          \ letter does a keel-shaped cross-section look like?\r\ngenerated text:\
          \ ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
          \ What letter does a keel-shaped cross-section look like?\\x04 B']\r\n\r\
          \ninput text: If in the food web shown in the diagram, Douglas fir tree\
          \ needles are absent, which organism would starve?\r\ngenerated text: ['Black-tailed\
          \ Deer.\\n\\nThe food web in the image shows deer, carpenter ants, fern,\
          \ lichen, cougars, and mountain beaver as food sources. If the deer were\
          \ to die, the carpenter ants would']\r\n```"
        updatedAt: '2023-10-24T06:49:01.679Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - linxi
    id: 653768dd426054e7ae3365d1
    type: comment
  author: YuntaoChen
  content: "My inference code is list below. \r\nFuyu-8b gives reasonable results\
    \ for input text & image prompts examples on the huggingface page, but outputs\
    \ non-sense for anything else. \r\nSee  \"life expectancy case v2\" and \"life\
    \ expectancy cas v3\" for results of just slightly modifying the text prompts.\
    \ \r\nSee \"big ben coco caption case\" and other cases below for text prompts\
    \ and image prompts used by the offical blog.\r\n\r\nI'm wondering if this is\
    \ the expected behavior of fuyu-8b or I made some mistake in the inference code\
    \ part.\r\n\r\n```\r\nimport torch\r\nfrom transformers import FuyuProcessor,\
    \ FuyuForCausalLM\r\nfrom PIL import Image\r\n\r\n# load model and processor\r\
    \nmodel_id = \"adept/fuyu-8b\"\r\nprocessor = FuyuProcessor.from_pretrained(model_id)\r\
    \nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\", torch_dtype=torch.float16)\r\
    \n\r\n\r\ndef inference_lmm(text_prompt, image_path):\r\n    image = Image.open(image_path)\r\
    \n\r\n    inputs = processor(text=text_prompt, images=image, return_tensors=\"\
    pt\")\r\n    for k, v in inputs.items():\r\n        inputs[k] = v.to(\"cuda:0\"\
    )\r\n\r\n    # autoregressively generate text\r\n    generation_output = model.generate(**inputs,\
    \ max_new_tokens=50)\r\n    generation_text = processor.batch_decode(generation_output[:,\
    \ -50:], skip_special_tokens=True)\r\n    print(f\"input text: {text_prompt}\"\
    )\r\n    print(f\"generated text: {generation_text}\")\r\n\r\n\r\n# coco caption\
    \ case\r\ntext_prompt = \"Generate a coco-style caption.\\n\"\r\nimage_path =\
    \ \"bus.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\n# bus color case\r\
    \ntext_prompt = \"What color is the bus?\\n\"\r\nimage_path = \"bus.png\"\r\n\
    inference_lmm(text_prompt, image_path)\r\n\r\n# life expectancy case\r\ntext_prompt\
    \ = \"What is the highest life expectancy at birth of male?\\n\"\r\nimage_path\
    \ = \"chart.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\n# life expectancy\
    \ case v2\r\ntext_prompt = \"What is the highest life expectancy at birth of female?\\\
    n\"\r\nimage_path = \"chart.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\
    \r\n# life expectancy case v3\r\ntext_prompt = \"What is the lowest life expectancy\
    \ at birth of male?\\n\"\r\nimage_path = \"chart.png\"\r\ninference_lmm(text_prompt,\
    \ image_path)\r\n\r\n# big ben coco caption case\r\ntext_prompt = \"Generate a\
    \ coco-style caption.\\n\"\r\nimage_path = \"big_ben.png\"\r\ninference_lmm(text_prompt,\
    \ image_path)\r\n\r\n# hbo case\r\ntext_prompt = \"Aidan Gillen acted in how many\
    \ series?\"\r\nimage_path = \"hbo.png\"\r\ninference_lmm(text_prompt, image_path)\r\
    \n\r\n# twitter_graph case\r\ntext_prompt = \"Find missing data of the sequence\
    \ 24, _ ,32, 33, 42?\"\r\nimage_path = \"twitter_graph.png\"\r\ninference_lmm(text_prompt,\
    \ image_path)\r\n\r\n# vacation_days case\r\ntext_prompt = \"What was the fair\
    \ amount of paid vacation days in the UK?\"\r\nimage_path = \"vacation_days.png\"\
    \r\ninference_lmm(text_prompt, image_path)\r\n\r\n# job case\r\ntext_prompt =\
    \ \"Which is the metro in California that has a good job Outlook?\"\r\nimage_path\
    \ = \"job.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\n# pdf case\r\n\
    text_prompt = \"What was the pack spinner capacity?\"\r\nimage_path = \"pdf.png\"\
    \r\ninference_lmm(text_prompt, image_path)\r\n\r\n# leaf_shapes case\r\ntext_prompt\
    \ = \"What letter does a keel-shaped cross-section look like?\"\r\nimage_path\
    \ = \"leaf_shapes.png\"\r\ninference_lmm(text_prompt, image_path)\r\n\r\n# red_tree_vole\
    \ case\r\ntext_prompt = \"If in the food web shown in the diagram, Douglas fir\
    \ tree needles are absent, which organism would starve?\"\r\nimage_path = \"red_tree_vole.png\"\
    \r\ninference_lmm(text_prompt, image_path)\r\n```\r\n\r\n\r\n```\r\ninput text:\
    \ Generate a coco-style caption.\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ Generate a coco-style caption.\\n\\x04 A bus parked on the side of a road.']\r\
    \n\r\ninput text: What color is the bus?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ What color is the bus?\\n\\x04 The bus is blue.\\n']\r\n\r\ninput text: What\
    \ is the highest life expectancy at birth of male?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ What is the highest life expectancy at birth of male?\\n\\x04 The life expectancy\
    \ at birth of males in 2018 is 80.7.\\n']\r\n\r\ninput text: What is the highest\
    \ life expectancy at birth of female?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ What is the highest life expectancy at birth of female?\\n\\x04 The life expectancy\
    \ at birth of female is 80.2.\\n']\r\n\r\ninput text: What is the lowest life\
    \ expectancy at birth of male?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ What is the lowest life expectancy at birth of male?\\n\\x04 The life expectancy\
    \ at birth of males in 2018 is 80.7.\\n']\r\n\r\ninput text: Generate a coco-style\
    \ caption.\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ Generate a coco-style caption.\\n\\x04 The city is lit up at night with traffic\
    \ and lights.']\r\n\r\ninput text: Aidan Gillen acted in how many series?\r\n\
    generated text: ['|NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ Aidan Gillen acted in how many series?\\x04 3']\r\n\r\ninput text: Find missing\
    \ data of the sequence 24, _ ,32, 33, 42?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ Find missing data of the sequence 24, _ ,32, 33, 42?\\x04 32, 33, 42']\r\n\r\
    \ninput text: What was the fair amount of paid vacation days in the UK?\r\ngenerated\
    \ text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ What was the fair amount of paid vacation days in the UK?\\x04 How many days\
    \ did the UK spend in paid vacation in 2019?']\r\n\r\ninput text: Which is the\
    \ metro in California that has a good job Outlook?\r\ngenerated text: ['\"PCO\
    \ Configurations\" \"Time Savers, New Orleans - 1977\" \"Results\" \"A Single\
    \ PCD with a capacity of 104 packs was used in the test.\" \"Results\" \"Sales\
    \ of brand style']\r\n\r\ninput text: What was the pack spinner capacity?\r\n\
    generated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ What was the pack spinner capacity?\\x04 Each display was loaded with various\
    \ styles of vantage, more and/or NOW.']\r\n\r\ninput text: What letter does a\
    \ keel-shaped cross-section look like?\r\ngenerated text: ['|SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s>\
    \ What letter does a keel-shaped cross-section look like?\\x04 B']\r\n\r\ninput\
    \ text: If in the food web shown in the diagram, Douglas fir tree needles are\
    \ absent, which organism would starve?\r\ngenerated text: ['Black-tailed Deer.\\\
    n\\nThe food web in the image shows deer, carpenter ants, fern, lichen, cougars,\
    \ and mountain beaver as food sources. If the deer were to die, the carpenter\
    \ ants would']\r\n```"
  created_at: 2023-10-24 05:49:01+00:00
  edited: false
  hidden: false
  id: 653768dd426054e7ae3365d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-10-25T13:40:46.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9540139436721802
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: "<p>I think we are gonna release an update to make sure the begin of\
          \ sentence and end of sentence are properly added to the sequences, making\
          \ sure our results match the demos! cc <span data-props=\"{&quot;user&quot;:&quot;Molbap&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Molbap\"\
          >@<span class=\"underline\">Molbap</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'I think we are gonna release an update to make sure the begin of sentence
          and end of sentence are properly added to the sequences, making sure our
          results match the demos! cc @Molbap '
        updatedAt: '2023-10-25T13:40:46.452Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - changgeli
    id: 65391adef940c8a035923cb5
    type: comment
  author: ArthurZ
  content: 'I think we are gonna release an update to make sure the begin of sentence
    and end of sentence are properly added to the sequences, making sure our results
    match the demos! cc @Molbap '
  created_at: 2023-10-25 12:40:46+00:00
  edited: false
  hidden: false
  id: 65391adef940c8a035923cb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
      fullname: Pablo Montalvo
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Molbap
      type: user
    createdAt: '2023-11-03T08:59:14.000Z'
    data:
      edited: false
      editors:
      - Molbap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7901286482810974
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64789feb79f2d49511ed7db4/IzaIwiVgnkTZHrcLDQk0C.jpeg?w=200&h=200&f=face
          fullname: Pablo Montalvo
          isHf: true
          isPro: false
          name: Molbap
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;YuntaoChen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YuntaoChen\"\
          >@<span class=\"underline\">YuntaoChen</span></a></span>\n\n\t</span></span>\
          \ , thanks a lot for the thorough testing, it was useful! We have an update\
          \ as part of the latest <code>transformers</code> release:<br>First of all,\
          \ now the model supports batching so you can input a list of prompts and\
          \ a equal length list of images to the processor to get the model inputs\
          \ ready for generation! And second, the prompt structure does matter as\
          \ you have to prompt the model to answer in a VQA fashion. We've also updated\
          \ a few examples to better reflect the capabilities of the released model.\
          \ Try this out:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\">import</span>\
          \ Image\n<span class=\"hljs-keyword\">import</span> requests\n<span class=\"\
          hljs-keyword\">import</span> io\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> FuyuForCausalLM,\
          \ FuyuProcessor\n\npretrained_path = <span class=\"hljs-string\">\"adept/fuyu-8b\"\
          </span>\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\nmodel\
          \ = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=<span class=\"\
          hljs-string\">'auto'</span>)\n\n\ntext_prompt = <span class=\"hljs-string\"\
          >\"Answer the following DocVQA question based on the image. \\n Which is\
          \ the metro in California that has a good job Outlook?\"</span>\njobs_image_url\
          \ = <span class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png\"\
          </span>\njobs_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(jobs_image_url).content))\n\
          \nsecond_text_prompt = <span class=\"hljs-string\">\"Answer the following\
          \ DocVQA question based on the image. \\n What if the maximum male life\
          \ expectancy?\"</span>\nchart_image_url = <span class=\"hljs-string\">\"\
          https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png\"\
          </span>\nchart_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(chart_image_url).content))\n\
          \nthird_text_prompt = <span class=\"hljs-string\">\"Answer the following\
          \ DocVQA question based on the image. \\n What sport is that?\"</span>\n\
          skate_image_url = <span class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png\"\
          </span>\nskate_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(skate_image_url).content))\n\
          \nfourth_text_prompt = <span class=\"hljs-string\">\"Answer the following\
          \ DocVQA question based on the image. \\n What was the fair amount of paid\
          \ vacation days in the United Kingdom?\"</span>\nvacations_image_url = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png\"\
          </span>\nvacations_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(vacations_image_url).content)).convert(<span\
          \ class=\"hljs-string\">'RGB'</span>)\n\nfifth_text_prompt = <span class=\"\
          hljs-string\">\"Answer the following VQAv2 question based on the image:\
          \ What type of foods are in the image?\"</span>\nfish_image_url = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
          </span>\nfish_image_pil = Image.<span class=\"hljs-built_in\">open</span>(io.BytesIO(requests.get(fish_image_url).content))\n\
          \n\ntexts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt,\
          \ fifth_text_prompt]\nimages = [jobs_image_pil, chart_image_pil, skate_image_pil,\
          \ vacations_image_pil, fish_image_pil]\n\nmodel_inputs = processor(text=texts,\
          \ images=images).to(<span class=\"hljs-string\">'cuda'</span>)\n\n\nmodel_outputs\
          \ = processor.tokenizer.batch_decode(model.generate(\n    **model_inputs,\
          \ max_new_tokens=<span class=\"hljs-number\">10</span>)[:, -<span class=\"\
          hljs-number\">10</span>:], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>)\n\nground_truths = [<span class=\"hljs-string\">'Los Angeles'</span>,\
          \ <span class=\"hljs-string\">'80.7'</span>, <span class=\"hljs-string\"\
          >'skateboarding'</span>, <span class=\"hljs-string\">'28'</span>, <span\
          \ class=\"hljs-string\">'fish, carrots, lemon'</span>]\n\n\n<span class=\"\
          hljs-keyword\">for</span> ground_truth, model_output <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">zip</span>(ground_truths, model_outputs):\n\
          \    prediction = model_output.split(<span class=\"hljs-string\">'\\x04\
          \ '</span>, <span class=\"hljs-number\">1</span>)[<span class=\"hljs-number\"\
          >1</span>] <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\"\
          >'\\x04'</span> <span class=\"hljs-keyword\">in</span> model_output <span\
          \ class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">''</span>\n\
          \    <span class=\"hljs-keyword\">assert</span> (ground_truth == prediction)\n\
          </code></pre>\n"
        raw: "Hi @YuntaoChen , thanks a lot for the thorough testing, it was useful!\
          \ We have an update as part of the latest `transformers` release:\nFirst\
          \ of all, now the model supports batching so you can input a list of prompts\
          \ and a equal length list of images to the processor to get the model inputs\
          \ ready for generation! And second, the prompt structure does matter as\
          \ you have to prompt the model to answer in a VQA fashion. We've also updated\
          \ a few examples to better reflect the capabilities of the released model.\
          \ Try this out:\n\n```python\nfrom PIL import Image\nimport requests\nimport\
          \ io\nfrom transformers import FuyuForCausalLM, FuyuProcessor\n\npretrained_path\
          \ = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
          model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\
          \n\ntext_prompt = \"Answer the following DocVQA question based on the image.\
          \ \\n Which is the metro in California that has a good job Outlook?\"\n\
          jobs_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png\"\
          \njobs_image_pil = Image.open(io.BytesIO(requests.get(jobs_image_url).content))\n\
          \nsecond_text_prompt = \"Answer the following DocVQA question based on the\
          \ image. \\n What if the maximum male life expectancy?\"\nchart_image_url\
          \ = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png\"\
          \nchart_image_pil = Image.open(io.BytesIO(requests.get(chart_image_url).content))\n\
          \nthird_text_prompt = \"Answer the following DocVQA question based on the\
          \ image. \\n What sport is that?\"\nskate_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png\"\
          \nskate_image_pil = Image.open(io.BytesIO(requests.get(skate_image_url).content))\n\
          \nfourth_text_prompt = \"Answer the following DocVQA question based on the\
          \ image. \\n What was the fair amount of paid vacation days in the United\
          \ Kingdom?\"\nvacations_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png\"\
          \nvacations_image_pil = Image.open(io.BytesIO(requests.get(vacations_image_url).content)).convert('RGB')\n\
          \nfifth_text_prompt = \"Answer the following VQAv2 question based on the\
          \ image: What type of foods are in the image?\"\nfish_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
          \nfish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))\n\
          \n\ntexts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt,\
          \ fifth_text_prompt]\nimages = [jobs_image_pil, chart_image_pil, skate_image_pil,\
          \ vacations_image_pil, fish_image_pil]\n\nmodel_inputs = processor(text=texts,\
          \ images=images).to('cuda')\n\n\nmodel_outputs = processor.tokenizer.batch_decode(model.generate(\n\
          \    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)\n\
          \nground_truths = ['Los Angeles', '80.7', 'skateboarding', '28', 'fish,\
          \ carrots, lemon']\n\n\nfor ground_truth, model_output in zip(ground_truths,\
          \ model_outputs):\n    prediction = model_output.split('\\x04 ', 1)[1] if\
          \ '\\x04' in model_output else ''\n    assert (ground_truth == prediction)\n\
          \n```\n"
        updatedAt: '2023-11-03T08:59:14.168Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - whpyjnqd
    id: 6544b662f0d77ba425f100d0
    type: comment
  author: Molbap
  content: "Hi @YuntaoChen , thanks a lot for the thorough testing, it was useful!\
    \ We have an update as part of the latest `transformers` release:\nFirst of all,\
    \ now the model supports batching so you can input a list of prompts and a equal\
    \ length list of images to the processor to get the model inputs ready for generation!\
    \ And second, the prompt structure does matter as you have to prompt the model\
    \ to answer in a VQA fashion. We've also updated a few examples to better reflect\
    \ the capabilities of the released model. Try this out:\n\n```python\nfrom PIL\
    \ import Image\nimport requests\nimport io\nfrom transformers import FuyuForCausalLM,\
    \ FuyuProcessor\n\npretrained_path = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(pretrained_path)\n\
    model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')\n\n\
    \ntext_prompt = \"Answer the following DocVQA question based on the image. \\\
    n Which is the metro in California that has a good job Outlook?\"\njobs_image_url\
    \ = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png\"\
    \njobs_image_pil = Image.open(io.BytesIO(requests.get(jobs_image_url).content))\n\
    \nsecond_text_prompt = \"Answer the following DocVQA question based on the image.\
    \ \\n What if the maximum male life expectancy?\"\nchart_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png\"\
    \nchart_image_pil = Image.open(io.BytesIO(requests.get(chart_image_url).content))\n\
    \nthird_text_prompt = \"Answer the following DocVQA question based on the image.\
    \ \\n What sport is that?\"\nskate_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png\"\
    \nskate_image_pil = Image.open(io.BytesIO(requests.get(skate_image_url).content))\n\
    \nfourth_text_prompt = \"Answer the following DocVQA question based on the image.\
    \ \\n What was the fair amount of paid vacation days in the United Kingdom?\"\n\
    vacations_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png\"\
    \nvacations_image_pil = Image.open(io.BytesIO(requests.get(vacations_image_url).content)).convert('RGB')\n\
    \nfifth_text_prompt = \"Answer the following VQAv2 question based on the image:\
    \ What type of foods are in the image?\"\nfish_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png\"\
    \nfish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))\n\
    \n\ntexts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt,\
    \ fifth_text_prompt]\nimages = [jobs_image_pil, chart_image_pil, skate_image_pil,\
    \ vacations_image_pil, fish_image_pil]\n\nmodel_inputs = processor(text=texts,\
    \ images=images).to('cuda')\n\n\nmodel_outputs = processor.tokenizer.batch_decode(model.generate(\n\
    \    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)\n\n\
    ground_truths = ['Los Angeles', '80.7', 'skateboarding', '28', 'fish, carrots,\
    \ lemon']\n\n\nfor ground_truth, model_output in zip(ground_truths, model_outputs):\n\
    \    prediction = model_output.split('\\x04 ', 1)[1] if '\\x04' in model_output\
    \ else ''\n    assert (ground_truth == prediction)\n\n```\n"
  created_at: 2023-11-03 07:59:14+00:00
  edited: false
  hidden: false
  id: 6544b662f0d77ba425f100d0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: adept/fuyu-8b
repo_type: model
status: open
target_branch: null
title: The 8b model could get correct results for case showed on the offical blog
