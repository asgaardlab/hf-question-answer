!!python/object:huggingface_hub.community.DiscussionWithDetails
author: m9e
conflicting_files: null
created_at: 2023-12-31 17:49:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
      fullname: Matthew Wallace
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: m9e
      type: user
    createdAt: '2023-12-31T17:49:31.000Z'
    data:
      edited: false
      editors:
      - m9e
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8401420712471008
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
          fullname: Matthew Wallace
          isHf: false
          isPro: false
          name: m9e
          type: user
        html: '<p>Sorry if this is inane, but the tokenizer_config.json specifies
          an eos_token of "&lt; | end_of_sentence | &gt;" but the config.json has
          an eos_token_id 32021 which is "&lt;|EOT|&gt;"; this means you get garbage
          post-generation to fill the whole max tokens if the software you are using
          (in this case, text-generation-webui) reads the eos_token from the tokenizer
          config rather than the config.json. (And I found adding the eos_token string
          to the custom stopping strings is fine in the ui, and have been on a little
          journey to figure out what param if any of the openai-compatible params
          can replicate that, and it may be none; and I''m unsure if there''s a reason
          the tokenizer config is different or if that ended up being a side-effect
          - but I do notice the current full weight og model appears to have the EOT
          syntax</p>

          <p><a href="https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct/blob/main/tokenizer_config.json">https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct/blob/main/tokenizer_config.json</a></p>

          '
        raw: "Sorry if this is inane, but the tokenizer_config.json specifies an eos_token\
          \ of \"< | end_of_sentence | >\" but the config.json has an eos_token_id\
          \ 32021 which is \"<|EOT|>\"; this means you get garbage post-generation\
          \ to fill the whole max tokens if the software you are using (in this case,\
          \ text-generation-webui) reads the eos_token from the tokenizer config rather\
          \ than the config.json. (And I found adding the eos_token string to the\
          \ custom stopping strings is fine in the ui, and have been on a little journey\
          \ to figure out what param if any of the openai-compatible params can replicate\
          \ that, and it may be none; and I'm unsure if there's a reason the tokenizer\
          \ config is different or if that ended up being a side-effect - but I do\
          \ notice the current full weight og model appears to have the EOT syntax\r\
          \n\r\nhttps://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct/blob/main/tokenizer_config.json"
        updatedAt: '2023-12-31T17:49:31.404Z'
      numEdits: 0
      reactions: []
    id: 6591a9ab89f1ff0463a62650
    type: comment
  author: m9e
  content: "Sorry if this is inane, but the tokenizer_config.json specifies an eos_token\
    \ of \"< | end_of_sentence | >\" but the config.json has an eos_token_id 32021\
    \ which is \"<|EOT|>\"; this means you get garbage post-generation to fill the\
    \ whole max tokens if the software you are using (in this case, text-generation-webui)\
    \ reads the eos_token from the tokenizer config rather than the config.json. (And\
    \ I found adding the eos_token string to the custom stopping strings is fine in\
    \ the ui, and have been on a little journey to figure out what param if any of\
    \ the openai-compatible params can replicate that, and it may be none; and I'm\
    \ unsure if there's a reason the tokenizer config is different or if that ended\
    \ up being a side-effect - but I do notice the current full weight og model appears\
    \ to have the EOT syntax\r\n\r\nhttps://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct/blob/main/tokenizer_config.json"
  created_at: 2023-12-31 17:49:31+00:00
  edited: false
  hidden: false
  id: 6591a9ab89f1ff0463a62650
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-01T05:17:23.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8926138281822205
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Thanks, I just need to update configs to match the revised base.</p>

          '
        raw: Thanks, I just need to update configs to match the revised base.
        updatedAt: '2024-01-01T05:17:23.544Z'
      numEdits: 0
      reactions: []
    id: 65924ae37fe02354737b7224
    type: comment
  author: LoneStriker
  content: Thanks, I just need to update configs to match the revised base.
  created_at: 2024-01-01 05:17:23+00:00
  edited: false
  hidden: false
  id: 65924ae37fe02354737b7224
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/deepseek-coder-33b-instruct-6.0bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Error in tokenizer_config.json?
