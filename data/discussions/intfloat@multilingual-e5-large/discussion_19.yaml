!!python/object:huggingface_hub.community.DiscussionWithDetails
author: scancet
conflicting_files: null
created_at: 2023-11-03 10:57:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b232c889c7c521ce50e5137523491f13.svg
      fullname: "Sabri Can \xC7etinda\u011F"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: scancet
      type: user
    createdAt: '2023-11-03T11:57:22.000Z'
    data:
      edited: false
      editors:
      - scancet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6249399185180664
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b232c889c7c521ce50e5137523491f13.svg
          fullname: "Sabri Can \xC7etinda\u011F"
          isHf: false
          isPro: false
          name: scancet
          type: user
        html: "<p>Hello,<br>I used transformers library of intfloat/multilingual-e5-large\
          \ this model. I used the same code that is shared in its model card. I dockerized\
          \ it and started to use. It increases the memory in each inference and then\
          \ it exceeding my memory limit after a while.</p>\n<p>Here is my code,</p>\n\
          <p>import torch.nn.functional as F<br>from torch import Tensor, no_grad,\
          \ cuda, device<br>from transformers import AutoTokenizer, AutoModel<br>import\
          \ gc</p>\n<p>class Model():</p>\n<pre><code>def __init__(self, path='resources/intfloat_multilingual-e5-large'):\n\
          \    self.tokenizer = AutoTokenizer.from_pretrained(path)\n    self.model\
          \ = AutoModel.from_pretrained(path)\n    dvc = device('cpu')\n    self.model.to(dvc)\n\
          \    self.model.eval()\n\ndef average_pool(self, last_hidden_states: Tensor,\
          \ attention_mask: Tensor) -&gt; Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[...,\
          \ None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[...,\
          \ None]\n\ndef inference(self, texts): \n    with no_grad():\n        batch_dict\
          \ = self.tokenizer(texts, max_length=512, padding=True, truncation=True,\
          \ return_tensors='pt')\n        #print(batch_dict)\n        #print(self.model.config)\n\
          \        outputs = self.model(**batch_dict)\n        embeddings = self.average_pool(outputs.last_hidden_state,\
          \ batch_dict['attention_mask'])\n        del outputs\n        embeddings\
          \ = F.normalize(embeddings, p=2, dim=1)\n        embeddings = embeddings.numpy().tolist()\n\
          \        gc.collect()\n        cuda.empty_cache()\n    return embeddings\n\
          </code></pre>\n<p>model = Model()</p>\n<p>Here is my docker stats. Its initially\
          \ uses around 2.6gb ram in memory. But in each iteration it increases slowly.<br>Please\
          \ let me know if I can clear the cache of the memory or in any way I can\
          \ stop this memory leak.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6544dda62a13610acc1843b8/tx6ig4tlWMOoK46hjdu2J.png\"\
          ><img alt=\"Screen Shot 2023-11-03 at 14.54.52.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6544dda62a13610acc1843b8/tx6ig4tlWMOoK46hjdu2J.png\"\
          ></a></p>\n<p>Thanks</p>\n"
        raw: "Hello,\r\nI used transformers library of intfloat/multilingual-e5-large\
          \ this model. I used the same code that is shared in its model card. I dockerized\
          \ it and started to use. It increases the memory in each inference and then\
          \ it exceeding my memory limit after a while.\r\n\r\nHere is my code,\r\n\
          \r\n\r\nimport torch.nn.functional as F\r\nfrom torch import Tensor, no_grad,\
          \ cuda, device\r\nfrom transformers import AutoTokenizer, AutoModel\r\n\
          import gc\r\n\r\nclass Model():\r\n\r\n    def __init__(self, path='resources/intfloat_multilingual-e5-large'):\r\
          \n        self.tokenizer = AutoTokenizer.from_pretrained(path)\r\n     \
          \   self.model = AutoModel.from_pretrained(path)\r\n        dvc = device('cpu')\r\
          \n        self.model.to(dvc)\r\n        self.model.eval()\r\n\r\n    def\
          \ average_pool(self, last_hidden_states: Tensor, attention_mask: Tensor)\
          \ -> Tensor:\r\n        last_hidden = last_hidden_states.masked_fill(~attention_mask[...,\
          \ None].bool(), 0.0)\r\n        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[...,\
          \ None]\r\n\r\n    def inference(self, texts): \r\n        with no_grad():\r\
          \n            batch_dict = self.tokenizer(texts, max_length=512, padding=True,\
          \ truncation=True, return_tensors='pt')\r\n            #print(batch_dict)\r\
          \n            #print(self.model.config)\r\n            outputs = self.model(**batch_dict)\r\
          \n            embeddings = self.average_pool(outputs.last_hidden_state,\
          \ batch_dict['attention_mask'])\r\n            del outputs\r\n         \
          \   embeddings = F.normalize(embeddings, p=2, dim=1)\r\n            embeddings\
          \ = embeddings.numpy().tolist()\r\n            gc.collect()\r\n        \
          \    cuda.empty_cache()\r\n        return embeddings\r\n\r\nmodel = Model()\r\
          \n\r\nHere is my docker stats. Its initially uses around 2.6gb ram in memory.\
          \ But in each iteration it increases slowly. \r\nPlease let me know if I\
          \ can clear the cache of the memory or in any way I can stop this memory\
          \ leak.\r\n\r\n![Screen Shot 2023-11-03 at 14.54.52.png](https://cdn-uploads.huggingface.co/production/uploads/6544dda62a13610acc1843b8/tx6ig4tlWMOoK46hjdu2J.png)\r\
          \n\r\n\r\nThanks"
        updatedAt: '2023-11-03T11:57:22.293Z'
      numEdits: 0
      reactions: []
    id: 6544e022321f0393f44a7c4d
    type: comment
  author: scancet
  content: "Hello,\r\nI used transformers library of intfloat/multilingual-e5-large\
    \ this model. I used the same code that is shared in its model card. I dockerized\
    \ it and started to use. It increases the memory in each inference and then it\
    \ exceeding my memory limit after a while.\r\n\r\nHere is my code,\r\n\r\n\r\n\
    import torch.nn.functional as F\r\nfrom torch import Tensor, no_grad, cuda, device\r\
    \nfrom transformers import AutoTokenizer, AutoModel\r\nimport gc\r\n\r\nclass\
    \ Model():\r\n\r\n    def __init__(self, path='resources/intfloat_multilingual-e5-large'):\r\
    \n        self.tokenizer = AutoTokenizer.from_pretrained(path)\r\n        self.model\
    \ = AutoModel.from_pretrained(path)\r\n        dvc = device('cpu')\r\n       \
    \ self.model.to(dvc)\r\n        self.model.eval()\r\n\r\n    def average_pool(self,\
    \ last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\r\n        last_hidden\
    \ = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\r\n\
    \        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\r\
    \n\r\n    def inference(self, texts): \r\n        with no_grad():\r\n        \
    \    batch_dict = self.tokenizer(texts, max_length=512, padding=True, truncation=True,\
    \ return_tensors='pt')\r\n            #print(batch_dict)\r\n            #print(self.model.config)\r\
    \n            outputs = self.model(**batch_dict)\r\n            embeddings = self.average_pool(outputs.last_hidden_state,\
    \ batch_dict['attention_mask'])\r\n            del outputs\r\n            embeddings\
    \ = F.normalize(embeddings, p=2, dim=1)\r\n            embeddings = embeddings.numpy().tolist()\r\
    \n            gc.collect()\r\n            cuda.empty_cache()\r\n        return\
    \ embeddings\r\n\r\nmodel = Model()\r\n\r\nHere is my docker stats. Its initially\
    \ uses around 2.6gb ram in memory. But in each iteration it increases slowly.\
    \ \r\nPlease let me know if I can clear the cache of the memory or in any way\
    \ I can stop this memory leak.\r\n\r\n![Screen Shot 2023-11-03 at 14.54.52.png](https://cdn-uploads.huggingface.co/production/uploads/6544dda62a13610acc1843b8/tx6ig4tlWMOoK46hjdu2J.png)\r\
    \n\r\n\r\nThanks"
  created_at: 2023-11-03 10:57:22+00:00
  edited: false
  hidden: false
  id: 6544e022321f0393f44a7c4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2023-11-04T02:42:38.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9161053895950317
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: '<p>This looks strange, python and pytorch should do GC automatically.
          Is it possible that you store too many embedding vectors that cause the
          OOM issue?</p>

          '
        raw: This looks strange, python and pytorch should do GC automatically. Is
          it possible that you store too many embedding vectors that cause the OOM
          issue?
        updatedAt: '2023-11-04T02:42:38.413Z'
      numEdits: 0
      reactions: []
    id: 6545af9e08568852401fde26
    type: comment
  author: intfloat
  content: This looks strange, python and pytorch should do GC automatically. Is it
    possible that you store too many embedding vectors that cause the OOM issue?
  created_at: 2023-11-04 01:42:38+00:00
  edited: false
  hidden: false
  id: 6545af9e08568852401fde26
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: intfloat/multilingual-e5-large
repo_type: model
status: open
target_branch: null
title: Memory leak(memory increasing slowly in each inference in cpu)
