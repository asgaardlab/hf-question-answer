!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ziizu
conflicting_files: null
created_at: 2023-11-19 23:08:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
      fullname: Nason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ziizu
      type: user
    createdAt: '2023-11-19T23:08:53.000Z'
    data:
      edited: false
      editors:
      - Ziizu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9321264028549194
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
          fullname: Nason
          isHf: false
          isPro: false
          name: Ziizu
          type: user
        html: '<p>Hello there,</p>

          <p>Firstly, I want to commend you on your outstanding benchmark scores and
          the quality of information you generously share.</p>

          <p>I am reaching out seeking guidance on the most suitable method for fine-tuning.
          Before I embark on the fine-tuning process myself, I would appreciate clarification
          and guidance. I am more than willing to share my results with you in exchange
          for feedback.</p>

          <p>My specific goal is to further fine-tune an e5 model to enhance its capabilities
          in the Dutch language. I''m working on an RAG system for a forum with over
          200k comments across a multitude of topics. I have the capacity to generate
          weakly supervised datasets from from Dutch news articles, papers, and forums
          Additionally, I can obtain supervised datasets such as Dutch Squad (<a rel="nofollow"
          href="https://gitlab.com/niels.rouws/dutch-squad-v2.0">https://gitlab.com/niels.rouws/dutch-squad-v2.0</a>)
          among others.</p>

          <p>My specific aim is to train an embedding model to return relevant documents
          chunks given a query or unlabeled sub-topic. I have the following questions:</p>

          <ol>

          <li>How should I go through the fine-tuning process, do you recommend that
          I go through pre-training + fine-tuning or just do fine-tuning?</li>

          <li>If only fine-tuning is required then section 4.2 in your paper, mentions
          that the MS-MARCO and NQ formats would be ideal formats to convert my corpus
          into, is my understanding correct and are there any other considerations
          or factors I should take into account when building my tuning set.</li>

          <li>Which model do you recommend I  start with? this multi-lingual model
          or the unsupervised bases?</li>

          </ol>

          '
        raw: "Hello there,\r\n\r\nFirstly, I want to commend you on your outstanding\
          \ benchmark scores and the quality of information you generously share.\r\
          \n\r\nI am reaching out seeking guidance on the most suitable method for\
          \ fine-tuning. Before I embark on the fine-tuning process myself, I would\
          \ appreciate clarification and guidance. I am more than willing to share\
          \ my results with you in exchange for feedback.\r\n\r\nMy specific goal\
          \ is to further fine-tune an e5 model to enhance its capabilities in the\
          \ Dutch language. I'm working on an RAG system for a forum with over 200k\
          \ comments across a multitude of topics. I have the capacity to generate\
          \ weakly supervised datasets from from Dutch news articles, papers, and\
          \ forums Additionally, I can obtain supervised datasets such as Dutch Squad\
          \ (https://gitlab.com/niels.rouws/dutch-squad-v2.0) among others.\r\n\r\n\
          My specific aim is to train an embedding model to return relevant documents\
          \ chunks given a query or unlabeled sub-topic. I have the following questions:\r\
          \n\r\n1) How should I go through the fine-tuning process, do you recommend\
          \ that I go through pre-training + fine-tuning or just do fine-tuning?\r\
          \n2) If only fine-tuning is required then section 4.2 in your paper, mentions\
          \ that the MS-MARCO and NQ formats would be ideal formats to convert my\
          \ corpus into, is my understanding correct and are there any other considerations\
          \ or factors I should take into account when building my tuning set.\r\n\
          3) Which model do you recommend I  start with? this multi-lingual model\
          \ or the unsupervised bases? \r\n\r\n\r\n\r\n\r\n\r\n\r\n \r\n\r\n"
        updatedAt: '2023-11-19T23:08:53.011Z'
      numEdits: 0
      reactions: []
    id: 655a9585deee83130a3fd4eb
    type: comment
  author: Ziizu
  content: "Hello there,\r\n\r\nFirstly, I want to commend you on your outstanding\
    \ benchmark scores and the quality of information you generously share.\r\n\r\n\
    I am reaching out seeking guidance on the most suitable method for fine-tuning.\
    \ Before I embark on the fine-tuning process myself, I would appreciate clarification\
    \ and guidance. I am more than willing to share my results with you in exchange\
    \ for feedback.\r\n\r\nMy specific goal is to further fine-tune an e5 model to\
    \ enhance its capabilities in the Dutch language. I'm working on an RAG system\
    \ for a forum with over 200k comments across a multitude of topics. I have the\
    \ capacity to generate weakly supervised datasets from from Dutch news articles,\
    \ papers, and forums Additionally, I can obtain supervised datasets such as Dutch\
    \ Squad (https://gitlab.com/niels.rouws/dutch-squad-v2.0) among others.\r\n\r\n\
    My specific aim is to train an embedding model to return relevant documents chunks\
    \ given a query or unlabeled sub-topic. I have the following questions:\r\n\r\n\
    1) How should I go through the fine-tuning process, do you recommend that I go\
    \ through pre-training + fine-tuning or just do fine-tuning?\r\n2) If only fine-tuning\
    \ is required then section 4.2 in your paper, mentions that the MS-MARCO and NQ\
    \ formats would be ideal formats to convert my corpus into, is my understanding\
    \ correct and are there any other considerations or factors I should take into\
    \ account when building my tuning set.\r\n3) Which model do you recommend I  start\
    \ with? this multi-lingual model or the unsupervised bases? \r\n\r\n\r\n\r\n\r\
    \n\r\n\r\n \r\n\r\n"
  created_at: 2023-11-19 23:08:53+00:00
  edited: false
  hidden: false
  id: 655a9585deee83130a3fd4eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2023-11-20T04:12:45.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9104340672492981
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Ziizu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Ziizu\">@<span class=\"\
          underline\">Ziizu</span></a></span>\n\n\t</span></span> ,</p>\n<p>Thanks\
          \ for the interest in this model.</p>\n<p>About your questions:</p>\n<ol>\n\
          <li><p>How should I go through the fine-tuning process, do you recommend\
          \ that I go through pre-training + fine-tuning or just do fine-tuning?<br>The\
          \ <code>multilingual-e5-*</code> models have already gone through extensive\
          \ multilingual pre-training. Just doing fine-tuning should get you a good\
          \ performance. Going through pre-training + fine-tuning involves more complicated\
          \ pipelines, and the improvements are likely marginal.</p>\n</li>\n<li><p>If\
          \ only fine-tuning is required then section 4.2 in your paper, mentions\
          \ that the MS-MARCO and NQ formats would be ideal formats to convert my\
          \ corpus into, is my understanding correct and are there any other considerations\
          \ or factors I should take into account when building my tuning set.<br>Yes,\
          \ your understanding is correct. You need to prepare the query, positive\
          \ documents, and hard negative documents for training.</p>\n</li>\n<li><p>Which\
          \ model do you recommend I start with? this multi-lingual model or the unsupervised\
          \ bases?<br>Do not use the unsupervised models, they are English-only. Please\
          \ use <code>multilingual-e5-*</code> models, choose the size (small / base\
          \ / large) based on your needs.</p>\n</li>\n</ol>\n<p>Best,<br>Liang</p>\n"
        raw: 'Hi @Ziizu ,


          Thanks for the interest in this model.


          About your questions:

          1. How should I go through the fine-tuning process, do you recommend that
          I go through pre-training + fine-tuning or just do fine-tuning?

          The `multilingual-e5-*` models have already gone through extensive multilingual
          pre-training. Just doing fine-tuning should get you a good performance.
          Going through pre-training + fine-tuning involves more complicated pipelines,
          and the improvements are likely marginal.


          2. If only fine-tuning is required then section 4.2 in your paper, mentions
          that the MS-MARCO and NQ formats would be ideal formats to convert my corpus
          into, is my understanding correct and are there any other considerations
          or factors I should take into account when building my tuning set.

          Yes, your understanding is correct. You need to prepare the query, positive
          documents, and hard negative documents for training.


          3. Which model do you recommend I start with? this multi-lingual model or
          the unsupervised bases?

          Do not use the unsupervised models, they are English-only. Please use `multilingual-e5-*`
          models, choose the size (small / base / large) based on your needs.


          Best,

          Liang'
        updatedAt: '2023-11-20T04:12:45.714Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Ziizu
    id: 655adcbd30ad83ad6b75f8e4
    type: comment
  author: intfloat
  content: 'Hi @Ziizu ,


    Thanks for the interest in this model.


    About your questions:

    1. How should I go through the fine-tuning process, do you recommend that I go
    through pre-training + fine-tuning or just do fine-tuning?

    The `multilingual-e5-*` models have already gone through extensive multilingual
    pre-training. Just doing fine-tuning should get you a good performance. Going
    through pre-training + fine-tuning involves more complicated pipelines, and the
    improvements are likely marginal.


    2. If only fine-tuning is required then section 4.2 in your paper, mentions that
    the MS-MARCO and NQ formats would be ideal formats to convert my corpus into,
    is my understanding correct and are there any other considerations or factors
    I should take into account when building my tuning set.

    Yes, your understanding is correct. You need to prepare the query, positive documents,
    and hard negative documents for training.


    3. Which model do you recommend I start with? this multi-lingual model or the
    unsupervised bases?

    Do not use the unsupervised models, they are English-only. Please use `multilingual-e5-*`
    models, choose the size (small / base / large) based on your needs.


    Best,

    Liang'
  created_at: 2023-11-20 04:12:45+00:00
  edited: false
  hidden: false
  id: 655adcbd30ad83ad6b75f8e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c0d5b7fbdde04fb294b8cf3cf097061.svg
      fullname: Lars Skaug
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: larsskaug
      type: user
    createdAt: '2023-11-20T15:56:43.000Z'
    data:
      edited: false
      editors:
      - larsskaug
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9495819807052612
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c0d5b7fbdde04fb294b8cf3cf097061.svg
          fullname: Lars Skaug
          isHf: false
          isPro: false
          name: larsskaug
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Ziizu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Ziizu\">@<span class=\"\
          underline\">Ziizu</span></a></span>\n\n\t</span></span>,</p>\n<p>May I ask\
          \ how you came to choose e5? I have been following it for a while as well.\
          \ The results on Hugging Face are certainly impressive, but in my testing,\
          \ I get better results from RoBERTa. What am I missing?<br>Especially for\
          \ Retrieval Augmented Generation, I would have thought a larger LLM would\
          \ be better.  If we stay in the Microsoft universe, would PHI-2 (launched\
          \ at Microsoft Ignite: <a rel=\"nofollow\" href=\"https://the-decoder.com/microsofts-tiny-but-mighty-phi-2-shows-dramatic-improvements\"\
          >https://the-decoder.com/microsofts-tiny-but-mighty-phi-2-shows-dramatic-improvements</a>\
          \ ) be a good alternative? </p>\n<p>Thank you,<br>Lars</p>\n"
        raw: "Hi @Ziizu,\n\nMay I ask how you came to choose e5? I have been following\
          \ it for a while as well. The results on Hugging Face are certainly impressive,\
          \ but in my testing, I get better results from RoBERTa. What am I missing?\n\
          Especially for Retrieval Augmented Generation, I would have thought a larger\
          \ LLM would be better.  If we stay in the Microsoft universe, would PHI-2\
          \ (launched at Microsoft Ignite: https://the-decoder.com/microsofts-tiny-but-mighty-phi-2-shows-dramatic-improvements\
          \ ) be a good alternative? \n\nThank you,\nLars"
        updatedAt: '2023-11-20T15:56:43.593Z'
      numEdits: 0
      reactions: []
    id: 655b81bb34e52eb3b890cb3a
    type: comment
  author: larsskaug
  content: "Hi @Ziizu,\n\nMay I ask how you came to choose e5? I have been following\
    \ it for a while as well. The results on Hugging Face are certainly impressive,\
    \ but in my testing, I get better results from RoBERTa. What am I missing?\nEspecially\
    \ for Retrieval Augmented Generation, I would have thought a larger LLM would\
    \ be better.  If we stay in the Microsoft universe, would PHI-2 (launched at Microsoft\
    \ Ignite: https://the-decoder.com/microsofts-tiny-but-mighty-phi-2-shows-dramatic-improvements\
    \ ) be a good alternative? \n\nThank you,\nLars"
  created_at: 2023-11-20 15:56:43+00:00
  edited: false
  hidden: false
  id: 655b81bb34e52eb3b890cb3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
      fullname: Wilfredo Martel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wilfoderek
      type: user
    createdAt: '2023-12-03T22:03:47.000Z'
    data:
      edited: false
      editors:
      - wilfoderek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7700852155685425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
          fullname: Wilfredo Martel
          isHf: false
          isPro: false
          name: wilfoderek
          type: user
        html: '<p>Could you share the script to fine tuning multilingual-e5?</p>

          '
        raw: Could you share the script to fine tuning multilingual-e5?
        updatedAt: '2023-12-03T22:03:47.627Z'
      numEdits: 0
      reactions: []
    id: 656cfb43d848a6683af48aa4
    type: comment
  author: wilfoderek
  content: Could you share the script to fine tuning multilingual-e5?
  created_at: 2023-12-03 22:03:47+00:00
  edited: false
  hidden: false
  id: 656cfb43d848a6683af48aa4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
      fullname: Nason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ziizu
      type: user
    createdAt: '2023-12-07T22:30:13.000Z'
    data:
      edited: false
      editors:
      - Ziizu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9476814270019531
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
          fullname: Nason
          isHf: false
          isPro: false
          name: Ziizu
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;larsskaug&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/larsskaug\"\
          >@<span class=\"underline\">larsskaug</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>We chose e5 off the benchmark scores too, the other motivating\
          \ factor was that the fine tuning process was well layed out and also agreeable\
          \ with the data we have available meaning there's a clear route to improvement\
          \ for our use case.</p>\n<p>At the time of posting, I had not yet methodically\
          \ bench-marked e5 or other models but I have shared your experience of seeing\
          \ differences between benchmark scores and real world performance with other\
          \ models which is the reason we've been developing our own benchmark/test\
          \ thats reflective of our use case.<br>We are now in the process of collecting\
          \ baseline scores on models we're interested in (e5, gte and instructor),\
          \ after doing so we will attempt to fine tune them and compare again. </p>\n\
          <p>Regarding RAG, from my understanding PHI-2 is a generative pre-trained\
          \ transformer (GPT) model which is primarily used to generate textual outputs,\
          \ whereas the E5 (and others of the MTEB leader board) are embedding models\
          \ which generate rich embeddings, these embeddings can be used for a variety\
          \ of  tasks such as information retrieval, semantic textual similarity,\
          \ text re-ranking, etc. In the context of a RAG system, embedding models\
          \ are primarily used to or retrieve (i.e. filter and pass) relevant information\
          \ to a GPT model which then generates a textual response based on information\
          \ filtered by the embedding model.  </p>\n<p>We use mistrial and GPT-3.5/4\
          \ for our generator but any half decent GPT will give good quality answers,\
          \ so I'm sure PHI-2 will give sufficient answers for most use cases. The\
          \ biggest impact on the quality on of answers of an RAG system is the quality\
          \ of information passed which is dependant on the embedding model.</p>\n\
          <p>Feel free to correct anything mentioned as I'm still getting up to speed\
          \ on this stuff :)</p>\n"
        raw: "Hey @larsskaug \n\nWe chose e5 off the benchmark scores too, the other\
          \ motivating factor was that the fine tuning process was well layed out\
          \ and also agreeable with the data we have available meaning there's a clear\
          \ route to improvement for our use case.\n\nAt the time of posting, I had\
          \ not yet methodically bench-marked e5 or other models but I have shared\
          \ your experience of seeing differences between benchmark scores and real\
          \ world performance with other models which is the reason we've been developing\
          \ our own benchmark/test thats reflective of our use case.\nWe are now in\
          \ the process of collecting baseline scores on models we're interested in\
          \ (e5, gte and instructor), after doing so we will attempt to fine tune\
          \ them and compare again. \n\nRegarding RAG, from my understanding PHI-2\
          \ is a generative pre-trained transformer (GPT) model which is primarily\
          \ used to generate textual outputs, whereas the E5 (and others of the MTEB\
          \ leader board) are embedding models which generate rich embeddings, these\
          \ embeddings can be used for a variety of  tasks such as information retrieval,\
          \ semantic textual similarity, text re-ranking, etc. In the context of a\
          \ RAG system, embedding models are primarily used to or retrieve (i.e. filter\
          \ and pass) relevant information to a GPT model which then generates a textual\
          \ response based on information filtered by the embedding model.  \n\n\n\
          We use mistrial and GPT-3.5/4 for our generator but any half decent GPT\
          \ will give good quality answers, so I'm sure PHI-2 will give sufficient\
          \ answers for most use cases. The biggest impact on the quality on of answers\
          \ of an RAG system is the quality of information passed which is dependant\
          \ on the embedding model.\n\nFeel free to correct anything mentioned as\
          \ I'm still getting up to speed on this stuff :)"
        updatedAt: '2023-12-07T22:30:13.112Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - larsskaug
    id: 657247750e01afdccfb633d1
    type: comment
  author: Ziizu
  content: "Hey @larsskaug \n\nWe chose e5 off the benchmark scores too, the other\
    \ motivating factor was that the fine tuning process was well layed out and also\
    \ agreeable with the data we have available meaning there's a clear route to improvement\
    \ for our use case.\n\nAt the time of posting, I had not yet methodically bench-marked\
    \ e5 or other models but I have shared your experience of seeing differences between\
    \ benchmark scores and real world performance with other models which is the reason\
    \ we've been developing our own benchmark/test thats reflective of our use case.\n\
    We are now in the process of collecting baseline scores on models we're interested\
    \ in (e5, gte and instructor), after doing so we will attempt to fine tune them\
    \ and compare again. \n\nRegarding RAG, from my understanding PHI-2 is a generative\
    \ pre-trained transformer (GPT) model which is primarily used to generate textual\
    \ outputs, whereas the E5 (and others of the MTEB leader board) are embedding\
    \ models which generate rich embeddings, these embeddings can be used for a variety\
    \ of  tasks such as information retrieval, semantic textual similarity, text re-ranking,\
    \ etc. In the context of a RAG system, embedding models are primarily used to\
    \ or retrieve (i.e. filter and pass) relevant information to a GPT model which\
    \ then generates a textual response based on information filtered by the embedding\
    \ model.  \n\n\nWe use mistrial and GPT-3.5/4 for our generator but any half decent\
    \ GPT will give good quality answers, so I'm sure PHI-2 will give sufficient answers\
    \ for most use cases. The biggest impact on the quality on of answers of an RAG\
    \ system is the quality of information passed which is dependant on the embedding\
    \ model.\n\nFeel free to correct anything mentioned as I'm still getting up to\
    \ speed on this stuff :)"
  created_at: 2023-12-07 22:30:13+00:00
  edited: false
  hidden: false
  id: 657247750e01afdccfb633d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
      fullname: Nason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ziizu
      type: user
    createdAt: '2023-12-07T22:32:06.000Z'
    data:
      edited: false
      editors:
      - Ziizu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6033185720443726
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
          fullname: Nason
          isHf: false
          isPro: false
          name: Ziizu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;wilfoderek&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wilfoderek\">@<span class=\"\
          underline\">wilfoderek</span></a></span>\n\n\t</span></span> </p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://github.com/run-llama/finetune-embedding/tree/main\"\
          >https://github.com/run-llama/finetune-embedding/tree/main</a></p>\n<p>This\
          \ is a good starting point.</p>\n"
        raw: "@wilfoderek \n\nhttps://github.com/run-llama/finetune-embedding/tree/main\n\
          \nThis is a good starting point."
        updatedAt: '2023-12-07T22:32:06.043Z'
      numEdits: 0
      reactions: []
    id: 657247e659773fd21b51e264
    type: comment
  author: Ziizu
  content: "@wilfoderek \n\nhttps://github.com/run-llama/finetune-embedding/tree/main\n\
    \nThis is a good starting point."
  created_at: 2023-12-07 22:32:06+00:00
  edited: false
  hidden: false
  id: 657247e659773fd21b51e264
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
      fullname: Dino Novak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnovak232
      type: user
    createdAt: '2024-01-02T21:25:18.000Z'
    data:
      edited: false
      editors:
      - dnovak232
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957176685333252
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
          fullname: Dino Novak
          isHf: false
          isPro: false
          name: dnovak232
          type: user
        html: "<p>Extremely interesting topic. <span data-props=\"{&quot;user&quot;:&quot;Ziizu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Ziizu\"\
          >@<span class=\"underline\">Ziizu</span></a></span>\n\n\t</span></span>\
          \ , as some time has passed have you managed to have any success in you\
          \ plans?</p>\n<p>Am I having some misunderstanding or you plan is to have\
          \ architecture like:  Vector DB - E5-model - GPT-3.5/4 somehow linked together?<br>Any\
          \ success in creating this pipeline.<br>I am using llama-index and for English\
          \ content it works very well (with Mixtral 8x7b instruct as LLM - I am using\
          \ hosted version on together.ai and it has unbeatable price/performance),\
          \ but I have tried with Croatian content and in this setup this is where\
          \ things fully fall apart.<br>Looking forward on your experience.<br>Thanx,<br>D\
          \ </p>\n"
        raw: "Extremely interesting topic. @Ziizu , as some time has passed have you\
          \ managed to have any success in you plans?\n\nAm I having some misunderstanding\
          \ or you plan is to have architecture like:  Vector DB - E5-model - GPT-3.5/4\
          \ somehow linked together?\nAny success in creating this pipeline. \nI am\
          \ using llama-index and for English content it works very well (with Mixtral\
          \ 8x7b instruct as LLM - I am using hosted version on together.ai and it\
          \ has unbeatable price/performance), but I have tried with Croatian content\
          \ and in this setup this is where things fully fall apart.\nLooking forward\
          \ on your experience.\nThanx,\nD "
        updatedAt: '2024-01-02T21:25:18.756Z'
      numEdits: 0
      reactions: []
    id: 65947f3edfca9fad61fa7922
    type: comment
  author: dnovak232
  content: "Extremely interesting topic. @Ziizu , as some time has passed have you\
    \ managed to have any success in you plans?\n\nAm I having some misunderstanding\
    \ or you plan is to have architecture like:  Vector DB - E5-model - GPT-3.5/4\
    \ somehow linked together?\nAny success in creating this pipeline. \nI am using\
    \ llama-index and for English content it works very well (with Mixtral 8x7b instruct\
    \ as LLM - I am using hosted version on together.ai and it has unbeatable price/performance),\
    \ but I have tried with Croatian content and in this setup this is where things\
    \ fully fall apart.\nLooking forward on your experience.\nThanx,\nD "
  created_at: 2024-01-02 21:25:18+00:00
  edited: false
  hidden: false
  id: 65947f3edfca9fad61fa7922
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
      fullname: Nason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ziizu
      type: user
    createdAt: '2024-01-03T14:27:12.000Z'
    data:
      edited: false
      editors:
      - Ziizu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9253343939781189
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
          fullname: Nason
          isHf: false
          isPro: false
          name: Ziizu
          type: user
        html: '<p>Your understanding of my plan is correct, my architecture is a standard
          Retrieval-Augmented Generation (RAG) pipeline (Vector DB - embedding model
          - GPT model), you can look it up online to figure out how to link them together.</p>

          <p>I have created the pipeline and its work great with the dutch fine tuned
          embedding model + GPT 3.5/4, however, I have had a similar experience to
          you where the performance drops significantly when I try to use a Mixtral/Mistral
          model.</p>

          <p>I''m going to attempt to follow <a rel="nofollow" href="https://old.reddit.com/r/LocalLLaMA/comments/18pu83i/finetune_llama2_for_any_language/">this
          guide on fine-tuning open-source models </a>, the steps should work for
          you to fine tune a Croatian Mixtral model. </p>

          '
        raw: 'Your understanding of my plan is correct, my architecture is a standard
          Retrieval-Augmented Generation (RAG) pipeline (Vector DB - embedding model
          - GPT model), you can look it up online to figure out how to link them together.


          I have created the pipeline and its work great with the dutch fine tuned
          embedding model + GPT 3.5/4, however, I have had a similar experience to
          you where the performance drops significantly when I try to use a Mixtral/Mistral
          model.


          I''m going to attempt to follow [this guide on fine-tuning open-source models
          ](https://old.reddit.com/r/LocalLLaMA/comments/18pu83i/finetune_llama2_for_any_language/),
          the steps should work for you to fine tune a Croatian Mixtral model. '
        updatedAt: '2024-01-03T14:27:12.007Z'
      numEdits: 0
      reactions: []
    id: 65956ec0d5f4ef7cabd6d615
    type: comment
  author: Ziizu
  content: 'Your understanding of my plan is correct, my architecture is a standard
    Retrieval-Augmented Generation (RAG) pipeline (Vector DB - embedding model - GPT
    model), you can look it up online to figure out how to link them together.


    I have created the pipeline and its work great with the dutch fine tuned embedding
    model + GPT 3.5/4, however, I have had a similar experience to you where the performance
    drops significantly when I try to use a Mixtral/Mistral model.


    I''m going to attempt to follow [this guide on fine-tuning open-source models
    ](https://old.reddit.com/r/LocalLLaMA/comments/18pu83i/finetune_llama2_for_any_language/),
    the steps should work for you to fine tune a Croatian Mixtral model. '
  created_at: 2024-01-03 14:27:12+00:00
  edited: false
  hidden: false
  id: 65956ec0d5f4ef7cabd6d615
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: intfloat/multilingual-e5-large
repo_type: model
status: open
target_branch: null
title: Fine tuning for a dutch forum.
