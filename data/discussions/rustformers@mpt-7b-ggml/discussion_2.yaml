!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chenhunghan
conflicting_files: null
created_at: 2023-05-22 18:08:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/be3209732a08e0697b6869380d273dd7.svg
      fullname: Hung-Han Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenhunghan
      type: user
    createdAt: '2023-05-22T19:08:58.000Z'
    data:
      edited: true
      editors:
      - chenhunghan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/be3209732a08e0697b6869380d273dd7.svg
          fullname: Hung-Han Chen
          isHf: false
          isPro: false
          name: chenhunghan
          type: user
        html: '<p>There seems to be rust exceptions when loading the model.</p>

          <p>(Using <code>llm-rs==0.1.1</code>)</p>

          <p>I tried to load the model like <code>model = Llama("./mpt-7b-q4_0-ggjt.bin")</code>,
          but got</p>

          <pre><code>thread ''&lt;unnamed&gt;'' panicked at ''called `Result::unwrap()`
          on an `Err` value: InvalidFormatVersion { container_type: Ggjt, version:
          2 }'', src/models.rs:5:1

          </code></pre>

          <p>also tried</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          llm_rs <span class="hljs-keyword">import</span> Llama


          <span class="hljs-comment">#load the model</span>

          model = Llama(<span class="hljs-string">"cache/mpt-7b-q4_0.bin"</span>)


          <span class="hljs-comment">#generate</span>

          <span class="hljs-built_in">print</span>(model.generate(<span class="hljs-string">"The
          meaning of life is"</span>))

          </code></pre>

          <p>but got</p>

          <pre><code>thread ''&lt;unnamed&gt;'' panicked at ''called `Result::unwrap()`
          on an `Err` value: Io(Error { kind: UnexpectedEof, message: "failed to fill
          whole buffer" })'', src/models.rs:5:1

          </code></pre>

          <p>any additional instructions to load the models?</p>

          '
        raw: 'There seems to be rust exceptions when loading the model.


          (Using `llm-rs==0.1.1`)


          I tried to load the model like `model = Llama("./mpt-7b-q4_0-ggjt.bin")`,
          but got

          ```

          thread ''<unnamed>'' panicked at ''called `Result::unwrap()` on an `Err`
          value: InvalidFormatVersion { container_type: Ggjt, version: 2 }'', src/models.rs:5:1

          ```

          also tried

          ```python

          from llm_rs import Llama


          #load the model

          model = Llama("cache/mpt-7b-q4_0.bin")


          #generate

          print(model.generate("The meaning of life is"))

          ```

          but got

          ```

          thread ''<unnamed>'' panicked at ''called `Result::unwrap()` on an `Err`
          value: Io(Error { kind: UnexpectedEof, message: "failed to fill whole buffer"
          })'', src/models.rs:5:1

          ```

          any additional instructions to load the models?'
        updatedAt: '2023-05-22T19:09:33.425Z'
      numEdits: 1
      reactions: []
    id: 646bbdca1da1b6d027fcb40b
    type: comment
  author: chenhunghan
  content: 'There seems to be rust exceptions when loading the model.


    (Using `llm-rs==0.1.1`)


    I tried to load the model like `model = Llama("./mpt-7b-q4_0-ggjt.bin")`, but
    got

    ```

    thread ''<unnamed>'' panicked at ''called `Result::unwrap()` on an `Err` value:
    InvalidFormatVersion { container_type: Ggjt, version: 2 }'', src/models.rs:5:1

    ```

    also tried

    ```python

    from llm_rs import Llama


    #load the model

    model = Llama("cache/mpt-7b-q4_0.bin")


    #generate

    print(model.generate("The meaning of life is"))

    ```

    but got

    ```

    thread ''<unnamed>'' panicked at ''called `Result::unwrap()` on an `Err` value:
    Io(Error { kind: UnexpectedEof, message: "failed to fill whole buffer" })'', src/models.rs:5:1

    ```

    any additional instructions to load the models?'
  created_at: 2023-05-22 18:08:58+00:00
  edited: true
  hidden: false
  id: 646bbdca1da1b6d027fcb40b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-05-22T19:15:02.000Z'
    data:
      edited: false
      editors:
      - LLukas22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: '<p>There were breaking changes in the ggml format, you need to use
          llm-rs==0.2.0 or greater (see <a rel="nofollow" href="https://github.com/LLukas22/llm-rs-python/releases/tag/0.2.0">here</a>)</p>

          <p>Also MPT isn''t a LLama model, sou you need to load it via the <code>Mpt</code>
          model, you can also see this in the model-card of this repo.</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          llm_rs <span class="hljs-keyword">import</span> Mpt


          model = Mpt(<span class="hljs-string">"cache/mpt-7b-q4_0.bin"</span>)

          </code></pre>

          '
        raw: 'There were breaking changes in the ggml format, you need to use llm-rs==0.2.0
          or greater (see [here](https://github.com/LLukas22/llm-rs-python/releases/tag/0.2.0))


          Also MPT isn''t a LLama model, sou you need to load it via the `Mpt` model,
          you can also see this in the model-card of this repo.


          ```python

          from llm_rs import Mpt


          model = Mpt("cache/mpt-7b-q4_0.bin")

          ```'
        updatedAt: '2023-05-22T19:15:02.795Z'
      numEdits: 0
      reactions: []
    id: 646bbf36db697c798a4847a5
    type: comment
  author: LLukas22
  content: 'There were breaking changes in the ggml format, you need to use llm-rs==0.2.0
    or greater (see [here](https://github.com/LLukas22/llm-rs-python/releases/tag/0.2.0))


    Also MPT isn''t a LLama model, sou you need to load it via the `Mpt` model, you
    can also see this in the model-card of this repo.


    ```python

    from llm_rs import Mpt


    model = Mpt("cache/mpt-7b-q4_0.bin")

    ```'
  created_at: 2023-05-22 18:15:02+00:00
  edited: false
  hidden: false
  id: 646bbf36db697c798a4847a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/be3209732a08e0697b6869380d273dd7.svg
      fullname: Hung-Han Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenhunghan
      type: user
    createdAt: '2023-05-22T19:27:02.000Z'
    data:
      edited: false
      editors:
      - chenhunghan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/be3209732a08e0697b6869380d273dd7.svg
          fullname: Hung-Han Chen
          isHf: false
          isPro: false
          name: chenhunghan
          type: user
        html: '<p>Thank you, works well!</p>

          '
        raw: Thank you, works well!
        updatedAt: '2023-05-22T19:27:02.886Z'
      numEdits: 0
      reactions: []
    id: 646bc206ed2282721339ee82
    type: comment
  author: chenhunghan
  content: Thank you, works well!
  created_at: 2023-05-22 18:27:02+00:00
  edited: false
  hidden: false
  id: 646bc206ed2282721339ee82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-05-23T07:45:39.000Z'
    data:
      status: closed
    id: 646c6f232c29b8753acdc471
    type: status-change
  author: LLukas22
  created_at: 2023-05-23 06:45:39+00:00
  id: 646c6f232c29b8753acdc471
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: rustformers/mpt-7b-ggml
repo_type: model
status: closed
target_branch: null
title: Panic when try to load the model bin files
