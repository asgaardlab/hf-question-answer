!!python/object:huggingface_hub.community.DiscussionWithDetails
author: luc18
conflicting_files: null
created_at: 2023-05-14 15:38:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
      fullname: luc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luc18
      type: user
    createdAt: '2023-05-14T16:38:18.000Z'
    data:
      edited: false
      editors:
      - luc18
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
          fullname: luc
          isHf: false
          isPro: false
          name: luc18
          type: user
        html: '<p>Hi,</p>

          <p>I got "libc++abi: terminating due to uncaught exception of type std::runtime_error:
          unexpectedly reached end of file" with llama.cpp </p>

          '
        raw: "Hi,\r\n\r\nI got \"libc++abi: terminating due to uncaught exception\
          \ of type std::runtime_error: unexpectedly reached end of file\" with llama.cpp "
        updatedAt: '2023-05-14T16:38:18.527Z'
      numEdits: 0
      reactions: []
    id: 64610e7a9aad9e149723b484
    type: comment
  author: luc18
  content: "Hi,\r\n\r\nI got \"libc++abi: terminating due to uncaught exception of\
    \ type std::runtime_error: unexpectedly reached end of file\" with llama.cpp "
  created_at: 2023-05-14 15:38:18+00:00
  edited: false
  hidden: false
  id: 64610e7a9aad9e149723b484
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-05-14T16:50:34.000Z'
    data:
      edited: false
      editors:
      - LLukas22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;luc18&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/luc18\">@<span class=\"\
          underline\">luc18</span></a></span>\n\n\t</span></span> the Quantization\
          \ format in llama.cpp  was  recently changed (see <a rel=\"nofollow\" href=\"\
          https://github.com/ggerganov/ggml/pull/154\">this</a>) you can use the <code>f16</code>\
          \ weights which will still work.<br>I will probably upload the new converted\
          \ weights later today and mark them with a V2. </p>\n<p>The weights in this\
          \ repo were created for development purposes in the rustformers/llm repo.</p>\n"
        raw: "@luc18 the Quantization format in llama.cpp  was  recently changed (see\
          \ [this](https://github.com/ggerganov/ggml/pull/154)) you can use the `f16`\
          \ weights which will still work. \nI will probably upload the new converted\
          \ weights later today and mark them with a V2. \n\nThe weights in this repo\
          \ were created for development purposes in the rustformers/llm repo."
        updatedAt: '2023-05-14T16:50:34.511Z'
      numEdits: 0
      reactions: []
    id: 6461115a7735f76a4a50b63c
    type: comment
  author: LLukas22
  content: "@luc18 the Quantization format in llama.cpp  was  recently changed (see\
    \ [this](https://github.com/ggerganov/ggml/pull/154)) you can use the `f16` weights\
    \ which will still work. \nI will probably upload the new converted weights later\
    \ today and mark them with a V2. \n\nThe weights in this repo were created for\
    \ development purposes in the rustformers/llm repo."
  created_at: 2023-05-14 15:50:34+00:00
  edited: false
  hidden: false
  id: 6461115a7735f76a4a50b63c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
      fullname: luc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luc18
      type: user
    createdAt: '2023-05-15T06:29:41.000Z'
    data:
      edited: false
      editors:
      - luc18
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
          fullname: luc
          isHf: false
          isPro: false
          name: luc18
          type: user
        html: '<p>Great! Thank you. I''ll try f16</p>

          '
        raw: Great! Thank you. I'll try f16
        updatedAt: '2023-05-15T06:29:41.453Z'
      numEdits: 0
      reactions: []
    id: 6461d155c67e1a494d8dd12a
    type: comment
  author: luc18
  content: Great! Thank you. I'll try f16
  created_at: 2023-05-15 05:29:41+00:00
  edited: false
  hidden: false
  id: 6461d155c67e1a494d8dd12a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
      fullname: luc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luc18
      type: user
    createdAt: '2023-05-15T07:15:48.000Z'
    data:
      edited: false
      editors:
      - luc18
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
          fullname: luc
          isHf: false
          isPro: false
          name: luc18
          type: user
        html: "<p>I tried former version of llama.cpp. Same error. F16 too. With rustformers/llm\
          \ I get (f16 and q4_0):</p>\n<p>llm llama infer -m mpt-7b-q4_0.bin -p \"\
          Tell me how to make handmade soap\"<br>\u28FE Loading model...Error:<br>\
          \   0: Could not load model<br>   1: unsupported f16_: 13</p>\n<p>  \u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 BACKTRACE \u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501<br>             \
          \                   \u22EE 4 frames hidden \u22EE<br>   5: llm::cli_args::ModelLoad::load::h41b58f148731b191<br>\
          \      at :<br>   6: llm::main::h0bbee8362f52fbc1<br>      at :<br>   7:\
          \ std::sys_common::backtrace::__rust_begin_short_backtrace::h2fe9760f1b0b902d<br>\
          \      at :<br>   8: std::rt::lang_start::{{closure}}::h37a98b48e88897d6<br>\
          \      at :<br>   9: core::ops::function::impls::&lt;impl core::ops::function::FnOnce<a\
          \ rel=\"nofollow\"> for &amp;F&gt;::call_once::hf2f6b444963da11f<br>   \
          \   at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/core/src/ops/function.rs:287<br>\
          \  10: std::panicking::try::do_call::h9152231fddd58858<br>      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:487<br>\
          \  11: std::panicking::try::hcc27eab3b8ee3cb1<br>      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:451<br>\
          \  12: std::panic::catch_unwind::hca546a4311ab9871<br>      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panic.rs:140<br>\
          \  13: std::rt::lang_start_internal::{{closure}}::h4e65aa71fe685c85<br>\
          \      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/rt.rs:148<br>\
          \  14: std::panicking::try::do_call::h61aea55fbdf97fc2<br>      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:487<br>\
          \  15: std::panicking::try::hcfc3b62fb8f6215e<br>      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:451<br>\
          \  16: std::panic::catch_unwind::h61a201e98b56a743<br>      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panic.rs:140<br>\
          \  17: std::rt::lang_start_internal::h91996717d3eb1d2a<br>      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/rt.rs:148<br>\
          \  18: _main<br>      at :</a></p><a rel=\"nofollow\">\n<p>Run with COLORBT_SHOW_HIDDEN=1\
          \ environment variable to disable frame filtering.<br>Run with RUST_BACKTRACE=full\
          \ to include source snippets.</p>\n</a>"
        raw: "I tried former version of llama.cpp. Same error. F16 too. With rustformers/llm\
          \ I get (f16 and q4_0):\n\nllm llama infer -m mpt-7b-q4_0.bin -p \"Tell\
          \ me how to make handmade soap\"\n\u28FE Loading model...Error:\n   0: Could\
          \ not load model\n   1: unsupported f16_: 13\n\n  \u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501 BACKTRACE \u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\n                                \u22EE\
          \ 4 frames hidden \u22EE\n   5: llm::cli_args::ModelLoad::load::h41b58f148731b191\n\
          \      at <unknown source file>:<unknown line>\n   6: llm::main::h0bbee8362f52fbc1\n\
          \      at <unknown source file>:<unknown line>\n   7: std::sys_common::backtrace::__rust_begin_short_backtrace::h2fe9760f1b0b902d\n\
          \      at <unknown source file>:<unknown line>\n   8: std::rt::lang_start::{{closure}}::h37a98b48e88897d6\n\
          \      at <unknown source file>:<unknown line>\n   9: core::ops::function::impls::<impl\
          \ core::ops::function::FnOnce<A> for &F>::call_once::hf2f6b444963da11f\n\
          \      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/core/src/ops/function.rs:287\n\
          \  10: std::panicking::try::do_call::h9152231fddd58858\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:487\n\
          \  11: std::panicking::try::hcc27eab3b8ee3cb1\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:451\n\
          \  12: std::panic::catch_unwind::hca546a4311ab9871\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panic.rs:140\n\
          \  13: std::rt::lang_start_internal::{{closure}}::h4e65aa71fe685c85\n  \
          \    at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/rt.rs:148\n\
          \  14: std::panicking::try::do_call::h61aea55fbdf97fc2\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:487\n\
          \  15: std::panicking::try::hcfc3b62fb8f6215e\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:451\n\
          \  16: std::panic::catch_unwind::h61a201e98b56a743\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panic.rs:140\n\
          \  17: std::rt::lang_start_internal::h91996717d3eb1d2a\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/rt.rs:148\n\
          \  18: _main<unknown>\n      at <unknown source file>:<unknown line>\n\n\
          Run with COLORBT_SHOW_HIDDEN=1 environment variable to disable frame filtering.\n\
          Run with RUST_BACKTRACE=full to include source snippets."
        updatedAt: '2023-05-15T07:15:48.553Z'
      numEdits: 0
      reactions: []
    id: 6461dc243026e7f163c6aac5
    type: comment
  author: luc18
  content: "I tried former version of llama.cpp. Same error. F16 too. With rustformers/llm\
    \ I get (f16 and q4_0):\n\nllm llama infer -m mpt-7b-q4_0.bin -p \"Tell me how\
    \ to make handmade soap\"\n\u28FE Loading model...Error:\n   0: Could not load\
    \ model\n   1: unsupported f16_: 13\n\n  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501 BACKTRACE \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n         \
    \                       \u22EE 4 frames hidden \u22EE\n   5: llm::cli_args::ModelLoad::load::h41b58f148731b191\n\
    \      at <unknown source file>:<unknown line>\n   6: llm::main::h0bbee8362f52fbc1\n\
    \      at <unknown source file>:<unknown line>\n   7: std::sys_common::backtrace::__rust_begin_short_backtrace::h2fe9760f1b0b902d\n\
    \      at <unknown source file>:<unknown line>\n   8: std::rt::lang_start::{{closure}}::h37a98b48e88897d6\n\
    \      at <unknown source file>:<unknown line>\n   9: core::ops::function::impls::<impl\
    \ core::ops::function::FnOnce<A> for &F>::call_once::hf2f6b444963da11f\n     \
    \ at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/core/src/ops/function.rs:287\n\
    \  10: std::panicking::try::do_call::h9152231fddd58858\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:487\n\
    \  11: std::panicking::try::hcc27eab3b8ee3cb1\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:451\n\
    \  12: std::panic::catch_unwind::hca546a4311ab9871\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panic.rs:140\n\
    \  13: std::rt::lang_start_internal::{{closure}}::h4e65aa71fe685c85\n      at\
    \ /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/rt.rs:148\n\
    \  14: std::panicking::try::do_call::h61aea55fbdf97fc2\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:487\n\
    \  15: std::panicking::try::hcfc3b62fb8f6215e\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panicking.rs:451\n\
    \  16: std::panic::catch_unwind::h61a201e98b56a743\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/panic.rs:140\n\
    \  17: std::rt::lang_start_internal::h91996717d3eb1d2a\n      at /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc/library/std/src/rt.rs:148\n\
    \  18: _main<unknown>\n      at <unknown source file>:<unknown line>\n\nRun with\
    \ COLORBT_SHOW_HIDDEN=1 environment variable to disable frame filtering.\nRun\
    \ with RUST_BACKTRACE=full to include source snippets."
  created_at: 2023-05-15 06:15:48+00:00
  edited: false
  hidden: false
  id: 6461dc243026e7f163c6aac5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-05-15T08:13:46.000Z'
    data:
      edited: false
      editors:
      - LLukas22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;luc18&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/luc18\">@<span class=\"\
          underline\">luc18</span></a></span>\n\n\t</span></span> Oh sorry I'm always\
          \ thinking from the developer perspective\U0001F605. MPT support is still\
          \ in development and has some bugs, llama.cpp also wont include it as it\
          \ is not based on the LLaMA architecture. GGML will include it (see <a rel=\"\
          nofollow\" href=\"https://github.com/ggerganov/ggml/pull/145\">this pull-request</a>)\
          \ and Rustformers is also working on an implementation (see <a rel=\"nofollow\"\
          \ href=\"https://github.com/rustformers/llm/pull/218\">this pull-request</a>).\
          \ </p>\n<p>I will add a disclaimer to this repo, to hint at the 'still in\
          \ development status'. </p>\n<p>Expect it to be finished in a few days.\
          \ I will then add instructions on how to use these models and '@' at you\
          \ again to signal its ready.</p>\n"
        raw: "@luc18 Oh sorry I'm always thinking from the developer perspective\U0001F605\
          . MPT support is still in development and has some bugs, llama.cpp also\
          \ wont include it as it is not based on the LLaMA architecture. GGML will\
          \ include it (see [this pull-request](https://github.com/ggerganov/ggml/pull/145))\
          \ and Rustformers is also working on an implementation (see [this pull-request](https://github.com/rustformers/llm/pull/218)).\
          \ \n\nI will add a disclaimer to this repo, to hint at the 'still in development\
          \ status'. \n\nExpect it to be finished in a few days. I will then add instructions\
          \ on how to use these models and '@' at you again to signal its ready."
        updatedAt: '2023-05-15T08:13:46.088Z'
      numEdits: 0
      reactions: []
    id: 6461e9ba805bf40127051a4b
    type: comment
  author: LLukas22
  content: "@luc18 Oh sorry I'm always thinking from the developer perspective\U0001F605\
    . MPT support is still in development and has some bugs, llama.cpp also wont include\
    \ it as it is not based on the LLaMA architecture. GGML will include it (see [this\
    \ pull-request](https://github.com/ggerganov/ggml/pull/145)) and Rustformers is\
    \ also working on an implementation (see [this pull-request](https://github.com/rustformers/llm/pull/218)).\
    \ \n\nI will add a disclaimer to this repo, to hint at the 'still in development\
    \ status'. \n\nExpect it to be finished in a few days. I will then add instructions\
    \ on how to use these models and '@' at you again to signal its ready."
  created_at: 2023-05-15 07:13:46+00:00
  edited: false
  hidden: false
  id: 6461e9ba805bf40127051a4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30c59d1d488116d6db1c43b749fa5422.svg
      fullname: Wouter Tichelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Azamorn
      type: user
    createdAt: '2023-05-15T09:12:53.000Z'
    data:
      edited: false
      editors:
      - Azamorn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30c59d1d488116d6db1c43b749fa5422.svg
          fullname: Wouter Tichelaar
          isHf: false
          isPro: false
          name: Azamorn
          type: user
        html: '<p>Also really looking forward to this!</p>

          '
        raw: Also really looking forward to this!
        updatedAt: '2023-05-15T09:12:53.859Z'
      numEdits: 0
      reactions: []
    id: 6461f795cf4bf38e231d172b
    type: comment
  author: Azamorn
  content: Also really looking forward to this!
  created_at: 2023-05-15 08:12:53+00:00
  edited: false
  hidden: false
  id: 6461f795cf4bf38e231d172b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
      fullname: luc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luc18
      type: user
    createdAt: '2023-05-15T09:28:01.000Z'
    data:
      edited: false
      editors:
      - luc18
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
          fullname: luc
          isHf: false
          isPro: false
          name: luc18
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LLukas22&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/LLukas22\">@<span class=\"\
          underline\">LLukas22</span></a></span>\n\n\t</span></span> Sorry I didn't\
          \ get it... Looking forward your release.</p>\n"
        raw: '@LLukas22 Sorry I didn''t get it... Looking forward your release.'
        updatedAt: '2023-05-15T09:28:01.249Z'
      numEdits: 0
      reactions: []
    id: 6461fb21805bf4012705d34a
    type: comment
  author: luc18
  content: '@LLukas22 Sorry I didn''t get it... Looking forward your release.'
  created_at: 2023-05-15 08:28:01+00:00
  edited: false
  hidden: false
  id: 6461fb21805bf4012705d34a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12b51169879c937e62ed13d8a4b728a5.svg
      fullname: gab luz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabluz
      type: user
    createdAt: '2023-05-17T08:45:22.000Z'
    data:
      edited: false
      editors:
      - gabluz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12b51169879c937e62ed13d8a4b728a5.svg
          fullname: gab luz
          isHf: false
          isPro: false
          name: gabluz
          type: user
        html: '<p>Not working yet with koboldcpp (uses llamacpp). Waiting for a new
          release... valeu Lukas.</p>

          '
        raw: Not working yet with koboldcpp (uses llamacpp). Waiting for a new release...
          valeu Lukas.
        updatedAt: '2023-05-17T08:45:22.216Z'
      numEdits: 0
      reactions: []
    id: 6464942227d27c1fe741b379
    type: comment
  author: gabluz
  content: Not working yet with koboldcpp (uses llamacpp). Waiting for a new release...
    valeu Lukas.
  created_at: 2023-05-17 07:45:22+00:00
  edited: false
  hidden: false
  id: 6464942227d27c1fe741b379
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e584cd353914b5320561199ddc383df5.svg
      fullname: Darx Kies
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darxkies
      type: user
    createdAt: '2023-05-18T16:14:08.000Z'
    data:
      edited: false
      editors:
      - darxkies
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e584cd353914b5320561199ddc383df5.svg
          fullname: Darx Kies
          isHf: false
          isPro: false
          name: darxkies
          type: user
        html: '<p>The chat version works with neither koboldcpp nor llama.cpp. The
          checksum of the bin file is ok. I use the master version of both programs.</p>

          '
        raw: The chat version works with neither koboldcpp nor llama.cpp. The checksum
          of the bin file is ok. I use the master version of both programs.
        updatedAt: '2023-05-18T16:14:08.434Z'
      numEdits: 0
      reactions: []
    id: 64664ed09c627c78f86b304a
    type: comment
  author: darxkies
  content: The chat version works with neither koboldcpp nor llama.cpp. The checksum
    of the bin file is ok. I use the master version of both programs.
  created_at: 2023-05-18 15:14:08+00:00
  edited: false
  hidden: false
  id: 64664ed09c627c78f86b304a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-05-18T16:36:18.000Z'
    data:
      edited: true
      editors:
      - LLukas22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;darxkies&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/darxkies\">@<span class=\"\
          underline\">darxkies</span></a></span>\n\n\t</span></span>  MPT will not\
          \ be supported in llama.cpp as it is not based  on the LLama architecture.\
          \ Currently it is only supported as an example in GGML directly, the usage\
          \ is described in the README. </p>\n<p>Simpler to use Python/Rust implementations\
          \ are not ready yet. </p>\n<p>If you want this supported in koboldcpp, you\
          \ should probably open an issue there.</p>\n"
        raw: "@darxkies  MPT will not be supported in llama.cpp as it is not based\
          \  on the LLama architecture. Currently it is only supported as an example\
          \ in GGML directly, the usage is described in the README. \n\nSimpler to\
          \ use Python/Rust implementations are not ready yet. \n\nIf you want this\
          \ supported in koboldcpp, you should probably open an issue there."
        updatedAt: '2023-05-18T16:36:47.237Z'
      numEdits: 1
      reactions: []
    id: 64665402e0fe831b4792c15b
    type: comment
  author: LLukas22
  content: "@darxkies  MPT will not be supported in llama.cpp as it is not based \
    \ on the LLama architecture. Currently it is only supported as an example in GGML\
    \ directly, the usage is described in the README. \n\nSimpler to use Python/Rust\
    \ implementations are not ready yet. \n\nIf you want this supported in koboldcpp,\
    \ you should probably open an issue there."
  created_at: 2023-05-18 15:36:18+00:00
  edited: true
  hidden: false
  id: 64665402e0fe831b4792c15b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e584cd353914b5320561199ddc383df5.svg
      fullname: Darx Kies
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darxkies
      type: user
    createdAt: '2023-05-18T18:02:49.000Z'
    data:
      edited: false
      editors:
      - darxkies
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e584cd353914b5320561199ddc383df5.svg
          fullname: Darx Kies
          isHf: false
          isPro: false
          name: darxkies
          type: user
        html: '<p>Ok. Thank you.</p>

          '
        raw: Ok. Thank you.
        updatedAt: '2023-05-18T18:02:49.031Z'
      numEdits: 0
      reactions: []
    id: 64666849e0fe831b47947e89
    type: comment
  author: darxkies
  content: Ok. Thank you.
  created_at: 2023-05-18 17:02:49+00:00
  edited: false
  hidden: false
  id: 64666849e0fe831b47947e89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-05-19T20:07:50.000Z'
    data:
      edited: false
      editors:
      - LLukas22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Azamorn&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Azamorn\">@<span class=\"\
          underline\">Azamorn</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;luc18&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/luc18\">@<span class=\"underline\">luc18</span></a></span>\n\
          \n\t</span></span> <span data-props=\"{&quot;user&quot;:&quot;gabluz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gabluz\"\
          >@<span class=\"underline\">gabluz</span></a></span>\n\n\t</span></span>\
          \ Development has  progressed to a point where most MPT models can be run.\
          \ I updated the README with instructions on how to run these models in Python/Rust/C.</p>\n"
        raw: '@Azamorn @luc18 @gabluz Development has  progressed to a point where
          most MPT models can be run. I updated the README with instructions on how
          to run these models in Python/Rust/C.'
        updatedAt: '2023-05-19T20:07:50.642Z'
      numEdits: 0
      reactions: []
    id: 6467d7163a7c8dda230adf06
    type: comment
  author: LLukas22
  content: '@Azamorn @luc18 @gabluz Development has  progressed to a point where most
    MPT models can be run. I updated the README with instructions on how to run these
    models in Python/Rust/C.'
  created_at: 2023-05-19 19:07:50+00:00
  edited: false
  hidden: false
  id: 6467d7163a7c8dda230adf06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
      fullname: luc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luc18
      type: user
    createdAt: '2023-06-06T06:57:56.000Z'
    data:
      edited: false
      editors:
      - luc18
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7849884033203125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/738ff2600ab945e1923919f76fb8ef42.svg
          fullname: luc
          isHf: false
          isPro: false
          name: luc18
          type: user
        html: '<p>Great! Thanks. Works with ggml. But not with rust nor python (mac
          M2).</p>

          '
        raw: Great! Thanks. Works with ggml. But not with rust nor python (mac M2).
        updatedAt: '2023-06-06T06:57:56.538Z'
      numEdits: 0
      reactions: []
    id: 647ed8f42a7bcaa30794c848
    type: comment
  author: luc18
  content: Great! Thanks. Works with ggml. But not with rust nor python (mac M2).
  created_at: 2023-06-06 05:57:56+00:00
  edited: false
  hidden: false
  id: 647ed8f42a7bcaa30794c848
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-06-06T09:27:27.000Z'
    data:
      edited: false
      editors:
      - LLukas22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8443474173545837
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;luc18&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/luc18\">@<span class=\"\
          underline\">luc18</span></a></span>\n\n\t</span></span>  Hm thats weird,\
          \ are you using the latest versions of the python package/rust project?\
          \ It should be tested on all platforms, if you still have issues on the\
          \ newest versions please create an issue <a rel=\"nofollow\" href=\"https://github.com/rustformers/llm/issues\"\
          >here</a></p>\n"
        raw: '@luc18  Hm thats weird, are you using the latest versions of the python
          package/rust project? It should be tested on all platforms, if you still
          have issues on the newest versions please create an issue [here](https://github.com/rustformers/llm/issues)'
        updatedAt: '2023-06-06T09:27:27.647Z'
      numEdits: 0
      reactions: []
    id: 647efbff9c310244579a50a6
    type: comment
  author: LLukas22
  content: '@luc18  Hm thats weird, are you using the latest versions of the python
    package/rust project? It should be tested on all platforms, if you still have
    issues on the newest versions please create an issue [here](https://github.com/rustformers/llm/issues)'
  created_at: 2023-06-06 08:27:27+00:00
  edited: false
  hidden: false
  id: 647efbff9c310244579a50a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rustformers/mpt-7b-ggml
repo_type: model
status: open
target_branch: null
title: llama.cpp
