!!python/object:huggingface_hub.community.DiscussionWithDetails
author: marinone94
conflicting_files: null
created_at: 2022-06-09 11:34:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80863661a763eda7f696616a56995794.svg
      fullname: Emilio Marinone
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marinone94
      type: user
    createdAt: '2022-06-09T12:34:15.000Z'
    data:
      edited: true
      editors:
      - marinone94
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80863661a763eda7f696616a56995794.svg
          fullname: Emilio Marinone
          isHf: false
          isPro: false
          name: marinone94
          type: user
        html: "<p>Hi,</p>\n<p>I was trying out this model and it seems there is an\
          \ issue with the tokenizer replacing Swedish characters with English ones\
          \ (\xE4 --&gt; a, \xE5 --&gt; a, \xF6 --&gt; o). It looks weird to me since\
          \ the vocab file contains words including those swedish characters.</p>\n\
          <p>Examples</p>\n<pre><code>Input:  f\xF6rs\xE4ndelse fr\xE5n utlandet\n\
          Decoded:  [CLS] forsandelse fran utlandet [SEP]\nInput:  \xD6rebro \xE4\
          r en fin stad\nDecoded:  [CLS] orebro ar en fin stad [SEP]\n</code></pre>\n\
          <p>To reproduce:</p>\n<pre><code>from transformers import AutoTokenizer\n\
          \nexamples = [\"f\xF6rs\xE4ndelse fr\xE5n utlandet\", \"\xD6rebro \xE4r\
          \ en fin stad\"]\ntokenizer = AutoTokenizer.from_pretrained(\"af-ai-center/bert-base-swedish-uncased\"\
          )\nenc = tokenizer(examples)\ndec = tokenizer.batch_decode(enc[\"input_ids\"\
          ])\nfor input_example, decoded_example in zip(examples, dec):\n    print(\"\
          Input: \", input_example)\n    print(\"Decoded: \", decoded_example)\n</code></pre>\n\
          <p>Env:</p>\n<pre><code>- `transformers` version: 4.18.0\n- Platform: macOS-10.16-x86_64-i386-64bit\n\
          - Python version: 3.8.6\n- Huggingface_hub version: 0.5.1\n- PyTorch version\
          \ (GPU?): 1.9.1 (False)\n- Tensorflow version (GPU?): not installed (NA)\n\
          - Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not\
          \ installed\n- JaxLib version: not installed\n- Using GPU in script?: no\n\
          - Using distributed or parallel set-up in script?: no\n</code></pre>\n"
        raw: "Hi,\r\n\r\nI was trying out this model and it seems there is an issue\
          \ with the tokenizer replacing Swedish characters with English ones (\xE4\
          \ --> a, \xE5 --> a, \xF6 --> o). It looks weird to me since the vocab file\
          \ contains words including those swedish characters.\r\n\r\nExamples\r\n\
          ```\r\nInput:  f\xF6rs\xE4ndelse fr\xE5n utlandet\r\nDecoded:  [CLS] forsandelse\
          \ fran utlandet [SEP]\r\nInput:  \xD6rebro \xE4r en fin stad\r\nDecoded:\
          \  [CLS] orebro ar en fin stad [SEP]\r\n```\r\n\r\nTo reproduce:\r\n```\r\
          \nfrom transformers import AutoTokenizer\r\n\r\nexamples = [\"f\xF6rs\xE4\
          ndelse fr\xE5n utlandet\", \"\xD6rebro \xE4r en fin stad\"]\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"af-ai-center/bert-base-swedish-uncased\"\
          )\r\nenc = tokenizer(examples)\r\ndec = tokenizer.batch_decode(enc[\"input_ids\"\
          ])\r\nfor input_example, decoded_example in zip(examples, dec):\r\n    print(\"\
          Input: \", input_example)\r\n    print(\"Decoded: \", decoded_example)\r\
          \n```\r\n\r\nEnv:\r\n```\r\n- `transformers` version: 4.18.0\r\n- Platform:\
          \ macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.6\r\n- Huggingface_hub\
          \ version: 0.5.1\r\n- PyTorch version (GPU?): 1.9.1 (False)\r\n- Tensorflow\
          \ version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?):\
          \ not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version:\
          \ not installed\r\n- Using GPU in script?: no\r\n- Using distributed or\
          \ parallel set-up in script?: no\r\n```"
        updatedAt: '2022-06-09T12:36:27.851Z'
      numEdits: 1
      reactions: []
    id: 62a1e8c76679492ca5ce8c0a
    type: comment
  author: marinone94
  content: "Hi,\r\n\r\nI was trying out this model and it seems there is an issue\
    \ with the tokenizer replacing Swedish characters with English ones (\xE4 -->\
    \ a, \xE5 --> a, \xF6 --> o). It looks weird to me since the vocab file contains\
    \ words including those swedish characters.\r\n\r\nExamples\r\n```\r\nInput: \
    \ f\xF6rs\xE4ndelse fr\xE5n utlandet\r\nDecoded:  [CLS] forsandelse fran utlandet\
    \ [SEP]\r\nInput:  \xD6rebro \xE4r en fin stad\r\nDecoded:  [CLS] orebro ar en\
    \ fin stad [SEP]\r\n```\r\n\r\nTo reproduce:\r\n```\r\nfrom transformers import\
    \ AutoTokenizer\r\n\r\nexamples = [\"f\xF6rs\xE4ndelse fr\xE5n utlandet\", \"\xD6\
    rebro \xE4r en fin stad\"]\r\ntokenizer = AutoTokenizer.from_pretrained(\"af-ai-center/bert-base-swedish-uncased\"\
    )\r\nenc = tokenizer(examples)\r\ndec = tokenizer.batch_decode(enc[\"input_ids\"\
    ])\r\nfor input_example, decoded_example in zip(examples, dec):\r\n    print(\"\
    Input: \", input_example)\r\n    print(\"Decoded: \", decoded_example)\r\n```\r\
    \n\r\nEnv:\r\n```\r\n- `transformers` version: 4.18.0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\
    \n- Python version: 3.8.6\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version\
    \ (GPU?): 1.9.1 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n\
    - Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\
    \n- JaxLib version: not installed\r\n- Using GPU in script?: no\r\n- Using distributed\
    \ or parallel set-up in script?: no\r\n```"
  created_at: 2022-06-09 11:34:15+00:00
  edited: true
  hidden: false
  id: 62a1e8c76679492ca5ce8c0a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: af-ai-center/bert-base-swedish-uncased
repo_type: model
status: open
target_branch: null
title: Issue with tokenizer
