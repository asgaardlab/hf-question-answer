!!python/object:huggingface_hub.community.DiscussionWithDetails
author: silverfisk
conflicting_files: null
created_at: 2023-07-31 11:54:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60ed9cb66ab37c2080f2d5d83d80f194.svg
      fullname: 'Tom '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: silverfisk
      type: user
    createdAt: '2023-07-31T12:54:55.000Z'
    data:
      edited: true
      editors:
      - silverfisk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8917189240455627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60ed9cb66ab37c2080f2d5d83d80f194.svg
          fullname: 'Tom '
          isHf: false
          isPro: false
          name: silverfisk
          type: user
        html: '<p>Is there some way of sharding this model to my two GPUs?<br>I have
          a Quadro RTX 5000 laptop GPU with 16 GB VRAM + an NVIDIA GeForce RTX 3090
          eGPU with 24GB. When loading the model it seems to only try to load it into
          the 3090 GPU.</p>

          <pre><code class="language-shell">torch.cuda.OutOfMemoryError: CUDA out
          of memory. Tried to allocate 128.00 MiB (GPU 0; 23.69 GiB total capacity;
          22.05 GiB already allocated; 117.69 MiB free; 23.21 GiB reserved in total
          by PyTorch)

          </code></pre>

          <p>I thought the model would be split automagically since I''ve seen other
          smaller models loads use both GPUs with the same code. But maybe this is
          a newbie question, I''ve just gotten started with this yesterday.</p>

          '
        raw: 'Is there some way of sharding this model to my two GPUs?

          I have a Quadro RTX 5000 laptop GPU with 16 GB VRAM + an NVIDIA GeForce
          RTX 3090 eGPU with 24GB. When loading the model it seems to only try to
          load it into the 3090 GPU.


          ```shell

          torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00
          MiB (GPU 0; 23.69 GiB total capacity; 22.05 GiB already allocated; 117.69
          MiB free; 23.21 GiB reserved in total by PyTorch)

          ```


          I thought the model would be split automagically since I''ve seen other
          smaller models loads use both GPUs with the same code. But maybe this is
          a newbie question, I''ve just gotten started with this yesterday.'
        updatedAt: '2023-07-31T12:58:52.806Z'
      numEdits: 2
      reactions: []
    id: 64c7af1faf935d3927b6b5f9
    type: comment
  author: silverfisk
  content: 'Is there some way of sharding this model to my two GPUs?

    I have a Quadro RTX 5000 laptop GPU with 16 GB VRAM + an NVIDIA GeForce RTX 3090
    eGPU with 24GB. When loading the model it seems to only try to load it into the
    3090 GPU.


    ```shell

    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB
    (GPU 0; 23.69 GiB total capacity; 22.05 GiB already allocated; 117.69 MiB free;
    23.21 GiB reserved in total by PyTorch)

    ```


    I thought the model would be split automagically since I''ve seen other smaller
    models loads use both GPUs with the same code. But maybe this is a newbie question,
    I''ve just gotten started with this yesterday.'
  created_at: 2023-07-31 11:54:55+00:00
  edited: true
  hidden: false
  id: 64c7af1faf935d3927b6b5f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60ed9cb66ab37c2080f2d5d83d80f194.svg
      fullname: 'Tom '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: silverfisk
      type: user
    createdAt: '2023-07-31T13:33:33.000Z'
    data:
      edited: true
      editors:
      - silverfisk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8401505947113037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60ed9cb66ab37c2080f2d5d83d80f194.svg
          fullname: 'Tom '
          isHf: false
          isPro: false
          name: silverfisk
          type: user
        html: "<p>Update:<br>I got it to load without OOM now. Now it's very slow,\
          \ but at least not OOM so far. Adding it here in case it is useful for someone\
          \ else.<br>The <code>device_map=\"auto\",</code> below did it:</p>\n<pre><code\
          \ class=\"language-python\">model = AutoGPTQForCausalLM.from_quantized(\n\
          \                model_id,\n                model_basename=model_basename,\n\
          \                use_safetensors=<span class=\"hljs-literal\">True</span>,\n\
          \                trust_remote_code=<span class=\"hljs-literal\">True</span>,\n\
          \                device_map=<span class=\"hljs-string\">\"auto\"</span>,\n\
          \                use_triton=<span class=\"hljs-literal\">False</span>,\n\
          \                quantize_config=<span class=\"hljs-literal\">None</span>,\n\
          \            )\n</code></pre>\n"
        raw: "Update:\nI got it to load without OOM now. Now it's very slow, but at\
          \ least not OOM so far. Adding it here in case it is useful for someone\
          \ else.\nThe ```device_map=\"auto\",``` below did it:\n\n```python\nmodel\
          \ = AutoGPTQForCausalLM.from_quantized(\n                model_id,\n   \
          \             model_basename=model_basename,\n                use_safetensors=True,\n\
          \                trust_remote_code=True,\n                device_map=\"\
          auto\",\n                use_triton=False,\n                quantize_config=None,\n\
          \            )\n```"
        updatedAt: '2023-07-31T13:34:57.963Z'
      numEdits: 1
      reactions: []
    id: 64c7b82dca9e7f2fb66830d1
    type: comment
  author: silverfisk
  content: "Update:\nI got it to load without OOM now. Now it's very slow, but at\
    \ least not OOM so far. Adding it here in case it is useful for someone else.\n\
    The ```device_map=\"auto\",``` below did it:\n\n```python\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
    \                model_id,\n                model_basename=model_basename,\n \
    \               use_safetensors=True,\n                trust_remote_code=True,\n\
    \                device_map=\"auto\",\n                use_triton=False,\n   \
    \             quantize_config=None,\n            )\n```"
  created_at: 2023-07-31 12:33:33+00:00
  edited: true
  hidden: false
  id: 64c7b82dca9e7f2fb66830d1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/WizardLM-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Can the model be sharded over several GPUs?
