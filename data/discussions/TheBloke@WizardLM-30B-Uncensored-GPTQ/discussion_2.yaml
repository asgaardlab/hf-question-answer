!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nichedreams
conflicting_files: null
created_at: 2023-05-22 23:13:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
      fullname: King  Bull
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nichedreams
      type: user
    createdAt: '2023-05-23T00:13:51.000Z'
    data:
      edited: false
      editors:
      - nichedreams
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
          fullname: King  Bull
          isHf: false
          isPro: false
          name: nichedreams
          type: user
        html: "<p>I see this model has no groupsize and every gptq model without a\
          \ groupsize gives me an error using runpod:</p>\n<p>NO_GROUP: tl.constexpr,\
          \ BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K:\
          \ tl.constexpr, GROUP_SIZE_M: tl.constexpr):\u201C\u201D\"Compute the matrix\
          \ multiplication C = A x B.A is of shape (M, K) float16B is of shape (K//8,\
          \ N) int32C is of shape (M, N) float16scales is of shape (G, N) float16zeros\
          \ is of shape (G, N) float16g_ptr is of shape (K) int32\u201C\u201D\"infearure_per_bits\
          \ = 32 // bits</p>\n<p>Or maybe there's a setting that I'm missing, but\
          \ I'm not proficient in docker and/or linux.</p>\n"
        raw: "I see this model has no groupsize and every gptq model without a groupsize\
          \ gives me an error using runpod:\r\n\r\nNO_GROUP: tl.constexpr, BLOCK_SIZE_M:\
          \ tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\
          \ GROUP_SIZE_M: tl.constexpr):\u201C\u201D\"Compute the matrix multiplication\
          \ C = A x B.A is of shape (M, K) float16B is of shape (K//8, N) int32C is\
          \ of shape (M, N) float16scales is of shape (G, N) float16zeros is of shape\
          \ (G, N) float16g_ptr is of shape (K) int32\u201C\u201D\"infearure_per_bits\
          \ = 32 // bits\r\n\r\nOr maybe there's a setting that I'm missing, but I'm\
          \ not proficient in docker and/or linux."
        updatedAt: '2023-05-23T00:13:51.687Z'
      numEdits: 0
      reactions: []
    id: 646c053f1da1b6d027fe0ed6
    type: comment
  author: nichedreams
  content: "I see this model has no groupsize and every gptq model without a groupsize\
    \ gives me an error using runpod:\r\n\r\nNO_GROUP: tl.constexpr, BLOCK_SIZE_M:\
    \ tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M:\
    \ tl.constexpr):\u201C\u201D\"Compute the matrix multiplication C = A x B.A is\
    \ of shape (M, K) float16B is of shape (K//8, N) int32C is of shape (M, N) float16scales\
    \ is of shape (G, N) float16zeros is of shape (G, N) float16g_ptr is of shape\
    \ (K) int32\u201C\u201D\"infearure_per_bits = 32 // bits\r\n\r\nOr maybe there's\
    \ a setting that I'm missing, but I'm not proficient in docker and/or linux."
  created_at: 2023-05-22 23:13:51+00:00
  edited: false
  hidden: false
  id: 646c053f1da1b6d027fe0ed6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T00:15:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m going to bed now but I''ll make another runpod template tomorrow
          that it will work with</p>

          '
        raw: I'm going to bed now but I'll make another runpod template tomorrow that
          it will work with
        updatedAt: '2023-05-23T00:15:45.873Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - nichedreams
        - miloice2022
        - jules241
    id: 646c05b15d68f5c15a377a7a
    type: comment
  author: TheBloke
  content: I'm going to bed now but I'll make another runpod template tomorrow that
    it will work with
  created_at: 2023-05-22 23:15:45+00:00
  edited: false
  hidden: false
  id: 646c05b15d68f5c15a377a7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
      fullname: King  Bull
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nichedreams
      type: user
    createdAt: '2023-05-23T00:21:55.000Z'
    data:
      edited: false
      editors:
      - nichedreams
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
          fullname: King  Bull
          isHf: false
          isPro: false
          name: nichedreams
          type: user
        html: '<p>That is great news, thank you. Quick side question about your old
          template, when I click apply and restart interface in webui after enabling
          extensions or changing webui gui (from notebook to chat for example) it
          always crashes it. Is that an inherent limitation of the docker template
          setup or am I not doing something correctly?</p>

          '
        raw: That is great news, thank you. Quick side question about your old template,
          when I click apply and restart interface in webui after enabling extensions
          or changing webui gui (from notebook to chat for example) it always crashes
          it. Is that an inherent limitation of the docker template setup or am I
          not doing something correctly?
        updatedAt: '2023-05-23T00:21:55.290Z'
      numEdits: 0
      reactions: []
    id: 646c07235d68f5c15a37a316
    type: comment
  author: nichedreams
  content: That is great news, thank you. Quick side question about your old template,
    when I click apply and restart interface in webui after enabling extensions or
    changing webui gui (from notebook to chat for example) it always crashes it. Is
    that an inherent limitation of the docker template setup or am I not doing something
    correctly?
  created_at: 2023-05-22 23:21:55+00:00
  edited: false
  hidden: false
  id: 646c07235d68f5c15a37a316
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b175ec915996e1343f9d796e60891cad.svg
      fullname: jul kul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jules241
      type: user
    createdAt: '2023-05-23T13:27:50.000Z'
    data:
      edited: false
      editors:
      - jules241
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b175ec915996e1343f9d796e60891cad.svg
          fullname: jul kul
          isHf: false
          isPro: false
          name: jules241
          type: user
        html: '<p>Hi there, quick side question, where can i find these runpod templates?
          Would also be very interested. Currently running my own ones, but on most
          hosts i get gpu enumerate errors as there is no way to run a docker container
          on runpod and set the --gpus all flag.</p>

          '
        raw: Hi there, quick side question, where can i find these runpod templates?
          Would also be very interested. Currently running my own ones, but on most
          hosts i get gpu enumerate errors as there is no way to run a docker container
          on runpod and set the --gpus all flag.
        updatedAt: '2023-05-23T13:27:50.329Z'
      numEdits: 0
      reactions: []
    id: 646cbf5687ed262149b1e551
    type: comment
  author: jules241
  content: Hi there, quick side question, where can i find these runpod templates?
    Would also be very interested. Currently running my own ones, but on most hosts
    i get gpu enumerate errors as there is no way to run a docker container on runpod
    and set the --gpus all flag.
  created_at: 2023-05-23 12:27:50+00:00
  edited: false
  hidden: false
  id: 646cbf5687ed262149b1e551
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b175ec915996e1343f9d796e60891cad.svg
      fullname: jul kul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jules241
      type: user
    createdAt: '2023-05-23T13:41:46.000Z'
    data:
      edited: false
      editors:
      - jules241
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b175ec915996e1343f9d796e60891cad.svg
          fullname: jul kul
          isHf: false
          isPro: false
          name: jules241
          type: user
        html: '<p>Also, is the Dockerfile to your image for the runpod template public?
          Been trying to find thaz one but no luck. Feel like im missing one little
          piece to be able to deploy my container reliably.</p>

          '
        raw: Also, is the Dockerfile to your image for the runpod template public?
          Been trying to find thaz one but no luck. Feel like im missing one little
          piece to be able to deploy my container reliably.
        updatedAt: '2023-05-23T13:41:46.822Z'
      numEdits: 0
      reactions: []
    id: 646cc29ac23858f13931dedb
    type: comment
  author: jules241
  content: Also, is the Dockerfile to your image for the runpod template public? Been
    trying to find thaz one but no luck. Feel like im missing one little piece to
    be able to deploy my container reliably.
  created_at: 2023-05-23 12:41:46+00:00
  edited: false
  hidden: false
  id: 646cc29ac23858f13931dedb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-23T14:28:45.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>We can probably help you over on the openaccess ai collective discord.
          <a rel="nofollow" href="https://discord.gg/Y24CzatG">https://discord.gg/Y24CzatG</a></p>

          '
        raw: We can probably help you over on the openaccess ai collective discord.
          https://discord.gg/Y24CzatG
        updatedAt: '2023-05-23T14:28:45.075Z'
      numEdits: 0
      reactions: []
    id: 646ccd9dc51833760b555b38
    type: comment
  author: winglian
  content: We can probably help you over on the openaccess ai collective discord.
    https://discord.gg/Y24CzatG
  created_at: 2023-05-23 13:28:45+00:00
  edited: false
  hidden: false
  id: 646ccd9dc51833760b555b38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T16:01:06.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK I have updated the one click template to now use the older ooba\
          \ CUDA GPTQ-for-LLaMA fork, for full compatibility with this and all the\
          \ other models I've released recently.</p>\n<p>I have tested it with WizardLM\
          \ 30B GPTQ and it works fine on a 24GB GPU, eg 3090.</p>\n<p><strong>GGML\
          \ Support</strong></p>\n<p>I've also added support for CUDA accelerated\
          \ GGML models. Download any GGML model and you can apply parameters like\
          \ these to offload to the GPU:</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/3XDUEQQM87gTPqwRXov84.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/3XDUEQQM87gTPqwRXov84.png\"\
          ></a></p>\n<p><strong>When to use GGML vs GPTQ?</strong></p>\n<p>Generally\
          \ speaking, if you have enough VRAM to load the model fully (including context),\
          \ I would use GPTQ.  In my testing it's faster than GGML in that scenario.</p>\n\
          <p>But if you want to load a model you don't have enough VRAM for - eg a\
          \ 65B model on a 24GB card, or a 30B model on a 16GB card - then GGML will\
          \ be much faster.  For example, on a 16GB GPU you can load  WizardLM 30B\
          \ GGML with 50 layers offloaded (which uses around 15.5GB VRAM) and get\
          \ 4-5 tokens/s, compared to ~1/token.s for GPTQ.  GPTQ/pytorch are really\
          \ slow in scenarios where the whole model can't be loaded into VRAM.</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;nichedreams&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nichedreams\">@<span\
          \ class=\"underline\">nichedreams</span></a></span>\n\n\t</span></span>\
          \ I haven't managed to fix the issue where you can't reload the UI. I'm\
          \ not quite sure why it happens.  I will keep investigating.</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;jules241&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jules241\">@<span class=\"\
          underline\">jules241</span></a></span>\n\n\t</span></span> you can find\
          \ the Runpod template here: <a rel=\"nofollow\" href=\"https://runpod.io/gsc?template=qk29nkmbfr&amp;ref=eexqfacd\"\
          >https://runpod.io/gsc?template=qk29nkmbfr&amp;ref=eexqfacd</a>   It's dead\
          \ easy to use. Just click that link and then you'll be at the normal Runpod\
          \ screen for selecting a pod, with that template selected.  Read the README\
          \ for further instructions on how to easily download and use GPTQ models.\
          \   And you can also find it by browsing the Templates section on Runpod,\
          \ which would allow you to copy it if you want to modify it (eg to change\
          \ the disk allocation defaults).</p>\n"
        raw: 'OK I have updated the one click template to now use the older ooba CUDA
          GPTQ-for-LLaMA fork, for full compatibility with this and all the other
          models I''ve released recently.


          I have tested it with WizardLM 30B GPTQ and it works fine on a 24GB GPU,
          eg 3090.


          **GGML Support**


          I''ve also added support for CUDA accelerated GGML models. Download any
          GGML model and you can apply parameters like these to offload to the GPU:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/3XDUEQQM87gTPqwRXov84.png)


          **When to use GGML vs GPTQ?**


          Generally speaking, if you have enough VRAM to load the model fully (including
          context), I would use GPTQ.  In my testing it''s faster than GGML in that
          scenario.


          But if you want to load a model you don''t have enough VRAM for - eg a 65B
          model on a 24GB card, or a 30B model on a 16GB card - then GGML will be
          much faster.  For example, on a 16GB GPU you can load  WizardLM 30B GGML
          with 50 layers offloaded (which uses around 15.5GB VRAM) and get 4-5 tokens/s,
          compared to ~1/token.s for GPTQ.  GPTQ/pytorch are really slow in scenarios
          where the whole model can''t be loaded into VRAM.



          @nichedreams I haven''t managed to fix the issue where you can''t reload
          the UI. I''m not quite sure why it happens.  I will keep investigating.


          @jules241 you can find the Runpod template here: https://runpod.io/gsc?template=qk29nkmbfr&ref=eexqfacd   It''s
          dead easy to use. Just click that link and then you''ll be at the normal
          Runpod screen for selecting a pod, with that template selected.  Read the
          README for further instructions on how to easily download and use GPTQ models.   And
          you can also find it by browsing the Templates section on Runpod, which
          would allow you to copy it if you want to modify it (eg to change the disk
          allocation defaults).'
        updatedAt: '2023-05-23T16:05:15.420Z'
      numEdits: 3
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - miloice2022
        - GrantPal
    id: 646ce342e0c5e395734b2263
    type: comment
  author: TheBloke
  content: 'OK I have updated the one click template to now use the older ooba CUDA
    GPTQ-for-LLaMA fork, for full compatibility with this and all the other models
    I''ve released recently.


    I have tested it with WizardLM 30B GPTQ and it works fine on a 24GB GPU, eg 3090.


    **GGML Support**


    I''ve also added support for CUDA accelerated GGML models. Download any GGML model
    and you can apply parameters like these to offload to the GPU:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/3XDUEQQM87gTPqwRXov84.png)


    **When to use GGML vs GPTQ?**


    Generally speaking, if you have enough VRAM to load the model fully (including
    context), I would use GPTQ.  In my testing it''s faster than GGML in that scenario.


    But if you want to load a model you don''t have enough VRAM for - eg a 65B model
    on a 24GB card, or a 30B model on a 16GB card - then GGML will be much faster.  For
    example, on a 16GB GPU you can load  WizardLM 30B GGML with 50 layers offloaded
    (which uses around 15.5GB VRAM) and get 4-5 tokens/s, compared to ~1/token.s for
    GPTQ.  GPTQ/pytorch are really slow in scenarios where the whole model can''t
    be loaded into VRAM.



    @nichedreams I haven''t managed to fix the issue where you can''t reload the UI.
    I''m not quite sure why it happens.  I will keep investigating.


    @jules241 you can find the Runpod template here: https://runpod.io/gsc?template=qk29nkmbfr&ref=eexqfacd   It''s
    dead easy to use. Just click that link and then you''ll be at the normal Runpod
    screen for selecting a pod, with that template selected.  Read the README for
    further instructions on how to easily download and use GPTQ models.   And you
    can also find it by browsing the Templates section on Runpod, which would allow
    you to copy it if you want to modify it (eg to change the disk allocation defaults).'
  created_at: 2023-05-23 15:01:06+00:00
  edited: true
  hidden: false
  id: 646ce342e0c5e395734b2263
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
      fullname: King  Bull
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nichedreams
      type: user
    createdAt: '2023-05-23T22:20:51.000Z'
    data:
      edited: false
      editors:
      - nichedreams
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
          fullname: King  Bull
          isHf: false
          isPro: false
          name: nichedreams
          type: user
        html: '<p>Perfect! Last question, would you know how I can send commandline
          arguments when starting the pod (to avoid having to set settings and reload
          interface)?</p>

          '
        raw: Perfect! Last question, would you know how I can send commandline arguments
          when starting the pod (to avoid having to set settings and reload interface)?
        updatedAt: '2023-05-23T22:20:51.581Z'
      numEdits: 0
      reactions: []
    id: 646d3c43e0c5e3957359576d
    type: comment
  author: nichedreams
  content: Perfect! Last question, would you know how I can send commandline arguments
    when starting the pod (to avoid having to set settings and reload interface)?
  created_at: 2023-05-23 21:20:51+00:00
  edited: false
  hidden: false
  id: 646d3c43e0c5e3957359576d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardLM-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Will this work with the Local LLMs One-Click UI runpod?
