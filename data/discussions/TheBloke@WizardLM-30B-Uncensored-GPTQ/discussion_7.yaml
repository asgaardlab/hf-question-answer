!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sumuks
conflicting_files: null
created_at: 2023-05-24 01:12:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf5ea4e2aa5649c460e7340fd6929f2a.svg
      fullname: Sumuk Shashidhar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sumuks
      type: user
    createdAt: '2023-05-24T02:12:50.000Z'
    data:
      edited: false
      editors:
      - sumuks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf5ea4e2aa5649c460e7340fd6929f2a.svg
          fullname: Sumuk Shashidhar
          isHf: false
          isPro: false
          name: sumuks
          type: user
        html: '<p>Hey Tom! </p>

          <p>Thank you so much for the work that you''re doing. </p>

          <p>As title. With a powerful enough card (for context, 24G VRAM), would
          it make more sense to run an unquantized 13B parameter model, or a 4bit
          quantized 30B model? I think this question actually translates to - how
          much does quantization affect performance. </p>

          <p>I''ve seen some of the synthetic benchmarks and perplexity scores, but
          I know that doesn''t usually translate 1-1 for real world tasks. I also
          don''t see a lot of evaluations done from an open ended content generation
          perspective, so I''d love any anecdotal evidence that you may have come
          across.</p>

          <p>Thank you once again!</p>

          '
        raw: "Hey Tom! \r\n\r\nThank you so much for the work that you're doing. \r\
          \n\r\nAs title. With a powerful enough card (for context, 24G VRAM), would\
          \ it make more sense to run an unquantized 13B parameter model, or a 4bit\
          \ quantized 30B model? I think this question actually translates to - how\
          \ much does quantization affect performance. \r\n\r\nI've seen some of the\
          \ synthetic benchmarks and perplexity scores, but I know that doesn't usually\
          \ translate 1-1 for real world tasks. I also don't see a lot of evaluations\
          \ done from an open ended content generation perspective, so I'd love any\
          \ anecdotal evidence that you may have come across.\r\n\r\nThank you once\
          \ again!"
        updatedAt: '2023-05-24T02:12:50.637Z'
      numEdits: 0
      reactions: []
    id: 646d72a24e59b82995e2a4cd
    type: comment
  author: sumuks
  content: "Hey Tom! \r\n\r\nThank you so much for the work that you're doing. \r\n\
    \r\nAs title. With a powerful enough card (for context, 24G VRAM), would it make\
    \ more sense to run an unquantized 13B parameter model, or a 4bit quantized 30B\
    \ model? I think this question actually translates to - how much does quantization\
    \ affect performance. \r\n\r\nI've seen some of the synthetic benchmarks and perplexity\
    \ scores, but I know that doesn't usually translate 1-1 for real world tasks.\
    \ I also don't see a lot of evaluations done from an open ended content generation\
    \ perspective, so I'd love any anecdotal evidence that you may have come across.\r\
    \n\r\nThank you once again!"
  created_at: 2023-05-24 01:12:50+00:00
  edited: false
  hidden: false
  id: 646d72a24e59b82995e2a4cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/405a9fedf88859e119360b79c4365259.svg
      fullname: Co0ode
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Llacer
      type: user
    createdAt: '2023-05-24T02:14:44.000Z'
    data:
      edited: false
      editors:
      - Llacer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/405a9fedf88859e119360b79c4365259.svg
          fullname: Co0ode
          isHf: false
          isPro: false
          name: Llacer
          type: user
        html: '<p>Just try both and see what you like.  Why would you have to ask
          this? If you have the hardware just try them with your own benchmarks.</p>

          '
        raw: Just try both and see what you like.  Why would you have to ask this?
          If you have the hardware just try them with your own benchmarks.
        updatedAt: '2023-05-24T02:14:44.357Z'
      numEdits: 0
      reactions: []
    id: 646d73142abe5323fe264568
    type: comment
  author: Llacer
  content: Just try both and see what you like.  Why would you have to ask this? If
    you have the hardware just try them with your own benchmarks.
  created_at: 2023-05-24 01:14:44+00:00
  edited: false
  hidden: false
  id: 646d73142abe5323fe264568
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf5ea4e2aa5649c460e7340fd6929f2a.svg
      fullname: Sumuk Shashidhar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sumuks
      type: user
    createdAt: '2023-05-24T02:20:12.000Z'
    data:
      edited: false
      editors:
      - sumuks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf5ea4e2aa5649c460e7340fd6929f2a.svg
          fullname: Sumuk Shashidhar
          isHf: false
          isPro: false
          name: sumuks
          type: user
        html: "<p>In my experience, benchmark performance has not exactly been perfectly\
          \ correlated with real world performance. I remember <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> mentioning in another discussion (perhaps on a different\
          \ release), that he had some experience with this, so I was curious regarding\
          \ his thought process. </p>\n<p>That being said, I'm also planning to conduct\
          \ some research on some metrics to evaluate open ended content generation,\
          \ (perhaps something like <a rel=\"nofollow\" href=\"https://aclanthology.org/2021.acl-long.500.pdf\"\
          >https://aclanthology.org/2021.acl-long.500.pdf</a> )</p>\n"
        raw: "In my experience, benchmark performance has not exactly been perfectly\
          \ correlated with real world performance. I remember @TheBloke mentioning\
          \ in another discussion (perhaps on a different release), that he had some\
          \ experience with this, so I was curious regarding his thought process.\
          \ \n\nThat being said, I'm also planning to conduct some research on some\
          \ metrics to evaluate open ended content generation, (perhaps something\
          \ like https://aclanthology.org/2021.acl-long.500.pdf )"
        updatedAt: '2023-05-24T02:20:12.910Z'
      numEdits: 0
      reactions: []
    id: 646d745c4a2db7744388edfa
    type: comment
  author: sumuks
  content: "In my experience, benchmark performance has not exactly been perfectly\
    \ correlated with real world performance. I remember @TheBloke mentioning in another\
    \ discussion (perhaps on a different release), that he had some experience with\
    \ this, so I was curious regarding his thought process. \n\nThat being said, I'm\
    \ also planning to conduct some research on some metrics to evaluate open ended\
    \ content generation, (perhaps something like https://aclanthology.org/2021.acl-long.500.pdf\
    \ )"
  created_at: 2023-05-24 01:20:12+00:00
  edited: false
  hidden: false
  id: 646d745c4a2db7744388edfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/834acb53a59f1b614b02d61a3b7fb387.svg
      fullname: Mihail
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MihailRus
      type: user
    createdAt: '2023-07-11T22:20:56.000Z'
    data:
      edited: false
      editors:
      - MihailRus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9829490780830383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/834acb53a59f1b614b02d61a3b7fb387.svg
          fullname: Mihail
          isHf: false
          isPro: false
          name: MihailRus
          type: user
        html: '<p>sumuks, could you tell us what is more productive in the end?</p>

          '
        raw: sumuks, could you tell us what is more productive in the end?
        updatedAt: '2023-07-11T22:20:56.464Z'
      numEdits: 0
      reactions: []
    id: 64add5c87b5ff762771b28bd
    type: comment
  author: MihailRus
  content: sumuks, could you tell us what is more productive in the end?
  created_at: 2023-07-11 21:20:56+00:00
  edited: false
  hidden: false
  id: 64add5c87b5ff762771b28bd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/WizardLM-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Model Performance Curiosity
