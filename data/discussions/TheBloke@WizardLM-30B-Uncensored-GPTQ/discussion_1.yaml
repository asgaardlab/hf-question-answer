!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DaveScream
conflicting_files: null
created_at: 2023-05-22 15:46:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9ddf10b2290f0e1f8b78a2ff5444605.svg
      fullname: Anton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DaveScream
      type: user
    createdAt: '2023-05-22T16:46:29.000Z'
    data:
      edited: false
      editors:
      - DaveScream
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9ddf10b2290f0e1f8b78a2ff5444605.svg
          fullname: Anton
          isHf: false
          isPro: false
          name: DaveScream
          type: user
        html: '<p>bump</p>

          '
        raw: bump
        updatedAt: '2023-05-22T16:46:29.807Z'
      numEdits: 0
      reactions: []
    id: 646b9c65e5abcbf670a1435f
    type: comment
  author: DaveScream
  content: bump
  created_at: 2023-05-22 15:46:29+00:00
  edited: false
  hidden: false
  id: 646b9c65e5abcbf670a1435f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-22T17:01:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Needs 24GB VRAM to load entirely on the GPU.  You can try using
          text-generation-webui''s pre_layer feature to load some layers on the GPU,
          some on the CPU.  try pre_layer 30 as a starting figure.</p>

          <p>I can''t remember if pre_layer is the number of layers on the CPU, or
          the number on the GPU. I think it means number of layers on the GPU, so
          if you get out-of-memory with 30, try decreasing it to 20.</p>

          '
        raw: 'Needs 24GB VRAM to load entirely on the GPU.  You can try using text-generation-webui''s
          pre_layer feature to load some layers on the GPU, some on the CPU.  try
          pre_layer 30 as a starting figure.


          I can''t remember if pre_layer is the number of layers on the CPU, or the
          number on the GPU. I think it means number of layers on the GPU, so if you
          get out-of-memory with 30, try decreasing it to 20.'
        updatedAt: '2023-05-22T17:01:54.720Z'
      numEdits: 0
      reactions: []
    id: 646ba002ed22827213346f1b
    type: comment
  author: TheBloke
  content: 'Needs 24GB VRAM to load entirely on the GPU.  You can try using text-generation-webui''s
    pre_layer feature to load some layers on the GPU, some on the CPU.  try pre_layer
    30 as a starting figure.


    I can''t remember if pre_layer is the number of layers on the CPU, or the number
    on the GPU. I think it means number of layers on the GPU, so if you get out-of-memory
    with 30, try decreasing it to 20.'
  created_at: 2023-05-22 16:01:54+00:00
  edited: false
  hidden: false
  id: 646ba002ed22827213346f1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-22T18:18:09.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>the hf 16fp version requires 63B VRAM. the GPTQ-4bit 128 group size
          version needs about 25GB, the GPTQ-4bit 1024 group size just fit in 24GB
          card but ooba  has trouble in dealing with 1024 group size though.</p>

          <p>The 30B and above versions of LLaMA are pretty unapproachable for commodity
          devices at this moment.</p>

          '
        raw: 'the hf 16fp version requires 63B VRAM. the GPTQ-4bit 128 group size
          version needs about 25GB, the GPTQ-4bit 1024 group size just fit in 24GB
          card but ooba  has trouble in dealing with 1024 group size though.


          The 30B and above versions of LLaMA are pretty unapproachable for commodity
          devices at this moment.'
        updatedAt: '2023-05-22T18:18:09.749Z'
      numEdits: 0
      reactions: []
    id: 646bb1e1db697c798a461752
    type: comment
  author: Yhyu13
  content: 'the hf 16fp version requires 63B VRAM. the GPTQ-4bit 128 group size version
    needs about 25GB, the GPTQ-4bit 1024 group size just fit in 24GB card but ooba  has
    trouble in dealing with 1024 group size though.


    The 30B and above versions of LLaMA are pretty unapproachable for commodity devices
    at this moment.'
  created_at: 2023-05-22 17:18:09+00:00
  edited: false
  hidden: false
  id: 646bb1e1db697c798a461752
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-22T18:41:41.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This version used no group size so it will definitely fit in 24gb.
          I stopped doing 1024 because for 30b will OOM with long responses. Group
          size none is reliable in 24 though</p>

          '
        raw: This version used no group size so it will definitely fit in 24gb. I
          stopped doing 1024 because for 30b will OOM with long responses. Group size
          none is reliable in 24 though
        updatedAt: '2023-05-22T18:41:54.586Z'
      numEdits: 1
      reactions: []
    id: 646bb765ed22827213383671
    type: comment
  author: TheBloke
  content: This version used no group size so it will definitely fit in 24gb. I stopped
    doing 1024 because for 30b will OOM with long responses. Group size none is reliable
    in 24 though
  created_at: 2023-05-22 17:41:41+00:00
  edited: true
  hidden: false
  id: 646bb765ed22827213383671
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/938ea0711fdf4cab5ef7d124043c3235.svg
      fullname: All In
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AllInn
      type: user
    createdAt: '2023-05-22T19:11:08.000Z'
    data:
      edited: false
      editors:
      - AllInn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/938ea0711fdf4cab5ef7d124043c3235.svg
          fullname: All In
          isHf: false
          isPro: false
          name: AllInn
          type: user
        html: '<p>I have a 3090 and still getting error about memory when loading
          it up.</p>

          '
        raw: I have a 3090 and still getting error about memory when loading it up.
        updatedAt: '2023-05-22T19:11:08.052Z'
      numEdits: 0
      reactions: []
    id: 646bbe4cdb697c798a48204e
    type: comment
  author: AllInn
  content: I have a 3090 and still getting error about memory when loading it up.
  created_at: 2023-05-22 18:11:08+00:00
  edited: false
  hidden: false
  id: 646bbe4cdb697c798a48204e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-22T19:40:48.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Then that must be something else.  It loads fine on a 24GB 4090
          for me, testing with the ooba GPTQ-for-LLaMA CUDA fork.  </p>

          <p>Loading the model uses around 18GB VRAM, and then this grows as the response
          comes back, up to a maximum of 2000 tokens which uses 24203 MiB, leaving
          13 MiB free :)</p>

          <pre><code>Output generated in 249.28 seconds (8.02 tokens/s, 1999 tokens,
          context 42, seed 953298877)


          timestamp, name, driver_version, pcie.link.gen.max, pcie.link.gen.current,
          utilization.gpu [%], utilization.memory [%], memory.total [MiB], memory.free
          [MiB], memory.used [MiB]

          2023/05/22 19:40:04.420, NVIDIA GeForce RTX 4090, 525.105.17, 4, 4, 20 %,
          16 %, 24564 MiB, 13 MiB, 24203 MiB

          </code></pre>

          '
        raw: "Then that must be something else.  It loads fine on a 24GB 4090 for\
          \ me, testing with the ooba GPTQ-for-LLaMA CUDA fork.  \n\nLoading the model\
          \ uses around 18GB VRAM, and then this grows as the response comes back,\
          \ up to a maximum of 2000 tokens which uses 24203 MiB, leaving 13 MiB free\
          \ :)\n\n```\nOutput generated in 249.28 seconds (8.02 tokens/s, 1999 tokens,\
          \ context 42, seed 953298877)\n\ntimestamp, name, driver_version, pcie.link.gen.max,\
          \ pcie.link.gen.current, utilization.gpu [%], utilization.memory [%], memory.total\
          \ [MiB], memory.free [MiB], memory.used [MiB]\n2023/05/22 19:40:04.420,\
          \ NVIDIA GeForce RTX 4090, 525.105.17, 4, 4, 20 %, 16 %, 24564 MiB, 13 MiB,\
          \ 24203 MiB\n```"
        updatedAt: '2023-05-22T19:40:48.295Z'
      numEdits: 0
      reactions: []
    id: 646bc540ed228272133a6f45
    type: comment
  author: TheBloke
  content: "Then that must be something else.  It loads fine on a 24GB 4090 for me,\
    \ testing with the ooba GPTQ-for-LLaMA CUDA fork.  \n\nLoading the model uses\
    \ around 18GB VRAM, and then this grows as the response comes back, up to a maximum\
    \ of 2000 tokens which uses 24203 MiB, leaving 13 MiB free :)\n\n```\nOutput generated\
    \ in 249.28 seconds (8.02 tokens/s, 1999 tokens, context 42, seed 953298877)\n\
    \ntimestamp, name, driver_version, pcie.link.gen.max, pcie.link.gen.current, utilization.gpu\
    \ [%], utilization.memory [%], memory.total [MiB], memory.free [MiB], memory.used\
    \ [MiB]\n2023/05/22 19:40:04.420, NVIDIA GeForce RTX 4090, 525.105.17, 4, 4, 20\
    \ %, 16 %, 24564 MiB, 13 MiB, 24203 MiB\n```"
  created_at: 2023-05-22 18:40:48+00:00
  edited: false
  hidden: false
  id: 646bc540ed228272133a6f45
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14a1c5191351506d44c2857411304dc8.svg
      fullname: Brian Moyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rivaidan
      type: user
    createdAt: '2023-05-22T19:45:44.000Z'
    data:
      edited: false
      editors:
      - Rivaidan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14a1c5191351506d44c2857411304dc8.svg
          fullname: Brian Moyer
          isHf: false
          isPro: false
          name: Rivaidan
          type: user
        html: "<p>I think their problem is normal ram. For me and my 4090 it first\
          \ loads entirely into my normal ram which maxed out my 32GB, then it shifts\
          \ to the gpu. So if they don\u2019t have enough system ram I don\u2019t\
          \ even think it tried to send to gpu</p>\n"
        raw: "I think their problem is normal ram. For me and my 4090 it first loads\
          \ entirely into my normal ram which maxed out my 32GB, then it shifts to\
          \ the gpu. So if they don\u2019t have enough system ram I don\u2019t even\
          \ think it tried to send to gpu"
        updatedAt: '2023-05-22T19:45:44.848Z'
      numEdits: 0
      reactions: []
    id: 646bc668db697c798a4971e7
    type: comment
  author: Rivaidan
  content: "I think their problem is normal ram. For me and my 4090 it first loads\
    \ entirely into my normal ram which maxed out my 32GB, then it shifts to the gpu.\
    \ So if they don\u2019t have enough system ram I don\u2019t even think it tried\
    \ to send to gpu"
  created_at: 2023-05-22 18:45:44+00:00
  edited: false
  hidden: false
  id: 646bc668db697c798a4971e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-22T19:46:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah yes, that could be it. You generally always need at least as
          much RAM as you have VRAM.</p>

          '
        raw: Ah yes, that could be it. You generally always need at least as much
          RAM as you have VRAM.
        updatedAt: '2023-05-22T19:46:47.769Z'
      numEdits: 0
      reactions: []
    id: 646bc6a75d68f5c15a2fcf0a
    type: comment
  author: TheBloke
  content: Ah yes, that could be it. You generally always need at least as much RAM
    as you have VRAM.
  created_at: 2023-05-22 18:46:47+00:00
  edited: false
  hidden: false
  id: 646bc6a75d68f5c15a2fcf0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
      fullname: Pluck
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BGLuck
      type: user
    createdAt: '2023-05-22T19:51:19.000Z'
    data:
      edited: false
      editors:
      - BGLuck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
          fullname: Pluck
          isHf: false
          isPro: false
          name: BGLuck
          type: user
        html: '<p>If using ooba, you need a lot of RAM to just load the model (or
          filepage if you don''t have enough RAM), for 65b models I need like 140+GB
          of RAM (between RAM and pagefile size)</p>

          '
        raw: If using ooba, you need a lot of RAM to just load the model (or filepage
          if you don't have enough RAM), for 65b models I need like 140+GB of RAM
          (between RAM and pagefile size)
        updatedAt: '2023-05-22T19:51:19.577Z'
      numEdits: 0
      reactions: []
    id: 646bc7b75d68f5c15a2ffd6e
    type: comment
  author: BGLuck
  content: If using ooba, you need a lot of RAM to just load the model (or filepage
    if you don't have enough RAM), for 65b models I need like 140+GB of RAM (between
    RAM and pagefile size)
  created_at: 2023-05-22 18:51:19+00:00
  edited: false
  hidden: false
  id: 646bc7b75d68f5c15a2ffd6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/938ea0711fdf4cab5ef7d124043c3235.svg
      fullname: All In
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AllInn
      type: user
    createdAt: '2023-05-22T20:22:19.000Z'
    data:
      edited: true
      editors:
      - AllInn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/938ea0711fdf4cab5ef7d124043c3235.svg
          fullname: All In
          isHf: false
          isPro: false
          name: AllInn
          type: user
        html: "<p>Interesting, I have 32gb of ram 31.7 usable<br>Here is the error\
          \ stack:<br>Traceback (most recent call last):<br>File \u201CE:\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 67, in load_model_wrapper<br>shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)<br>File \u201CE:\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 159, in load_model<br>model\
          \ = load_quantized(model_name)<br>File \u201CE:\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 178, in load_quantized<br>model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>File\
          \ \u201CE:\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 52, in _load_quant<br>model = AutoModelForCausalLM.from_config(config,\
          \ trust_remote_code=shared.args.trust_remote_code)<br>File \u201CE:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 411, in from_config<br>return model_class._from_config(config, **kwargs)<br>File\
          \ \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 1146, in _from_config<br>model\
          \ = cls(config, **kwargs)<br>File \u201CE:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 614, in init<br>self.model = LlamaModel(config)<br>File \u201CE:\\\
          oobabooga_windows\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          llama_inference_offload.py\u201D, line 21, in init<br>super().init(config)<br>File\
          \ \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in init<br>self.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])<br>File\
          \ \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in<br>self.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])<br>File\
          \ \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 256, in init<br>self.mlp\
          \ = LlamaMLP(<br>File \u201CE:\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 151, in init<br>self.gate_proj = nn.Linear(hidden_size, intermediate_size,\
          \ bias=False)<br>File \u201CE:\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\nn\\modules\\linear.py\u201D, line 96, in init<br>self.weight\
          \ = Parameter(torch.empty((out_features, in_features), **factory_kwargs))<br>RuntimeError:\
          \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\\
          alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 238551040 bytes.</p>\n"
        raw: "Interesting, I have 32gb of ram 31.7 usable\nHere is the error stack:\n\
          Traceback (most recent call last):\nFile \u201CE:\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 67, in load_model_wrapper\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\nFile \u201CE:\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 159, in load_model\nmodel = load_quantized(model_name)\n\
          File \u201CE:\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 178, in load_quantized\nmodel = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          File \u201CE:\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 52, in _load_quant\nmodel = AutoModelForCausalLM.from_config(config,\
          \ trust_remote_code=shared.args.trust_remote_code)\nFile \u201CE:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 411, in from_config\nreturn model_class._from_config(config, **kwargs)\n\
          File \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 1146, in _from_config\nmodel\
          \ = cls(config, **kwargs)\nFile \u201CE:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 614, in init\nself.model = LlamaModel(config)\nFile \u201CE:\\oobabooga_windows\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\u201D\
          , line 21, in init\nsuper().init(config)\nFile \u201CE:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 445, in init\nself.layers = nn.ModuleList([LlamaDecoderLayer(config)\
          \ for _ in range(config.num_hidden_layers)])\nFile \u201CE:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 445, in\nself.layers = nn.ModuleList([LlamaDecoderLayer(config)\
          \ for _ in range(config.num_hidden_layers)])\nFile \u201CE:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 256, in init\nself.mlp = LlamaMLP(\nFile \u201C\
          E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 151, in init\nself.gate_proj\
          \ = nn.Linear(hidden_size, intermediate_size, bias=False)\nFile \u201CE:\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\linear.py\u201D, line 96, in init\nself.weight = Parameter(torch.empty((out_features,\
          \ in_features), **factory_kwargs))\nRuntimeError: [enforce fail at C:\\\
          cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data.\
          \ DefaultCPUAllocator: not enough memory: you tried to allocate 238551040\
          \ bytes."
        updatedAt: '2023-05-22T20:29:28.359Z'
      numEdits: 1
      reactions: []
    id: 646bcefbed228272133be0f7
    type: comment
  author: AllInn
  content: "Interesting, I have 32gb of ram 31.7 usable\nHere is the error stack:\n\
    Traceback (most recent call last):\nFile \u201CE:\\oobabooga_windows\\text-generation-webui\\\
    server.py\u201D, line 67, in load_model_wrapper\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\nFile \u201CE:\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 159, in load_model\nmodel = load_quantized(model_name)\n\
    File \u201CE:\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 178, in load_quantized\nmodel = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    File \u201CE:\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 52, in _load_quant\nmodel = AutoModelForCausalLM.from_config(config, trust_remote_code=shared.args.trust_remote_code)\n\
    File \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\auto\\auto_factory.py\u201D, line 411, in from_config\nreturn model_class._from_config(config,\
    \ **kwargs)\nFile \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\u201D, line 1146, in _from_config\nmodel = cls(config,\
    \ **kwargs)\nFile \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 614, in init\nself.model\
    \ = LlamaModel(config)\nFile \u201CE:\\oobabooga_windows\\text-generation-webui\\\
    repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\u201D, line 21, in init\n\
    super().init(config)\nFile \u201CE:\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D, line\
    \ 445, in init\nself.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in\
    \ range(config.num_hidden_layers)])\nFile \u201CE:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 445, in\nself.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in\
    \ range(config.num_hidden_layers)])\nFile \u201CE:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 256, in init\nself.mlp = LlamaMLP(\nFile \u201CE:\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 151, in init\nself.gate_proj = nn.Linear(hidden_size, intermediate_size,\
    \ bias=False)\nFile \u201CE:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\linear.py\u201D, line 96, in init\nself.weight = Parameter(torch.empty((out_features,\
    \ in_features), **factory_kwargs))\nRuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
    work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough\
    \ memory: you tried to allocate 238551040 bytes."
  created_at: 2023-05-22 19:22:19+00:00
  edited: true
  hidden: false
  id: 646bcefbed228272133be0f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
      fullname: Pluck
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BGLuck
      type: user
    createdAt: '2023-05-22T23:23:57.000Z'
    data:
      edited: false
      editors:
      - BGLuck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
          fullname: Pluck
          isHf: false
          isPro: false
          name: BGLuck
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;AllInn&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AllInn\">@<span class=\"\
          underline\">AllInn</span></a></span>\n\n\t</span></span> that's because\
          \ not enough RAM. Try to increase your pagefile size, probably 80GB total\
          \ between RAM + pagesize.</p>\n"
        raw: '@AllInn that''s because not enough RAM. Try to increase your pagefile
          size, probably 80GB total between RAM + pagesize.'
        updatedAt: '2023-05-22T23:23:57.347Z'
      numEdits: 0
      reactions: []
    id: 646bf98ded2282721340fce4
    type: comment
  author: BGLuck
  content: '@AllInn that''s because not enough RAM. Try to increase your pagefile
    size, probably 80GB total between RAM + pagesize.'
  created_at: 2023-05-22 22:23:57+00:00
  edited: false
  hidden: false
  id: 646bf98ded2282721340fce4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad8daf7101b78ae4226f3ade0d64bbe6.svg
      fullname: Riggity Wrckd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RiggityWrckd
      type: user
    createdAt: '2023-05-23T01:27:07.000Z'
    data:
      edited: false
      editors:
      - RiggityWrckd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad8daf7101b78ae4226f3ade0d64bbe6.svg
          fullname: Riggity Wrckd
          isHf: false
          isPro: false
          name: RiggityWrckd
          type: user
        html: '<p>These 30b can take over 64gb of your system ram,  which is why you
          need that extra pagefile/swap area. Does everyone just kill their x server
          and plug into the motherboards hdmi to get their cards vram free? Any tricks
          would be welcome :)</p>

          '
        raw: These 30b can take over 64gb of your system ram,  which is why you need
          that extra pagefile/swap area. Does everyone just kill their x server and
          plug into the motherboards hdmi to get their cards vram free? Any tricks
          would be welcome :)
        updatedAt: '2023-05-23T01:27:07.862Z'
      numEdits: 0
      reactions: []
    id: 646c166bdb697c798a532406
    type: comment
  author: RiggityWrckd
  content: These 30b can take over 64gb of your system ram,  which is why you need
    that extra pagefile/swap area. Does everyone just kill their x server and plug
    into the motherboards hdmi to get their cards vram free? Any tricks would be welcome
    :)
  created_at: 2023-05-23 00:27:07+00:00
  edited: false
  hidden: false
  id: 646c166bdb697c798a532406
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-23T12:19:04.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Is there any existing framework that allows offloading even for
          GPTQ models? In principle, this should be doable.</p>

          '
        raw: Is there any existing framework that allows offloading even for GPTQ
          models? In principle, this should be doable.
        updatedAt: '2023-05-23T12:19:04.429Z'
      numEdits: 0
      reactions: []
    id: 646caf382c29b8753adc14d4
    type: comment
  author: Yhyu13
  content: Is there any existing framework that allows offloading even for GPTQ models?
    In principle, this should be doable.
  created_at: 2023-05-23 11:19:04+00:00
  edited: false
  hidden: false
  id: 646caf382c29b8753adc14d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T12:24:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, check my first response in this thread: pre_layer in GPTQ-for-LLaMa
          supports offloading.  This is supported in the text-generation-webui UI.</p>

          <p>Set <code>pre_layer</code> to the number of layers to put on the GPU.  There
          are 60 layers in total in this model.  So eg on a 16GB card, you could try
          <code>--pre_layer 35</code> to put 35 layers on the GPU and the rest on
          the CPU.  It will be really slow though.  If you don''t have enough VRAM
          to fully load the model, I recommend trying a GGML model instead, and load
          as many layers onto the GPU eg with <code>-ngl 50</code> to put 50 layers
          on the GPU (which fits in 16GB VRAM).</p>

          <p>With GPTQ, the GPU needs enough VRAM to fit both the model, and the context.  With
          GGML and llama.cpp, GPU offloading stores the model but does not store the
          context, so you can fit more layers in a given amount of VRAM.</p>

          <p>Generally GPTQ is faster than GGML if you have enough VRAM to fully load
          the model.  But if you don''t, GGML is now faster - and it can be <em>much</em>
          faster.  Eg testing this 30B model yesterday on a 16GB A4000 GPU, I less
          than 1 token/s with <code>--pre_layer 38</code> but 4.5 tokens/s with GGML
          and llama.cpp with <code>-ngl 50</code>.</p>

          <p>Regarding multi-GPU with GPTQ:</p>

          <p>In recent versions of text-generation-webui you can also use <code>pre_layer</code>
          for multi-GPU splitting, eg <code>--pre_layer 30 30</code> to put 30 layers
          on each GPU of two GPUs.</p>

          '
        raw: 'Yes, check my first response in this thread: pre_layer in GPTQ-for-LLaMa
          supports offloading.  This is supported in the text-generation-webui UI.


          Set `pre_layer` to the number of layers to put on the GPU.  There are 60
          layers in total in this model.  So eg on a 16GB card, you could try `--pre_layer
          35` to put 35 layers on the GPU and the rest on the CPU.  It will be really
          slow though.  If you don''t have enough VRAM to fully load the model, I
          recommend trying a GGML model instead, and load as many layers onto the
          GPU eg with `-ngl 50` to put 50 layers on the GPU (which fits in 16GB VRAM).


          With GPTQ, the GPU needs enough VRAM to fit both the model, and the context.  With
          GGML and llama.cpp, GPU offloading stores the model but does not store the
          context, so you can fit more layers in a given amount of VRAM.


          Generally GPTQ is faster than GGML if you have enough VRAM to fully load
          the model.  But if you don''t, GGML is now faster - and it can be *much*
          faster.  Eg testing this 30B model yesterday on a 16GB A4000 GPU, I less
          than 1 token/s with `--pre_layer 38` but 4.5 tokens/s with GGML and llama.cpp
          with `-ngl 50`.


          Regarding multi-GPU with GPTQ:


          In recent versions of text-generation-webui you can also use `pre_layer`
          for multi-GPU splitting, eg `--pre_layer 30 30` to put 30 layers on each
          GPU of two GPUs.'
        updatedAt: '2023-05-23T12:24:35.882Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - DaveScream
        - mikeyang01
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - mikeyang01
        - escaroda
    id: 646cb0832c29b8753adc5cfe
    type: comment
  author: TheBloke
  content: 'Yes, check my first response in this thread: pre_layer in GPTQ-for-LLaMa
    supports offloading.  This is supported in the text-generation-webui UI.


    Set `pre_layer` to the number of layers to put on the GPU.  There are 60 layers
    in total in this model.  So eg on a 16GB card, you could try `--pre_layer 35`
    to put 35 layers on the GPU and the rest on the CPU.  It will be really slow though.  If
    you don''t have enough VRAM to fully load the model, I recommend trying a GGML
    model instead, and load as many layers onto the GPU eg with `-ngl 50` to put 50
    layers on the GPU (which fits in 16GB VRAM).


    With GPTQ, the GPU needs enough VRAM to fit both the model, and the context.  With
    GGML and llama.cpp, GPU offloading stores the model but does not store the context,
    so you can fit more layers in a given amount of VRAM.


    Generally GPTQ is faster than GGML if you have enough VRAM to fully load the model.  But
    if you don''t, GGML is now faster - and it can be *much* faster.  Eg testing this
    30B model yesterday on a 16GB A4000 GPU, I less than 1 token/s with `--pre_layer
    38` but 4.5 tokens/s with GGML and llama.cpp with `-ngl 50`.


    Regarding multi-GPU with GPTQ:


    In recent versions of text-generation-webui you can also use `pre_layer` for multi-GPU
    splitting, eg `--pre_layer 30 30` to put 30 layers on each GPU of two GPUs.'
  created_at: 2023-05-23 11:24:35+00:00
  edited: false
  hidden: false
  id: 646cb0832c29b8753adc5cfe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-23T17:45:59.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Regarding VRAM capacity, remember that if your card is also a primary
          display unit, it will not have full 24GB available for the model because
          some portion (~300-500MB) will be used by the OS for display.</p>

          <p>Obviously, if you run this headless, or with 2 video cards, then there
          should be no issues.</p>

          '
        raw: 'Regarding VRAM capacity, remember that if your card is also a primary
          display unit, it will not have full 24GB available for the model because
          some portion (~300-500MB) will be used by the OS for display.


          Obviously, if you run this headless, or with 2 video cards, then there should
          be no issues.'
        updatedAt: '2023-05-23T17:45:59.982Z'
      numEdits: 0
      reactions: []
    id: 646cfbd74a2db7744376e529
    type: comment
  author: mancub
  content: 'Regarding VRAM capacity, remember that if your card is also a primary
    display unit, it will not have full 24GB available for the model because some
    portion (~300-500MB) will be used by the OS for display.


    Obviously, if you run this headless, or with 2 video cards, then there should
    be no issues.'
  created_at: 2023-05-23 16:45:59+00:00
  edited: false
  hidden: false
  id: 646cfbd74a2db7744376e529
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d072eac32ecc79643db22f9cddeea188.svg
      fullname: Davide Bertolini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reezlaw
      type: user
    createdAt: '2023-06-05T13:53:10.000Z'
    data:
      edited: true
      editors:
      - Reezlaw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9576809406280518
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d072eac32ecc79643db22f9cddeea188.svg
          fullname: Davide Bertolini
          isHf: false
          isPro: false
          name: Reezlaw
          type: user
        html: '<p>This model takes up about 18GB of VRAM on my 3090. I have auto-devices
          disabled in Ooba. It fits comfortably on the GPU with some room to spare.
          System RAM has nothing to do with it (I have 32GB of that).<br>If you''re
          getting OOM errors on a 24GB card you''re probably running some other GPU-intensive
          program at the same time, otherwise I have no explanation</p>

          '
        raw: 'This model takes up about 18GB of VRAM on my 3090. I have auto-devices
          disabled in Ooba. It fits comfortably on the GPU with some room to spare.
          System RAM has nothing to do with it (I have 32GB of that).

          If you''re getting OOM errors on a 24GB card you''re probably running some
          other GPU-intensive program at the same time, otherwise I have no explanation'
        updatedAt: '2023-06-05T13:54:15.661Z'
      numEdits: 1
      reactions: []
    id: 647de8c65214d172cbb91642
    type: comment
  author: Reezlaw
  content: 'This model takes up about 18GB of VRAM on my 3090. I have auto-devices
    disabled in Ooba. It fits comfortably on the GPU with some room to spare. System
    RAM has nothing to do with it (I have 32GB of that).

    If you''re getting OOM errors on a 24GB card you''re probably running some other
    GPU-intensive program at the same time, otherwise I have no explanation'
  created_at: 2023-06-05 12:53:10+00:00
  edited: true
  hidden: false
  id: 647de8c65214d172cbb91642
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56d45b9c4a63b8903710104a2f12a39e.svg
      fullname: snake`
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: snakebaconer
      type: user
    createdAt: '2023-06-07T01:08:04.000Z'
    data:
      edited: false
      editors:
      - snakebaconer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579495787620544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56d45b9c4a63b8903710104a2f12a39e.svg
          fullname: snake`
          isHf: false
          isPro: false
          name: snakebaconer
          type: user
        html: '<blockquote>

          <p>This model takes up about 18GB of VRAM on my 3090. I have auto-devices
          disabled in Ooba. It fits comfortably on the GPU with some room to spare.
          System RAM has nothing to do with it (I have 32GB of that).<br>If you''re
          getting OOM errors on a 24GB card you''re probably running some other GPU-intensive
          program at the same time, otherwise I have no explanation</p>

          </blockquote>

          <p>What settings are you using to load the model? I have the same rig as
          you and it keeps crashing</p>

          '
        raw: '> This model takes up about 18GB of VRAM on my 3090. I have auto-devices
          disabled in Ooba. It fits comfortably on the GPU with some room to spare.
          System RAM has nothing to do with it (I have 32GB of that).

          > If you''re getting OOM errors on a 24GB card you''re probably running
          some other GPU-intensive program at the same time, otherwise I have no explanation


          What settings are you using to load the model? I have the same rig as you
          and it keeps crashing'
        updatedAt: '2023-06-07T01:08:04.584Z'
      numEdits: 0
      reactions: []
    id: 647fd8741637c1c0e6f2c81b
    type: comment
  author: snakebaconer
  content: '> This model takes up about 18GB of VRAM on my 3090. I have auto-devices
    disabled in Ooba. It fits comfortably on the GPU with some room to spare. System
    RAM has nothing to do with it (I have 32GB of that).

    > If you''re getting OOM errors on a 24GB card you''re probably running some other
    GPU-intensive program at the same time, otherwise I have no explanation


    What settings are you using to load the model? I have the same rig as you and
    it keeps crashing'
  created_at: 2023-06-07 00:08:04+00:00
  edited: false
  hidden: false
  id: 647fd8741637c1c0e6f2c81b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T01:21:04.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5352089405059814
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>With the latest text-gen-webui you really don''t have to do anything,
          AutoGPTQ is used automaticall, and unless you specify --triton it''ll default
          to CUDA.</p>

          <p>So probably:</p>

          <p>python server.py --wbits 4 --groupsize -1 --model_type LLaMA --model
          </p>

          '
        raw: 'With the latest text-gen-webui you really don''t have to do anything,
          AutoGPTQ is used automaticall, and unless you specify --triton it''ll default
          to CUDA.


          So probably:


          python server.py --wbits 4 --groupsize -1 --model_type LLaMA --model <path-to-model>'
        updatedAt: '2023-06-07T01:21:04.701Z'
      numEdits: 0
      reactions: []
    id: 647fdb801637c1c0e6f31620
    type: comment
  author: mancub
  content: 'With the latest text-gen-webui you really don''t have to do anything,
    AutoGPTQ is used automaticall, and unless you specify --triton it''ll default
    to CUDA.


    So probably:


    python server.py --wbits 4 --groupsize -1 --model_type LLaMA --model <path-to-model>'
  created_at: 2023-06-07 00:21:04+00:00
  edited: false
  hidden: false
  id: 647fdb801637c1c0e6f31620
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d072eac32ecc79643db22f9cddeea188.svg
      fullname: Davide Bertolini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reezlaw
      type: user
    createdAt: '2023-06-07T08:47:12.000Z'
    data:
      edited: false
      editors:
      - Reezlaw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9271842837333679
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d072eac32ecc79643db22f9cddeea188.svg
          fullname: Davide Bertolini
          isHf: false
          isPro: false
          name: Reezlaw
          type: user
        html: '<p>Pretty much, but I don''t even specify groupsize, then I made sure
          that auto-devices is unflagged in the UI</p>

          '
        raw: Pretty much, but I don't even specify groupsize, then I made sure that
          auto-devices is unflagged in the UI
        updatedAt: '2023-06-07T08:47:12.944Z'
      numEdits: 0
      reactions: []
    id: 6480441015bde163f4b2cfe9
    type: comment
  author: Reezlaw
  content: Pretty much, but I don't even specify groupsize, then I made sure that
    auto-devices is unflagged in the UI
  created_at: 2023-06-07 07:47:12+00:00
  edited: false
  hidden: false
  id: 6480441015bde163f4b2cfe9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56d45b9c4a63b8903710104a2f12a39e.svg
      fullname: snake`
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: snakebaconer
      type: user
    createdAt: '2023-06-09T02:17:16.000Z'
    data:
      edited: false
      editors:
      - snakebaconer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5859837532043457
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56d45b9c4a63b8903710104a2f12a39e.svg
          fullname: snake`
          isHf: false
          isPro: false
          name: snakebaconer
          type: user
        html: '<blockquote>

          <p>With the latest text-gen-webui you really don''t have to do anything,
          AutoGPTQ is used automaticall, and unless you specify --triton it''ll default
          to CUDA.</p>

          <p>So probably:</p>

          <p>python server.py --wbits 4 --groupsize -1 --model_type LLaMA --model
          </p>

          </blockquote>

          <p>I tried to follow the suggestions you made, but am not sure what I''m
          still doing wrong. I encounter this error every time:</p>

          <p>INFO:Loading TheBloke_WizardLM-30B-Uncensored-GPTQ...<br>INFO:The AutoGPTQ
          params are: {''model_basename'': ''WizardLM-30B-Uncensored-GPTQ-4bit.act-order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''use_safetensors'': True,
          ''trust_remote_code'': False, ''max_memory'': None, ''quantize_config'':
          None}<br>WARNING:The safetensors archive passed at models\TheBloke_WizardLM-30B-Uncensored-GPTQ\WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.<br>Press any key to continue . .</p>

          '
        raw: "> With the latest text-gen-webui you really don't have to do anything,\
          \ AutoGPTQ is used automaticall, and unless you specify --triton it'll default\
          \ to CUDA.\n> \n> So probably:\n> \n> python server.py --wbits 4 --groupsize\
          \ -1 --model_type LLaMA --model <path-to-model>\n\nI tried to follow the\
          \ suggestions you made, but am not sure what I'm still doing wrong. I encounter\
          \ this error every time:\n\nINFO:Loading TheBloke_WizardLM-30B-Uncensored-GPTQ...\n\
          INFO:The AutoGPTQ params are: {'model_basename': 'WizardLM-30B-Uncensored-GPTQ-4bit.act-order',\
          \ 'device': 'cuda:0', 'use_triton': False, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None}\nWARNING:The safetensors\
          \ archive passed at models\\TheBloke_WizardLM-30B-Uncensored-GPTQ\\WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nPress any key to continue . ."
        updatedAt: '2023-06-09T02:17:16.295Z'
      numEdits: 0
      reactions: []
    id: 64828bac3bf1ad0981d7264f
    type: comment
  author: snakebaconer
  content: "> With the latest text-gen-webui you really don't have to do anything,\
    \ AutoGPTQ is used automaticall, and unless you specify --triton it'll default\
    \ to CUDA.\n> \n> So probably:\n> \n> python server.py --wbits 4 --groupsize -1\
    \ --model_type LLaMA --model <path-to-model>\n\nI tried to follow the suggestions\
    \ you made, but am not sure what I'm still doing wrong. I encounter this error\
    \ every time:\n\nINFO:Loading TheBloke_WizardLM-30B-Uncensored-GPTQ...\nINFO:The\
    \ AutoGPTQ params are: {'model_basename': 'WizardLM-30B-Uncensored-GPTQ-4bit.act-order',\
    \ 'device': 'cuda:0', 'use_triton': False, 'use_safetensors': True, 'trust_remote_code':\
    \ False, 'max_memory': None, 'quantize_config': None}\nWARNING:The safetensors\
    \ archive passed at models\\TheBloke_WizardLM-30B-Uncensored-GPTQ\\WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\nPress any key to continue . ."
  created_at: 2023-06-09 01:17:16+00:00
  edited: false
  hidden: false
  id: 64828bac3bf1ad0981d7264f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6fb471022335e3ac820f8eb2cc6c00b9.svg
      fullname: gpusmatter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gpusmatter
      type: user
    createdAt: '2023-06-11T10:49:16.000Z'
    data:
      edited: false
      editors:
      - gpusmatter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9597351551055908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6fb471022335e3ac820f8eb2cc6c00b9.svg
          fullname: gpusmatter
          isHf: false
          isPro: false
          name: gpusmatter
          type: user
        html: '<p>Just adding another data point RE: not enough system RAM</p>

          <p>I had a similar issue with my setup, where I have more than enough VRAM
          but wasn''t able to load the modal because text-gen-webui keeps running
          out of system memory (RAM). For me I just had to increase my virtual memory
          (swap if you on linux). And it fixed things. Also just watching the RAM
          and VRAM usage while the modal is loaded, I observed that it first loaded
          the modal (or more likely a part of it) to RAM (and swap because there wasn''t
          enough RAM) and then it would load it to VRAM.</p>

          '
        raw: 'Just adding another data point RE: not enough system RAM


          I had a similar issue with my setup, where I have more than enough VRAM
          but wasn''t able to load the modal because text-gen-webui keeps running
          out of system memory (RAM). For me I just had to increase my virtual memory
          (swap if you on linux). And it fixed things. Also just watching the RAM
          and VRAM usage while the modal is loaded, I observed that it first loaded
          the modal (or more likely a part of it) to RAM (and swap because there wasn''t
          enough RAM) and then it would load it to VRAM.'
        updatedAt: '2023-06-11T10:49:16.355Z'
      numEdits: 0
      reactions: []
    id: 6485a6ac3e39b40d99bba263
    type: comment
  author: gpusmatter
  content: 'Just adding another data point RE: not enough system RAM


    I had a similar issue with my setup, where I have more than enough VRAM but wasn''t
    able to load the modal because text-gen-webui keeps running out of system memory
    (RAM). For me I just had to increase my virtual memory (swap if you on linux).
    And it fixed things. Also just watching the RAM and VRAM usage while the modal
    is loaded, I observed that it first loaded the modal (or more likely a part of
    it) to RAM (and swap because there wasn''t enough RAM) and then it would load
    it to VRAM.'
  created_at: 2023-06-11 09:49:16+00:00
  edited: false
  hidden: false
  id: 6485a6ac3e39b40d99bba263
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/rfPtMFfI-Qr9ZWRDvtsC0.jpeg?w=200&h=200&f=face
      fullname: 'Mohammad Maizied Hasan Majumder '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maizied
      type: user
    createdAt: '2023-08-11T17:36:59.000Z'
    data:
      edited: false
      editors:
      - maizied
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7680729031562805
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/rfPtMFfI-Qr9ZWRDvtsC0.jpeg?w=200&h=200&f=face
          fullname: 'Mohammad Maizied Hasan Majumder '
          isHf: false
          isPro: false
          name: maizied
          type: user
        html: '<p>Which is better option(A6000 x2 or 3090 x2 SLI) for LLM model?</p>

          '
        raw: Which is better option(A6000 x2 or 3090 x2 SLI) for LLM model?
        updatedAt: '2023-08-11T17:36:59.597Z'
      numEdits: 0
      reactions: []
    id: 64d671bb1a81ece17d8d7643
    type: comment
  author: maizied
  content: Which is better option(A6000 x2 or 3090 x2 SLI) for LLM model?
  created_at: 2023-08-11 16:36:59+00:00
  edited: false
  hidden: false
  id: 64d671bb1a81ece17d8d7643
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: How much vram+ram 30B needs? I have 3060 12gb + 32gb ram.
