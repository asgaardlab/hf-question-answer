!!python/object:huggingface_hub.community.DiscussionWithDetails
author: crainto
conflicting_files: null
created_at: 2023-05-23 18:46:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b178d55460169cf519f4f842736712e.svg
      fullname: crain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crainto
      type: user
    createdAt: '2023-05-23T19:46:07.000Z'
    data:
      edited: false
      editors:
      - crainto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b178d55460169cf519f4f842736712e.svg
          fullname: crain
          isHf: false
          isPro: false
          name: crainto
          type: user
        html: "<p>I have this error and don't quite understand what i did wrong<br>type:\
          \ llama<br>wbits: 4<br>groupsize: none</p>\n<p>Traceback (most recent call\
          \ last): File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\server.py\u201D\
          , line 70, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 103, in load_model tokenizer = load_tokenizer(model_name,\
          \ model) File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 128, in load_tokenizer tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\u201C), clean_up_tokenization_spaces=True)\
          \ File \u201CG:\\ai\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py\u201D, line 1811, in from_pretrained\
          \ return cls.from_pretrained( File \u201CG:\\ai\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u201D\
          , line 1965, in from_pretrained tokenizer = cls(*init_inputs, **init_kwargs)\
          \ File \u201CG:\\ai\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\tokenization_llama.py\u201D, line 96, in init\
          \ self.sp_model.Load(vocab_file) File \"G:\\ai\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\sentencepiece_init.py\", line 905, in Load return\
          \ self.LoadFromFile(model_file) File \"G:\\ai\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\sentencepiece_init.py\u201D, line 310, in LoadFromFile\
          \ return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg) TypeError:\
          \ not a string</p>\n"
        raw: "I have this error and don't quite understand what i did wrong\r\ntype:\
          \ llama\r\nwbits: 4\r\ngroupsize: none\r\n\r\nTraceback (most recent call\
          \ last): File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\server.py\u201D\
          , line 70, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 103, in load_model tokenizer = load_tokenizer(model_name,\
          \ model) File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 128, in load_tokenizer tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\u201C), clean_up_tokenization_spaces=True)\
          \ File \u201CG:\\ai\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py\u201D, line 1811, in from_pretrained\
          \ return cls.from_pretrained( File \u201CG:\\ai\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u201D\
          , line 1965, in from_pretrained tokenizer = cls(*init_inputs, **init_kwargs)\
          \ File \u201CG:\\ai\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\tokenization_llama.py\u201D, line 96, in init\
          \ self.sp_model.Load(vocab_file) File \"G:\\ai\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\sentencepiece_init.py\", line 905, in Load return\
          \ self.LoadFromFile(model_file) File \"G:\\ai\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\sentencepiece_init.py\u201D, line 310, in LoadFromFile\
          \ return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg) TypeError:\
          \ not a string"
        updatedAt: '2023-05-23T19:46:07.107Z'
      numEdits: 0
      reactions: []
    id: 646d17ffd720ccd8cae42ca4
    type: comment
  author: crainto
  content: "I have this error and don't quite understand what i did wrong\r\ntype:\
    \ llama\r\nwbits: 4\r\ngroupsize: none\r\n\r\nTraceback (most recent call last):\
    \ File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\server.py\u201D\
    , line 70, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201CG:\\ai\\oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 103, in load_model tokenizer = load_tokenizer(model_name, model) File \u201C\
    G:\\ai\\oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 128, in load_tokenizer tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\u201C\
    ), clean_up_tokenization_spaces=True) File \u201CG:\\ai\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u201D, line\
    \ 1811, in from_pretrained return cls.from_pretrained( File \u201CG:\\ai\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u201D\
    , line 1965, in from_pretrained tokenizer = cls(*init_inputs, **init_kwargs) File\
    \ \u201CG:\\ai\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\tokenization_llama.py\u201D, line 96, in init self.sp_model.Load(vocab_file)\
    \ File \"G:\\ai\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    sentencepiece_init.py\", line 905, in Load return self.LoadFromFile(model_file)\
    \ File \"G:\\ai\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    sentencepiece_init.py\u201D, line 310, in LoadFromFile return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg) TypeError: not a string"
  created_at: 2023-05-23 18:46:07+00:00
  edited: false
  hidden: false
  id: 646d17ffd720ccd8cae42ca4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T23:08:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Check that your files are downloaded correctly, especially tokenizer.model,
          tokenizer.json and tokenizer_config.json</p>

          <p>If need be, download them again</p>

          '
        raw: 'Check that your files are downloaded correctly, especially tokenizer.model,
          tokenizer.json and tokenizer_config.json


          If need be, download them again'
        updatedAt: '2023-05-23T23:08:11.248Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - crainto
    id: 646d475be0c5e395735af3c2
    type: comment
  author: TheBloke
  content: 'Check that your files are downloaded correctly, especially tokenizer.model,
    tokenizer.json and tokenizer_config.json


    If need be, download them again'
  created_at: 2023-05-23 22:08:11+00:00
  edited: false
  hidden: false
  id: 646d475be0c5e395735af3c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
      fullname: jonah meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonahtme
      type: user
    createdAt: '2023-05-24T00:24:41.000Z'
    data:
      edited: false
      editors:
      - jonahtme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
          fullname: jonah meyer
          isHf: false
          isPro: false
          name: jonahtme
          type: user
        html: '<blockquote>

          <p>Check that your files are downloaded correctly, especially tokenizer.model,
          tokenizer.json and tokenizer_config.json</p>

          <p>If need be, download them again</p>

          </blockquote>

          <p>I also have a 4090, same issue. I have all the files you mentioned downloaded</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/ppJhhD1-52nRFUCfskLi_.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/ppJhhD1-52nRFUCfskLi_.png"></a></p>

          '
        raw: "> Check that your files are downloaded correctly, especially tokenizer.model,\
          \ tokenizer.json and tokenizer_config.json\n> \n> If need be, download them\
          \ again\n\nI also have a 4090, same issue. I have all the files you mentioned\
          \ downloaded\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/ppJhhD1-52nRFUCfskLi_.png)"
        updatedAt: '2023-05-24T00:24:41.467Z'
      numEdits: 0
      reactions: []
    id: 646d59494a2db7744384e910
    type: comment
  author: jonahtme
  content: "> Check that your files are downloaded correctly, especially tokenizer.model,\
    \ tokenizer.json and tokenizer_config.json\n> \n> If need be, download them again\n\
    \nI also have a 4090, same issue. I have all the files you mentioned downloaded\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/ppJhhD1-52nRFUCfskLi_.png)"
  created_at: 2023-05-23 23:24:41+00:00
  edited: false
  hidden: false
  id: 646d59494a2db7744384e910
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675512244297-63d3aa7c640bb0f77173178c.jpeg?w=200&h=200&f=face
      fullname: ItBurnZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Itburnz
      type: user
    createdAt: '2023-05-24T00:43:12.000Z'
    data:
      edited: false
      editors:
      - Itburnz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675512244297-63d3aa7c640bb0f77173178c.jpeg?w=200&h=200&f=face
          fullname: ItBurnZ
          isHf: false
          isPro: false
          name: Itburnz
          type: user
        html: '<p>I will test it on a 4090 and see if I can reproduce the error, from
          the top of my head sentence piece requires a particular version of python.</p>

          '
        raw: I will test it on a 4090 and see if I can reproduce the error, from the
          top of my head sentence piece requires a particular version of python.
        updatedAt: '2023-05-24T00:43:12.682Z'
      numEdits: 0
      reactions: []
    id: 646d5da02abe5323fe231235
    type: comment
  author: Itburnz
  content: I will test it on a 4090 and see if I can reproduce the error, from the
    top of my head sentence piece requires a particular version of python.
  created_at: 2023-05-23 23:43:12+00:00
  edited: false
  hidden: false
  id: 646d5da02abe5323fe231235
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
      fullname: jonah meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonahtme
      type: user
    createdAt: '2023-05-24T00:52:03.000Z'
    data:
      edited: false
      editors:
      - jonahtme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
          fullname: jonah meyer
          isHf: false
          isPro: false
          name: jonahtme
          type: user
        html: '<p>I have most recent version,<br>Python 3.10.11</p>

          <blockquote>

          <p>I will test it on a 4090 and see if I can reproduce the error, from the
          top of my head sentence piece requires a particular version of python.</p>

          </blockquote>

          '
        raw: "I have most recent version, \nPython 3.10.11\n> I will test it on a\
          \ 4090 and see if I can reproduce the error, from the top of my head sentence\
          \ piece requires a particular version of python."
        updatedAt: '2023-05-24T00:52:03.258Z'
      numEdits: 0
      reactions: []
    id: 646d5fb3e0c5e395735e3950
    type: comment
  author: jonahtme
  content: "I have most recent version, \nPython 3.10.11\n> I will test it on a 4090\
    \ and see if I can reproduce the error, from the top of my head sentence piece\
    \ requires a particular version of python."
  created_at: 2023-05-23 23:52:03+00:00
  edited: false
  hidden: false
  id: 646d5fb3e0c5e395735e3950
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-24T00:53:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s not related to GPU.  I don''t really know what''s causing
          it if you definitely have tokenizer.model downloaded and its sha256sum is
          correct.  Lack of tokenizer.model is the normal cause of this.</p>

          <p>Try deleting/renaming <code>tokenizer.model</code>, see if it loads the
          Fast Tokenizer (tokenizer.json) instead</p>

          '
        raw: 'It''s not related to GPU.  I don''t really know what''s causing it if
          you definitely have tokenizer.model downloaded and its sha256sum is correct.  Lack
          of tokenizer.model is the normal cause of this.


          Try deleting/renaming `tokenizer.model`, see if it loads the Fast Tokenizer
          (tokenizer.json) instead'
        updatedAt: '2023-05-24T00:53:38.633Z'
      numEdits: 0
      reactions: []
    id: 646d60122abe5323fe2364d6
    type: comment
  author: TheBloke
  content: 'It''s not related to GPU.  I don''t really know what''s causing it if
    you definitely have tokenizer.model downloaded and its sha256sum is correct.  Lack
    of tokenizer.model is the normal cause of this.


    Try deleting/renaming `tokenizer.model`, see if it loads the Fast Tokenizer (tokenizer.json)
    instead'
  created_at: 2023-05-23 23:53:38+00:00
  edited: false
  hidden: false
  id: 646d60122abe5323fe2364d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10f4fb580077532b35468b6e0a99f8e6.svg
      fullname: Tristan Trumble
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdtrumble
      type: user
    createdAt: '2023-05-24T01:23:15.000Z'
    data:
      edited: false
      editors:
      - tdtrumble
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10f4fb580077532b35468b6e0a99f8e6.svg
          fullname: Tristan Trumble
          isHf: false
          isPro: false
          name: tdtrumble
          type: user
        html: '<p>I had this same error. When I downloaded the repo the command I
          used didn''t grab LFS objects. I went in and manually downloaded the safetensors
          file but I didn''t catch that  tokenizer.model is flagged as LFS too and
          I neglected to download it. Once I downloaded the proper tokenizer.model,
          the model loaded right up on my 4090.</p>

          '
        raw: I had this same error. When I downloaded the repo the command I used
          didn't grab LFS objects. I went in and manually downloaded the safetensors
          file but I didn't catch that  tokenizer.model is flagged as LFS too and
          I neglected to download it. Once I downloaded the proper tokenizer.model,
          the model loaded right up on my 4090.
        updatedAt: '2023-05-24T01:23:15.823Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - crainto
    id: 646d6703e0c5e395735f4845
    type: comment
  author: tdtrumble
  content: I had this same error. When I downloaded the repo the command I used didn't
    grab LFS objects. I went in and manually downloaded the safetensors file but I
    didn't catch that  tokenizer.model is flagged as LFS too and I neglected to download
    it. Once I downloaded the proper tokenizer.model, the model loaded right up on
    my 4090.
  created_at: 2023-05-24 00:23:15+00:00
  edited: false
  hidden: false
  id: 646d6703e0c5e395735f4845
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
      fullname: jonah meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonahtme
      type: user
    createdAt: '2023-05-24T01:28:42.000Z'
    data:
      edited: false
      editors:
      - jonahtme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
          fullname: jonah meyer
          isHf: false
          isPro: false
          name: jonahtme
          type: user
        html: '<blockquote>

          <p>It''s not related to GPU.  I don''t really know what''s causing it if
          you definitely have tokenizer.model downloaded and its sha256sum is correct.  Lack
          of tokenizer.model is the normal cause of this.</p>

          <p>Try deleting/renaming <code>tokenizer.model</code>, see if it loads the
          Fast Tokenizer (tokenizer.json) instead</p>

          </blockquote>

          <p>Hmm, interesting. I tried deleting it, reloading... same error. I also
          tried putting it back, re loading... nothing. And same when I rename it
          while its still in the folder.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/DkN0QSmh8lucDHnEkg_GW.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/DkN0QSmh8lucDHnEkg_GW.png"></a></p>

          '
        raw: "> It's not related to GPU.  I don't really know what's causing it if\
          \ you definitely have tokenizer.model downloaded and its sha256sum is correct.\
          \  Lack of tokenizer.model is the normal cause of this.\n> \n> Try deleting/renaming\
          \ `tokenizer.model`, see if it loads the Fast Tokenizer (tokenizer.json)\
          \ instead\n\nHmm, interesting. I tried deleting it, reloading... same error.\
          \ I also tried putting it back, re loading... nothing. And same when I rename\
          \ it while its still in the folder.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/DkN0QSmh8lucDHnEkg_GW.png)"
        updatedAt: '2023-05-24T01:28:42.092Z'
      numEdits: 0
      reactions: []
    id: 646d684a2abe5323fe249376
    type: comment
  author: jonahtme
  content: "> It's not related to GPU.  I don't really know what's causing it if you\
    \ definitely have tokenizer.model downloaded and its sha256sum is correct.  Lack\
    \ of tokenizer.model is the normal cause of this.\n> \n> Try deleting/renaming\
    \ `tokenizer.model`, see if it loads the Fast Tokenizer (tokenizer.json) instead\n\
    \nHmm, interesting. I tried deleting it, reloading... same error. I also tried\
    \ putting it back, re loading... nothing. And same when I rename it while its\
    \ still in the folder.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/DkN0QSmh8lucDHnEkg_GW.png)"
  created_at: 2023-05-24 00:28:42+00:00
  edited: false
  hidden: false
  id: 646d684a2abe5323fe249376
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
      fullname: jonah meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonahtme
      type: user
    createdAt: '2023-05-24T01:32:53.000Z'
    data:
      edited: false
      editors:
      - jonahtme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd623e05c4ab8961438dc5a1d953e098.svg
          fullname: jonah meyer
          isHf: false
          isPro: false
          name: jonahtme
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/HlMzpypb7bHDuTPPB4XRe.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/HlMzpypb7bHDuTPPB4XRe.png"></a><br>This
          is what Bard AI says, sound correct?</p>

          '
        raw: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/HlMzpypb7bHDuTPPB4XRe.png)

          This is what Bard AI says, sound correct?'
        updatedAt: '2023-05-24T01:32:53.379Z'
      numEdits: 0
      reactions: []
    id: 646d6945acc13867a13ef5e4
    type: comment
  author: jonahtme
  content: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/646d58a1eb9268aeebc3ff1e/HlMzpypb7bHDuTPPB4XRe.png)

    This is what Bard AI says, sound correct?'
  created_at: 2023-05-24 00:32:53+00:00
  edited: false
  hidden: false
  id: 646d6945acc13867a13ef5e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7b178d55460169cf519f4f842736712e.svg
      fullname: crain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crainto
      type: user
    createdAt: '2023-05-24T07:56:25.000Z'
    data:
      status: closed
    id: 646dc329e34b2ec2d2d17a95
    type: status-change
  author: crainto
  created_at: 2023-05-24 06:56:25+00:00
  id: 646dc329e34b2ec2d2d17a95
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b178d55460169cf519f4f842736712e.svg
      fullname: crain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crainto
      type: user
    createdAt: '2023-05-24T08:01:12.000Z'
    data:
      edited: false
      editors:
      - crainto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b178d55460169cf519f4f842736712e.svg
          fullname: crain
          isHf: false
          isPro: false
          name: crainto
          type: user
        html: '<blockquote>

          <p>I had this same error. When I downloaded the repo the command I used
          didn''t grab LFS objects. I went in and manually downloaded the safetensors
          file but I didn''t catch that  tokenizer.model is flagged as LFS too and
          I neglected to download it. Once I downloaded the proper tokenizer.model,
          the model loaded right up on my 4090.</p>

          </blockquote>

          <p>He''s right, it was my mistake that LFS wasn''t downloaded.<br>But now
          i can''t load the model again, there''s no localhost, ip or something</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png"></a></p>

          '
        raw: '> I had this same error. When I downloaded the repo the command I used
          didn''t grab LFS objects. I went in and manually downloaded the safetensors
          file but I didn''t catch that  tokenizer.model is flagged as LFS too and
          I neglected to download it. Once I downloaded the proper tokenizer.model,
          the model loaded right up on my 4090.


          He''s right, it was my mistake that LFS wasn''t downloaded.

          But now i can''t load the model again, there''s no localhost, ip or something


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png)'
        updatedAt: '2023-05-24T08:01:12.487Z'
      numEdits: 0
      reactions: []
    id: 646dc44840e741b191305075
    type: comment
  author: crainto
  content: '> I had this same error. When I downloaded the repo the command I used
    didn''t grab LFS objects. I went in and manually downloaded the safetensors file
    but I didn''t catch that  tokenizer.model is flagged as LFS too and I neglected
    to download it. Once I downloaded the proper tokenizer.model, the model loaded
    right up on my 4090.


    He''s right, it was my mistake that LFS wasn''t downloaded.

    But now i can''t load the model again, there''s no localhost, ip or something


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png)'
  created_at: 2023-05-24 07:01:12+00:00
  edited: false
  hidden: false
  id: 646dc44840e741b191305075
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7b178d55460169cf519f4f842736712e.svg
      fullname: crain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crainto
      type: user
    createdAt: '2023-05-24T08:06:45.000Z'
    data:
      status: open
    id: 646dc5955c3c0df5aeebe317
    type: status-change
  author: crainto
  created_at: 2023-05-24 07:06:45+00:00
  id: 646dc5955c3c0df5aeebe317
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-24T08:23:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I had this same error. When I downloaded the repo the command I used
          didn''t grab LFS objects. I went in and manually downloaded the safetensors
          file but I didn''t catch that  tokenizer.model is flagged as LFS too and
          I neglected to download it. Once I downloaded the proper tokenizer.model,
          the model loaded right up on my 4090.</p>

          </blockquote>

          <p>He''s right, it was my mistake that LFS wasn''t downloaded.<br>But now
          i can''t load the model again, there''s no localhost, ip or something</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png"></a></p>

          </blockquote>

          <p>This problem is caused by not having enough RAM to load the model.  It
          uses a lot of RAM while it loads the model on to VRAM.</p>

          <p>To resolve it, increase your Windows pagefile size a lot - eg up to about
          90GB.  That should allow the model to load onto the GPU OK.</p>

          '
        raw: "> > I had this same error. When I downloaded the repo the command I\
          \ used didn't grab LFS objects. I went in and manually downloaded the safetensors\
          \ file but I didn't catch that  tokenizer.model is flagged as LFS too and\
          \ I neglected to download it. Once I downloaded the proper tokenizer.model,\
          \ the model loaded right up on my 4090.\n> \n> He's right, it was my mistake\
          \ that LFS wasn't downloaded.\n> But now i can't load the model again, there's\
          \ no localhost, ip or something\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png)\n\
          \nThis problem is caused by not having enough RAM to load the model.  It\
          \ uses a lot of RAM while it loads the model on to VRAM.\n\nTo resolve it,\
          \ increase your Windows pagefile size a lot - eg up to about 90GB.  That\
          \ should allow the model to load onto the GPU OK."
        updatedAt: '2023-05-24T08:23:59.218Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - crainto
    id: 646dc99f5c3c0df5aeecbffc
    type: comment
  author: TheBloke
  content: "> > I had this same error. When I downloaded the repo the command I used\
    \ didn't grab LFS objects. I went in and manually downloaded the safetensors file\
    \ but I didn't catch that  tokenizer.model is flagged as LFS too and I neglected\
    \ to download it. Once I downloaded the proper tokenizer.model, the model loaded\
    \ right up on my 4090.\n> \n> He's right, it was my mistake that LFS wasn't downloaded.\n\
    > But now i can't load the model again, there's no localhost, ip or something\n\
    > \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/633efb364a7a5d7dfbd38ee4/UoazHuAfhrum0XIcDKYR-.png)\n\
    \nThis problem is caused by not having enough RAM to load the model.  It uses\
    \ a lot of RAM while it loads the model on to VRAM.\n\nTo resolve it, increase\
    \ your Windows pagefile size a lot - eg up to about 90GB.  That should allow the\
    \ model to load onto the GPU OK."
  created_at: 2023-05-24 07:23:59+00:00
  edited: false
  hidden: false
  id: 646dc99f5c3c0df5aeecbffc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640590b591ee7d7f287183ae/PQefPEgtaNqnTNfumrZXr.jpeg?w=200&h=200&f=face
      fullname: David Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ReMeDy-TV
      type: user
    createdAt: '2023-05-25T07:24:24.000Z'
    data:
      edited: true
      editors:
      - ReMeDy-TV
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640590b591ee7d7f287183ae/PQefPEgtaNqnTNfumrZXr.jpeg?w=200&h=200&f=face
          fullname: David Scott
          isHf: false
          isPro: false
          name: ReMeDy-TV
          type: user
        html: '<p>I''m on an RTX 4090 (24 GB VRAM) and got it working on the first
          try despite me only having 16 GB RAM. I used Kobold''s GBTQ version branch.
          Used install_requirements.bat, opened Kobold and once inside I enabled the
          new UI mode. After installing the model into my model folder, I renamed
          the .safetensors file to 4bit.safetensors. I then loaded the model into
          Kobold via the AI button. I assigned all the layers to the GPU (and it all
          fit). Done. Took a while to load it all in tho.</p>

          '
        raw: I'm on an RTX 4090 (24 GB VRAM) and got it working on the first try despite
          me only having 16 GB RAM. I used Kobold's GBTQ version branch. Used install_requirements.bat,
          opened Kobold and once inside I enabled the new UI mode. After installing
          the model into my model folder, I renamed the .safetensors file to 4bit.safetensors.
          I then loaded the model into Kobold via the AI button. I assigned all the
          layers to the GPU (and it all fit). Done. Took a while to load it all in
          tho.
        updatedAt: '2023-05-25T07:26:16.296Z'
      numEdits: 3
      reactions: []
    id: 646f0d28ac3bff5945d5abd6
    type: comment
  author: ReMeDy-TV
  content: I'm on an RTX 4090 (24 GB VRAM) and got it working on the first try despite
    me only having 16 GB RAM. I used Kobold's GBTQ version branch. Used install_requirements.bat,
    opened Kobold and once inside I enabled the new UI mode. After installing the
    model into my model folder, I renamed the .safetensors file to 4bit.safetensors.
    I then loaded the model into Kobold via the AI button. I assigned all the layers
    to the GPU (and it all fit). Done. Took a while to load it all in tho.
  created_at: 2023-05-25 06:24:24+00:00
  edited: true
  hidden: false
  id: 646f0d28ac3bff5945d5abd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-05-25T07:27:07.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>This makes me glad I have 96GB of ram!</p>

          '
        raw: This makes me glad I have 96GB of ram!
        updatedAt: '2023-05-25T07:27:07.816Z'
      numEdits: 0
      reactions: []
    id: 646f0dcbe2a72c647b5f9e91
    type: comment
  author: cleverest
  content: This makes me glad I have 96GB of ram!
  created_at: 2023-05-25 06:27:07+00:00
  edited: false
  hidden: false
  id: 646f0dcbe2a72c647b5f9e91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-05-25T07:50:40.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: "<p>Sadly didn't help...I get this error:</p>\n<p>Traceback (most recent\
          \ call last): File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 68, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 95, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 275, in GPTQ_loader\
          \ model = modules.GPTQ_loader.load_quantized(model_name) File \u201CC:\\\
          Users\\cleverest\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041,\
          \ in load_state_dict raise RuntimeError(\u2018Error(s) in loading state_dict\
          \ for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM: size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in cu</p>\n"
        raw: "Sadly didn't help...I get this error:\n\nTraceback (most recent call\
          \ last): File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 68, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 95, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 275, in GPTQ_loader\
          \ model = modules.GPTQ_loader.load_quantized(model_name) File \u201CC:\\\
          Users\\cleverest\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041,\
          \ in load_state_dict raise RuntimeError(\u2018Error(s) in loading state_dict\
          \ for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM: size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in cu"
        updatedAt: '2023-05-25T07:50:40.460Z'
      numEdits: 0
      reactions: []
    id: 646f1350fa9253f3ae72af80
    type: comment
  author: cleverest
  content: "Sadly didn't help...I get this error:\n\nTraceback (most recent call last):\
    \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    server.py\u201D, line 68, in load_model_wrapper shared.model, shared.tokenizer\
    \ = load_model(shared.model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 95, in load_model output\
    \ = load_func(model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 275, in GPTQ_loader model\
    \ = modules.GPTQ_loader.load_quantized(model_name) File \u201CC:\\Users\\cleverest\\\
    oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D, line\
    \ 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
    \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
    \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\
    \ raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
    .format( RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM: size\
    \ mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with shape\
    \ torch.Size([1, 832]) from checkpoint, the shape in current model is torch.Size([52,\
    \ 832]). size mismatch for model.layers.0.self_attn.k_proj.scales: copying a param\
    \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current model\
    \ is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ cu"
  created_at: 2023-05-25 06:50:40+00:00
  edited: false
  hidden: false
  id: 646f1350fa9253f3ae72af80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-05-25T07:52:46.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: "<p>Sadly 96GB of RAM didn't help...I got this error (and it's much\
          \ longer, I cut it off to share some of it:</p>\n<p>Traceback (most recent\
          \ call last): File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 68, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 95, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 275, in GPTQ_loader\
          \ model = modules.GPTQ_loader.load_quantized(model_name) File \u201CC:\\\
          Users\\cleverest\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041,\
          \ in load_state_dict raise RuntimeError(\u2018Error(s) in loading state_dict\
          \ for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM: size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]). size mismatch for model.layers.0.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]). size mismatch for model.layers.0.mlp.gate_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]). size mismatch for model.layers.0.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 17920]). size mismatch for model.layers.0.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]). size mismatch for model.layers.0.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 17920]). size mismatch for model.layers.1.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.1.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoin</p>\n"
        raw: "Sadly 96GB of RAM didn't help...I got this error (and it's much longer,\
          \ I cut it off to share some of it:\n\nTraceback (most recent call last):\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 68, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 95, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 275, in GPTQ_loader\
          \ model = modules.GPTQ_loader.load_quantized(model_name) File \u201CC:\\\
          Users\\cleverest\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041,\
          \ in load_state_dict raise RuntimeError(\u2018Error(s) in loading state_dict\
          \ for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM: size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]). size mismatch for model.layers.0.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]). size mismatch for model.layers.0.mlp.gate_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]). size mismatch for model.layers.0.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 17920]). size mismatch for model.layers.0.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]). size mismatch for model.layers.0.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 17920]). size mismatch for model.layers.1.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.1.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoin"
        updatedAt: '2023-05-25T07:52:46.734Z'
      numEdits: 0
      reactions: []
    id: 646f13ceac3bff5945d73629
    type: comment
  author: cleverest
  content: "Sadly 96GB of RAM didn't help...I got this error (and it's much longer,\
    \ I cut it off to share some of it:\n\nTraceback (most recent call last): File\
    \ \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
    , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 95, in load_model output = load_func(model_name)\
    \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 275, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
    \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    modules\\GPTQ_loader.py\u201D, line 177, in load_quantized model = load_quant(str(path_to_model),\
    \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
    \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
    \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\
    \ raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
    .format( RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM: size\
    \ mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with shape\
    \ torch.Size([1, 832]) from checkpoint, the shape in current model is torch.Size([52,\
    \ 832]). size mismatch for model.layers.0.self_attn.k_proj.scales: copying a param\
    \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current model\
    \ is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]). size mismatch for model.layers.0.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]). size mismatch for model.layers.0.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]). size mismatch for model.layers.0.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]). size mismatch for model.layers.0.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]). size mismatch for model.layers.0.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]). size mismatch for model.layers.1.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.1.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoin"
  created_at: 2023-05-25 06:52:46+00:00
  edited: false
  hidden: false
  id: 646f13ceac3bff5945d73629
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-25T08:08:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>This happens when GPTQ params are not set correctly</p>\n<p>Please\
          \ ensure you set and saved these params:</p>\n<ul>\n<li>bits = 4</li>\n\
          <li>groupsize = None</li>\n<li>model type = llama</li>\n</ul>\n<p>If you're\
          \ still having the problem, please edit <code>models/config-user.yaml</code>\
          \ and find the entry for this model, and make sure it matches this:</p>\n\
          <pre><code> TheBloke_WizardLM-30B-Uncensored-GPTQ$:\n  auto_devices: false\n\
          \  bf16: false\n  cpu: false\n  cpu_memory: 0\n  disk: false\n  gpu_memory_0:\
          \ 0\n  groupsize: None\n  load_in_8bit: false\n  mlock: false\n  model_type:\
          \ llama\n  n_batch: 512\n  n_gpu_layers: 0\n  pre_layer: 0\n  threads: 0\n\
          \  wbits: '4'\n</code></pre>\n<p>In particular ensure <code>groupsize: None</code></p>\n"
        raw: "This happens when GPTQ params are not set correctly\n\nPlease ensure\
          \ you set and saved these params:\n- bits = 4\n- groupsize = None\n- model\
          \ type = llama\n\nIf you're still having the problem, please edit `models/config-user.yaml`\
          \ and find the entry for this model, and make sure it matches this:\n\n\
          ```\n TheBloke_WizardLM-30B-Uncensored-GPTQ$:\n  auto_devices: false\n \
          \ bf16: false\n  cpu: false\n  cpu_memory: 0\n  disk: false\n  gpu_memory_0:\
          \ 0\n  groupsize: None\n  load_in_8bit: false\n  mlock: false\n  model_type:\
          \ llama\n  n_batch: 512\n  n_gpu_layers: 0\n  pre_layer: 0\n  threads: 0\n\
          \  wbits: '4'\n```\n\nIn particular ensure `groupsize: None`"
        updatedAt: '2023-05-25T08:08:18.089Z'
      numEdits: 0
      reactions: []
    id: 646f1772fa9253f3ae73974a
    type: comment
  author: TheBloke
  content: "This happens when GPTQ params are not set correctly\n\nPlease ensure you\
    \ set and saved these params:\n- bits = 4\n- groupsize = None\n- model type =\
    \ llama\n\nIf you're still having the problem, please edit `models/config-user.yaml`\
    \ and find the entry for this model, and make sure it matches this:\n\n```\n TheBloke_WizardLM-30B-Uncensored-GPTQ$:\n\
    \  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n  disk:\
    \ false\n  gpu_memory_0: 0\n  groupsize: None\n  load_in_8bit: false\n  mlock:\
    \ false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n  pre_layer:\
    \ 0\n  threads: 0\n  wbits: '4'\n```\n\nIn particular ensure `groupsize: None`"
  created_at: 2023-05-25 07:08:18+00:00
  edited: false
  hidden: false
  id: 646f1772fa9253f3ae73974a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-05-25T14:29:39.000Z'
    data:
      edited: true
      editors:
      - cleverest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>Thank you. Those are the parameters that are already set, so I must
          have to edit the file...I''ll try that later when I''m home.</p>

          '
        raw: Thank you. Those are the parameters that are already set, so I must have
          to edit the file...I'll try that later when I'm home.
        updatedAt: '2023-05-25T14:30:16.568Z'
      numEdits: 1
      reactions: []
    id: 646f70d3150f4cab862ce2b1
    type: comment
  author: cleverest
  content: Thank you. Those are the parameters that are already set, so I must have
    to edit the file...I'll try that later when I'm home.
  created_at: 2023-05-25 13:29:39+00:00
  edited: true
  hidden: false
  id: 646f70d3150f4cab862ce2b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-05-26T06:49:38.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>So  I confirmed the default config settings for this model in OobaBooga
          is loading as:</p>

          <p>  auto_devices: false<br>  bf16: false<br>  cpu: false<br>  cpu_memory:
          0<br>  disk: false<br>  gpu_memory_0: 0<br>  groupsize: 128<br>  load_in_8bit:
          false<br>  mlock: false<br>  model_type: llama<br>  n_batch: 512<br>  n_gpu_layers:
          0<br>  no_mmap: false<br>  pre_layer: 0<br>  threads: 0<br>  wbits: 4</p>

          <p>I changed it to yours as recommended above, which I noticed two things
          changed: The groupsize from 128 to 0 (yet it IS set to NONE in the GUI BTW),
          and I removed this line ''no_mmap: false'' entirely.</p>

          <p>After doing this, AND RESTARTING it completely, it works! Thank you!</p>

          '
        raw: "So  I confirmed the default config settings for this model in OobaBooga\
          \ is loading as:\n\n  auto_devices: false\n  bf16: false\n  cpu: false\n\
          \  cpu_memory: 0\n  disk: false\n  gpu_memory_0: 0\n  groupsize: 128\n \
          \ load_in_8bit: false\n  mlock: false\n  model_type: llama\n  n_batch: 512\n\
          \  n_gpu_layers: 0\n  no_mmap: false\n  pre_layer: 0\n  threads: 0\n  wbits:\
          \ 4\n\nI changed it to yours as recommended above, which I noticed two things\
          \ changed: The groupsize from 128 to 0 (yet it IS set to NONE in the GUI\
          \ BTW), and I removed this line 'no_mmap: false' entirely.\n\nAfter doing\
          \ this, AND RESTARTING it completely, it works! Thank you!"
        updatedAt: '2023-05-26T06:49:38.696Z'
      numEdits: 0
      reactions: []
    id: 647056821f0e7ee7fb14ed15
    type: comment
  author: cleverest
  content: "So  I confirmed the default config settings for this model in OobaBooga\
    \ is loading as:\n\n  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory:\
    \ 0\n  disk: false\n  gpu_memory_0: 0\n  groupsize: 128\n  load_in_8bit: false\n\
    \  mlock: false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n  no_mmap:\
    \ false\n  pre_layer: 0\n  threads: 0\n  wbits: 4\n\nI changed it to yours as\
    \ recommended above, which I noticed two things changed: The groupsize from 128\
    \ to 0 (yet it IS set to NONE in the GUI BTW), and I removed this line 'no_mmap:\
    \ false' entirely.\n\nAfter doing this, AND RESTARTING it completely, it works!\
    \ Thank you!"
  created_at: 2023-05-26 05:49:38+00:00
  edited: false
  hidden: false
  id: 647056821f0e7ee7fb14ed15
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/WizardLM-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Have a problem with rtx4090
