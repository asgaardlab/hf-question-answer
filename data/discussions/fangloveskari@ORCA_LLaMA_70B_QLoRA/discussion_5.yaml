!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HCZhang
conflicting_files: null
created_at: 2023-09-04 13:38:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
      fullname: Haochen Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HCZhang
      type: user
    createdAt: '2023-09-04T14:38:46.000Z'
    data:
      edited: false
      editors:
      - HCZhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824774265289307
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
          fullname: Haochen Zhang
          isHf: false
          isPro: false
          name: HCZhang
          type: user
        html: '<p>Sorry to bother you.<br>This question may not be directly related
          to your model, but I''ve been looking around and yet to find a solution.</p>

          <p>I''ve fine-tuned a model using QLORA, and I can''t merge the adapters
          (checkpoint) back to the base model.<br>I''ve tried the script provided
          by The Block, but there were some errors showing layer sizes don''t match.</p>

          <p>And your model seems works fine, so I wonder how did you merge the model.<br>Thank
          you.</p>

          <p>Also, your model is really impressive </p>

          '
        raw: "Sorry to bother you.\r\nThis question may not be directly related to\
          \ your model, but I've been looking around and yet to find a solution.\r\
          \n\r\nI've fine-tuned a model using QLORA, and I can't merge the adapters\
          \ (checkpoint) back to the base model.\r\nI've tried the script provided\
          \ by The Block, but there were some errors showing layer sizes don't match.\r\
          \n\r\nAnd your model seems works fine, so I wonder how did you merge the\
          \ model.\r\nThank you.\r\n\r\nAlso, your model is really impressive "
        updatedAt: '2023-09-04T14:38:46.345Z'
      numEdits: 0
      reactions: []
    id: 64f5ebf60b24e548a862a684
    type: comment
  author: HCZhang
  content: "Sorry to bother you.\r\nThis question may not be directly related to your\
    \ model, but I've been looking around and yet to find a solution.\r\n\r\nI've\
    \ fine-tuned a model using QLORA, and I can't merge the adapters (checkpoint)\
    \ back to the base model.\r\nI've tried the script provided by The Block, but\
    \ there were some errors showing layer sizes don't match.\r\n\r\nAnd your model\
    \ seems works fine, so I wonder how did you merge the model.\r\nThank you.\r\n\
    \r\nAlso, your model is really impressive "
  created_at: 2023-09-04 13:38:46+00:00
  edited: false
  hidden: false
  id: 64f5ebf60b24e548a862a684
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65aa490dce4000dddd743ebbb763bb62.svg
      fullname: Frank Zhao
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: fangloveskari
      type: user
    createdAt: '2023-09-04T14:47:03.000Z'
    data:
      edited: true
      editors:
      - fangloveskari
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8720636367797852
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65aa490dce4000dddd743ebbb763bb62.svg
          fullname: Frank Zhao
          isHf: false
          isPro: false
          name: fangloveskari
          type: user
        html: '<p>Hi,<br>I tried two ways for fusion:</p>

          <ul>

          <li>as I trained the model with <a rel="nofollow" href="https://github.com/hiyouga/LLaMA-Efficient-Tuning">https://github.com/hiyouga/LLaMA-Efficient-Tuning</a>,
          I directly use <a rel="nofollow" href="https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/main/src/export_model.py">https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/main/src/export_model.py</a>
          to do the fusion job.</li>

          <li>or you can use <a rel="nofollow" href="https://github.com/jondurbin/qlora/blob/main/qmerge.py">https://github.com/jondurbin/qlora/blob/main/qmerge.py</a>
          to do it.</li>

          </ul>

          <p>I tried the two methods, generally, the second will get better ARC(+0.15)
          and Truthful_QA(+0.3) scores but the other two(MMLU(-0.2) and HelloSwag(-0.2))
          seems to degenerate.</p>

          <p>The version for leadboard is generated by the first fusion method.</p>

          '
        raw: "Hi, \nI tried two ways for fusion:\n* as I trained the model with https://github.com/hiyouga/LLaMA-Efficient-Tuning,\
          \ I directly use https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/main/src/export_model.py\
          \ to do the fusion job.\n* or you can use https://github.com/jondurbin/qlora/blob/main/qmerge.py\
          \ to do it.\n\nI tried the two methods, generally, the second will get better\
          \ ARC(+0.15) and Truthful_QA(+0.3) scores but the other two(MMLU(-0.2) and\
          \ HelloSwag(-0.2)) seems to degenerate.\n\nThe version for leadboard is\
          \ generated by the first fusion method."
        updatedAt: '2023-09-04T14:47:57.980Z'
      numEdits: 1
      reactions: []
    id: 64f5ede70b24e548a862ee16
    type: comment
  author: fangloveskari
  content: "Hi, \nI tried two ways for fusion:\n* as I trained the model with https://github.com/hiyouga/LLaMA-Efficient-Tuning,\
    \ I directly use https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/main/src/export_model.py\
    \ to do the fusion job.\n* or you can use https://github.com/jondurbin/qlora/blob/main/qmerge.py\
    \ to do it.\n\nI tried the two methods, generally, the second will get better\
    \ ARC(+0.15) and Truthful_QA(+0.3) scores but the other two(MMLU(-0.2) and HelloSwag(-0.2))\
    \ seems to degenerate.\n\nThe version for leadboard is generated by the first\
    \ fusion method."
  created_at: 2023-09-04 13:47:03+00:00
  edited: true
  hidden: false
  id: 64f5ede70b24e548a862ee16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
      fullname: Haochen Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HCZhang
      type: user
    createdAt: '2023-09-04T15:08:22.000Z'
    data:
      edited: false
      editors:
      - HCZhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9538253545761108
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
          fullname: Haochen Zhang
          isHf: false
          isPro: false
          name: HCZhang
          type: user
        html: '<p>Thank you so much, would check the two methods you mentioned.</p>

          '
        raw: Thank you so much, would check the two methods you mentioned.
        updatedAt: '2023-09-04T15:08:22.238Z'
      numEdits: 0
      reactions: []
    id: 64f5f2e63cd4ab07d65eecea
    type: comment
  author: HCZhang
  content: Thank you so much, would check the two methods you mentioned.
  created_at: 2023-09-04 14:08:22+00:00
  edited: false
  hidden: false
  id: 64f5f2e63cd4ab07d65eecea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/65aa490dce4000dddd743ebbb763bb62.svg
      fullname: Frank Zhao
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: fangloveskari
      type: user
    createdAt: '2023-09-04T15:37:00.000Z'
    data:
      status: closed
    id: 64f5f99cb093b649f41b7264
    type: status-change
  author: fangloveskari
  created_at: 2023-09-04 14:37:00+00:00
  id: 64f5f99cb093b649f41b7264
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: fangloveskari/ORCA_LLaMA_70B_QLoRA
repo_type: model
status: closed
target_branch: null
title: May I ask how the your method of merge adapters to base model?
