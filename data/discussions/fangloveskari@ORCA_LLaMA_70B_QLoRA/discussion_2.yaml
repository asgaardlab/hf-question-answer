!!python/object:huggingface_hub.community.DiscussionWithDetails
author: II-Matto
conflicting_files: null
created_at: 2023-09-01 08:33:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/597d00caff9ad9aa81a857ee996f9af2.svg
      fullname: Shuzhe Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: II-Matto
      type: user
    createdAt: '2023-09-01T09:33:32.000Z'
    data:
      edited: false
      editors:
      - II-Matto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9290050268173218
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/597d00caff9ad9aa81a857ee996f9af2.svg
          fullname: Shuzhe Wu
          isHf: false
          isPro: false
          name: II-Matto
          type: user
        html: '<p>Hi, thanks for sharing the great work! I was wondering what is the
          QLoRA implementation you use for the fine-tuning. Do you use the official
          QLoRA code (<a rel="nofollow" href="https://github.com/artidoro/qlora/tree/main">https://github.com/artidoro/qlora/tree/main</a>)
          or you implement it by yourself? Besides, could you describe the hardware
          settings and the corresponding training speed?</p>

          '
        raw: Hi, thanks for sharing the great work! I was wondering what is the QLoRA
          implementation you use for the fine-tuning. Do you use the official QLoRA
          code (https://github.com/artidoro/qlora/tree/main) or you implement it by
          yourself? Besides, could you describe the hardware settings and the corresponding
          training speed?
        updatedAt: '2023-09-01T09:33:32.198Z'
      numEdits: 0
      reactions: []
    id: 64f1afec6a85b6a971798d2e
    type: comment
  author: II-Matto
  content: Hi, thanks for sharing the great work! I was wondering what is the QLoRA
    implementation you use for the fine-tuning. Do you use the official QLoRA code
    (https://github.com/artidoro/qlora/tree/main) or you implement it by yourself?
    Besides, could you describe the hardware settings and the corresponding training
    speed?
  created_at: 2023-09-01 08:33:32+00:00
  edited: false
  hidden: false
  id: 64f1afec6a85b6a971798d2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65aa490dce4000dddd743ebbb763bb62.svg
      fullname: Frank Zhao
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: fangloveskari
      type: user
    createdAt: '2023-09-02T01:28:56.000Z'
    data:
      edited: false
      editors:
      - fangloveskari
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9499834179878235
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65aa490dce4000dddd743ebbb763bb62.svg
          fullname: Frank Zhao
          isHf: false
          isPro: false
          name: fangloveskari
          type: user
        html: '<p>nop, I use <a rel="nofollow" href="https://github.com/hiyouga/LLaMA-Efficient-Tuning">https://github.com/hiyouga/LLaMA-Efficient-Tuning</a>.<br>I
          use deepspeed ZERO-2 + FlashAttention2 + 4bit QLoRA to training with 8 A100(80G),
          then the batch size can be set to around 16.<br>as for training speed, It
          may take 5-6 hours for training? I''m not really clear with that.</p>

          <p>btw, Our team tends to upload a better model and details about the dataset
          mix-up strategy, the hardware setting, the training settings and steps will
          also be provided, but it may take several days for that.</p>

          '
        raw: 'nop, I use https://github.com/hiyouga/LLaMA-Efficient-Tuning.

          I use deepspeed ZERO-2 + FlashAttention2 + 4bit QLoRA to training with 8
          A100(80G), then the batch size can be set to around 16.

          as for training speed, It may take 5-6 hours for training? I''m not really
          clear with that.


          btw, Our team tends to upload a better model and details about the dataset
          mix-up strategy, the hardware setting, the training settings and steps will
          also be provided, but it may take several days for that.'
        updatedAt: '2023-09-02T01:28:56.767Z'
      numEdits: 0
      reactions: []
    id: 64f28fd87e01e3ff94656fb6
    type: comment
  author: fangloveskari
  content: 'nop, I use https://github.com/hiyouga/LLaMA-Efficient-Tuning.

    I use deepspeed ZERO-2 + FlashAttention2 + 4bit QLoRA to training with 8 A100(80G),
    then the batch size can be set to around 16.

    as for training speed, It may take 5-6 hours for training? I''m not really clear
    with that.


    btw, Our team tends to upload a better model and details about the dataset mix-up
    strategy, the hardware setting, the training settings and steps will also be provided,
    but it may take several days for that.'
  created_at: 2023-09-02 00:28:56+00:00
  edited: false
  hidden: false
  id: 64f28fd87e01e3ff94656fb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/597d00caff9ad9aa81a857ee996f9af2.svg
      fullname: Shuzhe Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: II-Matto
      type: user
    createdAt: '2023-09-02T04:10:29.000Z'
    data:
      edited: false
      editors:
      - II-Matto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9417416453361511
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/597d00caff9ad9aa81a857ee996f9af2.svg
          fullname: Shuzhe Wu
          isHf: false
          isPro: false
          name: II-Matto
          type: user
        html: '<p>Thanks for your prompt reply. I am anticipating your following work!</p>

          '
        raw: Thanks for your prompt reply. I am anticipating your following work!
        updatedAt: '2023-09-02T04:10:29.738Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64f2b5b5f35695dc8b278261
    id: 64f2b5b5f35695dc8b27825f
    type: comment
  author: II-Matto
  content: Thanks for your prompt reply. I am anticipating your following work!
  created_at: 2023-09-02 03:10:29+00:00
  edited: false
  hidden: false
  id: 64f2b5b5f35695dc8b27825f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/597d00caff9ad9aa81a857ee996f9af2.svg
      fullname: Shuzhe Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: II-Matto
      type: user
    createdAt: '2023-09-02T04:10:29.000Z'
    data:
      status: closed
    id: 64f2b5b5f35695dc8b278261
    type: status-change
  author: II-Matto
  created_at: 2023-09-02 03:10:29+00:00
  id: 64f2b5b5f35695dc8b278261
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: fangloveskari/ORCA_LLaMA_70B_QLoRA
repo_type: model
status: closed
target_branch: null
title: About QLoRA Implementation
