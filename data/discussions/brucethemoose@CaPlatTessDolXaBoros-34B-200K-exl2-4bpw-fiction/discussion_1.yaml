!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Human420
conflicting_files: null
created_at: 2023-12-13 17:02:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/516583f11431a2a8cf577aeb5137cbe0.svg
      fullname: "Vojt\u011Bch Kub\xEDn"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Human420
      type: user
    createdAt: '2023-12-13T17:02:11.000Z'
    data:
      edited: false
      editors:
      - Human420
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7580095529556274
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/516583f11431a2a8cf577aeb5137cbe0.svg
          fullname: "Vojt\u011Bch Kub\xEDn"
          isHf: false
          isPro: false
          name: Human420
          type: user
        html: '<p>Hi,<br>    I am trying to load the model via transformers module,
          but following error occurs:<br><code>OSError: Could not locate model-00001-of-00008.safetensors
          inside brucethemoose/CaPlatTessDolXaBoros-34B-200K-exl2-4bpw-fiction.</code></p>

          <p>If I am not mistaken the module is trying to load incorrect number of
          shards (8), even though the model has 5. I was not able to find any solution
          online. Am I doing something wrong?<br>I am using this code:</p>

          <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer


          which_model = ''brucethemoose/CaPlatTessDolXaBoros-34B-200K-exl2-4bpw-fiction''


          tokenizer = AutoTokenizer.from_pretrained(which_model)

          model = AutoModelForCausalLM.from_pretrained(which_model, device_map=''auto'',
          low_cpu_mem_usage=True)

          </code></pre>

          <p>Thank you!</p>

          '
        raw: "Hi,\r\n    I am trying to load the model via transformers module, but\
          \ following error occurs:\r\n`OSError: Could not locate model-00001-of-00008.safetensors\
          \ inside brucethemoose/CaPlatTessDolXaBoros-34B-200K-exl2-4bpw-fiction.`\r\
          \n\r\nIf I am not mistaken the module is trying to load incorrect number\
          \ of shards (8), even though the model has 5. I was not able to find any\
          \ solution online. Am I doing something wrong? \r\nI am using this code:\r\
          \n\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \n\r\nwhich_model = 'brucethemoose/CaPlatTessDolXaBoros-34B-200K-exl2-4bpw-fiction'\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(which_model)\r\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(which_model, device_map='auto', low_cpu_mem_usage=True)\r\
          \n```\r\n\r\nThank you!\r\n"
        updatedAt: '2023-12-13T17:02:11.719Z'
      numEdits: 0
      reactions: []
    id: 6579e3938ee8830a31ea0e65
    type: comment
  author: Human420
  content: "Hi,\r\n    I am trying to load the model via transformers module, but\
    \ following error occurs:\r\n`OSError: Could not locate model-00001-of-00008.safetensors\
    \ inside brucethemoose/CaPlatTessDolXaBoros-34B-200K-exl2-4bpw-fiction.`\r\n\r\
    \nIf I am not mistaken the module is trying to load incorrect number of shards\
    \ (8), even though the model has 5. I was not able to find any solution online.\
    \ Am I doing something wrong? \r\nI am using this code:\r\n\r\n```\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\nwhich_model = 'brucethemoose/CaPlatTessDolXaBoros-34B-200K-exl2-4bpw-fiction'\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(which_model)\r\nmodel = AutoModelForCausalLM.from_pretrained(which_model,\
    \ device_map='auto', low_cpu_mem_usage=True)\r\n```\r\n\r\nThank you!\r\n"
  created_at: 2023-12-13 17:02:11+00:00
  edited: false
  hidden: false
  id: 6579e3938ee8830a31ea0e65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-13T19:15:24.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8767821192741394
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>This not a transformers model, but a exllamav2 one. If you want
          to load it in transformers, the original model is here: </p>

          <p><a href="https://huggingface.co/brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity">https://huggingface.co/brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity</a></p>

          <p>However, I would highly recommend a quantization of this model + an optimized
          runtime like exLllama on any hardware. Vanilla transformers is extremely
          inefficient at huge context sizes, even on an A100. Prompt processing in
          particular will take forever.</p>

          '
        raw: "This not a transformers model, but a exllamav2 one. If you want to load\
          \ it in transformers, the original model is here: \n\nhttps://huggingface.co/brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity\n\
          \nHowever, I would highly recommend a quantization of this model + an optimized\
          \ runtime like exLllama on any hardware. Vanilla transformers is extremely\
          \ inefficient at huge context sizes, even on an A100. Prompt processing\
          \ in particular will take forever."
        updatedAt: '2023-12-13T19:18:30.699Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Human420
    id: 657a02cc330fe066775d6df6
    type: comment
  author: brucethemoose
  content: "This not a transformers model, but a exllamav2 one. If you want to load\
    \ it in transformers, the original model is here: \n\nhttps://huggingface.co/brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity\n\
    \nHowever, I would highly recommend a quantization of this model + an optimized\
    \ runtime like exLllama on any hardware. Vanilla transformers is extremely inefficient\
    \ at huge context sizes, even on an A100. Prompt processing in particular will\
    \ take forever."
  created_at: 2023-12-13 19:15:24+00:00
  edited: true
  hidden: false
  id: 657a02cc330fe066775d6df6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: brucethemoose/CaPlatTessDolXaBoros-34B-200K-exl2-4bpw-fiction
repo_type: model
status: open
target_branch: null
title: Unable to load the model via transformers module
