!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ke99L
conflicting_files: null
created_at: 2023-07-13 16:19:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/277a5dd8603532e3853171d0cc1599ab.svg
      fullname: ke99L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ke99L
      type: user
    createdAt: '2023-07-13T17:19:26.000Z'
    data:
      edited: true
      editors:
      - ke99L
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6220986247062683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/277a5dd8603532e3853171d0cc1599ab.svg
          fullname: ke99L
          isHf: false
          isPro: false
          name: ke99L
          type: user
        html: '<p>today published work, SITTA: A Semantic Image-Text Alignment for
          Image Captioning<br><a rel="nofollow" href="https://github.com/ml-jku/semantic-image-text-alignment">https://github.com/ml-jku/semantic-image-text-alignment</a><br><a
          rel="nofollow" href="https://arxiv.org/pdf/2307.05591.pdf">https://arxiv.org/pdf/2307.05591.pdf</a><br><strong>CLIP
          text image jointed vision llms, no extra huge VIT, llama plugin, should
          easy adapt to danbooru database</strong></p>

          <p>reading note:<br>Language Embedding<br>random permutations</p>

          <p>Language Embedding related work:</p>

          <p><a rel="nofollow" href="https://arxiv.org/pdf/2305.01278.pdf">https://arxiv.org/pdf/2305.01278.pdf</a>
          </p>

          <blockquote>

          <p>Figure 5 llms logprob better than CLIP roberta contextual embedding and
          CNN classification head</p>

          </blockquote>

          <p><a rel="nofollow" href="https://arxiv.org/pdf/2306.17842.pdf">https://arxiv.org/pdf/2306.17842.pdf</a></p>

          <blockquote>

          <p>approaching way is llms understanding image, take llms token embedding
          map ( lexical tokens) to vqvae codebook<br>B.2 LLM Prompting / In-context
          denoising<br>Limitations task-specific conditions demo five choice is weak</p>

          </blockquote>

          <p><a rel="nofollow" href="https://arxiv.org/pdf/2304.05653.pdf">https://arxiv.org/pdf/2304.05653.pdf</a></p>

          <blockquote>

          <p>textual  redundant with bucket&amp;time emb used in SDXL </p>

          </blockquote>

          <p><a rel="nofollow" href="https://arxiv.org/pdf/2302.14383.pdf">https://arxiv.org/pdf/2302.14383.pdf</a></p>

          <blockquote>

          <p>linearly factored text embedding used concat in SDXL, Coca transformer
          mapping </p>

          </blockquote>

          '
        raw: "today published work, SITTA: A Semantic Image-Text Alignment for Image\
          \ Captioning\nhttps://github.com/ml-jku/semantic-image-text-alignment\n\
          https://arxiv.org/pdf/2307.05591.pdf\n**CLIP text image jointed vision llms,\
          \ no extra huge VIT, llama plugin, should easy adapt to danbooru database**\n\
          \nreading note:\nLanguage Embedding\nrandom permutations\n\nLanguage Embedding\
          \ related work:\n\nhttps://arxiv.org/pdf/2305.01278.pdf \n> Figure 5 llms\
          \ logprob better than CLIP roberta contextual embedding and CNN classification\
          \ head\n\nhttps://arxiv.org/pdf/2306.17842.pdf\n> approaching way is llms\
          \ understanding image, take llms token embedding map ( lexical tokens) to\
          \ vqvae codebook\nB.2 LLM Prompting / In-context denoising \nLimitations\
          \ task-specific conditions demo five choice is weak\n\nhttps://arxiv.org/pdf/2304.05653.pdf\n\
          > textual <eos> redundant with bucket&time emb used in SDXL \n\nhttps://arxiv.org/pdf/2302.14383.pdf\n\
          > linearly factored text embedding used concat in SDXL, Coca transformer\
          \ mapping \n\n"
        updatedAt: '2023-07-13T17:41:41.141Z'
      numEdits: 3
      reactions: []
    id: 64b0321e046ee558065cc2d1
    type: comment
  author: ke99L
  content: "today published work, SITTA: A Semantic Image-Text Alignment for Image\
    \ Captioning\nhttps://github.com/ml-jku/semantic-image-text-alignment\nhttps://arxiv.org/pdf/2307.05591.pdf\n\
    **CLIP text image jointed vision llms, no extra huge VIT, llama plugin, should\
    \ easy adapt to danbooru database**\n\nreading note:\nLanguage Embedding\nrandom\
    \ permutations\n\nLanguage Embedding related work:\n\nhttps://arxiv.org/pdf/2305.01278.pdf\
    \ \n> Figure 5 llms logprob better than CLIP roberta contextual embedding and\
    \ CNN classification head\n\nhttps://arxiv.org/pdf/2306.17842.pdf\n> approaching\
    \ way is llms understanding image, take llms token embedding map ( lexical tokens)\
    \ to vqvae codebook\nB.2 LLM Prompting / In-context denoising \nLimitations task-specific\
    \ conditions demo five choice is weak\n\nhttps://arxiv.org/pdf/2304.05653.pdf\n\
    > textual <eos> redundant with bucket&time emb used in SDXL \n\nhttps://arxiv.org/pdf/2302.14383.pdf\n\
    > linearly factored text embedding used concat in SDXL, Coca transformer mapping\
    \ \n\n"
  created_at: 2023-07-13 16:19:26+00:00
  edited: true
  hidden: false
  id: 64b0321e046ee558065cc2d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/277a5dd8603532e3853171d0cc1599ab.svg
      fullname: ke99L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ke99L
      type: user
    createdAt: '2023-07-21T13:58:04.000Z'
    data:
      status: closed
    id: 64ba8eec57f68b98b725ac19
    type: status-change
  author: ke99L
  created_at: 2023-07-21 12:58:04+00:00
  id: 64ba8eec57f68b98b725ac19
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: SmilingWolf/wd-v1-4-convnextv2-tagger-v2
repo_type: model
status: closed
target_branch: null
title: Prompt Inversion (captioner/tagger upstream task)
