!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ericlearner
conflicting_files: null
created_at: 2023-07-04 05:56:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46d849776d472006d1c75282262b7edf.svg
      fullname: maeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericlearner
      type: user
    createdAt: '2023-07-04T06:56:49.000Z'
    data:
      edited: false
      editors:
      - ericlearner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9462161660194397
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46d849776d472006d1c75282262b7edf.svg
          fullname: maeng
          isHf: false
          isPro: false
          name: ericlearner
          type: user
        html: '<p>I am Eric Maeng, Co-founder at Team learners.</p>

          <p>We sell AI-generated studio photos to hundreds of users a day . Customers
          love the photo-realistic quality and the fact that we''re the only service
          of its kind to offer such a wide variety of concepts.</p>

          <p>We''re experimenting with making photos that more closely resemble our
          customers and avoiding deformed hands/eyes. Lately, we''ve been thinking
          a lot about model training and the data required to capture and implement
          features such as face shape and eye shape.</p>

          <p>I''d love to discuss our challenges with you and talk about synergies
          and ways we can work together, including providing technical advice/data.</p>

          <p>Let me know if you are interested via email.<br>My contact is <a rel="nofollow"
          href="mailto:eric@learners.company">eric@learners.company</a> or +821020866510.</p>

          <p>Regards,<br>Eric</p>

          '
        raw: "I am Eric Maeng, Co-founder at Team learners.\r\n\r\nWe sell AI-generated\
          \ studio photos to hundreds of users a day . Customers love the photo-realistic\
          \ quality and the fact that we're the only service of its kind to offer\
          \ such a wide variety of concepts.\r\n \r\nWe're experimenting with making\
          \ photos that more closely resemble our customers and avoiding deformed\
          \ hands/eyes. Lately, we've been thinking a lot about model training and\
          \ the data required to capture and implement features such as face shape\
          \ and eye shape.\r\n\r\nI'd love to discuss our challenges with you and\
          \ talk about synergies and ways we can work together, including providing\
          \ technical advice/data.\r\n\r\nLet me know if you are interested via email.\r\
          \nMy contact is eric@learners.company or +821020866510.\r\n\r\nRegards,\r\
          \nEric"
        updatedAt: '2023-07-04T06:56:49.863Z'
      numEdits: 0
      reactions: []
    id: 64a3c2b1275825d2c9b0057c
    type: comment
  author: ericlearner
  content: "I am Eric Maeng, Co-founder at Team learners.\r\n\r\nWe sell AI-generated\
    \ studio photos to hundreds of users a day . Customers love the photo-realistic\
    \ quality and the fact that we're the only service of its kind to offer such a\
    \ wide variety of concepts.\r\n \r\nWe're experimenting with making photos that\
    \ more closely resemble our customers and avoiding deformed hands/eyes. Lately,\
    \ we've been thinking a lot about model training and the data required to capture\
    \ and implement features such as face shape and eye shape.\r\n\r\nI'd love to\
    \ discuss our challenges with you and talk about synergies and ways we can work\
    \ together, including providing technical advice/data.\r\n\r\nLet me know if you\
    \ are interested via email.\r\nMy contact is eric@learners.company or +821020866510.\r\
    \n\r\nRegards,\r\nEric"
  created_at: 2023-07-04 05:56:49+00:00
  edited: false
  hidden: false
  id: 64a3c2b1275825d2c9b0057c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/277a5dd8603532e3853171d0cc1599ab.svg
      fullname: ke99L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ke99L
      type: user
    createdAt: '2023-07-06T06:40:21.000Z'
    data:
      edited: true
      editors:
      - ke99L
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8010025024414062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/277a5dd8603532e3853171d0cc1599ab.svg
          fullname: ke99L
          isHf: false
          isPro: false
          name: ke99L
          type: user
        html: '<p>hey, Eric and SmilingWolf</p>

          <h4 id="to-eric">to Eric</h4>

          <p>hands/eyes is focused object, nowdays those details easy solve in webui
          plugins or holistic mixed model<br>another way is XL experiment refinement
          model, look actually give so much boost</p>

          <p>but yet <strong>lots issue use text prompting only</strong><br>current
          architectures</p>

          <ul>

          <li><strong>mode collapsed seem baked into model</strong>, un-common object
          structure still low quality even effects tuning<br>from HPSv2 tell us, current
          lora mix practices just higher fidelity its really not much gain compare
          dallemini/vqganclip<br>we still need study refer GANs solved OOD and long-tails
          issue<br>actual XL thinking just applies basic patching sampling trick found
          in vqganclip (augmentations) <a rel="nofollow" href="https://arxiv.org/pdf/2306.16805.pdf">https://arxiv.org/pdf/2306.16805.pdf</a></li>

          <li>feels locked overall visual (composition) / nowday tough became loras
          addicted<br>not like llms, <strong>we cant get capacity leap from unet or
          unet lora mix</strong> ( maybe we can gain from text encoder lora<br>affected
          by Resolution batch size more factor made much Inaccessible for holistic,
          also most paper used full-model fine-tuning relatively cost for holistic
          practices</li>

          <li>inconsistent artstyle distribution / <strong>if no multiple lora over
          expressed</strong> you''ll get very unstable sampling<br>latent allocation
          thinking Dr. Franken Stein, and maybe better apply forget model before tuning
          (I saw some practices)?</li>

          <li>easy Negative huge needed, wired prompting template (PEZ)<br>I saw linearly
          factored text embedding maybe solve this <a rel="nofollow" href="https://arxiv.org/pdf/2302.14383.pdf">https://arxiv.org/pdf/2302.14383.pdf</a></li>

          <li>prompting robust, automated reweight bias tuning(MJ shorten)<br><a rel="nofollow"
          href="https://arxiv.org/pdf/2306.16805.pdf">https://arxiv.org/pdf/2306.16805.pdf</a>,
          <a rel="nofollow" href="https://arxiv.org/pdf/2305.16934.pdf">https://arxiv.org/pdf/2305.16934.pdf</a></li>

          </ul>

          <p>real ai community need</p>

          <ul>

          <li>better FID/CLIPscore scoring like DreamSim, Automated evaluate holistic
          for text2im like <a rel="nofollow" href="https://github.com/aigoopy/llm-jeopardy">https://github.com/aigoopy/llm-jeopardy</a><br>parti
          benchmark <a href="https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard">https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard</a><br>HPSv2,
          Benchmarking-Awesome-Diffusion-Models only include few holistic model<br>and
          they are not automate prompting <a rel="nofollow" href="https://arxiv.org/pdf/2304.05390.pdf">https://arxiv.org/pdf/2304.05390.pdf</a>
          or a probe can traverses <a rel="nofollow" href="https://arxiv.org/pdf/2306.08687.pdf">https://arxiv.org/pdf/2306.08687.pdf</a></li>

          <li>generated or prompting analysis / AI-generated detection<br>civitai-337k
          <a href="https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k">https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k</a><br>SDv1.3
          vs MJv3 user groups Analysis <a rel="nofollow" href="https://arxiv.org/pdf/2303.04587.pdf">https://arxiv.org/pdf/2303.04587.pdf</a><br>Insight
          aiart tweet <a rel="nofollow" href="https://arxiv.org/pdf/2306.08310.pdf">https://arxiv.org/pdf/2306.08310.pdf</a></li>

          </ul>

          <p>holistic practice leads <strong>most everythings used is untraceable
          solutions</strong>, we still dont know What kind of dataset introduced and
          how they solve </p>

          <blockquote>

          <p>take example multiple character blended issue solved by introduced "anime
          couple" lora<br>just see SD team how hard get any mitigation progress in
          XL pretraining<br>community trend private tires, practices and founds (
          version control needed <a rel="nofollow" href="https://github.com/r-three/git-theta">https://github.com/r-three/git-theta</a></p>

          </blockquote>

          <p>they data lack on stylization domain, and we will be still use this tagger
          in <strong>XL tuning, we will meet really fatal tuning issue</strong><br>we
          saw XL didnt handle stylization well, they cant get mitigation anime images
          / use outdate BLIP2<br>they are restricted to <strong>CC crawl (datacomp)
          junks We can''t get any more stylization understanding or content diversity</strong>
          competition with waifulabs/MJ</p>

          <blockquote>

          <p>the current architectures seem a bit swamped in terms of controllability
          for artists. If one is just looking at generating generic photorealistic
          images (which of/c is an impressive feat) it is fine, but for highly stylized
          stuff the larger the model less control artists have</p>

          </blockquote>

          <p>take example these task that can use ft to reach g4 performance (orca)
          while others require understanding (textbook)</p>

          <blockquote>

          <p>pretraining need on rational-diversity controlled, small model reflect
          fact (tinystories/textbook highlight)</p>

          </blockquote>

          <h4 id="to-smilingwolf">to SmilingWolf</h4>

          <p>It been a while this "foundation model" out even its a classifier<br>I
          saw its became foundation of webui system. 14k download per mo<br>data clean,
          finetuning suit, prompting idea etc...</p>

          <p>here some future research</p>

          <p>LLM logprob better than classifier<br>Figure 5 <a rel="nofollow" href="https://arxiv.org/pdf/2305.01278.pdf">https://arxiv.org/pdf/2305.01278.pdf</a></p>

          <p>Personalizing Vison-Language Models needed<br>deterministic <a rel="nofollow"
          href="https://arxiv.org/pdf/2204.01694.pdf">https://arxiv.org/pdf/2204.01694.pdf</a><br>probabilistic
          <a rel="nofollow" href="https://arxiv.org/pdf/2307.00398.pdf">https://arxiv.org/pdf/2307.00398.pdf</a></p>

          <p>Personalizing Vison-LLMs needed, Prompt Inversion/ (actually captioner/tagger
          upstream)<br><a rel="nofollow" href="https://arxiv.org/pdf/2307.00716.pdf">https://arxiv.org/pdf/2307.00716.pdf</a><br>JourneyDB
          cluster the style into a hierarchical structure / summarize the 150, 000
          style words / 70, 521 fine-grained styles into 334 style categories ( per-category
          reach 41% in their Validation(OOD set)<br>In this way we can skip collecting
          from Internet to improved clip-interrogator</p>

          <p>combine refinement model idea,<br>looks really need <strong>style controlled
          trained base with increasing resolution cascaded trained refiner (save compute)</strong></p>

          '
        raw: "hey, Eric and SmilingWolf\n\n#### to Eric\nhands/eyes is focused object,\
          \ nowdays those details easy solve in webui plugins or holistic mixed model\
          \  \nanother way is XL experiment refinement model, look actually give so\
          \ much boost\n\nbut yet **lots issue use text prompting only**  \ncurrent\
          \ architectures\n\n-   **mode collapsed seem baked into model**, un-common\
          \ object structure still low quality even effects tuning\n    from HPSv2\
          \ tell us, current lora mix practices just higher fidelity its really not\
          \ much gain compare dallemini/vqganclip\n    we still need study refer GANs\
          \ solved OOD and long-tails issue\n    actual XL thinking just applies basic\
          \ patching sampling trick found in vqganclip (augmentations) https://arxiv.org/pdf/2306.16805.pdf\n\
          -   feels locked overall visual (composition) / nowday tough became loras\
          \ addicted  \n    not like llms, **we cant get capacity leap from unet or\
          \ unet lora mix** ( maybe we can gain from text encoder lora\n    affected\
          \ by Resolution batch size more factor made much Inaccessible for holistic,\
          \ also most paper used full-model fine-tuning relatively cost for holistic\
          \ practices\n-   inconsistent artstyle distribution / **if no multiple lora\
          \ over expressed** you'll get very unstable sampling  \n    latent allocation\
          \ thinking Dr. Franken Stein, and maybe better apply forget model before\
          \ tuning (I saw some practices)?\n-   easy Negative huge needed, wired prompting\
          \ template (PEZ)  \n    I saw linearly factored text embedding maybe solve\
          \ this [https://arxiv.org/pdf/2302.14383.pdf](https://arxiv.org/pdf/2302.14383.pdf)\n\
          -   prompting robust, automated reweight bias tuning(MJ shorten)  \n   \
          \ [https://arxiv.org/pdf/2306.16805.pdf](https://arxiv.org/pdf/2306.16805.pdf),\
          \ [https://arxiv.org/pdf/2305.16934.pdf](https://arxiv.org/pdf/2305.16934.pdf)\n\
          \nreal ai community need\n\n-   better FID/CLIPscore scoring like DreamSim,\
          \ Automated evaluate holistic for text2im like [https://github.com/aigoopy/llm-jeopardy](https://github.com/aigoopy/llm-jeopardy)\
          \  \n    parti benchmark [https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard)\
          \  \n    HPSv2, Benchmarking-Awesome-Diffusion-Models only include few holistic\
          \ model  \n    and they are not automate prompting [https://arxiv.org/pdf/2304.05390.pdf](https://arxiv.org/pdf/2304.05390.pdf)\
          \ or a probe can traverses https://arxiv.org/pdf/2306.08687.pdf\n-   generated\
          \ or prompting analysis / AI-generated detection  \n    civitai-337k [https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k](https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k)\
          \  \n    SDv1.3 vs MJv3 user groups Analysis [https://arxiv.org/pdf/2303.04587.pdf](https://arxiv.org/pdf/2303.04587.pdf)\
          \  \n    Insight aiart tweet [https://arxiv.org/pdf/2306.08310.pdf](https://arxiv.org/pdf/2306.08310.pdf)\n\
          \nholistic practice leads **most everythings used is untraceable solutions**,\
          \ we still dont know What kind of dataset introduced and how they solve\
          \ \n> take example multiple character blended issue solved by introduced\
          \ \"anime couple\" lora \njust see SD team how hard get any mitigation progress\
          \ in XL pretraining  \ncommunity trend private tires, practices and founds\
          \ ( version control needed [https://github.com/r-three/git-theta](https://github.com/r-three/git-theta)\n\
          \nthey data lack on stylization domain, and we will be still use this tagger\
          \ in **XL tuning, we will meet really fatal tuning issue**  \nwe saw XL\
          \ didnt handle stylization well, they cant get mitigation anime images /\
          \ use outdate BLIP2  \nthey are restricted to **CC crawl (datacomp) junks\
          \ We can't get any more stylization understanding or content diversity**\
          \ competition with waifulabs/MJ\n> the current architectures seem a bit\
          \ swamped in terms of controllability for artists. If one is just looking\
          \ at generating generic photorealistic images (which of/c is an impressive\
          \ feat) it is fine, but for highly stylized stuff the larger the model less\
          \ control artists have\n\ntake example these task that can use ft to reach\
          \ g4 performance (orca) while others require understanding (textbook)\n\
          > pretraining need on rational-diversity controlled, small model reflect\
          \ fact (tinystories/textbook highlight)\n\n#### to SmilingWolf\n\nIt been\
          \ a while this \"foundation model\" out even its a classifier  \nI saw its\
          \ became foundation of webui system. 14k download per mo  \ndata clean,\
          \ finetuning suit, prompting idea etc...\n\nhere some future research\n\n\
          LLM logprob better than classifier  \nFigure 5 [https://arxiv.org/pdf/2305.01278.pdf](https://arxiv.org/pdf/2305.01278.pdf)\n\
          \nPersonalizing Vison-Language Models needed  \ndeterministic [https://arxiv.org/pdf/2204.01694.pdf](https://arxiv.org/pdf/2204.01694.pdf)\
          \  \nprobabilistic [https://arxiv.org/pdf/2307.00398.pdf](https://arxiv.org/pdf/2307.00398.pdf)\n\
          \nPersonalizing Vison-LLMs needed, Prompt Inversion/ (actually captioner/tagger\
          \ upstream)  \n[https://arxiv.org/pdf/2307.00716.pdf](https://arxiv.org/pdf/2307.00716.pdf)\
          \  \nJourneyDB cluster the style into a hierarchical structure / summarize\
          \ the 150, 000 style words / 70, 521 fine-grained styles into 334 style\
          \ categories ( per-category reach 41% in their Validation(OOD set)  \nIn\
          \ this way we can skip collecting from Internet to improved clip-interrogator\n\
          \ncombine refinement model idea,  \nlooks really need **style controlled\
          \ trained base with increasing resolution cascaded trained refiner (save\
          \ compute)**"
        updatedAt: '2023-07-06T13:55:36.659Z'
      numEdits: 24
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ericlearner
    id: 64a661d52c59891d5705f918
    type: comment
  author: ke99L
  content: "hey, Eric and SmilingWolf\n\n#### to Eric\nhands/eyes is focused object,\
    \ nowdays those details easy solve in webui plugins or holistic mixed model  \n\
    another way is XL experiment refinement model, look actually give so much boost\n\
    \nbut yet **lots issue use text prompting only**  \ncurrent architectures\n\n\
    -   **mode collapsed seem baked into model**, un-common object structure still\
    \ low quality even effects tuning\n    from HPSv2 tell us, current lora mix practices\
    \ just higher fidelity its really not much gain compare dallemini/vqganclip\n\
    \    we still need study refer GANs solved OOD and long-tails issue\n    actual\
    \ XL thinking just applies basic patching sampling trick found in vqganclip (augmentations)\
    \ https://arxiv.org/pdf/2306.16805.pdf\n-   feels locked overall visual (composition)\
    \ / nowday tough became loras addicted  \n    not like llms, **we cant get capacity\
    \ leap from unet or unet lora mix** ( maybe we can gain from text encoder lora\n\
    \    affected by Resolution batch size more factor made much Inaccessible for\
    \ holistic, also most paper used full-model fine-tuning relatively cost for holistic\
    \ practices\n-   inconsistent artstyle distribution / **if no multiple lora over\
    \ expressed** you'll get very unstable sampling  \n    latent allocation thinking\
    \ Dr. Franken Stein, and maybe better apply forget model before tuning (I saw\
    \ some practices)?\n-   easy Negative huge needed, wired prompting template (PEZ)\
    \  \n    I saw linearly factored text embedding maybe solve this [https://arxiv.org/pdf/2302.14383.pdf](https://arxiv.org/pdf/2302.14383.pdf)\n\
    -   prompting robust, automated reweight bias tuning(MJ shorten)  \n    [https://arxiv.org/pdf/2306.16805.pdf](https://arxiv.org/pdf/2306.16805.pdf),\
    \ [https://arxiv.org/pdf/2305.16934.pdf](https://arxiv.org/pdf/2305.16934.pdf)\n\
    \nreal ai community need\n\n-   better FID/CLIPscore scoring like DreamSim, Automated\
    \ evaluate holistic for text2im like [https://github.com/aigoopy/llm-jeopardy](https://github.com/aigoopy/llm-jeopardy)\
    \  \n    parti benchmark [https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard)\
    \  \n    HPSv2, Benchmarking-Awesome-Diffusion-Models only include few holistic\
    \ model  \n    and they are not automate prompting [https://arxiv.org/pdf/2304.05390.pdf](https://arxiv.org/pdf/2304.05390.pdf)\
    \ or a probe can traverses https://arxiv.org/pdf/2306.08687.pdf\n-   generated\
    \ or prompting analysis / AI-generated detection  \n    civitai-337k [https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k](https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k)\
    \  \n    SDv1.3 vs MJv3 user groups Analysis [https://arxiv.org/pdf/2303.04587.pdf](https://arxiv.org/pdf/2303.04587.pdf)\
    \  \n    Insight aiart tweet [https://arxiv.org/pdf/2306.08310.pdf](https://arxiv.org/pdf/2306.08310.pdf)\n\
    \nholistic practice leads **most everythings used is untraceable solutions**,\
    \ we still dont know What kind of dataset introduced and how they solve \n> take\
    \ example multiple character blended issue solved by introduced \"anime couple\"\
    \ lora \njust see SD team how hard get any mitigation progress in XL pretraining\
    \  \ncommunity trend private tires, practices and founds ( version control needed\
    \ [https://github.com/r-three/git-theta](https://github.com/r-three/git-theta)\n\
    \nthey data lack on stylization domain, and we will be still use this tagger in\
    \ **XL tuning, we will meet really fatal tuning issue**  \nwe saw XL didnt handle\
    \ stylization well, they cant get mitigation anime images / use outdate BLIP2\
    \  \nthey are restricted to **CC crawl (datacomp) junks We can't get any more\
    \ stylization understanding or content diversity** competition with waifulabs/MJ\n\
    > the current architectures seem a bit swamped in terms of controllability for\
    \ artists. If one is just looking at generating generic photorealistic images\
    \ (which of/c is an impressive feat) it is fine, but for highly stylized stuff\
    \ the larger the model less control artists have\n\ntake example these task that\
    \ can use ft to reach g4 performance (orca) while others require understanding\
    \ (textbook)\n> pretraining need on rational-diversity controlled, small model\
    \ reflect fact (tinystories/textbook highlight)\n\n#### to SmilingWolf\n\nIt been\
    \ a while this \"foundation model\" out even its a classifier  \nI saw its became\
    \ foundation of webui system. 14k download per mo  \ndata clean, finetuning suit,\
    \ prompting idea etc...\n\nhere some future research\n\nLLM logprob better than\
    \ classifier  \nFigure 5 [https://arxiv.org/pdf/2305.01278.pdf](https://arxiv.org/pdf/2305.01278.pdf)\n\
    \nPersonalizing Vison-Language Models needed  \ndeterministic [https://arxiv.org/pdf/2204.01694.pdf](https://arxiv.org/pdf/2204.01694.pdf)\
    \  \nprobabilistic [https://arxiv.org/pdf/2307.00398.pdf](https://arxiv.org/pdf/2307.00398.pdf)\n\
    \nPersonalizing Vison-LLMs needed, Prompt Inversion/ (actually captioner/tagger\
    \ upstream)  \n[https://arxiv.org/pdf/2307.00716.pdf](https://arxiv.org/pdf/2307.00716.pdf)\
    \  \nJourneyDB cluster the style into a hierarchical structure / summarize the\
    \ 150, 000 style words / 70, 521 fine-grained styles into 334 style categories\
    \ ( per-category reach 41% in their Validation(OOD set)  \nIn this way we can\
    \ skip collecting from Internet to improved clip-interrogator\n\ncombine refinement\
    \ model idea,  \nlooks really need **style controlled trained base with increasing\
    \ resolution cascaded trained refiner (save compute)**"
  created_at: 2023-07-06 05:40:21+00:00
  edited: true
  hidden: false
  id: 64a661d52c59891d5705f918
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46d849776d472006d1c75282262b7edf.svg
      fullname: maeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericlearner
      type: user
    createdAt: '2023-07-06T14:07:01.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/46d849776d472006d1c75282262b7edf.svg
          fullname: maeng
          isHf: false
          isPro: false
          name: ericlearner
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-06T14:13:40.227Z'
      numEdits: 0
      reactions: []
    id: 64a6ca8535c3af3adeff759f
    type: comment
  author: ericlearner
  content: This comment has been hidden
  created_at: 2023-07-06 13:07:01+00:00
  edited: true
  hidden: true
  id: 64a6ca8535c3af3adeff759f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca937eaa96c468de7329fd88b3bd101a.svg
      fullname: seungjin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nevermindddd
      type: user
    createdAt: '2023-07-06T14:21:48.000Z'
    data:
      edited: false
      editors:
      - nevermindddd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.21692879498004913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca937eaa96c468de7329fd88b3bd101a.svg
          fullname: seungjin
          isHf: false
          isPro: false
          name: nevermindddd
          type: user
        html: '<p>turing test falied</p>

          '
        raw: turing test falied
        updatedAt: '2023-07-06T14:21:48.524Z'
      numEdits: 0
      reactions: []
    id: 64a6cdfc9eef427695c23de3
    type: comment
  author: nevermindddd
  content: turing test falied
  created_at: 2023-07-06 13:21:48+00:00
  edited: false
  hidden: false
  id: 64a6cdfc9eef427695c23de3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46d849776d472006d1c75282262b7edf.svg
      fullname: maeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericlearner
      type: user
    createdAt: '2023-07-06T14:28:12.000Z'
    data:
      edited: false
      editors:
      - ericlearner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8812968730926514
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46d849776d472006d1c75282262b7edf.svg
          fullname: maeng
          isHf: false
          isPro: false
          name: ericlearner
          type: user
        html: '<p>Lets keep the discussion focused on personalization of generative
          models.<br>Why dont we hop in for a call?</p>

          '
        raw: 'Lets keep the discussion focused on personalization of generative models.

          Why dont we hop in for a call?'
        updatedAt: '2023-07-06T14:28:12.377Z'
      numEdits: 0
      reactions: []
    id: 64a6cf7c8d069cd624a7a381
    type: comment
  author: ericlearner
  content: 'Lets keep the discussion focused on personalization of generative models.

    Why dont we hop in for a call?'
  created_at: 2023-07-06 13:28:12+00:00
  edited: false
  hidden: false
  id: 64a6cf7c8d069cd624a7a381
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/277a5dd8603532e3853171d0cc1599ab.svg
      fullname: ke99L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ke99L
      type: user
    createdAt: '2023-07-06T15:43:01.000Z'
    data:
      edited: true
      editors:
      - ke99L
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.902941882610321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/277a5dd8603532e3853171d0cc1599ab.svg
          fullname: ke99L
          isHf: false
          isPro: false
          name: ke99L
          type: user
        html: "<blockquote>\n<p>Lets keep the discussion focused on personalization\
          \ of generative models.<br>Why dont we hop in for a call?</p>\n</blockquote>\n\
          <p>nope, this discussion under the tagger<br>actual I'm not interested Interpret\
          \ \"Independent Developers\" high fidelity recipe, but natural language\
          \ friendly text2im like MJ<br>since there have been countless holistic practices...\
          \ and weird naming ((  why not just post issue under webui or trainer?<br>everyone\
          \ mostly used is mainly handcraft mitigation using few shot (SAM made this\
          \ easy) attempt (svd lora, diff lora merged)</p>\n<h3 id=\"little-anxious-nowadays-community-also-a-bit-dead-make-hard-assemble-knowledge-also-impossible-to-produce-next-generation-models\"\
          >little anxious, <strong>Nowadays community also a bit \"dead\", make hard\
          \ assemble knowledge also Impossible to produce next generation models</strong></h3>\n\
          <p>when vqganclip we \"using complex gradient to drawing (channel offset\
          \ noise been used idea in vqganclip)\", \u200Band now \"picking lora crayon\
          \ to drawing\", whats next?</p>\n<h2 id=\"based-on-a-mode-collapse-model-did-they-really-get-any-progress\"\
          >based on a mode collapse model, did they really get any progress?</h2>\n\
          <p>mode collapse resulting (I think) almost half of the latent space was\
          \ dead<br>If they havent found the diff lora merged (local mode fix) &amp;\
          \ negative embedding (reject hole) in practices, doing tuning will be very\
          \ challenging</p>\n<h3 id=\"about-1-hours-he-got-xl-in-1-random-test-i-was-able-to-train-25-styles-into-1-xl-lora--gosh-whole-pixiv-can-baked-into-xl-\"\
          >(about 1 hours he got XL), in 1 random test I was able to train 25 styles\
          \ into 1 xl lora. ( gosh, whole pixiv can baked into XL ....</h3>\n<p>Im\
          \ care \"experts lora\" no more merged, care take all lora on only one XL\
          \ base (the official), SD do exact opposite way to MJ</p>\n<h2 id=\"object-mode-collapse-happen-on-every-training-practices-is-architectures-fault\"\
          >object mode collapse happen on every training practices. is architectures\
          \ fault</h2>\n<p>mode collapse wont happen in <strong>base</strong> step,\
          \ is not Stability fault, this step common is large-scale, long-tail mixtures.</p>\n\
          <p>but will severe happen on <strong>full-model</strong> baking step, can\
          \ mitigation by domain data, ood data</p>\n<p>recall : we made <strong>mixes</strong>\
          \ and lora hell using target few shot, regularization</p>\n<h2 id=\"same-as-palavra-not-only-focused-object\"\
          >same as palavra not only focused object</h2>\n<p>next gen CLIP will be\
          \ <a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2304.05523.pdf\">https://arxiv.org/pdf/2304.05523.pdf</a>\
          \ we can use apply unCLIP (hyb emb) to nextgen text2im</p>\n"
        raw: "> Lets keep the discussion focused on personalization of generative\
          \ models.\n> Why dont we hop in for a call?\n\nnope, this discussion under\
          \ the tagger\nactual I'm not interested Interpret \"Independent Developers\"\
          \ high fidelity recipe, but natural language friendly text2im like MJ\n\
          since there have been countless holistic practices... and weird naming ((\
          \  why not just post issue under webui or trainer?\neveryone mostly used\
          \ is mainly handcraft mitigation using few shot (SAM made this easy) attempt\
          \ (svd lora, diff lora merged)\n\n### little anxious, **Nowadays community\
          \ also a bit \"dead\", make hard assemble knowledge also Impossible to produce\
          \ next generation models**\nwhen vqganclip we \"using complex gradient to\
          \ drawing (channel offset noise been used idea in vqganclip)\", \u200Band\
          \ now \"picking lora crayon to drawing\", whats next?\n\n## based on a mode\
          \ collapse model, did they really get any progress?\nmode collapse resulting\
          \ (I think) almost half of the latent space was dead\nIf they havent found\
          \ the diff lora merged (local mode fix) & negative embedding (reject hole)\
          \ in practices, doing tuning will be very challenging\n\n### (about 1 hours\
          \ he got XL), in 1 random test I was able to train 25 styles into 1 xl lora.\
          \ ( gosh, whole pixiv can baked into XL ....\nIm care \"experts lora\" no\
          \ more merged, care take all lora on only one XL base (the official), SD\
          \ do exact opposite way to MJ\n\n## object mode collapse happen on every\
          \ training practices. is architectures fault\n\nmode collapse wont happen\
          \ in **base** step, is not Stability fault, this step common is large-scale,\
          \ long-tail mixtures.\n\nbut will severe happen on **full-model** baking\
          \ step, can mitigation by domain data, ood data\n\nrecall : we made **mixes**\
          \ and lora hell using target few shot, regularization\n\n## same as palavra\
          \ not only focused object\nnext gen CLIP will be https://arxiv.org/pdf/2304.05523.pdf\
          \ we can use apply unCLIP (hyb emb) to nextgen text2im\n"
        updatedAt: '2023-07-08T09:29:03.444Z'
      numEdits: 16
      reactions: []
    id: 64a6e1058fc97fb32a798cee
    type: comment
  author: ke99L
  content: "> Lets keep the discussion focused on personalization of generative models.\n\
    > Why dont we hop in for a call?\n\nnope, this discussion under the tagger\nactual\
    \ I'm not interested Interpret \"Independent Developers\" high fidelity recipe,\
    \ but natural language friendly text2im like MJ\nsince there have been countless\
    \ holistic practices... and weird naming ((  why not just post issue under webui\
    \ or trainer?\neveryone mostly used is mainly handcraft mitigation using few shot\
    \ (SAM made this easy) attempt (svd lora, diff lora merged)\n\n### little anxious,\
    \ **Nowadays community also a bit \"dead\", make hard assemble knowledge also\
    \ Impossible to produce next generation models**\nwhen vqganclip we \"using complex\
    \ gradient to drawing (channel offset noise been used idea in vqganclip)\", \u200B\
    and now \"picking lora crayon to drawing\", whats next?\n\n## based on a mode\
    \ collapse model, did they really get any progress?\nmode collapse resulting (I\
    \ think) almost half of the latent space was dead\nIf they havent found the diff\
    \ lora merged (local mode fix) & negative embedding (reject hole) in practices,\
    \ doing tuning will be very challenging\n\n### (about 1 hours he got XL), in 1\
    \ random test I was able to train 25 styles into 1 xl lora. ( gosh, whole pixiv\
    \ can baked into XL ....\nIm care \"experts lora\" no more merged, care take all\
    \ lora on only one XL base (the official), SD do exact opposite way to MJ\n\n\
    ## object mode collapse happen on every training practices. is architectures fault\n\
    \nmode collapse wont happen in **base** step, is not Stability fault, this step\
    \ common is large-scale, long-tail mixtures.\n\nbut will severe happen on **full-model**\
    \ baking step, can mitigation by domain data, ood data\n\nrecall : we made **mixes**\
    \ and lora hell using target few shot, regularization\n\n## same as palavra not\
    \ only focused object\nnext gen CLIP will be https://arxiv.org/pdf/2304.05523.pdf\
    \ we can use apply unCLIP (hyb emb) to nextgen text2im\n"
  created_at: 2023-07-06 14:43:01+00:00
  edited: true
  hidden: false
  id: 64a6e1058fc97fb32a798cee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: SmilingWolf/wd-v1-4-convnextv2-tagger-v2
repo_type: model
status: open
target_branch: null
title: Proposal for collaboration
