!!python/object:huggingface_hub.community.DiscussionWithDetails
author: finilok
conflicting_files: null
created_at: 2023-11-11 17:00:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/810ad829eca7a0814558d558ee4cc579.svg
      fullname: Nora Estrela
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: finilok
      type: user
    createdAt: '2023-11-11T17:00:57.000Z'
    data:
      edited: false
      editors:
      - finilok
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658759236335754
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/810ad829eca7a0814558d558ee4cc579.svg
          fullname: Nora Estrela
          isHf: false
          isPro: false
          name: finilok
          type: user
        html: '<p>There''s no way that a filled 200K context maxes out at only a few
          gigs more than the model itself, is there? How much RAM/VRAM would it actually
          take to fill the whole context window?</p>

          '
        raw: There's no way that a filled 200K context maxes out at only a few gigs
          more than the model itself, is there? How much RAM/VRAM would it actually
          take to fill the whole context window?
        updatedAt: '2023-11-11T17:00:57.647Z'
      numEdits: 0
      reactions: []
    id: 654fb3490c11ee1eb9c1cb45
    type: comment
  author: finilok
  content: There's no way that a filled 200K context maxes out at only a few gigs
    more than the model itself, is there? How much RAM/VRAM would it actually take
    to fill the whole context window?
  created_at: 2023-11-11 17:00:57+00:00
  edited: false
  hidden: false
  id: 654fb3490c11ee1eb9c1cb45
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-11T18:04:01.000Z'
    data:
      edited: true
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8095110058784485
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<p>llama.cpp uses 16bit KV by default.. So for <code>-c 200000</code>
          with the 34B you''ll need 46.875 GiB for context in addition to whatever
          memory is needed to load the model. For the 6B 200K , context requires <code>12.91</code>
          GiB.</p>

          '
        raw: llama.cpp uses 16bit KV by default.. So for `-c 200000` with the 34B
          you'll need 46.875 GiB for context in addition to whatever memory is needed
          to load the model. For the 6B 200K , context requires `12.91` GiB.
        updatedAt: '2023-11-11T20:08:44.303Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Beck777
        - martinmashy
    id: 654fc2111b20e97bf6e4dcc6
    type: comment
  author: KerfuffleV2
  content: llama.cpp uses 16bit KV by default.. So for `-c 200000` with the 34B you'll
    need 46.875 GiB for context in addition to whatever memory is needed to load the
    model. For the 6B 200K , context requires `12.91` GiB.
  created_at: 2023-11-11 18:04:01+00:00
  edited: true
  hidden: false
  id: 654fc2111b20e97bf6e4dcc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-11T18:08:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.946951687335968
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;finilok&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/finilok\">@<span class=\"\
          underline\">finilok</span></a></span>\n\n\t</span></span> Yeah you're right,\
          \ I haven't ever set that table up to update for the max context length\
          \ of extended context models. It assumes 4096 context.  I have been meaning\
          \ to look at that for a while.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;KerfuffleV2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KerfuffleV2\"\
          >@<span class=\"underline\">KerfuffleV2</span></a></span>\n\n\t</span></span>\
          \ awesome, thanks for the details! I thought I was going to have to do some\
          \ tedious testing of models at different context lengths, but if there's\
          \ a formula then I can set up some code to calculate it automatically when\
          \ making the README.</p>\n<p>Can you tell me how you got those figures and\
          \ whether there's a metadata field I could read from the gguf to do that\
          \ automatically? Which I hope will be straightforward now that we have your\
          \ great get/set metadata script!</p>\n<p>The context length I can already\
          \ from config.json, but I don't know what model hyperparms I then use to\
          \ get the figures you've shown.</p>\n"
        raw: '@finilok Yeah you''re right, I haven''t ever set that table up to update
          for the max context length of extended context models. It assumes 4096 context.  I
          have been meaning to look at that for a while.


          @KerfuffleV2 awesome, thanks for the details! I thought I was going to have
          to do some tedious testing of models at different context lengths, but if
          there''s a formula then I can set up some code to calculate it automatically
          when making the README.


          Can you tell me how you got those figures and whether there''s a metadata
          field I could read from the gguf to do that automatically? Which I hope
          will be straightforward now that we have your great get/set metadata script!


          The context length I can already from config.json, but I don''t know what
          model hyperparms I then use to get the figures you''ve shown.'
        updatedAt: '2023-11-11T18:08:52.878Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Igor7777
    id: 654fc3341b3d3092a1c4aacc
    type: comment
  author: TheBloke
  content: '@finilok Yeah you''re right, I haven''t ever set that table up to update
    for the max context length of extended context models. It assumes 4096 context.  I
    have been meaning to look at that for a while.


    @KerfuffleV2 awesome, thanks for the details! I thought I was going to have to
    do some tedious testing of models at different context lengths, but if there''s
    a formula then I can set up some code to calculate it automatically when making
    the README.


    Can you tell me how you got those figures and whether there''s a metadata field
    I could read from the gguf to do that automatically? Which I hope will be straightforward
    now that we have your great get/set metadata script!


    The context length I can already from config.json, but I don''t know what model
    hyperparms I then use to get the figures you''ve shown.'
  created_at: 2023-11-11 18:08:52+00:00
  edited: false
  hidden: false
  id: 654fc3341b3d3092a1c4aacc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-11T20:08:09.000Z'
    data:
      edited: true
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7555449604988098
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I just loaded\
          \ the model with <code>-c</code> set and looked at the output:</p>\n<p>For\
          \ example:</p>\n<p>6B with <code>-c 200000</code>:</p>\n<pre><code>llama_new_context_with_model:\
          \ kv self size  = 12500.00 MB\n</code></pre>\n<p>34B with <code>-c 50000</code>:</p>\n\
          <pre><code>llama_new_context_with_model: kv self size  = 11718.75 MB\n</code></pre>\n\
          <p>34B with <code>-c 10000</code>:</p>\n<pre><code>llama_new_context_with_model:\
          \ kv self size  = 2343.75 MB\n</code></pre>\n<p><code>2343.75 * 5</code>\
          \ is <code>11718.75</code>(same as <code>-c 50000</code>) so <code>-c 200000</code>\
          \ must be <code>11718.75 * 4</code> = <code>46875.0</code></p>\n<p>As for\
          \ calculating it automatically, kind of a pain. I dug into the KV cache\
          \ init code and came up with this (interactive Python interpreter result):</p>\n\
          <pre><code class=\"language-plaintext\">&gt;&gt;&gt; n_layer=60 # llama.block_count\n\
          &gt;&gt;&gt; n_head=56 # llama.attention.head_count\n&gt;&gt;&gt; n_head_kv=8\
          \ # llama.attention.head_count_kv\n&gt;&gt;&gt; n_embd=7168 # llama.embedding_length\n\
          &gt;&gt;&gt; n_gqa=n_head/n_head_kv\n&gt;&gt;&gt; n_embd_gqa=n_embd/n_gqa\n\
          &gt;&gt;&gt; n_ctx=200000\n&gt;&gt;&gt; n_elements=n_embd_gqa*(n_layer*n_ctx)\n\
          &gt;&gt;&gt; 2 * n_elements * 2 # in bytes\n49152000000.0\n&gt;&gt;&gt;\
          \ (2 * n_elements * 2) / (1024 * 1024) # In MiB\n46875.0\n</code></pre>\n\
          <p>Also, whoops - it seems like the original values already were MiB even\
          \ though the output had \"MB\". </p>\n<p>The reason it's <code>2 * 2 * n_elements</code>\
          \ at the end is because there's both <code>k</code> and <code>v</code> parts\
          \ of the KV cache each with <code>n_elements</code> elements, and each element\
          \ is 16bit (2 bytes).</p>\n<p>BTW, not sure if you noticed but <code>gguf-dump.py</code>\
          \ supports JSON output so extracting metadata to JSON format should be pretty\
          \ easy.</p>\n"
        raw: "@TheBloke I just loaded the model with `-c` set and looked at the output:\n\
          \nFor example:\n\n6B with `-c 200000`:\n\n    llama_new_context_with_model:\
          \ kv self size  = 12500.00 MB\n\n34B with `-c 50000`:\n\n    llama_new_context_with_model:\
          \ kv self size  = 11718.75 MB\n\n34B with `-c 10000`:\n\n    llama_new_context_with_model:\
          \ kv self size  = 2343.75 MB\n\n`2343.75 * 5` is `11718.75`(same as `-c\
          \ 50000`) so `-c 200000` must be `11718.75 * 4` = `46875.0`\n\nAs for calculating\
          \ it automatically, kind of a pain. I dug into the KV cache init code and\
          \ came up with this (interactive Python interpreter result):\n\n```plaintext\n\
          >>> n_layer=60 # llama.block_count\n>>> n_head=56 # llama.attention.head_count\n\
          >>> n_head_kv=8 # llama.attention.head_count_kv\n>>> n_embd=7168 # llama.embedding_length\n\
          >>> n_gqa=n_head/n_head_kv\n>>> n_embd_gqa=n_embd/n_gqa\n>>> n_ctx=200000\n\
          >>> n_elements=n_embd_gqa*(n_layer*n_ctx)\n>>> 2 * n_elements * 2 # in bytes\n\
          49152000000.0\n>>> (2 * n_elements * 2) / (1024 * 1024) # In MiB\n46875.0\n\
          ```\n\nAlso, whoops - it seems like the original values already were MiB\
          \ even though the output had \"MB\". \n\nThe reason it's `2 * 2 * n_elements`\
          \ at the end is because there's both `k` and `v` parts of the KV cache each\
          \ with `n_elements` elements, and each element is 16bit (2 bytes).\n\nBTW,\
          \ not sure if you noticed but `gguf-dump.py` supports JSON output so extracting\
          \ metadata to JSON format should be pretty easy."
        updatedAt: '2023-11-11T20:42:01.252Z'
      numEdits: 2
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - Igor7777
        - jlzhou
        - ssbg2
        - Beck777
      - count: 2
        reaction: "\U0001F917"
        users:
        - Yhyu13
        - jlzhou
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - jlzhou
    id: 654fdf2981c52527f4062237
    type: comment
  author: KerfuffleV2
  content: "@TheBloke I just loaded the model with `-c` set and looked at the output:\n\
    \nFor example:\n\n6B with `-c 200000`:\n\n    llama_new_context_with_model: kv\
    \ self size  = 12500.00 MB\n\n34B with `-c 50000`:\n\n    llama_new_context_with_model:\
    \ kv self size  = 11718.75 MB\n\n34B with `-c 10000`:\n\n    llama_new_context_with_model:\
    \ kv self size  = 2343.75 MB\n\n`2343.75 * 5` is `11718.75`(same as `-c 50000`)\
    \ so `-c 200000` must be `11718.75 * 4` = `46875.0`\n\nAs for calculating it automatically,\
    \ kind of a pain. I dug into the KV cache init code and came up with this (interactive\
    \ Python interpreter result):\n\n```plaintext\n>>> n_layer=60 # llama.block_count\n\
    >>> n_head=56 # llama.attention.head_count\n>>> n_head_kv=8 # llama.attention.head_count_kv\n\
    >>> n_embd=7168 # llama.embedding_length\n>>> n_gqa=n_head/n_head_kv\n>>> n_embd_gqa=n_embd/n_gqa\n\
    >>> n_ctx=200000\n>>> n_elements=n_embd_gqa*(n_layer*n_ctx)\n>>> 2 * n_elements\
    \ * 2 # in bytes\n49152000000.0\n>>> (2 * n_elements * 2) / (1024 * 1024) # In\
    \ MiB\n46875.0\n```\n\nAlso, whoops - it seems like the original values already\
    \ were MiB even though the output had \"MB\". \n\nThe reason it's `2 * 2 * n_elements`\
    \ at the end is because there's both `k` and `v` parts of the KV cache each with\
    \ `n_elements` elements, and each element is 16bit (2 bytes).\n\nBTW, not sure\
    \ if you noticed but `gguf-dump.py` supports JSON output so extracting metadata\
    \ to JSON format should be pretty easy."
  created_at: 2023-11-11 20:08:09+00:00
  edited: true
  hidden: false
  id: 654fdf2981c52527f4062237
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-12T05:04:27.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8769754767417908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KerfuffleV2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KerfuffleV2\"\
          >@<span class=\"underline\">KerfuffleV2</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Seems the ctx len memeory cost is going to be an issue in the\
          \ near future. Just wonder if there is any effor in reducing each coefficient\
          \ in the formula</p>\n<pre><code>2 * n_elements * 2 # in bytes\n</code></pre>\n\
          <p>E.g.  using FP4/8 to compress kv cache, or considering the sparsity of\
          \ kv cache. Just throwing some ideas out of my mind</p>\n"
        raw: "@KerfuffleV2 \n\nSeems the ctx len memeory cost is going to be an issue\
          \ in the near future. Just wonder if there is any effor in reducing each\
          \ coefficient in the formula\n\n```\n2 * n_elements * 2 # in bytes\n```\n\
          \nE.g.  using FP4/8 to compress kv cache, or considering the sparsity of\
          \ kv cache. Just throwing some ideas out of my mind"
        updatedAt: '2023-11-12T05:04:27.567Z'
      numEdits: 0
      reactions: []
    id: 65505cdb9b42dac8f17c11b8
    type: comment
  author: Yhyu13
  content: "@KerfuffleV2 \n\nSeems the ctx len memeory cost is going to be an issue\
    \ in the near future. Just wonder if there is any effor in reducing each coefficient\
    \ in the formula\n\n```\n2 * n_elements * 2 # in bytes\n```\n\nE.g.  using FP4/8\
    \ to compress kv cache, or considering the sparsity of kv cache. Just throwing\
    \ some ideas out of my mind"
  created_at: 2023-11-12 05:04:27+00:00
  edited: false
  hidden: false
  id: 65505cdb9b42dac8f17c11b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-12T06:14:47.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9804080724716187
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> There's this pull\
          \ to allow quantizing the KV cache to Q8_0 (8bit): <a rel=\"nofollow\" href=\"\
          https://github.com/ggerganov/llama.cpp/pull/2969\">https://github.com/ggerganov/llama.cpp/pull/2969</a></p>\n\
          <p>However, it was a big, complicated pull and touched a lot of other stuff.\
          \ It hasn't been updated in some time, so I think the author may have given\
          \ up on it. 4bit would probably have a pretty noticeable effect on quality.</p>\n"
        raw: '@Yhyu13 There''s this pull to allow quantizing the KV cache to Q8_0
          (8bit): https://github.com/ggerganov/llama.cpp/pull/2969


          However, it was a big, complicated pull and touched a lot of other stuff.
          It hasn''t been updated in some time, so I think the author may have given
          up on it. 4bit would probably have a pretty noticeable effect on quality.'
        updatedAt: '2023-11-12T06:14:47.166Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Yhyu13
        - Igor7777
    id: 65506d57734a0692ecab8d8f
    type: comment
  author: KerfuffleV2
  content: '@Yhyu13 There''s this pull to allow quantizing the KV cache to Q8_0 (8bit):
    https://github.com/ggerganov/llama.cpp/pull/2969


    However, it was a big, complicated pull and touched a lot of other stuff. It hasn''t
    been updated in some time, so I think the author may have given up on it. 4bit
    would probably have a pretty noticeable effect on quality.'
  created_at: 2023-11-12 06:14:47+00:00
  edited: false
  hidden: false
  id: 65506d57734a0692ecab8d8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4c0efea9bb6341dd3893dfa78d3d20c.svg
      fullname: Chip Andre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: candre23
      type: user
    createdAt: '2023-11-12T16:03:58.000Z'
    data:
      edited: false
      editors:
      - candre23
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9145744442939758
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4c0efea9bb6341dd3893dfa78d3d20c.svg
          fullname: Chip Andre
          isHf: false
          isPro: false
          name: candre23
          type: user
        html: '<p>Using KCPP set for 32k context limit with the Q5_M quant, context
          takes about 15GB.  Setting KCPP for 64k context (the max it will allow),
          context alone completely fills a 24GB P40.  It doesn''t go OOM, but it fails
          to fully load the model with other cuda errors.</p>

          <p>32k context is the most I can successfully do with a pair of P40s using
          KCPP.  Though it nominally works, the output is pure gibberish.  A string
          of random words in a mix of english and chinese.  Completely unusable.  I
          don''t know if this is an issue with the model itself, the quantization
          method, some incompatibility in KCPP, or a combination of factors.  In any
          event, it''s sadly unusable as-is.</p>

          <p>For reference, it took a solid 15 minutes to generate a ~500 token response
          to a 32k prompt on a pair of P40s.  Possibly still worthwhile in some situations,
          if the output was usable.</p>

          '
        raw: 'Using KCPP set for 32k context limit with the Q5_M quant, context takes
          about 15GB.  Setting KCPP for 64k context (the max it will allow), context
          alone completely fills a 24GB P40.  It doesn''t go OOM, but it fails to
          fully load the model with other cuda errors.


          32k context is the most I can successfully do with a pair of P40s using
          KCPP.  Though it nominally works, the output is pure gibberish.  A string
          of random words in a mix of english and chinese.  Completely unusable.  I
          don''t know if this is an issue with the model itself, the quantization
          method, some incompatibility in KCPP, or a combination of factors.  In any
          event, it''s sadly unusable as-is.


          For reference, it took a solid 15 minutes to generate a ~500 token response
          to a 32k prompt on a pair of P40s.  Possibly still worthwhile in some situations,
          if the output was usable.'
        updatedAt: '2023-11-12T16:03:58.283Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Yhyu13
    id: 6550f76e7bbfce1878fbe8be
    type: comment
  author: candre23
  content: 'Using KCPP set for 32k context limit with the Q5_M quant, context takes
    about 15GB.  Setting KCPP for 64k context (the max it will allow), context alone
    completely fills a 24GB P40.  It doesn''t go OOM, but it fails to fully load the
    model with other cuda errors.


    32k context is the most I can successfully do with a pair of P40s using KCPP.  Though
    it nominally works, the output is pure gibberish.  A string of random words in
    a mix of english and chinese.  Completely unusable.  I don''t know if this is
    an issue with the model itself, the quantization method, some incompatibility
    in KCPP, or a combination of factors.  In any event, it''s sadly unusable as-is.


    For reference, it took a solid 15 minutes to generate a ~500 token response to
    a 32k prompt on a pair of P40s.  Possibly still worthwhile in some situations,
    if the output was usable.'
  created_at: 2023-11-12 16:03:58+00:00
  edited: false
  hidden: false
  id: 6550f76e7bbfce1878fbe8be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-15T03:44:37.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9543707966804504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\"\
          >@<span class=\"underline\">Yhyu13</span></a></span>\n\n\t</span></span>\
          \ There's this pull to allow quantizing the KV cache to Q8_0 (8bit): <a\
          \ rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/pull/2969\"\
          >https://github.com/ggerganov/llama.cpp/pull/2969</a></p>\n<p>However, it\
          \ was a big, complicated pull and touched a lot of other stuff. It hasn't\
          \ been updated in some time, so I think the author may have given up on\
          \ it. 4bit would probably have a pretty noticeable effect on quality.</p>\n\
          </blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;KerfuffleV2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KerfuffleV2\"\
          >@<span class=\"underline\">KerfuffleV2</span></a></span>\n\n\t</span></span>\
          \ LMDeploy has actually successfully deploy a KV int8 PTQ method here <a\
          \ rel=\"nofollow\" href=\"https://github.com/yhyu13/lmdeploy/blob/main/docs/en/kv_int8.md\"\
          >https://github.com/yhyu13/lmdeploy/blob/main/docs/en/kv_int8.md</a>. </p>\n\
          <p>BTW, I found two papers presented by Google on using multi-query attention\
          \ instead of multi-head attention to reduce mem requirment for attentioin\
          \ kv cache<br><a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2211.05102.pdf\"\
          >https://arxiv.org/pdf/2211.05102.pdf</a><br><a rel=\"nofollow\" href=\"\
          https://ar5iv.labs.arxiv.org/html/2305.13245\">https://ar5iv.labs.arxiv.org/html/2305.13245</a><br>Not\
          \ sure if any of the open-source model severing platform has adapt to those</p>\n\
          <p>Moreover, Flash-attention is method to drop multi-head attention mem\
          \ scaling from quadratic to linear <a href=\"https://huggingface.co/blog/optimize-llm\"\
          >https://huggingface.co/blog/optimize-llm</a>, but forgive my ignorance,\
          \ I haven't found any clue that flash-attention would work with attention\
          \ weight quantization</p>\n"
        raw: "> @Yhyu13 There's this pull to allow quantizing the KV cache to Q8_0\
          \ (8bit): https://github.com/ggerganov/llama.cpp/pull/2969\n> \n> However,\
          \ it was a big, complicated pull and touched a lot of other stuff. It hasn't\
          \ been updated in some time, so I think the author may have given up on\
          \ it. 4bit would probably have a pretty noticeable effect on quality.\n\n\
          @KerfuffleV2 LMDeploy has actually successfully deploy a KV int8 PTQ method\
          \ here https://github.com/yhyu13/lmdeploy/blob/main/docs/en/kv_int8.md.\
          \ \n\nBTW, I found two papers presented by Google on using multi-query attention\
          \ instead of multi-head attention to reduce mem requirment for attentioin\
          \ kv cache\nhttps://arxiv.org/pdf/2211.05102.pdf\nhttps://ar5iv.labs.arxiv.org/html/2305.13245\n\
          Not sure if any of the open-source model severing platform has adapt to\
          \ those\n\nMoreover, Flash-attention is method to drop multi-head attention\
          \ mem scaling from quadratic to linear https://huggingface.co/blog/optimize-llm,\
          \ but forgive my ignorance, I haven't found any clue that flash-attention\
          \ would work with attention weight quantization"
        updatedAt: '2023-11-15T04:09:44.671Z'
      numEdits: 2
      reactions: []
    id: 65543ea55e88eda4381da374
    type: comment
  author: Yhyu13
  content: "> @Yhyu13 There's this pull to allow quantizing the KV cache to Q8_0 (8bit):\
    \ https://github.com/ggerganov/llama.cpp/pull/2969\n> \n> However, it was a big,\
    \ complicated pull and touched a lot of other stuff. It hasn't been updated in\
    \ some time, so I think the author may have given up on it. 4bit would probably\
    \ have a pretty noticeable effect on quality.\n\n@KerfuffleV2 LMDeploy has actually\
    \ successfully deploy a KV int8 PTQ method here https://github.com/yhyu13/lmdeploy/blob/main/docs/en/kv_int8.md.\
    \ \n\nBTW, I found two papers presented by Google on using multi-query attention\
    \ instead of multi-head attention to reduce mem requirment for attentioin kv cache\n\
    https://arxiv.org/pdf/2211.05102.pdf\nhttps://ar5iv.labs.arxiv.org/html/2305.13245\n\
    Not sure if any of the open-source model severing platform has adapt to those\n\
    \nMoreover, Flash-attention is method to drop multi-head attention mem scaling\
    \ from quadratic to linear https://huggingface.co/blog/optimize-llm, but forgive\
    \ my ignorance, I haven't found any clue that flash-attention would work with\
    \ attention weight quantization"
  created_at: 2023-11-15 03:44:37+00:00
  edited: true
  hidden: false
  id: 65543ea55e88eda4381da374
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-15T07:23:03.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9627050161361694
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> </p>\n<blockquote>\n\
          <p>LMDeploy has actually successfully deploy a KV int8 PTQ method here</p>\n\
          </blockquote>\n<p>The problem from the llama.cpp side wasn't that there\
          \ wasn't an existing 8bit quantization to use or that quality or anything\
          \ like that, the issue was making it fit in with the rest of the GGML/llama.cpp\
          \ code. It also can't really borrow directly from projects like what you\
          \ linked, most of the other stuff is written in Python while llama.cpp is...\
          \ written in C++/C.</p>\n<blockquote>\n<p>I found two papers presented by\
          \ Google on using multi-query attention instead of multi-head attention\
          \ to reduce mem requirment for attentioin kv cache</p>\n</blockquote>\n\
          <p>llama.cpp already uses that as far as I know, at least for 70B LLaMAv2\
          \ models.</p>\n<blockquote>\n<p>I haven't found any clue that flash-attention\
          \ would work with attention weight quantization</p>\n</blockquote>\n<p>I\
          \ think it's an approach to dealing with attention and doesn't really care\
          \ about the exact format of the KV cache. So I think it could work whether\
          \ the KV cache was f32, f16, or a quantized format.</p>\n"
        raw: "@Yhyu13 \n\n> LMDeploy has actually successfully deploy a KV int8 PTQ\
          \ method here\n\nThe problem from the llama.cpp side wasn't that there wasn't\
          \ an existing 8bit quantization to use or that quality or anything like\
          \ that, the issue was making it fit in with the rest of the GGML/llama.cpp\
          \ code. It also can't really borrow directly from projects like what you\
          \ linked, most of the other stuff is written in Python while llama.cpp is...\
          \ written in C++/C.\n\n> I found two papers presented by Google on using\
          \ multi-query attention instead of multi-head attention to reduce mem requirment\
          \ for attentioin kv cache\n\nllama.cpp already uses that as far as I know,\
          \ at least for 70B LLaMAv2 models.\n\n> I haven't found any clue that flash-attention\
          \ would work with attention weight quantization\n\nI think it's an approach\
          \ to dealing with attention and doesn't really care about the exact format\
          \ of the KV cache. So I think it could work whether the KV cache was f32,\
          \ f16, or a quantized format."
        updatedAt: '2023-11-15T07:23:03.349Z'
      numEdits: 0
      reactions: []
    id: 655471d727ba688d1b8f9253
    type: comment
  author: KerfuffleV2
  content: "@Yhyu13 \n\n> LMDeploy has actually successfully deploy a KV int8 PTQ\
    \ method here\n\nThe problem from the llama.cpp side wasn't that there wasn't\
    \ an existing 8bit quantization to use or that quality or anything like that,\
    \ the issue was making it fit in with the rest of the GGML/llama.cpp code. It\
    \ also can't really borrow directly from projects like what you linked, most of\
    \ the other stuff is written in Python while llama.cpp is... written in C++/C.\n\
    \n> I found two papers presented by Google on using multi-query attention instead\
    \ of multi-head attention to reduce mem requirment for attentioin kv cache\n\n\
    llama.cpp already uses that as far as I know, at least for 70B LLaMAv2 models.\n\
    \n> I haven't found any clue that flash-attention would work with attention weight\
    \ quantization\n\nI think it's an approach to dealing with attention and doesn't\
    \ really care about the exact format of the KV cache. So I think it could work\
    \ whether the KV cache was f32, f16, or a quantized format."
  created_at: 2023-11-15 07:23:03+00:00
  edited: false
  hidden: false
  id: 655471d727ba688d1b8f9253
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-12-09T13:54:09.000Z'
    data:
      edited: false
      editors:
      - Green-Sky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7582968473434448
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4309">https://github.com/ggerganov/llama.cpp/pull/4309</a>
          added kv cache quantization</p>

          '
        raw: https://github.com/ggerganov/llama.cpp/pull/4309 added kv cache quantization
        updatedAt: '2023-12-09T13:54:09.034Z'
      numEdits: 0
      reactions: []
    id: 6574718106fdcd4ca930c316
    type: comment
  author: Green-Sky
  content: https://github.com/ggerganov/llama.cpp/pull/4309 added kv cache quantization
  created_at: 2023-12-09 13:54:09+00:00
  edited: false
  hidden: false
  id: 6574718106fdcd4ca930c316
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Yi-34B-200K-GGUF
repo_type: model
status: open
target_branch: null
title: Actual max RAM usage?
