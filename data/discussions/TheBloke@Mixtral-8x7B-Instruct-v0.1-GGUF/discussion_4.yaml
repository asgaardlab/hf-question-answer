!!python/object:huggingface_hub.community.DiscussionWithDetails
author: littleworth
conflicting_files: null
created_at: 2023-12-12 02:51:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-12-12T02:51:27.000Z'
    data:
      edited: false
      editors:
      - littleworth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7157523036003113
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: '<p>Hi,</p>

          <p>I''ve encountered an issue while using the <a href="https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF">Mixtral-8x7B-Instruct-v0.1-GGUF</a>
          model. Below are the details of my execution command and the error I received.</p>

          <p><strong>Command Used:</strong></p>

          <p>Using <a rel="nofollow" href="https://github.com/Mozilla-Ocho/llamafile">llamafile</a></p>

          <pre><code>llamafile -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c
          32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "&lt;s&gt;[INST] {prompt}
          [/INST]"

          </code></pre>

          <p>Using llama.cpp:</p>

          <pre><code> ~/Tools/llama.cpp/main -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
          --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "&lt;s&gt;[INST]
          {prompt} [/INST]"

          </code></pre>

          <p><strong>Error Received:</strong></p>

          <p>Both gave this error message</p>

          <pre><code>error: create_tensor: tensor ''blk.0.ffn_gate.weight'' not found

          </code></pre>

          <p>This error occurs at the conclusion of the process. I''m uncertain if
          this is an issue with the <code>llamafile</code> command or a problem inherent
          to the Mixtral model itself.</p>

          <p>Could you provide guidance on how to resolve this issue? Any help would
          be greatly appreciated.</p>

          <p>Thanks,<br>LW</p>

          '
        raw: "Hi,\r\n\r\nI've encountered an issue while using the [Mixtral-8x7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF)\
          \ model. Below are the details of my execution command and the error I received.\r\
          \n\r\n**Command Used:**\r\n\r\nUsing [llamafile](https://github.com/Mozilla-Ocho/llamafile)\r\
          \n```\r\nllamafile -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c\
          \ 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<s>[INST] {prompt} [/INST]\"\
          \r\n```\r\n\r\nUsing llama.cpp:\r\n\r\n```\r\n ~/Tools/llama.cpp/main -m\
          \ mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty\
          \ 1.1 -n -1 -p \"<s>[INST] {prompt} [/INST]\"\r\n```\r\n\r\n**Error Received:**\r\
          \n\r\nBoth gave this error message\r\n\r\n```\r\nerror: create_tensor: tensor\
          \ 'blk.0.ffn_gate.weight' not found\r\n```\r\n\r\nThis error occurs at the\
          \ conclusion of the process. I'm uncertain if this is an issue with the\
          \ `llamafile` command or a problem inherent to the Mixtral model itself.\r\
          \n\r\nCould you provide guidance on how to resolve this issue? Any help\
          \ would be greatly appreciated.\r\n\r\nThanks,\r\nLW\r\n\r\n"
        updatedAt: '2023-12-12T02:51:27.595Z'
      numEdits: 0
      reactions: []
    id: 6577caaf821f439498cad1ba
    type: comment
  author: littleworth
  content: "Hi,\r\n\r\nI've encountered an issue while using the [Mixtral-8x7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF)\
    \ model. Below are the details of my execution command and the error I received.\r\
    \n\r\n**Command Used:**\r\n\r\nUsing [llamafile](https://github.com/Mozilla-Ocho/llamafile)\r\
    \n```\r\nllamafile -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c 32768\
    \ --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<s>[INST] {prompt} [/INST]\"\r\n\
    ```\r\n\r\nUsing llama.cpp:\r\n\r\n```\r\n ~/Tools/llama.cpp/main -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\
    \ --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<s>[INST] {prompt}\
    \ [/INST]\"\r\n```\r\n\r\n**Error Received:**\r\n\r\nBoth gave this error message\r\
    \n\r\n```\r\nerror: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\n\
    ```\r\n\r\nThis error occurs at the conclusion of the process. I'm uncertain if\
    \ this is an issue with the `llamafile` command or a problem inherent to the Mixtral\
    \ model itself.\r\n\r\nCould you provide guidance on how to resolve this issue?\
    \ Any help would be greatly appreciated.\r\n\r\nThanks,\r\nLW\r\n\r\n"
  created_at: 2023-12-12 02:51:27+00:00
  edited: false
  hidden: false
  id: 6577caaf821f439498cad1ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/D5fy6v5eFpdVcg43yoB1r.png?w=200&h=200&f=face
      fullname: Yuki Sasaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nss-ysasaki
      type: user
    createdAt: '2023-12-12T05:15:21.000Z'
    data:
      edited: true
      editors:
      - nss-ysasaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.861443042755127
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/D5fy6v5eFpdVcg43yoB1r.png?w=200&h=200&f=face
          fullname: Yuki Sasaki
          isHf: false
          isPro: false
          name: nss-ysasaki
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;littleworth&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/littleworth\"\
          >@<span class=\"underline\">littleworth</span></a></span>\n\n\t</span></span>!<br>For\
          \ llama.cpp, perhaps you forgot to check out the <code>mixtral</code> branch?\
          \ Then <code>git checkout mixtral</code> would fix it.</p>\n<pre><code class=\"\
          language-console\"><span class=\"hljs-meta prompt_\">$ </span><span class=\"\
          language-bash\">git <span class=\"hljs-built_in\">clone</span> https://github.com/ggerganov/llama.cpp</span>\n\
          <span class=\"hljs-meta prompt_\"></span>\n<span class=\"hljs-meta prompt_\"\
          >$ </span><span class=\"language-bash\"><span class=\"hljs-comment\"># Mixtral\
          \ support is not merged to `main` yet - see PR 4406</span></span>\n<span\
          \ class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\"><span\
          \ class=\"hljs-comment\"># https://github.com/ggerganov/llama.cpp/pull/4406</span></span>\n\
          <span class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\"\
          >git checkout mixtral</span>\n<span class=\"hljs-meta prompt_\"></span>\n\
          <span class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\"\
          >make LLAMA_CUBLAS=1</span>\n</code></pre>\n<p>I can't speak for <code>llamafile</code>,\
          \ but I guess it does not support Mixtral at the moment - the issue seems\
          \ to be tracked <a rel=\"nofollow\" href=\"https://github.com/Mozilla-Ocho/llamafile/issues/86\"\
          >here</a>.</p>\n"
        raw: 'Hi @littleworth!

          For llama.cpp, perhaps you forgot to check out the `mixtral` branch? Then
          `git checkout mixtral` would fix it.


          ```console

          $ git clone https://github.com/ggerganov/llama.cpp


          $ # Mixtral support is not merged to `main` yet - see PR 4406

          $ # https://github.com/ggerganov/llama.cpp/pull/4406

          $ git checkout mixtral


          $ make LLAMA_CUBLAS=1

          ```


          I can''t speak for `llamafile`, but I guess it does not support Mixtral
          at the moment - the issue seems to be tracked [here](https://github.com/Mozilla-Ocho/llamafile/issues/86).'
        updatedAt: '2023-12-12T05:18:59.375Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Gravitas
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Gravitas
    id: 6577ec69411e14898b88601e
    type: comment
  author: nss-ysasaki
  content: 'Hi @littleworth!

    For llama.cpp, perhaps you forgot to check out the `mixtral` branch? Then `git
    checkout mixtral` would fix it.


    ```console

    $ git clone https://github.com/ggerganov/llama.cpp


    $ # Mixtral support is not merged to `main` yet - see PR 4406

    $ # https://github.com/ggerganov/llama.cpp/pull/4406

    $ git checkout mixtral


    $ make LLAMA_CUBLAS=1

    ```


    I can''t speak for `llamafile`, but I guess it does not support Mixtral at the
    moment - the issue seems to be tracked [here](https://github.com/Mozilla-Ocho/llamafile/issues/86).'
  created_at: 2023-12-12 05:15:21+00:00
  edited: true
  hidden: false
  id: 6577ec69411e14898b88601e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-12-12T06:52:16.000Z'
    data:
      edited: false
      editors:
      - littleworth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5228523015975952
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;littleworth&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/littleworth\"\
          >@<span class=\"underline\">littleworth</span></a></span>\n\n\t</span></span>!<br>For\
          \ llama.cpp, perhaps you forgot to check out the <code>mixtral</code> branch?\
          \ Then <code>git checkout mixtral</code> would fix it.</p>\n<pre><code class=\"\
          language-console\"><span class=\"hljs-meta prompt_\">$ </span><span class=\"\
          language-bash\">git <span class=\"hljs-built_in\">clone</span> https://github.com/ggerganov/llama.cpp</span>\n\
          <span class=\"hljs-meta prompt_\"></span>\n<span class=\"hljs-meta prompt_\"\
          >$ </span><span class=\"language-bash\"><span class=\"hljs-comment\"># Mixtral\
          \ support is not merged to `main` yet - see PR 4406</span></span>\n<span\
          \ class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\"><span\
          \ class=\"hljs-comment\"># https://github.com/ggerganov/llama.cpp/pull/4406</span></span>\n\
          <span class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\"\
          >git checkout mixtral</span>\n<span class=\"hljs-meta prompt_\"></span>\n\
          <span class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\"\
          >make LLAMA_CUBLAS=1</span>\n</code></pre>\n<p>I can't speak for <code>llamafile</code>,\
          \ but I guess it does not support Mixtral at the moment - the issue seems\
          \ to be tracked <a rel=\"nofollow\" href=\"https://github.com/Mozilla-Ocho/llamafile/issues/86\"\
          >here</a>.</p>\n</blockquote>\n<p>Hi, </p>\n<p>Thanks for your reply. After\
          \ following your instruction,  I got this error:</p>\n<pre><code>$ make\
          \ LLAMA_CUBLAS=1\nI llama.cpp build info:\nI UNAME_S:   Linux\nI UNAME_P:\
          \   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600\
          \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith\
          \ -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration\
          \ -Wdouble-promotion -pthread -march=native -mtune=native\nI CXXFLAGS: \
          \ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
          \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds\
          \ -Wno-format-truncation -Wextra-semi -march=native -mtune=native\nI NVCCFLAGS:\
          \ --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32\
          \ -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128\
          \ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
          \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler\
          \ \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native \"\nI LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt\
          \ -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n\
          I CC:        cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nI CXX:       g++\
          \ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n\ncc  -I. -Icommon -D_XOPEN_SOURCE=600\
          \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith\
          \ -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration\
          \ -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml.c -o\
          \ ggml.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
          \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds\
          \ -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c llama.cpp\
          \ -o llama.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG\
          \ -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds\
          \ -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/common.cpp\
          \ -o common.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG\
          \ -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds\
          \ -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/sampling.cpp\
          \ -o sampling.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG\
          \ -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds\
          \ -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/grammar-parser.cpp\
          \ -o grammar-parser.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE\
          \ -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
          \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native  -c common/build-info.cpp -o build-info.o\ng++ -I. -Icommon\
          \ -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include\
          \ -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC\
          \ -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations\
          \ -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation\
          \ -Wextra-semi -march=native -mtune=native  -c common/console.cpp -o console.o\n\
          nvcc --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32\
          \ -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128\
          \ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
          \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler\
          \ \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native \" -c ggml-cuda.cu -o ggml-cuda.o\nnvcc fatal   : Value\
          \ 'native' is not defined for option 'gpu-architecture'\nmake: *** [Makefile:457:\
          \ ggml-cuda.o] Error 1\n</code></pre>\n<p>Any idea how to solve?</p>\n<p>LW</p>\n"
        raw: "> Hi @littleworth!\n> For llama.cpp, perhaps you forgot to check out\
          \ the `mixtral` branch? Then `git checkout mixtral` would fix it.\n> \n\
          > ```console\n> $ git clone https://github.com/ggerganov/llama.cpp\n> \n\
          > $ # Mixtral support is not merged to `main` yet - see PR 4406\n> $ # https://github.com/ggerganov/llama.cpp/pull/4406\n\
          > $ git checkout mixtral\n> \n> $ make LLAMA_CUBLAS=1\n> ```\n> \n> I can't\
          \ speak for `llamafile`, but I guess it does not support Mixtral at the\
          \ moment - the issue seems to be tracked [here](https://github.com/Mozilla-Ocho/llamafile/issues/86).\n\
          \nHi, \n\nThanks for your reply. After following your instruction,  I got\
          \ this error:\n\n```\n$ make LLAMA_CUBLAS=1\nI llama.cpp build info:\nI\
          \ UNAME_S:   Linux\nI UNAME_P:   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:\
          \    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
          \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int\
          \ -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native\
          \ -mtune=native\nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE\
          \ -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
          \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native\nI NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math\
          \ -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2\
          \ -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE\
          \ -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
          \ -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation\
          \ -Wextra-semi -march=native -mtune=native \"\nI LDFLAGS:   -lcublas -lculibos\
          \ -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64\
          \ -L/targets/x86_64-linux/lib\nI CC:        cc (Ubuntu 9.4.0-1ubuntu1~20.04.2)\
          \ 9.4.0\nI CXX:       g++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n\ncc  -I.\
          \ -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
          \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int\
          \ -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native\
          \ -mtune=native    -c ggml.c -o ggml.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600\
          \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
          \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native  -c llama.cpp -o llama.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600\
          \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
          \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native  -c common/common.cpp -o common.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600\
          \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
          \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native  -c common/sampling.cpp -o sampling.o\ng++ -I. -Icommon\
          \ -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include\
          \ -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC\
          \ -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations\
          \ -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation\
          \ -Wextra-semi -march=native -mtune=native  -c common/grammar-parser.cpp\
          \ -o grammar-parser.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE\
          \ -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
          \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
          \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
          \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native  -c common/build-info.cpp -o build-info.o\ng++ -I. -Icommon\
          \ -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include\
          \ -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC\
          \ -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations\
          \ -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation\
          \ -Wextra-semi -march=native -mtune=native  -c common/console.cpp -o console.o\n\
          nvcc --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32\
          \ -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128\
          \ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
          \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
          \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
          \ -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler\
          \ \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
          \ -mtune=native \" -c ggml-cuda.cu -o ggml-cuda.o\nnvcc fatal   : Value\
          \ 'native' is not defined for option 'gpu-architecture'\nmake: *** [Makefile:457:\
          \ ggml-cuda.o] Error 1\n```\n\nAny idea how to solve?\n\nLW"
        updatedAt: '2023-12-12T06:52:16.332Z'
      numEdits: 0
      reactions: []
    id: 65780320197a6182c33b4d42
    type: comment
  author: littleworth
  content: "> Hi @littleworth!\n> For llama.cpp, perhaps you forgot to check out the\
    \ `mixtral` branch? Then `git checkout mixtral` would fix it.\n> \n> ```console\n\
    > $ git clone https://github.com/ggerganov/llama.cpp\n> \n> $ # Mixtral support\
    \ is not merged to `main` yet - see PR 4406\n> $ # https://github.com/ggerganov/llama.cpp/pull/4406\n\
    > $ git checkout mixtral\n> \n> $ make LLAMA_CUBLAS=1\n> ```\n> \n> I can't speak\
    \ for `llamafile`, but I guess it does not support Mixtral at the moment - the\
    \ issue seems to be tracked [here](https://github.com/Mozilla-Ocho/llamafile/issues/86).\n\
    \nHi, \n\nThanks for your reply. After following your instruction,  I got this\
    \ error:\n\n```\n$ make LLAMA_CUBLAS=1\nI llama.cpp build info:\nI UNAME_S:  \
    \ Linux\nI UNAME_P:   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600\
    \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
    \ -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic\
    \ -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith\
    \ -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration\
    \ -Wdouble-promotion -pthread -march=native -mtune=native\nI CXXFLAGS:  -I. -Icommon\
    \ -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include\
    \ -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall\
    \ -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
    \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
    \ -mtune=native\nI NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math\
    \ -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2\
    \ -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE\
    \ -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
    \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
    \ -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler\
    \ \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native\
    \ \"\nI LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt\
    \ -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\nI CC:\
    \        cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nI CXX:       g++ (Ubuntu 9.4.0-1ubuntu1~20.04.2)\
    \ 9.4.0\n\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS\
    \ -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
    \  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
    \ -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int\
    \ -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native\
    \ -mtune=native    -c ggml.c -o ggml.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE\
    \ -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
    \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
    \ -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation\
    \ -Wextra-semi -march=native -mtune=native  -c llama.cpp -o llama.o\ng++ -I. -Icommon\
    \ -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include\
    \ -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall\
    \ -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn\
    \ -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\
    \ -mtune=native  -c common/common.cpp -o common.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600\
    \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
    \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
    \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread\
    \  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native\
    \  -c common/sampling.cpp -o sampling.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600\
    \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
    \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
    \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread\
    \  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native\
    \  -c common/grammar-parser.cpp -o grammar-parser.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600\
    \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
    \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
    \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread\
    \  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native\
    \  -c common/build-info.cpp -o build-info.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600\
    \ -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include\
    \ -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic\
    \ -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread\
    \  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native\
    \  -c common/console.cpp -o console.o\nnvcc --forward-unknown-to-host-compiler\
    \ -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2\
    \ -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE\
    \ -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\
    \  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\
    \ -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler\
    \ \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native\
    \ \" -c ggml-cuda.cu -o ggml-cuda.o\nnvcc fatal   : Value 'native' is not defined\
    \ for option 'gpu-architecture'\nmake: *** [Makefile:457: ggml-cuda.o] Error 1\n\
    ```\n\nAny idea how to solve?\n\nLW"
  created_at: 2023-12-12 06:52:16+00:00
  edited: false
  hidden: false
  id: 65780320197a6182c33b4d42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-12T07:28:13.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6022993922233582
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Cuda must be 12.2<br>Check version installed </p>

          '
        raw: "Cuda must be 12.2 \nCheck version installed "
        updatedAt: '2023-12-12T07:29:08.844Z'
      numEdits: 1
      reactions: []
    id: 65780b8d411e14898b8d66d0
    type: comment
  author: mirek190
  content: "Cuda must be 12.2 \nCheck version installed "
  created_at: 2023-12-12 07:28:13+00:00
  edited: true
  hidden: false
  id: 65780b8d411e14898b8d66d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/212ddefc9d64b1f8b71d94b08f244519.svg
      fullname: S T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gravitas
      type: user
    createdAt: '2023-12-12T20:03:20.000Z'
    data:
      edited: true
      editors:
      - Gravitas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8065733909606934
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/212ddefc9d64b1f8b71d94b08f244519.svg
          fullname: S T
          isHf: false
          isPro: false
          name: Gravitas
          type: user
        html: '<p>On Windows 10 x64, switching to the <code>mixtral</code> branch
          worked for me.</p>

          <p>After running <code>cmake</code>, recompiled in Visual Studio 2022 Community,
          and success:</p>

          <pre><code>main -m models/mixtral-8x7b-v0.1.Q5_K_M.gguf -n 256 --repeat_penalty
          1.0 --color -i -r "User:" -f prompts/chat-with-bob.txt

          ...

          llm_load_tensors: mem required  = 30735.89 MiB

          llama_new_context_with_model: compute buffer total size = 117.85 MiB

          ...

          system_info: n_threads = 28 / 56 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI
          = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA
          = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |

          == Running in interactive mode. ==

          User:Meaning of life.

          Bob: The meaning of life is to be happy and find joy in each day.

          </code></pre>

          <p>To get the <code>.gguf</code> file, I ran <a rel="nofollow" href="https://lmstudio.ai/">
          LM Studio</a>, downloaded the model, then symlinked under Windows from the
          model file to the <code>models</code> dir in this git repo.</p>

          '
        raw: 'On Windows 10 x64, switching to the `mixtral` branch worked for me.


          After running `cmake`, recompiled in Visual Studio 2022 Community, and success:


          ```

          main -m models/mixtral-8x7b-v0.1.Q5_K_M.gguf -n 256 --repeat_penalty 1.0
          --color -i -r "User:" -f prompts/chat-with-bob.txt

          ...

          llm_load_tensors: mem required  = 30735.89 MiB

          llama_new_context_with_model: compute buffer total size = 117.85 MiB

          ...

          system_info: n_threads = 28 / 56 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI
          = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA
          = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |

          == Running in interactive mode. ==

          User:Meaning of life.

          Bob: The meaning of life is to be happy and find joy in each day.

          ```


          To get the `.gguf` file, I ran [ LM Studio](https://lmstudio.ai/), downloaded
          the model, then symlinked under Windows from the model file to the `models`
          dir in this git repo.'
        updatedAt: '2023-12-12T20:08:46.086Z'
      numEdits: 9
      reactions: []
    id: 6578bc8858d7a2cc895826e6
    type: comment
  author: Gravitas
  content: 'On Windows 10 x64, switching to the `mixtral` branch worked for me.


    After running `cmake`, recompiled in Visual Studio 2022 Community, and success:


    ```

    main -m models/mixtral-8x7b-v0.1.Q5_K_M.gguf -n 256 --repeat_penalty 1.0 --color
    -i -r "User:" -f prompts/chat-with-bob.txt

    ...

    llm_load_tensors: mem required  = 30735.89 MiB

    llama_new_context_with_model: compute buffer total size = 117.85 MiB

    ...

    system_info: n_threads = 28 / 56 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI
    = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA
    = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |

    == Running in interactive mode. ==

    User:Meaning of life.

    Bob: The meaning of life is to be happy and find joy in each day.

    ```


    To get the `.gguf` file, I ran [ LM Studio](https://lmstudio.ai/), downloaded
    the model, then symlinked under Windows from the model file to the `models` dir
    in this git repo.'
  created_at: 2023-12-12 20:03:20+00:00
  edited: true
  hidden: false
  id: 6578bc8858d7a2cc895826e6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: 'Issue with Mixtral-8x7B-Instruct-v0.1-GGUF Model: ''blk.0.ffn_gate.weight''
  Tensor Not Found'
