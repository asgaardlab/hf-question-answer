!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shalene
conflicting_files: null
created_at: 2023-12-14 15:34:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8696b92876ba52590c76fbdd96b03c49.svg
      fullname: shalene lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shalene
      type: user
    createdAt: '2023-12-14T15:34:40.000Z'
    data:
      edited: true
      editors:
      - shalene
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9374470710754395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8696b92876ba52590c76fbdd96b03c49.svg
          fullname: shalene lee
          isHf: false
          isPro: false
          name: shalene
          type: user
        html: '<p>Is it unique to gguf or also to gptq? I test it with llama.cpp 12.14</p>

          '
        raw: Is it unique to gguf or also to gptq? I test it with llama.cpp 12.14
        updatedAt: '2023-12-15T05:38:46.770Z'
      numEdits: 1
      reactions: []
    id: 657b209069a46ce96bd2f322
    type: comment
  author: shalene
  content: Is it unique to gguf or also to gptq? I test it with llama.cpp 12.14
  created_at: 2023-12-14 15:34:40+00:00
  edited: true
  hidden: false
  id: 657b209069a46ce96bd2f322
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-14T17:45:33.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9996155500411987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>for me is as fast as 13B model</p>

          '
        raw: for me is as fast as 13B model
        updatedAt: '2023-12-14T17:45:33.391Z'
      numEdits: 0
      reactions: []
    id: 657b3f3deda715a4be302b6f
    type: comment
  author: mirek190
  content: for me is as fast as 13B model
  created_at: 2023-12-14 17:45:33+00:00
  edited: false
  hidden: false
  id: 657b3f3deda715a4be302b6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cd3049107029eb004f968/G1A2zca8Yi_ii2YJtHHht.png?w=200&h=200&f=face
      fullname: "Rickard Ed\xE9n"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neph1
      type: user
    createdAt: '2023-12-14T19:11:13.000Z'
    data:
      edited: false
      editors:
      - neph1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9882954955101013
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cd3049107029eb004f968/G1A2zca8Yi_ii2YJtHHht.png?w=200&h=200&f=face
          fullname: "Rickard Ed\xE9n"
          isHf: false
          isPro: false
          name: neph1
          type: user
        html: '<p>I get varied results. When I first tested it out with llama.cpp
          it was about 5t/s. But when trying it later, it seems it can get ''stuck''
          parsing the prompt, leading to very long response times. I haven''t had
          time to look into this further.</p>

          '
        raw: I get varied results. When I first tested it out with llama.cpp it was
          about 5t/s. But when trying it later, it seems it can get 'stuck' parsing
          the prompt, leading to very long response times. I haven't had time to look
          into this further.
        updatedAt: '2023-12-14T19:11:13.740Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - shalene
        - AliceThirty
    id: 657b53515a9de24077fb36c9
    type: comment
  author: neph1
  content: I get varied results. When I first tested it out with llama.cpp it was
    about 5t/s. But when trying it later, it seems it can get 'stuck' parsing the
    prompt, leading to very long response times. I haven't had time to look into this
    further.
  created_at: 2023-12-14 19:11:13+00:00
  edited: false
  hidden: false
  id: 657b53515a9de24077fb36c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/241b0630c4604a9f1fd271a405eee73c.svg
      fullname: phil shi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phils666
      type: user
    createdAt: '2023-12-15T06:23:20.000Z'
    data:
      edited: false
      editors:
      - phils666
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9828090071678162
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/241b0630c4604a9f1fd271a405eee73c.svg
          fullname: phil shi
          isHf: false
          isPro: false
          name: phils666
          type: user
        html: "<p>It seems slow on Nvidia A100 with 80G VRAM, does anyone know why\
          \ ?\n </p>\n"
        raw: "It seems slow on Nvidia A100 with 80G VRAM, does anyone know why ?\n\
          \ "
        updatedAt: '2023-12-15T06:23:20.035Z'
      numEdits: 0
      reactions: []
    id: 657bf0d88d360b690d5d7f94
    type: comment
  author: phils666
  content: "It seems slow on Nvidia A100 with 80G VRAM, does anyone know why ?\n "
  created_at: 2023-12-15 06:23:20+00:00
  edited: false
  hidden: false
  id: 657bf0d88d360b690d5d7f94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-15T07:15:36.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9718555212020874
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I noticed praising prompt is slow with q4k_m version but much faster
          with q5k_m.</p>

          '
        raw: I noticed praising prompt is slow with q4k_m version but much faster
          with q5k_m.
        updatedAt: '2023-12-15T07:15:36.788Z'
      numEdits: 0
      reactions: []
    id: 657bfd182cbb7f6382576a88
    type: comment
  author: mirek190
  content: I noticed praising prompt is slow with q4k_m version but much faster with
    q5k_m.
  created_at: 2023-12-15 07:15:36+00:00
  edited: false
  hidden: false
  id: 657bfd182cbb7f6382576a88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
      fullname: Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vidyamantra
      type: user
    createdAt: '2023-12-15T12:24:08.000Z'
    data:
      edited: false
      editors:
      - vidyamantra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.386775404214859
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
          fullname: Gupta
          isHf: false
          isPro: false
          name: vidyamantra
          type: user
        html: "<p>On RTX 4090 &amp; i9-14900K. Benchmark using llama-bench from llama.cpp.</p>\n\
          <div class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th><strong>model</strong></th>\n\
          <th><strong>size</strong></th>\n<th><strong>params</strong></th>\n<th><strong>backend</strong></th>\n\
          <th><strong>ngl</strong></th>\n<th><strong>threads</strong></th>\n<th><strong>t/s\
          \ pp 512</strong></th>\n<th><strong>t/s tg 128</strong></th>\n</tr>\n\n\t\
          \t</thead><tbody><tr>\n<td>llama 7B mostly Q3_K - Medium</td>\n<td>18.96\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n<td>8</td>\n<td>205.07</td>\n\
          <td>83.16</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q3_K - Medium</td>\n<td>18.96\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n<td>16</td>\n\
          <td>204.48</td>\n<td>83.21</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q3_K -\
          \ Medium</td>\n<td>18.96 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n\
          <td>24</td>\n<td>204.28</td>\n<td>83.22</td>\n</tr>\n<tr>\n<td>llama 7B\
          \ mostly Q3_K - Medium</td>\n<td>18.96 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>33</td>\n<td>32</td>\n<td>203.82</td>\n<td>83.17</td>\n</tr>\n<tr>\n\
          <td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62 GiB</td>\n<td>46.70 B</td>\n\
          <td>CUDA</td>\n<td>27</td>\n<td>8</td>\n<td>145.54</td>\n<td>27.75</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62 GiB</td>\n\
          <td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n<td>16</td>\n<td>121.58</td>\n\
          <td>25.57</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n<td>24</td>\n\
          <td>147.14</td>\n<td>26.41</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q4_K -\
          \ Medium</td>\n<td>24.62 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n\
          <td>32</td>\n<td>145.23</td>\n<td>9.36</td>\n</tr>\n<tr>\n<td>llama 7B mostly\
          \ Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>8</td>\n<td>58.18</td>\n<td>15.12</td>\n</tr>\n<tr>\n<td>llama\
          \ 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>16</td>\n<td>49.28</td>\n<td>13.8</td>\n</tr>\n<tr>\n<td>llama\
          \ 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>24</td>\n<td>64.25</td>\n<td>15.07</td>\n</tr>\n<tr>\n\
          <td>llama 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n\
          <td>CUDA</td>\n<td>22</td>\n<td>32</td>\n<td>73.69</td>\n<td>12.02</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>8</td>\n<td>33.86</td>\n<td>10.5</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>16</td>\n<td>31.75</td>\n<td>9.5</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>24</td>\n<td>40.37</td>\n<td>10.58</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>32</td>\n<td>45.39</td>\n<td>8.8</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>8</td>\n<td>18.02</td>\n<td>7.1</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>16</td>\n<td>19.74</td>\n<td>5.9</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>24</td>\n<td>24.81</td>\n<td>6.74</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>32</td>\n<td>28.31</td>\n<td>5.62</td>\n\
          </tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: 'On RTX 4090 & i9-14900K. Benchmark using llama-bench from llama.cpp.


          | **model**                     | **size**  | **params** | **backend** |
          **ngl** | **threads** | **t/s pp 512** | **t/s tg 128** |

          |-------------------------------|-----------|------------|-------------|---------|-------------|----------------|----------------|

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 8           | 205.07         | 83.16          |

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 16          | 204.48         | 83.21          |

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 24          | 204.28         | 83.22          |

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 32          | 203.82         | 83.17          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 8           | 145.54         | 27.75          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 16          | 121.58         | 25.57          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 24          | 147.14         | 26.41          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 32          | 145.23         | 9.36           |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 8           | 58.18          | 15.12          |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 16          | 49.28          | 13.8           |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 24          | 64.25          | 15.07          |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 32          | 73.69          | 12.02          |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 8           | 33.86          | 10.5           |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 16          | 31.75          | 9.5            |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 24          | 40.37          | 10.58          |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 32          | 45.39          | 8.8            |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 8           | 18.02          | 7.1            |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 16          | 19.74          | 5.9            |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 24          | 24.81          | 6.74           |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 32          | 28.31          | 5.62           |'
        updatedAt: '2023-12-15T12:24:08.054Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - shalene
    id: 657c4568fb0285d857d7b50a
    type: comment
  author: vidyamantra
  content: 'On RTX 4090 & i9-14900K. Benchmark using llama-bench from llama.cpp.


    | **model**                     | **size**  | **params** | **backend** | **ngl**
    | **threads** | **t/s pp 512** | **t/s tg 128** |

    |-------------------------------|-----------|------------|-------------|---------|-------------|----------------|----------------|

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    8           | 205.07         | 83.16          |

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    16          | 204.48         | 83.21          |

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    24          | 204.28         | 83.22          |

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    32          | 203.82         | 83.17          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    8           | 145.54         | 27.75          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    16          | 121.58         | 25.57          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    24          | 147.14         | 26.41          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    32          | 145.23         | 9.36           |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    8           | 58.18          | 15.12          |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    16          | 49.28          | 13.8           |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    24          | 64.25          | 15.07          |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    32          | 73.69          | 12.02          |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    8           | 33.86          | 10.5           |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    16          | 31.75          | 9.5            |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    24          | 40.37          | 10.58          |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    32          | 45.39          | 8.8            |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    8           | 18.02          | 7.1            |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    16          | 19.74          | 5.9            |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    24          | 24.81          | 6.74           |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    32          | 28.31          | 5.62           |'
  created_at: 2023-12-15 12:24:08+00:00
  edited: false
  hidden: false
  id: 657c4568fb0285d857d7b50a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
      fullname: Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vidyamantra
      type: user
    createdAt: '2023-12-15T12:26:19.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
          fullname: Gupta
          isHf: false
          isPro: false
          name: vidyamantra
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-15T12:26:37.800Z'
      numEdits: 0
      reactions: []
    id: 657c45eb3a230db5894e8251
    type: comment
  author: vidyamantra
  content: This comment has been hidden
  created_at: 2023-12-15 12:26:19+00:00
  edited: true
  hidden: true
  id: 657c45eb3a230db5894e8251
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8696b92876ba52590c76fbdd96b03c49.svg
      fullname: shalene lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shalene
      type: user
    createdAt: '2023-12-15T14:28:24.000Z'
    data:
      edited: false
      editors:
      - shalene
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4107711613178253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8696b92876ba52590c76fbdd96b03c49.svg
          fullname: shalene lee
          isHf: false
          isPro: false
          name: shalene
          type: user
        html: "<blockquote>\n<p>On RTX 4090 &amp; i9-14900K. Benchmark using llama-bench\
          \ from llama.cpp.</p>\n<div class=\"max-w-full overflow-auto\">\n\t<table>\n\
          \t\t<thead><tr>\n<th><strong>model</strong></th>\n<th><strong>size</strong></th>\n\
          <th><strong>params</strong></th>\n<th><strong>backend</strong></th>\n<th><strong>ngl</strong></th>\n\
          <th><strong>threads</strong></th>\n<th><strong>t/s pp 512</strong></th>\n\
          <th><strong>t/s tg 128</strong></th>\n</tr>\n\n\t\t</thead><tbody><tr>\n\
          <td>llama 7B mostly Q3_K - Medium</td>\n<td>18.96 GiB</td>\n<td>46.70 B</td>\n\
          <td>CUDA</td>\n<td>33</td>\n<td>8</td>\n<td>205.07</td>\n<td>83.16</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q3_K - Medium</td>\n<td>18.96 GiB</td>\n\
          <td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n<td>16</td>\n<td>204.48</td>\n\
          <td>83.21</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q3_K - Medium</td>\n<td>18.96\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n<td>24</td>\n\
          <td>204.28</td>\n<td>83.22</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q3_K -\
          \ Medium</td>\n<td>18.96 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n\
          <td>32</td>\n<td>203.82</td>\n<td>83.17</td>\n</tr>\n<tr>\n<td>llama 7B\
          \ mostly Q4_K - Medium</td>\n<td>24.62 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>27</td>\n<td>8</td>\n<td>145.54</td>\n<td>27.75</td>\n</tr>\n<tr>\n\
          <td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62 GiB</td>\n<td>46.70 B</td>\n\
          <td>CUDA</td>\n<td>27</td>\n<td>16</td>\n<td>121.58</td>\n<td>25.57</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62 GiB</td>\n\
          <td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n<td>24</td>\n<td>147.14</td>\n\
          <td>26.41</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n<td>32</td>\n\
          <td>145.23</td>\n<td>9.36</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q5_K -\
          \ Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>22</td>\n\
          <td>8</td>\n<td>58.18</td>\n<td>15.12</td>\n</tr>\n<tr>\n<td>llama 7B mostly\
          \ Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>16</td>\n<td>49.28</td>\n<td>13.8</td>\n</tr>\n<tr>\n<td>llama\
          \ 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>24</td>\n<td>64.25</td>\n<td>15.07</td>\n</tr>\n<tr>\n\
          <td>llama 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n\
          <td>CUDA</td>\n<td>22</td>\n<td>32</td>\n<td>73.69</td>\n<td>12.02</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>8</td>\n<td>33.86</td>\n<td>10.5</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>16</td>\n<td>31.75</td>\n<td>9.5</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>24</td>\n<td>40.37</td>\n<td>10.58</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>32</td>\n<td>45.39</td>\n<td>8.8</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>8</td>\n<td>18.02</td>\n<td>7.1</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>16</td>\n<td>19.74</td>\n<td>5.9</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>24</td>\n<td>24.81</td>\n<td>6.74</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>32</td>\n<td>28.31</td>\n<td>5.62</td>\n\
          </tr>\n</tbody>\n\t</table>\n</div>\n</blockquote>\n<p>Maybe try 1 or 4\
          \ threads, thanks. I just have 5600g and 4060ti-16g.</p>\n"
        raw: "> On RTX 4090 & i9-14900K. Benchmark using llama-bench from llama.cpp.\n\
          > \n> | **model**                     | **size**  | **params** | **backend**\
          \ | **ngl** | **threads** | **t/s pp 512** | **t/s tg 128** |\n> |-------------------------------|-----------|------------|-------------|---------|-------------|----------------|----------------|\n\
          > | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA      \
          \  | 33      | 8           | 205.07         | 83.16          |\n> | llama\
          \ 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33  \
          \    | 16          | 204.48         | 83.21          |\n> | llama 7B mostly\
          \ Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      | 24  \
          \        | 204.28         | 83.22          |\n> | llama 7B mostly Q3_K -\
          \ Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      | 32         \
          \ | 203.82         | 83.17          |\n> | llama 7B mostly Q4_K - Medium\
          \ | 24.62 GiB | 46.70 B    | CUDA        | 27      | 8           | 145.54\
          \         | 27.75          |\n> | llama 7B mostly Q4_K - Medium | 24.62\
          \ GiB | 46.70 B    | CUDA        | 27      | 16          | 121.58      \
          \   | 25.57          |\n> | llama 7B mostly Q4_K - Medium | 24.62 GiB |\
          \ 46.70 B    | CUDA        | 27      | 24          | 147.14         | 26.41\
          \          |\n> | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B  \
          \  | CUDA        | 27      | 32          | 145.23         | 9.36       \
          \    |\n> | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA\
          \        | 22      | 8           | 58.18          | 15.12          |\n>\
          \ | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA      \
          \  | 22      | 16          | 49.28          | 13.8           |\n> | llama\
          \ 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22  \
          \    | 24          | 64.25          | 15.07          |\n> | llama 7B mostly\
          \ Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      | 32  \
          \        | 73.69          | 12.02          |\n> | llama 7B mostly Q6_K \
          \         | 35.74 GiB | 46.70 B    | CUDA        | 19      | 8         \
          \  | 33.86          | 10.5           |\n> | llama 7B mostly Q6_K       \
          \   | 35.74 GiB | 46.70 B    | CUDA        | 19      | 16          | 31.75\
          \          | 9.5            |\n> | llama 7B mostly Q6_K          | 35.74\
          \ GiB | 46.70 B    | CUDA        | 19      | 24          | 40.37       \
          \   | 10.58          |\n> | llama 7B mostly Q6_K          | 35.74 GiB |\
          \ 46.70 B    | CUDA        | 19      | 32          | 45.39          | 8.8\
          \            |\n> | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B\
          \    | CUDA        | 15      | 8           | 18.02          | 7.1      \
          \      |\n> | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA\
          \        | 15      | 16          | 19.74          | 5.9            |\n>\
          \ | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA      \
          \  | 15      | 24          | 24.81          | 6.74           |\n> | llama\
          \ 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15  \
          \    | 32          | 28.31          | 5.62           |\n\nMaybe try 1 or\
          \ 4 threads, thanks. I just have 5600g and 4060ti-16g."
        updatedAt: '2023-12-15T14:28:24.192Z'
      numEdits: 0
      reactions: []
    id: 657c628801f76efce86075c1
    type: comment
  author: shalene
  content: "> On RTX 4090 & i9-14900K. Benchmark using llama-bench from llama.cpp.\n\
    > \n> | **model**                     | **size**  | **params** | **backend** |\
    \ **ngl** | **threads** | **t/s pp 512** | **t/s tg 128** |\n> |-------------------------------|-----------|------------|-------------|---------|-------------|----------------|----------------|\n\
    > | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33\
    \      | 8           | 205.07         | 83.16          |\n> | llama 7B mostly\
    \ Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      | 16        \
    \  | 204.48         | 83.21          |\n> | llama 7B mostly Q3_K - Medium | 18.96\
    \ GiB | 46.70 B    | CUDA        | 33      | 24          | 204.28         | 83.22\
    \          |\n> | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA\
    \        | 33      | 32          | 203.82         | 83.17          |\n> | llama\
    \ 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      | 8\
    \           | 145.54         | 27.75          |\n> | llama 7B mostly Q4_K - Medium\
    \ | 24.62 GiB | 46.70 B    | CUDA        | 27      | 16          | 121.58    \
    \     | 25.57          |\n> | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70\
    \ B    | CUDA        | 27      | 24          | 147.14         | 26.41        \
    \  |\n> | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA      \
    \  | 27      | 32          | 145.23         | 9.36           |\n> | llama 7B mostly\
    \ Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      | 8         \
    \  | 58.18          | 15.12          |\n> | llama 7B mostly Q5_K - Medium | 30.02\
    \ GiB | 46.70 B    | CUDA        | 22      | 16          | 49.28          | 13.8\
    \           |\n> | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA\
    \        | 22      | 24          | 64.25          | 15.07          |\n> | llama\
    \ 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      | 32\
    \          | 73.69          | 12.02          |\n> | llama 7B mostly Q6_K     \
    \     | 35.74 GiB | 46.70 B    | CUDA        | 19      | 8           | 33.86 \
    \         | 10.5           |\n> | llama 7B mostly Q6_K          | 35.74 GiB |\
    \ 46.70 B    | CUDA        | 19      | 16          | 31.75          | 9.5    \
    \        |\n> | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA\
    \        | 19      | 24          | 40.37          | 10.58          |\n> | llama\
    \ 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      | 32\
    \          | 45.39          | 8.8            |\n> | llama 7B mostly Q8_0     \
    \     | 46.22 GiB | 46.70 B    | CUDA        | 15      | 8           | 18.02 \
    \         | 7.1            |\n> | llama 7B mostly Q8_0          | 46.22 GiB |\
    \ 46.70 B    | CUDA        | 15      | 16          | 19.74          | 5.9    \
    \        |\n> | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA\
    \        | 15      | 24          | 24.81          | 6.74           |\n> | llama\
    \ 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      | 32\
    \          | 28.31          | 5.62           |\n\nMaybe try 1 or 4 threads, thanks.\
    \ I just have 5600g and 4060ti-16g."
  created_at: 2023-12-15 14:28:24+00:00
  edited: false
  hidden: false
  id: 657c628801f76efce86075c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Why is the response slower than the 70B model?
