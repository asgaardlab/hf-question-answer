!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaferriereJC
conflicting_files: null
created_at: 2023-12-12 00:26:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-12-12T00:26:15.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5383714437484741
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>trying to run the boilerplate llama cpp code</p>

          <p>error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight''
          not found<br>llama_load_model_from_file: failed to load model<br>AVX = 1
          | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 |
          NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS =
          1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |<br>Traceback (most recent call last):<br>  File
          "/data/text-generation-webui/models/mixtral/mixtral.py", line 4, in <br>    llm
          = Llama(<br>  File "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py",
          line 923, in <strong>init</strong><br>    self._n_vocab = self.n_vocab()<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py",
          line 2184, in n_vocab<br>    return self._model.n_vocab()<br>  File "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py",
          line 250, in n_vocab<br>    assert self.model is not None<br>AssertionError</p>

          '
        raw: "trying to run the boilerplate llama cpp code\r\n\r\nerror loading model:\
          \ create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\nllama_load_model_from_file:\
          \ failed to load model\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX =\
          \ 0 |\r\nTraceback (most recent call last):\r\n  File \"/data/text-generation-webui/models/mixtral/mixtral.py\"\
          , line 4, in <module>\r\n    llm = Llama(\r\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
          , line 923, in __init__\r\n    self._n_vocab = self.n_vocab()\r\n  File\
          \ \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
          , line 2184, in n_vocab\r\n    return self._model.n_vocab()\r\n  File \"\
          /root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
          , line 250, in n_vocab\r\n    assert self.model is not None\r\nAssertionError\r\
          \n"
        updatedAt: '2023-12-12T00:26:15.964Z'
      numEdits: 0
      reactions: []
    id: 6577a8a76db22fc06c8a8c5f
    type: comment
  author: LaferriereJC
  content: "trying to run the boilerplate llama cpp code\r\n\r\nerror loading model:\
    \ create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\nllama_load_model_from_file:\
    \ failed to load model\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 |\
    \ AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0\
    \ | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\r\nTraceback (most\
    \ recent call last):\r\n  File \"/data/text-generation-webui/models/mixtral/mixtral.py\"\
    , line 4, in <module>\r\n    llm = Llama(\r\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
    , line 923, in __init__\r\n    self._n_vocab = self.n_vocab()\r\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
    , line 2184, in n_vocab\r\n    return self._model.n_vocab()\r\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
    , line 250, in n_vocab\r\n    assert self.model is not None\r\nAssertionError\r\
    \n"
  created_at: 2023-12-12 00:26:15+00:00
  edited: false
  hidden: false
  id: 6577a8a76db22fc06c8a8c5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
      fullname: MC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mclassHF2023
      type: user
    createdAt: '2023-12-12T01:17:37.000Z'
    data:
      edited: false
      editors:
      - mclassHF2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2072938233613968
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
          fullname: MC
          isHf: false
          isPro: false
          name: mclassHF2023
          type: user
        html: "<p>I'm getting a very similar error in Text Generation WebUI:</p>\n\
          <pre><code>File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
          modules\\ui_model_menu.py\", line 209, in load_model_wrapper\n\n\nshared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)\n\n         \
          \                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"\
          C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\modules\\\
          models.py\", line 88, in load_model\n\n\noutput = load_func_map[loader](model_name)\n\
          \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\modules\\models.py\", line 253, in llamacpp_loader\n\
          \n\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\n   \
          \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\\
          dev\\llamaindex_text_generation_webui\\text-generation-webui\\modules\\\
          llamacpp_model.py\", line 91, in from_pretrained\n\n\nresult.model = Llama(**params)\n\
          \n               ^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 923, in init\n\n\nself._n_vocab = self.n_vocab()\n\n  \
          \              ^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 2184, in n_vocab\n\n\nreturn self._model.n_vocab()\n\n\
          \       ^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 250, in n_vocab\n\n\nassert self.model is not None\n\n\
          \       ^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n</code></pre>\n"
        raw: "I'm getting a very similar error in Text Generation WebUI:\n```\nFile\
          \ \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\modules\\\
          ui_model_menu.py\", line 209, in load_model_wrapper\n\n\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\n\n                          \
          \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\modules\\models.py\", line 88, in load_model\n\n\n\
          output = load_func_map[loader](model_name)\n\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
          modules\\models.py\", line 253, in llamacpp_loader\n\n\nmodel, tokenizer\
          \ = LlamaCppModel.from_pretrained(model_file)\n\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 91, in from_pretrained\n\n\nresult.model\
          \ = Llama(**params)\n\n               ^^^^^^^^^^^^^^^\nFile \"C:\\dev\\\
          llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 923, in init\n\
          \n\nself._n_vocab = self.n_vocab()\n\n                ^^^^^^^^^^^^^^\nFile\
          \ \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 2184, in n_vocab\n\
          \n\nreturn self._model.n_vocab()\n\n       ^^^^^^^^^^^^^^^^^^^^^\nFile \"\
          C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 250, in n_vocab\n\
          \n\nassert self.model is not None\n\n       ^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\
          ```"
        updatedAt: '2023-12-12T01:17:37.645Z'
      numEdits: 0
      reactions: []
    id: 6577b4b1528e89e35f2ba7d8
    type: comment
  author: mclassHF2023
  content: "I'm getting a very similar error in Text Generation WebUI:\n```\nFile\
    \ \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\modules\\\
    ui_model_menu.py\", line 209, in load_model_wrapper\n\n\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name, loader)\n\n                                \
    \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
    text-generation-webui\\modules\\models.py\", line 88, in load_model\n\n\noutput\
    \ = load_func_map[loader](model_name)\n\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\modules\\\
    models.py\", line 253, in llamacpp_loader\n\n\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\
    \n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\\
    llamaindex_text_generation_webui\\text-generation-webui\\modules\\llamacpp_model.py\"\
    , line 91, in from_pretrained\n\n\nresult.model = Llama(**params)\n\n        \
    \       ^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 923,\
    \ in init\n\n\nself._n_vocab = self.n_vocab()\n\n                ^^^^^^^^^^^^^^\n\
    File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
    env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 2184, in n_vocab\n\n\
    \nreturn self._model.n_vocab()\n\n       ^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\\
    llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\env\\\
    Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 250, in n_vocab\n\n\nassert\
    \ self.model is not None\n\n       ^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n```"
  created_at: 2023-12-12 01:17:37+00:00
  edited: false
  hidden: false
  id: 6577b4b1528e89e35f2ba7d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/523a6f1db3e1c509a3347221d98026e3.svg
      fullname: John Doe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: errata
      type: user
    createdAt: '2023-12-12T01:19:14.000Z'
    data:
      edited: false
      editors:
      - errata
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7649340629577637
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/523a6f1db3e1c509a3347221d98026e3.svg
          fullname: John Doe
          isHf: false
          isPro: false
          name: errata
          type: user
        html: '<p>see the updated readme, you need to build from the <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/tree/mixtral">mixtral branch</a></p>

          '
        raw: see the updated readme, you need to build from the [mixtral branch](https://github.com/ggerganov/llama.cpp/tree/mixtral)
        updatedAt: '2023-12-12T01:19:14.075Z'
      numEdits: 0
      reactions: []
    id: 6577b5120c22bb8984a65154
    type: comment
  author: errata
  content: see the updated readme, you need to build from the [mixtral branch](https://github.com/ggerganov/llama.cpp/tree/mixtral)
  created_at: 2023-12-12 01:19:14+00:00
  edited: false
  hidden: false
  id: 6577b5120c22bb8984a65154
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663749541206-63184e6f1db54eb64dc6f4ff.png?w=200&h=200&f=face
      fullname: Frank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RandomLegend
      type: user
    createdAt: '2023-12-12T10:01:22.000Z'
    data:
      edited: false
      editors:
      - RandomLegend
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8699902296066284
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663749541206-63184e6f1db54eb64dc6f4ff.png?w=200&h=200&f=face
          fullname: Frank
          isHf: false
          isPro: false
          name: RandomLegend
          type: user
        html: '<blockquote>

          <p>see the updated readme, you need to build from the <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/tree/mixtral">mixtral branch</a></p>

          </blockquote>

          <p>Sorry for the naive question but how would i do that? I am on linux,
          how can i replace the llama.cpp that is inside oobabooga to the mixtral
          branch you linked to?</p>

          '
        raw: '> see the updated readme, you need to build from the [mixtral branch](https://github.com/ggerganov/llama.cpp/tree/mixtral)


          Sorry for the naive question but how would i do that? I am on linux, how
          can i replace the llama.cpp that is inside oobabooga to the mixtral branch
          you linked to?'
        updatedAt: '2023-12-12T10:01:22.727Z'
      numEdits: 0
      reactions: []
    id: 65782f72f7d98f6f61106dbb
    type: comment
  author: RandomLegend
  content: '> see the updated readme, you need to build from the [mixtral branch](https://github.com/ggerganov/llama.cpp/tree/mixtral)


    Sorry for the naive question but how would i do that? I am on linux, how can i
    replace the llama.cpp that is inside oobabooga to the mixtral branch you linked
    to?'
  created_at: 2023-12-12 10:01:22+00:00
  edited: false
  hidden: false
  id: 65782f72f7d98f6f61106dbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/921171db37d397559d875d3d49a64a3f.svg
      fullname: Jopaul Jose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DarkCoverUnleashed
      type: user
    createdAt: '2023-12-12T11:02:49.000Z'
    data:
      edited: false
      editors:
      - DarkCoverUnleashed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8859562277793884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/921171db37d397559d875d3d49a64a3f.svg
          fullname: Jopaul Jose
          isHf: false
          isPro: false
          name: DarkCoverUnleashed
          type: user
        html: '<blockquote>

          <p>see the updated readme, you need to build from the <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/tree/mixtral">mixtral branch</a></p>

          </blockquote>

          <p>And how exactly would you do that sir?</p>

          '
        raw: '> see the updated readme, you need to build from the [mixtral branch](https://github.com/ggerganov/llama.cpp/tree/mixtral)


          And how exactly would you do that sir?'
        updatedAt: '2023-12-12T11:02:49.938Z'
      numEdits: 0
      reactions: []
    id: 65783dd9d6ac68e25dfd7a69
    type: comment
  author: DarkCoverUnleashed
  content: '> see the updated readme, you need to build from the [mixtral branch](https://github.com/ggerganov/llama.cpp/tree/mixtral)


    And how exactly would you do that sir?'
  created_at: 2023-12-12 11:02:49+00:00
  edited: false
  hidden: false
  id: 65783dd9d6ac68e25dfd7a69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/523a6f1db3e1c509a3347221d98026e3.svg
      fullname: John Doe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: errata
      type: user
    createdAt: '2023-12-12T12:09:55.000Z'
    data:
      edited: false
      editors:
      - errata
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8416060209274292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/523a6f1db3e1c509a3347221d98026e3.svg
          fullname: John Doe
          isHf: false
          isPro: false
          name: errata
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RandomLegend&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RandomLegend\"\
          >@<span class=\"underline\">RandomLegend</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;DarkCoverUnleashed&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/DarkCoverUnleashed\"\
          >@<span class=\"underline\">DarkCoverUnleashed</span></a></span>\n\n\t</span></span>\
          \ sorry I'm not familiar with the structure of oobabooga, but if you have\
          \ cloned llama.cpp, just cd into it and run <code>git checkout mixtral</code>\
          \ to switch to the right branch, then compile it as before.</p>\n"
        raw: '@RandomLegend @DarkCoverUnleashed sorry I''m not familiar with the structure
          of oobabooga, but if you have cloned llama.cpp, just cd into it and run
          `git checkout mixtral` to switch to the right branch, then compile it as
          before.'
        updatedAt: '2023-12-12T12:09:55.785Z'
      numEdits: 0
      reactions: []
    id: 65784d939c5c26f96cfd4462
    type: comment
  author: errata
  content: '@RandomLegend @DarkCoverUnleashed sorry I''m not familiar with the structure
    of oobabooga, but if you have cloned llama.cpp, just cd into it and run `git checkout
    mixtral` to switch to the right branch, then compile it as before.'
  created_at: 2023-12-12 12:09:55+00:00
  edited: false
  hidden: false
  id: 65784d939c5c26f96cfd4462
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663749541206-63184e6f1db54eb64dc6f4ff.png?w=200&h=200&f=face
      fullname: Frank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RandomLegend
      type: user
    createdAt: '2023-12-12T15:29:59.000Z'
    data:
      edited: false
      editors:
      - RandomLegend
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8893756866455078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663749541206-63184e6f1db54eb64dc6f4ff.png?w=200&h=200&f=face
          fullname: Frank
          isHf: false
          isPro: false
          name: RandomLegend
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;errata&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/errata\">@<span class=\"\
          underline\">errata</span></a></span>\n\n\t</span></span> yeah i compiled\
          \ the mixtral branch and use it already in the terminal. Fascinating model.\
          \ But i have no idea how to get it running on oobabooga, ollama or gpt4all\
          \ :-D well i have to wait until they publish the patches then.</p>\n<p>Thanks!</p>\n"
        raw: '@errata yeah i compiled the mixtral branch and use it already in the
          terminal. Fascinating model. But i have no idea how to get it running on
          oobabooga, ollama or gpt4all :-D well i have to wait until they publish
          the patches then.


          Thanks!'
        updatedAt: '2023-12-12T15:29:59.416Z'
      numEdits: 0
      reactions: []
    id: 65787c779c5c26f96c0600ae
    type: comment
  author: RandomLegend
  content: '@errata yeah i compiled the mixtral branch and use it already in the terminal.
    Fascinating model. But i have no idea how to get it running on oobabooga, ollama
    or gpt4all :-D well i have to wait until they publish the patches then.


    Thanks!'
  created_at: 2023-12-12 15:29:59+00:00
  edited: false
  hidden: false
  id: 65787c779c5c26f96c0600ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
      fullname: MC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mclassHF2023
      type: user
    createdAt: '2023-12-12T15:45:21.000Z'
    data:
      edited: false
      editors:
      - mclassHF2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9413714408874512
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
          fullname: MC
          isHf: false
          isPro: false
          name: mclassHF2023
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RandomLegend&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RandomLegend\"\
          >@<span class=\"underline\">RandomLegend</span></a></span>\n\n\t</span></span>\
          \ Same here, I can use it on command line with that mixtral branch, LM Studio\
          \ has it built in now I think, but I think they have to merge this pull\
          \ request for it to get into anywhere \"stable\":<br><a rel=\"nofollow\"\
          \ href=\"https://github.com/ggerganov/llama.cpp/pull/4406\">https://github.com/ggerganov/llama.cpp/pull/4406</a><br>So\
          \ all we need is 1 review ;)</p>\n"
        raw: '@RandomLegend Same here, I can use it on command line with that mixtral
          branch, LM Studio has it built in now I think, but I think they have to
          merge this pull request for it to get into anywhere "stable":

          https://github.com/ggerganov/llama.cpp/pull/4406

          So all we need is 1 review ;)'
        updatedAt: '2023-12-12T15:45:21.168Z'
      numEdits: 0
      reactions: []
    id: 6578801132150025acbebe4a
    type: comment
  author: mclassHF2023
  content: '@RandomLegend Same here, I can use it on command line with that mixtral
    branch, LM Studio has it built in now I think, but I think they have to merge
    this pull request for it to get into anywhere "stable":

    https://github.com/ggerganov/llama.cpp/pull/4406

    So all we need is 1 review ;)'
  created_at: 2023-12-12 15:45:21+00:00
  edited: false
  hidden: false
  id: 6578801132150025acbebe4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42826154670c7772495c328affe6435d.svg
      fullname: 'Nicolas Goiginger '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nicoolodion
      type: user
    createdAt: '2023-12-13T18:53:56.000Z'
    data:
      edited: false
      editors:
      - Nicoolodion
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9950512647628784
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42826154670c7772495c328affe6435d.svg
          fullname: 'Nicolas Goiginger '
          isHf: false
          isPro: false
          name: Nicoolodion
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mclassHF2023&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mclassHF2023\"\
          >@<span class=\"underline\">mclassHF2023</span></a></span>\n\n\t</span></span>\
          \ Yeah, seemed like it was further away than one merge... Doesn't change\
          \ anything as long as oobabooga doesn't merge it too</p>\n"
        raw: '@mclassHF2023 Yeah, seemed like it was further away than one merge...
          Doesn''t change anything as long as oobabooga doesn''t merge it too'
        updatedAt: '2023-12-13T18:53:56.973Z'
      numEdits: 0
      reactions: []
    id: 6579fdc421a8b3183660c664
    type: comment
  author: Nicoolodion
  content: '@mclassHF2023 Yeah, seemed like it was further away than one merge...
    Doesn''t change anything as long as oobabooga doesn''t merge it too'
  created_at: 2023-12-13 18:53:56+00:00
  edited: false
  hidden: false
  id: 6579fdc421a8b3183660c664
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: error
