!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sagar3745
conflicting_files: null
created_at: 2023-12-16 05:24:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-16T05:24:17.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9175682663917542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: '<p>Hi </p>

          <p>I''m encountering an issue with my Google Colab notebook where it doesn''t
          seem to utilize the GPU, despite having the GPU runtime enabled. I''ve been
          working on a project that requires GPU acceleration, and this issue is hindering
          my progress.</p>

          <p>To respond with a simple Hi, it is taking 11 minutes, something is wrong,
          can someone help, please?</p>

          <p>Here is the link to my Colab notebook for reference: <a rel="nofollow"
          href="https://colab.research.google.com/drive/1331XPrqg4wKvT5ymQOwG4QY3Xk_kNLtl?usp=sharing">https://colab.research.google.com/drive/1331XPrqg4wKvT5ymQOwG4QY3Xk_kNLtl?usp=sharing</a></p>

          <p>To provide more context:</p>

          <p>I''ve ensured that the notebook settings are set to use GPU.<br>I''ve
          tried restarting the runtime and resetting all runtimes, but the issue persists.<br>model
          I have used:- mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf<br>I''m New to using
          these llama-cpp and gguf files.</p>

          <p>Thank you in advance for your time and assistance.</p>

          '
        raw: "\r\n\r\nHi \r\n\r\nI'm encountering an issue with my Google Colab notebook\
          \ where it doesn't seem to utilize the GPU, despite having the GPU runtime\
          \ enabled. I've been working on a project that requires GPU acceleration,\
          \ and this issue is hindering my progress.\r\n\r\nTo respond with a simple\
          \ Hi, it is taking 11 minutes, something is wrong, can someone help, please?\r\
          \n\r\nHere is the link to my Colab notebook for reference: https://colab.research.google.com/drive/1331XPrqg4wKvT5ymQOwG4QY3Xk_kNLtl?usp=sharing\r\
          \n\r\nTo provide more context:\r\n\r\nI've ensured that the notebook settings\
          \ are set to use GPU.\r\nI've tried restarting the runtime and resetting\
          \ all runtimes, but the issue persists.\r\nmodel I have used:- mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\r\
          \nI'm New to using these llama-cpp and gguf files.\r\n\r\nThank you in advance\
          \ for your time and assistance.\r\n\r\n"
        updatedAt: '2023-12-16T05:24:17.646Z'
      numEdits: 0
      reactions: []
    id: 657d3481365456e362f606cc
    type: comment
  author: Sagar3745
  content: "\r\n\r\nHi \r\n\r\nI'm encountering an issue with my Google Colab notebook\
    \ where it doesn't seem to utilize the GPU, despite having the GPU runtime enabled.\
    \ I've been working on a project that requires GPU acceleration, and this issue\
    \ is hindering my progress.\r\n\r\nTo respond with a simple Hi, it is taking 11\
    \ minutes, something is wrong, can someone help, please?\r\n\r\nHere is the link\
    \ to my Colab notebook for reference: https://colab.research.google.com/drive/1331XPrqg4wKvT5ymQOwG4QY3Xk_kNLtl?usp=sharing\r\
    \n\r\nTo provide more context:\r\n\r\nI've ensured that the notebook settings\
    \ are set to use GPU.\r\nI've tried restarting the runtime and resetting all runtimes,\
    \ but the issue persists.\r\nmodel I have used:- mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\r\
    \nI'm New to using these llama-cpp and gguf files.\r\n\r\nThank you in advance\
    \ for your time and assistance.\r\n\r\n"
  created_at: 2023-12-16 05:24:17+00:00
  edited: false
  hidden: false
  id: 657d3481365456e362f606cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-16T05:30:43.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8698071837425232
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: '<p>Here is a screen shot of where it is running, this is a second run,
          the first one I got for 11 minutes. Where it is not using anything CPU or
          GPU. I''m not getting it.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/VI7VCV8x2CHoPzKeEPVQ4.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/VI7VCV8x2CHoPzKeEPVQ4.png"></a></p>

          '
        raw: 'Here is a screen shot of where it is running, this is a second run,
          the first one I got for 11 minutes. Where it is not using anything CPU or
          GPU. I''m not getting it.



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/VI7VCV8x2CHoPzKeEPVQ4.png)





          '
        updatedAt: '2023-12-16T05:30:43.751Z'
      numEdits: 0
      reactions: []
    id: 657d3603e50ca9a699a1fa59
    type: comment
  author: Sagar3745
  content: 'Here is a screen shot of where it is running, this is a second run, the
    first one I got for 11 minutes. Where it is not using anything CPU or GPU. I''m
    not getting it.



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/VI7VCV8x2CHoPzKeEPVQ4.png)





    '
  created_at: 2023-12-16 05:30:43+00:00
  edited: false
  hidden: false
  id: 657d3603e50ca9a699a1fa59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
      fullname: hammad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hammad93
      type: user
    createdAt: '2023-12-16T10:57:16.000Z'
    data:
      edited: true
      editors:
      - hammad93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9136040806770325
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
          fullname: hammad
          isHf: false
          isPro: false
          name: hammad93
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sagar3745&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sagar3745\">@<span class=\"\
          underline\">Sagar3745</span></a></span>\n\n\t</span></span> I think the\
          \ problem might be with n_gpu_layers set to zero. If i understand correctly\
          \ this the number of layers you are offloading from CPU to GPU. try setting\
          \ n_gpu_layers to a number and test it from there. the higher the number\
          \ the faster inference works but you might run out of VRAM and the notebook\
          \ will crash.</p>\n"
        raw: '@Sagar3745 I think the problem might be with n_gpu_layers set to zero.
          If i understand correctly this the number of layers you are offloading from
          CPU to GPU. try setting n_gpu_layers to a number and test it from there.
          the higher the number the faster inference works but you might run out of
          VRAM and the notebook will crash.'
        updatedAt: '2023-12-16T11:02:29.827Z'
      numEdits: 1
      reactions: []
    id: 657d828c19ca6a5e925b12ea
    type: comment
  author: hammad93
  content: '@Sagar3745 I think the problem might be with n_gpu_layers set to zero.
    If i understand correctly this the number of layers you are offloading from CPU
    to GPU. try setting n_gpu_layers to a number and test it from there. the higher
    the number the faster inference works but you might run out of VRAM and the notebook
    will crash.'
  created_at: 2023-12-16 10:57:16+00:00
  edited: true
  hidden: false
  id: 657d828c19ca6a5e925b12ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-18T17:21:37.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9632065296173096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;hammad93&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hammad93\"\
          >@<span class=\"underline\">hammad93</span></a></span>\n\n\t</span></span>\
          \ thanks for the help.<br>I tried with some value for n_gpu_layers,  but\
          \ did not help. I have executed the same code in a server, it is working\
          \ in the CPU, the inference is slow, if the prompt tokens are high it is\
          \ taking a bit longer.<br>Now I'm working on running it on GPU. But no result.</p>\n\
          <p>But Extremely thanks for the help.</p>\n"
        raw: "Hi @hammad93 thanks for the help. \nI tried with some value for n_gpu_layers,\
          \  but did not help. I have executed the same code in a server, it is working\
          \ in the CPU, the inference is slow, if the prompt tokens are high it is\
          \ taking a bit longer. \nNow I'm working on running it on GPU. But no result.\n\
          \n\nBut Extremely thanks for the help."
        updatedAt: '2023-12-18T17:21:37.012Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - hammad93
    id: 65807fa1defc9c0d30eac5e7
    type: comment
  author: Sagar3745
  content: "Hi @hammad93 thanks for the help. \nI tried with some value for n_gpu_layers,\
    \  but did not help. I have executed the same code in a server, it is working\
    \ in the CPU, the inference is slow, if the prompt tokens are high it is taking\
    \ a bit longer. \nNow I'm working on running it on GPU. But no result.\n\n\nBut\
    \ Extremely thanks for the help."
  created_at: 2023-12-18 17:21:37+00:00
  edited: false
  hidden: false
  id: 65807fa1defc9c0d30eac5e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
      fullname: hammad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hammad93
      type: user
    createdAt: '2023-12-18T23:45:05.000Z'
    data:
      edited: true
      editors:
      - hammad93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7957127690315247
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
          fullname: hammad
          isHf: false
          isPro: false
          name: hammad93
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sagar3745&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sagar3745\">@<span class=\"\
          underline\">Sagar3745</span></a></span>\n\n\t</span></span> try adding this\
          \ to CMAKE_ARGS LLAMA_CUBLAS=1</p>\n"
        raw: '@Sagar3745 try adding this to CMAKE_ARGS LLAMA_CUBLAS=1'
        updatedAt: '2023-12-18T23:45:41.045Z'
      numEdits: 1
      reactions: []
    id: 6580d981abafd960c83c5936
    type: comment
  author: hammad93
  content: '@Sagar3745 try adding this to CMAKE_ARGS LLAMA_CUBLAS=1'
  created_at: 2023-12-18 23:45:05+00:00
  edited: true
  hidden: false
  id: 6580d981abafd960c83c5936
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-19T09:38:48.000Z'
    data:
      edited: true
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6588514447212219
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;hammad93&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hammad93\"\
          >@<span class=\"underline\">hammad93</span></a></span>\n\n\t</span></span>\
          \  Tried That But no use.</p>\n<p>This is what the model is printing in\
          \ my terminal.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/hXihq0Dk5jIec-JNNwD75.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/hXihq0Dk5jIec-JNNwD75.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/4vhXf2frhhJnwvvrPg4Jt.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/4vhXf2frhhJnwvvrPg4Jt.png\"\
          ></a><br>for each generation.</p>\n<p>I'm using the standard template same\
          \ as mentioned in the repo.<br><s>[INST] {prmopt} [/INST]</s></p>\n"
        raw: 'Hi @hammad93  Tried That But no use.


          This is what the model is printing in my terminal.



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/hXihq0Dk5jIec-JNNwD75.png)



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/4vhXf2frhhJnwvvrPg4Jt.png)

          for each generation.


          I''m using the standard template same as mentioned in the repo.

          <s>[INST] {prmopt} [/INST]</s>'
        updatedAt: '2023-12-19T09:40:19.528Z'
      numEdits: 1
      reactions: []
    id: 658164a8a760379049dd2cbe
    type: comment
  author: Sagar3745
  content: 'Hi @hammad93  Tried That But no use.


    This is what the model is printing in my terminal.



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/hXihq0Dk5jIec-JNNwD75.png)



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/4vhXf2frhhJnwvvrPg4Jt.png)

    for each generation.


    I''m using the standard template same as mentioned in the repo.

    <s>[INST] {prmopt} [/INST]</s>'
  created_at: 2023-12-19 09:38:48+00:00
  edited: true
  hidden: false
  id: 658164a8a760379049dd2cbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
      fullname: hammad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hammad93
      type: user
    createdAt: '2023-12-19T15:13:59.000Z'
    data:
      edited: true
      editors:
      - hammad93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9013445377349854
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
          fullname: hammad
          isHf: false
          isPro: false
          name: hammad93
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sagar3745&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sagar3745\">@<span class=\"\
          underline\">Sagar3745</span></a></span>\n\n\t</span></span>  thats weird\
          \ i got it to work on the 2xT4 gpus on kaggle using LLAMA_CUBLAS=1 and n_gpu_layers\
          \ and its using both GPUs while running inference, try pulling the repository\
          \ llama.cpp and building it from the repo.<br>github repo: <a rel=\"nofollow\"\
          \ href=\"https://github.com/ggerganov/llama.cpp\">https://github.com/ggerganov/llama.cpp</a><br>make\
          \ command: make LLAMA_CUBLAS=1</p>\n"
        raw: '@Sagar3745  thats weird i got it to work on the 2xT4 gpus on kaggle
          using LLAMA_CUBLAS=1 and n_gpu_layers and its using both GPUs while running
          inference, try pulling the repository llama.cpp and building it from the
          repo.

          github repo: https://github.com/ggerganov/llama.cpp

          make command: make LLAMA_CUBLAS=1'
        updatedAt: '2023-12-19T15:17:31.213Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Uname
    id: 6581b337d0938e7c6327b938
    type: comment
  author: hammad93
  content: '@Sagar3745  thats weird i got it to work on the 2xT4 gpus on kaggle using
    LLAMA_CUBLAS=1 and n_gpu_layers and its using both GPUs while running inference,
    try pulling the repository llama.cpp and building it from the repo.

    github repo: https://github.com/ggerganov/llama.cpp

    make command: make LLAMA_CUBLAS=1'
  created_at: 2023-12-19 15:13:59+00:00
  edited: true
  hidden: false
  id: 6581b337d0938e7c6327b938
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-20T06:40:36.000Z'
    data:
      edited: true
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9623958468437195
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hammad93&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hammad93\">@<span class=\"\
          underline\">hammad93</span></a></span>\n\n\t</span></span>  Thanks I got\
          \ it, was able to load it in GPU but crashed because of only 15GPU, in Colab.\
          \ But I'm not sure how you did in Kaggel, as the model is 24gb the allowed\
          \ storage is only 19.5 GB I'm not able to download the model. </p>\n<p>Can\
          \ you tell me how you can download the model in Kaggel?<br>model:- mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf</p>\n\
          <p>for now I have tried with smaller model:-</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/6lbc8tMeGOi6uLUk4NWlV.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/6lbc8tMeGOi6uLUk4NWlV.png\"\
          ></a></p>\n"
        raw: "@hammad93  Thanks I got it, was able to load it in GPU but crashed because\
          \ of only 15GPU, in Colab. But I'm not sure how you did in Kaggel, as the\
          \ model is 24gb the allowed storage is only 19.5 GB I'm not able to download\
          \ the model. \n\nCan you tell me how you can download the model in Kaggel?\n\
          model:- mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n\n\nfor now I have tried\
          \ with smaller model:-\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/6lbc8tMeGOi6uLUk4NWlV.png)\n"
        updatedAt: '2023-12-20T07:10:35.417Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hammad93
    id: 65828c6405c177eea3b0e017
    type: comment
  author: Sagar3745
  content: "@hammad93  Thanks I got it, was able to load it in GPU but crashed because\
    \ of only 15GPU, in Colab. But I'm not sure how you did in Kaggel, as the model\
    \ is 24gb the allowed storage is only 19.5 GB I'm not able to download the model.\
    \ \n\nCan you tell me how you can download the model in Kaggel?\nmodel:- mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n\
    \n\nfor now I have tried with smaller model:-\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/637b1a01e20e8765c5279e8f/6lbc8tMeGOi6uLUk4NWlV.png)\n"
  created_at: 2023-12-20 06:40:36+00:00
  edited: true
  hidden: false
  id: 65828c6405c177eea3b0e017
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
      fullname: hammad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hammad93
      type: user
    createdAt: '2023-12-20T13:01:26.000Z'
    data:
      edited: false
      editors:
      - hammad93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9204967617988586
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
          fullname: hammad
          isHf: false
          isPro: false
          name: hammad93
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sagar3745&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sagar3745\">@<span class=\"\
          underline\">Sagar3745</span></a></span>\n\n\t</span></span> Great! to use\
          \ the full 73GB change directory to /kaggle . Also try playing around the\
          \ n_gpu_layers number to try to fit the model in VRAM,and check out the\
          \ docs as there are other options that should help with memory allocation.</p>\n"
        raw: '@Sagar3745 Great! to use the full 73GB change directory to /kaggle .
          Also try playing around the n_gpu_layers number to try to fit the model
          in VRAM,and check out the docs as there are other options that should help
          with memory allocation.'
        updatedAt: '2023-12-20T13:01:26.982Z'
      numEdits: 0
      reactions: []
    id: 6582e5a66df15717695c924e
    type: comment
  author: hammad93
  content: '@Sagar3745 Great! to use the full 73GB change directory to /kaggle . Also
    try playing around the n_gpu_layers number to try to fit the model in VRAM,and
    check out the docs as there are other options that should help with memory allocation.'
  created_at: 2023-12-20 13:01:26+00:00
  edited: false
  hidden: false
  id: 6582e5a66df15717695c924e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-12-22T10:30:39.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9906057715415955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: '<p>How did you guys did it in kaggle? which accelerator? can you guys
          be kind enough to share notebook?</p>

          '
        raw: How did you guys did it in kaggle? which accelerator? can you guys be
          kind enough to share notebook?
        updatedAt: '2023-12-22T10:30:39.941Z'
      numEdits: 0
      reactions: []
    id: 6585654f132c2d3eff82d27d
    type: comment
  author: ianuvrat
  content: How did you guys did it in kaggle? which accelerator? can you guys be kind
    enough to share notebook?
  created_at: 2023-12-22 10:30:39+00:00
  edited: false
  hidden: false
  id: 6585654f132c2d3eff82d27d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3bb4d53cee449f5319d2fce23dd8031.svg
      fullname: Edwin Mathenge
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LilWonga
      type: user
    createdAt: '2023-12-22T13:40:09.000Z'
    data:
      edited: false
      editors:
      - LilWonga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9433019757270813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3bb4d53cee449f5319d2fce23dd8031.svg
          fullname: Edwin Mathenge
          isHf: false
          isPro: false
          name: LilWonga
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hammad93&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hammad93\">@<span class=\"\
          underline\">hammad93</span></a></span>\n\n\t</span></span> Hey, Could you\
          \ share your notebook?... I would greatly appreciate. I'm kinda having a\
          \ hard time with the implementation...</p>\n"
        raw: '@hammad93 Hey, Could you share your notebook?... I would greatly appreciate.
          I''m kinda having a hard time with the implementation...


          '
        updatedAt: '2023-12-22T13:40:09.943Z'
      numEdits: 0
      reactions: []
    id: 658591b9b48986be10e7d5e2
    type: comment
  author: LilWonga
  content: '@hammad93 Hey, Could you share your notebook?... I would greatly appreciate.
    I''m kinda having a hard time with the implementation...


    '
  created_at: 2023-12-22 13:40:09+00:00
  edited: false
  hidden: false
  id: 658591b9b48986be10e7d5e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
      fullname: hammad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hammad93
      type: user
    createdAt: '2023-12-23T16:22:41.000Z'
    data:
      edited: true
      editors:
      - hammad93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9390730857849121
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
          fullname: hammad
          isHf: false
          isPro: false
          name: hammad93
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LilWonga&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/LilWonga\">@<span class=\"\
          underline\">LilWonga</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;ianuvrat&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/ianuvrat\">@<span class=\"underline\">ianuvrat</span></a></span>\n\
          \n\t</span></span><br>repo:<a rel=\"nofollow\" href=\"https://github.com/mth93/mixtral_llama_cpp\"\
          >https://github.com/mth93/mixtral_llama_cpp</a></p>\n<p>you can use main\
          \ branch instead of mixtral as it's already merged into main. i tested this\
          \ notebook with many of the opensource LLMs in GGUF format and it works\
          \ great and utilizes both GPUs in the 2xT4 gpus notebook. you'll have to\
          \ play around with the context size (-c) and the n-gpu-layers with each\
          \ LLM as this(gpu layers) is how you utilize more vram opposed to ram if\
          \ i understand correctly. Also there are other options for the llama.cpp\
          \ server that might improve performance that i'm still experimenting with.\
          \ and the bigger the context the better as it allows the LLM to remember\
          \ bigger parts of the history of the conversation.<br>this works for launching\
          \ a chatbot and an openai compatible api.</p>\n<p>also check the model you're\
          \ going to use if it needs a prompt template you'll find it on the original\
          \ model's page on huggingface. there is no way to add the prompt template\
          \ to the llama.cpp server currently so you'll have to add it to the prompt\
          \ in the request(If you're using the API) or add it to the chat in the chatbot.</p>\n\
          <p>most LLMs will give you very weird responses if it requires a prompt\
          \ template and you don't use it based on what i tested.</p>\n<p>hope this\
          \ helps!</p>\n"
        raw: "@LilWonga @ianuvrat \nrepo:https://github.com/mth93/mixtral_llama_cpp\n\
          \nyou can use main branch instead of mixtral as it's already merged into\
          \ main. i tested this notebook with many of the opensource LLMs in GGUF\
          \ format and it works great and utilizes both GPUs in the 2xT4 gpus notebook.\
          \ you'll have to play around with the context size (-c) and the n-gpu-layers\
          \ with each LLM as this(gpu layers) is how you utilize more vram opposed\
          \ to ram if i understand correctly. Also there are other options for the\
          \ llama.cpp server that might improve performance that i'm still experimenting\
          \ with. and the bigger the context the better as it allows the LLM to remember\
          \ bigger parts of the history of the conversation.\nthis works for launching\
          \ a chatbot and an openai compatible api.\n\nalso check the model you're\
          \ going to use if it needs a prompt template you'll find it on the original\
          \ model's page on huggingface. there is no way to add the prompt template\
          \ to the llama.cpp server currently so you'll have to add it to the prompt\
          \ in the request(If you're using the API) or add it to the chat in the chatbot.\n\
          \nmost LLMs will give you very weird responses if it requires a prompt template\
          \ and you don't use it based on what i tested.\n\nhope this helps!"
        updatedAt: '2023-12-23T16:54:32.033Z'
      numEdits: 8
      reactions: []
    id: 6587095165df457a558eab62
    type: comment
  author: hammad93
  content: "@LilWonga @ianuvrat \nrepo:https://github.com/mth93/mixtral_llama_cpp\n\
    \nyou can use main branch instead of mixtral as it's already merged into main.\
    \ i tested this notebook with many of the opensource LLMs in GGUF format and it\
    \ works great and utilizes both GPUs in the 2xT4 gpus notebook. you'll have to\
    \ play around with the context size (-c) and the n-gpu-layers with each LLM as\
    \ this(gpu layers) is how you utilize more vram opposed to ram if i understand\
    \ correctly. Also there are other options for the llama.cpp server that might\
    \ improve performance that i'm still experimenting with. and the bigger the context\
    \ the better as it allows the LLM to remember bigger parts of the history of the\
    \ conversation.\nthis works for launching a chatbot and an openai compatible api.\n\
    \nalso check the model you're going to use if it needs a prompt template you'll\
    \ find it on the original model's page on huggingface. there is no way to add\
    \ the prompt template to the llama.cpp server currently so you'll have to add\
    \ it to the prompt in the request(If you're using the API) or add it to the chat\
    \ in the chatbot.\n\nmost LLMs will give you very weird responses if it requires\
    \ a prompt template and you don't use it based on what i tested.\n\nhope this\
    \ helps!"
  created_at: 2023-12-23 16:22:41+00:00
  edited: true
  hidden: false
  id: 6587095165df457a558eab62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
      fullname: hammad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hammad93
      type: user
    createdAt: '2023-12-23T16:35:15.000Z'
    data:
      edited: true
      editors:
      - hammad93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446230530738831
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef17e599a5f973f0dcab1b577cb50fdb.svg
          fullname: hammad
          isHf: false
          isPro: false
          name: hammad93
          type: user
        html: '<p>Also guys i''d appreciate it if anyone can explain the difference
          between GGUF,GGML,AWQ etc. if i understand correctly these are different
          quantization algorithms, but i have no idea what''s the difference between
          them and how that affects performance and model size.</p>

          '
        raw: Also guys i'd appreciate it if anyone can explain the difference between
          GGUF,GGML,AWQ etc. if i understand correctly these are different quantization
          algorithms, but i have no idea what's the difference between them and how
          that affects performance and model size.
        updatedAt: '2023-12-23T17:00:38.586Z'
      numEdits: 2
      reactions: []
    id: 65870c43a3bfb30fdb7b850e
    type: comment
  author: hammad93
  content: Also guys i'd appreciate it if anyone can explain the difference between
    GGUF,GGML,AWQ etc. if i understand correctly these are different quantization
    algorithms, but i have no idea what's the difference between them and how that
    affects performance and model size.
  created_at: 2023-12-23 16:35:15+00:00
  edited: true
  hidden: false
  id: 65870c43a3bfb30fdb7b850e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-26T03:57:31.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9326110482215881
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: '<p>An update on the GPU issue, could not be solved in my server, so
          I downloaded the original model mistralai/Mixtral-8x7B-Instruct-v0.1, and
          loaded it as a quantized.<br>GPU usage:- 24GB (without inference).</p>

          <p>This works perfectly fine for me. </p>

          '
        raw: "An update on the GPU issue, could not be solved in my server, so I downloaded\
          \ the original model mistralai/Mixtral-8x7B-Instruct-v0.1, and loaded it\
          \ as a quantized. \nGPU usage:- 24GB (without inference).\n\nThis works\
          \ perfectly fine for me. "
        updatedAt: '2023-12-26T03:57:31.297Z'
      numEdits: 0
      reactions: []
    id: 658a4f2b35f23c0f1c557495
    type: comment
  author: Sagar3745
  content: "An update on the GPU issue, could not be solved in my server, so I downloaded\
    \ the original model mistralai/Mixtral-8x7B-Instruct-v0.1, and loaded it as a\
    \ quantized. \nGPU usage:- 24GB (without inference).\n\nThis works perfectly fine\
    \ for me. "
  created_at: 2023-12-26 03:57:31+00:00
  edited: false
  hidden: false
  id: 658a4f2b35f23c0f1c557495
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-12-26T04:10:06.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9287365674972534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sagar3745&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sagar3745\">@<span class=\"\
          underline\">Sagar3745</span></a></span>\n\n\t</span></span> , sorry but\
          \ i did not understood. What and how you did? I want to use yhis model with\
          \ langchain agents  for inferencing</p>\n"
        raw: '@Sagar3745 , sorry but i did not understood. What and how you did? I
          want to use yhis model with langchain agents  for inferencing'
        updatedAt: '2023-12-26T04:10:06.119Z'
      numEdits: 0
      reactions: []
    id: 658a521e9835f3cce20690b4
    type: comment
  author: ianuvrat
  content: '@Sagar3745 , sorry but i did not understood. What and how you did? I want
    to use yhis model with langchain agents  for inferencing'
  created_at: 2023-12-26 04:10:06+00:00
  edited: false
  hidden: false
  id: 658a521e9835f3cce20690b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-26T04:22:40.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9297295212745667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ianuvrat&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ianuvrat\">@<span class=\"\
          underline\">ianuvrat</span></a></span>\n\n\t</span></span>, initially I\
          \ faced issues in loading the gguf model in GPU in my server. it worked\
          \ in Kaggel Notebook and Google Colab I was able to load it in the GPU.<br>So\
          \ instead of the .gguf model, I downloaded the original mixtral instruct\
          \ model from mistralai huggingface and just loaded it as a quantized model\
          \ which only takes 24GB GPU.</p>\n<p>There is nothing new I did, before\
          \ gguf models, I used to load the models like llama2 13b, and orca2 13b\
          \ as quantized models, in the same way, I did for this <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          >https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1</a> model.<br>Hope\
          \ this helps.</p>\n"
        raw: "@ianuvrat, initially I faced issues in loading the gguf model in GPU\
          \ in my server. it worked in Kaggel Notebook and Google Colab I was able\
          \ to load it in the GPU. \nSo instead of the .gguf model, I downloaded the\
          \ original mixtral instruct model from mistralai huggingface and just loaded\
          \ it as a quantized model which only takes 24GB GPU.\n\nThere is nothing\
          \ new I did, before gguf models, I used to load the models like llama2 13b,\
          \ and orca2 13b as quantized models, in the same way, I did for this https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\
          \ model. \nHope this helps."
        updatedAt: '2023-12-26T04:22:40.642Z'
      numEdits: 0
      reactions: []
    id: 658a5510dda02636b019b8e5
    type: comment
  author: Sagar3745
  content: "@ianuvrat, initially I faced issues in loading the gguf model in GPU in\
    \ my server. it worked in Kaggel Notebook and Google Colab I was able to load\
    \ it in the GPU. \nSo instead of the .gguf model, I downloaded the original mixtral\
    \ instruct model from mistralai huggingface and just loaded it as a quantized\
    \ model which only takes 24GB GPU.\n\nThere is nothing new I did, before gguf\
    \ models, I used to load the models like llama2 13b, and orca2 13b as quantized\
    \ models, in the same way, I did for this https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\
    \ model. \nHope this helps."
  created_at: 2023-12-26 04:22:40+00:00
  edited: false
  hidden: false
  id: 658a5510dda02636b019b8e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-12-26T04:58:11.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532430768013
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sagar3745&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sagar3745\">@<span class=\"\
          underline\">Sagar3745</span></a></span>\n\n\t</span></span>  interesting.\
          \ So uou downloded the original mode adn loaded it as quantizd? Right.</p>\n\
          <p>Can you be kind enough to share the colab notebook how you did this?\
          \ I\u2019ll also try the same and see if it works for me or not.</p>\n"
        raw: "@Sagar3745  interesting. So uou downloded the original mode adn loaded\
          \ it as quantizd? Right.\n\nCan you be kind enough to share the colab notebook\
          \ how you did this? I\u2019ll also try the same and see if it works for\
          \ me or not.\n"
        updatedAt: '2023-12-26T04:58:11.438Z'
      numEdits: 0
      reactions: []
    id: 658a5d63e878be571b801667
    type: comment
  author: ianuvrat
  content: "@Sagar3745  interesting. So uou downloded the original mode adn loaded\
    \ it as quantizd? Right.\n\nCan you be kind enough to share the colab notebook\
    \ how you did this? I\u2019ll also try the same and see if it works for me or\
    \ not.\n"
  created_at: 2023-12-26 04:58:11+00:00
  edited: false
  hidden: false
  id: 658a5d63e878be571b801667
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-12-26T05:07:49.000Z'
    data:
      edited: true
      editors:
      - Sagar3745
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41357913613319397
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ianuvrat&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ianuvrat\">@<span class=\"\
          underline\">ianuvrat</span></a></span>\n\n\t</span></span>, Sure.<br>here\
          \ is the Code, which uses the same model loading method as I did. </p>\n\
          <p>loading the model:-<br>................................................................................................................................<br>import\
          \ torch<br>import transformers<br>from torch import bfloat16<br>from langchain\
          \ import HuggingFacePipeline, PromptTemplate, LLMChain<br>import re<br>model_path\
          \ = 'microsoft/Orca-2-13b'<br>hf_key = \"hugging_api\"<br>import time</p>\n\
          <p>def load_model():<br>    device = f\"cuda:{torch.cuda.current_device()}\"\
          \ if torch.cuda.is_available() else \"cpu\"<br>    bnb_config = transformers.BitsAndBytesConfig(<br>\
          \        load_in_4bit=True,<br>        bnb_4bit_quant_type=\"nf4\",<br>\
          \        bnb_4bit_use_double_quant=True,<br>        bnb_4bit_compute_dtype=bfloat16,<br>\
          \    )<br>    model_config = transformers.AutoConfig.from_pretrained(<br>\
          \        model_path, use_auth_token=hf_key<br>    )<br>    model = transformers.AutoModelForCausalLM.from_pretrained(<br>\
          \        model_path,<br>        trust_remote_code=True,<br>        config=model_config,<br>\
          \        quantization_config=bnb_config,<br>        device_map=\"auto\"\
          ,<br>        use_auth_token=hf_key,<br>    )<br>    model.eval()<br>   \
          \ print(f\"Model loaded on {device}:- \")<br>    tokenizer = transformers.AutoTokenizer.from_pretrained(<br>\
          \        model_path, use_auth_token=hf_key<br>    )<br>    generate_text\
          \ = transformers.pipeline(<br>        model=model,<br>        tokenizer=tokenizer,<br>\
          \        return_full_text=True,<br>        task=\"text-generation\",<br>\
          \        temperature=0.0,<br>        max_new_tokens=400,<br>        repetition_penalty=1.1,<br>\
          \        do_sample=False,<br>        top_k=5,<br>        num_return_sequences=1,<br>\
          \        eos_token_id=tokenizer.eos_token_id,<br>    )<br>    return HuggingFacePipeline(<br>\
          \        pipeline=generate_text, model_kwargs={\"temperature\": 0}<br> \
          \   )</p>\n<p>llm_model = load_model()<br>....................................................................................................................</p>\n\
          <p>requirements:-<br>....................................................................................<br>pip\
          \ install -U llama-cpp-python<br>pip install -U transformers<br>pip install\
          \ -U accelerate<br>pip install -U bitsandbytes<br>pip install -U langchain<br>pip\
          \ install -U sentencepiece<br>....................................................................................<br>remember\
          \ the original model is too huge to download in the Colab or Kaggel, unless\
          \ you are a pro user. I did it on my server which has enough disk space\
          \ to download the model.</p>\n<p>colab link:- <a rel=\"nofollow\" href=\"\
          https://colab.research.google.com/drive/1oHRk8dHYhGc9z6Olrx4pmFymf-LwhF_F?usp=sharing\"\
          >https://colab.research.google.com/drive/1oHRk8dHYhGc9z6Olrx4pmFymf-LwhF_F?usp=sharing</a></p>\n"
        raw: "@ianuvrat, Sure.\nhere is the Code, which uses the same model loading\
          \ method as I did. \n\nloading the model:-\n................................................................................................................................\n\
          import torch\nimport transformers\nfrom torch import bfloat16\nfrom langchain\
          \ import HuggingFacePipeline, PromptTemplate, LLMChain\nimport re\nmodel_path\
          \ = 'microsoft/Orca-2-13b'\nhf_key = \"hugging_api\"\nimport time\n\ndef\
          \ load_model():\n    device = f\"cuda:{torch.cuda.current_device()}\" if\
          \ torch.cuda.is_available() else \"cpu\"\n    bnb_config = transformers.BitsAndBytesConfig(\n\
          \        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n    \
          \    bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=bfloat16,\n\
          \    )\n    model_config = transformers.AutoConfig.from_pretrained(\n  \
          \      model_path, use_auth_token=hf_key\n    )\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \        model_path,\n        trust_remote_code=True,\n        config=model_config,\n\
          \        quantization_config=bnb_config,\n        device_map=\"auto\",\n\
          \        use_auth_token=hf_key,\n    )\n    model.eval()\n    print(f\"\
          Model loaded on {device}:- \")\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n\
          \        model_path, use_auth_token=hf_key\n    )\n    generate_text = transformers.pipeline(\n\
          \        model=model,\n        tokenizer=tokenizer,\n        return_full_text=True,\n\
          \        task=\"text-generation\",\n        temperature=0.0,\n        max_new_tokens=400,\n\
          \        repetition_penalty=1.1,\n        do_sample=False,\n        top_k=5,\n\
          \        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n\
          \    )\n    return HuggingFacePipeline(\n        pipeline=generate_text,\
          \ model_kwargs={\"temperature\": 0}\n    )\n\n\nllm_model = load_model()\n\
          ....................................................................................................................\n\
          \nrequirements:-\n....................................................................................\n\
          pip install -U llama-cpp-python\npip install -U transformers\npip install\
          \ -U accelerate\npip install -U bitsandbytes\npip install -U langchain\n\
          pip install -U sentencepiece\n....................................................................................\n\
          remember the original model is too huge to download in the Colab or Kaggel,\
          \ unless you are a pro user. I did it on my server which has enough disk\
          \ space to download the model.\n\ncolab link:- https://colab.research.google.com/drive/1oHRk8dHYhGc9z6Olrx4pmFymf-LwhF_F?usp=sharing"
        updatedAt: '2023-12-26T05:09:33.920Z'
      numEdits: 1
      reactions: []
    id: 658a5fa59835f3cce208640a
    type: comment
  author: Sagar3745
  content: "@ianuvrat, Sure.\nhere is the Code, which uses the same model loading\
    \ method as I did. \n\nloading the model:-\n................................................................................................................................\n\
    import torch\nimport transformers\nfrom torch import bfloat16\nfrom langchain\
    \ import HuggingFacePipeline, PromptTemplate, LLMChain\nimport re\nmodel_path\
    \ = 'microsoft/Orca-2-13b'\nhf_key = \"hugging_api\"\nimport time\n\ndef load_model():\n\
    \    device = f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available()\
    \ else \"cpu\"\n    bnb_config = transformers.BitsAndBytesConfig(\n        load_in_4bit=True,\n\
    \        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n\
    \        bnb_4bit_compute_dtype=bfloat16,\n    )\n    model_config = transformers.AutoConfig.from_pretrained(\n\
    \        model_path, use_auth_token=hf_key\n    )\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \        model_path,\n        trust_remote_code=True,\n        config=model_config,\n\
    \        quantization_config=bnb_config,\n        device_map=\"auto\",\n     \
    \   use_auth_token=hf_key,\n    )\n    model.eval()\n    print(f\"Model loaded\
    \ on {device}:- \")\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n\
    \        model_path, use_auth_token=hf_key\n    )\n    generate_text = transformers.pipeline(\n\
    \        model=model,\n        tokenizer=tokenizer,\n        return_full_text=True,\n\
    \        task=\"text-generation\",\n        temperature=0.0,\n        max_new_tokens=400,\n\
    \        repetition_penalty=1.1,\n        do_sample=False,\n        top_k=5,\n\
    \        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n\
    \    )\n    return HuggingFacePipeline(\n        pipeline=generate_text, model_kwargs={\"\
    temperature\": 0}\n    )\n\n\nllm_model = load_model()\n....................................................................................................................\n\
    \nrequirements:-\n....................................................................................\n\
    pip install -U llama-cpp-python\npip install -U transformers\npip install -U accelerate\n\
    pip install -U bitsandbytes\npip install -U langchain\npip install -U sentencepiece\n\
    ....................................................................................\n\
    remember the original model is too huge to download in the Colab or Kaggel, unless\
    \ you are a pro user. I did it on my server which has enough disk space to download\
    \ the model.\n\ncolab link:- https://colab.research.google.com/drive/1oHRk8dHYhGc9z6Olrx4pmFymf-LwhF_F?usp=sharing"
  created_at: 2023-12-26 05:07:49+00:00
  edited: true
  hidden: false
  id: 658a5fa59835f3cce208640a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-12-26T05:09:56.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445669054985046
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: '<p>Thanks mate. Will try with this!</p>

          '
        raw: Thanks mate. Will try with this!
        updatedAt: '2023-12-26T05:09:56.039Z'
      numEdits: 0
      reactions: []
    id: 658a6024367c76b8ee4e2767
    type: comment
  author: ianuvrat
  content: Thanks mate. Will try with this!
  created_at: 2023-12-26 05:09:56+00:00
  edited: false
  hidden: false
  id: 658a6024367c76b8ee4e2767
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Issue with GPU Utilization in Colab Notebook
