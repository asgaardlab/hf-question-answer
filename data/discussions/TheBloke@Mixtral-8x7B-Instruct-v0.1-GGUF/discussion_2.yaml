!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jrell
conflicting_files: null
created_at: 2023-12-11 20:55:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/270f5aba4c3f89a4f9335ef8f475d16f.svg
      fullname: Vadim Karpenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jrell
      type: user
    createdAt: '2023-12-11T20:55:40.000Z'
    data:
      edited: false
      editors:
      - jrell
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7238180041313171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/270f5aba4c3f89a4f9335ef8f475d16f.svg
          fullname: Vadim Karpenko
          isHf: false
          isPro: false
          name: jrell
          type: user
        html: '<p>Has anyone tried to run it with LM Studio? It gives me an error:<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/630cec753dc31beba6ee060b/fGU0LKdhBKfmSTOCWHLiO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/630cec753dc31beba6ee060b/fGU0LKdhBKfmSTOCWHLiO.png"></a></p>

          '
        raw: "Has anyone tried to run it with LM Studio? It gives me an error:\r\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630cec753dc31beba6ee060b/fGU0LKdhBKfmSTOCWHLiO.png)\r\
          \n"
        updatedAt: '2023-12-11T20:55:40.273Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - Charlie-B
        - mfalcon
        - Francis89074
        - georgmuntingh
        - AntorK
        - kalidasu1984
    id: 6577774c8369b885d40789d9
    type: comment
  author: jrell
  content: "Has anyone tried to run it with LM Studio? It gives me an error:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/630cec753dc31beba6ee060b/fGU0LKdhBKfmSTOCWHLiO.png)\r\
    \n"
  created_at: 2023-12-11 20:55:40+00:00
  edited: false
  hidden: false
  id: 6577774c8369b885d40789d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87131a908cd8336ff931150dfe6b6194.svg
      fullname: Stefano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Charlie-B
      type: user
    createdAt: '2023-12-11T21:01:18.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/87131a908cd8336ff931150dfe6b6194.svg
          fullname: Stefano
          isHf: false
          isPro: false
          name: Charlie-B
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-12T22:41:58.820Z'
      numEdits: 0
      reactions: []
    id: 6577789edddc2360b0f08fd2
    type: comment
  author: Charlie-B
  content: This comment has been hidden
  created_at: 2023-12-11 21:01:18+00:00
  edited: true
  hidden: true
  id: 6577789edddc2360b0f08fd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b593eb9add1b0542b41addc725c2ef05.svg
      fullname: Trevor Hiley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dsmgeek
      type: user
    createdAt: '2023-12-11T21:44:01.000Z'
    data:
      edited: false
      editors:
      - dsmgeek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7932260036468506
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b593eb9add1b0542b41addc725c2ef05.svg
          fullname: Trevor Hiley
          isHf: false
          isPro: false
          name: dsmgeek
          type: user
        html: '<p>Same here, Windows with 4080.</p>

          '
        raw: Same here, Windows with 4080.
        updatedAt: '2023-12-11T21:44:01.651Z'
      numEdits: 0
      reactions: []
    id: 657782a17685e1ce5cfa2400
    type: comment
  author: dsmgeek
  content: Same here, Windows with 4080.
  created_at: 2023-12-11 21:44:01+00:00
  edited: false
  hidden: false
  id: 657782a17685e1ce5cfa2400
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bde2ea1f1a570ddef6d26b43638c7dc8.svg
      fullname: Mathew Deyo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: UltraCoInc
      type: user
    createdAt: '2023-12-11T21:57:29.000Z'
    data:
      edited: true
      editors:
      - UltraCoInc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5819945931434631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bde2ea1f1a570ddef6d26b43638c7dc8.svg
          fullname: Mathew Deyo
          isHf: false
          isPro: false
          name: UltraCoInc
          type: user
        html: '<p>May be related, I''m getting an error with llama.cpp. I''m using
          <code>mixtral-8x7b-instruct-v0.1.Q6_K.gguf</code>:</p>

          <p>llama.cpp</p>

          <pre><code>error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight''
          not found

          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''./models/mixtral-8x7b-instruct-v0.1.Q6_K.gguf''

          </code></pre>

          '
        raw: 'May be related, I''m getting an error with llama.cpp. I''m using `mixtral-8x7b-instruct-v0.1.Q6_K.gguf`:


          llama.cpp

          ```

          error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight'' not
          found

          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''./models/mixtral-8x7b-instruct-v0.1.Q6_K.gguf''

          ```'
        updatedAt: '2023-12-11T22:01:55.182Z'
      numEdits: 1
      reactions: []
    id: 657785c9eb02736add4d9c4f
    type: comment
  author: UltraCoInc
  content: 'May be related, I''m getting an error with llama.cpp. I''m using `mixtral-8x7b-instruct-v0.1.Q6_K.gguf`:


    llama.cpp

    ```

    error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight'' not found

    llama_load_model_from_file: failed to load model

    llama_init_from_gpt_params: error: failed to load model ''./models/mixtral-8x7b-instruct-v0.1.Q6_K.gguf''

    ```'
  created_at: 2023-12-11 21:57:29+00:00
  edited: true
  hidden: false
  id: 657785c9eb02736add4d9c4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
      fullname: "DAN\u2122"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dranger003
      type: user
    createdAt: '2023-12-11T22:05:38.000Z'
    data:
      edited: false
      editors:
      - dranger003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9517271518707275
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
          fullname: "DAN\u2122"
          isHf: false
          isPro: false
          name: dranger003
          type: user
        html: '<p>You currently need a special build of llama.cpp (branch mixtral)
          as the support has not been merged into master yet.</p>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4406">https://github.com/ggerganov/llama.cpp/pull/4406</a></p>

          '
        raw: 'You currently need a special build of llama.cpp (branch mixtral) as
          the support has not been merged into master yet.


          https://github.com/ggerganov/llama.cpp/pull/4406'
        updatedAt: '2023-12-11T22:05:38.287Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - UltraCoInc
        - georgmuntingh
        - stuzenz
    id: 657787b2f7d98f6f61f27690
    type: comment
  author: dranger003
  content: 'You currently need a special build of llama.cpp (branch mixtral) as the
    support has not been merged into master yet.


    https://github.com/ggerganov/llama.cpp/pull/4406'
  created_at: 2023-12-11 22:05:38+00:00
  edited: false
  hidden: false
  id: 657787b2f7d98f6f61f27690
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
      fullname: MC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mclassHF2023
      type: user
    createdAt: '2023-12-11T23:03:30.000Z'
    data:
      edited: false
      editors:
      - mclassHF2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24011841416358948
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
          fullname: MC
          isHf: false
          isPro: false
          name: mclassHF2023
          type: user
        html: "<p>I'm getting an error in Text Generation WebUI:</p>\n<pre><code class=\"\
          language-File\">\n\nshared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)\n\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
          modules\\models.py\", line 88, in load_model\n\n\noutput = load_func_map[loader](model_name)\n\
          \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\modules\\models.py\", line 253, in llamacpp_loader\n\
          \n\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\n   \
          \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\\
          dev\\llamaindex_text_generation_webui\\text-generation-webui\\modules\\\
          llamacpp_model.py\", line 91, in from_pretrained\n\n\nresult.model = Llama(**params)\n\
          \n               ^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 923, in init\n\n\nself._n_vocab = self.n_vocab()\n\n  \
          \              ^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 2184, in n_vocab\n\n\nreturn self._model.n_vocab()\n\n\
          \       ^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 250, in n_vocab\n\n\nassert self.model is not None\n\n\
          \       ^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n</code></pre>\n<p>I'll see\
          \ if I can try something with llama.cpp as mentioned above</p>\n"
        raw: "I'm getting an error in Text Generation WebUI:\n\n```File \"C:\\dev\\\
          llamaindex_text_generation_webui\\text-generation-webui\\modules\\ui_model_menu.py\"\
          , line 209, in load_model_wrapper\n\n\nshared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\n\n                            \
          \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
          text-generation-webui\\modules\\models.py\", line 88, in load_model\n\n\n\
          output = load_func_map[loader](model_name)\n\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
          modules\\models.py\", line 253, in llamacpp_loader\n\n\nmodel, tokenizer\
          \ = LlamaCppModel.from_pretrained(model_file)\n\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 91, in from_pretrained\n\n\nresult.model\
          \ = Llama(**params)\n\n               ^^^^^^^^^^^^^^^\nFile \"C:\\dev\\\
          llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 923, in init\n\
          \n\nself._n_vocab = self.n_vocab()\n\n                ^^^^^^^^^^^^^^\nFile\
          \ \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 2184, in n_vocab\n\
          \n\nreturn self._model.n_vocab()\n\n       ^^^^^^^^^^^^^^^^^^^^^\nFile \"\
          C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 250, in n_vocab\n\
          \n\nassert self.model is not None\n\n       ^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\
          ```\n\nI'll see if I can try something with llama.cpp as mentioned above"
        updatedAt: '2023-12-11T23:03:30.460Z'
      numEdits: 0
      reactions: []
    id: 65779542b7ad663d075121d7
    type: comment
  author: mclassHF2023
  content: "I'm getting an error in Text Generation WebUI:\n\n```File \"C:\\dev\\\
    llamaindex_text_generation_webui\\text-generation-webui\\modules\\ui_model_menu.py\"\
    , line 209, in load_model_wrapper\n\n\nshared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\n\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\modules\\\
    models.py\", line 88, in load_model\n\n\noutput = load_func_map[loader](model_name)\n\
    \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
    text-generation-webui\\modules\\models.py\", line 253, in llamacpp_loader\n\n\n\
    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\n             \
    \      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
    text-generation-webui\\modules\\llamacpp_model.py\", line 91, in from_pretrained\n\
    \n\nresult.model = Llama(**params)\n\n               ^^^^^^^^^^^^^^^\nFile \"\
    C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\installer_files\\\
    env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 923, in init\n\n\nself._n_vocab\
    \ = self.n_vocab()\n\n                ^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\\
    text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
    llama.py\", line 2184, in n_vocab\n\n\nreturn self._model.n_vocab()\n\n      \
    \ ^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\dev\\llamaindex_text_generation_webui\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 250,\
    \ in n_vocab\n\n\nassert self.model is not None\n\n       ^^^^^^^^^^^^^^^^^^^^^^\n\
    AssertionError\n```\n\nI'll see if I can try something with llama.cpp as mentioned\
    \ above"
  created_at: 2023-12-11 23:03:30+00:00
  edited: false
  hidden: false
  id: 65779542b7ad663d075121d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
      fullname: MC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mclassHF2023
      type: user
    createdAt: '2023-12-12T00:08:37.000Z'
    data:
      edited: false
      editors:
      - mclassHF2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9126636981964111
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
          fullname: MC
          isHf: false
          isPro: false
          name: mclassHF2023
          type: user
        html: '<p>I built that specific "mixtral" branch (<a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/tree/mixtral">https://github.com/ggerganov/llama.cpp/tree/mixtral</a>),
          but I''ve never used any llamacpp that I built myself with TextGenWebUi,
          are there any steps or docs?</p>

          '
        raw: I built that specific "mixtral" branch (https://github.com/ggerganov/llama.cpp/tree/mixtral),
          but I've never used any llamacpp that I built myself with TextGenWebUi,
          are there any steps or docs?
        updatedAt: '2023-12-12T00:08:37.947Z'
      numEdits: 0
      reactions: []
    id: 6577a485197a6182c32b24a6
    type: comment
  author: mclassHF2023
  content: I built that specific "mixtral" branch (https://github.com/ggerganov/llama.cpp/tree/mixtral),
    but I've never used any llamacpp that I built myself with TextGenWebUi, are there
    any steps or docs?
  created_at: 2023-12-12 00:08:37+00:00
  edited: false
  hidden: false
  id: 6577a485197a6182c32b24a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1edfc30393225a14cb47b6de083e6f10.svg
      fullname: Maksym
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: makromaksym
      type: user
    createdAt: '2023-12-12T04:35:47.000Z'
    data:
      edited: false
      editors:
      - makromaksym
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6384335160255432
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1edfc30393225a14cb47b6de083e6f10.svg
          fullname: Maksym
          isHf: false
          isPro: false
          name: makromaksym
          type: user
        html: '<p>#First go to your textwebui directory<br>cd yourdirectoryhere</p>

          <p>#Activate the conda env<br>cp ./start_linux.sh activate_conda.sh # also
          remove the last line of activate_conda.sh<br>chmod +x activate_conda.sh<br>./activate_conda.sh</p>

          <p>#Go to the repository setting and clone llama-cpp-python<br>cd repositories/<br>git
          clone <a rel="nofollow" href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a><br>cd
          llama-cpp-python</p>

          <p>#Delete the normal usual lama.cpp and get the new one<br>cd vendor<br>rm
          -R rm -R llama.cpp/<br>git clone <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp.git">https://github.com/ggerganov/llama.cpp.git</a><br>cd
          llama.cpp<br>git checkout mixtral</p>

          <p>#Build the good stuff, check here for more details: <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a><br>cd
          ../../<br>#I assume you use cuda here, check the link otherwise<br>make
          LLAMA_OPENBLAS=1</p>

          <p>#Uninstall the old lama.cpp<br>pip list # look for lama.cpp or something
          similar<br>pip uninstall lama.cpporsomethingsimilar</p>

          <p>#Install the new one<br>pip install .</p>

          <p>And you should be good ! Not near my computer so I''m writing this from
          memory. Don''t hesitate to correct this and I will edit.</p>

          '
        raw: "#First go to your textwebui directory\ncd yourdirectoryhere\n\n#Activate\
          \ the conda env\ncp ./start_linux.sh activate_conda.sh # also remove the\
          \ last line of activate_conda.sh\nchmod +x activate_conda.sh\n./activate_conda.sh\n\
          \n#Go to the repository setting and clone llama-cpp-python\ncd repositories/\n\
          git clone https://github.com/abetlen/llama-cpp-python\ncd llama-cpp-python\n\
          \n#Delete the normal usual lama.cpp and get the new one\ncd vendor\nrm -R\
          \ rm -R llama.cpp/\ngit clone https://github.com/ggerganov/llama.cpp.git\n\
          cd llama.cpp \ngit checkout mixtral\n\n#Build the good stuff, check here\
          \ for more details: https://github.com/ggerganov/llama.cpp\ncd ../../\n\
          #I assume you use cuda here, check the link otherwise\nmake LLAMA_OPENBLAS=1\n\
          \n#Uninstall the old lama.cpp\npip list # look for lama.cpp or something\
          \ similar\npip uninstall lama.cpporsomethingsimilar\n\n#Install the new\
          \ one\npip install .\n\nAnd you should be good ! Not near my computer so\
          \ I'm writing this from memory. Don't hesitate to correct this and I will\
          \ edit."
        updatedAt: '2023-12-12T04:35:47.402Z'
      numEdits: 0
      reactions: []
    id: 6577e3238628ec00e9dd20fb
    type: comment
  author: makromaksym
  content: "#First go to your textwebui directory\ncd yourdirectoryhere\n\n#Activate\
    \ the conda env\ncp ./start_linux.sh activate_conda.sh # also remove the last\
    \ line of activate_conda.sh\nchmod +x activate_conda.sh\n./activate_conda.sh\n\
    \n#Go to the repository setting and clone llama-cpp-python\ncd repositories/\n\
    git clone https://github.com/abetlen/llama-cpp-python\ncd llama-cpp-python\n\n\
    #Delete the normal usual lama.cpp and get the new one\ncd vendor\nrm -R rm -R\
    \ llama.cpp/\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\
    \ \ngit checkout mixtral\n\n#Build the good stuff, check here for more details:\
    \ https://github.com/ggerganov/llama.cpp\ncd ../../\n#I assume you use cuda here,\
    \ check the link otherwise\nmake LLAMA_OPENBLAS=1\n\n#Uninstall the old lama.cpp\n\
    pip list # look for lama.cpp or something similar\npip uninstall lama.cpporsomethingsimilar\n\
    \n#Install the new one\npip install .\n\nAnd you should be good ! Not near my\
    \ computer so I'm writing this from memory. Don't hesitate to correct this and\
    \ I will edit."
  created_at: 2023-12-12 04:35:47+00:00
  edited: false
  hidden: false
  id: 6577e3238628ec00e9dd20fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b593eb9add1b0542b41addc725c2ef05.svg
      fullname: Trevor Hiley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dsmgeek
      type: user
    createdAt: '2023-12-12T11:30:46.000Z'
    data:
      edited: false
      editors:
      - dsmgeek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581708908081055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b593eb9add1b0542b41addc725c2ef05.svg
          fullname: Trevor Hiley
          isHf: false
          isPro: false
          name: dsmgeek
          type: user
        html: '<p>Downloaded and installed the new LM Studio this morning which has
          experimental support for this model. LM Studio no longer gives me that error
          immediately, but I still get a "failed to load model" error after it tries
          to load for some time.</p>

          '
        raw: Downloaded and installed the new LM Studio this morning which has experimental
          support for this model. LM Studio no longer gives me that error immediately,
          but I still get a "failed to load model" error after it tries to load for
          some time.
        updatedAt: '2023-12-12T11:30:46.613Z'
      numEdits: 0
      reactions: []
    id: 657844660766f274f3cff531
    type: comment
  author: dsmgeek
  content: Downloaded and installed the new LM Studio this morning which has experimental
    support for this model. LM Studio no longer gives me that error immediately, but
    I still get a "failed to load model" error after it tries to load for some time.
  created_at: 2023-12-12 11:30:46+00:00
  edited: false
  hidden: false
  id: 657844660766f274f3cff531
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bd2f204eb00597d1b1b3174a8c31055.svg
      fullname: Zeb Palmer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zebpalmer
      type: user
    createdAt: '2023-12-14T03:37:05.000Z'
    data:
      edited: false
      editors:
      - zebpalmer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9734213948249817
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bd2f204eb00597d1b1b3174a8c31055.svg
          fullname: Zeb Palmer
          isHf: false
          isPro: false
          name: zebpalmer
          type: user
        html: '<p>working for me with LMStudio v0.2.9.  So far, I''m really impressed
          with this one. </p>

          '
        raw: 'working for me with LMStudio v0.2.9.  So far, I''m really impressed
          with this one. '
        updatedAt: '2023-12-14T03:37:05.099Z'
      numEdits: 0
      reactions: []
    id: 657a78617115d9f264920223
    type: comment
  author: zebpalmer
  content: 'working for me with LMStudio v0.2.9.  So far, I''m really impressed with
    this one. '
  created_at: 2023-12-14 03:37:05+00:00
  edited: false
  hidden: false
  id: 657a78617115d9f264920223
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/270f5aba4c3f89a4f9335ef8f475d16f.svg
      fullname: Vadim Karpenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jrell
      type: user
    createdAt: '2023-12-14T03:57:18.000Z'
    data:
      status: closed
    id: 657a7d1ebdfb77e5c7e7db62
    type: status-change
  author: jrell
  created_at: 2023-12-14 03:57:18+00:00
  id: 657a7d1ebdfb77e5c7e7db62
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: closed
target_branch: null
title: LM Studio crash
