!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Mikael110
conflicting_files: null
created_at: 2023-12-14 21:10:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-12-14T21:10:14.000Z'
    data:
      edited: true
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761251211166382
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> just wanted to\
          \ bring your attention to <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885\"\
          >this</a> GitHub comment in case you have not seen in yet. It strongly suggest\
          \ that there is something seriously wrong with the Q6_K quant as it has\
          \ far higher perplexity than even Q4_0 when using 2 experts.</p>\n<p>The\
          \ test does not include Q4_K_M or Q5_K_M, I think it would be a good idea\
          \ to run a perplexity test on those two as well to make sure this issue\
          \ does not affects all of the K variants. It's also pretty odd that the\
          \ K variants have the exact same file size as the Non-K variants, I don't\
          \ know if that is in any way connected but it's certainly an oddity and\
          \ it's present in all models based on Mixtral.</p>\n"
        raw: '@TheBloke just wanted to bring your attention to [this](https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885)
          GitHub comment in case you have not seen in yet. It strongly suggest that
          there is something seriously wrong with the Q6_K quant as it has far higher
          perplexity than even Q4_0 when using 2 experts.


          The test does not include Q4_K_M or Q5_K_M, I think it would be a good idea
          to run a perplexity test on those two as well to make sure this issue does
          not affects all of the K variants. It''s also pretty odd that the K variants
          have the exact same file size as the Non-K variants, I don''t know if that
          is in any way connected but it''s certainly an oddity and it''s present
          in all models based on Mixtral.'
        updatedAt: '2023-12-14T21:11:57.305Z'
      numEdits: 1
      reactions: []
    id: 657b6f36be51c126fd452f7a
    type: comment
  author: Mikael110
  content: '@TheBloke just wanted to bring your attention to [this](https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885)
    GitHub comment in case you have not seen in yet. It strongly suggest that there
    is something seriously wrong with the Q6_K quant as it has far higher perplexity
    than even Q4_0 when using 2 experts.


    The test does not include Q4_K_M or Q5_K_M, I think it would be a good idea to
    run a perplexity test on those two as well to make sure this issue does not affects
    all of the K variants. It''s also pretty odd that the K variants have the exact
    same file size as the Non-K variants, I don''t know if that is in any way connected
    but it''s certainly an oddity and it''s present in all models based on Mixtral.'
  created_at: 2023-12-14 21:10:14+00:00
  edited: true
  hidden: false
  id: 657b6f36be51c126fd452f7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
      fullname: Alexander Ljungberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sleven
      type: user
    createdAt: '2023-12-15T09:20:45.000Z'
    data:
      edited: false
      editors:
      - sleven
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8945356607437134
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
          fullname: Alexander Ljungberg
          isHf: false
          isPro: false
          name: sleven
          type: user
        html: '<p>Check the frequency base. For me, on Q8_K, loaded using oogabooga,
          I get <code>llama_new_context_with_model: freq_base  = 10000.0</code> in
          the console log by default, which is AFAIK not right. And indeed the model
          falls apart with increasing context length. If I override it with <code>rope_freq_base</code>
          set to 1000000, the model works much better. I didn''t try measuring the
          perplexity though.</p>

          '
        raw: 'Check the frequency base. For me, on Q8_K, loaded using oogabooga, I
          get `llama_new_context_with_model: freq_base  = 10000.0` in the console
          log by default, which is AFAIK not right. And indeed the model falls apart
          with increasing context length. If I override it with `rope_freq_base` set
          to 1000000, the model works much better. I didn''t try measuring the perplexity
          though.'
        updatedAt: '2023-12-15T09:20:45.264Z'
      numEdits: 0
      reactions: []
    id: 657c1a6dbd517f1488778204
    type: comment
  author: sleven
  content: 'Check the frequency base. For me, on Q8_K, loaded using oogabooga, I get
    `llama_new_context_with_model: freq_base  = 10000.0` in the console log by default,
    which is AFAIK not right. And indeed the model falls apart with increasing context
    length. If I override it with `rope_freq_base` set to 1000000, the model works
    much better. I didn''t try measuring the perplexity though.'
  created_at: 2023-12-15 09:20:45+00:00
  edited: false
  hidden: false
  id: 657c1a6dbd517f1488778204
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cd3049107029eb004f968/G1A2zca8Yi_ii2YJtHHht.png?w=200&h=200&f=face
      fullname: "Rickard Ed\xE9n"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neph1
      type: user
    createdAt: '2023-12-15T11:47:00.000Z'
    data:
      edited: false
      editors:
      - neph1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9565939903259277
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cd3049107029eb004f968/G1A2zca8Yi_ii2YJtHHht.png?w=200&h=200&f=face
          fullname: "Rickard Ed\xE9n"
          isHf: false
          isPro: false
          name: neph1
          type: user
        html: '<blockquote>

          <p>ust wanted to bring your attention to <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885">this</a>
          GitHub comment in case you have not seen in yet. It strongly suggest that
          there is something seriously wrong with the Q6_K quant as it has far higher
          perplexity than even Q4_0 when using 2 experts.</p>

          </blockquote>

          <p>I didn''t follow that discussion from the start, but just read it and
          saw this:</p>

          <p>"Re-tested Q6_K with the latest build, and the results are looking good
          now. Not sure about the source of the previous issue."</p>

          '
        raw: "> ust wanted to bring your attention to [this](https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885)\
          \ GitHub comment in case you have not seen in yet. It strongly suggest that\
          \ there is something seriously wrong with the Q6_K quant as it has far higher\
          \ perplexity than even Q4_0 when using 2 experts.\n> \n\nI didn't follow\
          \ that discussion from the start, but just read it and saw this:\n\n\"Re-tested\
          \ Q6_K with the latest build, and the results are looking good now. Not\
          \ sure about the source of the previous issue.\"\n"
        updatedAt: '2023-12-15T11:47:00.840Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Mikael110
    id: 657c3cb4ff16eeb2ee4e668f
    type: comment
  author: neph1
  content: "> ust wanted to bring your attention to [this](https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885)\
    \ GitHub comment in case you have not seen in yet. It strongly suggest that there\
    \ is something seriously wrong with the Q6_K quant as it has far higher perplexity\
    \ than even Q4_0 when using 2 experts.\n> \n\nI didn't follow that discussion\
    \ from the start, but just read it and saw this:\n\n\"Re-tested Q6_K with the\
    \ latest build, and the results are looking good now. Not sure about the source\
    \ of the previous issue.\"\n"
  created_at: 2023-12-15 11:47:00+00:00
  edited: false
  hidden: false
  id: 657c3cb4ff16eeb2ee4e668f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-12-15T13:06:13.000Z'
    data:
      edited: false
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9798339605331421
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: '<blockquote>

          <blockquote>

          <p>ust wanted to bring your attention to <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885">this</a>
          GitHub comment in case you have not seen in yet. It strongly suggest that
          there is something seriously wrong with the Q6_K quant as it has far higher
          perplexity than even Q4_0 when using 2 experts.</p>

          </blockquote>

          <p>I didn''t follow that discussion from the start, but just read it and
          saw this:</p>

          <p>"Re-tested Q6_K with the latest build, and the results are looking good
          now. Not sure about the source of the previous issue."</p>

          </blockquote>

          <p>Ah that''s good to see. In the original test the Q6_K had a perplexity
          of 4.86 which was way above what it should have been. The new result of
          3.90 is good to see, it seems to have been a false alarm in that case.<br>It
          would have been pretty bad if all of the K Quants had been underperforming
          this whole time, as that''s the main type people use these days.<br>Given
          those results I think it''s safe to close this issue.</p>

          '
        raw: "> > ust wanted to bring your attention to [this](https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885)\
          \ GitHub comment in case you have not seen in yet. It strongly suggest that\
          \ there is something seriously wrong with the Q6_K quant as it has far higher\
          \ perplexity than even Q4_0 when using 2 experts.\n> > \n> \n> I didn't\
          \ follow that discussion from the start, but just read it and saw this:\n\
          > \n> \"Re-tested Q6_K with the latest build, and the results are looking\
          \ good now. Not sure about the source of the previous issue.\"\n\nAh that's\
          \ good to see. In the original test the Q6_K had a perplexity of 4.86 which\
          \ was way above what it should have been. The new result of 3.90 is good\
          \ to see, it seems to have been a false alarm in that case. \nIt would have\
          \ been pretty bad if all of the K Quants had been underperforming this whole\
          \ time, as that's the main type people use these days.\nGiven those results\
          \ I think it's safe to close this issue."
        updatedAt: '2023-12-15T13:06:13.062Z'
      numEdits: 0
      reactions: []
      relatedEventId: 657c4f45e62c244a5b53cfbe
    id: 657c4f45e62c244a5b53cfbc
    type: comment
  author: Mikael110
  content: "> > ust wanted to bring your attention to [this](https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885)\
    \ GitHub comment in case you have not seen in yet. It strongly suggest that there\
    \ is something seriously wrong with the Q6_K quant as it has far higher perplexity\
    \ than even Q4_0 when using 2 experts.\n> > \n> \n> I didn't follow that discussion\
    \ from the start, but just read it and saw this:\n> \n> \"Re-tested Q6_K with\
    \ the latest build, and the results are looking good now. Not sure about the source\
    \ of the previous issue.\"\n\nAh that's good to see. In the original test the\
    \ Q6_K had a perplexity of 4.86 which was way above what it should have been.\
    \ The new result of 3.90 is good to see, it seems to have been a false alarm in\
    \ that case. \nIt would have been pretty bad if all of the K Quants had been underperforming\
    \ this whole time, as that's the main type people use these days.\nGiven those\
    \ results I think it's safe to close this issue."
  created_at: 2023-12-15 13:06:13+00:00
  edited: false
  hidden: false
  id: 657c4f45e62c244a5b53cfbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-12-15T13:06:13.000Z'
    data:
      status: closed
    id: 657c4f45e62c244a5b53cfbe
    type: status-change
  author: Mikael110
  created_at: 2023-12-15 13:06:13+00:00
  id: 657c4f45e62c244a5b53cfbe
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: closed
target_branch: null
title: Q6_K appears to be broken (And maybe other K-Quants as well)
