!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vidyamantra
conflicting_files: null
created_at: 2023-12-15 12:34:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
      fullname: Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vidyamantra
      type: user
    createdAt: '2023-12-15T12:34:57.000Z'
    data:
      edited: false
      editors:
      - vidyamantra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9593667984008789
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
          fullname: Gupta
          isHf: false
          isPro: false
          name: vidyamantra
          type: user
        html: '<p>Since both 8x7B (Q3) vs 7B would fit in GPU RAM of 24G. What would
          be more accurate? What is easy way to test?</p>

          <p>Performance wise 8x7B (Q3) is 83 t/s and 7B is 129 t/s on RTX 4090. As
          soon as we switch to 8x7B (Q4) it exceeds 24G GPU RAM and hence drop to
          27 t/s.</p>

          '
        raw: "Since both 8x7B (Q3) vs 7B would fit in GPU RAM of 24G. What would be\
          \ more accurate? What is easy way to test?\r\n\r\nPerformance wise 8x7B\
          \ (Q3) is 83 t/s and 7B is 129 t/s on RTX 4090. As soon as we switch to\
          \ 8x7B (Q4) it exceeds 24G GPU RAM and hence drop to 27 t/s."
        updatedAt: '2023-12-15T12:34:57.540Z'
      numEdits: 0
      reactions: []
    id: 657c47f18d360b690d6ab7d8
    type: comment
  author: vidyamantra
  content: "Since both 8x7B (Q3) vs 7B would fit in GPU RAM of 24G. What would be\
    \ more accurate? What is easy way to test?\r\n\r\nPerformance wise 8x7B (Q3) is\
    \ 83 t/s and 7B is 129 t/s on RTX 4090. As soon as we switch to 8x7B (Q4) it exceeds\
    \ 24G GPU RAM and hence drop to 27 t/s."
  created_at: 2023-12-15 12:34:57+00:00
  edited: false
  hidden: false
  id: 657c47f18d360b690d6ab7d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-15T12:52:41.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.862592875957489
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vidyamantra&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vidyamantra\"\
          >@<span class=\"underline\">vidyamantra</span></a></span>\n\n\t</span></span>\
          \ a bigger quantized model is always better then a smaller unquantized model.<br>so\
          \ use the 8x7b q3 if you want better quality</p>\n"
        raw: '@vidyamantra a bigger quantized model is always better then a smaller
          unquantized model.

          so use the 8x7b q3 if you want better quality'
        updatedAt: '2023-12-15T12:52:41.907Z'
      numEdits: 0
      reactions: []
    id: 657c4c19ddc32bef8bce4e1b
    type: comment
  author: YaTharThShaRma999
  content: '@vidyamantra a bigger quantized model is always better then a smaller
    unquantized model.

    so use the 8x7b q3 if you want better quality'
  created_at: 2023-12-15 12:52:41+00:00
  edited: false
  hidden: false
  id: 657c4c19ddc32bef8bce4e1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6320741021bbe5fca0f605d1/rC9eSkdvVCi_ckNBayJWo.jpeg?w=200&h=200&f=face
      fullname: Shroominic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shroominic
      type: user
    createdAt: '2023-12-16T07:17:59.000Z'
    data:
      edited: false
      editors:
      - shroominic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9624988436698914
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6320741021bbe5fca0f605d1/rC9eSkdvVCi_ckNBayJWo.jpeg?w=200&h=200&f=face
          fullname: Shroominic
          isHf: false
          isPro: false
          name: shroominic
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\"\
          >@<span class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \ I don't think this is always true, we should do benchmarks!</p>\n"
        raw: '@YaTharThShaRma999 I don''t think this is always true, we should do
          benchmarks!'
        updatedAt: '2023-12-16T07:17:59.976Z'
      numEdits: 0
      reactions: []
    id: 657d4f27504da7f6f3a07e03
    type: comment
  author: shroominic
  content: '@YaTharThShaRma999 I don''t think this is always true, we should do benchmarks!'
  created_at: 2023-12-16 07:17:59+00:00
  edited: false
  hidden: false
  id: 657d4f27504da7f6f3a07e03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-16T09:42:27.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7821534872055054
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;shroominic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/shroominic\">@<span class=\"\
          underline\">shroominic</span></a></span>\n\n\t</span></span> Here is the\
          \ bench mark useful to you <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md\"\
          >https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md</a></p>\n\
          <p>If you want to run locally, you can build llamacpp and run ppl score\
          \ on gguf models</p>\n"
        raw: '@shroominic Here is the bench mark useful to you https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md


          If you want to run locally, you can build llamacpp and run ppl score on
          gguf models'
        updatedAt: '2023-12-16T09:42:27.201Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vidyamantra
    id: 657d7103f5eacd4bdacdfb08
    type: comment
  author: Yhyu13
  content: '@shroominic Here is the bench mark useful to you https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md


    If you want to run locally, you can build llamacpp and run ppl score on gguf models'
  created_at: 2023-12-16 09:42:27+00:00
  edited: false
  hidden: false
  id: 657d7103f5eacd4bdacdfb08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a8e9ea321ab87aea71ad3c0df5b12c01.svg
      fullname: paretooptimal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paretooptimaldev
      type: user
    createdAt: '2023-12-23T22:00:33.000Z'
    data:
      edited: true
      editors:
      - paretooptimaldev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9094604849815369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a8e9ea321ab87aea71ad3c0df5b12c01.svg
          fullname: paretooptimal
          isHf: false
          isPro: false
          name: paretooptimaldev
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;vidyamantra&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vidyamantra\"\
          >@<span class=\"underline\">vidyamantra</span></a></span>\n\n\t</span></span>\
          \ a bigger quantized model is always better then a smaller unquantized model.<br>so\
          \ use the 8x7b q3 if you want better quality</p>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\">@<span\
          \ class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \ That doesn't seem true for things like MemGPT.  Perhaps not true for RAG\
          \ in general?</p>\n"
        raw: '> @vidyamantra a bigger quantized model is always better then a smaller
          unquantized model.

          > so use the 8x7b q3 if you want better quality


          @YaTharThShaRma999 That doesn''t seem true for things like MemGPT.  Perhaps
          not true for RAG in general?'
        updatedAt: '2023-12-23T22:00:52.820Z'
      numEdits: 1
      reactions: []
    id: 65875881509bcae23f592025
    type: comment
  author: paretooptimaldev
  content: '> @vidyamantra a bigger quantized model is always better then a smaller
    unquantized model.

    > so use the 8x7b q3 if you want better quality


    @YaTharThShaRma999 That doesn''t seem true for things like MemGPT.  Perhaps not
    true for RAG in general?'
  created_at: 2023-12-23 22:00:33+00:00
  edited: true
  hidden: false
  id: 65875881509bcae23f592025
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
      fullname: Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vidyamantra
      type: user
    createdAt: '2023-12-24T03:40:44.000Z'
    data:
      edited: false
      editors:
      - vidyamantra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7942707538604736
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
          fullname: Gupta
          isHf: false
          isPro: false
          name: vidyamantra
          type: user
        html: "<p>A very simple (amateurish) test. Given a multiple choice questions,\
          \ model was asked to give suitable drilled down subject tags. Total 109\
          \ questions were given and result was compared with already computed correct\
          \ answers. Score was given based on how much accurate was drilled down tags.</p>\n\
          <div class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th>Model</th>\n\
          <th>Average Score</th>\n<th>Correct Tags</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n\
          <td>gpt-3.5-turbo</td>\n<td>35.65834862</td>\n<td>82/109</td>\n</tr>\n<tr>\n\
          <td>gpt-3.5-turbo-instruct</td>\n<td>32.0842605</td>\n<td>72/109</td>\n\
          </tr>\n<tr>\n<td>mixtral-8x7b-instruct-v0.1.Q5_K_M</td>\n<td>25.17394495</td>\n\
          <td>59/109</td>\n</tr>\n<tr>\n<td>mixtral-8x7b-instruct-v0.1.Q6_K</td>\n\
          <td>23.64691589</td>\n<td>60/109</td>\n</tr>\n<tr>\n<td>mixtral-8x7b-instruct-v0.1.Q8_K</td>\n\
          <td>23.17743119</td>\n<td>59/109</td>\n</tr>\n<tr>\n<td>mixtral-8x7b-instruct-v0.1.Q4_K_M</td>\n\
          <td>23.06449541</td>\n<td>60/109</td>\n</tr>\n<tr>\n<td>mistralai_Mistral-7B-Instruct-v0.1</td>\n\
          <td>20.99638889</td>\n<td>51/109</td>\n</tr>\n<tr>\n<td>mixtral-8x7b-instruct-v0.1.Q3_K_M</td>\n\
          <td>20.74944444</td>\n<td>49/109</td>\n</tr>\n<tr>\n<td>Mistral-7B-Instruct-v0.2</td>\n\
          <td>18.59256881</td>\n<td>59/109</td>\n</tr>\n<tr>\n<td>upstage_SOLAR-10.7B-Instruct-v1.0</td>\n\
          <td>15.88376147</td>\n<td>52/109</td>\n</tr>\n<tr>\n<td>microsoft_phi-2</td>\n\
          <td>4.715196078</td>\n<td>14/109</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n\
          <p>I will try to write a better test. Any pointers?</p>\n"
        raw: 'A very simple (amateurish) test. Given a multiple choice questions,
          model was asked to give suitable drilled down subject tags. Total 109 questions
          were given and result was compared with already computed correct answers.
          Score was given based on how much accurate was drilled down tags.

          | Model                              | Average Score | Correct Tags |

          |------------------------------------|---------------|--------------|

          | gpt-3.5-turbo                      | 35.65834862   | 82/109       |

          | gpt-3.5-turbo-instruct             | 32.0842605    | 72/109       |

          | mixtral-8x7b-instruct-v0.1.Q5_K_M  | 25.17394495   | 59/109       |

          | mixtral-8x7b-instruct-v0.1.Q6_K    | 23.64691589   | 60/109       |

          | mixtral-8x7b-instruct-v0.1.Q8_K    | 23.17743119   | 59/109       |

          | mixtral-8x7b-instruct-v0.1.Q4_K_M  | 23.06449541   | 60/109       |

          | mistralai_Mistral-7B-Instruct-v0.1 | 20.99638889   | 51/109       |

          | mixtral-8x7b-instruct-v0.1.Q3_K_M  | 20.74944444   | 49/109       |

          | Mistral-7B-Instruct-v0.2           | 18.59256881   | 59/109       |

          | upstage_SOLAR-10.7B-Instruct-v1.0  | 15.88376147   | 52/109       |

          | microsoft_phi-2                    | 4.715196078   | 14/109       |


          I will try to write a better test. Any pointers?'
        updatedAt: '2023-12-24T03:40:44.492Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
    id: 6587a83ccef9b8827bf10fb1
    type: comment
  author: vidyamantra
  content: 'A very simple (amateurish) test. Given a multiple choice questions, model
    was asked to give suitable drilled down subject tags. Total 109 questions were
    given and result was compared with already computed correct answers. Score was
    given based on how much accurate was drilled down tags.

    | Model                              | Average Score | Correct Tags |

    |------------------------------------|---------------|--------------|

    | gpt-3.5-turbo                      | 35.65834862   | 82/109       |

    | gpt-3.5-turbo-instruct             | 32.0842605    | 72/109       |

    | mixtral-8x7b-instruct-v0.1.Q5_K_M  | 25.17394495   | 59/109       |

    | mixtral-8x7b-instruct-v0.1.Q6_K    | 23.64691589   | 60/109       |

    | mixtral-8x7b-instruct-v0.1.Q8_K    | 23.17743119   | 59/109       |

    | mixtral-8x7b-instruct-v0.1.Q4_K_M  | 23.06449541   | 60/109       |

    | mistralai_Mistral-7B-Instruct-v0.1 | 20.99638889   | 51/109       |

    | mixtral-8x7b-instruct-v0.1.Q3_K_M  | 20.74944444   | 49/109       |

    | Mistral-7B-Instruct-v0.2           | 18.59256881   | 59/109       |

    | upstage_SOLAR-10.7B-Instruct-v1.0  | 15.88376147   | 52/109       |

    | microsoft_phi-2                    | 4.715196078   | 14/109       |


    I will try to write a better test. Any pointers?'
  created_at: 2023-12-24 03:40:44+00:00
  edited: false
  hidden: false
  id: 6587a83ccef9b8827bf10fb1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: 8x7B (Q3) vs 7B
