!!python/object:huggingface_hub.community.DiscussionWithDetails
author: robert1968
conflicting_files: null
created_at: 2023-12-17 22:00:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-12-17T22:00:43.000Z'
    data:
      edited: true
      editors:
      - robert1968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7719341516494751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
          fullname: Robert Szabo
          isHf: false
          isPro: false
          name: robert1968
          type: user
        html: '<p>Hi,</p>

          <p>When i try to use GPU while loading TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
          model i got this error message:</p>

          <p>CUDA error 2 at /home/runner/work/llama-cpp-python-cuBLAS-wheels/llama-cpp-python-cuBLAS-wheels/vendor/llama.cpp/ggml-cuda.cu:8955:
          out of memory<br>current device: 0<br>GGML_ASSERT: /home/runner/work/llama-cpp-python-cuBLAS-wheels/llama-cpp-python-cuBLAS-wheels/vendor/llama.cpp/ggml-cuda.cu:8955:
          !"CUDA error"<br>Could not attach to process.  If your uid matches the uid
          of the target<br>process, check the setting of /proc/sys/kernel/yama/ptrace_scope,
          or try<br>again as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf<br>ptrace:
          Operation not permitted.<br>No stack.<br>The program is not being run.<br>Aborted
          (core dumped)</p>

          <p>is this only for me (some local install problem) or for everyone (llama.cpp)
          ?</p>

          <p>(With n-gpu-layers = 0, it loads the model and works perfectly but slow
          "a bit". :)</p>

          '
        raw: 'Hi,


          When i try to use GPU while loading TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
          model i got this error message:


          CUDA error 2 at /home/runner/work/llama-cpp-python-cuBLAS-wheels/llama-cpp-python-cuBLAS-wheels/vendor/llama.cpp/ggml-cuda.cu:8955:
          out of memory

          current device: 0

          GGML_ASSERT: /home/runner/work/llama-cpp-python-cuBLAS-wheels/llama-cpp-python-cuBLAS-wheels/vendor/llama.cpp/ggml-cuda.cu:8955:
          !"CUDA error"

          Could not attach to process.  If your uid matches the uid of the target

          process, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try

          again as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf

          ptrace: Operation not permitted.

          No stack.

          The program is not being run.

          Aborted (core dumped)


          is this only for me (some local install problem) or for everyone (llama.cpp)
          ?


          (With n-gpu-layers = 0, it loads the model and works perfectly but slow
          "a bit". :)

          '
        updatedAt: '2023-12-17T22:02:30.633Z'
      numEdits: 1
      reactions: []
    id: 657f6f8bbc9bceccf9291fea
    type: comment
  author: robert1968
  content: 'Hi,


    When i try to use GPU while loading TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF model
    i got this error message:


    CUDA error 2 at /home/runner/work/llama-cpp-python-cuBLAS-wheels/llama-cpp-python-cuBLAS-wheels/vendor/llama.cpp/ggml-cuda.cu:8955:
    out of memory

    current device: 0

    GGML_ASSERT: /home/runner/work/llama-cpp-python-cuBLAS-wheels/llama-cpp-python-cuBLAS-wheels/vendor/llama.cpp/ggml-cuda.cu:8955:
    !"CUDA error"

    Could not attach to process.  If your uid matches the uid of the target

    process, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try

    again as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf

    ptrace: Operation not permitted.

    No stack.

    The program is not being run.

    Aborted (core dumped)


    is this only for me (some local install problem) or for everyone (llama.cpp) ?


    (With n-gpu-layers = 0, it loads the model and works perfectly but slow "a bit".
    :)

    '
  created_at: 2023-12-17 22:00:43+00:00
  edited: true
  hidden: false
  id: 657f6f8bbc9bceccf9291fea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-18T02:34:49.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9509263038635254
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>I have not encounter such problem with llamacpp that I built from
          source when runnig mixtral, you might want to search issues for llamacpp.
          I found his one <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/4452">https://github.com/ggerganov/llama.cpp/issues/4452</a></p>

          '
        raw: I have not encounter such problem with llamacpp that I built from source
          when runnig mixtral, you might want to search issues for llamacpp. I found
          his one https://github.com/ggerganov/llama.cpp/issues/4452
        updatedAt: '2023-12-18T02:34:49.255Z'
      numEdits: 0
      reactions: []
    id: 657fafc919ca6a5e92b5458e
    type: comment
  author: Yhyu13
  content: I have not encounter such problem with llamacpp that I built from source
    when runnig mixtral, you might want to search issues for llamacpp. I found his
    one https://github.com/ggerganov/llama.cpp/issues/4452
  created_at: 2023-12-18 02:34:49+00:00
  edited: false
  hidden: false
  id: 657fafc919ca6a5e92b5458e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-12-18T05:42:02.000Z'
    data:
      edited: false
      editors:
      - robert1968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9641939997673035
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
          fullname: Robert Szabo
          isHf: false
          isPro: false
          name: robert1968
          type: user
        html: '<p>Many thanks.<br>Do you use Oobabooga, right?<br>I tried to compile
          llamacpp when previous Oobabooga was not working out of the box with Mixtral
          but compiled lib was not used by Oobabooga. Maybe i misused Conda env ?</p>

          '
        raw: 'Many thanks.

          Do you use Oobabooga, right?

          I tried to compile llamacpp when previous Oobabooga was not working out
          of the box with Mixtral but compiled lib was not used by Oobabooga. Maybe
          i misused Conda env ?

          '
        updatedAt: '2023-12-18T05:42:02.176Z'
      numEdits: 0
      reactions: []
    id: 657fdbaa869d5bb0e5bbcbb7
    type: comment
  author: robert1968
  content: 'Many thanks.

    Do you use Oobabooga, right?

    I tried to compile llamacpp when previous Oobabooga was not working out of the
    box with Mixtral but compiled lib was not used by Oobabooga. Maybe i misused Conda
    env ?

    '
  created_at: 2023-12-18 05:42:02+00:00
  edited: false
  hidden: false
  id: 657fdbaa869d5bb0e5bbcbb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-12-18T10:16:42.000Z'
    data:
      edited: false
      editors:
      - robert1968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8110626339912415
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
          fullname: Robert Szabo
          isHf: false
          isPro: false
          name: robert1968
          type: user
        html: '<p>GPU Works ! </p>

          <p>i miss used it - number of layers must be less the GPU size.  i mean
          i have 3060 with 12GB VRAM so n-gpu-layers &lt; 12 in my case 9 is the max.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64a98d11a347b957199412ce/k2F9xzHNCKCuQ1gJB4X4K.png"><img
          alt="Screenshot from 2023-12-18 11-13-21.png" src="https://cdn-uploads.huggingface.co/production/uploads/64a98d11a347b957199412ce/k2F9xzHNCKCuQ1gJB4X4K.png"></a></p>

          <p>Earlier i set n-gpu-layers to 25 so this changed in the new version.<br>this
          is much much faster.</p>

          '
        raw: "GPU Works ! \n\ni miss used it - number of layers must be less the GPU\
          \ size.  i mean i have 3060 with 12GB VRAM so n-gpu-layers < 12 in my case\
          \ 9 is the max.\n\n![Screenshot from 2023-12-18 11-13-21.png](https://cdn-uploads.huggingface.co/production/uploads/64a98d11a347b957199412ce/k2F9xzHNCKCuQ1gJB4X4K.png)\n\
          \nEarlier i set n-gpu-layers to 25 so this changed in the new version.\n\
          this is much much faster."
        updatedAt: '2023-12-18T10:16:42.958Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Noidriy
    id: 65801c0a461391c5b7ea14a1
    type: comment
  author: robert1968
  content: "GPU Works ! \n\ni miss used it - number of layers must be less the GPU\
    \ size.  i mean i have 3060 with 12GB VRAM so n-gpu-layers < 12 in my case 9 is\
    \ the max.\n\n![Screenshot from 2023-12-18 11-13-21.png](https://cdn-uploads.huggingface.co/production/uploads/64a98d11a347b957199412ce/k2F9xzHNCKCuQ1gJB4X4K.png)\n\
    \nEarlier i set n-gpu-layers to 25 so this changed in the new version.\nthis is\
    \ much much faster."
  created_at: 2023-12-18 10:16:42+00:00
  edited: false
  hidden: false
  id: 65801c0a461391c5b7ea14a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-12-18T11:56:29.000Z'
    data:
      from: Oobabooga GPU load error
      to: Oobabooga GPU load error - Solved!
    id: 6580336dfaeaf98cb4f9ee33
    type: title-change
  author: robert1968
  created_at: 2023-12-18 11:56:29+00:00
  id: 6580336dfaeaf98cb4f9ee33
  new_title: Oobabooga GPU load error - Solved!
  old_title: Oobabooga GPU load error
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Oobabooga GPU load error - Solved!
