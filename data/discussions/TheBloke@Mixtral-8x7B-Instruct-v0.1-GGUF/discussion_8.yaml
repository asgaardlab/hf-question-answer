!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alexcardo
conflicting_files: null
created_at: 2023-12-14 12:26:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
      fullname: Alex Cardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexcardo
      type: user
    createdAt: '2023-12-14T12:26:24.000Z'
    data:
      edited: false
      editors:
      - alexcardo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9412503838539124
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
          fullname: Alex Cardo
          isHf: false
          isPro: false
          name: alexcardo
          type: user
        html: '<p>Maybe someone can suggest me how to properly instruct the model
          to follow my instructions correctly?</p>

          <p>I ask the model to write an outline for the article titled "Here is the
          title".</p>

          <p>Next, I ask the model to write a detailed article using this outline.
          No one model managed to cope with this task even this one. Every model omit
          the subheadings created by itself. Why it''s too difficult for the modern
          model to follow this simple instruction? </p>

          <p>Also, I try to ask the model to markdown the article. And this one, for
          example, do it like this ===== and --------- instead of # and ##. Even a
          minor 4K Mistral is capable of coping with this issue. </p>

          <p>I''m completely frustrated. Every model omit the subheadings in the outline
          or rewrite them. Any my suggestions to strictly follow the subheadings structure
          give me no results. </p>

          <p>I can''t believe that such a high-end model can''t do such a simple thing.</p>

          <p>I would appreciate for any suggestion.</p>

          <p>Thank you!</p>

          '
        raw: "Maybe someone can suggest me how to properly instruct the model to follow\
          \ my instructions correctly?\r\n\r\nI ask the model to write an outline\
          \ for the article titled \"Here is the title\".\r\n\r\nNext, I ask the model\
          \ to write a detailed article using this outline. No one model managed to\
          \ cope with this task even this one. Every model omit the subheadings created\
          \ by itself. Why it's too difficult for the modern model to follow this\
          \ simple instruction? \r\n\r\nAlso, I try to ask the model to markdown the\
          \ article. And this one, for example, do it like this ===== and ---------\
          \ instead of # and ##. Even a minor 4K Mistral is capable of coping with\
          \ this issue. \r\n\r\nI'm completely frustrated. Every model omit the subheadings\
          \ in the outline or rewrite them. Any my suggestions to strictly follow\
          \ the subheadings structure give me no results. \r\n\r\nI can't believe\
          \ that such a high-end model can't do such a simple thing.\r\n\r\nI would\
          \ appreciate for any suggestion.\r\n\r\nThank you!"
        updatedAt: '2023-12-14T12:26:24.604Z'
      numEdits: 0
      reactions: []
    id: 657af47013e11aaa91b315b3
    type: comment
  author: alexcardo
  content: "Maybe someone can suggest me how to properly instruct the model to follow\
    \ my instructions correctly?\r\n\r\nI ask the model to write an outline for the\
    \ article titled \"Here is the title\".\r\n\r\nNext, I ask the model to write\
    \ a detailed article using this outline. No one model managed to cope with this\
    \ task even this one. Every model omit the subheadings created by itself. Why\
    \ it's too difficult for the modern model to follow this simple instruction? \r\
    \n\r\nAlso, I try to ask the model to markdown the article. And this one, for\
    \ example, do it like this ===== and --------- instead of # and ##. Even a minor\
    \ 4K Mistral is capable of coping with this issue. \r\n\r\nI'm completely frustrated.\
    \ Every model omit the subheadings in the outline or rewrite them. Any my suggestions\
    \ to strictly follow the subheadings structure give me no results. \r\n\r\nI can't\
    \ believe that such a high-end model can't do such a simple thing.\r\n\r\nI would\
    \ appreciate for any suggestion.\r\n\r\nThank you!"
  created_at: 2023-12-14 12:26:24+00:00
  edited: false
  hidden: false
  id: 657af47013e11aaa91b315b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e099f76b2ab4da90656db865cbaf136.svg
      fullname: 'no'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rotating
      type: user
    createdAt: '2023-12-14T13:45:47.000Z'
    data:
      edited: true
      editors:
      - Rotating
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9564189910888672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e099f76b2ab4da90656db865cbaf136.svg
          fullname: 'no'
          isHf: false
          isPro: false
          name: Rotating
          type: user
        html: '<p>Edited: false alarm with q6</p>

          '
        raw: 'Edited: false alarm with q6'
        updatedAt: '2023-12-19T15:15:47.413Z'
      numEdits: 1
      reactions: []
    id: 657b070b9f62ed61a2f17bc9
    type: comment
  author: Rotating
  content: 'Edited: false alarm with q6'
  created_at: 2023-12-14 13:45:47+00:00
  edited: true
  hidden: false
  id: 657b070b9f62ed61a2f17bc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
      fullname: Alex Cardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexcardo
      type: user
    createdAt: '2023-12-14T14:06:36.000Z'
    data:
      edited: true
      editors:
      - alexcardo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9422692060470581
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
          fullname: Alex Cardo
          isHf: false
          isPro: false
          name: alexcardo
          type: user
        html: '<p>I''m using a Q4_K_M on CPU on my dedicated sever. Q6 is too big
          for my server''s 32GB RAM. On my MacBook M1 I have only 8GB. So, Q4_K_M
          is the only way for me to run the model. </p>

          <p>The only thing I want is to force the model follow my instruction as
          it is. If I ask the model to write something using the outline provided,
          I don''t want it to omit subheadings. People claim that this model surpass
          GPT3.5 and even GPT4. Ok, I understand that the quantized model lost its
          quality. However, as for me, this is the key point the model should adhere,
          ... to follow the instructions.</p>

          <p>Meanwhile, GPT3.5 strictly follow these instructions. </p>

          <p>P.s.: I don''t use Chat GPT since the release of llama.cpp... But to
          clarify, ChatGPT really follow the instructions provided (when it comes
          to subheadings at least).</p>

          <p>P.p.s: By the way. Model is excellent. When I will force it to do what
          I want, I''ll forget ChatGPT even for urgent purposes.</p>

          '
        raw: "I'm using a Q4_K_M on CPU on my dedicated sever. Q6 is too big for my\
          \ server's 32GB RAM. On my MacBook M1 I have only 8GB. So, Q4_K_M is the\
          \ only way for me to run the model. \n\nThe only thing I want is to force\
          \ the model follow my instruction as it is. If I ask the model to write\
          \ something using the outline provided, I don't want it to omit subheadings.\
          \ People claim that this model surpass GPT3.5 and even GPT4. Ok, I understand\
          \ that the quantized model lost its quality. However, as for me, this is\
          \ the key point the model should adhere, ... to follow the instructions.\n\
          \nMeanwhile, GPT3.5 strictly follow these instructions. \n\nP.s.: I don't\
          \ use Chat GPT since the release of llama.cpp... But to clarify, ChatGPT\
          \ really follow the instructions provided (when it comes to subheadings\
          \ at least).\n\nP.p.s: By the way. Model is excellent. When I will force\
          \ it to do what I want, I'll forget ChatGPT even for urgent purposes."
        updatedAt: '2023-12-14T14:12:47.445Z'
      numEdits: 1
      reactions: []
    id: 657b0becb8d76880f877219e
    type: comment
  author: alexcardo
  content: "I'm using a Q4_K_M on CPU on my dedicated sever. Q6 is too big for my\
    \ server's 32GB RAM. On my MacBook M1 I have only 8GB. So, Q4_K_M is the only\
    \ way for me to run the model. \n\nThe only thing I want is to force the model\
    \ follow my instruction as it is. If I ask the model to write something using\
    \ the outline provided, I don't want it to omit subheadings. People claim that\
    \ this model surpass GPT3.5 and even GPT4. Ok, I understand that the quantized\
    \ model lost its quality. However, as for me, this is the key point the model\
    \ should adhere, ... to follow the instructions.\n\nMeanwhile, GPT3.5 strictly\
    \ follow these instructions. \n\nP.s.: I don't use Chat GPT since the release\
    \ of llama.cpp... But to clarify, ChatGPT really follow the instructions provided\
    \ (when it comes to subheadings at least).\n\nP.p.s: By the way. Model is excellent.\
    \ When I will force it to do what I want, I'll forget ChatGPT even for urgent\
    \ purposes."
  created_at: 2023-12-14 14:06:36+00:00
  edited: true
  hidden: false
  id: 657b0becb8d76880f877219e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-14T15:47:47.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9478760361671448
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;alexcardo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/alexcardo\">@<span class=\"\
          underline\">alexcardo</span></a></span>\n\n\t</span></span> theres a few\
          \ things you could do,<br>now im not sure if it would exactly work but you\
          \ can use Grammar in llama cpp to force the model to output in a specific\
          \ format like json or chess. You can make your own custom grammar file so\
          \ it will follow whatever your format is.</p>\n<p>making the grammar file\
          \ might be a bit tricky and might not even work so you could try doing few\
          \ shot<br>you just need to provide a few examples(even 1 will work) and\
          \ it should perform much better.</p>\n"
        raw: "@alexcardo theres a few things you could do, \nnow im not sure if it\
          \ would exactly work but you can use Grammar in llama cpp to force the model\
          \ to output in a specific format like json or chess. You can make your own\
          \ custom grammar file so it will follow whatever your format is.\n\nmaking\
          \ the grammar file might be a bit tricky and might not even work so you\
          \ could try doing few shot\nyou just need to provide a few examples(even\
          \ 1 will work) and it should perform much better."
        updatedAt: '2023-12-14T15:47:47.241Z'
      numEdits: 0
      reactions: []
    id: 657b23a36c149b6a964560c7
    type: comment
  author: YaTharThShaRma999
  content: "@alexcardo theres a few things you could do, \nnow im not sure if it would\
    \ exactly work but you can use Grammar in llama cpp to force the model to output\
    \ in a specific format like json or chess. You can make your own custom grammar\
    \ file so it will follow whatever your format is.\n\nmaking the grammar file might\
    \ be a bit tricky and might not even work so you could try doing few shot\nyou\
    \ just need to provide a few examples(even 1 will work) and it should perform\
    \ much better."
  created_at: 2023-12-14 15:47:47+00:00
  edited: false
  hidden: false
  id: 657b23a36c149b6a964560c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
      fullname: Alex Cardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexcardo
      type: user
    createdAt: '2023-12-14T16:03:58.000Z'
    data:
      edited: false
      editors:
      - alexcardo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9461507201194763
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
          fullname: Alex Cardo
          isHf: false
          isPro: false
          name: alexcardo
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;alexcardo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alexcardo\"\
          >@<span class=\"underline\">alexcardo</span></a></span>\n\n\t</span></span>\
          \ theres a few things you could do,<br>now im not sure if it would exactly\
          \ work but you can use Grammar in llama cpp to force the model to output\
          \ in a specific format like json or chess. You can make your own custom\
          \ grammar file so it will follow whatever your format is.</p>\n<p>making\
          \ the grammar file might be a bit tricky and might not even work so you\
          \ could try doing few shot<br>you just need to provide a few examples(even\
          \ 1 will work) and it should perform much better.</p>\n</blockquote>\n<p>Thank\
          \ you so much for this advice. I knew nothing about the Grammar usage. I\
          \ will Google this subject. Unfortunately, there is too few data on the\
          \ subject of local LLMs and Llama CPP. I would appreciate if you uncover\
          \ the subject a bit more!</p>\n<p>Thank you!</p>\n"
        raw: "> @alexcardo theres a few things you could do, \n> now im not sure if\
          \ it would exactly work but you can use Grammar in llama cpp to force the\
          \ model to output in a specific format like json or chess. You can make\
          \ your own custom grammar file so it will follow whatever your format is.\n\
          > \n> making the grammar file might be a bit tricky and might not even work\
          \ so you could try doing few shot\n> you just need to provide a few examples(even\
          \ 1 will work) and it should perform much better.\n\nThank you so much for\
          \ this advice. I knew nothing about the Grammar usage. I will Google this\
          \ subject. Unfortunately, there is too few data on the subject of local\
          \ LLMs and Llama CPP. I would appreciate if you uncover the subject a bit\
          \ more!\n\nThank you!"
        updatedAt: '2023-12-14T16:03:58.647Z'
      numEdits: 0
      reactions: []
    id: 657b276ebe51c126fd3706f8
    type: comment
  author: alexcardo
  content: "> @alexcardo theres a few things you could do, \n> now im not sure if\
    \ it would exactly work but you can use Grammar in llama cpp to force the model\
    \ to output in a specific format like json or chess. You can make your own custom\
    \ grammar file so it will follow whatever your format is.\n> \n> making the grammar\
    \ file might be a bit tricky and might not even work so you could try doing few\
    \ shot\n> you just need to provide a few examples(even 1 will work) and it should\
    \ perform much better.\n\nThank you so much for this advice. I knew nothing about\
    \ the Grammar usage. I will Google this subject. Unfortunately, there is too few\
    \ data on the subject of local LLMs and Llama CPP. I would appreciate if you uncover\
    \ the subject a bit more!\n\nThank you!"
  created_at: 2023-12-14 16:03:58+00:00
  edited: false
  hidden: false
  id: 657b276ebe51c126fd3706f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-14T16:18:29.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.906995952129364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>basically grammar is a notation that describes the valid syntax
          of text.</p>

          <p>Llama.cpp itself has examples of them you can check out, but lets say
          you basically can force the model to output in a certain format.<br>Here
          are some examples such as json and chess piecees and also much more information<br><a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md">https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md</a></p>

          '
        raw: "basically grammar is a notation that describes the valid syntax of text.\n\
          \nLlama.cpp itself has examples of them you can check out, but lets say\
          \ you basically can force the model to output in a certain format. \nHere\
          \ are some examples such as json and chess piecees and also much more information\
          \ \nhttps://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md"
        updatedAt: '2023-12-14T16:18:29.441Z'
      numEdits: 0
      reactions: []
    id: 657b2ad59f62ed61a2f81898
    type: comment
  author: YaTharThShaRma999
  content: "basically grammar is a notation that describes the valid syntax of text.\n\
    \nLlama.cpp itself has examples of them you can check out, but lets say you basically\
    \ can force the model to output in a certain format. \nHere are some examples\
    \ such as json and chess piecees and also much more information \nhttps://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md"
  created_at: 2023-12-14 16:18:29+00:00
  edited: false
  hidden: false
  id: 657b2ad59f62ed61a2f81898
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Even this excellent high-end model doesn't follow my instructions
