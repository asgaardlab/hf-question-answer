!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JDE65
conflicting_files: null
created_at: 2023-12-01 18:41:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675182370660-63d93dfe255ef6add20ffe34.jpeg?w=200&h=200&f=face
      fullname: Jean Dessain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JDE65
      type: user
    createdAt: '2023-12-01T18:41:00.000Z'
    data:
      edited: false
      editors:
      - JDE65
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8041402697563171
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675182370660-63d93dfe255ef6add20ffe34.jpeg?w=200&h=200&f=face
          fullname: Jean Dessain
          isHf: false
          isPro: false
          name: JDE65
          type: user
        html: '<p>I want to run mpt-7b-8k for summarization tasks for long text and
          to benefit from its 8k max tokens strength.<br>I have a desktop with 64.0
          GB RAM and a RTX-3090 with 24.0 GB GPU. I use python 3.10 and torch 2.1.1-gpu.</p>

          <p>The model is downloaded in a folder model_dir and I apply the following
          code:<br>model = AutoModelForCausalLM.from_pretrained(model_dir)<br>model
          = model.half()<br>model = model.to(device)<br>tokenizer = AutoTokenizer.from_pretrained(model_dir)</p>

          <p>I submit intputs of length 8192<br>inputs = tokenizer(chunk, return_tensors=''pt'').to(device)</p>

          <h1 id="convert-pdf-to-text-split-the-text-in-a-series-of-chunks-of-size-8096-and-then-summarize-each-chung-with-the-following-code">convert
          pdf to text, split the text in a series of chunks of size 8096 and then
          summarize each chung with the following code.</h1>

          <p>torch.cuda.empty_cache()<br>summary_ids = model.generate(inputs[''input_ids''],<br>                                    num_beams=4,<br>                                    max_length=CHUNK_SIZE,<br>                                    early_stopping=True)<br>summary
          = tokenizer.decode(summary_ids[0], skip_special_tokens=True)<br>torch.cuda.empty_cache()</p>

          <p>I get an error message that states:<br>OutOfMemoryError: CUDA out of
          memory. Tried to allocate 46.61 GiB. GPU 0 has a total capacty of 24.00
          GiB of which 5.71 GiB is free. Of the allocated memory 16.11 GiB is allocated
          by PyTorch, and 186.74 MiB is reserved by PyTorch but unallocated. If reserved
          but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>I have tried but haven''t found a workable solution.<br>Could you please
          help ?<br>Thanks in advance</p>

          '
        raw: "I want to run mpt-7b-8k for summarization tasks for long text and to\
          \ benefit from its 8k max tokens strength.\r\nI have a desktop with 64.0\
          \ GB RAM and a RTX-3090 with 24.0 GB GPU. I use python 3.10 and torch 2.1.1-gpu.\r\
          \n\r\nThe model is downloaded in a folder model_dir and I apply the following\
          \ code:\r\nmodel = AutoModelForCausalLM.from_pretrained(model_dir)\r\nmodel\
          \ = model.half()\r\nmodel = model.to(device)\r\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\r\
          \n\r\nI submit intputs of length 8192 \r\ninputs = tokenizer(chunk, return_tensors='pt').to(device)\r\
          \n\r\n# convert pdf to text, split the text in a series of chunks of size\
          \ 8096 and then summarize each chung with the following code.\r\ntorch.cuda.empty_cache()\r\
          \nsummary_ids = model.generate(inputs['input_ids'], \r\n               \
          \                     num_beams=4, \r\n                                \
          \    max_length=CHUNK_SIZE, \r\n                                    early_stopping=True)\r\
          \nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\r\
          \ntorch.cuda.empty_cache()\r\n\r\nI get an error message that states: \r\
          \nOutOfMemoryError: CUDA out of memory. Tried to allocate 46.61 GiB. GPU\
          \ 0 has a total capacty of 24.00 GiB of which 5.71 GiB is free. Of the allocated\
          \ memory 16.11 GiB is allocated by PyTorch, and 186.74 MiB is reserved by\
          \ PyTorch but unallocated. If reserved but unallocated memory is large try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nI have tried but\
          \ haven't found a workable solution.\r\nCould you please help ?\r\nThanks\
          \ in advance"
        updatedAt: '2023-12-01T18:41:00.190Z'
      numEdits: 0
      reactions: []
    id: 656a28bc8fb1ddf0d5c7b9c6
    type: comment
  author: JDE65
  content: "I want to run mpt-7b-8k for summarization tasks for long text and to benefit\
    \ from its 8k max tokens strength.\r\nI have a desktop with 64.0 GB RAM and a\
    \ RTX-3090 with 24.0 GB GPU. I use python 3.10 and torch 2.1.1-gpu.\r\n\r\nThe\
    \ model is downloaded in a folder model_dir and I apply the following code:\r\n\
    model = AutoModelForCausalLM.from_pretrained(model_dir)\r\nmodel = model.half()\r\
    \nmodel = model.to(device)\r\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\r\
    \n\r\nI submit intputs of length 8192 \r\ninputs = tokenizer(chunk, return_tensors='pt').to(device)\r\
    \n\r\n# convert pdf to text, split the text in a series of chunks of size 8096\
    \ and then summarize each chung with the following code.\r\ntorch.cuda.empty_cache()\r\
    \nsummary_ids = model.generate(inputs['input_ids'], \r\n                     \
    \               num_beams=4, \r\n                                    max_length=CHUNK_SIZE,\
    \ \r\n                                    early_stopping=True)\r\nsummary = tokenizer.decode(summary_ids[0],\
    \ skip_special_tokens=True)\r\ntorch.cuda.empty_cache()\r\n\r\nI get an error\
    \ message that states: \r\nOutOfMemoryError: CUDA out of memory. Tried to allocate\
    \ 46.61 GiB. GPU 0 has a total capacty of 24.00 GiB of which 5.71 GiB is free.\
    \ Of the allocated memory 16.11 GiB is allocated by PyTorch, and 186.74 MiB is\
    \ reserved by PyTorch but unallocated. If reserved but unallocated memory is large\
    \ try setting max_split_size_mb to avoid fragmentation.  See documentation for\
    \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nI have tried but haven't\
    \ found a workable solution.\r\nCould you please help ?\r\nThanks in advance"
  created_at: 2023-12-01 18:41:00+00:00
  edited: false
  hidden: false
  id: 656a28bc8fb1ddf0d5c7b9c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-12-01T19:05:34.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.965822160243988
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>you have ~10GB on your GPU given how much space the model takes
          up. With 4 beams, you are trying to load 4x8k sequences and those activations
          take up much more memory than you have. You should try top_p sampling instead,
          which does not need beams and gets similar quality. you may have to input
          shorter sequences and cap the max summary length. You can also experiment
          with 8bit with bitsandbytes and that gives you a bit more space for activations.</p>

          '
        raw: you have ~10GB on your GPU given how much space the model takes up. With
          4 beams, you are trying to load 4x8k sequences and those activations take
          up much more memory than you have. You should try top_p sampling instead,
          which does not need beams and gets similar quality. you may have to input
          shorter sequences and cap the max summary length. You can also experiment
          with 8bit with bitsandbytes and that gives you a bit more space for activations.
        updatedAt: '2023-12-01T19:05:34.494Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656a2e7eef1e27e9e3a01187
    id: 656a2e7eef1e27e9e3a01181
    type: comment
  author: sam-mosaic
  content: you have ~10GB on your GPU given how much space the model takes up. With
    4 beams, you are trying to load 4x8k sequences and those activations take up much
    more memory than you have. You should try top_p sampling instead, which does not
    need beams and gets similar quality. you may have to input shorter sequences and
    cap the max summary length. You can also experiment with 8bit with bitsandbytes
    and that gives you a bit more space for activations.
  created_at: 2023-12-01 19:05:34+00:00
  edited: false
  hidden: false
  id: 656a2e7eef1e27e9e3a01181
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-12-01T19:05:34.000Z'
    data:
      status: closed
    id: 656a2e7eef1e27e9e3a01187
    type: status-change
  author: sam-mosaic
  created_at: 2023-12-01 19:05:34+00:00
  id: 656a2e7eef1e27e9e3a01187
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: mosaicml/mpt-7b-8k
repo_type: model
status: closed
target_branch: null
title: Memory allocation
