!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wawoshashi
conflicting_files: null
created_at: 2023-09-22 09:45:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d529f18fc556bdeab552307c4d23c8e4.svg
      fullname: ya wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wawoshashi
      type: user
    createdAt: '2023-09-22T10:45:25.000Z'
    data:
      edited: false
      editors:
      - wawoshashi
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.728378176689148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d529f18fc556bdeab552307c4d23c8e4.svg
          fullname: ya wu
          isHf: false
          isPro: false
          name: wawoshashi
          type: user
        html: "<p>\u4E2A\u4EBA\u90E8\u7F7270B\u6A21\u578B\u6765\u505A\u9A8C\u8BC1\
          ,\u6709\u70B9\u56F0\u96BE, \u5173\u6CE8\u4E8B\u6001\u53D1\u5C55</p>\n"
        raw: "\u4E2A\u4EBA\u90E8\u7F7270B\u6A21\u578B\u6765\u505A\u9A8C\u8BC1,\u6709\
          \u70B9\u56F0\u96BE, \u5173\u6CE8\u4E8B\u6001\u53D1\u5C55"
        updatedAt: '2023-09-22T10:45:25.507Z'
      numEdits: 0
      reactions: []
    id: 650d704549e89cb674a4a281
    type: comment
  author: wawoshashi
  content: "\u4E2A\u4EBA\u90E8\u7F7270B\u6A21\u578B\u6765\u505A\u9A8C\u8BC1,\u6709\
    \u70B9\u56F0\u96BE, \u5173\u6CE8\u4E8B\u6001\u53D1\u5C55"
  created_at: 2023-09-22 09:45:25+00:00
  edited: false
  hidden: false
  id: 650d704549e89cb674a4a281
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-09-22T14:06:18.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8009666800498962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Try this quantized version <a href="https://huggingface.co/TheBloke/Xwin-LM-70B-V0.1-GGUF">https://huggingface.co/TheBloke/Xwin-LM-70B-V0.1-GGUF</a>
          which only needs a 48G Vram card, or 40GB RAM cpu only.</p>

          <p>You can try it now with llama.cpp</p>

          <p>There is also 7B GPTQ Version <a href="https://huggingface.co/TheBloke/Xwin-LM-7B-V0.1-GPTQ">https://huggingface.co/TheBloke/Xwin-LM-7B-V0.1-GPTQ</a>
          only need 6G VRAM</p>

          '
        raw: 'Try this quantized version https://huggingface.co/TheBloke/Xwin-LM-70B-V0.1-GGUF
          which only needs a 48G Vram card, or 40GB RAM cpu only.


          You can try it now with llama.cpp


          There is also 7B GPTQ Version https://huggingface.co/TheBloke/Xwin-LM-7B-V0.1-GPTQ
          only need 6G VRAM'
        updatedAt: '2023-09-22T14:06:18.700Z'
      numEdits: 0
      reactions: []
    id: 650d9f5ad5cbcf1984fc32f6
    type: comment
  author: Yhyu13
  content: 'Try this quantized version https://huggingface.co/TheBloke/Xwin-LM-70B-V0.1-GGUF
    which only needs a 48G Vram card, or 40GB RAM cpu only.


    You can try it now with llama.cpp


    There is also 7B GPTQ Version https://huggingface.co/TheBloke/Xwin-LM-7B-V0.1-GPTQ
    only need 6G VRAM'
  created_at: 2023-09-22 13:06:18+00:00
  edited: false
  hidden: false
  id: 650d9f5ad5cbcf1984fc32f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
      fullname: hai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cloudyu
      type: user
    createdAt: '2023-09-24T23:06:22.000Z'
    data:
      edited: false
      editors:
      - cloudyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7916994094848633
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
          fullname: hai
          isHf: false
          isPro: false
          name: cloudyu
          type: user
        html: '<p>I can run 70B quantized GGUF model (Q3_K - Small and offloaded 60/83
          layers to GPU ) on 3090 via llama.cpp.</p>

          '
        raw: 'I can run 70B quantized GGUF model (Q3_K - Small and offloaded 60/83
          layers to GPU ) on 3090 via llama.cpp.

          '
        updatedAt: '2023-09-24T23:06:22.320Z'
      numEdits: 0
      reactions: []
    id: 6510c0eec3b9d8d1eb67ca10
    type: comment
  author: cloudyu
  content: 'I can run 70B quantized GGUF model (Q3_K - Small and offloaded 60/83 layers
    to GPU ) on 3090 via llama.cpp.

    '
  created_at: 2023-09-24 22:06:22+00:00
  edited: false
  hidden: false
  id: 6510c0eec3b9d8d1eb67ca10
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Xwin-LM/Xwin-LM-70B-V0.1
repo_type: model
status: open
target_branch: null
title: It's a bit difficult to deploy the 70B model for verification, so let's keep
  an eye on how things develop
