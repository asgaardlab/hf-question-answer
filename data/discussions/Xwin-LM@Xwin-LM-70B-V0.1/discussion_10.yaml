!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AARon99
conflicting_files: null
created_at: 2023-11-25 23:26:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c148a32d2e7deeeea6db252b90973ea0.svg
      fullname: AaRon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AARon99
      type: user
    createdAt: '2023-11-25T23:26:54.000Z'
    data:
      edited: false
      editors:
      - AARon99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447686076164246
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c148a32d2e7deeeea6db252b90973ea0.svg
          fullname: AaRon
          isHf: false
          isPro: false
          name: AARon99
          type: user
        html: '<p>What are you doing to the base Llama2 model?  I have never used
          a model like this:</p>

          <ol>

          <li>Fine-tunes work extremely well, inputs after fine-tuning can be generalizations
          with respect to the fine-tuned dataset and the LLM responds very well.</li>

          <li>I''ve given the model thousands of tokens worth of code and it will
          lucidly edit portions while contextualizing how the code works.  I even
          did this on an 8-bit exllama2 quantization using rope scaling to get 8,192
          worth of tokens.</li>

          </ol>

          <p>I''ve checked out your github trying to find out more, I''ve seen others
          asking similar questions about how you are doing this.  I guess I''m just
          adding my voice to the choir.  </p>

          <p>Looking forward to Wxin-LM70B-v0.2!!  I hope the questions don''t come
          off the wrong way, there is just such little information with respect to
          the quality of the model.  I appreciate the work done and the sharing of
          the model.</p>

          '
        raw: "What are you doing to the base Llama2 model?  I have never used a model\
          \ like this:\r\n1. Fine-tunes work extremely well, inputs after fine-tuning\
          \ can be generalizations with respect to the fine-tuned dataset and the\
          \ LLM responds very well.\r\n2. I've given the model thousands of tokens\
          \ worth of code and it will lucidly edit portions while contextualizing\
          \ how the code works.  I even did this on an 8-bit exllama2 quantization\
          \ using rope scaling to get 8,192 worth of tokens.\r\n\r\nI've checked out\
          \ your github trying to find out more, I've seen others asking similar questions\
          \ about how you are doing this.  I guess I'm just adding my voice to the\
          \ choir.  \r\n\r\nLooking forward to Wxin-LM70B-v0.2!!  I hope the questions\
          \ don't come off the wrong way, there is just such little information with\
          \ respect to the quality of the model.  I appreciate the work done and the\
          \ sharing of the model."
        updatedAt: '2023-11-25T23:26:54.096Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - Yhyu13
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nss-ysasaki
    id: 656282beabefee6a6bfbbd28
    type: comment
  author: AARon99
  content: "What are you doing to the base Llama2 model?  I have never used a model\
    \ like this:\r\n1. Fine-tunes work extremely well, inputs after fine-tuning can\
    \ be generalizations with respect to the fine-tuned dataset and the LLM responds\
    \ very well.\r\n2. I've given the model thousands of tokens worth of code and\
    \ it will lucidly edit portions while contextualizing how the code works.  I even\
    \ did this on an 8-bit exllama2 quantization using rope scaling to get 8,192 worth\
    \ of tokens.\r\n\r\nI've checked out your github trying to find out more, I've\
    \ seen others asking similar questions about how you are doing this.  I guess\
    \ I'm just adding my voice to the choir.  \r\n\r\nLooking forward to Wxin-LM70B-v0.2!!\
    \  I hope the questions don't come off the wrong way, there is just such little\
    \ information with respect to the quality of the model.  I appreciate the work\
    \ done and the sharing of the model."
  created_at: 2023-11-25 23:26:54+00:00
  edited: false
  hidden: false
  id: 656282beabefee6a6bfbbd28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fed5d447375cd8c3b2bc9e096fe681de.svg
      fullname: Bolin Ni
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nbl97
      type: user
    createdAt: '2023-11-28T13:37:40.000Z'
    data:
      edited: false
      editors:
      - nbl97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9152224659919739
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fed5d447375cd8c3b2bc9e096fe681de.svg
          fullname: Bolin Ni
          isHf: false
          isPro: false
          name: nbl97
          type: user
        html: '<p>Many thanks for your interest. We will work hard for better models,
          and release new models and details ASAP.</p>

          <p>Looking forward to your feedback~</p>

          '
        raw: 'Many thanks for your interest. We will work hard for better models,
          and release new models and details ASAP.


          Looking forward to your feedback~'
        updatedAt: '2023-11-28T13:37:40.352Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AARon99
    id: 6565ed24d4c272e7a9f062d4
    type: comment
  author: nbl97
  content: 'Many thanks for your interest. We will work hard for better models, and
    release new models and details ASAP.


    Looking forward to your feedback~'
  created_at: 2023-11-28 13:37:40+00:00
  edited: false
  hidden: false
  id: 6565ed24d4c272e7a9f062d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4VOzArmrRaX_DUTxGmm59.jpeg?w=200&h=200&f=face
      fullname: Charles McSneed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChuckMcSneed
      type: user
    createdAt: '2024-01-07T01:22:39.000Z'
    data:
      edited: false
      editors:
      - ChuckMcSneed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7675126194953918
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4VOzArmrRaX_DUTxGmm59.jpeg?w=200&h=200&f=face
          fullname: Charles McSneed
          isHf: false
          isPro: false
          name: ChuckMcSneed
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nbl97&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nbl97\">@<span class=\"\
          underline\">nbl97</span></a></span>\n\n\t</span></span> When can we expect\
          \ new models/publication of the dataset?</p>\n"
        raw: '@nbl97 When can we expect new models/publication of the dataset?'
        updatedAt: '2024-01-07T01:22:39.669Z'
      numEdits: 0
      reactions: []
    id: 6599fcdf5f7a6d40f74375ec
    type: comment
  author: ChuckMcSneed
  content: '@nbl97 When can we expect new models/publication of the dataset?'
  created_at: 2024-01-07 01:22:39+00:00
  edited: false
  hidden: false
  id: 6599fcdf5f7a6d40f74375ec
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: Xwin-LM/Xwin-LM-70B-V0.1
repo_type: model
status: open
target_branch: null
title: This is a very interesting model, and I'm very curious how you did this.
