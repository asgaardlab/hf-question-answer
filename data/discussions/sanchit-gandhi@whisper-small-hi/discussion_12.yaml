!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Martha-987
conflicting_files: null
created_at: 2023-03-06 05:58:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
      fullname: 'Martha  Fikry '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Martha-987
      type: user
    createdAt: '2023-03-06T05:58:29.000Z'
    data:
      edited: false
      editors:
      - Martha-987
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
          fullname: 'Martha  Fikry '
          isHf: false
          isPro: false
          name: Martha-987
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span><br>I\
          \ need your help please, can you help me to compute different loss function\
          \ in the notebook? how do I write the code for this and run it for getting\
          \ results for this loss ?<br>thanks in advance :)</p>\n"
        raw: "hi @sanchit-gandhi \r\nI need your help please, can you help me to compute\
          \ different loss function in the notebook? how do I write the code for this\
          \ and run it for getting results for this loss ? \r\nthanks in advance :)"
        updatedAt: '2023-03-06T05:58:29.499Z'
      numEdits: 0
      reactions: []
    id: 640581051a3babee78e85d32
    type: comment
  author: Martha-987
  content: "hi @sanchit-gandhi \r\nI need your help please, can you help me to compute\
    \ different loss function in the notebook? how do I write the code for this and\
    \ run it for getting results for this loss ? \r\nthanks in advance :)"
  created_at: 2023-03-06 05:58:29+00:00
  edited: false
  hidden: false
  id: 640581051a3babee78e85d32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-03-06T16:20:09.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Martha-987&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Martha-987\"\
          >@<span class=\"underline\">Martha-987</span></a></span>\n\n\t</span></span>!\
          \ For this kind of custom behaviour, you'll need to perform a bit of a different\
          \ set-up:</p>\n<p>First, you need to uninstall transformers:</p>\n<pre><code>pip\
          \ uninstall transformers\n</code></pre>\n<p>Then, you need to clone the\
          \ transformers repository and install from source. Follow steps 1-4 from\
          \ this guide: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request\"\
          >https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request</a></p>\n\
          <p>Now when you make changes to transformers, it will be reflected in your\
          \ pip install.</p>\n<p>You can go into the Whisper modelling code and make\
          \ the changes you require, namely to these three lines: <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/transformers/blob/64d95c44ec44e5f8d50b47cbcadb0471bf99ef17/src/transformers/models/whisper/modeling_whisper.py#L1389-L1391\"\
          >https://github.com/huggingface/transformers/blob/64d95c44ec44e5f8d50b47cbcadb0471bf99ef17/src/transformers/models/whisper/modeling_whisper.py#L1389-L1391</a></p>\n\
          <p>Hope that helps!</p>\n<p>Out of interest, what changes do you need to\
          \ make to the loss?</p>\n"
        raw: 'Hey @Martha-987! For this kind of custom behaviour, you''ll need to
          perform a bit of a different set-up:


          First, you need to uninstall transformers:

          ```

          pip uninstall transformers

          ```


          Then, you need to clone the transformers repository and install from source.
          Follow steps 1-4 from this guide: https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request


          Now when you make changes to transformers, it will be reflected in your
          pip install.


          You can go into the Whisper modelling code and make the changes you require,
          namely to these three lines: https://github.com/huggingface/transformers/blob/64d95c44ec44e5f8d50b47cbcadb0471bf99ef17/src/transformers/models/whisper/modeling_whisper.py#L1389-L1391


          Hope that helps!


          Out of interest, what changes do you need to make to the loss?'
        updatedAt: '2023-03-06T16:20:09.841Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Martha-987
    id: 640612b983b59abcdddb329d
    type: comment
  author: sanchit-gandhi
  content: 'Hey @Martha-987! For this kind of custom behaviour, you''ll need to perform
    a bit of a different set-up:


    First, you need to uninstall transformers:

    ```

    pip uninstall transformers

    ```


    Then, you need to clone the transformers repository and install from source. Follow
    steps 1-4 from this guide: https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request


    Now when you make changes to transformers, it will be reflected in your pip install.


    You can go into the Whisper modelling code and make the changes you require, namely
    to these three lines: https://github.com/huggingface/transformers/blob/64d95c44ec44e5f8d50b47cbcadb0471bf99ef17/src/transformers/models/whisper/modeling_whisper.py#L1389-L1391


    Hope that helps!


    Out of interest, what changes do you need to make to the loss?'
  created_at: 2023-03-06 16:20:09+00:00
  edited: false
  hidden: false
  id: 640612b983b59abcdddb329d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
      fullname: 'Martha  Fikry '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Martha-987
      type: user
    createdAt: '2023-03-07T11:31:38.000Z'
    data:
      edited: false
      editors:
      - Martha-987
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
          fullname: 'Martha  Fikry '
          isHf: false
          isPro: false
          name: Martha-987
          type: user
        html: "<p>Thanks alot <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \  for your help , I need to train with different loss function and make\
          \ comparison for results to know the best, can i do this ?</p>\n"
        raw: Thanks alot @sanchit-gandhi  for your help , I need to train with different
          loss function and make comparison for results to know the best, can i do
          this ?
        updatedAt: '2023-03-07T11:31:38.266Z'
      numEdits: 0
      reactions: []
    id: 6407209aad466ab0a56f8fe0
    type: comment
  author: Martha-987
  content: Thanks alot @sanchit-gandhi  for your help , I need to train with different
    loss function and make comparison for results to know the best, can i do this
    ?
  created_at: 2023-03-07 11:31:38+00:00
  edited: false
  hidden: false
  id: 6407209aad466ab0a56f8fe0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
      fullname: 'Martha  Fikry '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Martha-987
      type: user
    createdAt: '2023-03-08T13:17:43.000Z'
    data:
      edited: false
      editors:
      - Martha-987
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
          fullname: 'Martha  Fikry '
          isHf: false
          isPro: false
          name: Martha-987
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \  Can I change CrossEntropyLoss to HingeEmbeddingLoss  or any another loss\
          \ function ??? and how?</p>\n"
        raw: '@sanchit-gandhi  Can I change CrossEntropyLoss to HingeEmbeddingLoss  or
          any another loss function ??? and how?'
        updatedAt: '2023-03-08T13:17:43.646Z'
      numEdits: 0
      reactions: []
    id: 64088af704cad621ef90ac91
    type: comment
  author: Martha-987
  content: '@sanchit-gandhi  Can I change CrossEntropyLoss to HingeEmbeddingLoss  or
    any another loss function ??? and how?'
  created_at: 2023-03-08 13:17:43+00:00
  edited: false
  hidden: false
  id: 64088af704cad621ef90ac91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a9d08e123b6c9f0db156da657a7de836.svg
      fullname: sagar raikar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: raikarsagar
      type: user
    createdAt: '2023-03-14T06:15:25.000Z'
    data:
      edited: false
      editors:
      - raikarsagar
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a9d08e123b6c9f0db156da657a7de836.svg
          fullname: sagar raikar
          isHf: false
          isPro: false
          name: raikarsagar
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \  How do we obtain groundtruth alignments for training if loss is CrossEntropy\
          \ ?</p>\n"
        raw: '@sanchit-gandhi  How do we obtain groundtruth alignments for training
          if loss is CrossEntropy ?'
        updatedAt: '2023-03-14T06:15:25.275Z'
      numEdits: 0
      reactions: []
    id: 641010fdb99903b2a44bed85
    type: comment
  author: raikarsagar
  content: '@sanchit-gandhi  How do we obtain groundtruth alignments for training
    if loss is CrossEntropy ?'
  created_at: 2023-03-14 05:15:25+00:00
  edited: false
  hidden: false
  id: 641010fdb99903b2a44bed85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-03-17T15:42:42.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Martha-987&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Martha-987\"\
          >@<span class=\"underline\">Martha-987</span></a></span>\n\n\t</span></span>!\
          \ You can either follow the approach I've outlined for modifying the source\
          \ code, or copy and paste the transformers code and make modifications there.</p>\n\
          <p>E.g. copy this code into a file called <code>modeling_whisper.py</code></p>\n\
          <details>\n\n<summary> Code: </summary>\n\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-comment\"># coding=utf-8</span>\n<span class=\"hljs-comment\"\
          ># Copyright 2022 The OpenAI Authors and The HuggingFace Inc. team. All\
          \ rights reserved.</span>\n<span class=\"hljs-comment\">#</span>\n<span\
          \ class=\"hljs-comment\"># Licensed under the Apache License, Version 2.0\
          \ (the \"License\");</span>\n<span class=\"hljs-comment\"># you may not\
          \ use this file except in compliance with the License.</span>\n<span class=\"\
          hljs-comment\"># You may obtain a copy of the License at</span>\n<span class=\"\
          hljs-comment\">#</span>\n<span class=\"hljs-comment\">#     http://www.apache.org/licenses/LICENSE-2.0</span>\n\
          <span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># Unless\
          \ required by applicable law or agreed to in writing, software</span>\n\
          <span class=\"hljs-comment\"># distributed under the License is distributed\
          \ on an \"AS IS\" BASIS,</span>\n<span class=\"hljs-comment\"># WITHOUT\
          \ WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>\n\
          <span class=\"hljs-comment\"># See the License for the specific language\
          \ governing permissions and</span>\n<span class=\"hljs-comment\"># limitations\
          \ under the License.</span>\n<span class=\"hljs-string\">\"\"\" PyTorch\
          \ Whisper model.\"\"\"</span>\n\n\n<span class=\"hljs-keyword\">import</span>\
          \ math\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"\
          hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span>\
          \ <span class=\"hljs-type\">Optional</span>, <span class=\"hljs-type\">Tuple</span>,\
          \ <span class=\"hljs-type\">Union</span>\n\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">import</span> torch.utils.checkpoint\n\
          <span class=\"hljs-keyword\">from</span> torch <span class=\"hljs-keyword\"\
          >import</span> nn\n<span class=\"hljs-keyword\">from</span> torch.nn <span\
          \ class=\"hljs-keyword\">import</span> CrossEntropyLoss\n\n<span class=\"\
          hljs-keyword\">from</span> transformers.activations <span class=\"hljs-keyword\"\
          >import</span> ACT2FN\n<span class=\"hljs-keyword\">from</span> transformers.generation.logits_process\
          \ <span class=\"hljs-keyword\">import</span> WhisperTimeStampLogitsProcessor\n\
          <span class=\"hljs-keyword\">from</span> transformers.modeling_outputs <span\
          \ class=\"hljs-keyword\">import</span> (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n\
          \    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\n<span class=\"hljs-keyword\"\
          >from</span> transformers.modeling_utils <span class=\"hljs-keyword\">import</span>\
          \ PreTrainedModel\n<span class=\"hljs-keyword\">from</span> transformers.utils\
          \ <span class=\"hljs-keyword\">import</span> (\n    add_start_docstrings,\n\
          \    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n\
          )\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> WhisperConfig\n\n\nlogger = logging.get_logger(__name__)\n\
          \n_CONFIG_FOR_DOC = <span class=\"hljs-string\">\"WhisperConfig\"</span>\n\
          _CHECKPOINT_FOR_DOC = <span class=\"hljs-string\">\"openai/whisper-tiny\"\
          </span>\n\n\nWHISPER_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    <span class=\"\
          hljs-string\">\"openai/whisper-base\"</span>,\n    <span class=\"hljs-comment\"\
          ># See all Whisper models at https://huggingface.co/models?filter=whisper</span>\n\
          ]\n\n\n<span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.shift_tokens_right</span>\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >shift_tokens_right</span>(<span class=\"hljs-params\">input_ids: torch.Tensor,\
          \ pad_token_id: <span class=\"hljs-built_in\">int</span>, decoder_start_token_id:\
          \ <span class=\"hljs-built_in\">int</span></span>):\n    <span class=\"\
          hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\">    Shift input\
          \ ids one token to the right.</span>\n<span class=\"hljs-string\">    \"\
          \"\"</span>\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n\
          \    shifted_input_ids[:, <span class=\"hljs-number\">1</span>:] = input_ids[:,\
          \ :-<span class=\"hljs-number\">1</span>].clone()\n    shifted_input_ids[:,\
          \ <span class=\"hljs-number\">0</span>] = decoder_start_token_id\n\n   \
          \ <span class=\"hljs-keyword\">if</span> pad_token_id <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-literal\">None</span>:\n        <span class=\"\
          hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">\"self.model.config.pad_token_id\
          \ has to be defined.\"</span>)\n    <span class=\"hljs-comment\"># replace\
          \ possible -100 values in labels by `pad_token_id`</span>\n    shifted_input_ids.masked_fill_(shifted_input_ids\
          \ == -<span class=\"hljs-number\">100</span>, pad_token_id)\n\n    <span\
          \ class=\"hljs-keyword\">return</span> shifted_input_ids\n\n\n<span class=\"\
          hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper._make_causal_mask</span>\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >_make_causal_mask</span>(<span class=\"hljs-params\">input_ids_shape: torch.Size,\
          \ dtype: torch.dtype, past_key_values_length: <span class=\"hljs-built_in\"\
          >int</span> = <span class=\"hljs-number\">0</span></span>):\n    <span class=\"\
          hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\">    Make causal\
          \ mask used for bi-directional self-attention.</span>\n<span class=\"hljs-string\"\
          >    \"\"\"</span>\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len,\
          \ tgt_len), torch.tensor(torch.finfo(dtype).<span class=\"hljs-built_in\"\
          >min</span>))\n    mask_cond = torch.arange(mask.size(-<span class=\"hljs-number\"\
          >1</span>))\n    mask.masked_fill_(mask_cond &lt; (mask_cond + <span class=\"\
          hljs-number\">1</span>).view(mask.size(-<span class=\"hljs-number\">1</span>),\
          \ <span class=\"hljs-number\">1</span>), <span class=\"hljs-number\">0</span>)\n\
          \    mask = mask.to(dtype)\n\n    <span class=\"hljs-keyword\">if</span>\
          \ past_key_values_length &gt; <span class=\"hljs-number\">0</span>:\n  \
          \      mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype),\
          \ mask], dim=-<span class=\"hljs-number\">1</span>)\n    <span class=\"\
          hljs-keyword\">return</span> mask[<span class=\"hljs-literal\">None</span>,\
          \ <span class=\"hljs-literal\">None</span>, :, :].expand(bsz, <span class=\"\
          hljs-number\">1</span>, tgt_len, tgt_len + past_key_values_length)\n\n\n\
          <span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper._expand_mask</span>\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >_expand_mask</span>(<span class=\"hljs-params\">mask: torch.Tensor, dtype:\
          \ torch.dtype, tgt_len: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-built_in\">int</span>] = <span class=\"hljs-literal\">None</span></span>):\n\
          \    <span class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\"\
          >    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len,\
          \ src_seq_len]`.</span>\n<span class=\"hljs-string\">    \"\"\"</span>\n\
          \    bsz, src_len = mask.size()\n    tgt_len = tgt_len <span class=\"hljs-keyword\"\
          >if</span> tgt_len <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span> <span\
          \ class=\"hljs-keyword\">else</span> src_len\n\n    expanded_mask = mask[:,\
          \ <span class=\"hljs-literal\">None</span>, <span class=\"hljs-literal\"\
          >None</span>, :].expand(bsz, <span class=\"hljs-number\">1</span>, tgt_len,\
          \ src_len).to(dtype)\n\n    inverted_mask = <span class=\"hljs-number\"\
          >1.0</span> - expanded_mask\n\n    <span class=\"hljs-keyword\">return</span>\
          \ inverted_mask.masked_fill(inverted_mask.to(torch.<span class=\"hljs-built_in\"\
          >bool</span>), torch.finfo(dtype).<span class=\"hljs-built_in\">min</span>)\n\
          \n\n<span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperPositionalEmbedding</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperPositionalEmbedding</span>(nn.Embedding):\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"\
          hljs-params\">self, num_positions: <span class=\"hljs-built_in\">int</span>,\
          \ embedding_dim: <span class=\"hljs-built_in\">int</span>, padding_idx:\
          \ <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-built_in\"\
          >int</span>] = <span class=\"hljs-literal\">None</span></span>):\n     \
          \   <span class=\"hljs-built_in\">super</span>().__init__(num_positions,\
          \ embedding_dim)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">forward</span>(<span class=\"hljs-params\">self,\
          \ input_ids, past_key_values_length=<span class=\"hljs-number\">0</span></span>):\n\
          \        <span class=\"hljs-keyword\">return</span> self.weight[past_key_values_length\
          \ : past_key_values_length + input_ids.shape[-<span class=\"hljs-number\"\
          >1</span>]]\n\n\n<span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperAttention</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperAttention</span>(nn.Module):\n    <span class=\"hljs-string\">\"\
          \"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\
          </span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">__init__</span>(<span class=\"hljs-params\"></span>\n<span\
          \ class=\"hljs-params\">        self,</span>\n<span class=\"hljs-params\"\
          >        embed_dim: <span class=\"hljs-built_in\">int</span>,</span>\n<span\
          \ class=\"hljs-params\">        num_heads: <span class=\"hljs-built_in\"\
          >int</span>,</span>\n<span class=\"hljs-params\">        dropout: <span\
          \ class=\"hljs-built_in\">float</span> = <span class=\"hljs-number\">0.0</span>,</span>\n\
          <span class=\"hljs-params\">        is_decoder: <span class=\"hljs-built_in\"\
          >bool</span> = <span class=\"hljs-literal\">False</span>,</span>\n<span\
          \ class=\"hljs-params\">        bias: <span class=\"hljs-built_in\">bool</span>\
          \ = <span class=\"hljs-literal\">True</span>,</span>\n<span class=\"hljs-params\"\
          >    </span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n\
          \        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n\
          \        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n\
          \n        <span class=\"hljs-keyword\">if</span> (self.head_dim * num_heads)\
          \ != self.embed_dim:\n            <span class=\"hljs-keyword\">raise</span>\
          \ ValueError(\n                <span class=\"hljs-string\">f\"embed_dim\
          \ must be divisible by num_heads (got `embed_dim`: <span class=\"hljs-subst\"\
          >{self.embed_dim}</span>\"</span>\n                <span class=\"hljs-string\"\
          >f\" and `num_heads`: <span class=\"hljs-subst\">{num_heads}</span>).\"\
          </span>\n            )\n        self.scaling = self.head_dim**-<span class=\"\
          hljs-number\">0.5</span>\n        self.is_decoder = is_decoder\n\n     \
          \   self.k_proj = nn.Linear(embed_dim, embed_dim, bias=<span class=\"hljs-literal\"\
          >False</span>)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\
          \        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n    \
          \    self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n    <span\
          \ class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.BartAttention._shape\
          \ with BART-&gt;whisper</span>\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">_shape</span>(<span class=\"hljs-params\"\
          >self, tensor: torch.Tensor, seq_len: <span class=\"hljs-built_in\">int</span>,\
          \ bsz: <span class=\"hljs-built_in\">int</span></span>):\n        <span\
          \ class=\"hljs-keyword\">return</span> tensor.view(bsz, seq_len, self.num_heads,\
          \ self.head_dim).transpose(<span class=\"hljs-number\">1</span>, <span class=\"\
          hljs-number\">2</span>).contiguous()\n\n    <span class=\"hljs-comment\"\
          ># Copied from transformers.models.whisper.modeling_whisper.BartAttention.forward\
          \ with BART-&gt;whisper</span>\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\"\
          ></span>\n<span class=\"hljs-params\">        self,</span>\n<span class=\"\
          hljs-params\">        hidden_states: torch.Tensor,</span>\n<span class=\"\
          hljs-params\">        key_value_states: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        past_key_value: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[torch.Tensor]] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        attention_mask:\
          \ <span class=\"hljs-type\">Optional</span>[torch.Tensor] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ layer_head_mask: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        output_attentions: <span class=\"hljs-built_in\">bool</span> =\
          \ <span class=\"hljs-literal\">False</span>,</span>\n<span class=\"hljs-params\"\
          >    </span>) -&gt; <span class=\"hljs-type\">Tuple</span>[torch.Tensor,\
          \ <span class=\"hljs-type\">Optional</span>[torch.Tensor], <span class=\"\
          hljs-type\">Optional</span>[<span class=\"hljs-type\">Tuple</span>[torch.Tensor]]]:\n\
          \        <span class=\"hljs-string\">\"\"\"Input shape: Batch x Time x Channel\"\
          \"\"</span>\n\n        <span class=\"hljs-comment\"># if key_value_states\
          \ are provided this layer is used as a cross-attention layer</span>\n  \
          \      <span class=\"hljs-comment\"># for the decoder</span>\n        is_cross_attention\
          \ = key_value_states <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>\n\n \
          \       bsz, tgt_len, _ = hidden_states.size()\n\n        <span class=\"\
          hljs-comment\"># get query proj</span>\n        query_states = self.q_proj(hidden_states)\
          \ * self.scaling\n        <span class=\"hljs-comment\"># get key, value\
          \ proj</span>\n        <span class=\"hljs-comment\"># `past_key_value[0].shape[2]\
          \ == key_value_states.shape[1]`</span>\n        <span class=\"hljs-comment\"\
          ># is checking that the `sequence_length` of the `past_key_value` is the\
          \ same as</span>\n        <span class=\"hljs-comment\"># the provided `key_value_states`\
          \ to support prefix tuning</span>\n        <span class=\"hljs-keyword\"\
          >if</span> (\n            is_cross_attention\n            <span class=\"\
          hljs-keyword\">and</span> past_key_value <span class=\"hljs-keyword\">is</span>\
          \ <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>\n\
          \            <span class=\"hljs-keyword\">and</span> past_key_value[<span\
          \ class=\"hljs-number\">0</span>].shape[<span class=\"hljs-number\">2</span>]\
          \ == key_value_states.shape[<span class=\"hljs-number\">1</span>]\n    \
          \    ):\n            <span class=\"hljs-comment\"># reuse k,v, cross_attentions</span>\n\
          \            key_states = past_key_value[<span class=\"hljs-number\">0</span>]\n\
          \            value_states = past_key_value[<span class=\"hljs-number\">1</span>]\n\
          \        <span class=\"hljs-keyword\">elif</span> is_cross_attention:\n\
          \            <span class=\"hljs-comment\"># cross_attentions</span>\n  \
          \          key_states = self._shape(self.k_proj(key_value_states), -<span\
          \ class=\"hljs-number\">1</span>, bsz)\n            value_states = self._shape(self.v_proj(key_value_states),\
          \ -<span class=\"hljs-number\">1</span>, bsz)\n        <span class=\"hljs-keyword\"\
          >elif</span> past_key_value <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n\
          \            <span class=\"hljs-comment\"># reuse k, v, self_attention</span>\n\
          \            key_states = self._shape(self.k_proj(hidden_states), -<span\
          \ class=\"hljs-number\">1</span>, bsz)\n            value_states = self._shape(self.v_proj(hidden_states),\
          \ -<span class=\"hljs-number\">1</span>, bsz)\n            key_states =\
          \ torch.cat([past_key_value[<span class=\"hljs-number\">0</span>], key_states],\
          \ dim=<span class=\"hljs-number\">2</span>)\n            value_states =\
          \ torch.cat([past_key_value[<span class=\"hljs-number\">1</span>], value_states],\
          \ dim=<span class=\"hljs-number\">2</span>)\n        <span class=\"hljs-keyword\"\
          >else</span>:\n            <span class=\"hljs-comment\"># self_attention</span>\n\
          \            key_states = self._shape(self.k_proj(hidden_states), -<span\
          \ class=\"hljs-number\">1</span>, bsz)\n            value_states = self._shape(self.v_proj(hidden_states),\
          \ -<span class=\"hljs-number\">1</span>, bsz)\n\n        <span class=\"\
          hljs-keyword\">if</span> self.is_decoder:\n            <span class=\"hljs-comment\"\
          ># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross\
          \ attention key/value_states.</span>\n            <span class=\"hljs-comment\"\
          ># Further calls to cross_attention layer can then reuse all cross-attention</span>\n\
          \            <span class=\"hljs-comment\"># key/value_states (first \"if\"\
          \ case)</span>\n            <span class=\"hljs-comment\"># if uni-directional\
          \ self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span>\n\
          \            <span class=\"hljs-comment\"># all previous decoder key/value_states.\
          \ Further calls to uni-directional self-attention</span>\n            <span\
          \ class=\"hljs-comment\"># can concat previous decoder key/value_states\
          \ to current projected key/value_states (third \"elif\" case)</span>\n \
          \           <span class=\"hljs-comment\"># if encoder bi-directional self-attention\
          \ `past_key_value` is always `None`</span>\n            past_key_value =\
          \ (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads,\
          \ -<span class=\"hljs-number\">1</span>, self.head_dim)\n        query_states\
          \ = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n        key_states\
          \ = key_states.reshape(*proj_shape)\n        value_states = value_states.reshape(*proj_shape)\n\
          \n        src_len = key_states.size(<span class=\"hljs-number\">1</span>)\n\
          \        attn_weights = torch.bmm(query_states, key_states.transpose(<span\
          \ class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>))\n\
          \n        <span class=\"hljs-keyword\">if</span> attn_weights.size() !=\
          \ (bsz * self.num_heads, tgt_len, src_len):\n            <span class=\"\
          hljs-keyword\">raise</span> ValueError(\n                <span class=\"\
          hljs-string\">f\"Attention weights should be of size <span class=\"hljs-subst\"\
          >{(bsz * self.num_heads, tgt_len, src_len)}</span>, but is\"</span>\n  \
          \              <span class=\"hljs-string\">f\" <span class=\"hljs-subst\"\
          >{attn_weights.size()}</span>\"</span>\n            )\n\n        <span class=\"\
          hljs-keyword\">if</span> attention_mask <span class=\"hljs-keyword\">is</span>\
          \ <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n\
          \            <span class=\"hljs-keyword\">if</span> attention_mask.size()\
          \ != (bsz, <span class=\"hljs-number\">1</span>, tgt_len, src_len):\n  \
          \              <span class=\"hljs-keyword\">raise</span> ValueError(\n \
          \                   <span class=\"hljs-string\">f\"Attention mask should\
          \ be of size <span class=\"hljs-subst\">{(bsz, <span class=\"hljs-number\"\
          >1</span>, tgt_len, src_len)}</span>, but is <span class=\"hljs-subst\"\
          >{attention_mask.size()}</span>\"</span>\n                )\n          \
          \  attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\
          \ + attention_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads,\
          \ tgt_len, src_len)\n\n        attn_weights = nn.functional.softmax(attn_weights,\
          \ dim=-<span class=\"hljs-number\">1</span>)\n\n        <span class=\"hljs-keyword\"\
          >if</span> layer_head_mask <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n\
          \            <span class=\"hljs-keyword\">if</span> layer_head_mask.size()\
          \ != (self.num_heads,):\n                <span class=\"hljs-keyword\">raise</span>\
          \ ValueError(\n                    <span class=\"hljs-string\">f\"Head mask\
          \ for a single layer should be of size <span class=\"hljs-subst\">{(self.num_heads,)}</span>,\
          \ but is\"</span>\n                    <span class=\"hljs-string\">f\" <span\
          \ class=\"hljs-subst\">{layer_head_mask.size()}</span>\"</span>\n      \
          \          )\n            attn_weights = layer_head_mask.view(<span class=\"\
          hljs-number\">1</span>, -<span class=\"hljs-number\">1</span>, <span class=\"\
          hljs-number\">1</span>, <span class=\"hljs-number\">1</span>) * attn_weights.view(bsz,\
          \ self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights.view(bsz\
          \ * self.num_heads, tgt_len, src_len)\n\n        <span class=\"hljs-keyword\"\
          >if</span> output_attentions:\n            <span class=\"hljs-comment\"\
          ># this operation is a bit awkward, but it's required to</span>\n      \
          \      <span class=\"hljs-comment\"># make sure that attn_weights keeps\
          \ its gradient.</span>\n            <span class=\"hljs-comment\"># In order\
          \ to do so, attn_weights have to be reshaped</span>\n            <span class=\"\
          hljs-comment\"># twice and have to be reused in the following</span>\n \
          \           attn_weights_reshaped = attn_weights.view(bsz, self.num_heads,\
          \ tgt_len, src_len)\n            attn_weights = attn_weights_reshaped.view(bsz\
          \ * self.num_heads, tgt_len, src_len)\n        <span class=\"hljs-keyword\"\
          >else</span>:\n            attn_weights_reshaped = <span class=\"hljs-literal\"\
          >None</span>\n\n        attn_probs = nn.functional.dropout(attn_weights,\
          \ p=self.dropout, training=self.training)\n\n        attn_output = torch.bmm(attn_probs,\
          \ value_states)\n\n        <span class=\"hljs-keyword\">if</span> attn_output.size()\
          \ != (bsz * self.num_heads, tgt_len, self.head_dim):\n            <span\
          \ class=\"hljs-keyword\">raise</span> ValueError(\n                <span\
          \ class=\"hljs-string\">f\"`attn_output` should be of size <span class=\"\
          hljs-subst\">{(bsz * self.num_heads, tgt_len, self.head_dim)}</span>, but\
          \ is\"</span>\n                <span class=\"hljs-string\">f\" <span class=\"\
          hljs-subst\">{attn_output.size()}</span>\"</span>\n            )\n\n   \
          \     attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n\
          \        attn_output = attn_output.transpose(<span class=\"hljs-number\"\
          >1</span>, <span class=\"hljs-number\">2</span>)\n\n        <span class=\"\
          hljs-comment\"># Use the `embed_dim` from the config (stored in the class)\
          \ rather than `hidden_state` because `attn_output` can be</span>\n     \
          \   <span class=\"hljs-comment\"># partitioned across GPUs when using tensor-parallelism.</span>\n\
          \        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\
          \n        attn_output = self.out_proj(attn_output)\n\n        <span class=\"\
          hljs-keyword\">return</span> attn_output, attn_weights_reshaped, past_key_value\n\
          \n\n<span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperEncoderLayer</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperEncoderLayer</span>(nn.Module):\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"\
          hljs-params\">self, config: WhisperConfig</span>):\n        <span class=\"\
          hljs-built_in\">super</span>().__init__()\n        self.embed_dim = config.d_model\n\
          \        self.self_attn = WhisperAttention(\n            embed_dim=self.embed_dim,\n\
          \            num_heads=config.encoder_attention_heads,\n            dropout=config.attention_dropout,\n\
          \        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\
          \        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n\
          \        self.activation_dropout = config.activation_dropout\n        self.fc1\
          \ = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2\
          \ = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm\
          \ = nn.LayerNorm(self.embed_dim)\n\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\"\
          ></span>\n<span class=\"hljs-params\">        self,</span>\n<span class=\"\
          hljs-params\">        hidden_states: torch.Tensor,</span>\n<span class=\"\
          hljs-params\">        attention_mask: torch.Tensor,</span>\n<span class=\"\
          hljs-params\">        layer_head_mask: torch.Tensor,</span>\n<span class=\"\
          hljs-params\">        output_attentions: <span class=\"hljs-built_in\">bool</span>\
          \ = <span class=\"hljs-literal\">False</span>,</span>\n<span class=\"hljs-params\"\
          >    </span>) -&gt; torch.Tensor:\n        <span class=\"hljs-string\">\"\
          \"\"</span>\n<span class=\"hljs-string\">        Args:</span>\n<span class=\"\
          hljs-string\">            hidden_states (`torch.FloatTensor`): input to\
          \ the layer of shape `(seq_len, batch, embed_dim)`</span>\n<span class=\"\
          hljs-string\">            attention_mask (`torch.FloatTensor`): attention\
          \ mask of size</span>\n<span class=\"hljs-string\">                `(batch,\
          \ 1, tgt_len, src_len)` where padding elements are indicated by very large\
          \ negative values.</span>\n<span class=\"hljs-string\">            layer_head_mask\
          \ (`torch.FloatTensor`): mask for attention heads in a given layer of size</span>\n\
          <span class=\"hljs-string\">                `(encoder_attention_heads,)`.</span>\n\
          <span class=\"hljs-string\">            output_attentions (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Whether or not to return the\
          \ attentions tensors of all attention layers. See `attentions` under</span>\n\
          <span class=\"hljs-string\">                returned tensors for more detail.</span>\n\
          <span class=\"hljs-string\">        \"\"\"</span>\n        residual = hidden_states\n\
          \        hidden_states = self.self_attn_layer_norm(hidden_states)\n    \
          \    hidden_states, attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n\
          \            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n\
          \            output_attentions=output_attentions,\n        )\n        hidden_states\
          \ = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\
          \        hidden_states = residual + hidden_states\n\n        residual =\
          \ hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\
          \        hidden_states = self.activation_fn(self.fc1(hidden_states))\n \
          \       hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout,\
          \ training=self.training)\n        hidden_states = self.fc2(hidden_states)\n\
          \        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout,\
          \ training=self.training)\n        hidden_states = residual + hidden_states\n\
          \n        <span class=\"hljs-keyword\">if</span> hidden_states.dtype ==\
          \ torch.float16 <span class=\"hljs-keyword\">and</span> (\n            torch.isinf(hidden_states).<span\
          \ class=\"hljs-built_in\">any</span>() <span class=\"hljs-keyword\">or</span>\
          \ torch.isnan(hidden_states).<span class=\"hljs-built_in\">any</span>()\n\
          \        ):\n            clamp_value = torch.finfo(hidden_states.dtype).<span\
          \ class=\"hljs-built_in\">max</span> - <span class=\"hljs-number\">1000</span>\n\
          \            hidden_states = torch.clamp(hidden_states, <span class=\"hljs-built_in\"\
          >min</span>=-clamp_value, <span class=\"hljs-built_in\">max</span>=clamp_value)\n\
          \n        outputs = (hidden_states,)\n\n        <span class=\"hljs-keyword\"\
          >if</span> output_attentions:\n            outputs += (attn_weights,)\n\n\
          \        <span class=\"hljs-keyword\">return</span> outputs\n\n\n<span class=\"\
          hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperDecoderLayer</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperDecoderLayer</span>(nn.Module):\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"\
          hljs-params\">self, config: WhisperConfig</span>):\n        <span class=\"\
          hljs-built_in\">super</span>().__init__()\n        self.embed_dim = config.d_model\n\
          \n        self.self_attn = WhisperAttention(\n            embed_dim=self.embed_dim,\n\
          \            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n\
          \            is_decoder=<span class=\"hljs-literal\">True</span>,\n    \
          \    )\n        self.dropout = config.dropout\n        self.activation_fn\
          \ = ACT2FN[config.activation_function]\n        self.activation_dropout\
          \ = config.activation_dropout\n\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\
          \        self.encoder_attn = WhisperAttention(\n            self.embed_dim,\n\
          \            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n\
          \            is_decoder=<span class=\"hljs-literal\">True</span>,\n    \
          \    )\n        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\
          \        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n\
          \        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n\
          \        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\n    <span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >forward</span>(<span class=\"hljs-params\"></span>\n<span class=\"hljs-params\"\
          >        self,</span>\n<span class=\"hljs-params\">        hidden_states:\
          \ torch.Tensor,</span>\n<span class=\"hljs-params\">        attention_mask:\
          \ <span class=\"hljs-type\">Optional</span>[torch.Tensor] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ encoder_hidden_states: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        encoder_attention_mask: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        layer_head_mask: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        cross_attn_layer_head_mask: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        past_key_value: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[torch.Tensor]] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        output_attentions:\
          \ <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-built_in\"\
          >bool</span>] = <span class=\"hljs-literal\">False</span>,</span>\n<span\
          \ class=\"hljs-params\">        use_cache: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-built_in\">bool</span>] = <span class=\"hljs-literal\">True</span>,</span>\n\
          <span class=\"hljs-params\">    </span>) -&gt; torch.Tensor:\n        <span\
          \ class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\">    \
          \    Args:</span>\n<span class=\"hljs-string\">            hidden_states\
          \ (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`</span>\n\
          <span class=\"hljs-string\">            attention_mask (`torch.FloatTensor`):\
          \ attention mask of size</span>\n<span class=\"hljs-string\">          \
          \      `(batch, 1, tgt_len, src_len)` where padding elements are indicated\
          \ by very large negative values.</span>\n<span class=\"hljs-string\">  \
          \          encoder_hidden_states (`torch.FloatTensor`):</span>\n<span class=\"\
          hljs-string\">                cross attention input to the layer of shape\
          \ `(batch, seq_len, embed_dim)`</span>\n<span class=\"hljs-string\">   \
          \         encoder_attention_mask (`torch.FloatTensor`): encoder attention\
          \ mask of size</span>\n<span class=\"hljs-string\">                `(batch,\
          \ 1, tgt_len, src_len)` where padding elements are indicated by very large\
          \ negative values.</span>\n<span class=\"hljs-string\">            layer_head_mask\
          \ (`torch.FloatTensor`): mask for attention heads in a given layer of size</span>\n\
          <span class=\"hljs-string\">                `(encoder_attention_heads,)`.</span>\n\
          <span class=\"hljs-string\">            cross_attn_layer_head_mask (`torch.FloatTensor`):\
          \ mask for cross-attention heads in a given layer of</span>\n<span class=\"\
          hljs-string\">                size `(decoder_attention_heads,)`.</span>\n\
          <span class=\"hljs-string\">            past_key_value (`Tuple(torch.FloatTensor)`):\
          \ cached past key and value projection states</span>\n<span class=\"hljs-string\"\
          >            output_attentions (`bool`, *optional*):</span>\n<span class=\"\
          hljs-string\">                Whether or not to return the attentions tensors\
          \ of all attention layers. See `attentions` under</span>\n<span class=\"\
          hljs-string\">                returned tensors for more detail.</span>\n\
          <span class=\"hljs-string\">        \"\"\"</span>\n        residual = hidden_states\n\
          \        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n  \
          \      <span class=\"hljs-comment\"># Self Attention</span>\n        <span\
          \ class=\"hljs-comment\"># decoder uni-directional self-attention cached\
          \ key/values tuple is at positions 1,2</span>\n        self_attn_past_key_value\
          \ = past_key_value[:<span class=\"hljs-number\">2</span>] <span class=\"\
          hljs-keyword\">if</span> past_key_value <span class=\"hljs-keyword\">is</span>\
          \ <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>\n        <span class=\"hljs-comment\"># add present self-attn\
          \ cache to positions 1,2 of present_key_value tuple</span>\n        hidden_states,\
          \ self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n\
          \            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n\
          \            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n\
          \        )\n        hidden_states = nn.functional.dropout(hidden_states,\
          \ p=self.dropout, training=self.training)\n        hidden_states = residual\
          \ + hidden_states\n\n        <span class=\"hljs-comment\"># Cross-Attention\
          \ Block</span>\n        cross_attn_present_key_value = <span class=\"hljs-literal\"\
          >None</span>\n        cross_attn_weights = <span class=\"hljs-literal\"\
          >None</span>\n        <span class=\"hljs-keyword\">if</span> encoder_hidden_states\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span>:\n            residual = hidden_states\n\
          \            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n\
          \n            <span class=\"hljs-comment\"># cross_attn cached key/values\
          \ tuple is at positions 3,4 of present_key_value tuple</span>\n        \
          \    cross_attn_past_key_value = past_key_value[-<span class=\"hljs-number\"\
          >2</span>:] <span class=\"hljs-keyword\">if</span> past_key_value <span\
          \ class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\"\
          >else</span> <span class=\"hljs-literal\">None</span>\n            hidden_states,\
          \ cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n\
          \                hidden_states=hidden_states,\n                key_value_states=encoder_hidden_states,\n\
          \                attention_mask=encoder_attention_mask,\n              \
          \  layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n\
          \                output_attentions=output_attentions,\n            )\n \
          \           hidden_states = nn.functional.dropout(hidden_states, p=self.dropout,\
          \ training=self.training)\n            hidden_states = residual + hidden_states\n\
          \n            <span class=\"hljs-comment\"># add cross-attn to positions\
          \ 3,4 of present_key_value tuple</span>\n            present_key_value =\
          \ present_key_value + cross_attn_present_key_value\n\n        <span class=\"\
          hljs-comment\"># Fully Connected</span>\n        residual = hidden_states\n\
          \        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states\
          \ = self.activation_fn(self.fc1(hidden_states))\n        hidden_states =\
          \ nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n\
          \        hidden_states = self.fc2(hidden_states)\n        hidden_states\
          \ = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\
          \        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\
          \n        <span class=\"hljs-keyword\">if</span> output_attentions:\n  \
          \          outputs += (self_attn_weights, cross_attn_weights)\n\n      \
          \  <span class=\"hljs-keyword\">if</span> use_cache:\n            outputs\
          \ += (present_key_value,)\n\n        <span class=\"hljs-keyword\">return</span>\
          \ outputs\n\n\n<span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperPreTrainedModel</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperPreTrainedModel</span>(<span class=\"hljs-title class_ inherited__\"\
          >PreTrainedModel</span>):\n    config_class = WhisperConfig\n    base_model_prefix\
          \ = <span class=\"hljs-string\">\"model\"</span>\n    main_input_name =\
          \ <span class=\"hljs-string\">\"input_features\"</span>\n    supports_gradient_checkpointing\
          \ = <span class=\"hljs-literal\">True</span>\n    _no_split_modules = [<span\
          \ class=\"hljs-string\">\"WhisperEncoderLayer\"</span>]\n\n    <span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">_init_weights</span>(<span\
          \ class=\"hljs-params\">self, module</span>):\n        std = self.config.init_std\n\
          \        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >isinstance</span>(module, (nn.Linear, nn.Conv1d)):\n            module.weight.data.normal_(mean=<span\
          \ class=\"hljs-number\">0.0</span>, std=std)\n            <span class=\"\
          hljs-keyword\">if</span> module.bias <span class=\"hljs-keyword\">is</span>\
          \ <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n\
          \                module.bias.data.zero_()\n        <span class=\"hljs-keyword\"\
          >elif</span> <span class=\"hljs-built_in\">isinstance</span>(module, nn.Embedding):\n\
          \            module.weight.data.normal_(mean=<span class=\"hljs-number\"\
          >0.0</span>, std=std)\n            <span class=\"hljs-keyword\">if</span>\
          \ module.padding_idx <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n  \
          \              module.weight.data[module.padding_idx].zero_()\n\n    <span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >_set_gradient_checkpointing</span>(<span class=\"hljs-params\">self, module,\
          \ value=<span class=\"hljs-literal\">False</span></span>):\n        <span\
          \ class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(module,\
          \ (WhisperDecoder, WhisperEncoder)):\n            module.gradient_checkpointing\
          \ = value\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">_get_feat_extract_output_lengths</span>(<span class=\"\
          hljs-params\">self, input_lengths: torch.LongTensor</span>):\n        <span\
          \ class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\">    \
          \    Computes the output length of the convolutional layers</span>\n<span\
          \ class=\"hljs-string\">        \"\"\"</span>\n        input_lengths = (input_lengths\
          \ - <span class=\"hljs-number\">1</span>) // <span class=\"hljs-number\"\
          >2</span> + <span class=\"hljs-number\">1</span>\n\n        <span class=\"\
          hljs-keyword\">return</span> input_lengths\n\n\nWHISPER_START_DOCSTRING\
          \ = <span class=\"hljs-string\">r\"\"\"</span>\n<span class=\"hljs-string\"\
          >    This model inherits from [`PreTrainedModel`]. Check the superclass\
          \ documentation for the generic methods the</span>\n<span class=\"hljs-string\"\
          >    library implements for all its model (such as downloading or saving,\
          \ resizing the input embeddings, pruning heads</span>\n<span class=\"hljs-string\"\
          >    etc.)</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"\
          >    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)\
          \ subclass.</span>\n<span class=\"hljs-string\">    Use it as a regular\
          \ PyTorch Module and refer to the PyTorch documentation for all matter related\
          \ to general usage</span>\n<span class=\"hljs-string\">    and behavior.</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">    Parameters:</span>\n\
          <span class=\"hljs-string\">        config ([`WhisperConfig`]):</span>\n\
          <span class=\"hljs-string\">            Model configuration class with all\
          \ the parameters of the model. Initializing with a config file does not</span>\n\
          <span class=\"hljs-string\">            load the weights associated with\
          \ the model, only the configuration. Check out the</span>\n<span class=\"\
          hljs-string\">            [`~PreTrainedModel.from_pretrained`] method to\
          \ load the model weights.</span>\n<span class=\"hljs-string\">\"\"\"</span>\n\
          \nWHISPER_INPUTS_DOCSTRING = <span class=\"hljs-string\">r\"\"\"</span>\n\
          <span class=\"hljs-string\">    Args:</span>\n<span class=\"hljs-string\"\
          >        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size,\
          \ sequence_length)`):</span>\n<span class=\"hljs-string\">            Float\
          \ values mel features extracted from the raw speech waveform. Raw speech\
          \ waveform can be obtained by</span>\n<span class=\"hljs-string\">     \
          \       loading a `.flac` or `.wav` audio file into an array of type `List[float]`\
          \ or a `numpy.ndarray`, *e.g.* via</span>\n<span class=\"hljs-string\">\
          \            the soundfile library (`pip install soundfile`). To prepare\
          \ the array into `input_features`, the</span>\n<span class=\"hljs-string\"\
          >            [`AutoFeatureExtractor`] should be used for extracting the\
          \ mel features, padding and conversion into a</span>\n<span class=\"hljs-string\"\
          >            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]</span>\n\
          <span class=\"hljs-string\">        attention_mask (`torch.LongTensor` of\
          \ shape `(batch_size, sequence_length)`, *optional*):</span>\n<span class=\"\
          hljs-string\">            Mask to avoid performing *SpecAugment* data augmentation\
          \ on padding token indices. Mask values selected in</span>\n<span class=\"\
          hljs-string\">            `[0, 1]`:</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">            - 1 for tokens that are\
          \ **not masked**,</span>\n<span class=\"hljs-string\">            - 0 for\
          \ tokens that are **masked**.</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">            [What are attention masks?](transformers/glossary#attention-mask)</span>\n\
          <span class=\"hljs-string\">        decoder_input_ids (`torch.LongTensor`\
          \ of shape `(batch_size, target_sequence_length)`, *optional*):</span>\n\
          <span class=\"hljs-string\">            Indices of decoder input sequence\
          \ tokens in the vocabulary.</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">            Indices can be obtained using [`WhisperTokenizer`].\
          \ See [`PreTrainedTokenizer.encode`] and</span>\n<span class=\"hljs-string\"\
          >            [`PreTrainedTokenizer.__call__`] for details.</span>\n<span\
          \ class=\"hljs-string\"></span>\n<span class=\"hljs-string\">          \
          \  [What are decoder input IDs?](transformers/glossary#decoder-input-ids)</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \      Whisper uses the `decoder_start_token_id` as the starting token for\
          \ `decoder_input_ids` generation. If</span>\n<span class=\"hljs-string\"\
          >            `past_key_values` is used, optionally only the last `decoder_input_ids`\
          \ have to be input (see</span>\n<span class=\"hljs-string\">           \
          \ `past_key_values`).</span>\n<span class=\"hljs-string\">        decoder_attention_mask\
          \ (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):</span>\n\
          <span class=\"hljs-string\">            Default behavior: generate a tensor\
          \ that ignores pad tokens in `decoder_input_ids`. Causal mask will also</span>\n\
          <span class=\"hljs-string\">            be used by default.</span>\n<span\
          \ class=\"hljs-string\"></span>\n<span class=\"hljs-string\">          \
          \  If you want to change padding behavior, you should read</span>\n<span\
          \ class=\"hljs-string\">            [`modeling_whisper._prepare_decoder_attention_mask`]\
          \ and modify to your needs. See diagram 1 in [the BART</span>\n<span class=\"\
          hljs-string\">            paper](https://arxiv.org/abs/1910.13461) for more\
          \ information on the default strategy.</span>\n<span class=\"hljs-string\"\
          >        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,\
          \ *optional*):</span>\n<span class=\"hljs-string\">            Mask to nullify\
          \ selected heads of the attention modules in the encoder. Mask values selected\
          \ in `[0, 1]`:</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">            - 1 indicates the head is **not masked**,</span>\n\
          <span class=\"hljs-string\">            - 0 indicates the head is **masked**.</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \  decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,\
          \ *optional*):</span>\n<span class=\"hljs-string\">            Mask to nullify\
          \ selected heads of the attention modules in the decoder. Mask values selected\
          \ in `[0, 1]`:</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">            - 1 indicates the head is **not masked**,</span>\n\
          <span class=\"hljs-string\">            - 0 indicates the head is **masked**.</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \  cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,\
          \ *optional*):</span>\n<span class=\"hljs-string\">            Mask to nullify\
          \ selected heads of the cross-attention modules. Mask values selected in\
          \ `[0, 1]`:</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">            - 1 indicates the head is **not masked**,</span>\n\
          <span class=\"hljs-string\">            - 0 indicates the head is **masked**.</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \  encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):</span>\n\
          <span class=\"hljs-string\">            Tuple consists of (`last_hidden_state`,\
          \ *optional*: `hidden_states`, *optional*: `attentions`)</span>\n<span class=\"\
          hljs-string\">            `last_hidden_state` of shape `(batch_size, sequence_length,\
          \ hidden_size)`, *optional*) is a sequence of</span>\n<span class=\"hljs-string\"\
          >            hidden-states at the output of the last layer of the encoder.\
          \ Used in the cross-attention of the decoder.</span>\n<span class=\"hljs-string\"\
          >        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*,\
          \ returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>\n\
          <span class=\"hljs-string\">            Tuple of `tuple(torch.FloatTensor)`\
          \ of length `config.n_layers`, with each tuple having 2 tensors of shape</span>\n\
          <span class=\"hljs-string\">            `(batch_size, num_heads, sequence_length,\
          \ embed_size_per_head)`) and 2 additional tensors of shape</span>\n<span\
          \ class=\"hljs-string\">            `(batch_size, num_heads, encoder_sequence_length,\
          \ embed_size_per_head)`.</span>\n<span class=\"hljs-string\"></span>\n<span\
          \ class=\"hljs-string\">            Contains pre-computed hidden-states\
          \ (key and values in the self-attention blocks and in the cross-attention</span>\n\
          <span class=\"hljs-string\">            blocks) that can be used (see `past_key_values`\
          \ input) to speed up sequential decoding.</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">            If `past_key_values` are\
          \ used, the user can optionally input only the last `decoder_input_ids`\
          \ (those that</span>\n<span class=\"hljs-string\">            don't have\
          \ their past key value states given to this model) of shape `(batch_size,\
          \ 1)` instead of all</span>\n<span class=\"hljs-string\">            `decoder_input_ids`\
          \ of shape `(batch_size, sequence_length)`.</span>\n<span class=\"hljs-string\"\
          >        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size,\
          \ target_sequence_length, hidden_size)`, *optional*):</span>\n<span class=\"\
          hljs-string\">            Optionally, instead of passing `decoder_input_ids`\
          \ you can choose to directly pass an embedded</span>\n<span class=\"hljs-string\"\
          >            representation. If `past_key_values` is used, optionally only\
          \ the last `decoder_inputs_embeds` have to be</span>\n<span class=\"hljs-string\"\
          >            input (see `past_key_values`). This is useful if you want more\
          \ control over how to convert</span>\n<span class=\"hljs-string\">     \
          \       `decoder_input_ids` indices into associated vectors than the model's\
          \ internal embedding lookup matrix.</span>\n<span class=\"hljs-string\"\
          >        use_cache (`bool`, *optional*):</span>\n<span class=\"hljs-string\"\
          >            If set to `True`, `past_key_values` key value states are returned\
          \ and can be used to speed up decoding (see</span>\n<span class=\"hljs-string\"\
          >            `past_key_values`).</span>\n<span class=\"hljs-string\">  \
          \      output_attentions (`bool`, *optional*):</span>\n<span class=\"hljs-string\"\
          >            Whether or not to return the attentions tensors of all attention\
          \ layers. See `attentions` under returned</span>\n<span class=\"hljs-string\"\
          >            tensors for more detail.</span>\n<span class=\"hljs-string\"\
          >        output_hidden_states (`bool`, *optional*):</span>\n<span class=\"\
          hljs-string\">            Whether or not to return the hidden states of\
          \ all layers. See `hidden_states` under returned tensors for</span>\n<span\
          \ class=\"hljs-string\">            more detail.</span>\n<span class=\"\
          hljs-string\">        return_dict (`bool`, *optional*):</span>\n<span class=\"\
          hljs-string\">            Whether or not to return a [`~utils.ModelOutput`]\
          \ instead of a plain tuple.</span>\n<span class=\"hljs-string\">\"\"\"</span>\n\
          \nWHISPER_ENCODER_INPUTS_DOCSTRING = <span class=\"hljs-string\">r\"\"\"\
          </span>\n<span class=\"hljs-string\">    Args:</span>\n<span class=\"hljs-string\"\
          >        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size,\
          \ sequence_length)`):</span>\n<span class=\"hljs-string\">            Float\
          \ values mel features extracted from the raw speech waveform. Raw speech\
          \ waveform can be obtained by</span>\n<span class=\"hljs-string\">     \
          \       loading a `.flac` or `.wav` audio file into an array of type `List[float]`\
          \ or a `numpy.ndarray`, *e.g.* via</span>\n<span class=\"hljs-string\">\
          \            the soundfile library (`pip install soundfile`). To prepare\
          \ the array into `input_features`, the</span>\n<span class=\"hljs-string\"\
          >            [`AutoFeatureExtractor`] should be used for extracting the\
          \ mel features, padding and conversion into a</span>\n<span class=\"hljs-string\"\
          >            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]</span>\n\
          <span class=\"hljs-string\">        head_mask (`torch.Tensor` of shape `(encoder_layers,\
          \ encoder_attention_heads)`, *optional*):</span>\n<span class=\"hljs-string\"\
          >            Mask to nullify selected heads of the attention modules in\
          \ the encoder. Mask values selected in `[0, 1]`:</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">            - 1 indicates\
          \ the head is **not masked**,</span>\n<span class=\"hljs-string\">     \
          \       - 0 indicates the head is **masked**.</span>\n<span class=\"hljs-string\"\
          >        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):</span>\n\
          <span class=\"hljs-string\">            Tuple consists of (`last_hidden_state`,\
          \ *optional*: `hidden_states`, *optional*: `attentions`)</span>\n<span class=\"\
          hljs-string\">            `last_hidden_state` of shape `(batch_size, sequence_length,\
          \ hidden_size)`, *optional*) is a sequence of</span>\n<span class=\"hljs-string\"\
          >            hidden-states at the output of the last layer of the encoder.</span>\n\
          <span class=\"hljs-string\">        output_attentions (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">            Whether or not to return the attentions\
          \ tensors of all attention layers. See `attentions` under returned</span>\n\
          <span class=\"hljs-string\">            tensors for more detail.</span>\n\
          <span class=\"hljs-string\">        output_hidden_states (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">            Whether or not to return the hidden\
          \ states of all layers. See `hidden_states` under returned tensors for</span>\n\
          <span class=\"hljs-string\">            more detail.</span>\n<span class=\"\
          hljs-string\">        return_dict (`bool`, *optional*):</span>\n<span class=\"\
          hljs-string\">            Whether or not to return a [`~utils.ModelOutput`]\
          \ instead of a plain tuple.</span>\n<span class=\"hljs-string\">\"\"\"</span>\n\
          \n\n<span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperEncoder</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperEncoder</span>(<span class=\"hljs-title class_ inherited__\">WhisperPreTrainedModel</span>):\n\
          \    <span class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\"\
          >    Transformer encoder consisting of *config.encoder_layers* self attention\
          \ layers. Each layer is a</span>\n<span class=\"hljs-string\">    [`WhisperEncoderLayer`].</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">    Args:</span>\n\
          <span class=\"hljs-string\">        config: WhisperConfig</span>\n<span\
          \ class=\"hljs-string\">        embed_tokens (nn.Embedding): output embedding</span>\n\
          <span class=\"hljs-string\">    \"\"\"</span>\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"\
          hljs-params\">self, config: WhisperConfig</span>):\n        <span class=\"\
          hljs-built_in\">super</span>().__init__(config)\n        self.dropout =\
          \ config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\n\
          \        embed_dim = config.d_model\n        self.num_mel_bins = config.num_mel_bins\n\
          \        self.padding_idx = config.pad_token_id\n        self.max_source_positions\
          \ = config.max_source_positions\n        self.embed_scale = math.sqrt(embed_dim)\
          \ <span class=\"hljs-keyword\">if</span> config.scale_embedding <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-number\">1.0</span>\n\n  \
          \      self.conv1 = nn.Conv1d(self.num_mel_bins, embed_dim, kernel_size=<span\
          \ class=\"hljs-number\">3</span>, padding=<span class=\"hljs-number\">1</span>)\n\
          \        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=<span\
          \ class=\"hljs-number\">3</span>, stride=<span class=\"hljs-number\">2</span>,\
          \ padding=<span class=\"hljs-number\">1</span>)\n\n        self.embed_positions\
          \ = nn.Embedding(self.max_source_positions, embed_dim)\n\n        self.layers\
          \ = nn.ModuleList([WhisperEncoderLayer(config) <span class=\"hljs-keyword\"\
          >for</span> _ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >range</span>(config.encoder_layers)])\n        self.layer_norm = nn.LayerNorm(config.d_model)\n\
          \n        self.gradient_checkpointing = <span class=\"hljs-literal\">False</span>\n\
          \        <span class=\"hljs-comment\"># Initialize weights and apply final\
          \ processing</span>\n        self.post_init()\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">_freeze_parameters</span>(<span\
          \ class=\"hljs-params\">self</span>):\n        <span class=\"hljs-keyword\"\
          >for</span> param <span class=\"hljs-keyword\">in</span> self.parameters():\n\
          \            param.requires_grad = <span class=\"hljs-literal\">False</span>\n\
          \        self._requires_grad = <span class=\"hljs-literal\">False</span>\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >forward</span>(<span class=\"hljs-params\"></span>\n<span class=\"hljs-params\"\
          >        self,</span>\n<span class=\"hljs-params\">        input_features,</span>\n\
          <span class=\"hljs-params\">        attention_mask=<span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        head_mask=<span\
          \ class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        output_attentions=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        output_hidden_states=<span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ return_dict=<span class=\"hljs-literal\">None</span>,</span>\n<span class=\"\
          hljs-params\">    </span>):\n        <span class=\"hljs-string\">r\"\"\"\
          </span>\n<span class=\"hljs-string\">        Args:</span>\n<span class=\"\
          hljs-string\">            input_features (`torch.LongTensor` of shape `(batch_size,\
          \ feature_size, sequence_length)`):</span>\n<span class=\"hljs-string\"\
          >                Float values of mel features extracted from the raw speech\
          \ waveform. Raw speech waveform can be</span>\n<span class=\"hljs-string\"\
          >                obtained by loading a `.flac` or `.wav` audio file into\
          \ an array of type `List[float]` or a</span>\n<span class=\"hljs-string\"\
          >                `numpy.ndarray`, *e.g.* via the soundfile library (`pip\
          \ install soundfile`). To prepare the array into</span>\n<span class=\"\
          hljs-string\">                `input_features`, the [`AutoFeatureExtractor`]\
          \ should be used for extracting the mel features, padding</span>\n<span\
          \ class=\"hljs-string\">                and conversion into a tensor of\
          \ type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]</span>\n\
          <span class=\"hljs-string\">            attention_mask (`torch.Tensor`)`,\
          \ *optional*):</span>\n<span class=\"hljs-string\">                Whisper\
          \ does not support masking of the `input_features`, this argument is preserved\
          \ for compatibility,</span>\n<span class=\"hljs-string\">              \
          \  but it is not used. By default the silence in the input log mel spectrogram\
          \ are ignored.</span>\n<span class=\"hljs-string\">            head_mask\
          \ (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,\
          \ *optional*):</span>\n<span class=\"hljs-string\">                Mask\
          \ to nullify selected heads of the attention modules. Mask values selected\
          \ in `[0, 1]`:</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">                - 1 indicates the head is **not masked**,</span>\n\
          <span class=\"hljs-string\">                - 0 indicates the head is **masked**.</span>\n\
          <span class=\"hljs-string\">            output_attentions (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Whether or not to return the\
          \ attentions tensors of all attention layers. See `attentions` under</span>\n\
          <span class=\"hljs-string\">                returned tensors for more detail.</span>\n\
          <span class=\"hljs-string\">            output_hidden_states (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Whether or not to return the\
          \ hidden states of all layers. See `hidden_states` under returned tensors</span>\n\
          <span class=\"hljs-string\">                for more detail.</span>\n<span\
          \ class=\"hljs-string\">            return_dict (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Whether or not to return a [`~utils.ModelOutput`]\
          \ instead of a plain tuple.</span>\n<span class=\"hljs-string\">       \
          \ \"\"\"</span>\n        output_attentions = output_attentions <span class=\"\
          hljs-keyword\">if</span> output_attentions <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> self.config.output_attentions\n\
          \        output_hidden_states = (\n            output_hidden_states <span\
          \ class=\"hljs-keyword\">if</span> output_hidden_states <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> self.config.output_hidden_states\n\
          \        )\n        return_dict = return_dict <span class=\"hljs-keyword\"\
          >if</span> return_dict <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span> <span\
          \ class=\"hljs-keyword\">else</span> self.config.use_return_dict\n     \
          \   inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n   \
          \     inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n\n\
          \        inputs_embeds = inputs_embeds.permute(<span class=\"hljs-number\"\
          >0</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\"\
          >1</span>)\n        embed_pos = self.embed_positions.weight\n\n        hidden_states\
          \ = inputs_embeds + embed_pos\n        hidden_states = nn.functional.dropout(hidden_states,\
          \ p=self.dropout, training=self.training)\n\n        encoder_states = ()\
          \ <span class=\"hljs-keyword\">if</span> output_hidden_states <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-literal\">None</span>\n  \
          \      all_attentions = () <span class=\"hljs-keyword\">if</span> output_attentions\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>\n\n        <span class=\"hljs-comment\"># check if head_mask\
          \ has a correct number of layers specified if desired</span>\n        <span\
          \ class=\"hljs-keyword\">if</span> head_mask <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span>:\n            <span class=\"hljs-keyword\">assert</span> head_mask.size()[<span\
          \ class=\"hljs-number\">0</span>] == (\n                <span class=\"hljs-built_in\"\
          >len</span>(self.layers)\n            ), <span class=\"hljs-string\">f\"\
          The head_mask should be specified for <span class=\"hljs-subst\">{<span\
          \ class=\"hljs-built_in\">len</span>(self.layers)}</span> layers, but it\
          \ is for <span class=\"hljs-subst\">{head_mask.size()[<span class=\"hljs-number\"\
          >0</span>]}</span>.\"</span>\n\n        <span class=\"hljs-keyword\">for</span>\
          \ idx, encoder_layer <span class=\"hljs-keyword\">in</span> <span class=\"\
          hljs-built_in\">enumerate</span>(self.layers):\n            <span class=\"\
          hljs-keyword\">if</span> output_hidden_states:\n                encoder_states\
          \ = encoder_states + (hidden_states,)\n            <span class=\"hljs-comment\"\
          ># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>\n\
          \            dropout_probability = random.uniform(<span class=\"hljs-number\"\
          >0</span>, <span class=\"hljs-number\">1</span>)\n            <span class=\"\
          hljs-keyword\">if</span> self.training <span class=\"hljs-keyword\">and</span>\
          \ (dropout_probability &lt; self.layerdrop):  <span class=\"hljs-comment\"\
          ># skip the layer</span>\n                layer_outputs = (<span class=\"\
          hljs-literal\">None</span>, <span class=\"hljs-literal\">None</span>)\n\
          \            <span class=\"hljs-keyword\">else</span>:\n               \
          \ <span class=\"hljs-keyword\">if</span> self.gradient_checkpointing <span\
          \ class=\"hljs-keyword\">and</span> self.training:\n\n                 \
          \   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >create_custom_forward</span>(<span class=\"hljs-params\">module</span>):\n\
          \                        <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">custom_forward</span>(<span class=\"hljs-params\"\
          >*inputs</span>):\n                            <span class=\"hljs-keyword\"\
          >return</span> module(*inputs, output_attentions)\n\n                  \
          \      <span class=\"hljs-keyword\">return</span> custom_forward\n\n   \
          \                 layer_outputs = torch.utils.checkpoint.checkpoint(\n \
          \                       create_custom_forward(encoder_layer),\n        \
          \                hidden_states,\n                        <span class=\"\
          hljs-literal\">None</span>,\n                        (head_mask[idx] <span\
          \ class=\"hljs-keyword\">if</span> head_mask <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>),\n                    )\n                <span class=\"hljs-keyword\"\
          >else</span>:\n                    layer_outputs = encoder_layer(\n    \
          \                    hidden_states,\n                        <span class=\"\
          hljs-literal\">None</span>,\n                        layer_head_mask=(head_mask[idx]\
          \ <span class=\"hljs-keyword\">if</span> head_mask <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>),\n                        output_attentions=output_attentions,\n\
          \                    )\n\n                hidden_states = layer_outputs[<span\
          \ class=\"hljs-number\">0</span>]\n\n            <span class=\"hljs-keyword\"\
          >if</span> output_attentions:\n                all_attentions = all_attentions\
          \ + (layer_outputs[<span class=\"hljs-number\">1</span>],)\n\n        hidden_states\
          \ = self.layer_norm(hidden_states)\n        <span class=\"hljs-keyword\"\
          >if</span> output_hidden_states:\n            encoder_states = encoder_states\
          \ + (hidden_states,)\n\n        <span class=\"hljs-keyword\">if</span> <span\
          \ class=\"hljs-keyword\">not</span> return_dict:\n            <span class=\"\
          hljs-keyword\">return</span> <span class=\"hljs-built_in\">tuple</span>(v\
          \ <span class=\"hljs-keyword\">for</span> v <span class=\"hljs-keyword\"\
          >in</span> [hidden_states, encoder_states, all_attentions] <span class=\"\
          hljs-keyword\">if</span> v <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>)\n\
          \        <span class=\"hljs-keyword\">return</span> BaseModelOutput(\n \
          \           last_hidden_state=hidden_states, hidden_states=encoder_states,\
          \ attentions=all_attentions\n        )\n\n\n<span class=\"hljs-comment\"\
          ># Copied from transformers.models.whisper.modeling_whisper.WhisperDecoder</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperDecoder</span>(<span class=\"hljs-title class_ inherited__\">WhisperPreTrainedModel</span>):\n\
          \    <span class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\"\
          >    Transformer decoder consisting of *config.decoder_layers* layers. Each\
          \ layer is a [`WhisperDecoderLayer`]</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">    Args:</span>\n<span class=\"hljs-string\"\
          >        config: WhisperConfig</span>\n<span class=\"hljs-string\">    \"\
          \"\"</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self,\
          \ config: WhisperConfig</span>):\n        <span class=\"hljs-built_in\"\
          >super</span>().__init__(config)\n        self.dropout = config.dropout\n\
          \        self.layerdrop = config.decoder_layerdrop\n        self.padding_idx\
          \ = config.pad_token_id\n        self.max_target_positions = config.max_target_positions\n\
          \        self.max_source_positions = config.max_source_positions\n     \
          \   self.embed_scale = math.sqrt(config.d_model) <span class=\"hljs-keyword\"\
          >if</span> config.scale_embedding <span class=\"hljs-keyword\">else</span>\
          \ <span class=\"hljs-number\">1.0</span>\n\n        self.embed_tokens =\
          \ nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n  \
          \      self.embed_positions = WhisperPositionalEmbedding(self.max_target_positions,\
          \ config.d_model)\n\n        self.layers = nn.ModuleList([WhisperDecoderLayer(config)\
          \ <span class=\"hljs-keyword\">for</span> _ <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">range</span>(config.decoder_layers)])\n\
          \n        self.layer_norm = nn.LayerNorm(config.d_model)\n\n        self.gradient_checkpointing\
          \ = <span class=\"hljs-literal\">False</span>\n        <span class=\"hljs-comment\"\
          ># Initialize weights and apply final processing</span>\n        self.post_init()\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >get_input_embeddings</span>(<span class=\"hljs-params\">self</span>):\n\
          \        <span class=\"hljs-keyword\">return</span> self.embed_tokens\n\n\
          \    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >set_input_embeddings</span>(<span class=\"hljs-params\">self, value</span>):\n\
          \        self.embed_tokens = value\n\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">_prepare_decoder_attention_mask</span>(<span\
          \ class=\"hljs-params\">self, attention_mask, input_shape, inputs_embeds,\
          \ past_key_values_length</span>):\n        <span class=\"hljs-comment\"\
          ># create causal mask</span>\n        <span class=\"hljs-comment\"># [bsz,\
          \ seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>\n        combined_attention_mask\
          \ = <span class=\"hljs-literal\">None</span>\n\n        <span class=\"hljs-keyword\"\
          >if</span> input_shape[-<span class=\"hljs-number\">1</span>] &gt; <span\
          \ class=\"hljs-number\">1</span>:\n            combined_attention_mask =\
          \ _make_causal_mask(\n                input_shape, inputs_embeds.dtype,\
          \ past_key_values_length=past_key_values_length\n            ).to(inputs_embeds.device)\n\
          \n        <span class=\"hljs-keyword\">if</span> attention_mask <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span>:\n            <span class=\"hljs-comment\">#\
          \ [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>\n     \
          \       expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype,\
          \ tgt_len=input_shape[-<span class=\"hljs-number\">1</span>])\n        \
          \    combined_attention_mask = (\n                expanded_attn_mask <span\
          \ class=\"hljs-keyword\">if</span> combined_attention_mask <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span> <span\
          \ class=\"hljs-keyword\">else</span> expanded_attn_mask + combined_attention_mask\n\
          \            )\n\n        <span class=\"hljs-keyword\">return</span> combined_attention_mask\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >forward</span>(<span class=\"hljs-params\"></span>\n<span class=\"hljs-params\"\
          >        self,</span>\n<span class=\"hljs-params\">        input_ids=<span\
          \ class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        attention_mask=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        encoder_hidden_states=<span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ head_mask=<span class=\"hljs-literal\">None</span>,</span>\n<span class=\"\
          hljs-params\">        cross_attn_head_mask=<span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        past_key_values=<span\
          \ class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        inputs_embeds=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        use_cache=<span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        output_attentions=<span\
          \ class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        output_hidden_states=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        return_dict=<span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">    </span>):\n      \
          \  <span class=\"hljs-string\">r\"\"\"</span>\n<span class=\"hljs-string\"\
          >        Args:</span>\n<span class=\"hljs-string\">            input_ids\
          \ (`torch.LongTensor` of shape `(batch_size, sequence_length)`):</span>\n\
          <span class=\"hljs-string\">                Indices of input sequence tokens\
          \ in the vocabulary. Padding will be ignored by default should you</span>\n\
          <span class=\"hljs-string\">                provide it.</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">                Indices\
          \ can be obtained using [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`]\
          \ and</span>\n<span class=\"hljs-string\">                [`PreTrainedTokenizer.__call__`]\
          \ for details.</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">                [What are input IDs?](transformers/glossary#input-ids)</span>\n\
          <span class=\"hljs-string\">            attention_mask (`torch.Tensor` of\
          \ shape `(batch_size, sequence_length)`, *optional*):</span>\n<span class=\"\
          hljs-string\">                Mask to avoid performing attention on padding\
          \ token indices. Mask values selected in `[0, 1]`:</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">                - 1 for\
          \ tokens that are **not masked**,</span>\n<span class=\"hljs-string\"> \
          \               - 0 for tokens that are **masked**.</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">                [What\
          \ are attention masks?](transformers/glossary#attention-mask)</span>\n<span\
          \ class=\"hljs-string\">            encoder_hidden_states (`torch.FloatTensor`\
          \ of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Sequence of hidden-states at\
          \ the output of the last layer of the encoder. Used in the cross-attention</span>\n\
          <span class=\"hljs-string\">                of the decoder.</span>\n<span\
          \ class=\"hljs-string\">            head_mask (`torch.Tensor` of shape `(decoder_layers,\
          \ decoder_attention_heads)`, *optional*):</span>\n<span class=\"hljs-string\"\
          >                Mask to nullify selected heads of the attention modules.\
          \ Mask values selected in `[0, 1]`:</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">                - 1 indicates the\
          \ head is **not masked**,</span>\n<span class=\"hljs-string\">         \
          \       - 0 indicates the head is **masked**.</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">            cross_attn_head_mask (`torch.Tensor`\
          \ of shape `(decoder_layers, decoder_attention_heads)`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Mask to nullify selected heads\
          \ of the attention modules in encoder to avoid performing cross-attention</span>\n\
          <span class=\"hljs-string\">                on hidden heads. Mask values\
          \ selected in `[0, 1]`:</span>\n<span class=\"hljs-string\"></span>\n<span\
          \ class=\"hljs-string\">                - 1 indicates the head is **not\
          \ masked**,</span>\n<span class=\"hljs-string\">                - 0 indicates\
          \ the head is **masked**.</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">            past_key_values (`tuple(tuple(torch.FloatTensor))`,\
          \ *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>\n\
          <span class=\"hljs-string\">                Tuple of `tuple(torch.FloatTensor)`\
          \ of length `config.n_layers`, with each tuple having 2 tensors of</span>\n\
          <span class=\"hljs-string\">                shape `(batch_size, num_heads,\
          \ sequence_length, embed_size_per_head)`) and 2 additional tensors of</span>\n\
          <span class=\"hljs-string\">                shape `(batch_size, num_heads,\
          \ encoder_sequence_length, embed_size_per_head)`.</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">                Contains\
          \ pre-computed hidden-states (key and values in the self-attention blocks\
          \ and in the</span>\n<span class=\"hljs-string\">                cross-attention\
          \ blocks) that can be used (see `past_key_values` input) to speed up sequential\
          \ decoding.</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">                If `past_key_values` are used, the user can\
          \ optionally input only the last `decoder_input_ids` (those</span>\n<span\
          \ class=\"hljs-string\">                that don't have their past key value\
          \ states given to this model) of shape `(batch_size, 1)` instead of</span>\n\
          <span class=\"hljs-string\">                all `decoder_input_ids` of shape\
          \ `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of</span>\n\
          <span class=\"hljs-string\">                shape `(batch_size, sequence_length,\
          \ hidden_size)`, *optional*): Optionally, instead of passing</span>\n<span\
          \ class=\"hljs-string\">                `input_ids` you can choose to directly\
          \ pass an embedded representation. This is useful if you want more</span>\n\
          <span class=\"hljs-string\">                control over how to convert\
          \ `input_ids` indices into associated vectors than the model's internal</span>\n\
          <span class=\"hljs-string\">                embedding lookup matrix.</span>\n\
          <span class=\"hljs-string\">            output_attentions (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Whether or not to return the\
          \ attentions tensors of all attention layers. See `attentions` under</span>\n\
          <span class=\"hljs-string\">                returned tensors for more detail.</span>\n\
          <span class=\"hljs-string\">            output_hidden_states (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Whether or not to return the\
          \ hidden states of all layers. See `hidden_states` under returned tensors</span>\n\
          <span class=\"hljs-string\">                for more detail.</span>\n<span\
          \ class=\"hljs-string\">            return_dict (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Whether or not to return a [`~utils.ModelOutput`]\
          \ instead of a plain tuple.</span>\n<span class=\"hljs-string\">       \
          \ \"\"\"</span>\n        output_attentions = output_attentions <span class=\"\
          hljs-keyword\">if</span> output_attentions <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> self.config.output_attentions\n\
          \        output_hidden_states = (\n            output_hidden_states <span\
          \ class=\"hljs-keyword\">if</span> output_hidden_states <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> self.config.output_hidden_states\n\
          \        )\n        use_cache = use_cache <span class=\"hljs-keyword\">if</span>\
          \ use_cache <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\"\
          >else</span> self.config.use_cache\n        return_dict = return_dict <span\
          \ class=\"hljs-keyword\">if</span> return_dict <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> self.config.use_return_dict\n\
          \n        <span class=\"hljs-comment\"># retrieve input_ids and inputs_embeds</span>\n\
          \        <span class=\"hljs-keyword\">if</span> input_ids <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span> <span class=\"hljs-keyword\">and</span> inputs_embeds\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span>:\n            <span class=\"\
          hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">\"You\
          \ cannot specify both decoder_input_ids and decoder_inputs_embeds at the\
          \ same time\"</span>)\n        <span class=\"hljs-keyword\">elif</span>\
          \ input_ids <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-literal\">None</span>:\n            input_shape\
          \ = input_ids.size()\n            input_ids = input_ids.view(-<span class=\"\
          hljs-number\">1</span>, input_shape[-<span class=\"hljs-number\">1</span>])\n\
          \        <span class=\"hljs-keyword\">elif</span> inputs_embeds <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span>:\n            input_shape = inputs_embeds.size()[:-<span\
          \ class=\"hljs-number\">1</span>]\n        <span class=\"hljs-keyword\"\
          >else</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span\
          \ class=\"hljs-string\">\"You have to specify either decoder_input_ids or\
          \ decoder_inputs_embeds\"</span>)\n\n        <span class=\"hljs-comment\"\
          ># past_key_values_length</span>\n        past_key_values_length = past_key_values[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-number\">0</span>].shape[<span\
          \ class=\"hljs-number\">2</span>] <span class=\"hljs-keyword\">if</span>\
          \ past_key_values <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span> <span\
          \ class=\"hljs-keyword\">else</span> <span class=\"hljs-number\">0</span>\n\
          \n        <span class=\"hljs-keyword\">if</span> inputs_embeds <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n   \
          \         inputs_embeds = self.embed_tokens(input_ids)\n\n        attention_mask\
          \ = self._prepare_decoder_attention_mask(\n            attention_mask, input_shape,\
          \ inputs_embeds, past_key_values_length\n        )\n\n        <span class=\"\
          hljs-comment\"># embed positions</span>\n        positions = self.embed_positions(input_ids,\
          \ past_key_values_length=past_key_values_length)\n\n        hidden_states\
          \ = inputs_embeds + positions\n        hidden_states = nn.functional.dropout(hidden_states,\
          \ p=self.dropout, training=self.training)\n\n        <span class=\"hljs-comment\"\
          ># decoder layers</span>\n        all_hidden_states = () <span class=\"\
          hljs-keyword\">if</span> output_hidden_states <span class=\"hljs-keyword\"\
          >else</span> <span class=\"hljs-literal\">None</span>\n        all_self_attns\
          \ = () <span class=\"hljs-keyword\">if</span> output_attentions <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-literal\">None</span>\n  \
          \      all_cross_attentions = () <span class=\"hljs-keyword\">if</span>\
          \ (output_attentions <span class=\"hljs-keyword\">and</span> encoder_hidden_states\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span>) <span class=\"hljs-keyword\"\
          >else</span> <span class=\"hljs-literal\">None</span>\n        next_decoder_cache\
          \ = () <span class=\"hljs-keyword\">if</span> use_cache <span class=\"hljs-keyword\"\
          >else</span> <span class=\"hljs-literal\">None</span>\n\n        <span class=\"\
          hljs-comment\"># check if head_mask/cross_attn_head_mask has a correct number\
          \ of layers specified if desired</span>\n        <span class=\"hljs-keyword\"\
          >for</span> attn_mask, mask_name <span class=\"hljs-keyword\">in</span>\
          \ <span class=\"hljs-built_in\">zip</span>([head_mask, cross_attn_head_mask],\
          \ [<span class=\"hljs-string\">\"head_mask\"</span>, <span class=\"hljs-string\"\
          >\"cross_attn_head_mask\"</span>]):\n            <span class=\"hljs-keyword\"\
          >if</span> attn_mask <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n  \
          \              <span class=\"hljs-keyword\">assert</span> attn_mask.size()[<span\
          \ class=\"hljs-number\">0</span>] == (<span class=\"hljs-built_in\">len</span>(self.layers)),\
          \ (\n                    <span class=\"hljs-string\">f\"The `<span class=\"\
          hljs-subst\">{mask_name}</span>` should be specified for <span class=\"\
          hljs-subst\">{<span class=\"hljs-built_in\">len</span>(self.layers)}</span>\
          \ layers, but it is for\"</span>\n                    <span class=\"hljs-string\"\
          >f\" <span class=\"hljs-subst\">{head_mask.size()[<span class=\"hljs-number\"\
          >0</span>]}</span>.\"</span>\n                )\n        <span class=\"\
          hljs-keyword\">for</span> idx, decoder_layer <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">enumerate</span>(self.layers):\n\
          \            <span class=\"hljs-comment\"># add LayerDrop (see https://arxiv.org/abs/1909.11556\
          \ for description)</span>\n            <span class=\"hljs-keyword\">if</span>\
          \ output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\
          \            dropout_probability = random.uniform(<span class=\"hljs-number\"\
          >0</span>, <span class=\"hljs-number\">1</span>)\n            <span class=\"\
          hljs-keyword\">if</span> self.training <span class=\"hljs-keyword\">and</span>\
          \ (dropout_probability &lt; self.layerdrop):\n                <span class=\"\
          hljs-keyword\">continue</span>\n\n            past_key_value = past_key_values[idx]\
          \ <span class=\"hljs-keyword\">if</span> past_key_values <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span> <span class=\"hljs-keyword\">else</span> <span\
          \ class=\"hljs-literal\">None</span>\n\n            <span class=\"hljs-keyword\"\
          >if</span> self.gradient_checkpointing <span class=\"hljs-keyword\">and</span>\
          \ self.training:\n                <span class=\"hljs-keyword\">if</span>\
          \ use_cache:\n                    logger.warning(\n                    \
          \    <span class=\"hljs-string\">\"`use_cache = True` is incompatible with\
          \ gradient checkpointing. Setting `use_cache =\"</span>\n              \
          \          <span class=\"hljs-string\">\" False`transformers.\"</span>\n\
          \                    )\n                    use_cache = <span class=\"hljs-literal\"\
          >False</span>\n\n                <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">create_custom_forward</span>(<span\
          \ class=\"hljs-params\">module</span>):\n                    <span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">custom_forward</span>(<span\
          \ class=\"hljs-params\">*inputs</span>):\n                        <span\
          \ class=\"hljs-comment\"># None for past_key_value</span>\n            \
          \            <span class=\"hljs-keyword\">return</span> module(*inputs,\
          \ output_attentions, use_cache)\n\n                    <span class=\"hljs-keyword\"\
          >return</span> custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n\
          \                    create_custom_forward(decoder_layer),\n           \
          \         hidden_states,\n                    attention_mask,\n        \
          \            encoder_hidden_states,\n                    <span class=\"\
          hljs-literal\">None</span>,  <span class=\"hljs-comment\"># encoder attention\
          \ mask</span>\n                    head_mask[idx] <span class=\"hljs-keyword\"\
          >if</span> head_mask <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span> <span\
          \ class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\">None</span>,\n\
          \                    cross_attn_head_mask[idx] <span class=\"hljs-keyword\"\
          >if</span> cross_attn_head_mask <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>,\n                    <span class=\"hljs-literal\">None</span>,\
          \  <span class=\"hljs-comment\"># past_key_value</span>\n              \
          \  )\n            <span class=\"hljs-keyword\">else</span>:\n          \
          \      layer_outputs = decoder_layer(\n                    hidden_states,\n\
          \                    attention_mask=attention_mask,\n                  \
          \  encoder_hidden_states=encoder_hidden_states,\n                    layer_head_mask=(head_mask[idx]\
          \ <span class=\"hljs-keyword\">if</span> head_mask <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>),\n                    cross_attn_layer_head_mask=(\n     \
          \                   cross_attn_head_mask[idx] <span class=\"hljs-keyword\"\
          >if</span> cross_attn_head_mask <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>\n                    ),\n                    past_key_value=past_key_value,\n\
          \                    output_attentions=output_attentions,\n            \
          \        use_cache=use_cache,\n                )\n            hidden_states\
          \ = layer_outputs[<span class=\"hljs-number\">0</span>]\n\n            <span\
          \ class=\"hljs-keyword\">if</span> use_cache:\n                next_decoder_cache\
          \ += (layer_outputs[<span class=\"hljs-number\">3</span> <span class=\"\
          hljs-keyword\">if</span> output_attentions <span class=\"hljs-keyword\"\
          >else</span> <span class=\"hljs-number\">1</span>],)\n\n            <span\
          \ class=\"hljs-keyword\">if</span> output_attentions:\n                all_self_attns\
          \ += (layer_outputs[<span class=\"hljs-number\">1</span>],)\n\n        \
          \        <span class=\"hljs-keyword\">if</span> encoder_hidden_states <span\
          \ class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span>:\n                    all_cross_attentions\
          \ += (layer_outputs[<span class=\"hljs-number\">2</span>],)\n\n        hidden_states\
          \ = self.layer_norm(hidden_states)\n        <span class=\"hljs-comment\"\
          ># add hidden states from the last decoder layer</span>\n        <span class=\"\
          hljs-keyword\">if</span> output_hidden_states:\n            all_hidden_states\
          \ += (hidden_states,)\n\n        next_cache = next_decoder_cache <span class=\"\
          hljs-keyword\">if</span> use_cache <span class=\"hljs-keyword\">else</span>\
          \ <span class=\"hljs-literal\">None</span>\n        <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-keyword\">not</span> return_dict:\n      \
          \      <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\"\
          >tuple</span>(\n                v\n                <span class=\"hljs-keyword\"\
          >for</span> v <span class=\"hljs-keyword\">in</span> [hidden_states, next_cache,\
          \ all_hidden_states, all_self_attns, all_cross_attentions]\n           \
          \     <span class=\"hljs-keyword\">if</span> v <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span>\n            )\n        <span class=\"hljs-keyword\">return</span>\
          \ BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n\
          \            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n\
          \            attentions=all_self_attns,\n            cross_attentions=all_cross_attentions,\n\
          \        )\n\n\n<span class=\"hljs-meta\">@add_start_docstrings(<span class=\"\
          hljs-params\"></span></span>\n<span class=\"hljs-meta\"><span class=\"hljs-params\"\
          >    <span class=\"hljs-string\">\"The bare Whisper Model outputting raw\
          \ hidden-states without any specific head on top.\"</span>,</span></span>\n\
          <span class=\"hljs-meta\"><span class=\"hljs-params\">    WHISPER_START_DOCSTRING,</span></span>\n\
          <span class=\"hljs-meta\"><span class=\"hljs-params\"></span>)</span>\n\
          <span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperModel</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperModel</span>(<span class=\"hljs-title class_ inherited__\">WhisperPreTrainedModel</span>):\n\
          \    _keys_to_ignore_on_load_missing = [<span class=\"hljs-string\">r\"\
          proj_out.weight\"</span>]\n\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\"\
          >self, config: WhisperConfig</span>):\n        <span class=\"hljs-built_in\"\
          >super</span>().__init__(config)\n\n        self.encoder = WhisperEncoder(config)\n\
          \        self.decoder = WhisperDecoder(config)\n        <span class=\"hljs-comment\"\
          ># Initialize weights and apply final processing</span>\n        self.post_init()\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >get_input_embeddings</span>(<span class=\"hljs-params\">self</span>):\n\
          \        <span class=\"hljs-keyword\">return</span> self.decoder.embed_tokens\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >set_input_embeddings</span>(<span class=\"hljs-params\">self, value</span>):\n\
          \        self.decoder.embed_tokens = value\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">get_encoder</span>(<span\
          \ class=\"hljs-params\">self</span>):\n        <span class=\"hljs-keyword\"\
          >return</span> self.encoder\n\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">get_decoder</span>(<span class=\"\
          hljs-params\">self</span>):\n        <span class=\"hljs-keyword\">return</span>\
          \ self.decoder\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">freeze_encoder</span>(<span class=\"hljs-params\"\
          >self</span>):\n        <span class=\"hljs-string\">\"\"\"</span>\n<span\
          \ class=\"hljs-string\">        Calling this function will disable the gradient\
          \ computation for the Whisper encoder so that its parameters will</span>\n\
          <span class=\"hljs-string\">        not be updated during training.</span>\n\
          <span class=\"hljs-string\">        \"\"\"</span>\n        self.encoder._freeze_parameters()\n\
          \n<span class=\"hljs-meta\">    @add_start_docstrings_to_model_forward(<span\
          \ class=\"hljs-params\">WHISPER_INPUTS_DOCSTRING</span>)</span>\n<span class=\"\
          hljs-meta\">    @replace_return_docstrings(<span class=\"hljs-params\">output_type=Seq2SeqModelOutput,\
          \ config_class=_CONFIG_FOR_DOC</span>)</span>\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"\
          hljs-params\"></span>\n<span class=\"hljs-params\">        self,</span>\n\
          <span class=\"hljs-params\">        input_features: <span class=\"hljs-type\"\
          >Optional</span>[torch.FloatTensor] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        attention_mask: <span class=\"hljs-type\"\
          >Optional</span>[torch.LongTensor] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        decoder_input_ids: <span class=\"hljs-type\"\
          >Optional</span>[torch.LongTensor] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        decoder_attention_mask: <span class=\"\
          hljs-type\">Optional</span>[torch.LongTensor] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        head_mask: <span\
          \ class=\"hljs-type\">Optional</span>[torch.Tensor] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        decoder_head_mask:\
          \ <span class=\"hljs-type\">Optional</span>[torch.Tensor] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ cross_attn_head_mask: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        encoder_outputs: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[<span class=\"hljs-type\">Tuple</span>[torch.FloatTensor]]]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        past_key_values: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[<span class=\"hljs-type\">Tuple</span>[torch.FloatTensor]]]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        decoder_inputs_embeds: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[torch.FloatTensor]] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ use_cache: <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-built_in\"\
          >bool</span>] = <span class=\"hljs-literal\">None</span>,</span>\n<span\
          \ class=\"hljs-params\">        output_attentions: <span class=\"hljs-type\"\
          >Optional</span>[<span class=\"hljs-built_in\">bool</span>] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ output_hidden_states: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-built_in\">bool</span>] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        return_dict: <span class=\"hljs-type\"\
          >Optional</span>[<span class=\"hljs-built_in\">bool</span>] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">    </span>)\
          \ -&gt; <span class=\"hljs-type\">Union</span>[<span class=\"hljs-type\"\
          >Tuple</span>[torch.Tensor], Seq2SeqModelOutput]:\n        <span class=\"\
          hljs-string\">r\"\"\"</span>\n<span class=\"hljs-string\">        Returns:</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \  Example:</span>\n<span class=\"hljs-string\">         ```python</span>\n\
          <span class=\"hljs-string\">         &gt;&gt;&gt; import torch</span>\n\
          <span class=\"hljs-string\">         &gt;&gt;&gt; from transformers import\
          \ AutoFeatureExtractor, WhisperModel</span>\n<span class=\"hljs-string\"\
          >         &gt;&gt;&gt; from datasets import load_dataset</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">         &gt;&gt;&gt;\
          \ model = WhisperModel.from_pretrained(\"openai/whisper-base\")</span>\n\
          <span class=\"hljs-string\">         &gt;&gt;&gt; feature_extractor = AutoFeatureExtractor.from_pretrained(\"\
          openai/whisper-base\")</span>\n<span class=\"hljs-string\">         &gt;&gt;&gt;\
          \ ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\"\
          , split=\"validation\")</span>\n<span class=\"hljs-string\">         &gt;&gt;&gt;\
          \ inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"\
          pt\")</span>\n<span class=\"hljs-string\">         &gt;&gt;&gt; input_features\
          \ = inputs.input_features</span>\n<span class=\"hljs-string\">         &gt;&gt;&gt;\
          \ decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id</span>\n\
          <span class=\"hljs-string\">         &gt;&gt;&gt; last_hidden_state = model(input_features,\
          \ decoder_input_ids=decoder_input_ids).last_hidden_state</span>\n<span class=\"\
          hljs-string\">         &gt;&gt;&gt; list(last_hidden_state.shape)</span>\n\
          <span class=\"hljs-string\">         [1, 2, 512]</span>\n<span class=\"\
          hljs-string\">         ```\"\"\"</span>\n        output_attentions = output_attentions\
          \ <span class=\"hljs-keyword\">if</span> output_attentions <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span> <span class=\"hljs-keyword\">else</span> self.config.output_attentions\n\
          \        output_hidden_states = (\n            output_hidden_states <span\
          \ class=\"hljs-keyword\">if</span> output_hidden_states <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> self.config.output_hidden_states\n\
          \        )\n        use_cache = use_cache <span class=\"hljs-keyword\">if</span>\
          \ use_cache <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\"\
          >else</span> self.config.use_cache\n        return_dict = return_dict <span\
          \ class=\"hljs-keyword\">if</span> return_dict <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span> <span class=\"hljs-keyword\">else</span> self.config.use_return_dict\n\
          \n        <span class=\"hljs-keyword\">if</span> encoder_outputs <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n   \
          \         encoder_outputs = self.encoder(\n                input_features,\n\
          \                head_mask=head_mask,\n                output_attentions=output_attentions,\n\
          \                output_hidden_states=output_hidden_states,\n          \
          \      return_dict=return_dict,\n            )\n        <span class=\"hljs-comment\"\
          ># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput\
          \ when return_dict=True</span>\n        <span class=\"hljs-keyword\">elif</span>\
          \ return_dict <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-built_in\">isinstance</span>(encoder_outputs,\
          \ BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n  \
          \              last_hidden_state=encoder_outputs[<span class=\"hljs-number\"\
          >0</span>],\n                hidden_states=encoder_outputs[<span class=\"\
          hljs-number\">1</span>] <span class=\"hljs-keyword\">if</span> <span class=\"\
          hljs-built_in\">len</span>(encoder_outputs) &gt; <span class=\"hljs-number\"\
          >1</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>,\n                attentions=encoder_outputs[<span class=\"\
          hljs-number\">2</span>] <span class=\"hljs-keyword\">if</span> <span class=\"\
          hljs-built_in\">len</span>(encoder_outputs) &gt; <span class=\"hljs-number\"\
          >2</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-literal\"\
          >None</span>,\n            )\n\n        <span class=\"hljs-comment\"># decoder\
          \ outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>\n\
          \        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n\
          \            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[<span\
          \ class=\"hljs-number\">0</span>],\n            head_mask=decoder_head_mask,\n\
          \            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n\
          \            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n\
          \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
          \            return_dict=return_dict,\n        )\n\n        <span class=\"\
          hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> return_dict:\n\
          \            <span class=\"hljs-keyword\">return</span> decoder_outputs\
          \ + encoder_outputs\n\n        <span class=\"hljs-keyword\">return</span>\
          \ Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n\
          \            past_key_values=decoder_outputs.past_key_values,\n        \
          \    decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n\
          \            cross_attentions=decoder_outputs.cross_attentions,\n      \
          \      encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n  \
          \          encoder_hidden_states=encoder_outputs.hidden_states,\n      \
          \      encoder_attentions=encoder_outputs.attentions,\n        )\n\n\n<span\
          \ class=\"hljs-meta\">@add_start_docstrings(<span class=\"hljs-params\"\
          ></span></span>\n<span class=\"hljs-meta\"><span class=\"hljs-params\">\
          \    <span class=\"hljs-string\">\"The Whisper Model with a language modeling\
          \ head. Can be used for automatic speech recognition.\"</span>,</span></span>\n\
          <span class=\"hljs-meta\"><span class=\"hljs-params\">    WHISPER_START_DOCSTRING,</span></span>\n\
          <span class=\"hljs-meta\"><span class=\"hljs-params\"></span>)</span>\n\
          <span class=\"hljs-comment\"># Copied from transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >WhisperBnbForConditionalGeneration</span>(<span class=\"hljs-title class_\
          \ inherited__\">WhisperPreTrainedModel</span>):\n    base_model_prefix =\
          \ <span class=\"hljs-string\">\"model\"</span>\n    _keys_to_ignore_on_load_missing\
          \ = [\n        <span class=\"hljs-string\">r\"encoder.version\"</span>,\n\
          \        <span class=\"hljs-string\">r\"decoder.version\"</span>,\n    \
          \    <span class=\"hljs-string\">r\"proj_out.weight\"</span>,\n    ]\n \
          \   _keys_to_ignore_on_save = [\n        <span class=\"hljs-string\">r\"\
          proj_out.weight\"</span>,\n    ]\n\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\"\
          >self, config: WhisperConfig</span>):\n        <span class=\"hljs-built_in\"\
          >super</span>().__init__(config)\n        self.model = WhisperModel(config)\n\
          \        self.proj_out = nn.Linear(config.d_model, config.vocab_size, bias=<span\
          \ class=\"hljs-literal\">False</span>)\n\n        <span class=\"hljs-comment\"\
          ># Initialize weights and apply final processing</span>\n        self.post_init()\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >get_encoder</span>(<span class=\"hljs-params\">self</span>):\n        <span\
          \ class=\"hljs-keyword\">return</span> self.model.get_encoder()\n\n    <span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >get_decoder</span>(<span class=\"hljs-params\">self</span>):\n        <span\
          \ class=\"hljs-keyword\">return</span> self.model.get_decoder()\n\n    <span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >resize_token_embeddings</span>(<span class=\"hljs-params\">self, new_num_tokens:\
          \ <span class=\"hljs-built_in\">int</span></span>) -&gt; nn.Embedding:\n\
          \        new_embeddings = <span class=\"hljs-built_in\">super</span>().resize_token_embeddings(new_num_tokens)\n\
          \        <span class=\"hljs-keyword\">return</span> new_embeddings\n\n \
          \   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >get_output_embeddings</span>(<span class=\"hljs-params\">self</span>):\n\
          \        <span class=\"hljs-keyword\">return</span> self.proj_out\n\n  \
          \  <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >set_output_embeddings</span>(<span class=\"hljs-params\">self, new_embeddings</span>):\n\
          \        self.proj_out = new_embeddings\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">freeze_encoder</span>(<span\
          \ class=\"hljs-params\">self</span>):\n        <span class=\"hljs-string\"\
          >\"\"\"</span>\n<span class=\"hljs-string\">        Calling this function\
          \ will disable the gradient computation for the Whisper encoder so that\
          \ its parameters will</span>\n<span class=\"hljs-string\">        not be\
          \ updated during training.</span>\n<span class=\"hljs-string\">        \"\
          \"\"</span>\n        self.model.encoder._freeze_parameters()\n\n<span class=\"\
          hljs-meta\">    @add_start_docstrings_to_model_forward(<span class=\"hljs-params\"\
          >WHISPER_INPUTS_DOCSTRING</span>)</span>\n<span class=\"hljs-meta\">   \
          \ @replace_return_docstrings(<span class=\"hljs-params\">output_type=Seq2SeqLMOutput,\
          \ config_class=_CONFIG_FOR_DOC</span>)</span>\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"\
          hljs-params\"></span>\n<span class=\"hljs-params\">        self,</span>\n\
          <span class=\"hljs-params\">        input_features: <span class=\"hljs-type\"\
          >Optional</span>[torch.FloatTensor] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        attention_mask: <span class=\"hljs-type\"\
          >Optional</span>[torch.LongTensor] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        decoder_input_ids: <span class=\"hljs-type\"\
          >Optional</span>[torch.LongTensor] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        decoder_attention_mask: <span class=\"\
          hljs-type\">Optional</span>[torch.LongTensor] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        head_mask: <span\
          \ class=\"hljs-type\">Optional</span>[torch.Tensor] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        decoder_head_mask:\
          \ <span class=\"hljs-type\">Optional</span>[torch.Tensor] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ cross_attn_head_mask: <span class=\"hljs-type\">Optional</span>[torch.Tensor]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        encoder_outputs: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[<span class=\"hljs-type\">Tuple</span>[torch.FloatTensor]]]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        past_key_values: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[<span class=\"hljs-type\">Tuple</span>[torch.FloatTensor]]]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        decoder_inputs_embeds: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-type\">Tuple</span>[torch.FloatTensor]] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ labels: <span class=\"hljs-type\">Optional</span>[torch.LongTensor] =\
          \ <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        use_cache: <span class=\"hljs-type\">Optional</span>[<span class=\"\
          hljs-built_in\">bool</span>] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        output_attentions: <span class=\"hljs-type\"\
          >Optional</span>[<span class=\"hljs-built_in\">bool</span>] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ output_hidden_states: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-built_in\">bool</span>] = <span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        return_dict: <span class=\"hljs-type\"\
          >Optional</span>[<span class=\"hljs-built_in\">bool</span>] = <span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">    </span>)\
          \ -&gt; <span class=\"hljs-type\">Union</span>[<span class=\"hljs-type\"\
          >Tuple</span>[torch.Tensor], Seq2SeqLMOutput]:\n        <span class=\"hljs-string\"\
          >r\"\"\"</span>\n<span class=\"hljs-string\">        labels (`torch.LongTensor`\
          \ of shape `(batch_size, sequence_length)`, *optional*):</span>\n<span class=\"\
          hljs-string\">            Labels for computing the language modeling loss.\
          \ Indices should either be in `[0, transformers., config.vocab_size]`</span>\n\
          <span class=\"hljs-string\">            or -100 (see `input_ids` docstring).\
          \ Tokens with indices set to `-100` are ignored (masked), the loss is</span>\n\
          <span class=\"hljs-string\">            only computed for the tokens with\
          \ labels in `[0, transformers., config.vocab_size]`.</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">        Returns:</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \  Example:</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">        ```python</span>\n<span class=\"hljs-string\">   \
          \     &gt;&gt;&gt; import torch</span>\n<span class=\"hljs-string\">   \
          \     &gt;&gt;&gt; from transformers import AutoProcessor, WhisperForConditionalGeneration</span>\n\
          <span class=\"hljs-string\">        &gt;&gt;&gt; from datasets import load_dataset</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \  &gt;&gt;&gt; processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\"\
          )</span>\n<span class=\"hljs-string\">        &gt;&gt;&gt; model = WhisperForConditionalGeneration.from_pretrained(\"\
          openai/whisper-tiny.en\")</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">        &gt;&gt;&gt; ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\")</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">        &gt;&gt;&gt; inputs = processor(ds[0][\"\
          audio\"][\"array\"], return_tensors=\"pt\")</span>\n<span class=\"hljs-string\"\
          >        &gt;&gt;&gt; input_features = inputs.input_features</span>\n<span\
          \ class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        &gt;&gt;&gt;\
          \ generated_ids = model.generate(inputs=input_features)</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">        &gt;&gt;&gt;\
          \ transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]</span>\n\
          <span class=\"hljs-string\">        &gt;&gt;&gt; transcription</span>\n\
          <span class=\"hljs-string\">        ' Mr. Quilter is the apostle of the\
          \ middle classes, and we are glad to welcome his gospel.'</span>\n<span\
          \ class=\"hljs-string\">        ```\"\"\"</span>\n        return_dict =\
          \ return_dict <span class=\"hljs-keyword\">if</span> return_dict <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span> <span class=\"hljs-keyword\">else</span> self.config.use_return_dict\n\
          \n        <span class=\"hljs-keyword\">if</span> labels <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span>:\n            <span class=\"hljs-keyword\">if</span> decoder_input_ids\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>\
          \ <span class=\"hljs-keyword\">and</span> decoder_inputs_embeds <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n   \
          \             decoder_input_ids = shift_tokens_right(\n                \
          \    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n\
          \                )\n\n        outputs = self.model(\n            input_features,\n\
          \            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n\
          \            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n\
          \            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n\
          \            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n\
          \            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n\
          \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
          \            return_dict=return_dict,\n        )\n        lm_logits = self.proj_out(outputs[<span\
          \ class=\"hljs-number\">0</span>])\n\n        loss = <span class=\"hljs-literal\"\
          >None</span>\n        <span class=\"hljs-keyword\">if</span> labels <span\
          \ class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span>:\n            loss_fct = CrossEntropyLoss()\n\
          \            loss = loss_fct(lm_logits.view(-<span class=\"hljs-number\"\
          >1</span>, self.config.vocab_size), labels.reshape(-<span class=\"hljs-number\"\
          >1</span>))\n\n        <span class=\"hljs-keyword\">if</span> <span class=\"\
          hljs-keyword\">not</span> return_dict:\n            output = (lm_logits,)\
          \ + outputs[<span class=\"hljs-number\">1</span>:]\n            <span class=\"\
          hljs-keyword\">return</span> ((loss,) + output) <span class=\"hljs-keyword\"\
          >if</span> loss <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\"\
          >else</span> output\n\n        <span class=\"hljs-keyword\">return</span>\
          \ Seq2SeqLMOutput(\n            loss=loss,\n            logits=lm_logits,\n\
          \            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n\
          \            decoder_attentions=outputs.decoder_attentions,\n          \
          \  cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n\
          \            encoder_hidden_states=outputs.encoder_hidden_states,\n    \
          \        encoder_attentions=outputs.encoder_attentions,\n        )\n\n \
          \   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >generate</span>(<span class=\"hljs-params\"></span>\n<span class=\"hljs-params\"\
          >        self,</span>\n<span class=\"hljs-params\">        inputs: <span\
          \ class=\"hljs-type\">Optional</span>[torch.Tensor] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        generation_config=<span\
          \ class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        logits_processor=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        stopping_criteria=<span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        prefix_allowed_tokens_fn=<span\
          \ class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        synced_gpus=<span class=\"hljs-literal\">False</span>,</span>\n\
          <span class=\"hljs-params\">        return_timestamps=<span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        task=<span class=\"\
          hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\">       \
          \ language=<span class=\"hljs-literal\">None</span>,</span>\n<span class=\"\
          hljs-params\">        is_multilingual=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        **kwargs,</span>\n<span class=\"hljs-params\"\
          >    </span>):\n        <span class=\"hljs-string\">\"\"\"</span>\n<span\
          \ class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Generates\
          \ sequences of token ids for models with a language modeling head.</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \  &lt;Tip warning={true}&gt;</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">        Most generation-controlling parameters\
          \ are set in `generation_config` which, if not passed, will be set to the</span>\n\
          <span class=\"hljs-string\">        model's default generation configuration.\
          \ You can override any `generation_config` by passing the corresponding</span>\n\
          <span class=\"hljs-string\">        parameters to generate(), e.g. `.generate(inputs,\
          \ num_beams=4, do_sample=True)`.</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">        For an overview of generation strategies\
          \ and code examples, check out the [following</span>\n<span class=\"hljs-string\"\
          >        guide](./generation_strategies).</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">        &lt;/Tip&gt;</span>\n<span\
          \ class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Parameters:</span>\n\
          <span class=\"hljs-string\">            inputs (`torch.Tensor` of varying\
          \ shape depending on the modality, *optional*):</span>\n<span class=\"hljs-string\"\
          >                The sequence used as a prompt for the generation or as\
          \ model inputs to the encoder. If `None` the</span>\n<span class=\"hljs-string\"\
          >                method initializes it with `bos_token_id` and a batch size\
          \ of 1. For decoder-only models `inputs`</span>\n<span class=\"hljs-string\"\
          >                should of in the format of `input_ids`. For encoder-decoder\
          \ models *inputs* can represent any of</span>\n<span class=\"hljs-string\"\
          >                `input_ids`, `input_values`, `input_features`, or `pixel_values`.</span>\n\
          <span class=\"hljs-string\">            generation_config (`~generation.GenerationConfig`,\
          \ *optional*):</span>\n<span class=\"hljs-string\">                The generation\
          \ configuration to be used as base parametrization for the generation call.\
          \ `**kwargs`</span>\n<span class=\"hljs-string\">                passed\
          \ to generate matching the attributes of `generation_config` will override\
          \ them. If</span>\n<span class=\"hljs-string\">                `generation_config`\
          \ is not provided, the default will be used, which had the following loading</span>\n\
          <span class=\"hljs-string\">                priority: 1) from the `generation_config.json`\
          \ model file, if it exists; 2) from the model</span>\n<span class=\"hljs-string\"\
          >                configuration. Please note that unspecified parameters\
          \ will inherit [`~generation.GenerationConfig`]'s</span>\n<span class=\"\
          hljs-string\">                default values, whose documentation should\
          \ be checked to parameterize generation.</span>\n<span class=\"hljs-string\"\
          >            logits_processor (`LogitsProcessorList`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Custom logits processors that\
          \ complement the default logits processors built from arguments and</span>\n\
          <span class=\"hljs-string\">                generation config. If a logit\
          \ processor is passed that is already created with the arguments or a</span>\n\
          <span class=\"hljs-string\">                generation config an error is\
          \ thrown. This feature is intended for advanced users.</span>\n<span class=\"\
          hljs-string\">            stopping_criteria (`StoppingCriteriaList`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Custom stopping criteria that\
          \ complement the default stopping criteria built from arguments and a</span>\n\
          <span class=\"hljs-string\">                generation config. If a stopping\
          \ criteria is passed that is already created with the arguments or a</span>\n\
          <span class=\"hljs-string\">                generation config an error is\
          \ thrown. This feature is intended for advanced users.</span>\n<span class=\"\
          hljs-string\">            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor],\
          \ List[int]]`, *optional*):</span>\n<span class=\"hljs-string\">       \
          \         If provided, this function constraints the beam search to allowed\
          \ tokens only at each step. If not</span>\n<span class=\"hljs-string\">\
          \                provided no constraint is applied. This function takes\
          \ 2 arguments: the batch ID `batch_id` and</span>\n<span class=\"hljs-string\"\
          >                `input_ids`. It has to return a list with the allowed tokens\
          \ for the next generation step conditioned</span>\n<span class=\"hljs-string\"\
          >                on the batch ID `batch_id` and the previously generated\
          \ tokens `inputs_ids`. This argument is useful</span>\n<span class=\"hljs-string\"\
          >                for constrained generation conditioned on the prefix, as\
          \ described in [Autoregressive Entity</span>\n<span class=\"hljs-string\"\
          >                Retrieval](https://arxiv.org/abs/2010.00904).</span>\n\
          <span class=\"hljs-string\">            synced_gpus (`bool`, *optional*,\
          \ defaults to `False`):</span>\n<span class=\"hljs-string\">           \
          \     Whether to continue running the while loop until max_length (needed\
          \ for ZeRO stage 3)</span>\n<span class=\"hljs-string\">            return_timestamps\
          \ (`bool`, *optional*):</span>\n<span class=\"hljs-string\">           \
          \     Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.</span>\n\
          <span class=\"hljs-string\">            task (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Task to use for generation,\
          \ either \"translate\" or \"transcribe\". The `model.config.forced_decoder_ids`</span>\n\
          <span class=\"hljs-string\">                will be updated accordingly.</span>\n\
          <span class=\"hljs-string\">            language (`bool`, *optional*):</span>\n\
          <span class=\"hljs-string\">                Language token to use for generation,\
          \ should be in the form `&lt;|en|&gt;`. You can find all the possible</span>\n\
          <span class=\"hljs-string\">                language tokens in the `model.generation_config.lang_to_id`\
          \ dictionary.</span>\n<span class=\"hljs-string\">            is_multilingual\
          \ (`bool`, *optional*):</span>\n<span class=\"hljs-string\">           \
          \     Whether or not the model is multilingual.</span>\n<span class=\"hljs-string\"\
          >            kwargs:</span>\n<span class=\"hljs-string\">              \
          \  Ad hoc parametrization of `generate_config` and/or additional model-specific\
          \ kwargs that will be</span>\n<span class=\"hljs-string\">             \
          \   forwarded to the `forward` function of the model. If the model is an\
          \ encoder-decoder model, encoder</span>\n<span class=\"hljs-string\">  \
          \              specific kwargs should not be prefixed and decoder specific\
          \ kwargs should be prefixed with *decoder_*.</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">        Return:</span>\n<span class=\"\
          hljs-string\">            [`~utils.ModelOutput`] or `torch.LongTensor`:\
          \ A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>\n<span\
          \ class=\"hljs-string\">            or when `config.return_dict_in_generate=True`)\
          \ or a `torch.FloatTensor`.</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">                If the model is *not* an encoder-decoder\
          \ model (`model.config.is_encoder_decoder=False`), the possible</span>\n\
          <span class=\"hljs-string\">                [`~utils.ModelOutput`] types\
          \ are:</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"\
          >                    - [`~generation.GreedySearchDecoderOnlyOutput`],</span>\n\
          <span class=\"hljs-string\">                    - [`~generation.SampleDecoderOnlyOutput`],</span>\n\
          <span class=\"hljs-string\">                    - [`~generation.BeamSearchDecoderOnlyOutput`],</span>\n\
          <span class=\"hljs-string\">                    - [`~generation.BeamSampleDecoderOnlyOutput`]</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">      \
          \          If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),\
          \ the possible</span>\n<span class=\"hljs-string\">                [`~utils.ModelOutput`]\
          \ types are:</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">                    - [`~generation.GreedySearchEncoderDecoderOutput`],</span>\n\
          <span class=\"hljs-string\">                    - [`~generation.SampleEncoderDecoderOutput`],</span>\n\
          <span class=\"hljs-string\">                    - [`~generation.BeamSearchEncoderDecoderOutput`],</span>\n\
          <span class=\"hljs-string\">                    - [`~generation.BeamSampleEncoderDecoderOutput`]</span>\n\
          <span class=\"hljs-string\">        \"\"\"</span>\n        <span class=\"\
          hljs-keyword\">if</span> generation_config <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-literal\">None</span>:\n            generation_config\
          \ = self.generation_config\n\n        <span class=\"hljs-keyword\">if</span>\
          \ return_timestamps <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n  \
          \          generation_config.return_timestamps = return_timestamps\n\n \
          \       <span class=\"hljs-keyword\">if</span> task <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span>:\n            generation_config.task = task\n\n        <span\
          \ class=\"hljs-keyword\">if</span> is_multilingual <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span>:\n            generation_config.is_multilingual = is_multilingual\n\
          \n        <span class=\"hljs-keyword\">if</span> language <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span>:\n            generation_config.language = language\n\
          \n        forced_decoder_ids = []\n\n        <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">hasattr</span>(generation_config,\
          \ <span class=\"hljs-string\">\"is_multilingual\"</span>) <span class=\"\
          hljs-keyword\">and</span> generation_config.is_multilingual:\n         \
          \   <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >hasattr</span>(generation_config, <span class=\"hljs-string\">\"language\"\
          </span>):\n                forced_decoder_ids.append((<span class=\"hljs-number\"\
          >1</span>, generation_config.lang_to_id[generation_config.language]))\n\
          \            <span class=\"hljs-keyword\">else</span>:\n               \
          \ forced_decoder_ids.append((<span class=\"hljs-number\">1</span>, <span\
          \ class=\"hljs-literal\">None</span>))\n\n            <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">hasattr</span>(generation_config,\
          \ <span class=\"hljs-string\">\"task\"</span>):\n                forced_decoder_ids.append((<span\
          \ class=\"hljs-number\">2</span>, generation_config.task_to_id[generation_config.task]))\n\
          \            <span class=\"hljs-keyword\">else</span>:\n               \
          \ forced_decoder_ids.append((<span class=\"hljs-number\">2</span>, generation_config.task_to_id[<span\
          \ class=\"hljs-string\">\"transcribe\"</span>]))\n\n        <span class=\"\
          hljs-keyword\">if</span> (\n            <span class=\"hljs-built_in\">hasattr</span>(generation_config,\
          \ <span class=\"hljs-string\">\"return_timestamps\"</span>) <span class=\"\
          hljs-keyword\">and</span> generation_config.return_timestamps\n        )\
          \ <span class=\"hljs-keyword\">or</span> return_timestamps:\n          \
          \  logits_processor = [WhisperTimeStampLogitsProcessor(generation_config)]\n\
          \        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"\
          hljs-keyword\">if</span> forced_decoder_ids <span class=\"hljs-keyword\"\
          >and</span> forced_decoder_ids[-<span class=\"hljs-number\">1</span>][<span\
          \ class=\"hljs-number\">0</span>] != generation_config.no_timestamps_token_id:\n\
          \                idx = forced_decoder_ids[-<span class=\"hljs-number\">1</span>][<span\
          \ class=\"hljs-number\">0</span>] + <span class=\"hljs-number\">1</span>\
          \ <span class=\"hljs-keyword\">if</span> forced_decoder_ids <span class=\"\
          hljs-keyword\">else</span> <span class=\"hljs-number\">1</span>\n      \
          \          forced_decoder_ids.append((idx, generation_config.no_timestamps_token_id))\n\
          \n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >len</span>(forced_decoder_ids) &gt; <span class=\"hljs-number\">0</span>:\n\
          \            generation_config.forced_decoder_ids = forced_decoder_ids\n\
          \n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\"\
          >super</span>().generate(\n            inputs,\n            generation_config,\n\
          \            logits_processor,\n            stopping_criteria,\n       \
          \     prefix_allowed_tokens_fn,\n            synced_gpus,\n            **kwargs,\n\
          \        )\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">prepare_inputs_for_generation</span>(<span class=\"\
          hljs-params\"></span>\n<span class=\"hljs-params\">        self,</span>\n\
          <span class=\"hljs-params\">        decoder_input_ids,</span>\n<span class=\"\
          hljs-params\">        past_key_values=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        use_cache=<span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        encoder_outputs=<span\
          \ class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >        attention_mask=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        **kwargs,</span>\n<span class=\"hljs-params\"\
          >    </span>):\n        <span class=\"hljs-comment\"># cut decoder_input_ids\
          \ if past is used</span>\n        <span class=\"hljs-keyword\">if</span>\
          \ past_key_values <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n  \
          \          decoder_input_ids = decoder_input_ids[:, -<span class=\"hljs-number\"\
          >1</span>:]\n\n        <span class=\"hljs-keyword\">return</span> {\n  \
          \          <span class=\"hljs-string\">\"encoder_outputs\"</span>: encoder_outputs,\n\
          \            <span class=\"hljs-string\">\"past_key_values\"</span>: past_key_values,\n\
          \            <span class=\"hljs-string\">\"decoder_input_ids\"</span>: decoder_input_ids,\n\
          \            <span class=\"hljs-string\">\"use_cache\"</span>: use_cache,\n\
          \            <span class=\"hljs-string\">\"decoder_attention_mask\"</span>:\
          \ <span class=\"hljs-literal\">None</span>,\n        }\n\n    <span class=\"\
          hljs-comment\">#</span>\n<span class=\"hljs-meta\">    @staticmethod</span>\n\
          \    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >_reorder_cache</span>(<span class=\"hljs-params\">past_key_values, beam_idx</span>):\n\
          \        reordered_past = ()\n        <span class=\"hljs-keyword\">for</span>\
          \ layer_past <span class=\"hljs-keyword\">in</span> past_key_values:\n \
          \           reordered_past += (<span class=\"hljs-built_in\">tuple</span>(past_state.index_select(<span\
          \ class=\"hljs-number\">0</span>, beam_idx) <span class=\"hljs-keyword\"\
          >for</span> past_state <span class=\"hljs-keyword\">in</span> layer_past),)\n\
          \        <span class=\"hljs-keyword\">return</span> reordered_past\n</code></pre>\n\
          </details>\n\n<p>And now import the model from this file:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> modeling_whisper\
          \ <span class=\"hljs-keyword\">import</span> WhisperForConditionalGeneration\n\
          \n<span class=\"hljs-comment\"># do your thing</span>\n</code></pre>\n<p>For\
          \ modifying the loss, first import the required loss:</p>\n<pre><code class=\"\
          language-diff\"><span class=\"hljs-deletion\">- from torch.nn import CrossEntropyLoss</span>\n\
          <span class=\"hljs-addition\">+ from torch.nn import HingeEmbeddingLoss</span>\n\
          </code></pre>\n<p>And then change the loss function:</p>\n<pre><code class=\"\
          language-diff\">        if labels is not None:\n<span class=\"hljs-deletion\"\
          >-           loss_fct = CrossEntropyLoss()</span>\n<span class=\"hljs-addition\"\
          >+           loss_fct = HingeEmbeddingLoss()</span>\n            loss =\
          \ loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.reshape(-1))\n\
          </code></pre>\n"
        raw: "Hey @Martha-987! You can either follow the approach I've outlined for\
          \ modifying the source code, or copy and paste the transformers code and\
          \ make modifications there.\n\nE.g. copy this code into a file called `modeling_whisper.py`\n\
          \n<details>\n\n<summary> Code: </summary>\n\n```python\n# coding=utf-8\n\
          # Copyright 2022 The OpenAI Authors and The HuggingFace Inc. team. All rights\
          \ reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"\
          License\");\n# you may not use this file except in compliance with the License.\n\
          # You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n\
          #\n# Unless required by applicable law or agreed to in writing, software\n\
          # distributed under the License is distributed on an \"AS IS\" BASIS,\n\
          # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\
          # See the License for the specific language governing permissions and\n\
          # limitations under the License.\n\"\"\" PyTorch Whisper model.\"\"\"\n\n\
          \nimport math\nimport random\nfrom typing import Optional, Tuple, Union\n\
          \nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom\
          \ torch.nn import CrossEntropyLoss\n\nfrom transformers.activations import\
          \ ACT2FN\nfrom transformers.generation.logits_process import WhisperTimeStampLogitsProcessor\n\
          from transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n\
          \    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\nfrom transformers.modeling_utils\
          \ import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n\
          \    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n\
          )\nfrom transformers import WhisperConfig\n\n\nlogger = logging.get_logger(__name__)\n\
          \n_CONFIG_FOR_DOC = \"WhisperConfig\"\n_CHECKPOINT_FOR_DOC = \"openai/whisper-tiny\"\
          \n\n\nWHISPER_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"openai/whisper-base\"\
          ,\n    # See all Whisper models at https://huggingface.co/models?filter=whisper\n\
          ]\n\n\n# Copied from transformers.models.whisper.modeling_whisper.shift_tokens_right\n\
          def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id:\
          \ int):\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\
          \"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n   \
          \ shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:,\
          \ 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise\
          \ ValueError(\"self.model.config.pad_token_id has to be defined.\")\n  \
          \  # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids\
          \ == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\n# Copied from\
          \ transformers.models.whisper.modeling_whisper._make_causal_mask\ndef _make_causal_mask(input_ids_shape:\
          \ torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n  \
          \  \"\"\"\n    Make causal mask used for bi-directional self-attention.\n\
          \    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len,\
          \ tgt_len), torch.tensor(torch.finfo(dtype).min))\n    mask_cond = torch.arange(mask.size(-1))\n\
          \    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1),\
          \ 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n\
          \        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length,\
          \ dtype=dtype), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz,\
          \ 1, tgt_len, tgt_len + past_key_values_length)\n\n\n# Copied from transformers.models.whisper.modeling_whisper._expand_mask\n\
          def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int]\
          \ = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]`\
          \ to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len\
          \ = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\
          \n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\
          \n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool),\
          \ torch.finfo(dtype).min)\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperPositionalEmbedding\n\
          class WhisperPositionalEmbedding(nn.Embedding):\n    def __init__(self,\
          \ num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n\
          \        super().__init__(num_positions, embedding_dim)\n\n    def forward(self,\
          \ input_ids, past_key_values_length=0):\n        return self.weight[past_key_values_length\
          \ : past_key_values_length + input_ids.shape[-1]]\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperAttention\n\
          class WhisperAttention(nn.Module):\n    \"\"\"Multi-headed attention from\
          \ 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n       \
          \ self,\n        embed_dim: int,\n        num_heads: int,\n        dropout:\
          \ float = 0.0,\n        is_decoder: bool = False,\n        bias: bool =\
          \ True,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n\
          \        self.num_heads = num_heads\n        self.dropout = dropout\n  \
          \      self.head_dim = embed_dim // num_heads\n\n        if (self.head_dim\
          \ * num_heads) != self.embed_dim:\n            raise ValueError(\n     \
          \           f\"embed_dim must be divisible by num_heads (got `embed_dim`:\
          \ {self.embed_dim}\"\n                f\" and `num_heads`: {num_heads}).\"\
          \n            )\n        self.scaling = self.head_dim**-0.5\n        self.is_decoder\
          \ = is_decoder\n\n        self.k_proj = nn.Linear(embed_dim, embed_dim,\
          \ bias=False)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\
          \        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n    \
          \    self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n    #\
          \ Copied from transformers.models.whisper.modeling_whisper.BartAttention._shape\
          \ with BART->whisper\n    def _shape(self, tensor: torch.Tensor, seq_len:\
          \ int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads,\
          \ self.head_dim).transpose(1, 2).contiguous()\n\n    # Copied from transformers.models.whisper.modeling_whisper.BartAttention.forward\
          \ with BART->whisper\n    def forward(\n        self,\n        hidden_states:\
          \ torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n\
          \        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n       \
          \ attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask:\
          \ Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n\
          \    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\
          \        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if\
          \ key_value_states are provided this layer is used as a cross-attention\
          \ layer\n        # for the decoder\n        is_cross_attention = key_value_states\
          \ is not None\n\n        bsz, tgt_len, _ = hidden_states.size()\n\n    \
          \    # get query proj\n        query_states = self.q_proj(hidden_states)\
          \ * self.scaling\n        # get key, value proj\n        # `past_key_value[0].shape[2]\
          \ == key_value_states.shape[1]`\n        # is checking that the `sequence_length`\
          \ of the `past_key_value` is the same as\n        # the provided `key_value_states`\
          \ to support prefix tuning\n        if (\n            is_cross_attention\n\
          \            and past_key_value is not None\n            and past_key_value[0].shape[2]\
          \ == key_value_states.shape[1]\n        ):\n            # reuse k,v, cross_attentions\n\
          \            key_states = past_key_value[0]\n            value_states =\
          \ past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n\
          \            key_states = self._shape(self.k_proj(key_value_states), -1,\
          \ bsz)\n            value_states = self._shape(self.v_proj(key_value_states),\
          \ -1, bsz)\n        elif past_key_value is not None:\n            # reuse\
          \ k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states),\
          \ -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states),\
          \ -1, bsz)\n            key_states = torch.cat([past_key_value[0], key_states],\
          \ dim=2)\n            value_states = torch.cat([past_key_value[1], value_states],\
          \ dim=2)\n        else:\n            # self_attention\n            key_states\
          \ = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states\
          \ = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n\
          \            # if cross_attention save Tuple(torch.Tensor, torch.Tensor)\
          \ of all cross attention key/value_states.\n            # Further calls\
          \ to cross_attention layer can then reuse all cross-attention\n        \
          \    # key/value_states (first \"if\" case)\n            # if uni-directional\
          \ self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n \
          \           # all previous decoder key/value_states. Further calls to uni-directional\
          \ self-attention\n            # can concat previous decoder key/value_states\
          \ to current projected key/value_states (third \"elif\" case)\n        \
          \    # if encoder bi-directional self-attention `past_key_value` is always\
          \ `None`\n            past_key_value = (key_states, value_states)\n\n  \
          \      proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states\
          \ = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n        key_states\
          \ = key_states.reshape(*proj_shape)\n        value_states = value_states.reshape(*proj_shape)\n\
          \n        src_len = key_states.size(1)\n        attn_weights = torch.bmm(query_states,\
          \ key_states.transpose(1, 2))\n\n        if attn_weights.size() != (bsz\
          \ * self.num_heads, tgt_len, src_len):\n            raise ValueError(\n\
          \                f\"Attention weights should be of size {(bsz * self.num_heads,\
          \ tgt_len, src_len)}, but is\"\n                f\" {attn_weights.size()}\"\
          \n            )\n\n        if attention_mask is not None:\n            if\
          \ attention_mask.size() != (bsz, 1, tgt_len, src_len):\n               \
          \ raise ValueError(\n                    f\"Attention mask should be of\
          \ size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n\
          \                )\n            attn_weights = attn_weights.view(bsz, self.num_heads,\
          \ tgt_len, src_len) + attention_mask\n            attn_weights = attn_weights.view(bsz\
          \ * self.num_heads, tgt_len, src_len)\n\n        attn_weights = nn.functional.softmax(attn_weights,\
          \ dim=-1)\n\n        if layer_head_mask is not None:\n            if layer_head_mask.size()\
          \ != (self.num_heads,):\n                raise ValueError(\n           \
          \         f\"Head mask for a single layer should be of size {(self.num_heads,)},\
          \ but is\"\n                    f\" {layer_head_mask.size()}\"\n       \
          \         )\n            attn_weights = layer_head_mask.view(1, -1, 1, 1)\
          \ * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n         \
          \   attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\
          \n        if output_attentions:\n            # this operation is a bit awkward,\
          \ but it's required to\n            # make sure that attn_weights keeps\
          \ its gradient.\n            # In order to do so, attn_weights have to be\
          \ reshaped\n            # twice and have to be reused in the following\n\
          \            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads,\
          \ tgt_len, src_len)\n            attn_weights = attn_weights_reshaped.view(bsz\
          \ * self.num_heads, tgt_len, src_len)\n        else:\n            attn_weights_reshaped\
          \ = None\n\n        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout,\
          \ training=self.training)\n\n        attn_output = torch.bmm(attn_probs,\
          \ value_states)\n\n        if attn_output.size() != (bsz * self.num_heads,\
          \ tgt_len, self.head_dim):\n            raise ValueError(\n            \
          \    f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len,\
          \ self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n\
          \            )\n\n        attn_output = attn_output.view(bsz, self.num_heads,\
          \ tgt_len, self.head_dim)\n        attn_output = attn_output.transpose(1,\
          \ 2)\n\n        # Use the `embed_dim` from the config (stored in the class)\
          \ rather than `hidden_state` because `attn_output` can be\n        # partitioned\
          \ across GPUs when using tensor-parallelism.\n        attn_output = attn_output.reshape(bsz,\
          \ tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\
          \n        return attn_output, attn_weights_reshaped, past_key_value\n\n\n\
          # Copied from transformers.models.whisper.modeling_whisper.WhisperEncoderLayer\n\
          class WhisperEncoderLayer(nn.Module):\n    def __init__(self, config: WhisperConfig):\n\
          \        super().__init__()\n        self.embed_dim = config.d_model\n \
          \       self.self_attn = WhisperAttention(\n            embed_dim=self.embed_dim,\n\
          \            num_heads=config.encoder_attention_heads,\n            dropout=config.attention_dropout,\n\
          \        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\
          \        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n\
          \        self.activation_dropout = config.activation_dropout\n        self.fc1\
          \ = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2\
          \ = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm\
          \ = nn.LayerNorm(self.embed_dim)\n\n    def forward(\n        self,\n  \
          \      hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n\
          \        layer_head_mask: torch.Tensor,\n        output_attentions: bool\
          \ = False,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n    \
          \        hidden_states (`torch.FloatTensor`): input to the layer of shape\
          \ `(seq_len, batch, embed_dim)`\n            attention_mask (`torch.FloatTensor`):\
          \ attention mask of size\n                `(batch, 1, tgt_len, src_len)`\
          \ where padding elements are indicated by very large negative values.\n\
          \            layer_head_mask (`torch.FloatTensor`): mask for attention heads\
          \ in a given layer of size\n                `(encoder_attention_heads,)`.\n\
          \            output_attentions (`bool`, *optional*):\n                Whether\
          \ or not to return the attentions tensors of all attention layers. See `attentions`\
          \ under\n                returned tensors for more detail.\n        \"\"\
          \"\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\
          \        hidden_states, attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n\
          \            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n\
          \            output_attentions=output_attentions,\n        )\n        hidden_states\
          \ = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\
          \        hidden_states = residual + hidden_states\n\n        residual =\
          \ hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\
          \        hidden_states = self.activation_fn(self.fc1(hidden_states))\n \
          \       hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout,\
          \ training=self.training)\n        hidden_states = self.fc2(hidden_states)\n\
          \        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout,\
          \ training=self.training)\n        hidden_states = residual + hidden_states\n\
          \n        if hidden_states.dtype == torch.float16 and (\n            torch.isinf(hidden_states).any()\
          \ or torch.isnan(hidden_states).any()\n        ):\n            clamp_value\
          \ = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states\
          \ = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n  \
          \      outputs = (hidden_states,)\n\n        if output_attentions:\n   \
          \         outputs += (attn_weights,)\n\n        return outputs\n\n\n# Copied\
          \ from transformers.models.whisper.modeling_whisper.WhisperDecoderLayer\n\
          class WhisperDecoderLayer(nn.Module):\n    def __init__(self, config: WhisperConfig):\n\
          \        super().__init__()\n        self.embed_dim = config.d_model\n\n\
          \        self.self_attn = WhisperAttention(\n            embed_dim=self.embed_dim,\n\
          \            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n\
          \            is_decoder=True,\n        )\n        self.dropout = config.dropout\n\
          \        self.activation_fn = ACT2FN[config.activation_function]\n     \
          \   self.activation_dropout = config.activation_dropout\n\n        self.self_attn_layer_norm\
          \ = nn.LayerNorm(self.embed_dim)\n        self.encoder_attn = WhisperAttention(\n\
          \            self.embed_dim,\n            config.decoder_attention_heads,\n\
          \            dropout=config.attention_dropout,\n            is_decoder=True,\n\
          \        )\n        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\
          \        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n\
          \        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n\
          \        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\n    def\
          \ forward(\n        self,\n        hidden_states: torch.Tensor,\n      \
          \  attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states:\
          \ Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor]\
          \ = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n   \
          \     cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n     \
          \   past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions:\
          \ Optional[bool] = False,\n        use_cache: Optional[bool] = True,\n \
          \   ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states\
          \ (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n\
          \            attention_mask (`torch.FloatTensor`): attention mask of size\n\
          \                `(batch, 1, tgt_len, src_len)` where padding elements are\
          \ indicated by very large negative values.\n            encoder_hidden_states\
          \ (`torch.FloatTensor`):\n                cross attention input to the layer\
          \ of shape `(batch, seq_len, embed_dim)`\n            encoder_attention_mask\
          \ (`torch.FloatTensor`): encoder attention mask of size\n              \
          \  `(batch, 1, tgt_len, src_len)` where padding elements are indicated by\
          \ very large negative values.\n            layer_head_mask (`torch.FloatTensor`):\
          \ mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n\
          \            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for\
          \ cross-attention heads in a given layer of\n                size `(decoder_attention_heads,)`.\n\
          \            past_key_value (`Tuple(torch.FloatTensor)`): cached past key\
          \ and value projection states\n            output_attentions (`bool`, *optional*):\n\
          \                Whether or not to return the attentions tensors of all\
          \ attention layers. See `attentions` under\n                returned tensors\
          \ for more detail.\n        \"\"\"\n        residual = hidden_states\n \
          \       hidden_states = self.self_attn_layer_norm(hidden_states)\n\n   \
          \     # Self Attention\n        # decoder uni-directional self-attention\
          \ cached key/values tuple is at positions 1,2\n        self_attn_past_key_value\
          \ = past_key_value[:2] if past_key_value is not None else None\n       \
          \ # add present self-attn cache to positions 1,2 of present_key_value tuple\n\
          \        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\
          \            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n\
          \            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n\
          \            output_attentions=output_attentions,\n        )\n        hidden_states\
          \ = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\
          \        hidden_states = residual + hidden_states\n\n        # Cross-Attention\
          \ Block\n        cross_attn_present_key_value = None\n        cross_attn_weights\
          \ = None\n        if encoder_hidden_states is not None:\n            residual\
          \ = hidden_states\n            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n\
          \n            # cross_attn cached key/values tuple is at positions 3,4 of\
          \ present_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:]\
          \ if past_key_value is not None else None\n            hidden_states, cross_attn_weights,\
          \ cross_attn_present_key_value = self.encoder_attn(\n                hidden_states=hidden_states,\n\
          \                key_value_states=encoder_hidden_states,\n             \
          \   attention_mask=encoder_attention_mask,\n                layer_head_mask=cross_attn_layer_head_mask,\n\
          \                past_key_value=cross_attn_past_key_value,\n           \
          \     output_attentions=output_attentions,\n            )\n            hidden_states\
          \ = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\
          \            hidden_states = residual + hidden_states\n\n            # add\
          \ cross-attn to positions 3,4 of present_key_value tuple\n            present_key_value\
          \ = present_key_value + cross_attn_present_key_value\n\n        # Fully\
          \ Connected\n        residual = hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\
          \        hidden_states = self.activation_fn(self.fc1(hidden_states))\n \
          \       hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout,\
          \ training=self.training)\n        hidden_states = self.fc2(hidden_states)\n\
          \        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout,\
          \ training=self.training)\n        hidden_states = residual + hidden_states\n\
          \n        outputs = (hidden_states,)\n\n        if output_attentions:\n\
          \            outputs += (self_attn_weights, cross_attn_weights)\n\n    \
          \    if use_cache:\n            outputs += (present_key_value,)\n\n    \
          \    return outputs\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperPreTrainedModel\n\
          class WhisperPreTrainedModel(PreTrainedModel):\n    config_class = WhisperConfig\n\
          \    base_model_prefix = \"model\"\n    main_input_name = \"input_features\"\
          \n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"\
          WhisperEncoderLayer\"]\n\n    def _init_weights(self, module):\n       \
          \ std = self.config.init_std\n        if isinstance(module, (nn.Linear,\
          \ nn.Conv1d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n\
          \            if module.bias is not None:\n                module.bias.data.zero_()\n\
          \        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0,\
          \ std=std)\n            if module.padding_idx is not None:\n           \
          \     module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self,\
          \ module, value=False):\n        if isinstance(module, (WhisperDecoder,\
          \ WhisperEncoder)):\n            module.gradient_checkpointing = value\n\
          \n    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n\
          \        \"\"\"\n        Computes the output length of the convolutional\
          \ layers\n        \"\"\"\n        input_lengths = (input_lengths - 1) //\
          \ 2 + 1\n\n        return input_lengths\n\n\nWHISPER_START_DOCSTRING = r\"\
          \"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass\
          \ documentation for the generic methods the\n    library implements for\
          \ all its model (such as downloading or saving, resizing the input embeddings,\
          \ pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)\
          \ subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch\
          \ documentation for all matter related to general usage\n    and behavior.\n\
          \n    Parameters:\n        config ([`WhisperConfig`]):\n            Model\
          \ configuration class with all the parameters of the model. Initializing\
          \ with a config file does not\n            load the weights associated with\
          \ the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`]\
          \ method to load the model weights.\n\"\"\"\n\nWHISPER_INPUTS_DOCSTRING\
          \ = r\"\"\"\n    Args:\n        input_features (`torch.FloatTensor` of shape\
          \ `(batch_size, feature_size, sequence_length)`):\n            Float values\
          \ mel features extracted from the raw speech waveform. Raw speech waveform\
          \ can be obtained by\n            loading a `.flac` or `.wav` audio file\
          \ into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n\
          \            the soundfile library (`pip install soundfile`). To prepare\
          \ the array into `input_features`, the\n            [`AutoFeatureExtractor`]\
          \ should be used for extracting the mel features, padding and conversion\
          \ into a\n            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n\
          \        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`,\
          \ *optional*):\n            Mask to avoid performing *SpecAugment* data\
          \ augmentation on padding token indices. Mask values selected in\n     \
          \       `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n\
          \            - 0 for tokens that are **masked**.\n\n            [What are\
          \ attention masks?](transformers/glossary#attention-mask)\n        decoder_input_ids\
          \ (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n\
          \            Indices of decoder input sequence tokens in the vocabulary.\n\
          \n            Indices can be obtained using [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`]\
          \ and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n   \
          \         [What are decoder input IDs?](transformers/glossary#decoder-input-ids)\n\
          \n            Whisper uses the `decoder_start_token_id` as the starting\
          \ token for `decoder_input_ids` generation. If\n            `past_key_values`\
          \ is used, optionally only the last `decoder_input_ids` have to be input\
          \ (see\n            `past_key_values`).\n        decoder_attention_mask\
          \ (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n\
          \            Default behavior: generate a tensor that ignores pad tokens\
          \ in `decoder_input_ids`. Causal mask will also\n            be used by\
          \ default.\n\n            If you want to change padding behavior, you should\
          \ read\n            [`modeling_whisper._prepare_decoder_attention_mask`]\
          \ and modify to your needs. See diagram 1 in [the BART\n            paper](https://arxiv.org/abs/1910.13461)\
          \ for more information on the default strategy.\n        head_mask (`torch.Tensor`\
          \ of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n \
          \           Mask to nullify selected heads of the attention modules in the\
          \ encoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates\
          \ the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\
          \n        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,\
          \ *optional*):\n            Mask to nullify selected heads of the attention\
          \ modules in the decoder. Mask values selected in `[0, 1]`:\n\n        \
          \    - 1 indicates the head is **not masked**,\n            - 0 indicates\
          \ the head is **masked**.\n\n        cross_attn_head_mask (`torch.Tensor`\
          \ of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n \
          \           Mask to nullify selected heads of the cross-attention modules.\
          \ Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head\
          \ is **not masked**,\n            - 0 indicates the head is **masked**.\n\
          \n        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n\
          \            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`,\
          \ *optional*: `attentions`)\n            `last_hidden_state` of shape `(batch_size,\
          \ sequence_length, hidden_size)`, *optional*) is a sequence of\n       \
          \     hidden-states at the output of the last layer of the encoder. Used\
          \ in the cross-attention of the decoder.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`,\
          \ *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n\
          \            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`,\
          \ with each tuple having 2 tensors of shape\n            `(batch_size, num_heads,\
          \ sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n\
          \            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\
          \n            Contains pre-computed hidden-states (key and values in the\
          \ self-attention blocks and in the cross-attention\n            blocks)\
          \ that can be used (see `past_key_values` input) to speed up sequential\
          \ decoding.\n\n            If `past_key_values` are used, the user can optionally\
          \ input only the last `decoder_input_ids` (those that\n            don't\
          \ have their past key value states given to this model) of shape `(batch_size,\
          \ 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size,\
          \ sequence_length)`.\n        decoder_inputs_embeds (`torch.FloatTensor`\
          \ of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n\
          \            Optionally, instead of passing `decoder_input_ids` you can\
          \ choose to directly pass an embedded\n            representation. If `past_key_values`\
          \ is used, optionally only the last `decoder_inputs_embeds` have to be\n\
          \            input (see `past_key_values`). This is useful if you want more\
          \ control over how to convert\n            `decoder_input_ids` indices into\
          \ associated vectors than the model's internal embedding lookup matrix.\n\
          \        use_cache (`bool`, *optional*):\n            If set to `True`,\
          \ `past_key_values` key value states are returned and can be used to speed\
          \ up decoding (see\n            `past_key_values`).\n        output_attentions\
          \ (`bool`, *optional*):\n            Whether or not to return the attentions\
          \ tensors of all attention layers. See `attentions` under returned\n   \
          \         tensors for more detail.\n        output_hidden_states (`bool`,\
          \ *optional*):\n            Whether or not to return the hidden states of\
          \ all layers. See `hidden_states` under returned tensors for\n         \
          \   more detail.\n        return_dict (`bool`, *optional*):\n          \
          \  Whether or not to return a [`~utils.ModelOutput`] instead of a plain\
          \ tuple.\n\"\"\"\n\nWHISPER_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n\
          \        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size,\
          \ sequence_length)`):\n            Float values mel features extracted from\
          \ the raw speech waveform. Raw speech waveform can be obtained by\n    \
          \        loading a `.flac` or `.wav` audio file into an array of type `List[float]`\
          \ or a `numpy.ndarray`, *e.g.* via\n            the soundfile library (`pip\
          \ install soundfile`). To prepare the array into `input_features`, the\n\
          \            [`AutoFeatureExtractor`] should be used for extracting the\
          \ mel features, padding and conversion into a\n            tensor of type\
          \ `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n     \
          \   head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,\
          \ *optional*):\n            Mask to nullify selected heads of the attention\
          \ modules in the encoder. Mask values selected in `[0, 1]`:\n\n        \
          \    - 1 indicates the head is **not masked**,\n            - 0 indicates\
          \ the head is **masked**.\n        encoder_outputs (`tuple(tuple(torch.FloatTensor)`,\
          \ *optional*):\n            Tuple consists of (`last_hidden_state`, *optional*:\
          \ `hidden_states`, *optional*: `attentions`)\n            `last_hidden_state`\
          \ of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is\
          \ a sequence of\n            hidden-states at the output of the last layer\
          \ of the encoder.\n        output_attentions (`bool`, *optional*):\n   \
          \         Whether or not to return the attentions tensors of all attention\
          \ layers. See `attentions` under returned\n            tensors for more\
          \ detail.\n        output_hidden_states (`bool`, *optional*):\n        \
          \    Whether or not to return the hidden states of all layers. See `hidden_states`\
          \ under returned tensors for\n            more detail.\n        return_dict\
          \ (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`]\
          \ instead of a plain tuple.\n\"\"\"\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperEncoder\n\
          class WhisperEncoder(WhisperPreTrainedModel):\n    \"\"\"\n    Transformer\
          \ encoder consisting of *config.encoder_layers* self attention layers. Each\
          \ layer is a\n    [`WhisperEncoderLayer`].\n\n    Args:\n        config:\
          \ WhisperConfig\n        embed_tokens (nn.Embedding): output embedding\n\
          \    \"\"\"\n\n    def __init__(self, config: WhisperConfig):\n        super().__init__(config)\n\
          \        self.dropout = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\
          \n        embed_dim = config.d_model\n        self.num_mel_bins = config.num_mel_bins\n\
          \        self.padding_idx = config.pad_token_id\n        self.max_source_positions\
          \ = config.max_source_positions\n        self.embed_scale = math.sqrt(embed_dim)\
          \ if config.scale_embedding else 1.0\n\n        self.conv1 = nn.Conv1d(self.num_mel_bins,\
          \ embed_dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(embed_dim,\
          \ embed_dim, kernel_size=3, stride=2, padding=1)\n\n        self.embed_positions\
          \ = nn.Embedding(self.max_source_positions, embed_dim)\n\n        self.layers\
          \ = nn.ModuleList([WhisperEncoderLayer(config) for _ in range(config.encoder_layers)])\n\
          \        self.layer_norm = nn.LayerNorm(config.d_model)\n\n        self.gradient_checkpointing\
          \ = False\n        # Initialize weights and apply final processing\n   \
          \     self.post_init()\n\n    def _freeze_parameters(self):\n        for\
          \ param in self.parameters():\n            param.requires_grad = False\n\
          \        self._requires_grad = False\n\n    def forward(\n        self,\n\
          \        input_features,\n        attention_mask=None,\n        head_mask=None,\n\
          \        output_attentions=None,\n        output_hidden_states=None,\n \
          \       return_dict=None,\n    ):\n        r\"\"\"\n        Args:\n    \
          \        input_features (`torch.LongTensor` of shape `(batch_size, feature_size,\
          \ sequence_length)`):\n                Float values of mel features extracted\
          \ from the raw speech waveform. Raw speech waveform can be\n           \
          \     obtained by loading a `.flac` or `.wav` audio file into an array of\
          \ type `List[float]` or a\n                `numpy.ndarray`, *e.g.* via the\
          \ soundfile library (`pip install soundfile`). To prepare the array into\n\
          \                `input_features`, the [`AutoFeatureExtractor`] should be\
          \ used for extracting the mel features, padding\n                and conversion\
          \ into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n\
          \            attention_mask (`torch.Tensor`)`, *optional*):\n          \
          \      Whisper does not support masking of the `input_features`, this argument\
          \ is preserved for compatibility,\n                but it is not used. By\
          \ default the silence in the input log mel spectrogram are ignored.\n  \
          \          head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,\
          \ *optional*):\n                Mask to nullify selected heads of the attention\
          \ modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates\
          \ the head is **not masked**,\n                - 0 indicates the head is\
          \ **masked**.\n            output_attentions (`bool`, *optional*):\n   \
          \             Whether or not to return the attentions tensors of all attention\
          \ layers. See `attentions` under\n                returned tensors for more\
          \ detail.\n            output_hidden_states (`bool`, *optional*):\n    \
          \            Whether or not to return the hidden states of all layers. See\
          \ `hidden_states` under returned tensors\n                for more detail.\n\
          \            return_dict (`bool`, *optional*):\n                Whether\
          \ or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\
          \        \"\"\"\n        output_attentions = output_attentions if output_attentions\
          \ is not None else self.config.output_attentions\n        output_hidden_states\
          \ = (\n            output_hidden_states if output_hidden_states is not None\
          \ else self.config.output_hidden_states\n        )\n        return_dict\
          \ = return_dict if return_dict is not None else self.config.use_return_dict\n\
          \        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n\
          \        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n\
          \n        inputs_embeds = inputs_embeds.permute(0, 2, 1)\n        embed_pos\
          \ = self.embed_positions.weight\n\n        hidden_states = inputs_embeds\
          \ + embed_pos\n        hidden_states = nn.functional.dropout(hidden_states,\
          \ p=self.dropout, training=self.training)\n\n        encoder_states = ()\
          \ if output_hidden_states else None\n        all_attentions = () if output_attentions\
          \ else None\n\n        # check if head_mask has a correct number of layers\
          \ specified if desired\n        if head_mask is not None:\n            assert\
          \ head_mask.size()[0] == (\n                len(self.layers)\n         \
          \   ), f\"The head_mask should be specified for {len(self.layers)} layers,\
          \ but it is for {head_mask.size()[0]}.\"\n\n        for idx, encoder_layer\
          \ in enumerate(self.layers):\n            if output_hidden_states:\n   \
          \             encoder_states = encoder_states + (hidden_states,)\n     \
          \       # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\
          \            dropout_probability = random.uniform(0, 1)\n            if\
          \ self.training and (dropout_probability < self.layerdrop):  # skip the\
          \ layer\n                layer_outputs = (None, None)\n            else:\n\
          \                if self.gradient_checkpointing and self.training:\n\n \
          \                   def create_custom_forward(module):\n               \
          \         def custom_forward(*inputs):\n                            return\
          \ module(*inputs, output_attentions)\n\n                        return custom_forward\n\
          \n                    layer_outputs = torch.utils.checkpoint.checkpoint(\n\
          \                        create_custom_forward(encoder_layer),\n       \
          \                 hidden_states,\n                        None,\n      \
          \                  (head_mask[idx] if head_mask is not None else None),\n\
          \                    )\n                else:\n                    layer_outputs\
          \ = encoder_layer(\n                        hidden_states,\n           \
          \             None,\n                        layer_head_mask=(head_mask[idx]\
          \ if head_mask is not None else None),\n                        output_attentions=output_attentions,\n\
          \                    )\n\n                hidden_states = layer_outputs[0]\n\
          \n            if output_attentions:\n                all_attentions = all_attentions\
          \ + (layer_outputs[1],)\n\n        hidden_states = self.layer_norm(hidden_states)\n\
          \        if output_hidden_states:\n            encoder_states = encoder_states\
          \ + (hidden_states,)\n\n        if not return_dict:\n            return\
          \ tuple(v for v in [hidden_states, encoder_states, all_attentions] if v\
          \ is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states,\
          \ hidden_states=encoder_states, attentions=all_attentions\n        )\n\n\
          \n# Copied from transformers.models.whisper.modeling_whisper.WhisperDecoder\n\
          class WhisperDecoder(WhisperPreTrainedModel):\n    \"\"\"\n    Transformer\
          \ decoder consisting of *config.decoder_layers* layers. Each layer is a\
          \ [`WhisperDecoderLayer`]\n\n    Args:\n        config: WhisperConfig\n\
          \    \"\"\"\n\n    def __init__(self, config: WhisperConfig):\n        super().__init__(config)\n\
          \        self.dropout = config.dropout\n        self.layerdrop = config.decoder_layerdrop\n\
          \        self.padding_idx = config.pad_token_id\n        self.max_target_positions\
          \ = config.max_target_positions\n        self.max_source_positions = config.max_source_positions\n\
          \        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding\
          \ else 1.0\n\n        self.embed_tokens = nn.Embedding(config.vocab_size,\
          \ config.d_model, self.padding_idx)\n        self.embed_positions = WhisperPositionalEmbedding(self.max_target_positions,\
          \ config.d_model)\n\n        self.layers = nn.ModuleList([WhisperDecoderLayer(config)\
          \ for _ in range(config.decoder_layers)])\n\n        self.layer_norm = nn.LayerNorm(config.d_model)\n\
          \n        self.gradient_checkpointing = False\n        # Initialize weights\
          \ and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n\
          \        return self.embed_tokens\n\n    def set_input_embeddings(self,\
          \ value):\n        self.embed_tokens = value\n\n    def _prepare_decoder_attention_mask(self,\
          \ attention_mask, input_shape, inputs_embeds, past_key_values_length):\n\
          \        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len,\
          \ src_seq_len]\n        combined_attention_mask = None\n\n        if input_shape[-1]\
          \ > 1:\n            combined_attention_mask = _make_causal_mask(\n     \
          \           input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n\
          \            ).to(inputs_embeds.device)\n\n        if attention_mask is\
          \ not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n\
          \            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype,\
          \ tgt_len=input_shape[-1])\n            combined_attention_mask = (\n  \
          \              expanded_attn_mask if combined_attention_mask is None else\
          \ expanded_attn_mask + combined_attention_mask\n            )\n\n      \
          \  return combined_attention_mask\n\n    def forward(\n        self,\n \
          \       input_ids=None,\n        attention_mask=None,\n        encoder_hidden_states=None,\n\
          \        head_mask=None,\n        cross_attn_head_mask=None,\n        past_key_values=None,\n\
          \        inputs_embeds=None,\n        use_cache=None,\n        output_attentions=None,\n\
          \        output_hidden_states=None,\n        return_dict=None,\n    ):\n\
          \        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor`\
          \ of shape `(batch_size, sequence_length)`):\n                Indices of\
          \ input sequence tokens in the vocabulary. Padding will be ignored by default\
          \ should you\n                provide it.\n\n                Indices can\
          \ be obtained using [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`]\
          \ and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n\
          \                [What are input IDs?](transformers/glossary#input-ids)\n\
          \            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`,\
          \ *optional*):\n                Mask to avoid performing attention on padding\
          \ token indices. Mask values selected in `[0, 1]`:\n\n                -\
          \ 1 for tokens that are **not masked**,\n                - 0 for tokens\
          \ that are **masked**.\n\n                [What are attention masks?](transformers/glossary#attention-mask)\n\
          \            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size,\
          \ encoder_sequence_length, hidden_size)`, *optional*):\n               \
          \ Sequence of hidden-states at the output of the last layer of the encoder.\
          \ Used in the cross-attention\n                of the decoder.\n       \
          \     head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,\
          \ *optional*):\n                Mask to nullify selected heads of the attention\
          \ modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates\
          \ the head is **not masked**,\n                - 0 indicates the head is\
          \ **masked**.\n\n            cross_attn_head_mask (`torch.Tensor` of shape\
          \ `(decoder_layers, decoder_attention_heads)`, *optional*):\n          \
          \      Mask to nullify selected heads of the attention modules in encoder\
          \ to avoid performing cross-attention\n                on hidden heads.\
          \ Mask values selected in `[0, 1]`:\n\n                - 1 indicates the\
          \ head is **not masked**,\n                - 0 indicates the head is **masked**.\n\
          \n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*,\
          \ returned when `use_cache=True` is passed or when `config.use_cache=True`):\n\
          \                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`,\
          \ with each tuple having 2 tensors of\n                shape `(batch_size,\
          \ num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors\
          \ of\n                shape `(batch_size, num_heads, encoder_sequence_length,\
          \ embed_size_per_head)`.\n\n                Contains pre-computed hidden-states\
          \ (key and values in the self-attention blocks and in the\n            \
          \    cross-attention blocks) that can be used (see `past_key_values` input)\
          \ to speed up sequential decoding.\n\n                If `past_key_values`\
          \ are used, the user can optionally input only the last `decoder_input_ids`\
          \ (those\n                that don't have their past key value states given\
          \ to this model) of shape `(batch_size, 1)` instead of\n               \
          \ all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds\
          \ (`torch.FloatTensor` of\n                shape `(batch_size, sequence_length,\
          \ hidden_size)`, *optional*): Optionally, instead of passing\n         \
          \       `input_ids` you can choose to directly pass an embedded representation.\
          \ This is useful if you want more\n                control over how to convert\
          \ `input_ids` indices into associated vectors than the model's internal\n\
          \                embedding lookup matrix.\n            output_attentions\
          \ (`bool`, *optional*):\n                Whether or not to return the attentions\
          \ tensors of all attention layers. See `attentions` under\n            \
          \    returned tensors for more detail.\n            output_hidden_states\
          \ (`bool`, *optional*):\n                Whether or not to return the hidden\
          \ states of all layers. See `hidden_states` under returned tensors\n   \
          \             for more detail.\n            return_dict (`bool`, *optional*):\n\
          \                Whether or not to return a [`~utils.ModelOutput`] instead\
          \ of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions\
          \ if output_attentions is not None else self.config.output_attentions\n\
          \        output_hidden_states = (\n            output_hidden_states if output_hidden_states\
          \ is not None else self.config.output_hidden_states\n        )\n       \
          \ use_cache = use_cache if use_cache is not None else self.config.use_cache\n\
          \        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\
          \n        # retrieve input_ids and inputs_embeds\n        if input_ids is\
          \ not None and inputs_embeds is not None:\n            raise ValueError(\"\
          You cannot specify both decoder_input_ids and decoder_inputs_embeds at the\
          \ same time\")\n        elif input_ids is not None:\n            input_shape\
          \ = input_ids.size()\n            input_ids = input_ids.view(-1, input_shape[-1])\n\
          \        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n\
          \        else:\n            raise ValueError(\"You have to specify either\
          \ decoder_input_ids or decoder_inputs_embeds\")\n\n        # past_key_values_length\n\
          \        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values\
          \ is not None else 0\n\n        if inputs_embeds is None:\n            inputs_embeds\
          \ = self.embed_tokens(input_ids)\n\n        attention_mask = self._prepare_decoder_attention_mask(\n\
          \            attention_mask, input_shape, inputs_embeds, past_key_values_length\n\
          \        )\n\n        # embed positions\n        positions = self.embed_positions(input_ids,\
          \ past_key_values_length=past_key_values_length)\n\n        hidden_states\
          \ = inputs_embeds + positions\n        hidden_states = nn.functional.dropout(hidden_states,\
          \ p=self.dropout, training=self.training)\n\n        # decoder layers\n\
          \        all_hidden_states = () if output_hidden_states else None\n    \
          \    all_self_attns = () if output_attentions else None\n        all_cross_attentions\
          \ = () if (output_attentions and encoder_hidden_states is not None) else\
          \ None\n        next_decoder_cache = () if use_cache else None\n\n     \
          \   # check if head_mask/cross_attn_head_mask has a correct number of layers\
          \ specified if desired\n        for attn_mask, mask_name in zip([head_mask,\
          \ cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n \
          \           if attn_mask is not None:\n                assert attn_mask.size()[0]\
          \ == (len(self.layers)), (\n                    f\"The `{mask_name}` should\
          \ be specified for {len(self.layers)} layers, but it is for\"\n        \
          \            f\" {head_mask.size()[0]}.\"\n                )\n        for\
          \ idx, decoder_layer in enumerate(self.layers):\n            # add LayerDrop\
          \ (see https://arxiv.org/abs/1909.11556 for description)\n            if\
          \ output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\
          \            dropout_probability = random.uniform(0, 1)\n            if\
          \ self.training and (dropout_probability < self.layerdrop):\n          \
          \      continue\n\n            past_key_value = past_key_values[idx] if\
          \ past_key_values is not None else None\n\n            if self.gradient_checkpointing\
          \ and self.training:\n                if use_cache:\n                  \
          \  logger.warning(\n                        \"`use_cache = True` is incompatible\
          \ with gradient checkpointing. Setting `use_cache =\"\n                \
          \        \" False`transformers.\"\n                    )\n             \
          \       use_cache = False\n\n                def create_custom_forward(module):\n\
          \                    def custom_forward(*inputs):\n                    \
          \    # None for past_key_value\n                        return module(*inputs,\
          \ output_attentions, use_cache)\n\n                    return custom_forward\n\
          \n                layer_outputs = torch.utils.checkpoint.checkpoint(\n \
          \                   create_custom_forward(decoder_layer),\n            \
          \        hidden_states,\n                    attention_mask,\n         \
          \           encoder_hidden_states,\n                    None,  # encoder\
          \ attention mask\n                    head_mask[idx] if head_mask is not\
          \ None else None,\n                    cross_attn_head_mask[idx] if cross_attn_head_mask\
          \ is not None else None,\n                    None,  # past_key_value\n\
          \                )\n            else:\n                layer_outputs = decoder_layer(\n\
          \                    hidden_states,\n                    attention_mask=attention_mask,\n\
          \                    encoder_hidden_states=encoder_hidden_states,\n    \
          \                layer_head_mask=(head_mask[idx] if head_mask is not None\
          \ else None),\n                    cross_attn_layer_head_mask=(\n      \
          \                  cross_attn_head_mask[idx] if cross_attn_head_mask is\
          \ not None else None\n                    ),\n                    past_key_value=past_key_value,\n\
          \                    output_attentions=output_attentions,\n            \
          \        use_cache=use_cache,\n                )\n            hidden_states\
          \ = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache\
          \ += (layer_outputs[3 if output_attentions else 1],)\n\n            if output_attentions:\n\
          \                all_self_attns += (layer_outputs[1],)\n\n             \
          \   if encoder_hidden_states is not None:\n                    all_cross_attentions\
          \ += (layer_outputs[2],)\n\n        hidden_states = self.layer_norm(hidden_states)\n\
          \        # add hidden states from the last decoder layer\n        if output_hidden_states:\n\
          \            all_hidden_states += (hidden_states,)\n\n        next_cache\
          \ = next_decoder_cache if use_cache else None\n        if not return_dict:\n\
          \            return tuple(\n                v\n                for v in\
          \ [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n\
          \                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n\
          \            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n\
          \            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n\
          \            cross_attentions=all_cross_attentions,\n        )\n\n\n@add_start_docstrings(\n\
          \    \"The bare Whisper Model outputting raw hidden-states without any specific\
          \ head on top.\",\n    WHISPER_START_DOCSTRING,\n)\n# Copied from transformers.models.whisper.modeling_whisper.WhisperModel\n\
          class WhisperModel(WhisperPreTrainedModel):\n    _keys_to_ignore_on_load_missing\
          \ = [r\"proj_out.weight\"]\n\n    def __init__(self, config: WhisperConfig):\n\
          \        super().__init__(config)\n\n        self.encoder = WhisperEncoder(config)\n\
          \        self.decoder = WhisperDecoder(config)\n        # Initialize weights\
          \ and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n\
          \        return self.decoder.embed_tokens\n\n    def set_input_embeddings(self,\
          \ value):\n        self.decoder.embed_tokens = value\n\n    def get_encoder(self):\n\
          \        return self.encoder\n\n    def get_decoder(self):\n        return\
          \ self.decoder\n\n    def freeze_encoder(self):\n        \"\"\"\n      \
          \  Calling this function will disable the gradient computation for the Whisper\
          \ encoder so that its parameters will\n        not be updated during training.\n\
          \        \"\"\"\n        self.encoder._freeze_parameters()\n\n    @add_start_docstrings_to_model_forward(WHISPER_INPUTS_DOCSTRING)\n\
          \    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n\
          \    def forward(\n        self,\n        input_features: Optional[torch.FloatTensor]\
          \ = None,\n        attention_mask: Optional[torch.LongTensor] = None,\n\
          \        decoder_input_ids: Optional[torch.LongTensor] = None,\n       \
          \ decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask:\
          \ Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor]\
          \ = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n\
          \        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n\
          \        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n\
          \        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n\
          \        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool]\
          \ = None,\n        output_hidden_states: Optional[bool] = None,\n      \
          \  return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor],\
          \ Seq2SeqModelOutput]:\n        r\"\"\"\n        Returns:\n\n        Example:\n\
          \         ```python\n         >>> import torch\n         >>> from transformers\
          \ import AutoFeatureExtractor, WhisperModel\n         >>> from datasets\
          \ import load_dataset\n\n         >>> model = WhisperModel.from_pretrained(\"\
          openai/whisper-base\")\n         >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"\
          openai/whisper-base\")\n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\")\n         >>> inputs = feature_extractor(ds[0][\"\
          audio\"][\"array\"], return_tensors=\"pt\")\n         >>> input_features\
          \ = inputs.input_features\n         >>> decoder_input_ids = torch.tensor([[1,\
          \ 1]]) * model.config.decoder_start_token_id\n         >>> last_hidden_state\
          \ = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n\
          \         >>> list(last_hidden_state.shape)\n         [1, 2, 512]\n    \
          \     ```\"\"\"\n        output_attentions = output_attentions if output_attentions\
          \ is not None else self.config.output_attentions\n        output_hidden_states\
          \ = (\n            output_hidden_states if output_hidden_states is not None\
          \ else self.config.output_hidden_states\n        )\n        use_cache =\
          \ use_cache if use_cache is not None else self.config.use_cache\n      \
          \  return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\
          \n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n\
          \                input_features,\n                head_mask=head_mask,\n\
          \                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n\
          \                return_dict=return_dict,\n            )\n        # If the\
          \ user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput\
          \ when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs,\
          \ BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n  \
          \              last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1]\
          \ if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2]\
          \ if len(encoder_outputs) > 2 else None,\n            )\n\n        # decoder\
          \ outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n\
          \        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n\
          \            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n\
          \            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n\
          \            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n\
          \            use_cache=use_cache,\n            output_attentions=output_attentions,\n\
          \            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n\
          \        )\n\n        if not return_dict:\n            return decoder_outputs\
          \ + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n\
          \            past_key_values=decoder_outputs.past_key_values,\n        \
          \    decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n\
          \            cross_attentions=decoder_outputs.cross_attentions,\n      \
          \      encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n  \
          \          encoder_hidden_states=encoder_outputs.hidden_states,\n      \
          \      encoder_attentions=encoder_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n\
          \    \"The Whisper Model with a language modeling head. Can be used for\
          \ automatic speech recognition.\",\n    WHISPER_START_DOCSTRING,\n)\n# Copied\
          \ from transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration\n\
          class WhisperBnbForConditionalGeneration(WhisperPreTrainedModel):\n    base_model_prefix\
          \ = \"model\"\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.version\"\
          ,\n        r\"decoder.version\",\n        r\"proj_out.weight\",\n    ]\n\
          \    _keys_to_ignore_on_save = [\n        r\"proj_out.weight\",\n    ]\n\
          \n    def __init__(self, config: WhisperConfig):\n        super().__init__(config)\n\
          \        self.model = WhisperModel(config)\n        self.proj_out = nn.Linear(config.d_model,\
          \ config.vocab_size, bias=False)\n\n        # Initialize weights and apply\
          \ final processing\n        self.post_init()\n\n    def get_encoder(self):\n\
          \        return self.model.get_encoder()\n\n    def get_decoder(self):\n\
          \        return self.model.get_decoder()\n\n    def resize_token_embeddings(self,\
          \ new_num_tokens: int) -> nn.Embedding:\n        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n\
          \        return new_embeddings\n\n    def get_output_embeddings(self):\n\
          \        return self.proj_out\n\n    def set_output_embeddings(self, new_embeddings):\n\
          \        self.proj_out = new_embeddings\n\n    def freeze_encoder(self):\n\
          \        \"\"\"\n        Calling this function will disable the gradient\
          \ computation for the Whisper encoder so that its parameters will\n    \
          \    not be updated during training.\n        \"\"\"\n        self.model.encoder._freeze_parameters()\n\
          \n    @add_start_docstrings_to_model_forward(WHISPER_INPUTS_DOCSTRING)\n\
          \    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n\
          \    def forward(\n        self,\n        input_features: Optional[torch.FloatTensor]\
          \ = None,\n        attention_mask: Optional[torch.LongTensor] = None,\n\
          \        decoder_input_ids: Optional[torch.LongTensor] = None,\n       \
          \ decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask:\
          \ Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor]\
          \ = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n\
          \        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n\
          \        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n\
          \        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n\
          \        labels: Optional[torch.LongTensor] = None,\n        use_cache:\
          \ Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n\
          \        output_hidden_states: Optional[bool] = None,\n        return_dict:\
          \ Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n\
          \        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,\
          \ sequence_length)`, *optional*):\n            Labels for computing the\
          \ language modeling loss. Indices should either be in `[0, transformers.,\
          \ config.vocab_size]`\n            or -100 (see `input_ids` docstring).\
          \ Tokens with indices set to `-100` are ignored (masked), the loss is\n\
          \            only computed for the tokens with labels in `[0, transformers.,\
          \ config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n     \
          \   ```python\n        >>> import torch\n        >>> from transformers import\
          \ AutoProcessor, WhisperForConditionalGeneration\n        >>> from datasets\
          \ import load_dataset\n\n        >>> processor = AutoProcessor.from_pretrained(\"\
          openai/whisper-tiny.en\")\n        >>> model = WhisperForConditionalGeneration.from_pretrained(\"\
          openai/whisper-tiny.en\")\n\n        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\")\n\n        >>> inputs = processor(ds[0][\"\
          audio\"][\"array\"], return_tensors=\"pt\")\n        >>> input_features\
          \ = inputs.input_features\n\n        >>> generated_ids = model.generate(inputs=input_features)\n\
          \n        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\
          \        >>> transcription\n        ' Mr. Quilter is the apostle of the\
          \ middle classes, and we are glad to welcome his gospel.'\n        ```\"\
          \"\"\n        return_dict = return_dict if return_dict is not None else\
          \ self.config.use_return_dict\n\n        if labels is not None:\n      \
          \      if decoder_input_ids is None and decoder_inputs_embeds is None:\n\
          \                decoder_input_ids = shift_tokens_right(\n             \
          \       labels, self.config.pad_token_id, self.config.decoder_start_token_id\n\
          \                )\n\n        outputs = self.model(\n            input_features,\n\
          \            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n\
          \            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n\
          \            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n\
          \            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n\
          \            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n\
          \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
          \            return_dict=return_dict,\n        )\n        lm_logits = self.proj_out(outputs[0])\n\
          \n        loss = None\n        if labels is not None:\n            loss_fct\
          \ = CrossEntropyLoss()\n            loss = loss_fct(lm_logits.view(-1, self.config.vocab_size),\
          \ labels.reshape(-1))\n\n        if not return_dict:\n            output\
          \ = (lm_logits,) + outputs[1:]\n            return ((loss,) + output) if\
          \ loss is not None else output\n\n        return Seq2SeqLMOutput(\n    \
          \        loss=loss,\n            logits=lm_logits,\n            past_key_values=outputs.past_key_values,\n\
          \            decoder_hidden_states=outputs.decoder_hidden_states,\n    \
          \        decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n\
          \            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n\
          \            encoder_hidden_states=outputs.encoder_hidden_states,\n    \
          \        encoder_attentions=outputs.encoder_attentions,\n        )\n\n \
          \   def generate(\n        self,\n        inputs: Optional[torch.Tensor]\
          \ = None,\n        generation_config=None,\n        logits_processor=None,\n\
          \        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n\
          \        synced_gpus=False,\n        return_timestamps=None,\n        task=None,\n\
          \        language=None,\n        is_multilingual=None,\n        **kwargs,\n\
          \    ):\n        \"\"\"\n\n        Generates sequences of token ids for\
          \ models with a language modeling head.\n\n        <Tip warning={true}>\n\
          \n        Most generation-controlling parameters are set in `generation_config`\
          \ which, if not passed, will be set to the\n        model's default generation\
          \ configuration. You can override any `generation_config` by passing the\
          \ corresponding\n        parameters to generate(), e.g. `.generate(inputs,\
          \ num_beams=4, do_sample=True)`.\n\n        For an overview of generation\
          \ strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\
          \n        </Tip>\n\n        Parameters:\n            inputs (`torch.Tensor`\
          \ of varying shape depending on the modality, *optional*):\n           \
          \     The sequence used as a prompt for the generation or as model inputs\
          \ to the encoder. If `None` the\n                method initializes it with\
          \ `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n\
          \                should of in the format of `input_ids`. For encoder-decoder\
          \ models *inputs* can represent any of\n                `input_ids`, `input_values`,\
          \ `input_features`, or `pixel_values`.\n            generation_config (`~generation.GenerationConfig`,\
          \ *optional*):\n                The generation configuration to be used\
          \ as base parametrization for the generation call. `**kwargs`\n        \
          \        passed to generate matching the attributes of `generation_config`\
          \ will override them. If\n                `generation_config` is not provided,\
          \ the default will be used, which had the following loading\n          \
          \      priority: 1) from the `generation_config.json` model file, if it\
          \ exists; 2) from the model\n                configuration. Please note\
          \ that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n\
          \                default values, whose documentation should be checked to\
          \ parameterize generation.\n            logits_processor (`LogitsProcessorList`,\
          \ *optional*):\n                Custom logits processors that complement\
          \ the default logits processors built from arguments and\n             \
          \   generation config. If a logit processor is passed that is already created\
          \ with the arguments or a\n                generation config an error is\
          \ thrown. This feature is intended for advanced users.\n            stopping_criteria\
          \ (`StoppingCriteriaList`, *optional*):\n                Custom stopping\
          \ criteria that complement the default stopping criteria built from arguments\
          \ and a\n                generation config. If a stopping criteria is passed\
          \ that is already created with the arguments or a\n                generation\
          \ config an error is thrown. This feature is intended for advanced users.\n\
          \            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`,\
          \ *optional*):\n                If provided, this function constraints the\
          \ beam search to allowed tokens only at each step. If not\n            \
          \    provided no constraint is applied. This function takes 2 arguments:\
          \ the batch ID `batch_id` and\n                `input_ids`. It has to return\
          \ a list with the allowed tokens for the next generation step conditioned\n\
          \                on the batch ID `batch_id` and the previously generated\
          \ tokens `inputs_ids`. This argument is useful\n                for constrained\
          \ generation conditioned on the prefix, as described in [Autoregressive\
          \ Entity\n                Retrieval](https://arxiv.org/abs/2010.00904).\n\
          \            synced_gpus (`bool`, *optional*, defaults to `False`):\n  \
          \              Whether to continue running the while loop until max_length\
          \ (needed for ZeRO stage 3)\n            return_timestamps (`bool`, *optional*):\n\
          \                Whether to return the timestamps with the text. This enables\
          \ the `WhisperTimestampsLogitsProcessor`.\n            task (`bool`, *optional*):\n\
          \                Task to use for generation, either \"translate\" or \"\
          transcribe\". The `model.config.forced_decoder_ids`\n                will\
          \ be updated accordingly.\n            language (`bool`, *optional*):\n\
          \                Language token to use for generation, should be in the\
          \ form `<|en|>`. You can find all the possible\n                language\
          \ tokens in the `model.generation_config.lang_to_id` dictionary.\n     \
          \       is_multilingual (`bool`, *optional*):\n                Whether or\
          \ not the model is multilingual.\n            kwargs:\n                Ad\
          \ hoc parametrization of `generate_config` and/or additional model-specific\
          \ kwargs that will be\n                forwarded to the `forward` function\
          \ of the model. If the model is an encoder-decoder model, encoder\n    \
          \            specific kwargs should not be prefixed and decoder specific\
          \ kwargs should be prefixed with *decoder_*.\n\n        Return:\n      \
          \      [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`]\
          \ (if `return_dict_in_generate=True`\n            or when `config.return_dict_in_generate=True`)\
          \ or a `torch.FloatTensor`.\n\n                If the model is *not* an\
          \ encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n\
          \                [`~utils.ModelOutput`] types are:\n\n                 \
          \   - [`~generation.GreedySearchDecoderOnlyOutput`],\n                 \
          \   - [`~generation.SampleDecoderOnlyOutput`],\n                    - [`~generation.BeamSearchDecoderOnlyOutput`],\n\
          \                    - [`~generation.BeamSampleDecoderOnlyOutput`]\n\n \
          \               If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),\
          \ the possible\n                [`~utils.ModelOutput`] types are:\n\n  \
          \                  - [`~generation.GreedySearchEncoderDecoderOutput`],\n\
          \                    - [`~generation.SampleEncoderDecoderOutput`],\n   \
          \                 - [`~generation.BeamSearchEncoderDecoderOutput`],\n  \
          \                  - [`~generation.BeamSampleEncoderDecoderOutput`]\n  \
          \      \"\"\"\n        if generation_config is None:\n            generation_config\
          \ = self.generation_config\n\n        if return_timestamps is not None:\n\
          \            generation_config.return_timestamps = return_timestamps\n\n\
          \        if task is not None:\n            generation_config.task = task\n\
          \n        if is_multilingual is not None:\n            generation_config.is_multilingual\
          \ = is_multilingual\n\n        if language is not None:\n            generation_config.language\
          \ = language\n\n        forced_decoder_ids = []\n\n        if hasattr(generation_config,\
          \ \"is_multilingual\") and generation_config.is_multilingual:\n        \
          \    if hasattr(generation_config, \"language\"):\n                forced_decoder_ids.append((1,\
          \ generation_config.lang_to_id[generation_config.language]))\n         \
          \   else:\n                forced_decoder_ids.append((1, None))\n\n    \
          \        if hasattr(generation_config, \"task\"):\n                forced_decoder_ids.append((2,\
          \ generation_config.task_to_id[generation_config.task]))\n            else:\n\
          \                forced_decoder_ids.append((2, generation_config.task_to_id[\"\
          transcribe\"]))\n\n        if (\n            hasattr(generation_config,\
          \ \"return_timestamps\") and generation_config.return_timestamps\n     \
          \   ) or return_timestamps:\n            logits_processor = [WhisperTimeStampLogitsProcessor(generation_config)]\n\
          \        else:\n            if forced_decoder_ids and forced_decoder_ids[-1][0]\
          \ != generation_config.no_timestamps_token_id:\n                idx = forced_decoder_ids[-1][0]\
          \ + 1 if forced_decoder_ids else 1\n                forced_decoder_ids.append((idx,\
          \ generation_config.no_timestamps_token_id))\n\n        if len(forced_decoder_ids)\
          \ > 0:\n            generation_config.forced_decoder_ids = forced_decoder_ids\n\
          \n        return super().generate(\n            inputs,\n            generation_config,\n\
          \            logits_processor,\n            stopping_criteria,\n       \
          \     prefix_allowed_tokens_fn,\n            synced_gpus,\n            **kwargs,\n\
          \        )\n\n    def prepare_inputs_for_generation(\n        self,\n  \
          \      decoder_input_ids,\n        past_key_values=None,\n        use_cache=None,\n\
          \        encoder_outputs=None,\n        attention_mask=None,\n        **kwargs,\n\
          \    ):\n        # cut decoder_input_ids if past is used\n        if past_key_values\
          \ is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\
          \n        return {\n            \"encoder_outputs\": encoder_outputs,\n\
          \            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\"\
          : decoder_input_ids,\n            \"use_cache\": use_cache,\n          \
          \  \"decoder_attention_mask\": None,\n        }\n\n    #\n    @staticmethod\n\
          \    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past\
          \ = ()\n        for layer_past in past_key_values:\n            reordered_past\
          \ += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n\
          \        return reordered_past\n```\n\n</details>\n\nAnd now import the\
          \ model from this file:\n```python\nfrom modeling_whisper import WhisperForConditionalGeneration\n\
          \n# do your thing\n```\n\nFor modifying the loss, first import the required\
          \ loss:\n```diff\n- from torch.nn import CrossEntropyLoss\n+ from torch.nn\
          \ import HingeEmbeddingLoss\n```\n\nAnd then change the loss function:\n\
          ```diff\n        if labels is not None:\n-           loss_fct = CrossEntropyLoss()\n\
          +           loss_fct = HingeEmbeddingLoss()\n            loss = loss_fct(lm_logits.view(-1,\
          \ self.config.vocab_size), labels.reshape(-1))\n```"
        updatedAt: '2023-03-17T15:44:43.049Z'
      numEdits: 2
      reactions: []
    id: 64148a72315d256ba75a7fd7
    type: comment
  author: sanchit-gandhi
  content: "Hey @Martha-987! You can either follow the approach I've outlined for\
    \ modifying the source code, or copy and paste the transformers code and make\
    \ modifications there.\n\nE.g. copy this code into a file called `modeling_whisper.py`\n\
    \n<details>\n\n<summary> Code: </summary>\n\n```python\n# coding=utf-8\n# Copyright\
    \ 2022 The OpenAI Authors and The HuggingFace Inc. team. All rights reserved.\n\
    #\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you\
    \ may not use this file except in compliance with the License.\n# You may obtain\
    \ a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n\
    #\n# Unless required by applicable law or agreed to in writing, software\n# distributed\
    \ under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES\
    \ OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for\
    \ the specific language governing permissions and\n# limitations under the License.\n\
    \"\"\" PyTorch Whisper model.\"\"\"\n\n\nimport math\nimport random\nfrom typing\
    \ import Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\n\
    from torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers.activations\
    \ import ACT2FN\nfrom transformers.generation.logits_process import WhisperTimeStampLogitsProcessor\n\
    from transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n\
    \    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\nfrom transformers.modeling_utils\
    \ import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n\
    \    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n\
    )\nfrom transformers import WhisperConfig\n\n\nlogger = logging.get_logger(__name__)\n\
    \n_CONFIG_FOR_DOC = \"WhisperConfig\"\n_CHECKPOINT_FOR_DOC = \"openai/whisper-tiny\"\
    \n\n\nWHISPER_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"openai/whisper-base\",\n\
    \    # See all Whisper models at https://huggingface.co/models?filter=whisper\n\
    ]\n\n\n# Copied from transformers.models.whisper.modeling_whisper.shift_tokens_right\n\
    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id:\
    \ int):\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n\
    \    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:,\
    \ 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\
    \n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id\
    \ has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n\
    \    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\
    \n    return shifted_input_ids\n\n\n# Copied from transformers.models.whisper.modeling_whisper._make_causal_mask\n\
    def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length:\
    \ int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n\
    \    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len,\
    \ tgt_len), torch.tensor(torch.finfo(dtype).min))\n    mask_cond = torch.arange(mask.size(-1))\n\
    \    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n\
    \    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask\
    \ = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask],\
    \ dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len +\
    \ past_key_values_length)\n\n\n# Copied from transformers.models.whisper.modeling_whisper._expand_mask\n\
    def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int]\
    \ = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz,\
    \ 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n\
    \    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask\
    \ = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask\
    \ = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool),\
    \ torch.finfo(dtype).min)\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperPositionalEmbedding\n\
    class WhisperPositionalEmbedding(nn.Embedding):\n    def __init__(self, num_positions:\
    \ int, embedding_dim: int, padding_idx: Optional[int] = None):\n        super().__init__(num_positions,\
    \ embedding_dim)\n\n    def forward(self, input_ids, past_key_values_length=0):\n\
    \        return self.weight[past_key_values_length : past_key_values_length +\
    \ input_ids.shape[-1]]\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperAttention\n\
    class WhisperAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention\
    \ Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        embed_dim:\
    \ int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder:\
    \ bool = False,\n        bias: bool = True,\n    ):\n        super().__init__()\n\
    \        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n    \
    \    self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n\n\
    \        if (self.head_dim * num_heads) != self.embed_dim:\n            raise\
    \ ValueError(\n                f\"embed_dim must be divisible by num_heads (got\
    \ `embed_dim`: {self.embed_dim}\"\n                f\" and `num_heads`: {num_heads}).\"\
    \n            )\n        self.scaling = self.head_dim**-0.5\n        self.is_decoder\
    \ = is_decoder\n\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n\
    \        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj\
    \ = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim,\
    \ embed_dim, bias=bias)\n\n    # Copied from transformers.models.whisper.modeling_whisper.BartAttention._shape\
    \ with BART->whisper\n    def _shape(self, tensor: torch.Tensor, seq_len: int,\
    \ bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1,\
    \ 2).contiguous()\n\n    # Copied from transformers.models.whisper.modeling_whisper.BartAttention.forward\
    \ with BART->whisper\n    def forward(\n        self,\n        hidden_states:\
    \ torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n  \
    \      past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask:\
    \ Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor]\
    \ = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor,\
    \ Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input\
    \ shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided\
    \ this layer is used as a cross-attention layer\n        # for the decoder\n \
    \       is_cross_attention = key_value_states is not None\n\n        bsz, tgt_len,\
    \ _ = hidden_states.size()\n\n        # get query proj\n        query_states =\
    \ self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n \
    \       # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n        #\
    \ is checking that the `sequence_length` of the `past_key_value` is the same as\n\
    \        # the provided `key_value_states` to support prefix tuning\n        if\
    \ (\n            is_cross_attention\n            and past_key_value is not None\n\
    \            and past_key_value[0].shape[2] == key_value_states.shape[1]\n   \
    \     ):\n            # reuse k,v, cross_attentions\n            key_states =\
    \ past_key_value[0]\n            value_states = past_key_value[1]\n        elif\
    \ is_cross_attention:\n            # cross_attentions\n            key_states\
    \ = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states\
    \ = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value\
    \ is not None:\n            # reuse k, v, self_attention\n            key_states\
    \ = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states\
    \ = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states =\
    \ torch.cat([past_key_value[0], key_states], dim=2)\n            value_states\
    \ = torch.cat([past_key_value[1], value_states], dim=2)\n        else:\n     \
    \       # self_attention\n            key_states = self._shape(self.k_proj(hidden_states),\
    \ -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states),\
    \ -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save\
    \ Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n\
    \            # Further calls to cross_attention layer can then reuse all cross-attention\n\
    \            # key/value_states (first \"if\" case)\n            # if uni-directional\
    \ self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n       \
    \     # all previous decoder key/value_states. Further calls to uni-directional\
    \ self-attention\n            # can concat previous decoder key/value_states to\
    \ current projected key/value_states (third \"elif\" case)\n            # if encoder\
    \ bi-directional self-attention `past_key_value` is always `None`\n          \
    \  past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz *\
    \ self.num_heads, -1, self.head_dim)\n        query_states = self._shape(query_states,\
    \ tgt_len, bsz).view(*proj_shape)\n        key_states = key_states.reshape(*proj_shape)\n\
    \        value_states = value_states.reshape(*proj_shape)\n\n        src_len =\
    \ key_states.size(1)\n        attn_weights = torch.bmm(query_states, key_states.transpose(1,\
    \ 2))\n\n        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n\
    \            raise ValueError(\n                f\"Attention weights should be\
    \ of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n             \
    \   f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is\
    \ not None:\n            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n\
    \                raise ValueError(\n                    f\"Attention mask should\
    \ be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n\
    \                )\n            attn_weights = attn_weights.view(bsz, self.num_heads,\
    \ tgt_len, src_len) + attention_mask\n            attn_weights = attn_weights.view(bsz\
    \ * self.num_heads, tgt_len, src_len)\n\n        attn_weights = nn.functional.softmax(attn_weights,\
    \ dim=-1)\n\n        if layer_head_mask is not None:\n            if layer_head_mask.size()\
    \ != (self.num_heads,):\n                raise ValueError(\n                 \
    \   f\"Head mask for a single layer should be of size {(self.num_heads,)}, but\
    \ is\"\n                    f\" {layer_head_mask.size()}\"\n                )\n\
    \            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz,\
    \ self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights.view(bsz\
    \ * self.num_heads, tgt_len, src_len)\n\n        if output_attentions:\n     \
    \       # this operation is a bit awkward, but it's required to\n            #\
    \ make sure that attn_weights keeps its gradient.\n            # In order to do\
    \ so, attn_weights have to be reshaped\n            # twice and have to be reused\
    \ in the following\n            attn_weights_reshaped = attn_weights.view(bsz,\
    \ self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights_reshaped.view(bsz\
    \ * self.num_heads, tgt_len, src_len)\n        else:\n            attn_weights_reshaped\
    \ = None\n\n        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout,\
    \ training=self.training)\n\n        attn_output = torch.bmm(attn_probs, value_states)\n\
    \n        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n\
    \            raise ValueError(\n                f\"`attn_output` should be of\
    \ size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n          \
    \      f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.view(bsz,\
    \ self.num_heads, tgt_len, self.head_dim)\n        attn_output = attn_output.transpose(1,\
    \ 2)\n\n        # Use the `embed_dim` from the config (stored in the class) rather\
    \ than `hidden_state` because `attn_output` can be\n        # partitioned across\
    \ GPUs when using tensor-parallelism.\n        attn_output = attn_output.reshape(bsz,\
    \ tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\
    \n        return attn_output, attn_weights_reshaped, past_key_value\n\n\n# Copied\
    \ from transformers.models.whisper.modeling_whisper.WhisperEncoderLayer\nclass\
    \ WhisperEncoderLayer(nn.Module):\n    def __init__(self, config: WhisperConfig):\n\
    \        super().__init__()\n        self.embed_dim = config.d_model\n       \
    \ self.self_attn = WhisperAttention(\n            embed_dim=self.embed_dim,\n\
    \            num_heads=config.encoder_attention_heads,\n            dropout=config.attention_dropout,\n\
    \        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\
    \        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n\
    \        self.activation_dropout = config.activation_dropout\n        self.fc1\
    \ = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2 = nn.Linear(config.encoder_ffn_dim,\
    \ self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\
    \n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n    \
    \    attention_mask: torch.Tensor,\n        layer_head_mask: torch.Tensor,\n \
    \       output_attentions: bool = False,\n    ) -> torch.Tensor:\n        \"\"\
    \"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the\
    \ layer of shape `(seq_len, batch, embed_dim)`\n            attention_mask (`torch.FloatTensor`):\
    \ attention mask of size\n                `(batch, 1, tgt_len, src_len)` where\
    \ padding elements are indicated by very large negative values.\n            layer_head_mask\
    \ (`torch.FloatTensor`): mask for attention heads in a given layer of size\n \
    \               `(encoder_attention_heads,)`.\n            output_attentions (`bool`,\
    \ *optional*):\n                Whether or not to return the attentions tensors\
    \ of all attention layers. See `attentions` under\n                returned tensors\
    \ for more detail.\n        \"\"\"\n        residual = hidden_states\n       \
    \ hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states,\
    \ attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n\
    \            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n\
    \            output_attentions=output_attentions,\n        )\n        hidden_states\
    \ = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\
    \        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n\
    \        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states\
    \ = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = nn.functional.dropout(hidden_states,\
    \ p=self.activation_dropout, training=self.training)\n        hidden_states =\
    \ self.fc2(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states,\
    \ p=self.dropout, training=self.training)\n        hidden_states = residual +\
    \ hidden_states\n\n        if hidden_states.dtype == torch.float16 and (\n   \
    \         torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n\
    \        ):\n            clamp_value = torch.finfo(hidden_states.dtype).max -\
    \ 1000\n            hidden_states = torch.clamp(hidden_states, min=-clamp_value,\
    \ max=clamp_value)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n\
    \            outputs += (attn_weights,)\n\n        return outputs\n\n\n# Copied\
    \ from transformers.models.whisper.modeling_whisper.WhisperDecoderLayer\nclass\
    \ WhisperDecoderLayer(nn.Module):\n    def __init__(self, config: WhisperConfig):\n\
    \        super().__init__()\n        self.embed_dim = config.d_model\n\n     \
    \   self.self_attn = WhisperAttention(\n            embed_dim=self.embed_dim,\n\
    \            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n\
    \            is_decoder=True,\n        )\n        self.dropout = config.dropout\n\
    \        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout\
    \ = config.activation_dropout\n\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\
    \        self.encoder_attn = WhisperAttention(\n            self.embed_dim,\n\
    \            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n\
    \            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm\
    \ = nn.LayerNorm(self.embed_dim)\n        self.fc1 = nn.Linear(self.embed_dim,\
    \ config.decoder_ffn_dim)\n        self.fc2 = nn.Linear(config.decoder_ffn_dim,\
    \ self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\
    \n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n    \
    \    attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states:\
    \ Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor]\
    \ = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_layer_head_mask:\
    \ Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]]\
    \ = None,\n        output_attentions: Optional[bool] = False,\n        use_cache:\
    \ Optional[bool] = True,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n\
    \            hidden_states (`torch.FloatTensor`): input to the layer of shape\
    \ `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`):\
    \ attention mask of size\n                `(batch, 1, tgt_len, src_len)` where\
    \ padding elements are indicated by very large negative values.\n            encoder_hidden_states\
    \ (`torch.FloatTensor`):\n                cross attention input to the layer of\
    \ shape `(batch, seq_len, embed_dim)`\n            encoder_attention_mask (`torch.FloatTensor`):\
    \ encoder attention mask of size\n                `(batch, 1, tgt_len, src_len)`\
    \ where padding elements are indicated by very large negative values.\n      \
    \      layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given\
    \ layer of size\n                `(encoder_attention_heads,)`.\n            cross_attn_layer_head_mask\
    \ (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n\
    \                size `(decoder_attention_heads,)`.\n            past_key_value\
    \ (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n\
    \            output_attentions (`bool`, *optional*):\n                Whether\
    \ or not to return the attentions tensors of all attention layers. See `attentions`\
    \ under\n                returned tensors for more detail.\n        \"\"\"\n \
    \       residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\
    \n        # Self Attention\n        # decoder uni-directional self-attention cached\
    \ key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2]\
    \ if past_key_value is not None else None\n        # add present self-attn cache\
    \ to positions 1,2 of present_key_value tuple\n        hidden_states, self_attn_weights,\
    \ present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n\
    \            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n\
    \            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n\
    \        )\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout,\
    \ training=self.training)\n        hidden_states = residual + hidden_states\n\n\
    \        # Cross-Attention Block\n        cross_attn_present_key_value = None\n\
    \        cross_attn_weights = None\n        if encoder_hidden_states is not None:\n\
    \            residual = hidden_states\n            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n\
    \n            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value\
    \ tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value\
    \ is not None else None\n            hidden_states, cross_attn_weights, cross_attn_present_key_value\
    \ = self.encoder_attn(\n                hidden_states=hidden_states,\n       \
    \         key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n\
    \                layer_head_mask=cross_attn_layer_head_mask,\n               \
    \ past_key_value=cross_attn_past_key_value,\n                output_attentions=output_attentions,\n\
    \            )\n            hidden_states = nn.functional.dropout(hidden_states,\
    \ p=self.dropout, training=self.training)\n            hidden_states = residual\
    \ + hidden_states\n\n            # add cross-attn to positions 3,4 of present_key_value\
    \ tuple\n            present_key_value = present_key_value + cross_attn_present_key_value\n\
    \n        # Fully Connected\n        residual = hidden_states\n        hidden_states\
    \ = self.final_layer_norm(hidden_states)\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n\
    \        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout,\
    \ training=self.training)\n        hidden_states = self.fc2(hidden_states)\n \
    \       hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\
    \        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\
    \n        if output_attentions:\n            outputs += (self_attn_weights, cross_attn_weights)\n\
    \n        if use_cache:\n            outputs += (present_key_value,)\n\n     \
    \   return outputs\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperPreTrainedModel\n\
    class WhisperPreTrainedModel(PreTrainedModel):\n    config_class = WhisperConfig\n\
    \    base_model_prefix = \"model\"\n    main_input_name = \"input_features\"\n\
    \    supports_gradient_checkpointing = True\n    _no_split_modules = [\"WhisperEncoderLayer\"\
    ]\n\n    def _init_weights(self, module):\n        std = self.config.init_std\n\
    \        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            module.weight.data.normal_(mean=0.0,\
    \ std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\
    \        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0,\
    \ std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\
    \n    def _set_gradient_checkpointing(self, module, value=False):\n        if\
    \ isinstance(module, (WhisperDecoder, WhisperEncoder)):\n            module.gradient_checkpointing\
    \ = value\n\n    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n\
    \        \"\"\"\n        Computes the output length of the convolutional layers\n\
    \        \"\"\"\n        input_lengths = (input_lengths - 1) // 2 + 1\n\n    \
    \    return input_lengths\n\n\nWHISPER_START_DOCSTRING = r\"\"\"\n    This model\
    \ inherits from [`PreTrainedModel`]. Check the superclass documentation for the\
    \ generic methods the\n    library implements for all its model (such as downloading\
    \ or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This\
    \ model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)\
    \ subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation\
    \ for all matter related to general usage\n    and behavior.\n\n    Parameters:\n\
    \        config ([`WhisperConfig`]):\n            Model configuration class with\
    \ all the parameters of the model. Initializing with a config file does not\n\
    \            load the weights associated with the model, only the configuration.\
    \ Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load\
    \ the model weights.\n\"\"\"\n\nWHISPER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n\
    \        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size,\
    \ sequence_length)`):\n            Float values mel features extracted from the\
    \ raw speech waveform. Raw speech waveform can be obtained by\n            loading\
    \ a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,\
    \ *e.g.* via\n            the soundfile library (`pip install soundfile`). To\
    \ prepare the array into `input_features`, the\n            [`AutoFeatureExtractor`]\
    \ should be used for extracting the mel features, padding and conversion into\
    \ a\n            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n\
    \        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`,\
    \ *optional*):\n            Mask to avoid performing *SpecAugment* data augmentation\
    \ on padding token indices. Mask values selected in\n            `[0, 1]`:\n\n\
    \            - 1 for tokens that are **not masked**,\n            - 0 for tokens\
    \ that are **masked**.\n\n            [What are attention masks?](transformers/glossary#attention-mask)\n\
    \        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,\
    \ *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\
    \n            Indices can be obtained using [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`]\
    \ and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n         \
    \   [What are decoder input IDs?](transformers/glossary#decoder-input-ids)\n\n\
    \            Whisper uses the `decoder_start_token_id` as the starting token for\
    \ `decoder_input_ids` generation. If\n            `past_key_values` is used, optionally\
    \ only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n\
    \        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,\
    \ *optional*):\n            Default behavior: generate a tensor that ignores pad\
    \ tokens in `decoder_input_ids`. Causal mask will also\n            be used by\
    \ default.\n\n            If you want to change padding behavior, you should read\n\
    \            [`modeling_whisper._prepare_decoder_attention_mask`] and modify to\
    \ your needs. See diagram 1 in [the BART\n            paper](https://arxiv.org/abs/1910.13461)\
    \ for more information on the default strategy.\n        head_mask (`torch.Tensor`\
    \ of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n       \
    \     Mask to nullify selected heads of the attention modules in the encoder.\
    \ Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not\
    \ masked**,\n            - 0 indicates the head is **masked**.\n\n        decoder_head_mask\
    \ (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n\
    \            Mask to nullify selected heads of the attention modules in the decoder.\
    \ Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not\
    \ masked**,\n            - 0 indicates the head is **masked**.\n\n        cross_attn_head_mask\
    \ (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n\
    \            Mask to nullify selected heads of the cross-attention modules. Mask\
    \ values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not\
    \ masked**,\n            - 0 indicates the head is **masked**.\n\n        encoder_outputs\
    \ (`tuple(tuple(torch.FloatTensor)`, *optional*):\n            Tuple consists\
    \ of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n\
    \            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`,\
    \ *optional*) is a sequence of\n            hidden-states at the output of the\
    \ last layer of the encoder. Used in the cross-attention of the decoder.\n   \
    \     past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned\
    \ when `use_cache=True` is passed or when `config.use_cache=True`):\n        \
    \    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each\
    \ tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length,\
    \ embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size,\
    \ num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains\
    \ pre-computed hidden-states (key and values in the self-attention blocks and\
    \ in the cross-attention\n            blocks) that can be used (see `past_key_values`\
    \ input) to speed up sequential decoding.\n\n            If `past_key_values`\
    \ are used, the user can optionally input only the last `decoder_input_ids` (those\
    \ that\n            don't have their past key value states given to this model)\
    \ of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of\
    \ shape `(batch_size, sequence_length)`.\n        decoder_inputs_embeds (`torch.FloatTensor`\
    \ of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n\
    \            Optionally, instead of passing `decoder_input_ids` you can choose\
    \ to directly pass an embedded\n            representation. If `past_key_values`\
    \ is used, optionally only the last `decoder_inputs_embeds` have to be\n     \
    \       input (see `past_key_values`). This is useful if you want more control\
    \ over how to convert\n            `decoder_input_ids` indices into associated\
    \ vectors than the model's internal embedding lookup matrix.\n        use_cache\
    \ (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value\
    \ states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n\
    \        output_attentions (`bool`, *optional*):\n            Whether or not to\
    \ return the attentions tensors of all attention layers. See `attentions` under\
    \ returned\n            tensors for more detail.\n        output_hidden_states\
    \ (`bool`, *optional*):\n            Whether or not to return the hidden states\
    \ of all layers. See `hidden_states` under returned tensors for\n            more\
    \ detail.\n        return_dict (`bool`, *optional*):\n            Whether or not\
    \ to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\nWHISPER_ENCODER_INPUTS_DOCSTRING\
    \ = r\"\"\"\n    Args:\n        input_features (`torch.FloatTensor` of shape `(batch_size,\
    \ feature_size, sequence_length)`):\n            Float values mel features extracted\
    \ from the raw speech waveform. Raw speech waveform can be obtained by\n     \
    \       loading a `.flac` or `.wav` audio file into an array of type `List[float]`\
    \ or a `numpy.ndarray`, *e.g.* via\n            the soundfile library (`pip install\
    \ soundfile`). To prepare the array into `input_features`, the\n            [`AutoFeatureExtractor`]\
    \ should be used for extracting the mel features, padding and conversion into\
    \ a\n            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n\
    \        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,\
    \ *optional*):\n            Mask to nullify selected heads of the attention modules\
    \ in the encoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates\
    \ the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\
    \        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n   \
    \         Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`,\
    \ *optional*: `attentions`)\n            `last_hidden_state` of shape `(batch_size,\
    \ sequence_length, hidden_size)`, *optional*) is a sequence of\n            hidden-states\
    \ at the output of the last layer of the encoder.\n        output_attentions (`bool`,\
    \ *optional*):\n            Whether or not to return the attentions tensors of\
    \ all attention layers. See `attentions` under returned\n            tensors for\
    \ more detail.\n        output_hidden_states (`bool`, *optional*):\n         \
    \   Whether or not to return the hidden states of all layers. See `hidden_states`\
    \ under returned tensors for\n            more detail.\n        return_dict (`bool`,\
    \ *optional*):\n            Whether or not to return a [`~utils.ModelOutput`]\
    \ instead of a plain tuple.\n\"\"\"\n\n\n# Copied from transformers.models.whisper.modeling_whisper.WhisperEncoder\n\
    class WhisperEncoder(WhisperPreTrainedModel):\n    \"\"\"\n    Transformer encoder\
    \ consisting of *config.encoder_layers* self attention layers. Each layer is a\n\
    \    [`WhisperEncoderLayer`].\n\n    Args:\n        config: WhisperConfig\n  \
    \      embed_tokens (nn.Embedding): output embedding\n    \"\"\"\n\n    def __init__(self,\
    \ config: WhisperConfig):\n        super().__init__(config)\n        self.dropout\
    \ = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\n    \
    \    embed_dim = config.d_model\n        self.num_mel_bins = config.num_mel_bins\n\
    \        self.padding_idx = config.pad_token_id\n        self.max_source_positions\
    \ = config.max_source_positions\n        self.embed_scale = math.sqrt(embed_dim)\
    \ if config.scale_embedding else 1.0\n\n        self.conv1 = nn.Conv1d(self.num_mel_bins,\
    \ embed_dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(embed_dim,\
    \ embed_dim, kernel_size=3, stride=2, padding=1)\n\n        self.embed_positions\
    \ = nn.Embedding(self.max_source_positions, embed_dim)\n\n        self.layers\
    \ = nn.ModuleList([WhisperEncoderLayer(config) for _ in range(config.encoder_layers)])\n\
    \        self.layer_norm = nn.LayerNorm(config.d_model)\n\n        self.gradient_checkpointing\
    \ = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\
    \n    def _freeze_parameters(self):\n        for param in self.parameters():\n\
    \            param.requires_grad = False\n        self._requires_grad = False\n\
    \n    def forward(\n        self,\n        input_features,\n        attention_mask=None,\n\
    \        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n\
    \        return_dict=None,\n    ):\n        r\"\"\"\n        Args:\n         \
    \   input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n\
    \                Float values of mel features extracted from the raw speech waveform.\
    \ Raw speech waveform can be\n                obtained by loading a `.flac` or\
    \ `.wav` audio file into an array of type `List[float]` or a\n               \
    \ `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`).\
    \ To prepare the array into\n                `input_features`, the [`AutoFeatureExtractor`]\
    \ should be used for extracting the mel features, padding\n                and\
    \ conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n\
    \            attention_mask (`torch.Tensor`)`, *optional*):\n                Whisper\
    \ does not support masking of the `input_features`, this argument is preserved\
    \ for compatibility,\n                but it is not used. By default the silence\
    \ in the input log mel spectrogram are ignored.\n            head_mask (`torch.Tensor`\
    \ of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n       \
    \         Mask to nullify selected heads of the attention modules. Mask values\
    \ selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n\
    \                - 0 indicates the head is **masked**.\n            output_attentions\
    \ (`bool`, *optional*):\n                Whether or not to return the attentions\
    \ tensors of all attention layers. See `attentions` under\n                returned\
    \ tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n\
    \                Whether or not to return the hidden states of all layers. See\
    \ `hidden_states` under returned tensors\n                for more detail.\n \
    \           return_dict (`bool`, *optional*):\n                Whether or not\
    \ to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\
    \n        output_attentions = output_attentions if output_attentions is not None\
    \ else self.config.output_attentions\n        output_hidden_states = (\n     \
    \       output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n\
    \        )\n        return_dict = return_dict if return_dict is not None else\
    \ self.config.use_return_dict\n        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n\
    \        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n\n   \
    \     inputs_embeds = inputs_embeds.permute(0, 2, 1)\n        embed_pos = self.embed_positions.weight\n\
    \n        hidden_states = inputs_embeds + embed_pos\n        hidden_states = nn.functional.dropout(hidden_states,\
    \ p=self.dropout, training=self.training)\n\n        encoder_states = () if output_hidden_states\
    \ else None\n        all_attentions = () if output_attentions else None\n\n  \
    \      # check if head_mask has a correct number of layers specified if desired\n\
    \        if head_mask is not None:\n            assert head_mask.size()[0] ==\
    \ (\n                len(self.layers)\n            ), f\"The head_mask should\
    \ be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\
    \n\n        for idx, encoder_layer in enumerate(self.layers):\n            if\
    \ output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n\
    \            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\
    \            dropout_probability = random.uniform(0, 1)\n            if self.training\
    \ and (dropout_probability < self.layerdrop):  # skip the layer\n            \
    \    layer_outputs = (None, None)\n            else:\n                if self.gradient_checkpointing\
    \ and self.training:\n\n                    def create_custom_forward(module):\n\
    \                        def custom_forward(*inputs):\n                      \
    \      return module(*inputs, output_attentions)\n\n                        return\
    \ custom_forward\n\n                    layer_outputs = torch.utils.checkpoint.checkpoint(\n\
    \                        create_custom_forward(encoder_layer),\n             \
    \           hidden_states,\n                        None,\n                  \
    \      (head_mask[idx] if head_mask is not None else None),\n                \
    \    )\n                else:\n                    layer_outputs = encoder_layer(\n\
    \                        hidden_states,\n                        None,\n     \
    \                   layer_head_mask=(head_mask[idx] if head_mask is not None else\
    \ None),\n                        output_attentions=output_attentions,\n     \
    \               )\n\n                hidden_states = layer_outputs[0]\n\n    \
    \        if output_attentions:\n                all_attentions = all_attentions\
    \ + (layer_outputs[1],)\n\n        hidden_states = self.layer_norm(hidden_states)\n\
    \        if output_hidden_states:\n            encoder_states = encoder_states\
    \ + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v\
    \ for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n\
    \        return BaseModelOutput(\n            last_hidden_state=hidden_states,\
    \ hidden_states=encoder_states, attentions=all_attentions\n        )\n\n\n# Copied\
    \ from transformers.models.whisper.modeling_whisper.WhisperDecoder\nclass WhisperDecoder(WhisperPreTrainedModel):\n\
    \    \"\"\"\n    Transformer decoder consisting of *config.decoder_layers* layers.\
    \ Each layer is a [`WhisperDecoderLayer`]\n\n    Args:\n        config: WhisperConfig\n\
    \    \"\"\"\n\n    def __init__(self, config: WhisperConfig):\n        super().__init__(config)\n\
    \        self.dropout = config.dropout\n        self.layerdrop = config.decoder_layerdrop\n\
    \        self.padding_idx = config.pad_token_id\n        self.max_target_positions\
    \ = config.max_target_positions\n        self.max_source_positions = config.max_source_positions\n\
    \        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding\
    \ else 1.0\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model,\
    \ self.padding_idx)\n        self.embed_positions = WhisperPositionalEmbedding(self.max_target_positions,\
    \ config.d_model)\n\n        self.layers = nn.ModuleList([WhisperDecoderLayer(config)\
    \ for _ in range(config.decoder_layers)])\n\n        self.layer_norm = nn.LayerNorm(config.d_model)\n\
    \n        self.gradient_checkpointing = False\n        # Initialize weights and\
    \ apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n\
    \        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n\
    \        self.embed_tokens = value\n\n    def _prepare_decoder_attention_mask(self,\
    \ attention_mask, input_shape, inputs_embeds, past_key_values_length):\n     \
    \   # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n\
    \        combined_attention_mask = None\n\n        if input_shape[-1] > 1:\n \
    \           combined_attention_mask = _make_causal_mask(\n                input_shape,\
    \ inputs_embeds.dtype, past_key_values_length=past_key_values_length\n       \
    \     ).to(inputs_embeds.device)\n\n        if attention_mask is not None:\n \
    \           # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n         \
    \   expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n\
    \            combined_attention_mask = (\n                expanded_attn_mask if\
    \ combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n\
    \            )\n\n        return combined_attention_mask\n\n    def forward(\n\
    \        self,\n        input_ids=None,\n        attention_mask=None,\n      \
    \  encoder_hidden_states=None,\n        head_mask=None,\n        cross_attn_head_mask=None,\n\
    \        past_key_values=None,\n        inputs_embeds=None,\n        use_cache=None,\n\
    \        output_attentions=None,\n        output_hidden_states=None,\n       \
    \ return_dict=None,\n    ):\n        r\"\"\"\n        Args:\n            input_ids\
    \ (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n           \
    \     Indices of input sequence tokens in the vocabulary. Padding will be ignored\
    \ by default should you\n                provide it.\n\n                Indices\
    \ can be obtained using [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`]\
    \ and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n     \
    \           [What are input IDs?](transformers/glossary#input-ids)\n         \
    \   attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n\
    \                Mask to avoid performing attention on padding token indices.\
    \ Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are\
    \ **not masked**,\n                - 0 for tokens that are **masked**.\n\n   \
    \             [What are attention masks?](transformers/glossary#attention-mask)\n\
    \            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size,\
    \ encoder_sequence_length, hidden_size)`, *optional*):\n                Sequence\
    \ of hidden-states at the output of the last layer of the encoder. Used in the\
    \ cross-attention\n                of the decoder.\n            head_mask (`torch.Tensor`\
    \ of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n       \
    \         Mask to nullify selected heads of the attention modules. Mask values\
    \ selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n\
    \                - 0 indicates the head is **masked**.\n\n            cross_attn_head_mask\
    \ (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n\
    \                Mask to nullify selected heads of the attention modules in encoder\
    \ to avoid performing cross-attention\n                on hidden heads. Mask values\
    \ selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n\
    \                - 0 indicates the head is **masked**.\n\n            past_key_values\
    \ (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True`\
    \ is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)`\
    \ of length `config.n_layers`, with each tuple having 2 tensors of\n         \
    \       shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\
    \ and 2 additional tensors of\n                shape `(batch_size, num_heads,\
    \ encoder_sequence_length, embed_size_per_head)`.\n\n                Contains\
    \ pre-computed hidden-states (key and values in the self-attention blocks and\
    \ in the\n                cross-attention blocks) that can be used (see `past_key_values`\
    \ input) to speed up sequential decoding.\n\n                If `past_key_values`\
    \ are used, the user can optionally input only the last `decoder_input_ids` (those\n\
    \                that don't have their past key value states given to this model)\
    \ of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids`\
    \ of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor`\
    \ of\n                shape `(batch_size, sequence_length, hidden_size)`, *optional*):\
    \ Optionally, instead of passing\n                `input_ids` you can choose to\
    \ directly pass an embedded representation. This is useful if you want more\n\
    \                control over how to convert `input_ids` indices into associated\
    \ vectors than the model's internal\n                embedding lookup matrix.\n\
    \            output_attentions (`bool`, *optional*):\n                Whether\
    \ or not to return the attentions tensors of all attention layers. See `attentions`\
    \ under\n                returned tensors for more detail.\n            output_hidden_states\
    \ (`bool`, *optional*):\n                Whether or not to return the hidden states\
    \ of all layers. See `hidden_states` under returned tensors\n                for\
    \ more detail.\n            return_dict (`bool`, *optional*):\n              \
    \  Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\
    \        \"\"\"\n        output_attentions = output_attentions if output_attentions\
    \ is not None else self.config.output_attentions\n        output_hidden_states\
    \ = (\n            output_hidden_states if output_hidden_states is not None else\
    \ self.config.output_hidden_states\n        )\n        use_cache = use_cache if\
    \ use_cache is not None else self.config.use_cache\n        return_dict = return_dict\
    \ if return_dict is not None else self.config.use_return_dict\n\n        # retrieve\
    \ input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds\
    \ is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids\
    \ and decoder_inputs_embeds at the same time\")\n        elif input_ids is not\
    \ None:\n            input_shape = input_ids.size()\n            input_ids = input_ids.view(-1,\
    \ input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape\
    \ = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"\
    You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n \
    \       # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2]\
    \ if past_key_values is not None else 0\n\n        if inputs_embeds is None:\n\
    \            inputs_embeds = self.embed_tokens(input_ids)\n\n        attention_mask\
    \ = self._prepare_decoder_attention_mask(\n            attention_mask, input_shape,\
    \ inputs_embeds, past_key_values_length\n        )\n\n        # embed positions\n\
    \        positions = self.embed_positions(input_ids, past_key_values_length=past_key_values_length)\n\
    \n        hidden_states = inputs_embeds + positions\n        hidden_states = nn.functional.dropout(hidden_states,\
    \ p=self.dropout, training=self.training)\n\n        # decoder layers\n      \
    \  all_hidden_states = () if output_hidden_states else None\n        all_self_attns\
    \ = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions\
    \ and encoder_hidden_states is not None) else None\n        next_decoder_cache\
    \ = () if use_cache else None\n\n        # check if head_mask/cross_attn_head_mask\
    \ has a correct number of layers specified if desired\n        for attn_mask,\
    \ mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"\
    ]):\n            if attn_mask is not None:\n                assert attn_mask.size()[0]\
    \ == (len(self.layers)), (\n                    f\"The `{mask_name}` should be\
    \ specified for {len(self.layers)} layers, but it is for\"\n                 \
    \   f\" {head_mask.size()[0]}.\"\n                )\n        for idx, decoder_layer\
    \ in enumerate(self.layers):\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556\
    \ for description)\n            if output_hidden_states:\n                all_hidden_states\
    \ += (hidden_states,)\n            dropout_probability = random.uniform(0, 1)\n\
    \            if self.training and (dropout_probability < self.layerdrop):\n  \
    \              continue\n\n            past_key_value = past_key_values[idx] if\
    \ past_key_values is not None else None\n\n            if self.gradient_checkpointing\
    \ and self.training:\n                if use_cache:\n                    logger.warning(\n\
    \                        \"`use_cache = True` is incompatible with gradient checkpointing.\
    \ Setting `use_cache =\"\n                        \" False`transformers.\"\n \
    \                   )\n                    use_cache = False\n\n             \
    \   def create_custom_forward(module):\n                    def custom_forward(*inputs):\n\
    \                        # None for past_key_value\n                        return\
    \ module(*inputs, output_attentions, use_cache)\n\n                    return\
    \ custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n\
    \                    create_custom_forward(decoder_layer),\n                 \
    \   hidden_states,\n                    attention_mask,\n                    encoder_hidden_states,\n\
    \                    None,  # encoder attention mask\n                    head_mask[idx]\
    \ if head_mask is not None else None,\n                    cross_attn_head_mask[idx]\
    \ if cross_attn_head_mask is not None else None,\n                    None,  #\
    \ past_key_value\n                )\n            else:\n                layer_outputs\
    \ = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n\
    \                    encoder_hidden_states=encoder_hidden_states,\n          \
    \          layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n\
    \                    cross_attn_layer_head_mask=(\n                        cross_attn_head_mask[idx]\
    \ if cross_attn_head_mask is not None else None\n                    ),\n    \
    \                past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n\
    \                    use_cache=use_cache,\n                )\n            hidden_states\
    \ = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache\
    \ += (layer_outputs[3 if output_attentions else 1],)\n\n            if output_attentions:\n\
    \                all_self_attns += (layer_outputs[1],)\n\n                if encoder_hidden_states\
    \ is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\
    \n        hidden_states = self.layer_norm(hidden_states)\n        # add hidden\
    \ states from the last decoder layer\n        if output_hidden_states:\n     \
    \       all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache\
    \ if use_cache else None\n        if not return_dict:\n            return tuple(\n\
    \                v\n                for v in [hidden_states, next_cache, all_hidden_states,\
    \ all_self_attns, all_cross_attentions]\n                if v is not None\n  \
    \          )\n        return BaseModelOutputWithPastAndCrossAttentions(\n    \
    \        last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n\
    \            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n\
    \            cross_attentions=all_cross_attentions,\n        )\n\n\n@add_start_docstrings(\n\
    \    \"The bare Whisper Model outputting raw hidden-states without any specific\
    \ head on top.\",\n    WHISPER_START_DOCSTRING,\n)\n# Copied from transformers.models.whisper.modeling_whisper.WhisperModel\n\
    class WhisperModel(WhisperPreTrainedModel):\n    _keys_to_ignore_on_load_missing\
    \ = [r\"proj_out.weight\"]\n\n    def __init__(self, config: WhisperConfig):\n\
    \        super().__init__(config)\n\n        self.encoder = WhisperEncoder(config)\n\
    \        self.decoder = WhisperDecoder(config)\n        # Initialize weights and\
    \ apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n\
    \        return self.decoder.embed_tokens\n\n    def set_input_embeddings(self,\
    \ value):\n        self.decoder.embed_tokens = value\n\n    def get_encoder(self):\n\
    \        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\
    \n    def freeze_encoder(self):\n        \"\"\"\n        Calling this function\
    \ will disable the gradient computation for the Whisper encoder so that its parameters\
    \ will\n        not be updated during training.\n        \"\"\"\n        self.encoder._freeze_parameters()\n\
    \n    @add_start_docstrings_to_model_forward(WHISPER_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqModelOutput,\
    \ config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_features:\
    \ Optional[torch.FloatTensor] = None,\n        attention_mask: Optional[torch.LongTensor]\
    \ = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n   \
    \     decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask:\
    \ Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor]\
    \ = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n    \
    \    encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n    \
    \    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n    \
    \    decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n     \
    \   use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool]\
    \ = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict:\
    \ Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n\
    \        r\"\"\"\n        Returns:\n\n        Example:\n         ```python\n \
    \        >>> import torch\n         >>> from transformers import AutoFeatureExtractor,\
    \ WhisperModel\n         >>> from datasets import load_dataset\n\n         >>>\
    \ model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n         >>>\
    \ feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-base\"\
    )\n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
    , \"clean\", split=\"validation\")\n         >>> inputs = feature_extractor(ds[0][\"\
    audio\"][\"array\"], return_tensors=\"pt\")\n         >>> input_features = inputs.input_features\n\
    \         >>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n\
    \         >>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n\
    \         >>> list(last_hidden_state.shape)\n         [1, 2, 512]\n         ```\"\
    \"\"\n        output_attentions = output_attentions if output_attentions is not\
    \ None else self.config.output_attentions\n        output_hidden_states = (\n\
    \            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n\
    \        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\
    \        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\
    \n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n\
    \                input_features,\n                head_mask=head_mask,\n     \
    \           output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n\
    \                return_dict=return_dict,\n            )\n        # If the user\
    \ passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n\
    \        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n\
    \            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n\
    \                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1\
    \ else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs)\
    \ > 2 else None,\n            )\n\n        # decoder outputs consists of (dec_features,\
    \ past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.decoder(\n\
    \            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n\
    \            encoder_hidden_states=encoder_outputs[0],\n            head_mask=decoder_head_mask,\n\
    \            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n\
    \            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n\
    \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
    \            return_dict=return_dict,\n        )\n\n        if not return_dict:\n\
    \            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n\
    \            last_hidden_state=decoder_outputs.last_hidden_state,\n          \
    \  past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n\
    \            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n\
    \            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n  \
    \          encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n\
    \        )\n\n\n@add_start_docstrings(\n    \"The Whisper Model with a language\
    \ modeling head. Can be used for automatic speech recognition.\",\n    WHISPER_START_DOCSTRING,\n\
    )\n# Copied from transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration\n\
    class WhisperBnbForConditionalGeneration(WhisperPreTrainedModel):\n    base_model_prefix\
    \ = \"model\"\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.version\"\
    ,\n        r\"decoder.version\",\n        r\"proj_out.weight\",\n    ]\n    _keys_to_ignore_on_save\
    \ = [\n        r\"proj_out.weight\",\n    ]\n\n    def __init__(self, config:\
    \ WhisperConfig):\n        super().__init__(config)\n        self.model = WhisperModel(config)\n\
    \        self.proj_out = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\
    \n        # Initialize weights and apply final processing\n        self.post_init()\n\
    \n    def get_encoder(self):\n        return self.model.get_encoder()\n\n    def\
    \ get_decoder(self):\n        return self.model.get_decoder()\n\n    def resize_token_embeddings(self,\
    \ new_num_tokens: int) -> nn.Embedding:\n        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n\
    \        return new_embeddings\n\n    def get_output_embeddings(self):\n     \
    \   return self.proj_out\n\n    def set_output_embeddings(self, new_embeddings):\n\
    \        self.proj_out = new_embeddings\n\n    def freeze_encoder(self):\n   \
    \     \"\"\"\n        Calling this function will disable the gradient computation\
    \ for the Whisper encoder so that its parameters will\n        not be updated\
    \ during training.\n        \"\"\"\n        self.model.encoder._freeze_parameters()\n\
    \n    @add_start_docstrings_to_model_forward(WHISPER_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqLMOutput,\
    \ config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_features:\
    \ Optional[torch.FloatTensor] = None,\n        attention_mask: Optional[torch.LongTensor]\
    \ = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n   \
    \     decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask:\
    \ Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor]\
    \ = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n    \
    \    encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n    \
    \    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n    \
    \    decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n     \
    \   labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool]\
    \ = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states:\
    \ Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) ->\
    \ Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n        r\"\"\"\n        labels\
    \ (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n\
    \            Labels for computing the language modeling loss. Indices should either\
    \ be in `[0, transformers., config.vocab_size]`\n            or -100 (see `input_ids`\
    \ docstring). Tokens with indices set to `-100` are ignored (masked), the loss\
    \ is\n            only computed for the tokens with labels in `[0, transformers.,\
    \ config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n\
    \        >>> import torch\n        >>> from transformers import AutoProcessor,\
    \ WhisperForConditionalGeneration\n        >>> from datasets import load_dataset\n\
    \n        >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\"\
    )\n        >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\"\
    )\n\n        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
    , \"clean\", split=\"validation\")\n\n        >>> inputs = processor(ds[0][\"\
    audio\"][\"array\"], return_tensors=\"pt\")\n        >>> input_features = inputs.input_features\n\
    \n        >>> generated_ids = model.generate(inputs=input_features)\n\n      \
    \  >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\
    \        >>> transcription\n        ' Mr. Quilter is the apostle of the middle\
    \ classes, and we are glad to welcome his gospel.'\n        ```\"\"\"\n      \
    \  return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\
    \n        if labels is not None:\n            if decoder_input_ids is None and\
    \ decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n\
    \                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n\
    \                )\n\n        outputs = self.model(\n            input_features,\n\
    \            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n\
    \            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n\
    \            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n\
    \            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n\
    \            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n\
    \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
    \            return_dict=return_dict,\n        )\n        lm_logits = self.proj_out(outputs[0])\n\
    \n        loss = None\n        if labels is not None:\n            loss_fct =\
    \ CrossEntropyLoss()\n            loss = loss_fct(lm_logits.view(-1, self.config.vocab_size),\
    \ labels.reshape(-1))\n\n        if not return_dict:\n            output = (lm_logits,)\
    \ + outputs[1:]\n            return ((loss,) + output) if loss is not None else\
    \ output\n\n        return Seq2SeqLMOutput(\n            loss=loss,\n        \
    \    logits=lm_logits,\n            past_key_values=outputs.past_key_values,\n\
    \            decoder_hidden_states=outputs.decoder_hidden_states,\n          \
    \  decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n\
    \            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n  \
    \          encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n\
    \        )\n\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor]\
    \ = None,\n        generation_config=None,\n        logits_processor=None,\n \
    \       stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n    \
    \    synced_gpus=False,\n        return_timestamps=None,\n        task=None,\n\
    \        language=None,\n        is_multilingual=None,\n        **kwargs,\n  \
    \  ):\n        \"\"\"\n\n        Generates sequences of token ids for models with\
    \ a language modeling head.\n\n        <Tip warning={true}>\n\n        Most generation-controlling\
    \ parameters are set in `generation_config` which, if not passed, will be set\
    \ to the\n        model's default generation configuration. You can override any\
    \ `generation_config` by passing the corresponding\n        parameters to generate(),\
    \ e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n\n        For an overview\
    \ of generation strategies and code examples, check out the [following\n     \
    \   guide](./generation_strategies).\n\n        </Tip>\n\n        Parameters:\n\
    \            inputs (`torch.Tensor` of varying shape depending on the modality,\
    \ *optional*):\n                The sequence used as a prompt for the generation\
    \ or as model inputs to the encoder. If `None` the\n                method initializes\
    \ it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n\
    \                should of in the format of `input_ids`. For encoder-decoder models\
    \ *inputs* can represent any of\n                `input_ids`, `input_values`,\
    \ `input_features`, or `pixel_values`.\n            generation_config (`~generation.GenerationConfig`,\
    \ *optional*):\n                The generation configuration to be used as base\
    \ parametrization for the generation call. `**kwargs`\n                passed\
    \ to generate matching the attributes of `generation_config` will override them.\
    \ If\n                `generation_config` is not provided, the default will be\
    \ used, which had the following loading\n                priority: 1) from the\
    \ `generation_config.json` model file, if it exists; 2) from the model\n     \
    \           configuration. Please note that unspecified parameters will inherit\
    \ [`~generation.GenerationConfig`]'s\n                default values, whose documentation\
    \ should be checked to parameterize generation.\n            logits_processor\
    \ (`LogitsProcessorList`, *optional*):\n                Custom logits processors\
    \ that complement the default logits processors built from arguments and\n   \
    \             generation config. If a logit processor is passed that is already\
    \ created with the arguments or a\n                generation config an error\
    \ is thrown. This feature is intended for advanced users.\n            stopping_criteria\
    \ (`StoppingCriteriaList`, *optional*):\n                Custom stopping criteria\
    \ that complement the default stopping criteria built from arguments and a\n \
    \               generation config. If a stopping criteria is passed that is already\
    \ created with the arguments or a\n                generation config an error\
    \ is thrown. This feature is intended for advanced users.\n            prefix_allowed_tokens_fn\
    \ (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n                If\
    \ provided, this function constraints the beam search to allowed tokens only at\
    \ each step. If not\n                provided no constraint is applied. This function\
    \ takes 2 arguments: the batch ID `batch_id` and\n                `input_ids`.\
    \ It has to return a list with the allowed tokens for the next generation step\
    \ conditioned\n                on the batch ID `batch_id` and the previously generated\
    \ tokens `inputs_ids`. This argument is useful\n                for constrained\
    \ generation conditioned on the prefix, as described in [Autoregressive Entity\n\
    \                Retrieval](https://arxiv.org/abs/2010.00904).\n            synced_gpus\
    \ (`bool`, *optional*, defaults to `False`):\n                Whether to continue\
    \ running the while loop until max_length (needed for ZeRO stage 3)\n        \
    \    return_timestamps (`bool`, *optional*):\n                Whether to return\
    \ the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n\
    \            task (`bool`, *optional*):\n                Task to use for generation,\
    \ either \"translate\" or \"transcribe\". The `model.config.forced_decoder_ids`\n\
    \                will be updated accordingly.\n            language (`bool`, *optional*):\n\
    \                Language token to use for generation, should be in the form `<|en|>`.\
    \ You can find all the possible\n                language tokens in the `model.generation_config.lang_to_id`\
    \ dictionary.\n            is_multilingual (`bool`, *optional*):\n           \
    \     Whether or not the model is multilingual.\n            kwargs:\n       \
    \         Ad hoc parametrization of `generate_config` and/or additional model-specific\
    \ kwargs that will be\n                forwarded to the `forward` function of\
    \ the model. If the model is an encoder-decoder model, encoder\n             \
    \   specific kwargs should not be prefixed and decoder specific kwargs should\
    \ be prefixed with *decoder_*.\n\n        Return:\n            [`~utils.ModelOutput`]\
    \ or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n\
    \            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n\
    \n                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`),\
    \ the possible\n                [`~utils.ModelOutput`] types are:\n\n        \
    \            - [`~generation.GreedySearchDecoderOnlyOutput`],\n              \
    \      - [`~generation.SampleDecoderOnlyOutput`],\n                    - [`~generation.BeamSearchDecoderOnlyOutput`],\n\
    \                    - [`~generation.BeamSampleDecoderOnlyOutput`]\n\n       \
    \         If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),\
    \ the possible\n                [`~utils.ModelOutput`] types are:\n\n        \
    \            - [`~generation.GreedySearchEncoderDecoderOutput`],\n           \
    \         - [`~generation.SampleEncoderDecoderOutput`],\n                    -\
    \ [`~generation.BeamSearchEncoderDecoderOutput`],\n                    - [`~generation.BeamSampleEncoderDecoderOutput`]\n\
    \        \"\"\"\n        if generation_config is None:\n            generation_config\
    \ = self.generation_config\n\n        if return_timestamps is not None:\n    \
    \        generation_config.return_timestamps = return_timestamps\n\n        if\
    \ task is not None:\n            generation_config.task = task\n\n        if is_multilingual\
    \ is not None:\n            generation_config.is_multilingual = is_multilingual\n\
    \n        if language is not None:\n            generation_config.language = language\n\
    \n        forced_decoder_ids = []\n\n        if hasattr(generation_config, \"\
    is_multilingual\") and generation_config.is_multilingual:\n            if hasattr(generation_config,\
    \ \"language\"):\n                forced_decoder_ids.append((1, generation_config.lang_to_id[generation_config.language]))\n\
    \            else:\n                forced_decoder_ids.append((1, None))\n\n \
    \           if hasattr(generation_config, \"task\"):\n                forced_decoder_ids.append((2,\
    \ generation_config.task_to_id[generation_config.task]))\n            else:\n\
    \                forced_decoder_ids.append((2, generation_config.task_to_id[\"\
    transcribe\"]))\n\n        if (\n            hasattr(generation_config, \"return_timestamps\"\
    ) and generation_config.return_timestamps\n        ) or return_timestamps:\n \
    \           logits_processor = [WhisperTimeStampLogitsProcessor(generation_config)]\n\
    \        else:\n            if forced_decoder_ids and forced_decoder_ids[-1][0]\
    \ != generation_config.no_timestamps_token_id:\n                idx = forced_decoder_ids[-1][0]\
    \ + 1 if forced_decoder_ids else 1\n                forced_decoder_ids.append((idx,\
    \ generation_config.no_timestamps_token_id))\n\n        if len(forced_decoder_ids)\
    \ > 0:\n            generation_config.forced_decoder_ids = forced_decoder_ids\n\
    \n        return super().generate(\n            inputs,\n            generation_config,\n\
    \            logits_processor,\n            stopping_criteria,\n            prefix_allowed_tokens_fn,\n\
    \            synced_gpus,\n            **kwargs,\n        )\n\n    def prepare_inputs_for_generation(\n\
    \        self,\n        decoder_input_ids,\n        past_key_values=None,\n  \
    \      use_cache=None,\n        encoder_outputs=None,\n        attention_mask=None,\n\
    \        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n\
    \        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:,\
    \ -1:]\n\n        return {\n            \"encoder_outputs\": encoder_outputs,\n\
    \            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\"\
    : decoder_input_ids,\n            \"use_cache\": use_cache,\n            \"decoder_attention_mask\"\
    : None,\n        }\n\n    #\n    @staticmethod\n    def _reorder_cache(past_key_values,\
    \ beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n\
    \            reordered_past += (tuple(past_state.index_select(0, beam_idx) for\
    \ past_state in layer_past),)\n        return reordered_past\n```\n\n</details>\n\
    \nAnd now import the model from this file:\n```python\nfrom modeling_whisper import\
    \ WhisperForConditionalGeneration\n\n# do your thing\n```\n\nFor modifying the\
    \ loss, first import the required loss:\n```diff\n- from torch.nn import CrossEntropyLoss\n\
    + from torch.nn import HingeEmbeddingLoss\n```\n\nAnd then change the loss function:\n\
    ```diff\n        if labels is not None:\n-           loss_fct = CrossEntropyLoss()\n\
    +           loss_fct = HingeEmbeddingLoss()\n            loss = loss_fct(lm_logits.view(-1,\
    \ self.config.vocab_size), labels.reshape(-1))\n```"
  created_at: 2023-03-17 14:42:42+00:00
  edited: true
  hidden: false
  id: 64148a72315d256ba75a7fd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-03-17T15:45:46.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;raikarsagar&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/raikarsagar\"\
          >@<span class=\"underline\">raikarsagar</span></a></span>\n\n\t</span></span>\
          \ - the ground truths are our \"labels\", see <a href=\"https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.labels\"\
          >https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.labels</a></p>\n\
          <p>For the alignments, the model implicitly learns to align it's token predictions\
          \ to the labels through the CE loss</p>\n"
        raw: 'Hey @raikarsagar - the ground truths are our "labels", see https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.labels


          For the alignments, the model implicitly learns to align it''s token predictions
          to the labels through the CE loss'
        updatedAt: '2023-03-17T15:46:32.218Z'
      numEdits: 1
      reactions: []
    id: 64148b2a2763074a53bfe803
    type: comment
  author: sanchit-gandhi
  content: 'Hey @raikarsagar - the ground truths are our "labels", see https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.labels


    For the alignments, the model implicitly learns to align it''s token predictions
    to the labels through the CE loss'
  created_at: 2023-03-17 14:45:46+00:00
  edited: true
  hidden: false
  id: 64148b2a2763074a53bfe803
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
      fullname: 'Martha  Fikry '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Martha-987
      type: user
    createdAt: '2023-03-19T21:37:11.000Z'
    data:
      edited: false
      editors:
      - Martha-987
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1a69c8f2ffe585e9652efb5a8799721.svg
          fullname: 'Martha  Fikry '
          isHf: false
          isPro: false
          name: Martha-987
          type: user
        html: "<p>hey <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1679261808299-6356390f1c93c1ef4e9ed702.png\"\
          ><img alt=\"Untitled.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1679261808299-6356390f1c93c1ef4e9ed702.png\"\
          ></a></p>\n<p>I have that error :\\</p>\n"
        raw: "hey @sanchit-gandhi \n\n![Untitled.png](https://cdn-uploads.huggingface.co/production/uploads/1679261808299-6356390f1c93c1ef4e9ed702.png)\n\
          \n\nI have that error :\\"
        updatedAt: '2023-03-19T21:37:11.057Z'
      numEdits: 0
      reactions: []
    id: 64178087c5e5d7e5acaec04a
    type: comment
  author: Martha-987
  content: "hey @sanchit-gandhi \n\n![Untitled.png](https://cdn-uploads.huggingface.co/production/uploads/1679261808299-6356390f1c93c1ef4e9ed702.png)\n\
    \n\nI have that error :\\"
  created_at: 2023-03-19 20:37:11+00:00
  edited: false
  hidden: false
  id: 64178087c5e5d7e5acaec04a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-03-24T14:39:46.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Martha-987&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Martha-987\"\
          >@<span class=\"underline\">Martha-987</span></a></span>\n\n\t</span></span>\
          \ - it looks like your logits dimension (51865) doesn't match the labels\
          \ dimension (2000). Can you make sure the shapes match? Your logits should\
          \ be shape [batch size, num labels] and your labels shape [batch size],\
          \ with minimum value 0 and maximum value (num labels - 1)</p>\n"
        raw: Hey @Martha-987 - it looks like your logits dimension (51865) doesn't
          match the labels dimension (2000). Can you make sure the shapes match? Your
          logits should be shape [batch size, num labels] and your labels shape [batch
          size], with minimum value 0 and maximum value (num labels - 1)
        updatedAt: '2023-03-24T14:39:46.010Z'
      numEdits: 0
      reactions: []
    id: 641db63220d921a696e7d35e
    type: comment
  author: sanchit-gandhi
  content: Hey @Martha-987 - it looks like your logits dimension (51865) doesn't match
    the labels dimension (2000). Can you make sure the shapes match? Your logits should
    be shape [batch size, num labels] and your labels shape [batch size], with minimum
    value 0 and maximum value (num labels - 1)
  created_at: 2023-03-24 13:39:46+00:00
  edited: false
  hidden: false
  id: 641db63220d921a696e7d35e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: sanchit-gandhi/whisper-small-hi
repo_type: model
status: open
target_branch: null
title: 'add loss function '
