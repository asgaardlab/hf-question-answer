!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ares09
conflicting_files: null
created_at: 2023-06-06 07:06:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce70eb5affca6ff97b494cc4580ea4f0.svg
      fullname: Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ares09
      type: user
    createdAt: '2023-06-06T08:06:12.000Z'
    data:
      edited: false
      editors:
      - Ares09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5507330894470215
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce70eb5affca6ff97b494cc4580ea4f0.svg
          fullname: Huang
          isHf: false
          isPro: false
          name: Ares09
          type: user
        html: '<p>I use your code for inference, I successfully loaded the model,
          but in the process of inference will keep answering the same question, I
          only need one inference result, what should I do?</p>

          <p>#Load model<br>quantized_model_dir = "falcon"<br>model_basename = "gptq_model-4bit--1g"<br>use_strict
          = False<br>use_triton = False<br>print("Loading tokenizer")<br>tokenizer
          = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>         trust_remote_code=True,<br>        use_safetensors=True,<br>        strict=use_strict,<br>        torch_dtype=torch.float32,<br>        device="cuda:3",<br>        use_triton=use_triton,<br>                            )</p>

          <h2 id="inference">inference</h2>

          <p>  input_ids = tokenizer(prompt_template, return_tensors=''pt'').to("cuda:3").input_ids<br>     output
          = model.generate(inputs=input_ids,<br>              temperature=0.01,<br>                        do_sample=True,<br>              max_new_tokens=20)<br>response=tokenizer.decode(output[0])</p>

          <p>result</p>

          <h3 id="instruction-11">Instruction: 1+1</h3>

          <h3 id="response2endoftextthe-answer-to-11-is-2endoftext1-is-the-first-number">Response:2&lt;|endoftext|&gt;The
          answer to 1+1 is 2.&lt;|endoftext|&gt;#1 is the first number</h3>

          <pre><code>                                                       ( I don''t
          need this)

          </code></pre>

          <p>i just need answer (2)   How do I truncate model generation after &lt;|endoftext|&gt;?</p>

          <p>tks~</p>

          '
        raw: "I use your code for inference, I successfully loaded the model, but\
          \ in the process of inference will keep answering the same question, I only\
          \ need one inference result, what should I do?\r\n\r\n\r\n#Load model\r\n\
          quantized_model_dir = \"falcon\"\r\nmodel_basename = \"gptq_model-4bit--1g\"\
          \r\nuse_strict = False\r\nuse_triton = False\r\nprint(\"Loading tokenizer\"\
          )\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\r\
          \n\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\r\n\
          \         trust_remote_code=True,\r\n        use_safetensors=True,\r\n \
          \       strict=use_strict,\r\n        torch_dtype=torch.float32,\r\n   \
          \     device=\"cuda:3\",\r\n        use_triton=use_triton,\r\n         \
          \                   )\r\n\r\n## inference \r\n  input_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').to(\"cuda:3\").input_ids\r\n     output = model.generate(inputs=input_ids,\r\
          \n              temperature=0.01,\r\n                        do_sample=True,\r\
          \n              max_new_tokens=20)\r\nresponse=tokenizer.decode(output[0])\r\
          \n\r\n\r\nresult\r\n### Instruction: 1+1\r\n\r\n### Response:2<|endoftext|>The\
          \ answer to 1+1 is 2.<|endoftext|>#1 is the first number\r\n           \
          \                                                ( I don't need this)\r\n\
          \r\ni just need answer (2)   How do I truncate model generation after <|endoftext|>?\r\
          \n\r\n\r\ntks~"
        updatedAt: '2023-06-06T08:06:12.706Z'
      numEdits: 0
      reactions: []
    id: 647ee8f4e9c81260ff848f38
    type: comment
  author: Ares09
  content: "I use your code for inference, I successfully loaded the model, but in\
    \ the process of inference will keep answering the same question, I only need\
    \ one inference result, what should I do?\r\n\r\n\r\n#Load model\r\nquantized_model_dir\
    \ = \"falcon\"\r\nmodel_basename = \"gptq_model-4bit--1g\"\r\nuse_strict = False\r\
    \nuse_triton = False\r\nprint(\"Loading tokenizer\")\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=False)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\r\
    \n         trust_remote_code=True,\r\n        use_safetensors=True,\r\n      \
    \  strict=use_strict,\r\n        torch_dtype=torch.float32,\r\n        device=\"\
    cuda:3\",\r\n        use_triton=use_triton,\r\n                            )\r\
    \n\r\n## inference \r\n  input_ids = tokenizer(prompt_template, return_tensors='pt').to(\"\
    cuda:3\").input_ids\r\n     output = model.generate(inputs=input_ids,\r\n    \
    \          temperature=0.01,\r\n                        do_sample=True,\r\n  \
    \            max_new_tokens=20)\r\nresponse=tokenizer.decode(output[0])\r\n\r\n\
    \r\nresult\r\n### Instruction: 1+1\r\n\r\n### Response:2<|endoftext|>The answer\
    \ to 1+1 is 2.<|endoftext|>#1 is the first number\r\n                        \
    \                                   ( I don't need this)\r\n\r\ni just need answer\
    \ (2)   How do I truncate model generation after <|endoftext|>?\r\n\r\n\r\ntks~"
  created_at: 2023-06-06 07:06:12+00:00
  edited: false
  hidden: false
  id: 647ee8f4e9c81260ff848f38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ce70eb5affca6ff97b494cc4580ea4f0.svg
      fullname: Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ares09
      type: user
    createdAt: '2023-06-06T08:24:09.000Z'
    data:
      status: closed
    id: 647eed29f41cf810e370946f
    type: status-change
  author: Ares09
  created_at: 2023-06-06 07:24:09+00:00
  id: 647eed29f41cf810e370946f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ffdc16df87b66666d789482c630d08ca.svg
      fullname: Philip Dakin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pdakin
      type: user
    createdAt: '2023-06-20T01:21:29.000Z'
    data:
      edited: false
      editors:
      - pdakin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9872630834579468
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ffdc16df87b66666d789482c630d08ca.svg
          fullname: Philip Dakin
          isHf: false
          isPro: false
          name: pdakin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Ares09&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Ares09\">@<span class=\"\
          underline\">Ares09</span></a></span>\n\n\t</span></span> how did you resolve\
          \ this?</p>\n"
        raw: '@Ares09 how did you resolve this?'
        updatedAt: '2023-06-20T01:21:29.632Z'
      numEdits: 0
      reactions: []
    id: 6490ff19df7c85dd93833f42
    type: comment
  author: pdakin
  content: '@Ares09 how did you resolve this?'
  created_at: 2023-06-20 00:21:29+00:00
  edited: false
  hidden: false
  id: 6490ff19df7c85dd93833f42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ffdc16df87b66666d789482c630d08ca.svg
      fullname: Philip Dakin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pdakin
      type: user
    createdAt: '2023-06-22T17:24:25.000Z'
    data:
      edited: false
      editors:
      - pdakin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7754115462303162
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ffdc16df87b66666d789482c630d08ca.svg
          fullname: Philip Dakin
          isHf: false
          isPro: false
          name: pdakin
          type: user
        html: '<p>This issue helped me figure it out - <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/22794#issuecomment-1598977285">https://github.com/huggingface/transformers/issues/22794#issuecomment-1598977285</a></p>

          '
        raw: This issue helped me figure it out - https://github.com/huggingface/transformers/issues/22794#issuecomment-1598977285
        updatedAt: '2023-06-22T17:24:25.714Z'
      numEdits: 0
      reactions: []
    id: 649483c9274a080f596063df
    type: comment
  author: pdakin
  content: This issue helped me figure it out - https://github.com/huggingface/transformers/issues/22794#issuecomment-1598977285
  created_at: 2023-06-22 16:24:25+00:00
  edited: false
  hidden: false
  id: 649483c9274a080f596063df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: My reasoning keeps repeating. How do I conclude with <<|endoftext|>>
