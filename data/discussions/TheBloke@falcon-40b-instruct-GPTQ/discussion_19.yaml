!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mattkallo
conflicting_files: null
created_at: 2023-06-23 18:06:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f22b37e61147cbb2c513da4f4efae35.svg
      fullname: Matt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattkallo
      type: user
    createdAt: '2023-06-23T19:06:59.000Z'
    data:
      edited: true
      editors:
      - mattkallo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8770145773887634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f22b37e61147cbb2c513da4f4efae35.svg
          fullname: Matt
          isHf: false
          isPro: false
          name: mattkallo
          type: user
        html: '<p>I am using A100 (40GB) to run this model (falcon-40b-instruct-GPTQ).
          It''s taking roughly 120 seconds to answer a question with a limit of 200
          output tokens. Is this expected? Whats the best performance seen so far?
          - Thanks</p>

          <p>I am using the same code as whats given in the model card. </p>

          '
        raw: 'I am using A100 (40GB) to run this model (falcon-40b-instruct-GPTQ).
          It''s taking roughly 120 seconds to answer a question with a limit of 200
          output tokens. Is this expected? Whats the best performance seen so far?
          - Thanks


          I am using the same code as whats given in the model card. '
        updatedAt: '2023-06-23T19:07:38.250Z'
      numEdits: 1
      reactions: []
    id: 6495ed535ea27790e4566934
    type: comment
  author: mattkallo
  content: 'I am using A100 (40GB) to run this model (falcon-40b-instruct-GPTQ). It''s
    taking roughly 120 seconds to answer a question with a limit of 200 output tokens.
    Is this expected? Whats the best performance seen so far? - Thanks


    I am using the same code as whats given in the model card. '
  created_at: 2023-06-23 18:06:59+00:00
  edited: true
  hidden: false
  id: 6495ed535ea27790e4566934
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T20:20:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9727424383163452
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I''m afraid that is expected with the Falcon GPTQ at the moment.
          It has a major speed problem that hasn''t been resolved yet. I put a note
          in the README about it.</p>

          <p>Recently we got preliminary support for GPU accelerated Falcon GGMLs.
          I have four repos for those.  They perform quite a bit better than the GPTQ.  Unfortunately
          they''re not supported in many clients/UIs yet, but they did just get support
          in ctransformers (Python library including support for Langchain), and also
          LoLLMS-UI.  So you may well find those preferable to the GPTQs.</p>

          <p>Another option is to download the original unquantised model and then
          use <code>load_in_4bit=True</code> to use bitsandbytes. That''s still very
          slow (maybe 4 tokens/s) and slower than the GGML, but it''s faster than
          the GGML.</p>

          '
        raw: 'Yeah I''m afraid that is expected with the Falcon GPTQ at the moment.
          It has a major speed problem that hasn''t been resolved yet. I put a note
          in the README about it.


          Recently we got preliminary support for GPU accelerated Falcon GGMLs. I
          have four repos for those.  They perform quite a bit better than the GPTQ.  Unfortunately
          they''re not supported in many clients/UIs yet, but they did just get support
          in ctransformers (Python library including support for Langchain), and also
          LoLLMS-UI.  So you may well find those preferable to the GPTQs.


          Another option is to download the original unquantised model and then use
          `load_in_4bit=True` to use bitsandbytes. That''s still very slow (maybe
          4 tokens/s) and slower than the GGML, but it''s faster than the GGML.'
        updatedAt: '2023-06-23T20:20:12.573Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mattkallo
    id: 6495fe7cbf36c2cc70f887f6
    type: comment
  author: TheBloke
  content: 'Yeah I''m afraid that is expected with the Falcon GPTQ at the moment.
    It has a major speed problem that hasn''t been resolved yet. I put a note in the
    README about it.


    Recently we got preliminary support for GPU accelerated Falcon GGMLs. I have four
    repos for those.  They perform quite a bit better than the GPTQ.  Unfortunately
    they''re not supported in many clients/UIs yet, but they did just get support
    in ctransformers (Python library including support for Langchain), and also LoLLMS-UI.  So
    you may well find those preferable to the GPTQs.


    Another option is to download the original unquantised model and then use `load_in_4bit=True`
    to use bitsandbytes. That''s still very slow (maybe 4 tokens/s) and slower than
    the GGML, but it''s faster than the GGML.'
  created_at: 2023-06-23 19:20:12+00:00
  edited: false
  hidden: false
  id: 6495fe7cbf36c2cc70f887f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f22b37e61147cbb2c513da4f4efae35.svg
      fullname: Matt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattkallo
      type: user
    createdAt: '2023-06-23T22:00:00.000Z'
    data:
      edited: true
      editors:
      - mattkallo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9712709784507751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f22b37e61147cbb2c513da4f4efae35.svg
          fullname: Matt
          isHf: false
          isPro: false
          name: mattkallo
          type: user
        html: '<p>Thanks for the update.  </p>

          <blockquote>

          <p>Yeah I''m afraid that is expected with the Falcon GPTQ at the moment.
          It has a major speed problem that hasn''t been resolved yet. I put a note
          in the README about it.</p>

          <p>Recently we got preliminary support for GPU accelerated Falcon GGMLs.
          I have four repos for those.  They perform quite a bit better than the GPTQ.  Unfortunately
          they''re not supported in many clients/UIs yet, but they did just get support
          in ctransformers (Python library including support for Langchain), and also
          LoLLMS-UI.  So you may well find those preferable to the GPTQs.</p>

          <p>Another option is to download the original unquantised model and then
          use <code>load_in_4bit=True</code> to use bitsandbytes. That''s still very
          slow (maybe 4 tokens/s) and slower than the GGML, but it''s faster than
          the GGML.</p>

          </blockquote>

          '
        raw: "Thanks for the update.  \n\n> Yeah I'm afraid that is expected with\
          \ the Falcon GPTQ at the moment. It has a major speed problem that hasn't\
          \ been resolved yet. I put a note in the README about it.\n> \n> Recently\
          \ we got preliminary support for GPU accelerated Falcon GGMLs. I have four\
          \ repos for those.  They perform quite a bit better than the GPTQ.  Unfortunately\
          \ they're not supported in many clients/UIs yet, but they did just get support\
          \ in ctransformers (Python library including support for Langchain), and\
          \ also LoLLMS-UI.  So you may well find those preferable to the GPTQs.\n\
          > \n> Another option is to download the original unquantised model and then\
          \ use `load_in_4bit=True` to use bitsandbytes. That's still very slow (maybe\
          \ 4 tokens/s) and slower than the GGML, but it's faster than the GGML.\n\
          \n"
        updatedAt: '2023-06-25T01:16:42.191Z'
      numEdits: 1
      reactions: []
      relatedEventId: 649615e033611fb433f1f5d5
    id: 649615e033611fb433f1f5d3
    type: comment
  author: mattkallo
  content: "Thanks for the update.  \n\n> Yeah I'm afraid that is expected with the\
    \ Falcon GPTQ at the moment. It has a major speed problem that hasn't been resolved\
    \ yet. I put a note in the README about it.\n> \n> Recently we got preliminary\
    \ support for GPU accelerated Falcon GGMLs. I have four repos for those.  They\
    \ perform quite a bit better than the GPTQ.  Unfortunately they're not supported\
    \ in many clients/UIs yet, but they did just get support in ctransformers (Python\
    \ library including support for Langchain), and also LoLLMS-UI.  So you may well\
    \ find those preferable to the GPTQs.\n> \n> Another option is to download the\
    \ original unquantised model and then use `load_in_4bit=True` to use bitsandbytes.\
    \ That's still very slow (maybe 4 tokens/s) and slower than the GGML, but it's\
    \ faster than the GGML.\n\n"
  created_at: 2023-06-23 21:00:00+00:00
  edited: true
  hidden: false
  id: 649615e033611fb433f1f5d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3f22b37e61147cbb2c513da4f4efae35.svg
      fullname: Matt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattkallo
      type: user
    createdAt: '2023-06-23T22:00:00.000Z'
    data:
      status: closed
    id: 649615e033611fb433f1f5d5
    type: status-change
  author: mattkallo
  created_at: 2023-06-23 21:00:00+00:00
  id: 649615e033611fb433f1f5d5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: Performance / speed of text generation
