!!python/object:huggingface_hub.community.DiscussionWithDetails
author: XceptDev
conflicting_files: null
created_at: 2023-08-09 19:13:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
      fullname: Krzysztof Jakowluk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XceptDev
      type: user
    createdAt: '2023-08-09T20:13:25.000Z'
    data:
      edited: false
      editors:
      - XceptDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3959210216999054
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
          fullname: Krzysztof Jakowluk
          isHf: false
          isPro: false
          name: XceptDev
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , Sorry for bothering you, but i am trying to run this falcon-40b-instruct-GPTQ\
          \ model on my oobabooga web ui. But i see an error when i try to load this,\
          \ \"RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72]\
          \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 536870912\
          \ bytes.\" my settings:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/640bb963a84bfdc022142fbe/iU11nDYle4fI-lVfMc9O2.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/640bb963a84bfdc022142fbe/iU11nDYle4fI-lVfMc9O2.png\"\
          ></a><br>entire log:<br>2023-08-09 22:03:42 INFO:Loading TheBloke_falcon-40b-instruct-GPTQ...<br>2023-08-09\
          \ 22:03:42 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit--1g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ True, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True}<br>2023-08-09\
          \ 22:03:42 WARNING:Exllama kernel is not installed, reset disable_exllama\
          \ to True. This may because you installed auto_gptq using a pre-build wheel\
          \ on Windows, in which exllama_kernels are not compiled. To use exllama_kernels\
          \ to further speedup inference, you can re-install auto_gptq from source.<br>2023-08-09\
          \ 22:03:42 ERROR:Failed to load the model.<br>Traceback (most recent call\
          \ last):<br>  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\\
          ui_model_menu.py\", line 179, in load_model_wrapper<br>    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)<br>  File \"\
          E:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\", line\
          \ 78, in load_model<br>    output = load_func_map<a rel=\"nofollow\" href=\"\
          model_name\">loader</a><br>  File \"E:\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\", line 292, in AutoGPTQ_loader<br>    return modules.AutoGPTQ_loader.load_quantized(model_name)<br>\
          \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\"\
          , line 56, in load_quantized<br>    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)<br>  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling\\auto.py\", line 108, in from_quantized<br>\
          \    return quant_func(<br>  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling_base.py\", line 817, in from_quantized<br>\
          \    model = AutoModelForCausalLM.from_config(<br>  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
          , line 428, in from_config<br>    return model_class._from_config(config,\
          \ **kwargs)<br>  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\modeling_utils.py\", line 1146, in _from_config<br>\
          \    model = cls(config, **kwargs)<br>  File \"C:\\Users\\Krzysztof/.cache\\\
          huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
          modelling_RW.py\", line 693, in <strong>init</strong><br>    self.transformer\
          \ = RWModel(config)<br>  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\\
          modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\"\
          , line 509, in <strong>init</strong><br>    self.h = nn.ModuleList([DecoderLayer(config)\
          \ for _ in range(config.num_hidden_layers)])<br>  File \"C:\\Users\\Krzysztof/.cache\\\
          huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
          modelling_RW.py\", line 509, in <br>    self.h = nn.ModuleList([DecoderLayer(config)\
          \ for _ in range(config.num_hidden_layers)])<br>  File \"C:\\Users\\Krzysztof/.cache\\\
          huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
          modelling_RW.py\", line 372, in <strong>init</strong><br>    self.mlp =\
          \ MLP(config)<br>  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\\
          transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\"\
          , line 352, in <strong>init</strong><br>    self.dense_4h_to_h = Linear(4\
          \ * hidden_size, hidden_size, bias=config.bias)<br>  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\"\
          , line 96, in <strong>init</strong><br>    self.weight = Parameter(torch.empty((out_features,\
          \ in_features), **factory_kwargs))<br>RuntimeError: [enforce fail at ..\\\
          c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough\
          \ memory: you tried to allocate 536870912 bytes.</p>\n<p>My system specs:<br>RAM:\
          \ 32GB DDR4 Ram<br>GPU: Gigabyte Geforce RTX 3060 12GB<br>CPU: 11th Gen\
          \ Intel(R) Core(TM) i9-11900F @ 2.50GHz   2.50 GHz<br>MB: Gigabyte Z590\
          \ UD AC</p>\n<p>Regards <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ !</p>\n"
        raw: "Hello @TheBloke , Sorry for bothering you, but i am trying to run this\
          \ falcon-40b-instruct-GPTQ model on my oobabooga web ui. But i see an error\
          \ when i try to load this, \"RuntimeError: [enforce fail at ..\\c10\\core\\\
          impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you\
          \ tried to allocate 536870912 bytes.\" my settings: \r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/640bb963a84bfdc022142fbe/iU11nDYle4fI-lVfMc9O2.png)\r\
          \nentire log:\r\n2023-08-09 22:03:42 INFO:Loading TheBloke_falcon-40b-instruct-GPTQ...\r\
          \n2023-08-09 22:03:42 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit--1g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ True, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True}\r\
          \n2023-08-09 22:03:42 WARNING:Exllama kernel is not installed, reset disable_exllama\
          \ to True. This may because you installed auto_gptq using a pre-build wheel\
          \ on Windows, in which exllama_kernels are not compiled. To use exllama_kernels\
          \ to further speedup inference, you can re-install auto_gptq from source.\r\
          \n2023-08-09 22:03:42 ERROR:Failed to load the model.\r\nTraceback (most\
          \ recent call last):\r\n  File \"E:\\oobabooga_windows\\text-generation-webui\\\
          modules\\ui_model_menu.py\", line 179, in load_model_wrapper\r\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)\r\n  File \"\
          E:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\", line\
          \ 78, in load_model\r\n    output = load_func_map[loader](model_name)\r\n\
          \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
          , line 292, in AutoGPTQ_loader\r\n    return modules.AutoGPTQ_loader.load_quantized(model_name)\r\
          \n  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\"\
          , line 56, in load_quantized\r\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)\r\n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling\\auto.py\", line 108, in from_quantized\r\
          \n    return quant_func(\r\n  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling\\_base.py\", line 817, in from_quantized\r\
          \n    model = AutoModelForCausalLM.from_config(\r\n  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
          , line 428, in from_config\r\n    return model_class._from_config(config,\
          \ **kwargs)\r\n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\modeling_utils.py\", line 1146, in _from_config\r\
          \n    model = cls(config, **kwargs)\r\n  File \"C:\\Users\\Krzysztof/.cache\\\
          huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
          modelling_RW.py\", line 693, in __init__\r\n    self.transformer = RWModel(config)\r\
          \n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
          TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 509, in __init__\r\
          \n    self.h = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_hidden_layers)])\r\
          \n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
          TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 509, in <listcomp>\r\
          \n    self.h = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_hidden_layers)])\r\
          \n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
          TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 372, in __init__\r\
          \n    self.mlp = MLP(config)\r\n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\\
          modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\"\
          , line 352, in __init__\r\n    self.dense_4h_to_h = Linear(4 * hidden_size,\
          \ hidden_size, bias=config.bias)\r\n  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 96, in __init__\r\
          \n    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\r\
          \nRuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72]\
          \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 536870912\
          \ bytes.\r\n\r\nMy system specs:\r\nRAM: 32GB DDR4 Ram\r\nGPU: Gigabyte\
          \ Geforce RTX 3060 12GB\r\nCPU: 11th Gen Intel(R) Core(TM) i9-11900F @ 2.50GHz\
          \   2.50 GHz\r\nMB: Gigabyte Z590 UD AC\r\n\r\nRegards @TheBloke !"
        updatedAt: '2023-08-09T20:13:25.054Z'
      numEdits: 0
      reactions: []
    id: 64d3f3653756703d462e8225
    type: comment
  author: XceptDev
  content: "Hello @TheBloke , Sorry for bothering you, but i am trying to run this\
    \ falcon-40b-instruct-GPTQ model on my oobabooga web ui. But i see an error when\
    \ i try to load this, \"RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 536870912\
    \ bytes.\" my settings: \r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/640bb963a84bfdc022142fbe/iU11nDYle4fI-lVfMc9O2.png)\r\
    \nentire log:\r\n2023-08-09 22:03:42 INFO:Loading TheBloke_falcon-40b-instruct-GPTQ...\r\
    \n2023-08-09 22:03:42 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit--1g',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': True, 'max_memory': None,\
    \ 'quantize_config': None, 'use_cuda_fp16': True}\r\n2023-08-09 22:03:42 WARNING:Exllama\
    \ kernel is not installed, reset disable_exllama to True. This may because you\
    \ installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels\
    \ are not compiled. To use exllama_kernels to further speedup inference, you can\
    \ re-install auto_gptq from source.\r\n2023-08-09 22:03:42 ERROR:Failed to load\
    \ the model.\r\nTraceback (most recent call last):\r\n  File \"E:\\oobabooga_windows\\\
    text-generation-webui\\modules\\ui_model_menu.py\", line 179, in load_model_wrapper\r\
    \n    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
    \n  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
    , line 78, in load_model\r\n    output = load_func_map[loader](model_name)\r\n\
    \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\",\
    \ line 292, in AutoGPTQ_loader\r\n    return modules.AutoGPTQ_loader.load_quantized(model_name)\r\
    \n  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\"\
    , line 56, in load_quantized\r\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params)\r\n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    auto_gptq\\modeling\\auto.py\", line 108, in from_quantized\r\n    return quant_func(\r\
    \n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
    modeling\\_base.py\", line 817, in from_quantized\r\n    model = AutoModelForCausalLM.from_config(\r\
    \n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\auto\\auto_factory.py\", line 428, in from_config\r\n    return model_class._from_config(config,\
    \ **kwargs)\r\n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\", line 1146, in _from_config\r\n    model = cls(config,\
    \ **kwargs)\r\n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
    TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 693, in __init__\r\n\
    \    self.transformer = RWModel(config)\r\n  File \"C:\\Users\\Krzysztof/.cache\\\
    huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
    modelling_RW.py\", line 509, in __init__\r\n    self.h = nn.ModuleList([DecoderLayer(config)\
    \ for _ in range(config.num_hidden_layers)])\r\n  File \"C:\\Users\\Krzysztof/.cache\\\
    huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
    modelling_RW.py\", line 509, in <listcomp>\r\n    self.h = nn.ModuleList([DecoderLayer(config)\
    \ for _ in range(config.num_hidden_layers)])\r\n  File \"C:\\Users\\Krzysztof/.cache\\\
    huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
    modelling_RW.py\", line 372, in __init__\r\n    self.mlp = MLP(config)\r\n  File\
    \ \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
    TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 352, in __init__\r\n\
    \    self.dense_4h_to_h = Linear(4 * hidden_size, hidden_size, bias=config.bias)\r\
    \n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\linear.py\", line 96, in __init__\r\n    self.weight = Parameter(torch.empty((out_features,\
    \ in_features), **factory_kwargs))\r\nRuntimeError: [enforce fail at ..\\c10\\\
    core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you\
    \ tried to allocate 536870912 bytes.\r\n\r\nMy system specs:\r\nRAM: 32GB DDR4\
    \ Ram\r\nGPU: Gigabyte Geforce RTX 3060 12GB\r\nCPU: 11th Gen Intel(R) Core(TM)\
    \ i9-11900F @ 2.50GHz   2.50 GHz\r\nMB: Gigabyte Z590 UD AC\r\n\r\nRegards @TheBloke\
    \ !"
  created_at: 2023-08-09 19:13:25+00:00
  edited: false
  hidden: false
  id: 64d3f3653756703d462e8225
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T09:26:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9874776005744934
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This means you don''t have enough RAM on this system.  You should
          be able to work around that by making sure you have a large Pagefile available
          - I''d recommend at least 100GB Pagefile.</p>

          '
        raw: This means you don't have enough RAM on this system.  You should be able
          to work around that by making sure you have a large Pagefile available -
          I'd recommend at least 100GB Pagefile.
        updatedAt: '2023-08-10T09:26:10.448Z'
      numEdits: 0
      reactions: []
    id: 64d4ad32c7170c62c9d65a14
    type: comment
  author: TheBloke
  content: This means you don't have enough RAM on this system.  You should be able
    to work around that by making sure you have a large Pagefile available - I'd recommend
    at least 100GB Pagefile.
  created_at: 2023-08-10 08:26:10+00:00
  edited: false
  hidden: false
  id: 64d4ad32c7170c62c9d65a14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
      fullname: Krzysztof Jakowluk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XceptDev
      type: user
    createdAt: '2023-08-10T09:49:57.000Z'
    data:
      edited: false
      editors:
      - XceptDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4103224277496338
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
          fullname: Krzysztof Jakowluk
          isHf: false
          isPro: false
          name: XceptDev
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , Now it successfully loads, but when i try to chat with that ai, i get\
          \ another weird error. Im gonna dono you if we can resolve this.</p>\n<p>Error:<br>Traceback\
          \ (most recent call last):<br>  File \"E:\\oobabooga_windows\\text-generation-webui\\\
          modules\\callbacks.py\", line 55, in gentask<br>    ret = self.mfunc(callback=_callback,\
          \ *args, **self.kwargs)<br>  File \"E:\\oobabooga_windows\\text-generation-webui\\\
          modules\\text_generation.py\", line 307, in generate_with_callback<br> \
          \   shared.model.generate(**kwargs)<br>  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling_base.py\"\
          , line 443, in generate<br>    return self.model.generate(**kwargs)<br>\
          \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\utils_contextlib.py\", line 115, in decorate_context<br>    return\
          \ func(*args, **kwargs)<br>  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1633,\
          \ in generate<br>    return self.sample(<br>  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 2755, in sample<br>    outputs = self(<br>  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
          TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 759, in forward<br>\
          \    transformer_outputs = self.transformer(<br>  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
          TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 654, in forward<br>\
          \    outputs = block(<br>  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl<br>    return forward_call(*args, **kwargs)<br>  File \"E:\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\\
          hooks.py\", line 165, in new_forward<br>    output = old_forward(*args,\
          \ **kwargs)<br>  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\\
          transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\"\
          , line 396, in forward<br>    attn_outputs = self.self_attention(<br>  File\
          \ \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl<br>    return forward_call(*args,\
          \ **kwargs)<br>  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\\
          transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\"\
          , line 252, in forward<br>    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]<br>  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          auto_gptq\\nn_modules\\qlinear\\qlinear_cuda_old.py\", line 271, in forward<br>\
          \    out = out + self.bias if self.bias is not None else out<br>RuntimeError:\
          \ Expected all tensors to be on the same device, but found at least two\
          \ devices, cuda:0 and cpu!<br>Exception in thread Thread-7 (gentask):<br>Traceback\
          \ (most recent call last):<br>  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\threading.py\", line 1016, in _bootstrap_inner<br>    self.run()<br>\
          \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\threading.py\"\
          , line 953, in run<br>    self._target(*self._args, **self._kwargs)<br>\
          \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
          , line 62, in gentask<br>    clear_torch_cache()<br>  File \"E:\\oobabooga_windows\\\
          text-generation-webui\\modules\\callbacks.py\", line 94, in clear_torch_cache<br>\
          \    torch.cuda.empty_cache()<br>  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\cuda\\memory.py\", line 133, in empty_cache<br>\
          \    torch._C._cuda_emptyCache()<br>RuntimeError: CUDA error: an illegal\
          \ memory access was encountered<br>CUDA kernel errors might be asynchronously\
          \ reported at some other API call, so the stacktrace below might be incorrect.<br>For\
          \ debugging consider passing CUDA_LAUNCH_BLOCKING=1.<br>Compile with <code>TORCH_USE_CUDA_DSA</code>\
          \ to enable device-side assertions.</p>\n"
        raw: "Hello @TheBloke , Now it successfully loads, but when i try to chat\
          \ with that ai, i get another weird error. Im gonna dono you if we can resolve\
          \ this.\n\nError:\nTraceback (most recent call last):\n  File \"E:\\oobabooga_windows\\\
          text-generation-webui\\modules\\callbacks.py\", line 55, in gentask\n  \
          \  ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n  File \"\
          E:\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 307, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          auto_gptq\\modeling\\_base.py\", line 443, in generate\n    return self.model.generate(**kwargs)\n\
          \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return\
          \ func(*args, **kwargs)\n  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1633,\
          \ in generate\n    return self.sample(\n  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 2755, in sample\n    outputs = self(\n  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
          TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 759, in forward\n\
          \    transformer_outputs = self.transformer(\n  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
          TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 654, in forward\n\
          \    outputs = block(\n  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py\", line 165,\
          \ in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"C:\\\
          Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
          modelling_RW.py\", line 396, in forward\n    attn_outputs = self.self_attention(\n\
          \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\\
          transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\"\
          , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\n  File \"E:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          auto_gptq\\nn_modules\\qlinear\\qlinear_cuda_old.py\", line 271, in forward\n\
          \    out = out + self.bias if self.bias is not None else out\nRuntimeError:\
          \ Expected all tensors to be on the same device, but found at least two\
          \ devices, cuda:0 and cpu!\nException in thread Thread-7 (gentask):\nTraceback\
          \ (most recent call last):\n  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n\
          \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\threading.py\"\
          , line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File\
          \ \"E:\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
          , line 62, in gentask\n    clear_torch_cache()\n  File \"E:\\oobabooga_windows\\\
          text-generation-webui\\modules\\callbacks.py\", line 94, in clear_torch_cache\n\
          \    torch.cuda.empty_cache()\n  File \"E:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\cuda\\memory.py\", line 133, in empty_cache\n\
          \    torch._C._cuda_emptyCache()\nRuntimeError: CUDA error: an illegal memory\
          \ access was encountered\nCUDA kernel errors might be asynchronously reported\
          \ at some other API call, so the stacktrace below might be incorrect.\n\
          For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
          \ to enable device-side assertions.\n\n"
        updatedAt: '2023-08-10T09:49:57.816Z'
      numEdits: 0
      reactions: []
    id: 64d4b2c527b4f1c737925747
    type: comment
  author: XceptDev
  content: "Hello @TheBloke , Now it successfully loads, but when i try to chat with\
    \ that ai, i get another weird error. Im gonna dono you if we can resolve this.\n\
    \nError:\nTraceback (most recent call last):\n  File \"E:\\oobabooga_windows\\\
    text-generation-webui\\modules\\callbacks.py\", line 55, in gentask\n    ret =\
    \ self.mfunc(callback=_callback, *args, **self.kwargs)\n  File \"E:\\oobabooga_windows\\\
    text-generation-webui\\modules\\text_generation.py\", line 307, in generate_with_callback\n\
    \    shared.model.generate(**kwargs)\n  File \"E:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling\\_base.py\", line 443, in generate\n\
    \    return self.model.generate(**kwargs)\n  File \"E:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n\
    \    return func(*args, **kwargs)\n  File \"E:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1633, in generate\n\
    \    return self.sample(\n  File \"E:\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\generation\\utils.py\", line 2755, in sample\n\
    \    outputs = self(\n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n   \
    \ return forward_call(*args, **kwargs)\n  File \"C:\\Users\\Krzysztof/.cache\\\
    huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
    modelling_RW.py\", line 759, in forward\n    transformer_outputs = self.transformer(\n\
    \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
    TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 654, in forward\n \
    \   outputs = block(\n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n   \
    \ return forward_call(*args, **kwargs)\n  File \"E:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\n  \
    \  output = old_forward(*args, **kwargs)\n  File \"C:\\Users\\Krzysztof/.cache\\\
    huggingface\\modules\\transformers_modules\\TheBloke_falcon-40b-instruct-GPTQ\\\
    modelling_RW.py\", line 396, in forward\n    attn_outputs = self.self_attention(\n\
    \  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"C:\\Users\\Krzysztof/.cache\\huggingface\\modules\\transformers_modules\\\
    TheBloke_falcon-40b-instruct-GPTQ\\modelling_RW.py\", line 252, in forward\n \
    \   fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length,\
    \ 3 x hidden_size]\n  File \"E:\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n   \
    \ return forward_call(*args, **kwargs)\n  File \"E:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\nn_modules\\qlinear\\qlinear_cuda_old.py\"\
    , line 271, in forward\n    out = out + self.bias if self.bias is not None else\
    \ out\nRuntimeError: Expected all tensors to be on the same device, but found\
    \ at least two devices, cuda:0 and cpu!\nException in thread Thread-7 (gentask):\n\
    Traceback (most recent call last):\n  File \"E:\\oobabooga_windows\\installer_files\\\
    env\\lib\\threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File\
    \ \"E:\\oobabooga_windows\\installer_files\\env\\lib\\threading.py\", line 953,\
    \ in run\n    self._target(*self._args, **self._kwargs)\n  File \"E:\\oobabooga_windows\\\
    text-generation-webui\\modules\\callbacks.py\", line 62, in gentask\n    clear_torch_cache()\n\
    \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
    , line 94, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"E:\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\cuda\\memory.py\", line 133,\
    \ in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError: CUDA error: an\
    \ illegal memory access was encountered\nCUDA kernel errors might be asynchronously\
    \ reported at some other API call, so the stacktrace below might be incorrect.\n\
    For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\n\n"
  created_at: 2023-08-10 08:49:57+00:00
  edited: false
  hidden: false
  id: 64d4b2c527b4f1c737925747
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T09:56:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9630858302116394
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah yeah, it looks like you don''t have enough VRAM so it is trying
          to split it across CPU and GPU, which is not supported with this model:</p>

          <pre><code>RuntimeError: Expected all tensors to be on the same device,
          but found at least two devices, cuda:0 and cpu!

          </code></pre>

          <p>There''s no solution for that I believe. You just need more VRAM, or
          to use another model.  Even if you do get this working, the Falcon 40B GPTQs
          are super slow.</p>

          <p>Or you could try my Falcon 40B GGMLs. They won''t work in text-generation-webui
          though.</p>

          <p>Personally I no longer recommend Falcon 40B to people. It is slow, has
          several problems, and has never been properly integrated into Transformers
          which suggests to me it''s not being actively maintained any more.</p>

          <p>With the release of Llama 2, it''s now possible to get a fully commercially
          licensed model with much better support and performance, and decent quality.  There''s
          no Llama 2 model of the same size as Falcon 40B yet (there''s meant to be
          a Llama 2 34B released at some point, but we haven''t heard any more news
          on that recently), but the 13B is very capable and the 70B is much better
          than Falcon 40B - if you have a big enough system to run it.</p>

          <p>Or if you don''t care about commercial licensing, check out one of the
          many Llama 1 30B/33B models which are very good and also very well supported.</p>

          <p>If you''re determined to run Falcon 40B in text-generation-webui, the
          option I would recommend is to download the original unquantised Falcon
          40B model (linked in my README)  and use Transformers + bitsandbytes <code>load_in_4bit</code>.  For
          that you will need 1 x 48GB GPU or 2 x 24GB GPU.  You can load that in text-generation-webui
          using the Transformers loader; there''s a checkbox for load in 4 bit.  </p>

          '
        raw: 'Ah yeah, it looks like you don''t have enough VRAM so it is trying to
          split it across CPU and GPU, which is not supported with this model:

          ```

          RuntimeError: Expected all tensors to be on the same device, but found at
          least two devices, cuda:0 and cpu!

          ```


          There''s no solution for that I believe. You just need more VRAM, or to
          use another model.  Even if you do get this working, the Falcon 40B GPTQs
          are super slow.


          Or you could try my Falcon 40B GGMLs. They won''t work in text-generation-webui
          though.


          Personally I no longer recommend Falcon 40B to people. It is slow, has several
          problems, and has never been properly integrated into Transformers which
          suggests to me it''s not being actively maintained any more.


          With the release of Llama 2, it''s now possible to get a fully commercially
          licensed model with much better support and performance, and decent quality.  There''s
          no Llama 2 model of the same size as Falcon 40B yet (there''s meant to be
          a Llama 2 34B released at some point, but we haven''t heard any more news
          on that recently), but the 13B is very capable and the 70B is much better
          than Falcon 40B - if you have a big enough system to run it.


          Or if you don''t care about commercial licensing, check out one of the many
          Llama 1 30B/33B models which are very good and also very well supported.


          If you''re determined to run Falcon 40B in text-generation-webui, the option
          I would recommend is to download the original unquantised Falcon 40B model
          (linked in my README)  and use Transformers + bitsandbytes `load_in_4bit`.  For
          that you will need 1 x 48GB GPU or 2 x 24GB GPU.  You can load that in text-generation-webui
          using the Transformers loader; there''s a checkbox for load in 4 bit.  '
        updatedAt: '2023-08-10T09:56:50.112Z'
      numEdits: 0
      reactions: []
    id: 64d4b462d7e30889c62e43a6
    type: comment
  author: TheBloke
  content: 'Ah yeah, it looks like you don''t have enough VRAM so it is trying to
    split it across CPU and GPU, which is not supported with this model:

    ```

    RuntimeError: Expected all tensors to be on the same device, but found at least
    two devices, cuda:0 and cpu!

    ```


    There''s no solution for that I believe. You just need more VRAM, or to use another
    model.  Even if you do get this working, the Falcon 40B GPTQs are super slow.


    Or you could try my Falcon 40B GGMLs. They won''t work in text-generation-webui
    though.


    Personally I no longer recommend Falcon 40B to people. It is slow, has several
    problems, and has never been properly integrated into Transformers which suggests
    to me it''s not being actively maintained any more.


    With the release of Llama 2, it''s now possible to get a fully commercially licensed
    model with much better support and performance, and decent quality.  There''s
    no Llama 2 model of the same size as Falcon 40B yet (there''s meant to be a Llama
    2 34B released at some point, but we haven''t heard any more news on that recently),
    but the 13B is very capable and the 70B is much better than Falcon 40B - if you
    have a big enough system to run it.


    Or if you don''t care about commercial licensing, check out one of the many Llama
    1 30B/33B models which are very good and also very well supported.


    If you''re determined to run Falcon 40B in text-generation-webui, the option I
    would recommend is to download the original unquantised Falcon 40B model (linked
    in my README)  and use Transformers + bitsandbytes `load_in_4bit`.  For that you
    will need 1 x 48GB GPU or 2 x 24GB GPU.  You can load that in text-generation-webui
    using the Transformers loader; there''s a checkbox for load in 4 bit.  '
  created_at: 2023-08-10 08:56:50+00:00
  edited: false
  hidden: false
  id: 64d4b462d7e30889c62e43a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
      fullname: Krzysztof Jakowluk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XceptDev
      type: user
    createdAt: '2023-08-10T10:37:22.000Z'
    data:
      edited: false
      editors:
      - XceptDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8543838262557983
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
          fullname: Krzysztof Jakowluk
          isHf: false
          isPro: false
          name: XceptDev
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ ! Thank you for great suggestions. for that 1 x 48GB GPU or 2 x 24GB GPU,\
          \ can I just add vram to my gpu through regedit?</p>\n"
        raw: Hello @TheBloke ! Thank you for great suggestions. for that 1 x 48GB
          GPU or 2 x 24GB GPU, can I just add vram to my gpu through regedit?
        updatedAt: '2023-08-10T10:37:22.879Z'
      numEdits: 0
      reactions: []
    id: 64d4bde267c967b0158d2c20
    type: comment
  author: XceptDev
  content: Hello @TheBloke ! Thank you for great suggestions. for that 1 x 48GB GPU
    or 2 x 24GB GPU, can I just add vram to my gpu through regedit?
  created_at: 2023-08-10 09:37:22+00:00
  edited: false
  hidden: false
  id: 64d4bde267c967b0158d2c20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T10:37:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41803768277168274
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>...no. You need real VRAM on a real GPU.</p>

          '
        raw: '...no. You need real VRAM on a real GPU.'
        updatedAt: '2023-08-10T10:37:53.689Z'
      numEdits: 0
      reactions: []
    id: 64d4be013ca2924d6e381f2d
    type: comment
  author: TheBloke
  content: '...no. You need real VRAM on a real GPU.'
  created_at: 2023-08-10 09:37:53+00:00
  edited: false
  hidden: false
  id: 64d4be013ca2924d6e381f2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
      fullname: Krzysztof Jakowluk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XceptDev
      type: user
    createdAt: '2023-08-10T11:32:32.000Z'
    data:
      edited: false
      editors:
      - XceptDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7439858913421631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
          fullname: Krzysztof Jakowluk
          isHf: false
          isPro: false
          name: XceptDev
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>.\
          \ I managed to run your llama 13B model. A weird question: Can you make\
          \ instruction for newbies (like me) on training/fine-tuning it using text\
          \ generation webui?</p>\n"
        raw: 'Hi @TheBloke. I managed to run your llama 13B model. A weird question:
          Can you make instruction for newbies (like me) on training/fine-tuning it
          using text generation webui?'
        updatedAt: '2023-08-10T11:32:32.820Z'
      numEdits: 0
      reactions: []
    id: 64d4cad09849ca4720ef4654
    type: comment
  author: XceptDev
  content: 'Hi @TheBloke. I managed to run your llama 13B model. A weird question:
    Can you make instruction for newbies (like me) on training/fine-tuning it using
    text generation webui?'
  created_at: 2023-08-10 10:32:32+00:00
  edited: false
  hidden: false
  id: 64d4cad09849ca4720ef4654
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77f1009063efe198432a4c9358c6cd30.svg
      fullname: Timothy Trieu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tim1210
      type: user
    createdAt: '2023-08-16T20:19:41.000Z'
    data:
      edited: false
      editors:
      - Tim1210
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9813254475593567
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77f1009063efe198432a4c9358c6cd30.svg
          fullname: Timothy Trieu
          isHf: false
          isPro: false
          name: Tim1210
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;XceptDev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/XceptDev\">@<span class=\"\
          underline\">XceptDev</span></a></span>\n\n\t</span></span> How did you get\
          \ it to run?</p>\n"
        raw: '@XceptDev How did you get it to run?

          '
        updatedAt: '2023-08-16T20:19:41.614Z'
      numEdits: 0
      reactions: []
    id: 64dd2f5dc79e3245ebb4f16b
    type: comment
  author: Tim1210
  content: '@XceptDev How did you get it to run?

    '
  created_at: 2023-08-16 19:19:41+00:00
  edited: false
  hidden: false
  id: 64dd2f5dc79e3245ebb4f16b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77f1009063efe198432a4c9358c6cd30.svg
      fullname: Timothy Trieu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tim1210
      type: user
    createdAt: '2023-08-18T13:47:42.000Z'
    data:
      edited: false
      editors:
      - Tim1210
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9222767949104309
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77f1009063efe198432a4c9358c6cd30.svg
          fullname: Timothy Trieu
          isHf: false
          isPro: false
          name: Tim1210
          type: user
        html: '<p>I couldnt run the model on my 4090 (24GB VRAM)<br>I was in dependency
          nightmare mode between pip and NVDIA CUDA, so I went back to the OOBABOOGA
          repo and followed the instructions to set it up in Docker.<br>snags for
          docker setup</p>

          <ul>

          <li>need to move files from docker folder to text generation folder</li>

          <li>may need to use localhost:7860 not 0.0.0.0:7860 depending on your OS
          browser</li>

          </ul>

          <p>After setting up all the prereqs for docker it nice that dependency management
          is all on the container, and also a nice extra level of safety. Good luck
          to everyone out there. I will try again on my local machine after I rebuild
          my computer</p>

          '
        raw: 'I couldnt run the model on my 4090 (24GB VRAM)

          I was in dependency nightmare mode between pip and NVDIA CUDA, so I went
          back to the OOBABOOGA repo and followed the instructions to set it up in
          Docker.

          snags for docker setup

          - need to move files from docker folder to text generation folder

          - may need to use localhost:7860 not 0.0.0.0:7860 depending on your OS browser


          After setting up all the prereqs for docker it nice that dependency management
          is all on the container, and also a nice extra level of safety. Good luck
          to everyone out there. I will try again on my local machine after I rebuild
          my computer

          '
        updatedAt: '2023-08-18T13:47:42.332Z'
      numEdits: 0
      reactions: []
    id: 64df767e8276275de579d77d
    type: comment
  author: Tim1210
  content: 'I couldnt run the model on my 4090 (24GB VRAM)

    I was in dependency nightmare mode between pip and NVDIA CUDA, so I went back
    to the OOBABOOGA repo and followed the instructions to set it up in Docker.

    snags for docker setup

    - need to move files from docker folder to text generation folder

    - may need to use localhost:7860 not 0.0.0.0:7860 depending on your OS browser


    After setting up all the prereqs for docker it nice that dependency management
    is all on the container, and also a nice extra level of safety. Good luck to everyone
    out there. I will try again on my local machine after I rebuild my computer

    '
  created_at: 2023-08-18 12:47:42+00:00
  edited: false
  hidden: false
  id: 64df767e8276275de579d77d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
      fullname: Krzysztof Jakowluk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XceptDev
      type: user
    createdAt: '2023-08-26T20:55:15.000Z'
    data:
      edited: false
      editors:
      - XceptDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8200636506080627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e825c9ca2acd3fce4e1f0be6222e43f.svg
          fullname: Krzysztof Jakowluk
          isHf: false
          isPro: false
          name: XceptDev
          type: user
        html: '<p>Guys, I will teoll you soon when I will be back home</p>

          '
        raw: Guys, I will teoll you soon when I will be back home
        updatedAt: '2023-08-26T20:55:15.886Z'
      numEdits: 0
      reactions: []
    id: 64ea66b3a1e1a36f960b0014
    type: comment
  author: XceptDev
  content: Guys, I will teoll you soon when I will be back home
  created_at: 2023-08-26 19:55:15+00:00
  edited: false
  hidden: false
  id: 64ea66b3a1e1a36f960b0014
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b8db8928c0d4e4566440d3164474cab.svg
      fullname: Muhammad Adnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AdnanBajwa
      type: user
    createdAt: '2023-12-12T08:14:52.000Z'
    data:
      edited: false
      editors:
      - AdnanBajwa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4965806007385254
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b8db8928c0d4e4566440d3164474cab.svg
          fullname: Muhammad Adnan
          isHf: false
          isPro: false
          name: AdnanBajwa
          type: user
        html: '<p>from transformers import AutoTokenizer, pipeline, logging<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import argparse<br>import
          torch<br>model_name_or_path = "TheBloke/falcon-7b-instruct-GPTQ"</p>

          <p>model_basename = "model"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        disable_exllamav2=False,<br>        torch_dtype
          = torch.float16,<br>        quantize_config=None)</p>

          <p>prompt = "Tell me about AI"<br>prompt_template=f''''''A helpful assistant
          who helps the user with any questions asked.<br>User: {prompt}<br>Assistant:''''''</p>

          <p>print("\n\n*** Generate:")</p>

          <p>logging.set_verbosity(logging.CRITICAL)</p>

          <p>print("*** Pipeline:")<br>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>    temperature=0.7,<br>    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>

          <p>print(pipe(prompt_template)[0][''generated_text''])<br>I am using this
          code for inference , i have tried this code with "torch_dtype = torch.float16,
          " and without this. but get same error every time.</p>

          <p>File /media/adnan/66f2dbd6-5cbe-49fb-9bc5-7622a4fbc0f5/Text_Work/updated_python/penv/lib/python3.11/site-packages/transformers/generation/utils.py:1673,
          in GenerationMixin.generate(self, inputs, generation_config, logits_processor,
          stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,
          streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)<br>   1656     return
          self.assisted_decoding(<br>...<br>     31 output = torch.empty((x.shape[0],
          q4_width), dtype = torch.half, device = x.device)<br>---&gt; 32 gemm_half_q_half(x,
          q_handle, output, force_cuda)<br>     33 return output.view(output_shape)</p>

          <p>RuntimeError: a is incorrect datatype, must be kHalf</p>

          '
        raw: "from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\nimport\
          \ torch\nmodel_name_or_path = \"TheBloke/falcon-7b-instruct-GPTQ\"\n\nmodel_basename\
          \ = \"model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        disable_exllamav2=False,\n        torch_dtype = torch.float16,\
          \ \n        quantize_config=None)\n\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A\
          \ helpful assistant who helps the user with any questions asked.\nUser:\
          \ {prompt}\nAssistant:'''\n\nprint(\"\\n\\n*** Generate:\")\n\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\nI am using this code\
          \ for inference , i have tried this code with \"torch_dtype = torch.float16,\
          \ \" and without this. but get same error every time.\n\nFile /media/adnan/66f2dbd6-5cbe-49fb-9bc5-7622a4fbc0f5/Text_Work/updated_python/penv/lib/python3.11/site-packages/transformers/generation/utils.py:1673,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n\
          \   1656     return self.assisted_decoding(\n...\n     31 output = torch.empty((x.shape[0],\
          \ q4_width), dtype = torch.half, device = x.device)\n---> 32 gemm_half_q_half(x,\
          \ q_handle, output, force_cuda)\n     33 return output.view(output_shape)\n\
          \nRuntimeError: a is incorrect datatype, must be kHalf"
        updatedAt: '2023-12-12T08:14:52.657Z'
      numEdits: 0
      reactions: []
    id: 6578167c2f2e4058aed2c101
    type: comment
  author: AdnanBajwa
  content: "from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\nimport torch\n\
    model_name_or_path = \"TheBloke/falcon-7b-instruct-GPTQ\"\n\nmodel_basename =\
    \ \"model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        disable_exllamav2=False,\n        torch_dtype = torch.float16, \n   \
    \     quantize_config=None)\n\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A\
    \ helpful assistant who helps the user with any questions asked.\nUser: {prompt}\n\
    Assistant:'''\n\nprint(\"\\n\\n*** Generate:\")\n\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n   \
    \ top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    I am using this code for inference , i have tried this code with \"torch_dtype\
    \ = torch.float16, \" and without this. but get same error every time.\n\nFile\
    \ /media/adnan/66f2dbd6-5cbe-49fb-9bc5-7622a4fbc0f5/Text_Work/updated_python/penv/lib/python3.11/site-packages/transformers/generation/utils.py:1673,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n   1656    \
    \ return self.assisted_decoding(\n...\n     31 output = torch.empty((x.shape[0],\
    \ q4_width), dtype = torch.half, device = x.device)\n---> 32 gemm_half_q_half(x,\
    \ q_handle, output, force_cuda)\n     33 return output.view(output_shape)\n\n\
    RuntimeError: a is incorrect datatype, must be kHalf"
  created_at: 2023-12-12 08:14:52+00:00
  edited: false
  hidden: false
  id: 6578167c2f2e4058aed2c101
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b8db8928c0d4e4566440d3164474cab.svg
      fullname: Muhammad Adnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AdnanBajwa
      type: user
    createdAt: '2023-12-12T08:17:55.000Z'
    data:
      edited: false
      editors:
      - AdnanBajwa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5831021666526794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b8db8928c0d4e4566440d3164474cab.svg
          fullname: Muhammad Adnan
          isHf: false
          isPro: false
          name: AdnanBajwa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;AdnanBajwa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AdnanBajwa\">@<span class=\"\
          underline\">AdnanBajwa</span></a></span>\n\n\t</span></span>  i have 2 x\
          \  NVIDIA Quadro RTX 6000 (24GB) and GPU memory 48 GB. and 128 GB RAM.</p>\n"
        raw: '@AdnanBajwa  i have 2 x  NVIDIA Quadro RTX 6000 (24GB) and GPU memory
          48 GB. and 128 GB RAM.'
        updatedAt: '2023-12-12T08:17:55.441Z'
      numEdits: 0
      reactions: []
    id: 65781733f7d98f6f610c6ebd
    type: comment
  author: AdnanBajwa
  content: '@AdnanBajwa  i have 2 x  NVIDIA Quadro RTX 6000 (24GB) and GPU memory
    48 GB. and 128 GB RAM.'
  created_at: 2023-12-12 08:17:55+00:00
  edited: false
  hidden: false
  id: 65781733f7d98f6f610c6ebd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: A weird bug
