!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jdc4429
conflicting_files: null
created_at: 2023-05-31 12:28:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T13:28:27.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Good morning,</p>

          <p>I have been trying to get the model runing can''t figure out why as it
          states files are missing even though I downloaded all the files.  Here is
          the error:</p>

          <p>OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index
          or flax_model.msgpack found in directory models/falcon-40b-instruct-GPTQ.</p>

          <p>How do you tell it to use .safetensors? Why does it not see the file
          by default?</p>

          <p>Regards,</p>

          <p>Jeff</p>

          '
        raw: "Good morning,\r\n\r\nI have been trying to get the model runing can't\
          \ figure out why as it states files are missing even though I downloaded\
          \ all the files.  Here is the error:\r\n\r\nOSError: Error no file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models/falcon-40b-instruct-GPTQ.\r\n\r\nHow do you\
          \ tell it to use .safetensors? Why does it not see the file by default?\r\
          \n\r\nRegards,\r\n\r\nJeff"
        updatedAt: '2023-05-31T13:28:27.450Z'
      numEdits: 0
      reactions: []
    id: 64774b7bdbc2a416f8b5cc07
    type: comment
  author: jdc4429
  content: "Good morning,\r\n\r\nI have been trying to get the model runing can't\
    \ figure out why as it states files are missing even though I downloaded all the\
    \ files.  Here is the error:\r\n\r\nOSError: Error no file named pytorch_model.bin,\
    \ tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory models/falcon-40b-instruct-GPTQ.\r\
    \n\r\nHow do you tell it to use .safetensors? Why does it not see the file by\
    \ default?\r\n\r\nRegards,\r\n\r\nJeff"
  created_at: 2023-05-31 12:28:27+00:00
  edited: false
  hidden: false
  id: 64774b7bdbc2a416f8b5cc07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T13:34:30.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Did you launch text-generation-webui with the <code>--autogptq</code>
          flag?  If not, please tick "AutoGPTQ" under model parameters, then "Save
          settings for this model" and "reload this model"</p>

          '
        raw: Did you launch text-generation-webui with the `--autogptq` flag?  If
          not, please tick "AutoGPTQ" under model parameters, then "Save settings
          for this model" and "reload this model"
        updatedAt: '2023-05-31T13:34:30.990Z'
      numEdits: 0
      reactions: []
    id: 64774ce633a888101f73fb19
    type: comment
  author: TheBloke
  content: Did you launch text-generation-webui with the `--autogptq` flag?  If not,
    please tick "AutoGPTQ" under model parameters, then "Save settings for this model"
    and "reload this model"
  created_at: 2023-05-31 12:34:30+00:00
  edited: false
  hidden: false
  id: 64774ce633a888101f73fb19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T13:34:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>And you''re right it should be able to detect this automatically.
          I will discuss that with oobabooga.</p>

          '
        raw: And you're right it should be able to detect this automatically. I will
          discuss that with oobabooga.
        updatedAt: '2023-05-31T13:34:53.982Z'
      numEdits: 0
      reactions: []
    id: 64774cfdf911e9e76c60734b
    type: comment
  author: TheBloke
  content: And you're right it should be able to detect this automatically. I will
    discuss that with oobabooga.
  created_at: 2023-05-31 12:34:53+00:00
  edited: false
  hidden: false
  id: 64774cfdf911e9e76c60734b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T13:57:28.000Z'
    data:
      edited: true
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Yes, I added the option right to the webui.py file where I have
          all the other options.. Should be fine in that regard.</p>

          <p>CMD_FLAGS = ''--chat --model-menu --gpu-memory 6800MiB 11000MiB 11000MiB
          --cpu-memory 64 --share --trust-remote-code --autogptq''</p>

          <p>Arg.. I had compiled AutogpTQ and it compiled without errors I though
          for 11.7.  Now when I try and compile again it''s stating :</p>

          <p>The detected CUDA version (11.5) has a minor version mismatch with the
          version that was used to compile PyTorch (11.7). Most likely this shouldn''t
          be a problem.</p>

          <p>FAILED: /home/jeff/AutoGPTQ/build/temp.linux-x86_64-3.10/autogptq_cuda/autogptq_cuda_kernel.o
          </p>

          <p>I''m not sure why it''s stating I''m using 11.5...</p>

          <p>+-----------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7     |<br>|-------------------------------+----------------------+----------------------+</p>

          <p>OK.. Nevermind.. Edited.. I did python setup.py clean and then build
          again and it compiled ok.  let''s try this again.. :)</p>

          <p>I am still getting an error stating the module can''t be found even though
          I did build, install ???</p>

          <p>ModuleNotFoundError: No module named ''auto_gptq''</p>

          <p>So I tried.. pip install auto_gptq  which installed accelerate..</p>

          <p>But I''m still getting the error...</p>

          <p>ModuleNotFoundError: No module named ''auto_gptq''</p>

          <p>Full error:</p>

          <p>Traceback (most recent call last):<br>  File "/home/jeff/oobabooga_linux/text-generation-webui/server.py",
          line 1087, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py", line
          95, in load_model<br>    output = load_func(model_name)<br>  File "/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py",
          line 297, in AutoGPTQ_loader<br>    import modules.AutoGPTQ_loader<br>  File
          "/home/jeff/oobabooga_linux/text-generation-webui/modules/AutoGPTQ_loader.py",
          line 3, in <br>    from auto_gptq import AutoGPTQForCausalLM<br>ModuleNotFoundError:
          No module named ''auto_gptq''</p>

          '
        raw: "Yes, I added the option right to the webui.py file where I have all\
          \ the other options.. Should be fine in that regard.\n\nCMD_FLAGS = '--chat\
          \ --model-menu --gpu-memory 6800MiB 11000MiB 11000MiB --cpu-memory 64 --share\
          \ --trust-remote-code --autogptq'\n\nArg.. I had compiled AutogpTQ and it\
          \ compiled without errors I though for 11.7.  Now when I try and compile\
          \ again it's stating :\n\nThe detected CUDA version (11.5) has a minor version\
          \ mismatch with the version that was used to compile PyTorch (11.7). Most\
          \ likely this shouldn't be a problem.\n\nFAILED: /home/jeff/AutoGPTQ/build/temp.linux-x86_64-3.10/autogptq_cuda/autogptq_cuda_kernel.o\
          \ \n\nI'm not sure why it's stating I'm using 11.5...\n\n+-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          \nOK.. Nevermind.. Edited.. I did python setup.py clean and then build again\
          \ and it compiled ok.  let's try this again.. :)\n\nI am still getting an\
          \ error stating the module can't be found even though I did build, install\
          \ ???\n\nModuleNotFoundError: No module named 'auto_gptq'\n\nSo I tried..\
          \ pip install auto_gptq  which installed accelerate..\n\nBut I'm still getting\
          \ the error...\n\nModuleNotFoundError: No module named 'auto_gptq'\n\nFull\
          \ error:\n\nTraceback (most recent call last):\n  File \"/home/jeff/oobabooga_linux/text-generation-webui/server.py\"\
          , line 1087, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\"\
          , line 95, in load_model\n    output = load_func(model_name)\n  File \"\
          /home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\", line\
          \ 297, in AutoGPTQ_loader\n    import modules.AutoGPTQ_loader\n  File \"\
          /home/jeff/oobabooga_linux/text-generation-webui/modules/AutoGPTQ_loader.py\"\
          , line 3, in <module>\n    from auto_gptq import AutoGPTQForCausalLM\nModuleNotFoundError:\
          \ No module named 'auto_gptq'"
        updatedAt: '2023-05-31T14:09:49.561Z'
      numEdits: 5
      reactions: []
    id: 64775248f911e9e76c610ceb
    type: comment
  author: jdc4429
  content: "Yes, I added the option right to the webui.py file where I have all the\
    \ other options.. Should be fine in that regard.\n\nCMD_FLAGS = '--chat --model-menu\
    \ --gpu-memory 6800MiB 11000MiB 11000MiB --cpu-memory 64 --share --trust-remote-code\
    \ --autogptq'\n\nArg.. I had compiled AutogpTQ and it compiled without errors\
    \ I though for 11.7.  Now when I try and compile again it's stating :\n\nThe detected\
    \ CUDA version (11.5) has a minor version mismatch with the version that was used\
    \ to compile PyTorch (11.7). Most likely this shouldn't be a problem.\n\nFAILED:\
    \ /home/jeff/AutoGPTQ/build/temp.linux-x86_64-3.10/autogptq_cuda/autogptq_cuda_kernel.o\
    \ \n\nI'm not sure why it's stating I'm using 11.5...\n\n+-----------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7    \
    \ |\n|-------------------------------+----------------------+----------------------+\n\
    \nOK.. Nevermind.. Edited.. I did python setup.py clean and then build again and\
    \ it compiled ok.  let's try this again.. :)\n\nI am still getting an error stating\
    \ the module can't be found even though I did build, install ???\n\nModuleNotFoundError:\
    \ No module named 'auto_gptq'\n\nSo I tried.. pip install auto_gptq  which installed\
    \ accelerate..\n\nBut I'm still getting the error...\n\nModuleNotFoundError: No\
    \ module named 'auto_gptq'\n\nFull error:\n\nTraceback (most recent call last):\n\
    \  File \"/home/jeff/oobabooga_linux/text-generation-webui/server.py\", line 1087,\
    \ in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\"\
    , line 95, in load_model\n    output = load_func(model_name)\n  File \"/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\"\
    , line 297, in AutoGPTQ_loader\n    import modules.AutoGPTQ_loader\n  File \"\
    /home/jeff/oobabooga_linux/text-generation-webui/modules/AutoGPTQ_loader.py\"\
    , line 3, in <module>\n    from auto_gptq import AutoGPTQForCausalLM\nModuleNotFoundError:\
    \ No module named 'auto_gptq'"
  created_at: 2023-05-31 12:57:28+00:00
  edited: true
  hidden: false
  id: 64775248f911e9e76c610ceb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T14:20:56.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: "<p>I tried also as you suggested to select from within the interface\
          \ (auto_gptq) and it does not give me an error but I am back to the problem\
          \ with it not detecting the .safetensors file. :)</p>\n<p>Traceback (most\
          \ recent call last): File \u201C/home/jeff/oobabooga_linux/text-generation-webui/server.py\u201D\
          , line 71, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\u201D\
          , line 95, in load_model output = load_func(model_name) File \u201C/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\u201D\
          , line 225, in huggingface_loader model = LoaderClass.from_pretrained(checkpoint,\
          \ **params) File \u201C/home/jeff/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
          , line 466, in from_pretrained return model_class.from_pretrained( File\
          \ \u201C/home/jeff/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 2405, in from_pretrained raise EnvironmentError( OSError: Error no\
          \ file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models/falcon-40b-instruct-GPTQ.</p>\n"
        raw: "I tried also as you suggested to select from within the interface (auto_gptq)\
          \ and it does not give me an error but I am back to the problem with it\
          \ not detecting the .safetensors file. :)\n\nTraceback (most recent call\
          \ last): File \u201C/home/jeff/oobabooga_linux/text-generation-webui/server.py\u201D\
          , line 71, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\u201D\
          , line 95, in load_model output = load_func(model_name) File \u201C/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\u201D\
          , line 225, in huggingface_loader model = LoaderClass.from_pretrained(checkpoint,\
          \ **params) File \u201C/home/jeff/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
          , line 466, in from_pretrained return model_class.from_pretrained( File\
          \ \u201C/home/jeff/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 2405, in from_pretrained raise EnvironmentError( OSError: Error no\
          \ file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models/falcon-40b-instruct-GPTQ."
        updatedAt: '2023-05-31T14:20:56.379Z'
      numEdits: 0
      reactions: []
    id: 647757c8f911e9e76c619af2
    type: comment
  author: jdc4429
  content: "I tried also as you suggested to select from within the interface (auto_gptq)\
    \ and it does not give me an error but I am back to the problem with it not detecting\
    \ the .safetensors file. :)\n\nTraceback (most recent call last): File \u201C\
    /home/jeff/oobabooga_linux/text-generation-webui/server.py\u201D, line 71, in\
    \ load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201C/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\u201D\
    , line 95, in load_model output = load_func(model_name) File \u201C/home/jeff/oobabooga_linux/text-generation-webui/modules/models.py\u201D\
    , line 225, in huggingface_loader model = LoaderClass.from_pretrained(checkpoint,\
    \ **params) File \u201C/home/jeff/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
    , line 466, in from_pretrained return model_class.from_pretrained( File \u201C\
    /home/jeff/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
    , line 2405, in from_pretrained raise EnvironmentError( OSError: Error no file\
    \ named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
    \ found in directory models/falcon-40b-instruct-GPTQ."
  created_at: 2023-05-31 13:20:56+00:00
  edited: false
  hidden: false
  id: 647757c8f911e9e76c619af2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T15:00:27.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Is it possible the name of the safetensors file is causing the issue?  Does
          it have to have .4bit at the end or something?</p>

          '
        raw: Is it possible the name of the safetensors file is causing the issue?  Does
          it have to have .4bit at the end or something?
        updatedAt: '2023-05-31T15:00:27.802Z'
      numEdits: 0
      reactions: []
    id: 6477610b04aa03da2abc59b7
    type: comment
  author: jdc4429
  content: Is it possible the name of the safetensors file is causing the issue?  Does
    it have to have .4bit at the end or something?
  created_at: 2023-05-31 14:00:27+00:00
  edited: false
  hidden: false
  id: 6477610b04aa03da2abc59b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T15:03:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This error means it''s still not loading it with AutoGPTQ</p>

          <p>If the checkbox doesn''t work, can you please launch server.py with <code>--autogptq
          --trust_remote_code</code> speciflcally.</p>

          <p>And if that doesn''t work, please show me a screenshot of the contents
          of the <code>TheBloke_falcon-40b-instruct-GPTQ</code> model folder</p>

          '
        raw: 'This error means it''s still not loading it with AutoGPTQ


          If the checkbox doesn''t work, can you please launch server.py with `--autogptq
          --trust_remote_code` speciflcally.


          And if that doesn''t work, please show me a screenshot of the contents of
          the `TheBloke_falcon-40b-instruct-GPTQ` model folder'
        updatedAt: '2023-05-31T15:03:16.073Z'
      numEdits: 0
      reactions: []
    id: 647761b4f911e9e76c629783
    type: comment
  author: TheBloke
  content: 'This error means it''s still not loading it with AutoGPTQ


    If the checkbox doesn''t work, can you please launch server.py with `--autogptq
    --trust_remote_code` speciflcally.


    And if that doesn''t work, please show me a screenshot of the contents of the
    `TheBloke_falcon-40b-instruct-GPTQ` model folder'
  created_at: 2023-05-31 14:03:16+00:00
  edited: false
  hidden: false
  id: 647761b4f911e9e76c629783
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T15:10:04.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Got an error stating module chardet not found.  Was not in requirements.  Did
          a pip install to fix that.<br>Then got an error, no module markdown. Installed
          via pip.</p>

          <p>Now I get another error:</p>

          <p>ImportError: cannot import name ''storage_ptr'' from ''safetensors.torch''</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644db80c16703fd6702b98ff/3dgUF6topB3lQhXeBeIPs.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644db80c16703fd6702b98ff/3dgUF6topB3lQhXeBeIPs.png"></a></p>

          '
        raw: 'Got an error stating module chardet not found.  Was not in requirements.  Did
          a pip install to fix that.

          Then got an error, no module markdown. Installed via pip.


          Now I get another error:


          ImportError: cannot import name ''storage_ptr'' from ''safetensors.torch''



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644db80c16703fd6702b98ff/3dgUF6topB3lQhXeBeIPs.png)'
        updatedAt: '2023-05-31T15:10:04.542Z'
      numEdits: 0
      reactions: []
    id: 6477634cbb7681ad67068b82
    type: comment
  author: jdc4429
  content: 'Got an error stating module chardet not found.  Was not in requirements.  Did
    a pip install to fix that.

    Then got an error, no module markdown. Installed via pip.


    Now I get another error:


    ImportError: cannot import name ''storage_ptr'' from ''safetensors.torch''



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644db80c16703fd6702b98ff/3dgUF6topB3lQhXeBeIPs.png)'
  created_at: 2023-05-31 14:10:04+00:00
  edited: false
  hidden: false
  id: 6477634cbb7681ad67068b82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T15:26:18.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>I added the auto_gptq setting to the model settings from the interface
          now.  I think the only issue is ooba not detecting the .safetensors file...<br>Which
          may be something to do with the filename..  </p>

          <p>But it''s like 10 errors before you get something working in Ubuntu it
          seems so maybe not the last issue with it. lol</p>

          <p>I have avoided downloading safetensor versions for just this reason.
          lol<br>Would it be possible for you to put the falcon 40b up in another
          format? :)</p>

          '
        raw: "I added the auto_gptq setting to the model settings from the interface\
          \ now.  I think the only issue is ooba not detecting the .safetensors file...\n\
          Which may be something to do with the filename..  \n\nBut it's like 10 errors\
          \ before you get something working in Ubuntu it seems so maybe not the last\
          \ issue with it. lol\n\nI have avoided downloading safetensor versions for\
          \ just this reason. lol\nWould it be possible for you to put the falcon\
          \ 40b up in another format? :)"
        updatedAt: '2023-05-31T15:26:18.011Z'
      numEdits: 0
      reactions: []
    id: 6477671a33a888101f76a4da
    type: comment
  author: jdc4429
  content: "I added the auto_gptq setting to the model settings from the interface\
    \ now.  I think the only issue is ooba not detecting the .safetensors file...\n\
    Which may be something to do with the filename..  \n\nBut it's like 10 errors\
    \ before you get something working in Ubuntu it seems so maybe not the last issue\
    \ with it. lol\n\nI have avoided downloading safetensor versions for just this\
    \ reason. lol\nWould it be possible for you to put the falcon 40b up in another\
    \ format? :)"
  created_at: 2023-05-31 14:26:18+00:00
  edited: false
  hidden: false
  id: 6477671a33a888101f76a4da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T15:34:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Your installation is messed up or incomplete somehow.  You shouldn''t
          be needing to manually install pip packages. </p>

          <p>Please go to the text-generation-webui directory and run: <code>pip install
          -r requirements.txt</code></p>

          <p>Then try again.</p>

          '
        raw: "Your installation is messed up or incomplete somehow.  You shouldn't\
          \ be needing to manually install pip packages. \n\nPlease go to the text-generation-webui\
          \ directory and run: `pip install -r requirements.txt`\n\nThen try again."
        updatedAt: '2023-05-31T15:34:31.927Z'
      numEdits: 0
      reactions: []
    id: 64776907bb7681ad6707184d
    type: comment
  author: TheBloke
  content: "Your installation is messed up or incomplete somehow.  You shouldn't be\
    \ needing to manually install pip packages. \n\nPlease go to the text-generation-webui\
    \ directory and run: `pip install -r requirements.txt`\n\nThen try again."
  created_at: 2023-05-31 14:34:31+00:00
  edited: false
  hidden: false
  id: 64776907bb7681ad6707184d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T15:37:45.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The issue might be that you''re using the text-gen-ui one-click-installer
          which I believe creates a Python conda environment. But you''re not now
          using that environment.</p>

          <p>If in doubt, please start again:</p>

          <pre><code>cd /some/folder

          git clone https://github.com/oobabooga/text-generation-webui

          cd text-generation-webui

          pip install -r requirements.txt

          cd ..

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          cd ../text-generation-webui

          mv ~/oobabooga_linux/text-generation-webui/models/TheBloke_falcon* models/

          python server.py --autogptq --trust-remote-code

          </code></pre>

          <p>Also: just checking you know that you won''t be able to load this model
          unless you have 2 x 24GB GPUs, or 1 x 48GB GPU?</p>

          '
        raw: 'The issue might be that you''re using the text-gen-ui one-click-installer
          which I believe creates a Python conda environment. But you''re not now
          using that environment.


          If in doubt, please start again:


          ```

          cd /some/folder

          git clone https://github.com/oobabooga/text-generation-webui

          cd text-generation-webui

          pip install -r requirements.txt

          cd ..

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          cd ../text-generation-webui

          mv ~/oobabooga_linux/text-generation-webui/models/TheBloke_falcon* models/

          python server.py --autogptq --trust-remote-code

          ```

          Also: just checking you know that you won''t be able to load this model
          unless you have 2 x 24GB GPUs, or 1 x 48GB GPU?'
        updatedAt: '2023-05-31T15:38:35.123Z'
      numEdits: 1
      reactions: []
    id: 647769c933a888101f76e5a8
    type: comment
  author: TheBloke
  content: 'The issue might be that you''re using the text-gen-ui one-click-installer
    which I believe creates a Python conda environment. But you''re not now using
    that environment.


    If in doubt, please start again:


    ```

    cd /some/folder

    git clone https://github.com/oobabooga/text-generation-webui

    cd text-generation-webui

    pip install -r requirements.txt

    cd ..

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip install .

    cd ../text-generation-webui

    mv ~/oobabooga_linux/text-generation-webui/models/TheBloke_falcon* models/

    python server.py --autogptq --trust-remote-code

    ```

    Also: just checking you know that you won''t be able to load this model unless
    you have 2 x 24GB GPUs, or 1 x 48GB GPU?'
  created_at: 2023-05-31 14:37:45+00:00
  edited: true
  hidden: false
  id: 647769c933a888101f76e5a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T16:06:13.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>I did run the requirements.. I just figure some things were not
          listed.<br>My other models work.  In fact I just loaded the Falcon 7B without
          issue. (Falcon 7B takes 14gb but running with 8GB GPU + CPU)<br>You don''t
          need all of it in VRAM.  But I have an RTX 2070 and a K80 (not installed)
          Waiting on a P40 ATM.  I have 72gb ram.  I can run it with 30gb VRAM and
          cpu if it''s over 30 for the rest.  4bit would not need that much.. Why
          I wanted to try even though it''s slow atm.  I plan on getting another p40
          which would get me to around 54gb VRAM.. but also need to upgrade the motherboard
          with 3x pci16 to hold 3 cards.</p>

          <p>I will try installing again in another directory just in case.  But my
          only issue now might just be it doesn''t recognize the .safetensor file
          to load it.</p>

          '
        raw: 'I did run the requirements.. I just figure some things were not listed.

          My other models work.  In fact I just loaded the Falcon 7B without issue.
          (Falcon 7B takes 14gb but running with 8GB GPU + CPU)

          You don''t need all of it in VRAM.  But I have an RTX 2070 and a K80 (not
          installed) Waiting on a P40 ATM.  I have 72gb ram.  I can run it with 30gb
          VRAM and cpu if it''s over 30 for the rest.  4bit would not need that much..
          Why I wanted to try even though it''s slow atm.  I plan on getting another
          p40 which would get me to around 54gb VRAM.. but also need to upgrade the
          motherboard with 3x pci16 to hold 3 cards.


          I will try installing again in another directory just in case.  But my only
          issue now might just be it doesn''t recognize the .safetensor file to load
          it.'
        updatedAt: '2023-05-31T16:06:13.596Z'
      numEdits: 0
      reactions: []
    id: 6477707504aa03da2abdd47e
    type: comment
  author: jdc4429
  content: 'I did run the requirements.. I just figure some things were not listed.

    My other models work.  In fact I just loaded the Falcon 7B without issue. (Falcon
    7B takes 14gb but running with 8GB GPU + CPU)

    You don''t need all of it in VRAM.  But I have an RTX 2070 and a K80 (not installed)
    Waiting on a P40 ATM.  I have 72gb ram.  I can run it with 30gb VRAM and cpu if
    it''s over 30 for the rest.  4bit would not need that much.. Why I wanted to try
    even though it''s slow atm.  I plan on getting another p40 which would get me
    to around 54gb VRAM.. but also need to upgrade the motherboard with 3x pci16 to
    hold 3 cards.


    I will try installing again in another directory just in case.  But my only issue
    now might just be it doesn''t recognize the .safetensor file to load it.'
  created_at: 2023-05-31 15:06:13+00:00
  edited: false
  hidden: false
  id: 6477707504aa03da2abdd47e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T16:08:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Everyting you need to run text-gen-ui is in its requirements.txt.  If
          you''re having problems it''s not because requirements weren''t installed.
          It''s because you installed them in a different envionrment to the one you''re
          using, or something like that</p>

          <p>If Falcon-7B worked then please try running Falcon 40B again, in exactly
          the same way, and show me everything you see on screen</p>

          '
        raw: 'Everyting you need to run text-gen-ui is in its requirements.txt.  If
          you''re having problems it''s not because requirements weren''t installed.
          It''s because you installed them in a different envionrment to the one you''re
          using, or something like that


          If Falcon-7B worked then please try running Falcon 40B again, in exactly
          the same way, and show me everything you see on screen'
        updatedAt: '2023-05-31T16:08:45.901Z'
      numEdits: 0
      reactions: []
    id: 6477710dbb7681ad6707d490
    type: comment
  author: TheBloke
  content: 'Everyting you need to run text-gen-ui is in its requirements.txt.  If
    you''re having problems it''s not because requirements weren''t installed. It''s
    because you installed them in a different envionrment to the one you''re using,
    or something like that


    If Falcon-7B worked then please try running Falcon 40B again, in exactly the same
    way, and show me everything you see on screen'
  created_at: 2023-05-31 15:08:45+00:00
  edited: false
  hidden: false
  id: 6477710dbb7681ad6707d490
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T16:24:35.000Z'
    data:
      edited: true
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: "<p>I tried from scratch as suggested, also compiled bitsandbytes from\
          \ scratch.  </p>\n<p>Get a new error..</p>\n<p>RuntimeError: Unexpected\
          \ error from hipGetDeviceCount(). Did you run some cuda<br>functions before\
          \ calling NumHipDevices() that might have already set an error? Error<br>101:\
          \ hipErrorInvalidDevice</p>\n<p>When running as instructed: python server.py\
          \ --autogptq --trust-remote-code</p>\n<p>Here is the whole Traceback:</p>\n\
          <p>Also got some warnings.. Not sure why.</p>\n<p>WARNING:The AutoGPTQ params\
          \ are: {'model_basename': 'gptq_model-4bit--1g', 'device': 'cuda:0', 'use_triton':\
          \ False, 'use_safetensors': True, 'trust_remote_code': True, 'max_memory':\
          \ None}<br>WARNING:CUDA extension not installed.<br>WARNING:The safetensors\
          \ archive passed at models/falcon-40b-instruct-GPTQ/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>\
          \ method. Defaulting to 'pt' metadata.</p>\n<p>\u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E<br>\u2502 /home/jeff/TEST/text-generation-webui/server.py:1094\
          \ in                   \u2502<br>\u2502                                \
          \                                                   \u2502<br>\u2502   1091\
          \ \u2502   \u2502   update_model_parameters(model_settings, initial=True)\
          \  # hijacking \u2502<br>\u2502   1092 \u2502   \u2502                 \
          \                                                     \u2502<br>\u2502 \
          \  1093 \u2502   \u2502   # Load the model                             \
          \                      \u2502<br>\u2502 \u2771 1094 \u2502   \u2502   shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)     \u2502<br>\u2502\
          \   1095 \u2502   \u2502   if shared.args.lora:                        \
          \                       \u2502<br>\u2502   1096 \u2502   \u2502   \u2502\
          \   add_lora_to_model(shared.args.lora)                            \u2502\
          <br>\u2502   1097                                                      \
          \                      \u2502<br>\u2502                                \
          \                                                   \u2502<br>\u2502 /home/jeff/TEST/text-generation-webui/modules/models.py:97\
          \ in load_model          \u2502<br>\u2502                              \
          \                                                     \u2502<br>\u2502 \
          \   94 \u2502   else:                                                  \
          \                 \u2502<br>\u2502    95 \u2502   \u2502   load_func = huggingface_loader\
          \                                      \u2502<br>\u2502    96 \u2502   \
          \                                                                      \
          \  \u2502<br>\u2502 \u2771  97 \u2502   output = load_func(model_name) \
          \                                         \u2502<br>\u2502    98 \u2502\
          \   if type(output) is tuple:                                          \
          \     \u2502<br>\u2502    99 \u2502   \u2502   model, tokenizer = output\
          \                                           \u2502<br>\u2502   100 \u2502\
          \   else:                                                              \
          \     \u2502<br>\u2502                                                 \
          \                                  \u2502<br>\u2502 /home/jeff/TEST/text-generation-webui/modules/models.py:299\
          \ in AutoGPTQ_loader    \u2502<br>\u2502                               \
          \                                                    \u2502<br>\u2502  \
          \ 296 def AutoGPTQ_loader(model_name):                                 \
          \           \u2502<br>\u2502   297 \u2502   import modules.AutoGPTQ_loader\
          \                                          \u2502<br>\u2502   298 \u2502\
          \                                                                      \
          \     \u2502<br>\u2502 \u2771 299 \u2502   return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \               \u2502<br>\u2502   300                                 \
          \                                            \u2502<br>\u2502   301    \
          \                                                                      \
          \   \u2502<br>\u2502   302 def get_max_memory_dict():                  \
          \                                \u2502<br>\u2502                      \
          \                                                             \u2502<br>\u2502\
          \ /home/jeff/TEST/text-generation-webui/modules/AutoGPTQ_loader.py:43 in\
          \            \u2502<br>\u2502 load_quantized                           \
          \                                         \u2502<br>\u2502             \
          \                                                                      \u2502\
          <br>\u2502   40 \u2502   }                                             \
          \                           \u2502<br>\u2502   41 \u2502               \
          \                                                             \u2502<br>\u2502\
          \   42 \u2502   logger.warning(f\"The AutoGPTQ params are: {params}\") \
          \                    \u2502<br>\u2502 \u2771 43 \u2502   model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)      \u2502<br>\u2502   44 \u2502   return model           \
          \                                                  \u2502<br>\u2502   45\
          \                                                                      \
          \        \u2502<br>\u2502                                              \
          \                                     \u2502<br>\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling\
          \ \u2502<br>\u2502 /auto.py:82 in from_quantized                       \
          \                              \u2502<br>\u2502                        \
          \                                                           \u2502<br>\u2502\
          \    79 \u2502   \u2502   model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_pat \u2502<br>\u2502    80 \u2502   \u2502   quant_func\
          \ = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized    \u2502<br>\u2502\
          \    81 \u2502   \u2502   keywords = {key: kwargs[key] for key in signature(quant_func).param\
          \ \u2502<br>\u2502 \u2771  82 \u2502   \u2502   return quant_func(     \
          \                                             \u2502<br>\u2502    83 \u2502\
          \   \u2502   \u2502   model_name_or_path=model_name_or_path,           \
          \               \u2502<br>\u2502    84 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                              \u2502<br>\u2502    85 \u2502\
          \   \u2502   \u2502   device_map=device_map,                           \
          \               \u2502<br>\u2502                                       \
          \                                            \u2502<br>\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling\
          \ \u2502<br>\u2502 /<em>base.py:773 in from_quantized                  \
          \                                 \u2502<br>\u2502                     \
          \                                                              \u2502<br>\u2502\
          \   770 \u2502   \u2502   if low_cpu_mem_usage:                        \
          \                       \u2502<br>\u2502   771 \u2502   \u2502   \u2502\
          \   make_sure_no_tensor_in_meta_device(model, use_triton, quantize</em>\
          \ \u2502<br>\u2502   772 \u2502   \u2502                               \
          \                                        \u2502<br>\u2502 \u2771 773 \u2502\
          \   \u2502   accelerate.utils.modeling.load_checkpoint_in_model(       \
          \          \u2502<br>\u2502   774 \u2502   \u2502   \u2502   model,    \
          \                                                      \u2502<br>\u2502\
          \   775 \u2502   \u2502   \u2502   checkpoint=model_save_name,         \
          \                            \u2502<br>\u2502   776 \u2502   \u2502   \u2502\
          \   device_map=device_map,                                          \u2502\
          <br>\u2502                                                             \
          \                      \u2502<br>\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/m\
          \ \u2502<br>\u2502 odeling.py:998 in load_checkpoint_in_model          \
          \                              \u2502<br>\u2502                        \
          \                                                           \u2502<br>\u2502\
          \    995 \u2502   buffer_names = [name for name, _ in model.named_buffers()]\
          \             \u2502<br>\u2502    996 \u2502                           \
          \                                               \u2502<br>\u2502    997\
          \ \u2502   for checkpoint_file in checkpoint_files:                    \
          \           \u2502<br>\u2502 \u2771  998 \u2502   \u2502   checkpoint =\
          \ load_state_dict(checkpoint_file, device_map=device_ma \u2502<br>\u2502\
          \    999 \u2502   \u2502   if device_map is None:                      \
          \                       \u2502<br>\u2502   1000 \u2502   \u2502   \u2502\
          \   model.load_state_dict(checkpoint, strict=False)                \u2502\
          <br>\u2502   1001 \u2502   \u2502   else:                              \
          \                                \u2502<br>\u2502                      \
          \                                                             \u2502<br>\u2502\
          \ /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/m\
          \ \u2502<br>\u2502 odeling.py:859 in load_state_dict                   \
          \                              \u2502<br>\u2502                        \
          \                                                           \u2502<br>\u2502\
          \    856 \u2502   \u2502   \u2502                                      \
          \                            \u2502<br>\u2502    857 \u2502   \u2502   \u2502\
          \   # if we only have one device we can load everything directly   \u2502\
          <br>\u2502    858 \u2502   \u2502   \u2502   if len(devices) == 1:     \
          \                                     \u2502<br>\u2502 \u2771  859 \u2502\
          \   \u2502   \u2502   \u2502   return safe_load_file(checkpoint_file, device=devices[0])\
          \  \u2502<br>\u2502    860 \u2502   \u2502   \u2502                    \
          \                                              \u2502<br>\u2502    861 \u2502\
          \   \u2502   \u2502   # cpu device should always exist as fallback option\
          \            \u2502<br>\u2502    862 \u2502   \u2502   \u2502   if \"cpu\"\
          \ not in devices:                                       \u2502<br>\u2502\
          \                                                                      \
          \             \u2502<br>\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/safetensors/torch.\
          \ \u2502<br>\u2502 py:261 in load_file                                 \
          \                              \u2502<br>\u2502                        \
          \                                                           \u2502<br>\u2502\
          \   258 \u2502   result = {}                                           \
          \                  \u2502<br>\u2502   259 \u2502   with safe_open(filename,\
          \ framework=\"pt\", device=device) as f:           \u2502<br>\u2502   260\
          \ \u2502   \u2502   for k in f.keys():                                 \
          \                 \u2502<br>\u2502 \u2771 261 \u2502   \u2502   \u2502 \
          \  result[k] = f.get_tensor(k)                                     \u2502\
          <br>\u2502   262 \u2502   return result                                \
          \                           \u2502<br>\u2502   263                     \
          \                                                        \u2502<br>\u2502\
          \   264                                                                \
          \             \u2502<br>\u2502                                         \
          \                                          \u2502<br>\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/cuda/_<em>init</em>\
          \ \u2502<br>\u2502 _.py:229 in _lazy_init                              \
          \                              \u2502<br>\u2502                        \
          \                                                           \u2502<br>\u2502\
          \   226 \u2502   \u2502   # are found or any other error occurs        \
          \                       \u2502<br>\u2502   227 \u2502   \u2502   if 'CUDA_MODULE_LOADING'\
          \ not in os.environ:                         \u2502<br>\u2502   228 \u2502\
          \   \u2502   \u2502   os.environ['CUDA_MODULE_LOADING'] = 'LAZY'       \
          \               \u2502<br>\u2502 \u2771 229 \u2502   \u2502   torch._C._cuda_init()\
          \                                               \u2502<br>\u2502   230 \u2502\
          \   \u2502   # Some of the queued calls may reentrantly call _lazy_init();\
          \       \u2502<br>\u2502   231 \u2502   \u2502   # we need to just return\
          \ without initializing in that case.         \u2502<br>\u2502   232 \u2502\
          \   \u2502   # However, we must not let any <em>other</em> threads in! \
          \                 \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256F<br>RuntimeError: Unexpected error from hipGetDeviceCount().\
          \ Did you run some cuda<br>functions before calling NumHipDevices() that\
          \ might have already set an error? Error<br>101: hipErrorInvalidDevice</p>\n"
        raw: "I tried from scratch as suggested, also compiled bitsandbytes from scratch.\
          \  \n\nGet a new error..\n\nRuntimeError: Unexpected error from hipGetDeviceCount().\
          \ Did you run some cuda \nfunctions before calling NumHipDevices() that\
          \ might have already set an error? Error \n101: hipErrorInvalidDevice\n\n\
          When running as instructed: python server.py --autogptq --trust-remote-code\n\
          \nHere is the whole Traceback:\n\nAlso got some warnings.. Not sure why.\n\
          \nWARNING:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit--1g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'use_safetensors': True, 'trust_remote_code':\
          \ True, 'max_memory': None}\nWARNING:CUDA extension not installed.\nWARNING:The\
          \ safetensors archive passed at models/falcon-40b-instruct-GPTQ/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n\n\n\u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502 /home/jeff/TEST/text-generation-webui/server.py:1094\
          \ in <module>                  \u2502\n\u2502                          \
          \                                                         \u2502\n\u2502\
          \   1091 \u2502   \u2502   update_model_parameters(model_settings, initial=True)\
          \  # hijacking \u2502\n\u2502   1092 \u2502   \u2502                   \
          \                                                   \u2502\n\u2502   1093\
          \ \u2502   \u2502   # Load the model                                   \
          \                \u2502\n\u2502 \u2771 1094 \u2502   \u2502   shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)     \u2502\n\u2502  \
          \ 1095 \u2502   \u2502   if shared.args.lora:                          \
          \                     \u2502\n\u2502   1096 \u2502   \u2502   \u2502   add_lora_to_model(shared.args.lora)\
          \                            \u2502\n\u2502   1097                     \
          \                                                       \u2502\n\u2502 \
          \                                                                      \
          \            \u2502\n\u2502 /home/jeff/TEST/text-generation-webui/modules/models.py:97\
          \ in load_model          \u2502\n\u2502                                \
          \                                                   \u2502\n\u2502    94\
          \ \u2502   else:                                                       \
          \            \u2502\n\u2502    95 \u2502   \u2502   load_func = huggingface_loader\
          \                                      \u2502\n\u2502    96 \u2502     \
          \                                                                      \u2502\
          \n\u2502 \u2771  97 \u2502   output = load_func(model_name)            \
          \                              \u2502\n\u2502    98 \u2502   if type(output)\
          \ is tuple:                                               \u2502\n\u2502\
          \    99 \u2502   \u2502   model, tokenizer = output                    \
          \                       \u2502\n\u2502   100 \u2502   else:            \
          \                                                       \u2502\n\u2502 \
          \                                                                      \
          \            \u2502\n\u2502 /home/jeff/TEST/text-generation-webui/modules/models.py:299\
          \ in AutoGPTQ_loader    \u2502\n\u2502                                 \
          \                                                  \u2502\n\u2502   296\
          \ def AutoGPTQ_loader(model_name):                                     \
          \       \u2502\n\u2502   297 \u2502   import modules.AutoGPTQ_loader   \
          \                                       \u2502\n\u2502   298 \u2502    \
          \                                                                      \
          \ \u2502\n\u2502 \u2771 299 \u2502   return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \               \u2502\n\u2502   300                                   \
          \                                          \u2502\n\u2502   301        \
          \                                                                     \u2502\
          \n\u2502   302 def get_max_memory_dict():                              \
          \                    \u2502\n\u2502                                    \
          \                                               \u2502\n\u2502 /home/jeff/TEST/text-generation-webui/modules/AutoGPTQ_loader.py:43\
          \ in            \u2502\n\u2502 load_quantized                          \
          \                                          \u2502\n\u2502              \
          \                                                                     \u2502\
          \n\u2502   40 \u2502   }                                               \
          \                         \u2502\n\u2502   41 \u2502                   \
          \                                                         \u2502\n\u2502\
          \   42 \u2502   logger.warning(f\"The AutoGPTQ params are: {params}\") \
          \                    \u2502\n\u2502 \u2771 43 \u2502   model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)      \u2502\n\u2502   44 \u2502   return model             \
          \                                                \u2502\n\u2502   45   \
          \                                                                      \
          \     \u2502\n\u2502                                                   \
          \                                \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling\
          \ \u2502\n\u2502 /auto.py:82 in from_quantized                         \
          \                            \u2502\n\u2502                            \
          \                                                       \u2502\n\u2502 \
          \   79 \u2502   \u2502   model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_pat \u2502\n\u2502    80 \u2502   \u2502   quant_func\
          \ = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized    \u2502\n\u2502\
          \    81 \u2502   \u2502   keywords = {key: kwargs[key] for key in signature(quant_func).param\
          \ \u2502\n\u2502 \u2771  82 \u2502   \u2502   return quant_func(       \
          \                                           \u2502\n\u2502    83 \u2502\
          \   \u2502   \u2502   model_name_or_path=model_name_or_path,           \
          \               \u2502\n\u2502    84 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                              \u2502\n\u2502    85 \u2502\
          \   \u2502   \u2502   device_map=device_map,                           \
          \               \u2502\n\u2502                                         \
          \                                          \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling\
          \ \u2502\n\u2502 /_base.py:773 in from_quantized                       \
          \                            \u2502\n\u2502                            \
          \                                                       \u2502\n\u2502 \
          \  770 \u2502   \u2502   if low_cpu_mem_usage:                         \
          \                      \u2502\n\u2502   771 \u2502   \u2502   \u2502   make_sure_no_tensor_in_meta_device(model,\
          \ use_triton, quantize_ \u2502\n\u2502   772 \u2502   \u2502           \
          \                                                            \u2502\n\u2502\
          \ \u2771 773 \u2502   \u2502   accelerate.utils.modeling.load_checkpoint_in_model(\
          \                 \u2502\n\u2502   774 \u2502   \u2502   \u2502   model,\
          \                                                          \u2502\n\u2502\
          \   775 \u2502   \u2502   \u2502   checkpoint=model_save_name,         \
          \                            \u2502\n\u2502   776 \u2502   \u2502   \u2502\
          \   device_map=device_map,                                          \u2502\
          \n\u2502                                                               \
          \                    \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/m\
          \ \u2502\n\u2502 odeling.py:998 in load_checkpoint_in_model            \
          \                            \u2502\n\u2502                            \
          \                                                       \u2502\n\u2502 \
          \   995 \u2502   buffer_names = [name for name, _ in model.named_buffers()]\
          \             \u2502\n\u2502    996 \u2502                             \
          \                                             \u2502\n\u2502    997 \u2502\
          \   for checkpoint_file in checkpoint_files:                           \
          \    \u2502\n\u2502 \u2771  998 \u2502   \u2502   checkpoint = load_state_dict(checkpoint_file,\
          \ device_map=device_ma \u2502\n\u2502    999 \u2502   \u2502   if device_map\
          \ is None:                                             \u2502\n\u2502  \
          \ 1000 \u2502   \u2502   \u2502   model.load_state_dict(checkpoint, strict=False)\
          \                \u2502\n\u2502   1001 \u2502   \u2502   else:         \
          \                                                     \u2502\n\u2502   \
          \                                                                      \
          \          \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/m\
          \ \u2502\n\u2502 odeling.py:859 in load_state_dict                     \
          \                            \u2502\n\u2502                            \
          \                                                       \u2502\n\u2502 \
          \   856 \u2502   \u2502   \u2502                                       \
          \                           \u2502\n\u2502    857 \u2502   \u2502   \u2502\
          \   # if we only have one device we can load everything directly   \u2502\
          \n\u2502    858 \u2502   \u2502   \u2502   if len(devices) == 1:       \
          \                                   \u2502\n\u2502 \u2771  859 \u2502  \
          \ \u2502   \u2502   \u2502   return safe_load_file(checkpoint_file, device=devices[0])\
          \  \u2502\n\u2502    860 \u2502   \u2502   \u2502                      \
          \                                            \u2502\n\u2502    861 \u2502\
          \   \u2502   \u2502   # cpu device should always exist as fallback option\
          \            \u2502\n\u2502    862 \u2502   \u2502   \u2502   if \"cpu\"\
          \ not in devices:                                       \u2502\n\u2502 \
          \                                                                      \
          \            \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/safetensors/torch.\
          \ \u2502\n\u2502 py:261 in load_file                                   \
          \                            \u2502\n\u2502                            \
          \                                                       \u2502\n\u2502 \
          \  258 \u2502   result = {}                                            \
          \                 \u2502\n\u2502   259 \u2502   with safe_open(filename,\
          \ framework=\"pt\", device=device) as f:           \u2502\n\u2502   260\
          \ \u2502   \u2502   for k in f.keys():                                 \
          \                 \u2502\n\u2502 \u2771 261 \u2502   \u2502   \u2502   result[k]\
          \ = f.get_tensor(k)                                     \u2502\n\u2502 \
          \  262 \u2502   return result                                          \
          \                 \u2502\n\u2502   263                                 \
          \                                            \u2502\n\u2502   264      \
          \                                                                      \
          \ \u2502\n\u2502                                                       \
          \                            \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/cuda/__init_\
          \ \u2502\n\u2502 _.py:229 in _lazy_init                                \
          \                            \u2502\n\u2502                            \
          \                                                       \u2502\n\u2502 \
          \  226 \u2502   \u2502   # are found or any other error occurs         \
          \                      \u2502\n\u2502   227 \u2502   \u2502   if 'CUDA_MODULE_LOADING'\
          \ not in os.environ:                         \u2502\n\u2502   228 \u2502\
          \   \u2502   \u2502   os.environ['CUDA_MODULE_LOADING'] = 'LAZY'       \
          \               \u2502\n\u2502 \u2771 229 \u2502   \u2502   torch._C._cuda_init()\
          \                                               \u2502\n\u2502   230 \u2502\
          \   \u2502   # Some of the queued calls may reentrantly call _lazy_init();\
          \       \u2502\n\u2502   231 \u2502   \u2502   # we need to just return\
          \ without initializing in that case.         \u2502\n\u2502   232 \u2502\
          \   \u2502   # However, we must not let any *other* threads in!        \
          \          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F\nRuntimeError: Unexpected error from hipGetDeviceCount().\
          \ Did you run some cuda \nfunctions before calling NumHipDevices() that\
          \ might have already set an error? Error \n101: hipErrorInvalidDevice"
        updatedAt: '2023-05-31T16:25:30.127Z'
      numEdits: 1
      reactions: []
    id: 647774c3bb7681ad67083502
    type: comment
  author: jdc4429
  content: "I tried from scratch as suggested, also compiled bitsandbytes from scratch.\
    \  \n\nGet a new error..\n\nRuntimeError: Unexpected error from hipGetDeviceCount().\
    \ Did you run some cuda \nfunctions before calling NumHipDevices() that might\
    \ have already set an error? Error \n101: hipErrorInvalidDevice\n\nWhen running\
    \ as instructed: python server.py --autogptq --trust-remote-code\n\nHere is the\
    \ whole Traceback:\n\nAlso got some warnings.. Not sure why.\n\nWARNING:The AutoGPTQ\
    \ params are: {'model_basename': 'gptq_model-4bit--1g', 'device': 'cuda:0', 'use_triton':\
    \ False, 'use_safetensors': True, 'trust_remote_code': True, 'max_memory': None}\n\
    WARNING:CUDA extension not installed.\nWARNING:The safetensors archive passed\
    \ at models/falcon-40b-instruct-GPTQ/gptq_model-4bit--1g.safetensors does not\
    \ contain metadata. Make sure to save your model with the `save_pretrained` method.\
    \ Defaulting to 'pt' metadata.\n\n\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 /home/jeff/TEST/text-generation-webui/server.py:1094\
    \ in <module>                  \u2502\n\u2502                                \
    \                                                   \u2502\n\u2502   1091 \u2502\
    \   \u2502   update_model_parameters(model_settings, initial=True)  # hijacking\
    \ \u2502\n\u2502   1092 \u2502   \u2502                                      \
    \                                \u2502\n\u2502   1093 \u2502   \u2502   # Load\
    \ the model                                                   \u2502\n\u2502 \u2771\
    \ 1094 \u2502   \u2502   shared.model, shared.tokenizer = load_model(shared.model_name)\
    \     \u2502\n\u2502   1095 \u2502   \u2502   if shared.args.lora:           \
    \                                    \u2502\n\u2502   1096 \u2502   \u2502   \u2502\
    \   add_lora_to_model(shared.args.lora)                            \u2502\n\u2502\
    \   1097                                                                     \
    \       \u2502\n\u2502                                                       \
    \                            \u2502\n\u2502 /home/jeff/TEST/text-generation-webui/modules/models.py:97\
    \ in load_model          \u2502\n\u2502                                      \
    \                                             \u2502\n\u2502    94 \u2502   else:\
    \                                                                   \u2502\n\u2502\
    \    95 \u2502   \u2502   load_func = huggingface_loader                     \
    \                 \u2502\n\u2502    96 \u2502                                \
    \                                           \u2502\n\u2502 \u2771  97 \u2502 \
    \  output = load_func(model_name)                                          \u2502\
    \n\u2502    98 \u2502   if type(output) is tuple:                            \
    \                   \u2502\n\u2502    99 \u2502   \u2502   model, tokenizer =\
    \ output                                           \u2502\n\u2502   100 \u2502\
    \   else:                                                                   \u2502\
    \n\u2502                                                                     \
    \              \u2502\n\u2502 /home/jeff/TEST/text-generation-webui/modules/models.py:299\
    \ in AutoGPTQ_loader    \u2502\n\u2502                                       \
    \                                            \u2502\n\u2502   296 def AutoGPTQ_loader(model_name):\
    \                                            \u2502\n\u2502   297 \u2502   import\
    \ modules.AutoGPTQ_loader                                          \u2502\n\u2502\
    \   298 \u2502                                                               \
    \            \u2502\n\u2502 \u2771 299 \u2502   return modules.AutoGPTQ_loader.load_quantized(model_name)\
    \               \u2502\n\u2502   300                                         \
    \                                    \u2502\n\u2502   301                    \
    \                                                         \u2502\n\u2502   302\
    \ def get_max_memory_dict():                                                 \
    \ \u2502\n\u2502                                                             \
    \                      \u2502\n\u2502 /home/jeff/TEST/text-generation-webui/modules/AutoGPTQ_loader.py:43\
    \ in            \u2502\n\u2502 load_quantized                                \
    \                                    \u2502\n\u2502                          \
    \                                                         \u2502\n\u2502   40\
    \ \u2502   }                                                                 \
    \       \u2502\n\u2502   41 \u2502                                           \
    \                                 \u2502\n\u2502   42 \u2502   logger.warning(f\"\
    The AutoGPTQ params are: {params}\")                     \u2502\n\u2502 \u2771\
    \ 43 \u2502   model = AutoGPTQForCausalLM.from_quantized(path_to_model, **params)\
    \      \u2502\n\u2502   44 \u2502   return model                             \
    \                                \u2502\n\u2502   45                         \
    \                                                     \u2502\n\u2502         \
    \                                                                          \u2502\
    \n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling\
    \ \u2502\n\u2502 /auto.py:82 in from_quantized                               \
    \                      \u2502\n\u2502                                        \
    \                                           \u2502\n\u2502    79 \u2502   \u2502\
    \   model_type = check_and_get_model_type(save_dir or model_name_or_pat \u2502\
    \n\u2502    80 \u2502   \u2502   quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\
    \    \u2502\n\u2502    81 \u2502   \u2502   keywords = {key: kwargs[key] for key\
    \ in signature(quant_func).param \u2502\n\u2502 \u2771  82 \u2502   \u2502   return\
    \ quant_func(                                                  \u2502\n\u2502\
    \    83 \u2502   \u2502   \u2502   model_name_or_path=model_name_or_path,    \
    \                      \u2502\n\u2502    84 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                              \u2502\n\u2502    85 \u2502   \u2502\
    \   \u2502   device_map=device_map,                                          \u2502\
    \n\u2502                                                                     \
    \              \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling\
    \ \u2502\n\u2502 /_base.py:773 in from_quantized                             \
    \                      \u2502\n\u2502                                        \
    \                                           \u2502\n\u2502   770 \u2502   \u2502\
    \   if low_cpu_mem_usage:                                               \u2502\
    \n\u2502   771 \u2502   \u2502   \u2502   make_sure_no_tensor_in_meta_device(model,\
    \ use_triton, quantize_ \u2502\n\u2502   772 \u2502   \u2502                 \
    \                                                      \u2502\n\u2502 \u2771 773\
    \ \u2502   \u2502   accelerate.utils.modeling.load_checkpoint_in_model(      \
    \           \u2502\n\u2502   774 \u2502   \u2502   \u2502   model,           \
    \                                               \u2502\n\u2502   775 \u2502  \
    \ \u2502   \u2502   checkpoint=model_save_name,                              \
    \       \u2502\n\u2502   776 \u2502   \u2502   \u2502   device_map=device_map,\
    \                                          \u2502\n\u2502                    \
    \                                                               \u2502\n\u2502\
    \ /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/m\
    \ \u2502\n\u2502 odeling.py:998 in load_checkpoint_in_model                  \
    \                      \u2502\n\u2502                                        \
    \                                           \u2502\n\u2502    995 \u2502   buffer_names\
    \ = [name for name, _ in model.named_buffers()]             \u2502\n\u2502   \
    \ 996 \u2502                                                                 \
    \         \u2502\n\u2502    997 \u2502   for checkpoint_file in checkpoint_files:\
    \                               \u2502\n\u2502 \u2771  998 \u2502   \u2502   checkpoint\
    \ = load_state_dict(checkpoint_file, device_map=device_ma \u2502\n\u2502    999\
    \ \u2502   \u2502   if device_map is None:                                   \
    \          \u2502\n\u2502   1000 \u2502   \u2502   \u2502   model.load_state_dict(checkpoint,\
    \ strict=False)                \u2502\n\u2502   1001 \u2502   \u2502   else: \
    \                                                             \u2502\n\u2502 \
    \                                                                            \
    \      \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/m\
    \ \u2502\n\u2502 odeling.py:859 in load_state_dict                           \
    \                      \u2502\n\u2502                                        \
    \                                           \u2502\n\u2502    856 \u2502   \u2502\
    \   \u2502                                                                  \u2502\
    \n\u2502    857 \u2502   \u2502   \u2502   # if we only have one device we can\
    \ load everything directly   \u2502\n\u2502    858 \u2502   \u2502   \u2502  \
    \ if len(devices) == 1:                                          \u2502\n\u2502\
    \ \u2771  859 \u2502   \u2502   \u2502   \u2502   return safe_load_file(checkpoint_file,\
    \ device=devices[0])  \u2502\n\u2502    860 \u2502   \u2502   \u2502         \
    \                                                         \u2502\n\u2502    861\
    \ \u2502   \u2502   \u2502   # cpu device should always exist as fallback option\
    \            \u2502\n\u2502    862 \u2502   \u2502   \u2502   if \"cpu\" not in\
    \ devices:                                       \u2502\n\u2502              \
    \                                                                     \u2502\n\
    \u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/safetensors/torch.\
    \ \u2502\n\u2502 py:261 in load_file                                         \
    \                      \u2502\n\u2502                                        \
    \                                           \u2502\n\u2502   258 \u2502   result\
    \ = {}                                                             \u2502\n\u2502\
    \   259 \u2502   with safe_open(filename, framework=\"pt\", device=device) as\
    \ f:           \u2502\n\u2502   260 \u2502   \u2502   for k in f.keys():     \
    \                                             \u2502\n\u2502 \u2771 261 \u2502\
    \   \u2502   \u2502   result[k] = f.get_tensor(k)                            \
    \         \u2502\n\u2502   262 \u2502   return result                        \
    \                                   \u2502\n\u2502   263                     \
    \                                                        \u2502\n\u2502   264\
    \                                                                            \
    \ \u2502\n\u2502                                                             \
    \                      \u2502\n\u2502 /home/jeff/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/cuda/__init_\
    \ \u2502\n\u2502 _.py:229 in _lazy_init                                      \
    \                      \u2502\n\u2502                                        \
    \                                           \u2502\n\u2502   226 \u2502   \u2502\
    \   # are found or any other error occurs                               \u2502\
    \n\u2502   227 \u2502   \u2502   if 'CUDA_MODULE_LOADING' not in os.environ: \
    \                        \u2502\n\u2502   228 \u2502   \u2502   \u2502   os.environ['CUDA_MODULE_LOADING']\
    \ = 'LAZY'                      \u2502\n\u2502 \u2771 229 \u2502   \u2502   torch._C._cuda_init()\
    \                                               \u2502\n\u2502   230 \u2502  \
    \ \u2502   # Some of the queued calls may reentrantly call _lazy_init();     \
    \  \u2502\n\u2502   231 \u2502   \u2502   # we need to just return without initializing\
    \ in that case.         \u2502\n\u2502   232 \u2502   \u2502   # However, we must\
    \ not let any *other* threads in!                  \u2502\n\u2570\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u256F\nRuntimeError: Unexpected error from hipGetDeviceCount(). Did\
    \ you run some cuda \nfunctions before calling NumHipDevices() that might have\
    \ already set an error? Error \n101: hipErrorInvalidDevice"
  created_at: 2023-05-31 15:24:35+00:00
  edited: true
  hidden: false
  id: 647774c3bb7681ad67083502
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T16:59:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Wow your install is really messed up somehow.</p>

          <p>I really don''t know what you''ve done to get into this position. I think
          <code>hip</code> is normally used with AMD GPUs.</p>

          <p>I suggest making a new conda environment and starting from scratch:</p>

          <ol>

          <li>Make new conda and activate it</li>

          <li>Install the appropriate CUDA for your CUDA toolkit version.  Hopefully
          you have CUDA Toolkit 11.x installed, in which case you can do:</li>

          </ol>

          <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118

          </code></pre>

          <ol start="3">

          <li>Then run <code>python install -r requirements.txt</code> in text-generation-webui
          again</li>

          <li>And run <code>pip install .</code> in AutoGPTQ again.</li>

          </ol>

          <p>Do not compile bitsandbytes from scratch.  text-gen-ui installs the latest
          version from pip automatically.</p>

          '
        raw: 'Wow your install is really messed up somehow.


          I really don''t know what you''ve done to get into this position. I think
          `hip` is normally used with AMD GPUs.


          I suggest making a new conda environment and starting from scratch:

          1. Make new conda and activate it

          2. Install the appropriate CUDA for your CUDA toolkit version.  Hopefully
          you have CUDA Toolkit 11.x installed, in which case you can do:

          ```

          pip install torch --index-url https://download.pytorch.org/whl/cu118

          ```

          3. Then run `python install -r requirements.txt` in text-generation-webui
          again

          4. And run `pip install .` in AutoGPTQ again.


          Do not compile bitsandbytes from scratch.  text-gen-ui installs the latest
          version from pip automatically.'
        updatedAt: '2023-05-31T16:59:44.328Z'
      numEdits: 0
      reactions: []
    id: 64777d0033a888101f78b1f6
    type: comment
  author: TheBloke
  content: 'Wow your install is really messed up somehow.


    I really don''t know what you''ve done to get into this position. I think `hip`
    is normally used with AMD GPUs.


    I suggest making a new conda environment and starting from scratch:

    1. Make new conda and activate it

    2. Install the appropriate CUDA for your CUDA toolkit version.  Hopefully you
    have CUDA Toolkit 11.x installed, in which case you can do:

    ```

    pip install torch --index-url https://download.pytorch.org/whl/cu118

    ```

    3. Then run `python install -r requirements.txt` in text-generation-webui again

    4. And run `pip install .` in AutoGPTQ again.


    Do not compile bitsandbytes from scratch.  text-gen-ui installs the latest version
    from pip automatically.'
  created_at: 2023-05-31 15:59:44+00:00
  edited: false
  hidden: false
  id: 64777d0033a888101f78b1f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T20:18:33.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>This was all from a new environment..</p>

          <p>It''s just typical Linux.. fix one thing, break 10 others</p>

          <p>I started in Linux when it was Slackware. It''s been like 40 years and
          still the same BS.</p>

          <p>I was on 11.7 .. I am trying to get to 11.8.. but the stupid thing never
          works.  I have everything installed but nvidia-smi states incompatible version.  But
          I made utilities-520 and driver 520 and installed cuda toolkit 11.8 !!!  It''s
          so annoying.  So then I try mixing.. and trying 515, 510.. But of course
          they don''t work. </p>

          <p>So now do I try 12.1? because I pretty much figure like everything it
          will screw up pretty much everything that used to work in 11.7</p>

          '
        raw: "This was all from a new environment..\n\nIt's just typical Linux.. fix\
          \ one thing, break 10 others\n\nI started in Linux when it was Slackware.\
          \ It's been like 40 years and still the same BS.\n\nI was on 11.7 .. I am\
          \ trying to get to 11.8.. but the stupid thing never works.  I have everything\
          \ installed but nvidia-smi states incompatible version.  But I made utilities-520\
          \ and driver 520 and installed cuda toolkit 11.8 !!!  It's so annoying.\
          \  So then I try mixing.. and trying 515, 510.. But of course they don't\
          \ work. \n\nSo now do I try 12.1? because I pretty much figure like everything\
          \ it will screw up pretty much everything that used to work in 11.7"
        updatedAt: '2023-05-31T20:18:33.677Z'
      numEdits: 0
      reactions: []
    id: 6477ab99f911e9e76c693196
    type: comment
  author: jdc4429
  content: "This was all from a new environment..\n\nIt's just typical Linux.. fix\
    \ one thing, break 10 others\n\nI started in Linux when it was Slackware. It's\
    \ been like 40 years and still the same BS.\n\nI was on 11.7 .. I am trying to\
    \ get to 11.8.. but the stupid thing never works.  I have everything installed\
    \ but nvidia-smi states incompatible version.  But I made utilities-520 and driver\
    \ 520 and installed cuda toolkit 11.8 !!!  It's so annoying.  So then I try mixing..\
    \ and trying 515, 510.. But of course they don't work. \n\nSo now do I try 12.1?\
    \ because I pretty much figure like everything it will screw up pretty much everything\
    \ that used to work in 11.7"
  created_at: 2023-05-31 19:18:33+00:00
  edited: false
  hidden: false
  id: 6477ab99f911e9e76c693196
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T20:24:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I wouldn''t try 12.1, I''ve heard of weird performance bugs.  If
          you want to go 12.x, I''d try 12.0.1.  It''s what I''m using</p>

          <p>Though be aware you will then have to compile pytorch from source as
          there''s no pre-compiled binaries for 12.x yet.  So that''s extra work and
          takes a little while -around an hour usually.</p>

          <p>Or just go back to 11.7 if that worked OK for you</p>

          '
        raw: 'I wouldn''t try 12.1, I''ve heard of weird performance bugs.  If you
          want to go 12.x, I''d try 12.0.1.  It''s what I''m using


          Though be aware you will then have to compile pytorch from source as there''s
          no pre-compiled binaries for 12.x yet.  So that''s extra work and takes
          a little while -around an hour usually.


          Or just go back to 11.7 if that worked OK for you'
        updatedAt: '2023-05-31T20:24:15.609Z'
      numEdits: 0
      reactions: []
    id: 6477acefbb7681ad670d21b5
    type: comment
  author: TheBloke
  content: 'I wouldn''t try 12.1, I''ve heard of weird performance bugs.  If you want
    to go 12.x, I''d try 12.0.1.  It''s what I''m using


    Though be aware you will then have to compile pytorch from source as there''s
    no pre-compiled binaries for 12.x yet.  So that''s extra work and takes a little
    while -around an hour usually.


    Or just go back to 11.7 if that worked OK for you'
  created_at: 2023-05-31 19:24:15+00:00
  edited: false
  hidden: false
  id: 6477acefbb7681ad670d21b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T20:36:41.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>I''m only seeing: wget <a rel="nofollow" href="https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux.run">https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux.run</a></p>

          <p>Where did you get 12.0.1?</p>

          '
        raw: 'I''m only seeing: wget https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux.run


          Where did you get 12.0.1?'
        updatedAt: '2023-05-31T20:36:41.167Z'
      numEdits: 0
      reactions: []
    id: 6477afd9bb7681ad670d5b4c
    type: comment
  author: jdc4429
  content: 'I''m only seeing: wget https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux.run


    Where did you get 12.0.1?'
  created_at: 2023-05-31 19:36:41+00:00
  edited: false
  hidden: false
  id: 6477afd9bb7681ad670d5b4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T20:44:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p><a rel="nofollow" href="https://developer.nvidia.com/cuda-12-0-1-download-archive?target_os=Linux&amp;target_arch=x86_64">https://developer.nvidia.com/cuda-12-0-1-download-archive?target_os=Linux&amp;target_arch=x86_64</a></p>

          '
        raw: https://developer.nvidia.com/cuda-12-0-1-download-archive?target_os=Linux&target_arch=x86_64
        updatedAt: '2023-05-31T20:44:19.795Z'
      numEdits: 0
      reactions: []
    id: 6477b1a3bb7681ad670d7c6d
    type: comment
  author: TheBloke
  content: https://developer.nvidia.com/cuda-12-0-1-download-archive?target_os=Linux&target_arch=x86_64
  created_at: 2023-05-31 19:44:19+00:00
  edited: false
  hidden: false
  id: 6477b1a3bb7681ad670d7c6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T21:37:29.000Z'
    data:
      edited: true
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Thanks! I tried 12.0.0 and my RTX 2070 does not get detected. ARG..
          I am back at 11.7 but will try the 12.0.1...  RTX 2070 can''t be that outdated
          already right?</p>

          <p>I swear I searched for 12.0.1 (and 12.0 update 1) and it kept showing
          me 12.1!</p>

          '
        raw: 'Thanks! I tried 12.0.0 and my RTX 2070 does not get detected. ARG..
          I am back at 11.7 but will try the 12.0.1...  RTX 2070 can''t be that outdated
          already right?


          I swear I searched for 12.0.1 (and 12.0 update 1) and it kept showing me
          12.1!'
        updatedAt: '2023-05-31T21:38:48.338Z'
      numEdits: 1
      reactions: []
    id: 6477be1933a888101f7e310b
    type: comment
  author: jdc4429
  content: 'Thanks! I tried 12.0.0 and my RTX 2070 does not get detected. ARG.. I
    am back at 11.7 but will try the 12.0.1...  RTX 2070 can''t be that outdated already
    right?


    I swear I searched for 12.0.1 (and 12.0 update 1) and it kept showing me 12.1!'
  created_at: 2023-05-31 20:37:29+00:00
  edited: true
  hidden: false
  id: 6477be1933a888101f7e310b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-05-31T23:26:04.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>It seems none of the drivers past 11.7 want to work with my RTX
          2070 or else the module is not compatible with my kernel version?<br>Every
          driver past 515 gives me an error on boot that it can''t find the nvidia
          card. Don''t have the exact error.  But it happened on all 12.x versions.
          (tried 12.0,12.0.1,12.1.1)<br>I guess I''m stuck on 11.7<br>I may have the
          model working now though.. But I need P40 before I can test.. It gives out
          of memory trying on 8gb :) Does not allow splitting with CPU I guess because
          of AutoGPTQ</p>

          '
        raw: 'It seems none of the drivers past 11.7 want to work with my RTX 2070
          or else the module is not compatible with my kernel version?

          Every driver past 515 gives me an error on boot that it can''t find the
          nvidia card. Don''t have the exact error.  But it happened on all 12.x versions.
          (tried 12.0,12.0.1,12.1.1)

          I guess I''m stuck on 11.7

          I may have the model working now though.. But I need P40 before I can test..
          It gives out of memory trying on 8gb :) Does not allow splitting with CPU
          I guess because of AutoGPTQ'
        updatedAt: '2023-05-31T23:26:04.287Z'
      numEdits: 0
      reactions: []
    id: 6477d78cf911e9e76c6c63aa
    type: comment
  author: jdc4429
  content: 'It seems none of the drivers past 11.7 want to work with my RTX 2070 or
    else the module is not compatible with my kernel version?

    Every driver past 515 gives me an error on boot that it can''t find the nvidia
    card. Don''t have the exact error.  But it happened on all 12.x versions. (tried
    12.0,12.0.1,12.1.1)

    I guess I''m stuck on 11.7

    I may have the model working now though.. But I need P40 before I can test.. It
    gives out of memory trying on 8gb :) Does not allow splitting with CPU I guess
    because of AutoGPTQ'
  created_at: 2023-05-31 22:26:04+00:00
  edited: false
  hidden: false
  id: 6477d78cf911e9e76c6c63aa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Error when attempting to run.. Appears model files are missing or configuration
  issue
