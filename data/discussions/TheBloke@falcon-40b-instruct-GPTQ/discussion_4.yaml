!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mike-ravkine
conflicting_files: null
created_at: 2023-05-31 00:49:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-05-31T01:49:41.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<p>Hi TheBlocke!</p>\n<p>I'm trying the python code from the model\
          \ card with latest AutoGPTQ on an A100-40G, the model loads but I get a\
          \ failure during inference:</p>\n<pre><code>Loading tokenizer...\nLoading\
          \ model...\nExplicitly passing a `revision` is encouraged when loading a\
          \ model with custom code to ensure no malicious code has been contributed\
          \ in a newer revision.\nThe safetensors archive passed at /model/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\ncan't get model's sequence length\
          \ from model config, will set to 4096.\nRWGPTQForCausalLM hasn't fused attention\
          \ module yet, will skip inject fused attention.\nRWGPTQForCausalLM hasn't\
          \ fused mlp module yet, will skip inject fused mlp.\nModel loaded in 196.37s\n\
          The attention mask and the pad token id were not set. As a consequence,\
          \ you may observe unexpected behavior. Please pass your input's `attention_mask`\
          \ to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2\
          \ for open-end generation.\nTraceback (most recent call last):\n  File \"\
          /pkg/modal/_container_entrypoint.py\", line 330, in handle_input_exception\n\
          \    yield\n  File \"/pkg/modal/_container_entrypoint.py\", line 403, in\
          \ call_function_sync\n    res = fun(*args, **kwargs)\n  File \"/root/gptqfalcon.py\"\
          , line 72, in generate\n    output = self.model.generate(input_ids=tokens,\
          \ max_new_tokens=100, do_sample=True, temperature=0.8)\n  File \"/repositories/AutoGPTQ/auto_gptq/modeling/_base.py\"\
          , line 426, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\n    return self.sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\n    outputs = self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 759, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 654, in forward\n    outputs = block(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 396, in forward\n    attn_outputs = self.self_attention(\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/repositories/AutoGPTQ/auto_gptq/nn_modules/qlinear_old.py\", line\
          \ 189, in forward\n    autogptq_cuda.vecquant4matmul_faster_old(x, self.qweight,\
          \ out, self.scales.float(), self.qzeros, self.group_size, self.half_indim)\n\
          AttributeError: module 'autogptq_cuda' has no attribute 'vecquant4matmul_faster_old'\n\
          </code></pre>\n<p>Any specific version of AutoGPTQ that's known to be compatible\
          \ with this model?  I really want to give it a try!</p>\n"
        raw: "Hi TheBlocke!\r\n\r\nI'm trying the python code from the model card\
          \ with latest AutoGPTQ on an A100-40G, the model loads but I get a failure\
          \ during inference:\r\n\r\n```\r\nLoading tokenizer...\r\nLoading model...\r\
          \nExplicitly passing a `revision` is encouraged when loading a model with\
          \ custom code to ensure no malicious code has been contributed in a newer\
          \ revision.\r\nThe safetensors archive passed at /model/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\r\ncan't get model's sequence length\
          \ from model config, will set to 4096.\r\nRWGPTQForCausalLM hasn't fused\
          \ attention module yet, will skip inject fused attention.\r\nRWGPTQForCausalLM\
          \ hasn't fused mlp module yet, will skip inject fused mlp.\r\nModel loaded\
          \ in 196.37s\r\nThe attention mask and the pad token id were not set. As\
          \ a consequence, you may observe unexpected behavior. Please pass your input's\
          \ `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id`\
          \ to `eos_token_id`:2 for open-end generation.\r\nTraceback (most recent\
          \ call last):\r\n  File \"/pkg/modal/_container_entrypoint.py\", line 330,\
          \ in handle_input_exception\r\n    yield\r\n  File \"/pkg/modal/_container_entrypoint.py\"\
          , line 403, in call_function_sync\r\n    res = fun(*args, **kwargs)\r\n\
          \  File \"/root/gptqfalcon.py\", line 72, in generate\r\n    output = self.model.generate(input_ids=tokens,\
          \ max_new_tokens=100, do_sample=True, temperature=0.8)\r\n  File \"/repositories/AutoGPTQ/auto_gptq/modeling/_base.py\"\
          , line 426, in generate\r\n    return self.model.generate(**kwargs)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 759, in forward\r\n    transformer_outputs = self.transformer(\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 654, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 396, in forward\r\n    attn_outputs = self.self_attention(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
          , line 252, in forward\r\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/repositories/AutoGPTQ/auto_gptq/nn_modules/qlinear_old.py\"\
          , line 189, in forward\r\n    autogptq_cuda.vecquant4matmul_faster_old(x,\
          \ self.qweight, out, self.scales.float(), self.qzeros, self.group_size,\
          \ self.half_indim)\r\nAttributeError: module 'autogptq_cuda' has no attribute\
          \ 'vecquant4matmul_faster_old'\r\n```\r\n\r\nAny specific version of AutoGPTQ\
          \ that's known to be compatible with this model?  I really want to give\
          \ it a try!"
        updatedAt: '2023-05-31T01:49:41.853Z'
      numEdits: 0
      reactions: []
    id: 6476a7b53d8d1cb79190d4b5
    type: comment
  author: mike-ravkine
  content: "Hi TheBlocke!\r\n\r\nI'm trying the python code from the model card with\
    \ latest AutoGPTQ on an A100-40G, the model loads but I get a failure during inference:\r\
    \n\r\n```\r\nLoading tokenizer...\r\nLoading model...\r\nExplicitly passing a\
    \ `revision` is encouraged when loading a model with custom code to ensure no\
    \ malicious code has been contributed in a newer revision.\r\nThe safetensors\
    \ archive passed at /model/gptq_model-4bit--1g.safetensors does not contain metadata.\
    \ Make sure to save your model with the `save_pretrained` method. Defaulting to\
    \ 'pt' metadata.\r\ncan't get model's sequence length from model config, will\
    \ set to 4096.\r\nRWGPTQForCausalLM hasn't fused attention module yet, will skip\
    \ inject fused attention.\r\nRWGPTQForCausalLM hasn't fused mlp module yet, will\
    \ skip inject fused mlp.\r\nModel loaded in 196.37s\r\nThe attention mask and\
    \ the pad token id were not set. As a consequence, you may observe unexpected\
    \ behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\
    \nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\nTraceback\
    \ (most recent call last):\r\n  File \"/pkg/modal/_container_entrypoint.py\",\
    \ line 330, in handle_input_exception\r\n    yield\r\n  File \"/pkg/modal/_container_entrypoint.py\"\
    , line 403, in call_function_sync\r\n    res = fun(*args, **kwargs)\r\n  File\
    \ \"/root/gptqfalcon.py\", line 72, in generate\r\n    output = self.model.generate(input_ids=tokens,\
    \ max_new_tokens=100, do_sample=True, temperature=0.8)\r\n  File \"/repositories/AutoGPTQ/auto_gptq/modeling/_base.py\"\
    , line 426, in generate\r\n    return self.model.generate(**kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line\
    \ 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 1565, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 2612, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
    , line 759, in forward\r\n    transformer_outputs = self.transformer(\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    /root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
    , line 654, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
    , line 396, in forward\r\n    attn_outputs = self.self_attention(\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501,\
    \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/model/modelling_RW.py\"\
    , line 252, in forward\r\n    fused_qkv = self.query_key_value(hidden_states)\
    \  # [batch_size, seq_length, 3 x hidden_size]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/repositories/AutoGPTQ/auto_gptq/nn_modules/qlinear_old.py\", line 189, in\
    \ forward\r\n    autogptq_cuda.vecquant4matmul_faster_old(x, self.qweight, out,\
    \ self.scales.float(), self.qzeros, self.group_size, self.half_indim)\r\nAttributeError:\
    \ module 'autogptq_cuda' has no attribute 'vecquant4matmul_faster_old'\r\n```\r\
    \n\r\nAny specific version of AutoGPTQ that's known to be compatible with this\
    \ model?  I really want to give it a try!"
  created_at: 2023-05-31 00:49:41+00:00
  edited: false
  hidden: false
  id: 6476a7b53d8d1cb79190d4b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T17:07:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Can you confirm you built the latest version from source with these
          commands?</p>

          <pre><code>git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          </code></pre>

          '
        raw: 'Can you confirm you built the latest version from source with these
          commands?

          ```

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          ```'
        updatedAt: '2023-05-31T17:07:05.258Z'
      numEdits: 0
      reactions: []
    id: 64777eb904aa03da2abf251b
    type: comment
  author: TheBloke
  content: 'Can you confirm you built the latest version from source with these commands?

    ```

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip install .

    ```'
  created_at: 2023-05-31 16:07:05+00:00
  edited: false
  hidden: false
  id: 64777eb904aa03da2abf251b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-05-31T17:26:42.000Z'
    data:
      edited: true
      editors:
      - mike-ravkine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<p>Yes, also installing the extra dependency as noted:</p>\n<pre><code>\
          \        \"git clone https://github.com/PanQiWei/AutoGPTQ /repositories/AutoGPTQ\"\
          ,\n        \"cd /repositories/AutoGPTQ &amp;&amp; pip install . &amp;&amp;\
          \ pip install einops\",\n</code></pre>\n<p>I think I have a hunch as to\
          \ what's wrong. For the scripts that I have working AutoGPTQ, there's a\
          \ \"python setup.py install\" step.. I'm going to try adding that, as I\
          \ think its what actually compiles the CUDA.</p>\n"
        raw: "Yes, also installing the extra dependency as noted:\n\n```\n       \
          \ \"git clone https://github.com/PanQiWei/AutoGPTQ /repositories/AutoGPTQ\"\
          ,\n        \"cd /repositories/AutoGPTQ && pip install . && pip install einops\"\
          ,\n```\n\nI think I have a hunch as to what's wrong. For the scripts that\
          \ I have working AutoGPTQ, there's a \"python setup.py install\" step..\
          \ I'm going to try adding that, as I think its what actually compiles the\
          \ CUDA."
        updatedAt: '2023-05-31T17:27:37.586Z'
      numEdits: 1
      reactions: []
    id: 64778352bb7681ad67098d02
    type: comment
  author: mike-ravkine
  content: "Yes, also installing the extra dependency as noted:\n\n```\n        \"\
    git clone https://github.com/PanQiWei/AutoGPTQ /repositories/AutoGPTQ\",\n   \
    \     \"cd /repositories/AutoGPTQ && pip install . && pip install einops\",\n\
    ```\n\nI think I have a hunch as to what's wrong. For the scripts that I have\
    \ working AutoGPTQ, there's a \"python setup.py install\" step.. I'm going to\
    \ try adding that, as I think its what actually compiles the CUDA."
  created_at: 2023-05-31 16:26:42+00:00
  edited: true
  hidden: false
  id: 64778352bb7681ad67098d02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T17:31:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>pip install should do that, but yeah you can run it by hand as well
          if you want.</p>

          <p>Maybe first try:</p>

          <pre><code>pip uninstall auto-gptq

          pip install .

          </code></pre>

          <p>FYI PanQiWei just PR''d code that will soon provide pre-compiled binary
          wheels for AutoGPTQ, so soon it won''t be necessary to compile from source.</p>

          '
        raw: 'pip install should do that, but yeah you can run it by hand as well
          if you want.


          Maybe first try:

          ```

          pip uninstall auto-gptq

          pip install .

          ```


          FYI PanQiWei just PR''d code that will soon provide pre-compiled binary
          wheels for AutoGPTQ, so soon it won''t be necessary to compile from source.'
        updatedAt: '2023-05-31T17:31:14.841Z'
      numEdits: 0
      reactions: []
    id: 6477846233a888101f796ad4
    type: comment
  author: TheBloke
  content: 'pip install should do that, but yeah you can run it by hand as well if
    you want.


    Maybe first try:

    ```

    pip uninstall auto-gptq

    pip install .

    ```


    FYI PanQiWei just PR''d code that will soon provide pre-compiled binary wheels
    for AutoGPTQ, so soon it won''t be necessary to compile from source.'
  created_at: 2023-05-31 16:31:14+00:00
  edited: false
  hidden: false
  id: 6477846233a888101f796ad4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-05-31T17:41:07.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: '<p>Success!  Adding <code>&amp;&amp; python setup.py install</code>
          built the cuda module and fixed the crash and I get a responce!</p>

          <pre><code>### Instruction: write a story about llamas

          ### Response:

          A group of llamas were out exploring the countryside when they stumbled
          upon an old, forgotten temple. As they walked through the entrance, they
          were taken aback by the intricate carvings and the sheer size of the temple.
          They stayed for a while, marveling at the magnificent architecture and absorbing
          the peaceful energy it exuded. After awhile, they decided to continue their
          journey, inspired by the temple''s beauty and wisdom.&lt;|endoftext|&gt;-1:&lt;|endoftext|&gt;In
          the distance, the temple glowed with

          </code></pre>

          <p>I''ll look into whats up with <code>&lt;|endoftext|&gt;</code> and open
          another issue if problem is not in my code.</p>

          <p>Thanks! </p>

          '
        raw: 'Success!  Adding `&& python setup.py install` built the cuda module
          and fixed the crash and I get a responce!


          ```

          ### Instruction: write a story about llamas

          ### Response:

          A group of llamas were out exploring the countryside when they stumbled
          upon an old, forgotten temple. As they walked through the entrance, they
          were taken aback by the intricate carvings and the sheer size of the temple.
          They stayed for a while, marveling at the magnificent architecture and absorbing
          the peaceful energy it exuded. After awhile, they decided to continue their
          journey, inspired by the temple''s beauty and wisdom.<|endoftext|>-1:<|endoftext|>In
          the distance, the temple glowed with

          ```


          I''ll look into whats up with `<|endoftext|>` and open another issue if
          problem is not in my code.


          Thanks! '
        updatedAt: '2023-05-31T17:41:07.075Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647786b3f911e9e76c661615
    id: 647786b3f911e9e76c661614
    type: comment
  author: mike-ravkine
  content: 'Success!  Adding `&& python setup.py install` built the cuda module and
    fixed the crash and I get a responce!


    ```

    ### Instruction: write a story about llamas

    ### Response:

    A group of llamas were out exploring the countryside when they stumbled upon an
    old, forgotten temple. As they walked through the entrance, they were taken aback
    by the intricate carvings and the sheer size of the temple. They stayed for a
    while, marveling at the magnificent architecture and absorbing the peaceful energy
    it exuded. After awhile, they decided to continue their journey, inspired by the
    temple''s beauty and wisdom.<|endoftext|>-1:<|endoftext|>In the distance, the
    temple glowed with

    ```


    I''ll look into whats up with `<|endoftext|>` and open another issue if problem
    is not in my code.


    Thanks! '
  created_at: 2023-05-31 16:41:07+00:00
  edited: false
  hidden: false
  id: 647786b3f911e9e76c661614
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-05-31T17:41:07.000Z'
    data:
      status: closed
    id: 647786b3f911e9e76c661615
    type: status-change
  author: mike-ravkine
  created_at: 2023-05-31 16:41:07+00:00
  id: 647786b3f911e9e76c661615
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: Any working tag/release/hash of AutoGPTQ?
