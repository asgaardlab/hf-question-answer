!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joseph3553
conflicting_files: null
created_at: 2023-06-02 16:40:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
      fullname: nam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joseph3553
      type: user
    createdAt: '2023-06-02T17:40:42.000Z'
    data:
      edited: true
      editors:
      - joseph3553
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
          fullname: nam
          isHf: false
          isPro: false
          name: joseph3553
          type: user
        html: '<p>Hi I loaded the model successfully on ooba booga with 96GB VRAM
          which should be sufficient to run it but I get following errors:</p>

          <p>''''''<br>''"Traceback (most recent call last):<br>  File "/home/administrator/text-generation-webui/modules/callbacks.py",
          line 73, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "/home/administrator/text-generation-webui/modules/text_generation.py",
          line 286, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py",
          line 423, in generate<br>    return self.model.generate(**kwargs)<br>  File
          "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py",
          line 1568, in generate<br>    return self.sample(<br>  File "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py",
          line 2615, in sample<br>    outputs = self(<br>  File "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py",
          line 759, in forward<br>    transformer_outputs = self.transformer(<br>  File
          "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py",
          line 654, in forward<br>    outputs = block(<br>  File "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py",
          line 396, in forward<br>    attn_outputs = self.self_attention(<br>  File
          "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py",
          line 255, in forward<br>    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)<br>  File
          "/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py",
          line 201, in _split_heads<br>    k = qkv[:, :, :, [-2]]<br>RuntimeError:
          CUDA error: an illegal memory access was encountered<br>CUDA kernel errors
          might be asynchronously reported at some other API call, so the stacktrace
          below might be incorrect.<br>For debugging consider passing CUDA_LAUNCH_BLOCKING=1.<br>Compile
          with <code>TORCH_USE_CUDA_DSA</code> to enable device-side assertions.</p>

          <p>Exception in thread Thread-3 (gentask):<br>Traceback (most recent call
          last):<br>  File "/home/administrator/.conda/envs/textgen/lib/python3.10/threading.py",
          line 1016, in _bootstrap_inner<br>    self.run()<br>  File "/home/administrator/.conda/envs/textgen/lib/python3.10/threading.py",
          line 953, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "/home/administrator/text-generation-webui/modules/callbacks.py", line 80,
          in gentask<br>    clear_torch_cache()<br>  File "/home/administrator/text-generation-webui/modules/callbacks.py",
          line 112, in clear_torch_cache<br>    torch.cuda.empty_cache()<br>  File
          "/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/cuda/memory.py",
          line 133, in empty_cache<br>    torch._C._cuda_emptyCache()<br>RuntimeError:
          CUDA error: an illegal memory access was encountered<br>CUDA kernel errors
          might be asynchronously reported at some other API call, so the stacktrace
          below might be incorrect.<br>For debugging consider passing CUDA_LAUNCH_BLOCKING=1.<br>Compile
          with <code>TORCH_USE_CUDA_DSA</code> to enable device-side assertions."''<br>''''''</p>

          '
        raw: "Hi I loaded the model successfully on ooba booga with 96GB VRAM which\
          \ should be sufficient to run it but I get following errors:\n\n'''\n'\"\
          Traceback (most recent call last):\n  File \"/home/administrator/text-generation-webui/modules/callbacks.py\"\
          , line 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
          \  File \"/home/administrator/text-generation-webui/modules/text_generation.py\"\
          , line 286, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 423, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1568, in generate\n    return self.sample(\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2615, in sample\n    outputs = self(\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 759, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 654, in forward\n    outputs = block(\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 396, in forward\n    attn_outputs = self.self_attention(\n  File\
          \ \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 255, in forward\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n\
          \  File \"/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 201, in _split_heads\n    k = qkv[:, :, :, [-2]]\nRuntimeError: CUDA\
          \ error: an illegal memory access was encountered\nCUDA kernel errors might\
          \ be asynchronously reported at some other API call, so the stacktrace below\
          \ might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\
          Exception in thread Thread-3 (gentask):\nTraceback (most recent call last):\n\
          \  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/threading.py\"\
          , line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/threading.py\"\
          , line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File\
          \ \"/home/administrator/text-generation-webui/modules/callbacks.py\", line\
          \ 80, in gentask\n    clear_torch_cache()\n  File \"/home/administrator/text-generation-webui/modules/callbacks.py\"\
          , line 112, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"\
          /home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/cuda/memory.py\"\
          , line 133, in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError:\
          \ CUDA error: an illegal memory access was encountered\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\"'\n\
          '''"
        updatedAt: '2023-06-02T17:42:24.446Z'
      numEdits: 2
      reactions: []
    id: 647a299ab5c8ff1bf38a15fc
    type: comment
  author: joseph3553
  content: "Hi I loaded the model successfully on ooba booga with 96GB VRAM which\
    \ should be sufficient to run it but I get following errors:\n\n'''\n'\"Traceback\
    \ (most recent call last):\n  File \"/home/administrator/text-generation-webui/modules/callbacks.py\"\
    , line 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
    \  File \"/home/administrator/text-generation-webui/modules/text_generation.py\"\
    , line 286, in generate_with_callback\n    shared.model.generate(**kwargs)\n \
    \ File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 423, in generate\n    return self.model.generate(**kwargs)\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1568, in generate\n    return self.sample(\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2615, in sample\n    outputs = self(\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 759, in forward\n    transformer_outputs = self.transformer(\n  File \"\
    /home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 654, in forward\n    outputs = block(\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 396, in forward\n    attn_outputs = self.self_attention(\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 255, in forward\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n\
    \  File \"/home/administrator/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 201, in _split_heads\n    k = qkv[:, :, :, [-2]]\nRuntimeError: CUDA error:\
    \ an illegal memory access was encountered\nCUDA kernel errors might be asynchronously\
    \ reported at some other API call, so the stacktrace below might be incorrect.\n\
    For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\n\nException in thread Thread-3 (gentask):\n\
    Traceback (most recent call last):\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/threading.py\"\
    , line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/threading.py\"\
    , line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/administrator/text-generation-webui/modules/callbacks.py\"\
    , line 80, in gentask\n    clear_torch_cache()\n  File \"/home/administrator/text-generation-webui/modules/callbacks.py\"\
    , line 112, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"/home/administrator/.conda/envs/textgen/lib/python3.10/site-packages/torch/cuda/memory.py\"\
    , line 133, in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError: CUDA\
    \ error: an illegal memory access was encountered\nCUDA kernel errors might be\
    \ asynchronously reported at some other API call, so the stacktrace below might\
    \ be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile\
    \ with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\"'\n'''"
  created_at: 2023-06-02 16:40:42+00:00
  edited: true
  hidden: false
  id: 647a299ab5c8ff1bf38a15fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
      fullname: a749734
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: a749734
      type: user
    createdAt: '2023-06-03T05:46:06.000Z'
    data:
      edited: false
      editors:
      - a749734
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7012374401092529
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
          fullname: a749734
          isHf: false
          isPro: false
          name: a749734
          type: user
        html: '<p>yes getting same error , pls tell solution</p>

          '
        raw: yes getting same error , pls tell solution
        updatedAt: '2023-06-03T05:46:06.881Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - joseph3553
    id: 647ad39e42abe277476a9af4
    type: comment
  author: a749734
  content: yes getting same error , pls tell solution
  created_at: 2023-06-03 04:46:06+00:00
  edited: false
  hidden: false
  id: 647ad39e42abe277476a9af4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-03T10:30:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.940729022026062
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Wow 96GB VRAM? You've got an H100 SXM5?  Or is this 2 x 48GB cards?</p>\n\
          <p>I have never seen this error before so I am not sure what to say.  The\
          \ error is coming from the custom code provided with Falcon - it's not a\
          \ problem in text-generation-webui or AutoGPTQ.  So that might make it hard\
          \ to fix. </p>\n<p>Please describe what hardware you are using, <span data-props=\"\
          {&quot;user&quot;:&quot;joseph3553&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/joseph3553\">@<span class=\"underline\"\
          >joseph3553</span></a></span>\n\n\t</span></span> and <span data-props=\"\
          {&quot;user&quot;:&quot;a749734&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/a749734\">@<span class=\"underline\">a749734</span></a></span>\n\
          \n\t</span></span>  and I will try and test it.  Although I don't have access\
          \ to an H100 SXM5, sadly! :D</p>\n"
        raw: "Wow 96GB VRAM? You've got an H100 SXM5?  Or is this 2 x 48GB cards?\n\
          \nI have never seen this error before so I am not sure what to say.  The\
          \ error is coming from the custom code provided with Falcon - it's not a\
          \ problem in text-generation-webui or AutoGPTQ.  So that might make it hard\
          \ to fix. \n\nPlease describe what hardware you are using, @joseph3553 and\
          \ @a749734  and I will try and test it.  Although I don't have access to\
          \ an H100 SXM5, sadly! :D"
        updatedAt: '2023-06-03T10:30:13.214Z'
      numEdits: 0
      reactions: []
    id: 647b16354d7c0c3fccc7cd85
    type: comment
  author: TheBloke
  content: "Wow 96GB VRAM? You've got an H100 SXM5?  Or is this 2 x 48GB cards?\n\n\
    I have never seen this error before so I am not sure what to say.  The error is\
    \ coming from the custom code provided with Falcon - it's not a problem in text-generation-webui\
    \ or AutoGPTQ.  So that might make it hard to fix. \n\nPlease describe what hardware\
    \ you are using, @joseph3553 and @a749734  and I will try and test it.  Although\
    \ I don't have access to an H100 SXM5, sadly! :D"
  created_at: 2023-06-03 09:30:13+00:00
  edited: false
  hidden: false
  id: 647b16354d7c0c3fccc7cd85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
      fullname: nam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joseph3553
      type: user
    createdAt: '2023-06-04T04:26:17.000Z'
    data:
      edited: false
      editors:
      - joseph3553
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9717050194740295
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
          fullname: nam
          isHf: false
          isPro: false
          name: joseph3553
          type: user
        html: '<p>yes it is Quad A1000 48GB x 2! Also my ram is 1.5TB ... so I am
          sure it is not out of memory problem. It is one of companies'' work station.<br>but
          still when I search around deeply in other community, the error above means
          out of memory problem</p>

          <ul>

          <li>I mistakenly had to update my pytorch version because that seems to
          have fixed above issue in other community but then for ooba booga, it messes
          up the CUDA compatibility and ooba booga no longer uses GPU but just CPU
          so I had to run over the conda env and reset it all with pain. Of course,
          since CPU run is impossible, even at that state I was not able to test any
          feasible model including CUDA</li>

          </ul>

          <p>+So some other says it''s about batch size but as you mentioned, it''s
          Falcon code so it'' impossible for me to tweak it</p>

          <p>Please help!</p>

          '
        raw: 'yes it is Quad A1000 48GB x 2! Also my ram is 1.5TB ... so I am sure
          it is not out of memory problem. It is one of companies'' work station.

          but still when I search around deeply in other community, the error above
          means out of memory problem


          + I mistakenly had to update my pytorch version because that seems to have
          fixed above issue in other community but then for ooba booga, it messes
          up the CUDA compatibility and ooba booga no longer uses GPU but just CPU
          so I had to run over the conda env and reset it all with pain. Of course,
          since CPU run is impossible, even at that state I was not able to test any
          feasible model including CUDA


          +So some other says it''s about batch size but as you mentioned, it''s Falcon
          code so it'' impossible for me to tweak it


          Please help!'
        updatedAt: '2023-06-04T04:26:17.788Z'
      numEdits: 0
      reactions: []
    id: 647c12696dbad6ab058f1b73
    type: comment
  author: joseph3553
  content: 'yes it is Quad A1000 48GB x 2! Also my ram is 1.5TB ... so I am sure it
    is not out of memory problem. It is one of companies'' work station.

    but still when I search around deeply in other community, the error above means
    out of memory problem


    + I mistakenly had to update my pytorch version because that seems to have fixed
    above issue in other community but then for ooba booga, it messes up the CUDA
    compatibility and ooba booga no longer uses GPU but just CPU so I had to run over
    the conda env and reset it all with pain. Of course, since CPU run is impossible,
    even at that state I was not able to test any feasible model including CUDA


    +So some other says it''s about batch size but as you mentioned, it''s Falcon
    code so it'' impossible for me to tweak it


    Please help!'
  created_at: 2023-06-04 03:26:17+00:00
  edited: false
  hidden: false
  id: 647c12696dbad6ab058f1b73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T09:54:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9008060693740845
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It may be an issue specific to multi-GPU. I haven''t tested multi-GPU
          with Falcon GPTQ yet.</p>

          <p>However as you have so much VRAM, you can just load the unquantised model.
          It will be much faster than this GPTQ, which still has performance problems
          at the moment.</p>

          <p>Download: ehartford/WizardLM-Uncensored-Falcon-40b</p>

          <p>And in text-gen-ui, set the GPU memory for each GPU, like in this example:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uPzn_kDgRDb56j50REfB.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uPzn_kDgRDb56j50REfB.png"></a></p>

          <p>So in your case, set the sliders for GPUs 2, 3 and 4 to 46GB, and for
          GPU 1 set it to 20GB (to allow room for context).  Then load the unquantised
          model I linked above.  It should load in 96GB.</p>

          '
        raw: 'It may be an issue specific to multi-GPU. I haven''t tested multi-GPU
          with Falcon GPTQ yet.


          However as you have so much VRAM, you can just load the unquantised model.
          It will be much faster than this GPTQ, which still has performance problems
          at the moment.


          Download: ehartford/WizardLM-Uncensored-Falcon-40b


          And in text-gen-ui, set the GPU memory for each GPU, like in this example:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uPzn_kDgRDb56j50REfB.png)


          So in your case, set the sliders for GPUs 2, 3 and 4 to 46GB, and for GPU
          1 set it to 20GB (to allow room for context).  Then load the unquantised
          model I linked above.  It should load in 96GB.'
        updatedAt: '2023-06-04T09:54:15.437Z'
      numEdits: 0
      reactions: []
    id: 647c5f47c788767ab5c8395a
    type: comment
  author: TheBloke
  content: 'It may be an issue specific to multi-GPU. I haven''t tested multi-GPU
    with Falcon GPTQ yet.


    However as you have so much VRAM, you can just load the unquantised model. It
    will be much faster than this GPTQ, which still has performance problems at the
    moment.


    Download: ehartford/WizardLM-Uncensored-Falcon-40b


    And in text-gen-ui, set the GPU memory for each GPU, like in this example:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uPzn_kDgRDb56j50REfB.png)


    So in your case, set the sliders for GPUs 2, 3 and 4 to 46GB, and for GPU 1 set
    it to 20GB (to allow room for context).  Then load the unquantised model I linked
    above.  It should load in 96GB.'
  created_at: 2023-06-04 08:54:15+00:00
  edited: false
  hidden: false
  id: 647c5f47c788767ab5c8395a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
      fullname: nam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joseph3553
      type: user
    createdAt: '2023-06-04T14:02:14.000Z'
    data:
      edited: false
      editors:
      - joseph3553
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9064940214157104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
          fullname: nam
          isHf: false
          isPro: false
          name: joseph3553
          type: user
        html: '<p>hi thank you for the suggestion. I will try it out! but my current
          (newest version) only shows 2 gpu memory slide bar just like your screenshot.
          Is there any other way to get slide bar for GPU up to 4?</p>

          '
        raw: hi thank you for the suggestion. I will try it out! but my current (newest
          version) only shows 2 gpu memory slide bar just like your screenshot. Is
          there any other way to get slide bar for GPU up to 4?
        updatedAt: '2023-06-04T14:02:14.640Z'
      numEdits: 0
      reactions: []
    id: 647c996660dfe0f35d5306b7
    type: comment
  author: joseph3553
  content: hi thank you for the suggestion. I will try it out! but my current (newest
    version) only shows 2 gpu memory slide bar just like your screenshot. Is there
    any other way to get slide bar for GPU up to 4?
  created_at: 2023-06-04 13:02:14+00:00
  edited: false
  hidden: false
  id: 647c996660dfe0f35d5306b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T14:27:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5379990935325623
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh OK. I guess that''s a limit of the UI i text-gen-ui.</p>

          <p>I believe you can do this on the command line:</p>

          <pre><code>python server.py --listen --gpu-memory 20GiB 46GiB 46GiB 46GiB
          # -- other arguments here

          </code></pre>

          '
        raw: 'Oh OK. I guess that''s a limit of the UI i text-gen-ui.


          I believe you can do this on the command line:


          ```

          python server.py --listen --gpu-memory 20GiB 46GiB 46GiB 46GiB # -- other
          arguments here

          ```'
        updatedAt: '2023-06-04T14:27:34.851Z'
      numEdits: 0
      reactions: []
    id: 647c9f5660dfe0f35d53b6d7
    type: comment
  author: TheBloke
  content: 'Oh OK. I guess that''s a limit of the UI i text-gen-ui.


    I believe you can do this on the command line:


    ```

    python server.py --listen --gpu-memory 20GiB 46GiB 46GiB 46GiB # -- other arguments
    here

    ```'
  created_at: 2023-06-04 13:27:34+00:00
  edited: false
  hidden: false
  id: 647c9f5660dfe0f35d53b6d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
      fullname: nam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joseph3553
      type: user
    createdAt: '2023-06-04T14:38:23.000Z'
    data:
      edited: false
      editors:
      - joseph3553
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824084043502808
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
          fullname: nam
          isHf: false
          isPro: false
          name: joseph3553
          type: user
        html: '<p>Ok thank you! I will try it out and share it once I have viable
          result so that it could be informative to you and the community as well</p>

          '
        raw: Ok thank you! I will try it out and share it once I have viable result
          so that it could be informative to you and the community as well
        updatedAt: '2023-06-04T14:38:23.213Z'
      numEdits: 0
      reactions: []
    id: 647ca1dfc788767ab5cfbf40
    type: comment
  author: joseph3553
  content: Ok thank you! I will try it out and share it once I have viable result
    so that it could be informative to you and the community as well
  created_at: 2023-06-04 13:38:23+00:00
  edited: false
  hidden: false
  id: 647ca1dfc788767ab5cfbf40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T14:57:37.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9936124682426453
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK!</p>

          '
        raw: OK!
        updatedAt: '2023-06-04T14:58:17.300Z'
      numEdits: 1
      reactions: []
    id: 647ca6611c0644de8d28d13b
    type: comment
  author: TheBloke
  content: OK!
  created_at: 2023-06-04 13:57:37+00:00
  edited: true
  hidden: false
  id: 647ca6611c0644de8d28d13b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
      fullname: a749734
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: a749734
      type: user
    createdAt: '2023-06-04T17:34:58.000Z'
    data:
      edited: false
      editors:
      - a749734
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9500778317451477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
          fullname: a749734
          isHf: false
          isPro: false
          name: a749734
          type: user
        html: "<p>Yes , seems to me this is best open source Model , so pls try for\
          \ GPTQ <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span></p>\n"
        raw: Yes , seems to me this is best open source Model , so pls try for GPTQ
          @TheBloke
        updatedAt: '2023-06-04T17:34:58.874Z'
      numEdits: 0
      reactions: []
    id: 647ccb4260dfe0f35d58aa92
    type: comment
  author: a749734
  content: Yes , seems to me this is best open source Model , so pls try for GPTQ
    @TheBloke
  created_at: 2023-06-04 16:34:58+00:00
  edited: false
  hidden: false
  id: 647ccb4260dfe0f35d58aa92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
      fullname: a749734
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: a749734
      type: user
    createdAt: '2023-06-04T17:36:01.000Z'
    data:
      edited: false
      editors:
      - a749734
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9701722264289856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
          fullname: a749734
          isHf: false
          isPro: false
          name: a749734
          type: user
        html: '<p>what is required hardware for its Unquantised version</p>

          '
        raw: what is required hardware for its Unquantised version
        updatedAt: '2023-06-04T17:36:01.517Z'
      numEdits: 0
      reactions: []
    id: 647ccb8183c62f3249208ff3
    type: comment
  author: a749734
  content: what is required hardware for its Unquantised version
  created_at: 2023-06-04 16:36:01+00:00
  edited: false
  hidden: false
  id: 647ccb8183c62f3249208ff3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T18:46:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8810514807701111
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>2 x a100 80gb works well for unquantised</p>

          '
        raw: 2 x a100 80gb works well for unquantised
        updatedAt: '2023-06-04T18:46:53.761Z'
      numEdits: 0
      reactions: []
    id: 647cdc1d1c0644de8d2f3e66
    type: comment
  author: TheBloke
  content: 2 x a100 80gb works well for unquantised
  created_at: 2023-06-04 17:46:53+00:00
  edited: false
  hidden: false
  id: 647cdc1d1c0644de8d2f3e66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-04T20:09:56.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8380990624427795
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I''m getting the same error on a single 3090.</p>

          <p>WSL2/Ubuntu with Cuda 12.1</p>

          '
        raw: 'I''m getting the same error on a single 3090.


          WSL2/Ubuntu with Cuda 12.1'
        updatedAt: '2023-06-04T20:09:56.569Z'
      numEdits: 0
      reactions: []
    id: 647cef9483c62f324925433a
    type: comment
  author: mancub
  content: 'I''m getting the same error on a single 3090.


    WSL2/Ubuntu with Cuda 12.1'
  created_at: 2023-06-04 19:09:56+00:00
  edited: false
  hidden: false
  id: 647cef9483c62f324925433a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T20:28:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9794505834579468
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>1 x 3090 is definitely not enough VRAM.  Needs at least 40GB, maybe
          48GB.</p>

          '
        raw: 1 x 3090 is definitely not enough VRAM.  Needs at least 40GB, maybe 48GB.
        updatedAt: '2023-06-04T20:28:18.099Z'
      numEdits: 0
      reactions: []
    id: 647cf3e2c788767ab5d9b5ec
    type: comment
  author: TheBloke
  content: 1 x 3090 is definitely not enough VRAM.  Needs at least 40GB, maybe 48GB.
  created_at: 2023-06-04 19:28:18+00:00
  edited: false
  hidden: false
  id: 647cf3e2c788767ab5d9b5ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-05T00:19:52.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9816095232963562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Would that mean then that it''s a OOM error for the OP too?</p>

          <p>Though I think my error should be different if I was OOM, because the
          other two above had more VRAM than me, yet we got the same error.</p>

          <p>Are we sure it''s not some flipped bit in the model file causing this?
          :)</p>

          '
        raw: 'Would that mean then that it''s a OOM error for the OP too?


          Though I think my error should be different if I was OOM, because the other
          two above had more VRAM than me, yet we got the same error.


          Are we sure it''s not some flipped bit in the model file causing this? :)'
        updatedAt: '2023-06-05T00:19:52.745Z'
      numEdits: 0
      reactions: []
    id: 647d2a28c788767ab5e01dbe
    type: comment
  author: mancub
  content: 'Would that mean then that it''s a OOM error for the OP too?


    Though I think my error should be different if I was OOM, because the other two
    above had more VRAM than me, yet we got the same error.


    Are we sure it''s not some flipped bit in the model file causing this? :)'
  created_at: 2023-06-04 23:19:52+00:00
  edited: false
  hidden: false
  id: 647d2a28c788767ab5e01dbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
      fullname: nam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joseph3553
      type: user
    createdAt: '2023-06-05T01:55:50.000Z'
    data:
      edited: false
      editors:
      - joseph3553
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9176117777824402
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6eb83e282e5753283bbe67817abe948a.svg
          fullname: nam
          isHf: false
          isPro: false
          name: joseph3553
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ thank you for the tip. It worked out fine, and actually in 96 GB environment\
          \ Falcon 45B is not as slow as some of the youtube reviews.</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/646716c2696e7355f5d2747f/0Z-eldpJgA6aEGu8iHeIq.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/646716c2696e7355f5d2747f/0Z-eldpJgA6aEGu8iHeIq.png\"\
          ></a></p>\n<p>uploaded the image for your reference.<br>Thank you!</p>\n"
        raw: 'Hi @TheBloke thank you for the tip. It worked out fine, and actually
          in 96 GB environment Falcon 45B is not as slow as some of the youtube reviews.



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/646716c2696e7355f5d2747f/0Z-eldpJgA6aEGu8iHeIq.png)


          uploaded the image for your reference.

          Thank you!'
        updatedAt: '2023-06-05T01:55:50.480Z'
      numEdits: 0
      reactions: []
    id: 647d40a61c0644de8d3bd838
    type: comment
  author: joseph3553
  content: 'Hi @TheBloke thank you for the tip. It worked out fine, and actually in
    96 GB environment Falcon 45B is not as slow as some of the youtube reviews.



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/646716c2696e7355f5d2747f/0Z-eldpJgA6aEGu8iHeIq.png)


    uploaded the image for your reference.

    Thank you!'
  created_at: 2023-06-05 00:55:50+00:00
  edited: false
  hidden: false
  id: 647d40a61c0644de8d3bd838
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-05T02:13:13.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609369039535522
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>How is this model different from WizardLM-Falcon-40B, because I
          can load WizardLM-Falcon-40B into my 3090 but I can''t load this one?</p>

          '
        raw: How is this model different from WizardLM-Falcon-40B, because I can load
          WizardLM-Falcon-40B into my 3090 but I can't load this one?
        updatedAt: '2023-06-05T02:13:13.019Z'
      numEdits: 0
      reactions: []
    id: 647d44b960dfe0f35d67fb32
    type: comment
  author: mancub
  content: How is this model different from WizardLM-Falcon-40B, because I can load
    WizardLM-Falcon-40B into my 3090 but I can't load this one?
  created_at: 2023-06-05 01:13:13+00:00
  edited: false
  hidden: false
  id: 647d44b960dfe0f35d67fb32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T13:29:23.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9823371171951294
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There was an error with quantize_config.json in this model until
          2 hours.  It was incorrectly set to 3bits, not 4bits</p>

          <p>Anyone who had problems, please re-download quantize_config.json and
          try again</p>

          <p>Sorry about that!</p>

          '
        raw: 'There was an error with quantize_config.json in this model until 2 hours.  It
          was incorrectly set to 3bits, not 4bits


          Anyone who had problems, please re-download quantize_config.json and try
          again


          Sorry about that!'
        updatedAt: '2023-06-05T13:31:16.533Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mancub
    id: 647de333f14eafc3b450730c
    type: comment
  author: TheBloke
  content: 'There was an error with quantize_config.json in this model until 2 hours.  It
    was incorrectly set to 3bits, not 4bits


    Anyone who had problems, please re-download quantize_config.json and try again


    Sorry about that!'
  created_at: 2023-06-05 12:29:23+00:00
  edited: true
  hidden: false
  id: 647de333f14eafc3b450730c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-05T23:28:35.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9629423022270203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Works now, thanks !!</p>

          <p>Does any other model based on Falcon40B-GPTQ need updating as well?</p>

          '
        raw: 'Works now, thanks !!


          Does any other model based on Falcon40B-GPTQ need updating as well?'
        updatedAt: '2023-06-05T23:28:35.742Z'
      numEdits: 0
      reactions: []
    id: 647e6fa332c471a7fa9b8c0a
    type: comment
  author: mancub
  content: 'Works now, thanks !!


    Does any other model based on Falcon40B-GPTQ need updating as well?'
  created_at: 2023-06-05 22:28:35+00:00
  edited: false
  hidden: false
  id: 647e6fa332c471a7fa9b8c0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T23:32:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9169262647628784
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No, just this one</p>

          '
        raw: No, just this one
        updatedAt: '2023-06-05T23:32:34.960Z'
      numEdits: 0
      reactions: []
    id: 647e70925214d172cbc97a1b
    type: comment
  author: TheBloke
  content: No, just this one
  created_at: 2023-06-05 22:32:34+00:00
  edited: false
  hidden: false
  id: 647e70925214d172cbc97a1b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: error when loading sucessful and prompting simple text
