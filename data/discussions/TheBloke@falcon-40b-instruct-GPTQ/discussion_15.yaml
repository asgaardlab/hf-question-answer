!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Devonance
conflicting_files: null
created_at: 2023-06-08 00:32:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3143289fc4cda7dffd29cfee89cf9744.svg
      fullname: Kevin Horton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Devonance
      type: user
    createdAt: '2023-06-08T01:32:22.000Z'
    data:
      edited: false
      editors:
      - Devonance
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6417554020881653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3143289fc4cda7dffd29cfee89cf9744.svg
          fullname: Kevin Horton
          isHf: false
          isPro: false
          name: Devonance
          type: user
        html: "<p>I have successfully installed 12.1 CUDA toolkit and successfully\
          \ did the following instructions:</p>\n<pre><code class=\"language-bash\"\
          >git <span class=\"hljs-built_in\">clone</span> https://github.com/PanQiWei/AutoGPTQ\n\
          <span class=\"hljs-built_in\">cd</span> AutoGPTQ\npip install .\n</code></pre>\n\
          <p>Then I check using <code>pip freeze</code> that it is installed in a\
          \ weird way:</p>\n<pre><code class=\"language-bash\">auto-gptq @ file:///oobabooga_windows/text-generation-webui/modules/AutoGPTQ\n\
          </code></pre>\n<p>Trying to load it using test-generation-webui, I get this\
          \ error:</p>\n<pre><code class=\"language-bash\">Traceback (most recent\
          \ call last): File \u201C\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
          , line 69, <span class=\"hljs-keyword\">in</span> load_model_wrapper shared.model,\
          \ shared.tokenizer = load_model(shared.model_name) \n\nFile \u201C\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 94, <span class=\"\
          hljs-keyword\">in</span> load_model output = load_func(model_name) \n\n\
          File \u201C\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 294, <span class=\"hljs-keyword\">in</span> AutoGPTQ_loader import\
          \ modules.AutoGPTQ_loader \n\nFile \u201C\\oobabooga_windows\\text-generation-webui\\\
          modules\\AutoGPTQ_loader.py\u201D, line 3, <span class=\"hljs-keyword\"\
          >in</span> from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\
          \ ModuleNotFoundError: No module named \u2018auto_gptq\u2019\n</code></pre>\n\
          <p>Not really sure where to go for, here honestly. All the other models\
          \ have worked without issue<br>(first time trying Auto-gptq). I even uninstalled\
          \ and tried <code>pip install auto-gptq</code>, which also gave the same\
          \ issue.</p>\n"
        raw: "I have successfully installed 12.1 CUDA toolkit and successfully did\
          \ the following instructions:\r\n```bash\r\ngit clone https://github.com/PanQiWei/AutoGPTQ\r\
          \ncd AutoGPTQ\r\npip install .\r\n```\r\nThen I check using `pip freeze`\
          \ that it is installed in a weird way:\r\n```bash\r\nauto-gptq @ file:///oobabooga_windows/text-generation-webui/modules/AutoGPTQ\r\
          \n```\r\n\r\nTrying to load it using test-generation-webui, I get this error:\r\
          \n\r\n```bash\r\nTraceback (most recent call last): File \u201C\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 69, in load_model_wrapper shared.model,\
          \ shared.tokenizer = load_model(shared.model_name) \r\n\r\nFile \u201C\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 94, in load_model output = load_func(model_name) \r\n\r\nFile \u201C\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 294, in AutoGPTQ_loader import modules.AutoGPTQ_loader \r\n\r\nFile \u201C\
          \\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
          , line 3, in from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\
          \ ModuleNotFoundError: No module named \u2018auto_gptq\u2019\r\n```\r\n\r\
          \nNot really sure where to go for, here honestly. All the other models have\
          \ worked without issue \r\n(first time trying Auto-gptq). I even uninstalled\
          \ and tried `pip install auto-gptq`, which also gave the same issue."
        updatedAt: '2023-06-08T01:32:22.821Z'
      numEdits: 0
      reactions: []
    id: 64812fa65409aa3e3bbf483e
    type: comment
  author: Devonance
  content: "I have successfully installed 12.1 CUDA toolkit and successfully did the\
    \ following instructions:\r\n```bash\r\ngit clone https://github.com/PanQiWei/AutoGPTQ\r\
    \ncd AutoGPTQ\r\npip install .\r\n```\r\nThen I check using `pip freeze` that\
    \ it is installed in a weird way:\r\n```bash\r\nauto-gptq @ file:///oobabooga_windows/text-generation-webui/modules/AutoGPTQ\r\
    \n```\r\n\r\nTrying to load it using test-generation-webui, I get this error:\r\
    \n\r\n```bash\r\nTraceback (most recent call last): File \u201C\\oobabooga_windows\\\
    text-generation-webui\\server.py\u201D, line 69, in load_model_wrapper shared.model,\
    \ shared.tokenizer = load_model(shared.model_name) \r\n\r\nFile \u201C\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 94, in load_model output\
    \ = load_func(model_name) \r\n\r\nFile \u201C\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 294, in AutoGPTQ_loader import modules.AutoGPTQ_loader\
    \ \r\n\r\nFile \u201C\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
    , line 3, in from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig ModuleNotFoundError:\
    \ No module named \u2018auto_gptq\u2019\r\n```\r\n\r\nNot really sure where to\
    \ go for, here honestly. All the other models have worked without issue \r\n(first\
    \ time trying Auto-gptq). I even uninstalled and tried `pip install auto-gptq`,\
    \ which also gave the same issue."
  created_at: 2023-06-08 00:32:22+00:00
  edited: false
  hidden: false
  id: 64812fa65409aa3e3bbf483e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T08:23:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9206985831260681
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The one-click installer for text-gen-ui  creates a conda environment
          and installs all its packages in there. So if you used the installer, you''d
          need to activate that conda environment before manually installing any packages.</p>

          <p>As you''re using CUDA 12.1, you can''t use the pre-built binaries for
          pytorch 2.0.0 any more. Have you you compiled pytorch 2 from source (and
          in the same environment as you''re trying to run text-gen-ui?) If not, nothing
          will work.  Using CUDA toolkit 12.x is a pain at the moment, for that reason.</p>

          '
        raw: 'The one-click installer for text-gen-ui  creates a conda environment
          and installs all its packages in there. So if you used the installer, you''d
          need to activate that conda environment before manually installing any packages.


          As you''re using CUDA 12.1, you can''t use the pre-built binaries for pytorch
          2.0.0 any more. Have you you compiled pytorch 2 from source (and in the
          same environment as you''re trying to run text-gen-ui?) If not, nothing
          will work.  Using CUDA toolkit 12.x is a pain at the moment, for that reason.'
        updatedAt: '2023-06-08T08:23:20.401Z'
      numEdits: 0
      reactions: []
    id: 64818ff817f2fba00087d9c0
    type: comment
  author: TheBloke
  content: 'The one-click installer for text-gen-ui  creates a conda environment and
    installs all its packages in there. So if you used the installer, you''d need
    to activate that conda environment before manually installing any packages.


    As you''re using CUDA 12.1, you can''t use the pre-built binaries for pytorch
    2.0.0 any more. Have you you compiled pytorch 2 from source (and in the same environment
    as you''re trying to run text-gen-ui?) If not, nothing will work.  Using CUDA
    toolkit 12.x is a pain at the moment, for that reason.'
  created_at: 2023-06-08 07:23:20+00:00
  edited: false
  hidden: false
  id: 64818ff817f2fba00087d9c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-09T00:13:17.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8593084812164307
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>This is what I''ve been using successfully with CUDA 12.1:</p>

          <p>torch                    2.1.0.dev20230526+cu121<br>torchvision        0.16.0.dev20230527+cu121</p>

          <p>Did not compile torch from source, installed nightly builds.</p>

          '
        raw: 'This is what I''ve been using successfully with CUDA 12.1:


          torch                    2.1.0.dev20230526+cu121

          torchvision        0.16.0.dev20230527+cu121


          Did not compile torch from source, installed nightly builds.'
        updatedAt: '2023-06-09T00:13:17.077Z'
      numEdits: 0
      reactions: []
    id: 64826e9deb4befee37805a86
    type: comment
  author: mancub
  content: 'This is what I''ve been using successfully with CUDA 12.1:


    torch                    2.1.0.dev20230526+cu121

    torchvision        0.16.0.dev20230527+cu121


    Did not compile torch from source, installed nightly builds.'
  created_at: 2023-06-08 23:13:17+00:00
  edited: false
  hidden: false
  id: 64826e9deb4befee37805a86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T00:15:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9764066934585571
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah OK, fair enough</p>

          <p>I''ve been compiling 2.0.1 from source, in case anything changed in 2.1.0
          that might affect anything I do</p>

          '
        raw: 'Ah OK, fair enough


          I''ve been compiling 2.0.1 from source, in case anything changed in 2.1.0
          that might affect anything I do'
        updatedAt: '2023-06-09T00:15:46.183Z'
      numEdits: 0
      reactions: []
    id: 64826f32a5fce44ee49f19ed
    type: comment
  author: TheBloke
  content: 'Ah OK, fair enough


    I''ve been compiling 2.0.1 from source, in case anything changed in 2.1.0 that
    might affect anything I do'
  created_at: 2023-06-08 23:15:46+00:00
  edited: false
  hidden: false
  id: 64826f32a5fce44ee49f19ed
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Issues with Auto
