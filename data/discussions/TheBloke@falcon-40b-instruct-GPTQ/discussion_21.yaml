!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fahadh4ilyas
conflicting_files: null
created_at: 2023-08-08 12:06:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-08-08T13:06:12.000Z'
    data:
      edited: true
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44006967544555664
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: "<p>I got this error when loading this model with text generation web\
          \ ui with offloading to cpu</p>\n<pre><code>Traceback (most recent call\
          \ last):\n  File \"/home/fahadh/text-generation-webui/modules/callbacks.py\"\
          , line 55, in gentask \n    ret = self.mfunc(callback=_callback, *args,\
          \ **self.kwargs)\n  File \"/home/fahadh/text-generation-webui/modules/text_generation.py\"\
          , line 307, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 438, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1633, in generate\n    return self.sample(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2755, in sample\n    outputs = self(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 759, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 654, in forward\n    outputs = block(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 396, in forward\n    attn_outputs = self.self_attention(\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)   \
          \                                                                      \
          \                                             \n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda_old.py\"\
          , line 266, in forward\n    out = out + self.bias if self.bias is not None\
          \ else out\nRuntimeError: Expected all tensors to be on the same device,\
          \ but found at least two devices, cuda:0 and cpu!\nException in thread Thread-3\
          \ (gentask):\nTraceback (most recent call last):\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File\
          \ \"/home/fahadh/text-generation-webui/modules/callbacks.py\", line 62,\
          \ in gentask\n    clear_torch_cache()\n  File \"/home/fahadh/text-generation-webui/modules/callbacks.py\"\
          , line 94, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"\
          /home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/cuda/memory.py\"\
          , line 133, in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError:\
          \ CUDA error: an illegal memory access was encountered\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. \n</code></pre>\n"
        raw: "I got this error when loading this model with text generation web ui\
          \ with offloading to cpu\n```\nTraceback (most recent call last):\n  File\
          \ \"/home/fahadh/text-generation-webui/modules/callbacks.py\", line 55,\
          \ in gentask \n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n\
          \  File \"/home/fahadh/text-generation-webui/modules/text_generation.py\"\
          , line 307, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 438, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1633, in generate\n    return self.sample(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2755, in sample\n    outputs = self(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 759, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 654, in forward\n    outputs = block(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 396, in forward\n    attn_outputs = self.self_attention(\n  File\
          \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
          , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)   \
          \                                                                      \
          \                                             \n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda_old.py\"\
          , line 266, in forward\n    out = out + self.bias if self.bias is not None\
          \ else out\nRuntimeError: Expected all tensors to be on the same device,\
          \ but found at least two devices, cuda:0 and cpu!\nException in thread Thread-3\
          \ (gentask):\nTraceback (most recent call last):\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File\
          \ \"/home/fahadh/text-generation-webui/modules/callbacks.py\", line 62,\
          \ in gentask\n    clear_torch_cache()\n  File \"/home/fahadh/text-generation-webui/modules/callbacks.py\"\
          , line 94, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"\
          /home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/cuda/memory.py\"\
          , line 133, in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError:\
          \ CUDA error: an illegal memory access was encountered\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. \n```"
        updatedAt: '2023-08-08T13:11:06.954Z'
      numEdits: 1
      reactions: []
    id: 64d23dc4032a420d186601d8
    type: comment
  author: fahadh4ilyas
  content: "I got this error when loading this model with text generation web ui with\
    \ offloading to cpu\n```\nTraceback (most recent call last):\n  File \"/home/fahadh/text-generation-webui/modules/callbacks.py\"\
    , line 55, in gentask \n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n\
    \  File \"/home/fahadh/text-generation-webui/modules/text_generation.py\", line\
    \ 307, in generate_with_callback\n    shared.model.generate(**kwargs)\n  File\
    \ \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 438, in generate\n    return self.model.generate(**kwargs)\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1633, in generate\n    return self.sample(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2755, in sample\n    outputs = self(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 759, in forward\n    transformer_outputs = self.transformer(\n  File \"\
    /home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 654, in forward\n    outputs = block(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 396, in forward\n    attn_outputs = self.self_attention(\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/fahadh/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-40b-instruct-GPTQ/modelling_RW.py\"\
    , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)  #\
    \ [batch_size, seq_length, 3 x hidden_size]\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)         \
    \                                                                            \
    \                                 \n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda_old.py\"\
    , line 266, in forward\n    out = out + self.bias if self.bias is not None else\
    \ out\nRuntimeError: Expected all tensors to be on the same device, but found\
    \ at least two devices, cuda:0 and cpu!\nException in thread Thread-3 (gentask):\n\
    Traceback (most recent call last):\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
    , line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
    , line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/fahadh/text-generation-webui/modules/callbacks.py\"\
    , line 62, in gentask\n    clear_torch_cache()\n  File \"/home/fahadh/text-generation-webui/modules/callbacks.py\"\
    , line 94, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"/home/fahadh/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/cuda/memory.py\"\
    , line 133, in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError: CUDA\
    \ error: an illegal memory access was encountered\nCUDA kernel errors might be\
    \ asynchronously reported at some other API call, so the stacktrace below might\
    \ be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile\
    \ with `TORCH_USE_CUDA_DSA` to enable device-side assertions. \n```"
  created_at: 2023-08-08 12:06:12+00:00
  edited: true
  hidden: false
  id: 64d23dc4032a420d186601d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/085eefe5879fd84ecfb788f95ed241b2.svg
      fullname: Kartavya Bagga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kartavyabagga
      type: user
    createdAt: '2023-10-21T15:29:35.000Z'
    data:
      edited: false
      editors:
      - kartavyabagga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9693975448608398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/085eefe5879fd84ecfb788f95ed241b2.svg
          fullname: Kartavya Bagga
          isHf: false
          isPro: false
          name: kartavyabagga
          type: user
        html: '<p>How did you offloaded gptq layers to CPU ?</p>

          '
        raw: How did you offloaded gptq layers to CPU ?
        updatedAt: '2023-10-21T15:29:35.388Z'
      numEdits: 0
      reactions: []
    id: 6533ee5f35ad8b9e58bec770
    type: comment
  author: kartavyabagga
  content: How did you offloaded gptq layers to CPU ?
  created_at: 2023-10-21 14:29:35+00:00
  edited: false
  hidden: false
  id: 6533ee5f35ad8b9e58bec770
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Offloading to cpu not working?
