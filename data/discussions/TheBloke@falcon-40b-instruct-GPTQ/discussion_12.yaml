!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ichsan2895
conflicting_files: null
created_at: 2023-06-04 14:55:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-04T15:55:41.000Z'
    data:
      edited: false
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9029370546340942
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<p>That being said "This repo contains an experimantal <strong>GPTQ
          4bit</strong> model for Falcon-40B-Instruct."<br>But, recently I found a
          tutorial in a website that claimed this one is <strong>QLoRa</strong> version.</p>

          <p>AFAIK, the <strong>QLoRA</strong> version is using <code>bitsandbytes</code>
          library. Activating <code>bnb_4bit_quant_type=''nf4''</code> and <code>load_in_4bit=True</code>.
          I feel it more faster than GPTQ version. I tested both in runpods with A6000
          48GB VRAM.</p>

          <p>So, lets wrap my question. It was the same thing and whar is the different?</p>

          '
        raw: "That being said \"This repo contains an experimantal **GPTQ 4bit** model\
          \ for Falcon-40B-Instruct.\"\r\nBut, recently I found a tutorial in a website\
          \ that claimed this one is **QLoRa** version.\r\n\r\nAFAIK, the **QLoRA**\
          \ version is using `bitsandbytes` library. Activating `bnb_4bit_quant_type='nf4'`\
          \ and `load_in_4bit=True`. I feel it more faster than GPTQ version. I tested\
          \ both in runpods with A6000 48GB VRAM.\r\n\r\nSo, lets wrap my question.\
          \ It was the same thing and whar is the different?\r\n\r\n"
        updatedAt: '2023-06-04T15:55:41.960Z'
      numEdits: 0
      reactions: []
    id: 647cb3fde07cf9bb2d47b84c
    type: comment
  author: Ichsan2895
  content: "That being said \"This repo contains an experimantal **GPTQ 4bit** model\
    \ for Falcon-40B-Instruct.\"\r\nBut, recently I found a tutorial in a website\
    \ that claimed this one is **QLoRa** version.\r\n\r\nAFAIK, the **QLoRA** version\
    \ is using `bitsandbytes` library. Activating `bnb_4bit_quant_type='nf4'` and\
    \ `load_in_4bit=True`. I feel it more faster than GPTQ version. I tested both\
    \ in runpods with A6000 48GB VRAM.\r\n\r\nSo, lets wrap my question. It was the\
    \ same thing and whar is the different?\r\n\r\n"
  created_at: 2023-06-04 14:55:41+00:00
  edited: false
  hidden: false
  id: 647cb3fde07cf9bb2d47b84c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f22b37e61147cbb2c513da4f4efae35.svg
      fullname: Matt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattkallo
      type: user
    createdAt: '2023-06-23T18:34:30.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/3f22b37e61147cbb2c513da4f4efae35.svg
          fullname: Matt
          isHf: false
          isPro: false
          name: mattkallo
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-23T19:03:28.701Z'
      numEdits: 1
      reactions: []
    id: 6495e5b6256cb6b587071553
    type: comment
  author: mattkallo
  content: This comment has been hidden
  created_at: 2023-06-23 17:34:30+00:00
  edited: true
  hidden: true
  id: 6495e5b6256cb6b587071553
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T20:07:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9645601511001587
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Ichsan2895&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Ichsan2895\">@<span class=\"\
          underline\">Ichsan2895</span></a></span>\n\n\t</span></span> sorry, I missed\
          \ this question when it was first posted.  Assuming you've not found the\
          \ answer yet:</p>\n<ol>\n<li>No, this repo is not related to QLoRA in any\
          \ way. Falcon 40B Instruct was not trained as a QLoRA.</li>\n<li>QLoRA does\
          \ use bitsandbytes yes. It uses it to load an unquantised model in 4bit,\
          \ and then it does training on that model.</li>\n<li>You can also use bitsandbytes\
          \ to do 4bit and 8bit inference, as you mention. It is an automatic quantisation\
          \ library, that applies the quantisation 'live', rather than requiring the\
          \ user to save a quantised version in advance.</li>\n<li>GPTQ is also a\
          \ quantisation library , but it works by saving a quantised model and then\
          \ loading that quantised model. It can't do it as you load the model.  Generally\
          \ GPTQ is much faster than bitsandbytes.</li>\n<li>But yes, for Falcon models\
          \ only, bitsandbytes is faster than GPTQ.  That's because there's a major\
          \ performance problem with Falcon GPTQ at the moment.  It is being looked\
          \ at but there is no solution yet.</li>\n</ol>\n<p>Recently, after you posted,\
          \ we also got Falcon GGML support and this is much faster than GPTQ and\
          \ may be faster than bitsandbytes as well.  I have a repo for falcon-40b-GGML\
          \ which you could try using ctransformers (Python library) or using the\
          \ LoLLMS-UI.  They're not yet supported in text-generation-webui.</p>\n\
          <p>Hope that helps.</p>\n"
        raw: '@Ichsan2895 sorry, I missed this question when it was first posted.  Assuming
          you''ve not found the answer yet:


          1. No, this repo is not related to QLoRA in any way. Falcon 40B Instruct
          was not trained as a QLoRA.

          2. QLoRA does use bitsandbytes yes. It uses it to load an unquantised model
          in 4bit, and then it does training on that model.

          3. You can also use bitsandbytes to do 4bit and 8bit inference, as you mention.
          It is an automatic quantisation library, that applies the quantisation ''live'',
          rather than requiring the user to save a quantised version in advance.

          4.  GPTQ is also a quantisation library , but it works by saving a quantised
          model and then loading that quantised model. It can''t do it as you load
          the model.  Generally GPTQ is much faster than bitsandbytes.

          5. But yes, for Falcon models only, bitsandbytes is faster than GPTQ.  That''s
          because there''s a major performance problem with Falcon GPTQ at the moment.  It
          is being looked at but there is no solution yet.


          Recently, after you posted, we also got Falcon GGML support and this is
          much faster than GPTQ and may be faster than bitsandbytes as well.  I have
          a repo for falcon-40b-GGML which you could try using ctransformers (Python
          library) or using the LoLLMS-UI.  They''re not yet supported in text-generation-webui.


          Hope that helps.'
        updatedAt: '2023-06-23T20:07:13.314Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - smithlai
        - Sc0urge
        - luv2261
    id: 6495fb71711be94fb5a791f6
    type: comment
  author: TheBloke
  content: '@Ichsan2895 sorry, I missed this question when it was first posted.  Assuming
    you''ve not found the answer yet:


    1. No, this repo is not related to QLoRA in any way. Falcon 40B Instruct was not
    trained as a QLoRA.

    2. QLoRA does use bitsandbytes yes. It uses it to load an unquantised model in
    4bit, and then it does training on that model.

    3. You can also use bitsandbytes to do 4bit and 8bit inference, as you mention.
    It is an automatic quantisation library, that applies the quantisation ''live'',
    rather than requiring the user to save a quantised version in advance.

    4.  GPTQ is also a quantisation library , but it works by saving a quantised model
    and then loading that quantised model. It can''t do it as you load the model.  Generally
    GPTQ is much faster than bitsandbytes.

    5. But yes, for Falcon models only, bitsandbytes is faster than GPTQ.  That''s
    because there''s a major performance problem with Falcon GPTQ at the moment.  It
    is being looked at but there is no solution yet.


    Recently, after you posted, we also got Falcon GGML support and this is much faster
    than GPTQ and may be faster than bitsandbytes as well.  I have a repo for falcon-40b-GGML
    which you could try using ctransformers (Python library) or using the LoLLMS-UI.  They''re
    not yet supported in text-generation-webui.


    Hope that helps.'
  created_at: 2023-06-23 19:07:13+00:00
  edited: false
  hidden: false
  id: 6495fb71711be94fb5a791f6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: What is the different between GPTQ and QLoRA?
