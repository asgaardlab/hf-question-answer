!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Suoriks
conflicting_files: null
created_at: 2023-05-27 18:37:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b511314b2fcc9f7583c78d9767b4770.svg
      fullname: Eugene Chernov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Suoriks
      type: user
    createdAt: '2023-05-27T19:37:37.000Z'
    data:
      edited: true
      editors:
      - Suoriks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b511314b2fcc9f7583c78d9767b4770.svg
          fullname: Eugene Chernov
          isHf: false
          isPro: false
          name: Suoriks
          type: user
        html: "<p>Tell me, what am I doing wrong? I did everything according to the\
          \ instructions, </p>\n<ol>\n<li>add in update_windows.bat<br>pip install\
          \ autogptq<br>pip install einops</li>\n<li>run it</li>\n<li>add 'trust_remote_code':\
          \ shared.args.trust_remote_code, in AutoGPTQ_loader.py</li>\n<li>add --trust-remote-code\
          \ (instead --trust_remote_code) and --autogptq in webui.py<br>but I get\
          \ an error:</li>\n</ol>\n<p>Traceback (most recent call last): File \u201C\
          D:\\LLaMA\\oobabooga_windows\\text-generation-webui\\server.py\u201D, line\
          \ 71, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201CD:\\LLaMA\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 95, in load_model output = load_func(model_name) File\
          \ \u201CD:\\LLaMA\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 297, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201CD:\\LLaMA\\oobabooga_windows\\text-generation-webui\\modules\\\
          AutoGPTQ_loader.py\u201D, line 43, in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params) File \u201CD:\\LLaMA\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 62, in from_quantized\
          \ model_type = check_and_get_model_type(save_dir) File \u201CD:\\LLaMA\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 124, in check_and_get_model_type raise TypeError(f\"\
          {config.model_type} isn\u2019t supported yet.\") TypeError: RefinedWeb isn\u2019\
          t supported yet.</p>\n<p>Parameters on the tab GPTQ<br>wbits 4, groupsize\
          \ 64, model_type llama</p>\n"
        raw: "Tell me, what am I doing wrong? I did everything according to the instructions,\
          \ \n1. add in update_windows.bat\npip install autogptq\npip install einops\n\
          2. run it\n3. add 'trust_remote_code': shared.args.trust_remote_code, in\
          \ AutoGPTQ_loader.py\n4. add --trust-remote-code (instead --trust_remote_code)\
          \ and --autogptq in webui.py\nbut I get an error:\n\nTraceback (most recent\
          \ call last): File \u201CD:\\LLaMA\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CD:\\LLaMA\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 95, in load_model\
          \ output = load_func(model_name) File \u201CD:\\LLaMA\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 297, in AutoGPTQ_loader\
          \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201C\
          D:\\LLaMA\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
          , line 43, in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params) File \u201CD:\\LLaMA\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 62, in from_quantized\
          \ model_type = check_and_get_model_type(save_dir) File \u201CD:\\LLaMA\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 124, in check_and_get_model_type raise TypeError(f\"\
          {config.model_type} isn\u2019t supported yet.\") TypeError: RefinedWeb isn\u2019\
          t supported yet.\n\nParameters on the tab GPTQ\nwbits 4, groupsize 64, model_type\
          \ llama"
        updatedAt: '2023-05-27T20:05:00.120Z'
      numEdits: 1
      reactions: []
    id: 64725c01b628f9ecdf085268
    type: comment
  author: Suoriks
  content: "Tell me, what am I doing wrong? I did everything according to the instructions,\
    \ \n1. add in update_windows.bat\npip install autogptq\npip install einops\n2.\
    \ run it\n3. add 'trust_remote_code': shared.args.trust_remote_code, in AutoGPTQ_loader.py\n\
    4. add --trust-remote-code (instead --trust_remote_code) and --autogptq in webui.py\n\
    but I get an error:\n\nTraceback (most recent call last): File \u201CD:\\LLaMA\\\
    oobabooga_windows\\text-generation-webui\\server.py\u201D, line 71, in load_model_wrapper\
    \ shared.model, shared.tokenizer = load_model(shared.model_name) File \u201CD:\\\
    LLaMA\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 95, in load_model output = load_func(model_name) File \u201CD:\\LLaMA\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 297, in AutoGPTQ_loader\
    \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201CD:\\LLaMA\\\
    oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D, line\
    \ 43, in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params) File \u201CD:\\LLaMA\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 62, in from_quantized\
    \ model_type = check_and_get_model_type(save_dir) File \u201CD:\\LLaMA\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D,\
    \ line 124, in check_and_get_model_type raise TypeError(f\"{config.model_type}\
    \ isn\u2019t supported yet.\") TypeError: RefinedWeb isn\u2019t supported yet.\n\
    \nParameters on the tab GPTQ\nwbits 4, groupsize 64, model_type llama"
  created_at: 2023-05-27 18:37:37+00:00
  edited: true
  hidden: false
  id: 64725c01b628f9ecdf085268
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T19:41:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to update AutoGPTQ with:</p>

          <pre><code>git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install . # This step requires CUDA toolkit installed

          </code></pre>

          <p>I will make this clearer in the README!</p>

          '
        raw: 'You need to update AutoGPTQ with:

          ```

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install . # This step requires CUDA toolkit installed

          ```


          I will make this clearer in the README!'
        updatedAt: '2023-05-27T19:41:32.745Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - Thireus
        - latent-variable
        - dj1982
        - Hopps
        - 1sn0s
    id: 64725cec97a75cc77ab66f75
    type: comment
  author: TheBloke
  content: 'You need to update AutoGPTQ with:

    ```

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip install . # This step requires CUDA toolkit installed

    ```


    I will make this clearer in the README!'
  created_at: 2023-05-27 18:41:32+00:00
  edited: false
  hidden: false
  id: 64725cec97a75cc77ab66f75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T19:46:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Three more things to note: </p>

          <ol>

          <li>The GPTQ parameters don''t have any effect for AutoGPTQ models</li>

          <li>This 40B model requires more than 24GB VRAM. So you will have to use
          CPU offloading</li>

          <li>It''s slow as hell at the moment!  Even with enough VRAM (eg on a 48GB
          card), I was getting less than 1 tokens/s.</li>

          </ol>

          '
        raw: "Three more things to note: \n1. The GPTQ parameters don't have any effect\
          \ for AutoGPTQ models\n2. This 40B model requires more than 24GB VRAM. So\
          \ you will have to use CPU offloading\n3. It's slow as hell at the moment!\
          \  Even with enough VRAM (eg on a 48GB card), I was getting less than 1\
          \ tokens/s."
        updatedAt: '2023-05-27T19:46:31.683Z'
      numEdits: 0
      reactions: []
    id: 64725e1797a75cc77ab68846
    type: comment
  author: TheBloke
  content: "Three more things to note: \n1. The GPTQ parameters don't have any effect\
    \ for AutoGPTQ models\n2. This 40B model requires more than 24GB VRAM. So you\
    \ will have to use CPU offloading\n3. It's slow as hell at the moment!  Even with\
    \ enough VRAM (eg on a 48GB card), I was getting less than 1 tokens/s."
  created_at: 2023-05-27 18:46:31+00:00
  edited: false
  hidden: false
  id: 64725e1797a75cc77ab68846
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b511314b2fcc9f7583c78d9767b4770.svg
      fullname: Eugene Chernov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Suoriks
      type: user
    createdAt: '2023-05-27T20:04:15.000Z'
    data:
      edited: false
      editors:
      - Suoriks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b511314b2fcc9f7583c78d9767b4770.svg
          fullname: Eugene Chernov
          isHf: false
          isPro: false
          name: Suoriks
          type: user
        html: '<p>It''s working! Thanks!</p>

          '
        raw: It's working! Thanks!
        updatedAt: '2023-05-27T20:04:15.152Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dj1982
    id: 6472623f5afd6a69658a89ac
    type: comment
  author: Suoriks
  content: It's working! Thanks!
  created_at: 2023-05-27 19:04:15+00:00
  edited: false
  hidden: false
  id: 6472623f5afd6a69658a89ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-05-27T21:54:01.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<pre><code>Output generated in 24.27 seconds (0.45 tokens/s, 11 tokens,\
          \ context 48, seed 466795515)\nOutput generated in 43.11 seconds (0.49 tokens/s,\
          \ 21 tokens, context 48, seed 532631384)\nOutput generated in 40.49 seconds\
          \ (0.57 tokens/s, 23 tokens, context 41, seed 1349492009)\nOutput generated\
          \ in 334.20 seconds (0.33 tokens/s, 109 tokens, context 48, seed 1693397338)\n\
          </code></pre>\n<p>\U0001F62D</p>\n"
        raw: "```\nOutput generated in 24.27 seconds (0.45 tokens/s, 11 tokens, context\
          \ 48, seed 466795515)\nOutput generated in 43.11 seconds (0.49 tokens/s,\
          \ 21 tokens, context 48, seed 532631384)\nOutput generated in 40.49 seconds\
          \ (0.57 tokens/s, 23 tokens, context 41, seed 1349492009)\nOutput generated\
          \ in 334.20 seconds (0.33 tokens/s, 109 tokens, context 48, seed 1693397338)\n\
          ```\n\n\U0001F62D"
        updatedAt: '2023-05-27T21:54:01.742Z'
      numEdits: 0
      reactions: []
    id: 64727bf95afd6a69658cb00e
    type: comment
  author: Thireus
  content: "```\nOutput generated in 24.27 seconds (0.45 tokens/s, 11 tokens, context\
    \ 48, seed 466795515)\nOutput generated in 43.11 seconds (0.49 tokens/s, 21 tokens,\
    \ context 48, seed 532631384)\nOutput generated in 40.49 seconds (0.57 tokens/s,\
    \ 23 tokens, context 41, seed 1349492009)\nOutput generated in 334.20 seconds\
    \ (0.33 tokens/s, 109 tokens, context 48, seed 1693397338)\n```\n\n\U0001F62D"
  created_at: 2023-05-27 20:54:01+00:00
  edited: false
  hidden: false
  id: 64727bf95afd6a69658cb00e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T22:56:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yup :) It is slow as hell atm.  I''ve flagged it with qwopqwop and
          PanQiWei of AutoGPTQ so hopefully they can investigate if it''s anything
          on the AutoGPTQ side.</p>

          <p>But my feeling is that it may be as much to do with the custom code for
          loading the Falcon model - or some combination of that code with AutoGPTQ</p>

          '
        raw: 'Yup :) It is slow as hell atm.  I''ve flagged it with qwopqwop and PanQiWei
          of AutoGPTQ so hopefully they can investigate if it''s anything on the AutoGPTQ
          side.


          But my feeling is that it may be as much to do with the custom code for
          loading the Falcon model - or some combination of that code with AutoGPTQ'
        updatedAt: '2023-05-27T22:56:13.740Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - dj1982
        - Thireus
    id: 64728a8dc27f74a0ebaced2d
    type: comment
  author: TheBloke
  content: 'Yup :) It is slow as hell atm.  I''ve flagged it with qwopqwop and PanQiWei
    of AutoGPTQ so hopefully they can investigate if it''s anything on the AutoGPTQ
    side.


    But my feeling is that it may be as much to do with the custom code for loading
    the Falcon model - or some combination of that code with AutoGPTQ'
  created_at: 2023-05-27 21:56:13+00:00
  edited: false
  hidden: false
  id: 64728a8dc27f74a0ebaced2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/EFpV2gb6jQKjXouKxLzhM.jpeg?w=200&h=200&f=face
      fullname: ZheYuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yizer16
      type: user
    createdAt: '2023-06-06T02:21:32.000Z'
    data:
      edited: false
      editors:
      - yizer16
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9327754378318787
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/EFpV2gb6jQKjXouKxLzhM.jpeg?w=200&h=200&f=face
          fullname: ZheYuan
          isHf: false
          isPro: false
          name: yizer16
          type: user
        html: '<blockquote>

          <p>Three more things to note: </p>

          <ol>

          <li>The GPTQ parameters don''t have any effect for AutoGPTQ models</li>

          <li>This 40B model requires more than 24GB VRAM. So you will have to use
          CPU offloading</li>

          <li>It''s slow as hell at the moment!  Even with enough VRAM (eg on a 48GB
          card), I was getting less than 1 tokens/s.</li>

          </ol>

          </blockquote>

          <p>How to open CPU offloading, is there any possiable run this model in
          4090 24G?</p>

          '
        raw: "> Three more things to note: \n> 1. The GPTQ parameters don't have any\
          \ effect for AutoGPTQ models\n> 2. This 40B model requires more than 24GB\
          \ VRAM. So you will have to use CPU offloading\n> 3. It's slow as hell at\
          \ the moment!  Even with enough VRAM (eg on a 48GB card), I was getting\
          \ less than 1 tokens/s.\n\nHow to open CPU offloading, is there any possiable\
          \ run this model in 4090 24G?"
        updatedAt: '2023-06-06T02:21:32.292Z'
      numEdits: 0
      reactions: []
    id: 647e982cf14eafc3b4659ff0
    type: comment
  author: yizer16
  content: "> Three more things to note: \n> 1. The GPTQ parameters don't have any\
    \ effect for AutoGPTQ models\n> 2. This 40B model requires more than 24GB VRAM.\
    \ So you will have to use CPU offloading\n> 3. It's slow as hell at the moment!\
    \  Even with enough VRAM (eg on a 48GB card), I was getting less than 1 tokens/s.\n\
    \nHow to open CPU offloading, is there any possiable run this model in 4090 24G?"
  created_at: 2023-06-06 01:21:32+00:00
  edited: false
  hidden: false
  id: 647e982cf14eafc3b4659ff0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2f4a171b1f15d4e9c8a49a2b6844eb90.svg
      fullname: Plaban Nayak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Plaban81
      type: user
    createdAt: '2023-06-06T11:30:56.000Z'
    data:
      edited: false
      editors:
      - Plaban81
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2913632392883301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2f4a171b1f15d4e9c8a49a2b6844eb90.svg
          fullname: Plaban Nayak
          isHf: false
          isPro: false
          name: Plaban81
          type: user
        html: "<p>How to run it in google colab. I encounter the below error,Could\
          \ you please help to resolve the issue. Running with A100 GPU instance .<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E<br>\u2502  /opt/conda/lib/python3.7/site-packages/auto_gptq/modeling/_base.py:182\
          \                          \u2502<br>\u2502                 if (pos_ids\
          \ := kwargs.get(\"position_ids\", None)) is not None:                  \
          \  \u2502<br>\u2502                             \u25B2                 \
          \                                                   \u2502<br>\u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u256F<br>SyntaxError: invalid syntax</p>\n<h1 id=\"code-used\">Code\
          \ used </h1>\n<p>! BUILD_CUDA_EXT=0 pip install auto-gptq<br>import torch<br>from\
          \ transformers import AutoTokenizer<br>from auto_gptq import AutoGPTQForCausalLM</p>\n\
          <h1 id=\"download-the-model-from-hf-and-store-it-locally-then-reference-its-location-here\"\
          >Download the model from HF and store it locally, then reference its location\
          \ here:</h1>\n<p>quantized_model_dir = \"/TheBloke/falcon-40b-instruct-GPTQ\"\
          </p>\n<p>from transformers import AutoTokenizer<br>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=False)</p>\n<p>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>\
          \                                           device=\"cuda:0\",<br>     \
          \                                      use_triton=False,<br>           \
          \                                use_safetensors=True,<br>             \
          \                              torch_dtype=torch.float32,<br>          \
          \                                 trust_remote_code=True)</p>\n"
        raw: "How to run it in google colab. I encounter the below error,Could you\
          \ please help to resolve the issue. Running with A100 GPU instance .\n\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502  /opt/conda/lib/python3.7/site-packages/auto_gptq/modeling/_base.py:182\
          \                          \u2502\n\u2502                 if (pos_ids :=\
          \ kwargs.get(\"position_ids\", None)) is not None:                    \u2502\
          \n\u2502                             \u25B2                            \
          \                                        \u2502\n\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          \nSyntaxError: invalid syntax\n\nCode used \n========\n! BUILD_CUDA_EXT=0\
          \ pip install auto-gptq\nimport torch\nfrom transformers import AutoTokenizer\n\
          from auto_gptq import AutoGPTQForCausalLM\n\n# Download the model from HF\
          \ and store it locally, then reference its location here:\nquantized_model_dir\
          \ = \"/TheBloke/falcon-40b-instruct-GPTQ\"\n\nfrom transformers import AutoTokenizer\n\
          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, \n   \
          \                                        device=\"cuda:0\", \n         \
          \                                  use_triton=False, \n                \
          \                           use_safetensors=True, \n                   \
          \                        torch_dtype=torch.float32, \n                 \
          \                          trust_remote_code=True)"
        updatedAt: '2023-06-06T11:30:56.698Z'
      numEdits: 0
      reactions: []
    id: 647f18f0f41cf810e3771af0
    type: comment
  author: Plaban81
  content: "How to run it in google colab. I encounter the below error,Could you please\
    \ help to resolve the issue. Running with A100 GPU instance .\n\u256D\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256E\n\u2502  /opt/conda/lib/python3.7/site-packages/auto_gptq/modeling/_base.py:182\
    \                          \u2502\n\u2502                 if (pos_ids := kwargs.get(\"\
    position_ids\", None)) is not None:                    \u2502\n\u2502        \
    \                     \u25B2                                                 \
    \                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
    \nSyntaxError: invalid syntax\n\nCode used \n========\n! BUILD_CUDA_EXT=0 pip\
    \ install auto-gptq\nimport torch\nfrom transformers import AutoTokenizer\nfrom\
    \ auto_gptq import AutoGPTQForCausalLM\n\n# Download the model from HF and store\
    \ it locally, then reference its location here:\nquantized_model_dir = \"/TheBloke/falcon-40b-instruct-GPTQ\"\
    \n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=False)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
    \ \n                                           device=\"cuda:0\", \n         \
    \                                  use_triton=False, \n                      \
    \                     use_safetensors=True, \n                               \
    \            torch_dtype=torch.float32, \n                                   \
    \        trust_remote_code=True)"
  created_at: 2023-06-06 10:30:56+00:00
  edited: false
  hidden: false
  id: 647f18f0f41cf810e3771af0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T00:26:03.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7668275833129883
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>The models fully loads on my 3090 in WSL2 with text-gen-webui, using
          AutoGPTQ.</p>

          '
        raw: The models fully loads on my 3090 in WSL2 with text-gen-webui, using
          AutoGPTQ.
        updatedAt: '2023-06-07T00:26:03.176Z'
      numEdits: 0
      reactions: []
    id: 647fce9b1637c1c0e6f1cb26
    type: comment
  author: mancub
  content: The models fully loads on my 3090 in WSL2 with text-gen-webui, using AutoGPTQ.
  created_at: 2023-06-06 23:26:03+00:00
  edited: false
  hidden: false
  id: 647fce9b1637c1c0e6f1cb26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T08:47:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.773306667804718
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Plaban81&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Plaban81\">@<span class=\"\
          underline\">Plaban81</span></a></span>\n\n\t</span></span> you appear to\
          \ be compiling AutoGPTQ with the CUDA extension not compiled. That will\
          \ kill performance.  </p>\n<p>Please try:</p>\n<pre><code>pip uninstall\
          \ -y auto-gptq\npip install auto-gptq --no-cache-dir\n</code></pre>\n<p>And\
          \ report back.  AutoGPTQ just released version 0.2.2 which fixes some installation\
          \ issues.</p>\n"
        raw: "@Plaban81 you appear to be compiling AutoGPTQ with the CUDA extension\
          \ not compiled. That will kill performance.  \n\nPlease try:\n\n```\npip\
          \ uninstall -y auto-gptq\npip install auto-gptq --no-cache-dir\n```\n\n\
          And report back.  AutoGPTQ just released version 0.2.2 which fixes some\
          \ installation issues."
        updatedAt: '2023-06-08T08:47:59.684Z'
      numEdits: 0
      reactions: []
    id: 648195bf6f283a7468617751
    type: comment
  author: TheBloke
  content: "@Plaban81 you appear to be compiling AutoGPTQ with the CUDA extension\
    \ not compiled. That will kill performance.  \n\nPlease try:\n\n```\npip uninstall\
    \ -y auto-gptq\npip install auto-gptq --no-cache-dir\n```\n\nAnd report back.\
    \  AutoGPTQ just released version 0.2.2 which fixes some installation issues."
  created_at: 2023-06-08 07:47:59+00:00
  edited: false
  hidden: false
  id: 648195bf6f283a7468617751
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-09T00:05:13.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6450203657150269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Compiling AutoGPTQ in 0.3.0.dev0 from source still does not mark
          the module +cuXXX.</p>

          '
        raw: Compiling AutoGPTQ in 0.3.0.dev0 from source still does not mark the
          module +cuXXX.
        updatedAt: '2023-06-09T00:05:13.212Z'
      numEdits: 0
      reactions: []
    id: 64826cb999110097e537ea67
    type: comment
  author: mancub
  content: Compiling AutoGPTQ in 0.3.0.dev0 from source still does not mark the module
    +cuXXX.
  created_at: 2023-06-08 23:05:13+00:00
  edited: false
  hidden: false
  id: 64826cb999110097e537ea67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T00:10:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8432275652885437
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah he still hasn''t fixed that. But it does compile the module;
          or should.</p>

          <p>There''s a simple test as to whether you have the CUDA extension installed:</p>

          <pre><code>$ python -c ''import torch ; import autogptq_cuda''

          $

          </code></pre>

          <p>If that returns no output, it should be OK.</p>

          '
        raw: 'Yeah he still hasn''t fixed that. But it does compile the module; or
          should.


          There''s a simple test as to whether you have the CUDA extension installed:


          ```

          $ python -c ''import torch ; import autogptq_cuda''

          $

          ```


          If that returns no output, it should be OK.'
        updatedAt: '2023-06-09T00:10:49.488Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LeBot
    id: 64826e0947a05b7204626983
    type: comment
  author: TheBloke
  content: 'Yeah he still hasn''t fixed that. But it does compile the module; or should.


    There''s a simple test as to whether you have the CUDA extension installed:


    ```

    $ python -c ''import torch ; import autogptq_cuda''

    $

    ```


    If that returns no output, it should be OK.'
  created_at: 2023-06-08 23:10:49+00:00
  edited: false
  hidden: false
  id: 64826e0947a05b7204626983
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/falcon-40b-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Unfortunately I can't run on text-generation-webui
