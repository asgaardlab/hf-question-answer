!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KrisPi
conflicting_files: null
created_at: 2023-10-13 11:34:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9554e7f29422dc00d8c90e44c1ef330.svg
      fullname: Kris Podkanowicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KrisPi
      type: user
    createdAt: '2023-10-13T12:34:42.000Z'
    data:
      edited: false
      editors:
      - KrisPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8915393948554993
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9554e7f29422dc00d8c90e44c1ef330.svg
          fullname: Kris Podkanowicz
          isHf: false
          isPro: false
          name: KrisPi
          type: user
        html: '<p>Finally managed to run HumanEval for 5bits and the different calibration
          presets you created:<br>Exllama supports greedy decoding by specifying tok_k
          = 1; which I did, there is very slight randomness still but it''s a character
          or two.</p>

          <p>The benefit is really significant - in Transformers 8 bit(!!!) I cannot
          cross 70% with this model. 4bit runs 68%</p>

          <p>Your Evol quant is also much better than the ones I tried to recreate,
          the best I got is 70.7% while yours is very close to the official fp16 reported
          numbers (73% and btw. the community has a hard time recreating this result)</p>

          <p>Congrats! Would you also share some details on what makes your quant
          special? I..e exact file for calibration and commit of Exllamav2 if possible?</p>

          <p>Wiki<br>Base<br>{''pass@1'': 0.6890243902439024}<br>Base + Extra<br>{''pass@1'':
          0.6524390243902439}</p>

          <p>Evol<br>{''pass@1'': 0.725609756097561}<br>Base + Extra<br>{''pass@1'':
          0.6707317073170732}</p>

          <p>REDO<br>Base<br>{''pass@1'': 0.7195121951219512}<br>Base + Extra<br>{''pass@1'':
          0.6707317073170732}</p>

          '
        raw: "Finally managed to run HumanEval for 5bits and the different calibration\
          \ presets you created:\r\nExllama supports greedy decoding by specifying\
          \ tok_k = 1; which I did, there is very slight randomness still but it's\
          \ a character or two.\r\n\r\nThe benefit is really significant - in Transformers\
          \ 8 bit(!!!) I cannot cross 70% with this model. 4bit runs 68%\r\n\r\nYour\
          \ Evol quant is also much better than the ones I tried to recreate, the\
          \ best I got is 70.7% while yours is very close to the official fp16 reported\
          \ numbers (73% and btw. the community has a hard time recreating this result)\r\
          \n\r\nCongrats! Would you also share some details on what makes your quant\
          \ special? I..e exact file for calibration and commit of Exllamav2 if possible?\r\
          \n\r\nWiki\r\nBase\r\n{'pass@1': 0.6890243902439024}\r\nBase + Extra\r\n\
          {'pass@1': 0.6524390243902439}\r\n\r\nEvol\r\n{'pass@1': 0.725609756097561}\r\
          \nBase + Extra\r\n{'pass@1': 0.6707317073170732}\r\n\r\nREDO\r\nBase\r\n\
          {'pass@1': 0.7195121951219512}\r\nBase + Extra\r\n{'pass@1': 0.6707317073170732}"
        updatedAt: '2023-10-13T12:34:42.231Z'
      numEdits: 0
      reactions: []
    id: 65293962ad4a7ea3a53df948
    type: comment
  author: KrisPi
  content: "Finally managed to run HumanEval for 5bits and the different calibration\
    \ presets you created:\r\nExllama supports greedy decoding by specifying tok_k\
    \ = 1; which I did, there is very slight randomness still but it's a character\
    \ or two.\r\n\r\nThe benefit is really significant - in Transformers 8 bit(!!!)\
    \ I cannot cross 70% with this model. 4bit runs 68%\r\n\r\nYour Evol quant is\
    \ also much better than the ones I tried to recreate, the best I got is 70.7%\
    \ while yours is very close to the official fp16 reported numbers (73% and btw.\
    \ the community has a hard time recreating this result)\r\n\r\nCongrats! Would\
    \ you also share some details on what makes your quant special? I..e exact file\
    \ for calibration and commit of Exllamav2 if possible?\r\n\r\nWiki\r\nBase\r\n\
    {'pass@1': 0.6890243902439024}\r\nBase + Extra\r\n{'pass@1': 0.6524390243902439}\r\
    \n\r\nEvol\r\n{'pass@1': 0.725609756097561}\r\nBase + Extra\r\n{'pass@1': 0.6707317073170732}\r\
    \n\r\nREDO\r\nBase\r\n{'pass@1': 0.7195121951219512}\r\nBase + Extra\r\n{'pass@1':\
    \ 0.6707317073170732}"
  created_at: 2023-10-13 11:34:42+00:00
  edited: false
  hidden: false
  id: 65293962ad4a7ea3a53df948
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
      fullname: Vladimir Zorin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: latimar
      type: user
    createdAt: '2023-10-16T15:30:34.000Z'
    data:
      edited: false
      editors:
      - latimar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9565476179122925
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
          fullname: Vladimir Zorin
          isHf: false
          isPro: false
          name: latimar
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KrisPi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/KrisPi\">@<span class=\"\
          underline\">KrisPi</span></a></span>\n\n\t</span></span> Calibration dataset\
          \ for evol quant is linked in the README, it's <code>wizardLM-evol-instruct_70k</code>.\
          \  As for the commit hash of Exllama2 -- unfortunately I can't tell exactly,\
          \ I've updated exllama2 many times since then. All I can say is that I was\
          \ using the latest exllama2 at the moment, and it was about a month ago.\
          \ Interesting findings about the evol quant, I did not think calibration\
          \ dataset would matter much. Guess I'll make some more 4bpw quants using\
          \ evol-instruct for calibration. </p>\n"
        raw: '@KrisPi Calibration dataset for evol quant is linked in the README,
          it''s `wizardLM-evol-instruct_70k`.  As for the commit hash of Exllama2
          -- unfortunately I can''t tell exactly, I''ve updated exllama2 many times
          since then. All I can say is that I was using the latest exllama2 at the
          moment, and it was about a month ago. Interesting findings about the evol
          quant, I did not think calibration dataset would matter much. Guess I''ll
          make some more 4bpw quants using evol-instruct for calibration. '
        updatedAt: '2023-10-16T15:30:34.234Z'
      numEdits: 0
      reactions: []
    id: 652d571aa3da41257d4d33b4
    type: comment
  author: latimar
  content: '@KrisPi Calibration dataset for evol quant is linked in the README, it''s
    `wizardLM-evol-instruct_70k`.  As for the commit hash of Exllama2 -- unfortunately
    I can''t tell exactly, I''ve updated exllama2 many times since then. All I can
    say is that I was using the latest exllama2 at the moment, and it was about a
    month ago. Interesting findings about the evol quant, I did not think calibration
    dataset would matter much. Guess I''ll make some more 4bpw quants using evol-instruct
    for calibration. '
  created_at: 2023-10-16 14:30:34+00:00
  edited: false
  hidden: false
  id: 652d571aa3da41257d4d33b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
      fullname: Vladimir Zorin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: latimar
      type: user
    createdAt: '2023-10-20T12:21:46.000Z'
    data:
      edited: false
      editors:
      - latimar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7946876287460327
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
          fullname: Vladimir Zorin
          isHf: false
          isPro: false
          name: latimar
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KrisPi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/KrisPi\">@<span class=\"\
          underline\">KrisPi</span></a></span>\n\n\t</span></span> I made new quants\
          \ using megacode dataset, they seem to be much better. <a href=\"https://huggingface.co/latimar/Phind-Codellama-34B-v2-megacode-exl2\"\
          >https://huggingface.co/latimar/Phind-Codellama-34B-v2-megacode-exl2</a></p>\n"
        raw: '@KrisPi I made new quants using megacode dataset, they seem to be much
          better. https://huggingface.co/latimar/Phind-Codellama-34B-v2-megacode-exl2'
        updatedAt: '2023-10-20T12:21:46.917Z'
      numEdits: 0
      reactions: []
    id: 653270daed74ace6335a9819
    type: comment
  author: latimar
  content: '@KrisPi I made new quants using megacode dataset, they seem to be much
    better. https://huggingface.co/latimar/Phind-Codellama-34B-v2-megacode-exl2'
  created_at: 2023-10-20 11:21:46+00:00
  edited: false
  hidden: false
  id: 653270daed74ace6335a9819
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
      fullname: Vladimir Zorin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: latimar
      type: user
    createdAt: '2023-10-20T12:21:52.000Z'
    data:
      status: closed
    id: 653270e0575cd7a775e84490
    type: status-change
  author: latimar
  created_at: 2023-10-20 11:21:52+00:00
  id: 653270e0575cd7a775e84490
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: latimar/Phind-Codellama-34B-v2-exl2
repo_type: model
status: closed
target_branch: null
title: Confirming higher HumanEval for Evol instruct 5bit quant vs. Wikitext based
  one.
