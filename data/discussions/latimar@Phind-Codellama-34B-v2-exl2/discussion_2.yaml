!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SekkSea
conflicting_files: null
created_at: 2023-09-19 02:00:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b6689c0c08083a26b4b251a0a9d37ca.svg
      fullname: Edward Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SekkSea
      type: user
    createdAt: '2023-09-19T03:00:00.000Z'
    data:
      edited: false
      editors:
      - SekkSea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9379343390464783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b6689c0c08083a26b4b251a0a9d37ca.svg
          fullname: Edward Smith
          isHf: false
          isPro: false
          name: SekkSea
          type: user
        html: '<p>This is the first time I have been able to load a 34b model on my
          budget 3060! With 12gb of vram, the 2.55 bit variation mostly loads on my
          GPU, with a little spilling over into the CPU at 2048 context.</p>

          '
        raw: This is the first time I have been able to load a 34b model on my budget
          3060! With 12gb of vram, the 2.55 bit variation mostly loads on my GPU,
          with a little spilling over into the CPU at 2048 context.
        updatedAt: '2023-09-19T03:00:00.865Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - victor
      - count: 1
        reaction: "\U0001F917"
        users:
        - latimar
    id: 65090eb0abdde5290e6bacfe
    type: comment
  author: SekkSea
  content: This is the first time I have been able to load a 34b model on my budget
    3060! With 12gb of vram, the 2.55 bit variation mostly loads on my GPU, with a
    little spilling over into the CPU at 2048 context.
  created_at: 2023-09-19 02:00:00+00:00
  edited: false
  hidden: false
  id: 65090eb0abdde5290e6bacfe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e55c2053d58df3fcef8b18fc8958c31.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ixel1
      type: user
    createdAt: '2023-09-20T12:50:38.000Z'
    data:
      edited: false
      editors:
      - Ixel1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9415720105171204
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e55c2053d58df3fcef8b18fc8958c31.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Ixel1
          type: user
        html: '<p>Indeed, this is the best AI model I''ve used so far which also fits
          on a single 3090. I''m using the 5_0-bpw-h8-evol-ins variant. Thanks from
          me too.</p>

          '
        raw: Indeed, this is the best AI model I've used so far which also fits on
          a single 3090. I'm using the 5_0-bpw-h8-evol-ins variant. Thanks from me
          too.
        updatedAt: '2023-09-20T12:50:38.182Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - latimar
    id: 650aea9e7f89a2bf35ba0689
    type: comment
  author: Ixel1
  content: Indeed, this is the best AI model I've used so far which also fits on a
    single 3090. I'm using the 5_0-bpw-h8-evol-ins variant. Thanks from me too.
  created_at: 2023-09-20 11:50:38+00:00
  edited: false
  hidden: false
  id: 650aea9e7f89a2bf35ba0689
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
      fullname: Vladimir Zorin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: latimar
      type: user
    createdAt: '2023-09-21T18:09:59.000Z'
    data:
      edited: false
      editors:
      - latimar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9481335282325745
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
          fullname: Vladimir Zorin
          isHf: false
          isPro: false
          name: latimar
          type: user
        html: "<p>wow, glad it's useful.<br><span data-props=\"{&quot;user&quot;:&quot;SekkSea&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SekkSea\"\
          >@<span class=\"underline\">SekkSea</span></a></span>\n\n\t</span></span>\
          \   what's your experience with 2.55 variant? Is it actually usable and\
          \ helpful?<br>It's a shame Phind did not make a 13B variant of the model,\
          \ I'd love to compare compare 2.55 quant of 34B model with different quants\
          \ of 13B...</p>\n"
        raw: "wow, glad it's useful. \n@SekkSea   what's your experience with 2.55\
          \ variant? Is it actually usable and helpful? \nIt's a shame Phind did not\
          \ make a 13B variant of the model, I'd love to compare compare 2.55 quant\
          \ of 34B model with different quants of 13B..."
        updatedAt: '2023-09-21T18:09:59.048Z'
      numEdits: 0
      reactions: []
    id: 650c86f781637a0a91615bf6
    type: comment
  author: latimar
  content: "wow, glad it's useful. \n@SekkSea   what's your experience with 2.55 variant?\
    \ Is it actually usable and helpful? \nIt's a shame Phind did not make a 13B variant\
    \ of the model, I'd love to compare compare 2.55 quant of 34B model with different\
    \ quants of 13B..."
  created_at: 2023-09-21 17:09:59+00:00
  edited: false
  hidden: false
  id: 650c86f781637a0a91615bf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b6689c0c08083a26b4b251a0a9d37ca.svg
      fullname: Edward Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SekkSea
      type: user
    createdAt: '2023-09-24T15:51:14.000Z'
    data:
      edited: true
      editors:
      - SekkSea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9211965203285217
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b6689c0c08083a26b4b251a0a9d37ca.svg
          fullname: Edward Smith
          isHf: false
          isPro: false
          name: SekkSea
          type: user
        html: '<p>From what I''ve seen, I think the quality of 2.55-bit 34b exceeds
          comparable 6-bit or 8-bit 13b models, but that''s just my own subjective
          opinion. 34b models like this one are usable at 2 bpw, but the replies take
          a while, so it''s probably not the sweet spot for 12 VRAM. It''s fun to
          use on occasion, though, because of the higher quality responses.</p>

          <p>For the most part, I''m using 4 bpw 13b models for 4k context, 4.65 bpw
          13b models for 3k context, and 3 bpw 20b models for ~2k context.</p>

          '
        raw: 'From what I''ve seen, I think the quality of 2.55-bit 34b exceeds comparable
          6-bit or 8-bit 13b models, but that''s just my own subjective opinion. 34b
          models like this one are usable at 2 bpw, but the replies take a while,
          so it''s probably not the sweet spot for 12 VRAM. It''s fun to use on occasion,
          though, because of the higher quality responses.


          For the most part, I''m using 4 bpw 13b models for 4k context, 4.65 bpw
          13b models for 3k context, and 3 bpw 20b models for ~2k context.'
        updatedAt: '2023-09-24T16:16:15.886Z'
      numEdits: 5
      reactions: []
    id: 65105af2671604f140fdfebf
    type: comment
  author: SekkSea
  content: 'From what I''ve seen, I think the quality of 2.55-bit 34b exceeds comparable
    6-bit or 8-bit 13b models, but that''s just my own subjective opinion. 34b models
    like this one are usable at 2 bpw, but the replies take a while, so it''s probably
    not the sweet spot for 12 VRAM. It''s fun to use on occasion, though, because
    of the higher quality responses.


    For the most part, I''m using 4 bpw 13b models for 4k context, 4.65 bpw 13b models
    for 3k context, and 3 bpw 20b models for ~2k context.'
  created_at: 2023-09-24 14:51:14+00:00
  edited: true
  hidden: false
  id: 65105af2671604f140fdfebf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
      fullname: Richard Meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hisma
      type: user
    createdAt: '2023-10-25T15:14:18.000Z'
    data:
      edited: false
      editors:
      - Hisma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.932237982749939
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
          fullname: Richard Meyer
          isHf: false
          isPro: false
          name: Hisma
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;latimar&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/latimar\">@<span class=\"\
          underline\">latimar</span></a></span>\n\n\t</span></span>, can you or someone\
          \ else explain why the perplexity scores are worse on the \"5_0-bpw-h8-evol-ins\"\
          \ model versus the \"5_0-bpw-h8\" model?<br>I would assume fine-tuning the\
          \ model would improve the scores?<br>Also, in my personal non scientific\
          \ test, I give both LLMs a coding challenge, and the  \"5_0-bpw-h8-evol-ins\"\
          \ model gave a better response than the \"5_0-bpw-h8\" model.  So anecdotally,\
          \ \"5_0-bpw-h8-evol-ins\" is a better performing model for me, despite the\
          \ worse PPL score.</p>\n"
        raw: "@latimar, can you or someone else explain why the perplexity scores\
          \ are worse on the \"5_0-bpw-h8-evol-ins\" model versus the \"5_0-bpw-h8\"\
          \ model?\nI would assume fine-tuning the model would improve the scores?\
          \  \nAlso, in my personal non scientific test, I give both LLMs a coding\
          \ challenge, and the  \"5_0-bpw-h8-evol-ins\" model gave a better response\
          \ than the \"5_0-bpw-h8\" model.  So anecdotally, \"5_0-bpw-h8-evol-ins\"\
          \ is a better performing model for me, despite the worse PPL score.\n"
        updatedAt: '2023-10-25T15:14:18.626Z'
      numEdits: 0
      reactions: []
    id: 653930caee5888edef71cef4
    type: comment
  author: Hisma
  content: "@latimar, can you or someone else explain why the perplexity scores are\
    \ worse on the \"5_0-bpw-h8-evol-ins\" model versus the \"5_0-bpw-h8\" model?\n\
    I would assume fine-tuning the model would improve the scores?  \nAlso, in my\
    \ personal non scientific test, I give both LLMs a coding challenge, and the \
    \ \"5_0-bpw-h8-evol-ins\" model gave a better response than the \"5_0-bpw-h8\"\
    \ model.  So anecdotally, \"5_0-bpw-h8-evol-ins\" is a better performing model\
    \ for me, despite the worse PPL score.\n"
  created_at: 2023-10-25 14:14:18+00:00
  edited: false
  hidden: false
  id: 653930caee5888edef71cef4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
      fullname: Vladimir Zorin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: latimar
      type: user
    createdAt: '2023-10-25T21:25:11.000Z'
    data:
      edited: false
      editors:
      - latimar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.857942521572113
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
          fullname: Vladimir Zorin
          isHf: false
          isPro: false
          name: latimar
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Hisma&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Hisma\">@<span class=\"\
          underline\">Hisma</span></a></span>\n\n\t</span></span> <code>5_0-bpw-h8-evol-ins</code>\
          \ was converted using different calibration dataset, not wikitext, but evol-instruct.\
          \ It has worse ppl score on wikitext, yes, but its coding abilities are\
          \ actually  better that <code>5_0-bpw-h8</code>. The better metric to compare\
          \ different quants would be HumanEval score, or at least ppl score on evol-instruct\
          \ dataset.</p>\n"
        raw: '@Hisma `5_0-bpw-h8-evol-ins` was converted using different calibration
          dataset, not wikitext, but evol-instruct. It has worse ppl score on wikitext,
          yes, but its coding abilities are actually  better that `5_0-bpw-h8`. The
          better metric to compare different quants would be HumanEval score, or at
          least ppl score on evol-instruct dataset.'
        updatedAt: '2023-10-25T21:25:11.120Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Hisma
    id: 653987b70d111541df12ce88
    type: comment
  author: latimar
  content: '@Hisma `5_0-bpw-h8-evol-ins` was converted using different calibration
    dataset, not wikitext, but evol-instruct. It has worse ppl score on wikitext,
    yes, but its coding abilities are actually  better that `5_0-bpw-h8`. The better
    metric to compare different quants would be HumanEval score, or at least ppl score
    on evol-instruct dataset.'
  created_at: 2023-10-25 20:25:11+00:00
  edited: false
  hidden: false
  id: 653987b70d111541df12ce88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
      fullname: Richard Meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hisma
      type: user
    createdAt: '2023-10-25T23:34:34.000Z'
    data:
      edited: false
      editors:
      - Hisma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9680410027503967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
          fullname: Richard Meyer
          isHf: false
          isPro: false
          name: Hisma
          type: user
        html: '<p>Got it, thank you.  Would have been useful to include the humaneval
          scores with these models too like you did in your supercoder models.  But
          regardless, I can definitely confirm there is noticeablely better coding
          performance on <code>5_0-bpw-h8-evol-ins</code>, so based on what you''re
          saying this all makes sense.  Thank you for explaining!</p>

          '
        raw: Got it, thank you.  Would have been useful to include the humaneval scores
          with these models too like you did in your supercoder models.  But regardless,
          I can definitely confirm there is noticeablely better coding performance
          on `5_0-bpw-h8-evol-ins`, so based on what you're saying this all makes
          sense.  Thank you for explaining!
        updatedAt: '2023-10-25T23:34:34.333Z'
      numEdits: 0
      reactions: []
    id: 6539a60ad1ca3238d0de6328
    type: comment
  author: Hisma
  content: Got it, thank you.  Would have been useful to include the humaneval scores
    with these models too like you did in your supercoder models.  But regardless,
    I can definitely confirm there is noticeablely better coding performance on `5_0-bpw-h8-evol-ins`,
    so based on what you're saying this all makes sense.  Thank you for explaining!
  created_at: 2023-10-25 22:34:34+00:00
  edited: false
  hidden: false
  id: 6539a60ad1ca3238d0de6328
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
      fullname: Vladimir Zorin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: latimar
      type: user
    createdAt: '2023-10-26T10:11:44.000Z'
    data:
      edited: false
      editors:
      - latimar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9871910810470581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644464cf9504687e2d8788c8/SO_wpkGJfqviEkyLlOw7h.jpeg?w=200&h=200&f=face
          fullname: Vladimir Zorin
          isHf: false
          isPro: false
          name: latimar
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Hisma&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Hisma\">@<span class=\"\
          underline\">Hisma</span></a></span>\n\n\t</span></span> well, it was a first\
          \ naive attempt to quantize phind, I only measured PPL at the moment and\
          \ thought it was enough. I was young and stupid back then =) I'll probably\
          \ update the README in the repo to include HumanEval scores, but I want\
          \ to finish making new phind quants first. </p>\n"
        raw: '@Hisma well, it was a first naive attempt to quantize phind, I only
          measured PPL at the moment and thought it was enough. I was young and stupid
          back then =) I''ll probably update the README in the repo to include HumanEval
          scores, but I want to finish making new phind quants first. '
        updatedAt: '2023-10-26T10:11:44.852Z'
      numEdits: 0
      reactions: []
    id: 653a3b6071dc72e986732215
    type: comment
  author: latimar
  content: '@Hisma well, it was a first naive attempt to quantize phind, I only measured
    PPL at the moment and thought it was enough. I was young and stupid back then
    =) I''ll probably update the README in the repo to include HumanEval scores, but
    I want to finish making new phind quants first. '
  created_at: 2023-10-26 09:11:44+00:00
  edited: false
  hidden: false
  id: 653a3b6071dc72e986732215
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: latimar/Phind-Codellama-34B-v2-exl2
repo_type: model
status: open
target_branch: null
title: Thanks!
