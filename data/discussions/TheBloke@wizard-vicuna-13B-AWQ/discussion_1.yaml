!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Rohith1016
conflicting_files: null
created_at: 2023-10-19 09:11:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/214f490ef5705cde7a1f68df55d65388.svg
      fullname: RohithVaddeti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rohith1016
      type: user
    createdAt: '2023-10-19T10:11:35.000Z'
    data:
      edited: true
      editors:
      - Rohith1016
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8227626085281372
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/214f490ef5705cde7a1f68df55d65388.svg
          fullname: RohithVaddeti
          isHf: false
          isPro: false
          name: Rohith1016
          type: user
        html: '<p>Can you tell me the different of this all type of model :<br>GPTQ,
          GGUF, HF, GGML, AWQ, fp16<br>Uncensored GPTQ, Uncensored GGUF, Uncensored
          HF, Uncensored fp16, Uncensored AWQ, Uncensored GGML<br>And which one is
          better And fast Uncensored one or without Uncensored one and which model
          is fast and good on Google colab T4 GPU</p>

          '
        raw: 'Can you tell me the different of this all type of model :

          GPTQ, GGUF, HF, GGML, AWQ, fp16

          Uncensored GPTQ, Uncensored GGUF, Uncensored HF, Uncensored fp16, Uncensored
          AWQ, Uncensored GGML

          And which one is better And fast Uncensored one or without Uncensored one
          and which model is fast and good on Google colab T4 GPU'
        updatedAt: '2023-10-19T10:15:03.276Z'
      numEdits: 1
      reactions: []
    id: 653100d7575598d7255bfded
    type: comment
  author: Rohith1016
  content: 'Can you tell me the different of this all type of model :

    GPTQ, GGUF, HF, GGML, AWQ, fp16

    Uncensored GPTQ, Uncensored GGUF, Uncensored HF, Uncensored fp16, Uncensored AWQ,
    Uncensored GGML

    And which one is better And fast Uncensored one or without Uncensored one and
    which model is fast and good on Google colab T4 GPU'
  created_at: 2023-10-19 09:11:35+00:00
  edited: true
  hidden: false
  id: 653100d7575598d7255bfded
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-19T15:30:16.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9031499028205872
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p>Uncensored just means that it will try to answer questions and not\
          \ refuse nsfw questions. It\u2019s not a format</p>\n<p>Gguf is a format\
          \ for llama cpp best for cpu or Mac<br>Gptq with exllama will be fastest\
          \ format and<br>Awq should be Hughes quality<br>Fp16 is the original model\
          \ unquantized<br>Ggml is an old format similar to gguf<br>Hf is a format\
          \ designed for huggingface transformers which is usually fp16</p>\n"
        raw: "Uncensored just means that it will try to answer questions and not refuse\
          \ nsfw questions. It\u2019s not a format\n\nGguf is a format for llama cpp\
          \ best for cpu or Mac\nGptq with exllama will be fastest format and\nAwq\
          \ should be Hughes quality\nFp16 is the original model unquantized\nGgml\
          \ is an old format similar to gguf\nHf is a format designed for huggingface\
          \ transformers which is usually fp16"
        updatedAt: '2023-10-19T15:30:16.078Z'
      numEdits: 0
      reactions: []
    id: 65314b884380f354397cabe2
    type: comment
  author: YaTharThShaRma999
  content: "Uncensored just means that it will try to answer questions and not refuse\
    \ nsfw questions. It\u2019s not a format\n\nGguf is a format for llama cpp best\
    \ for cpu or Mac\nGptq with exllama will be fastest format and\nAwq should be\
    \ Hughes quality\nFp16 is the original model unquantized\nGgml is an old format\
    \ similar to gguf\nHf is a format designed for huggingface transformers which\
    \ is usually fp16"
  created_at: 2023-10-19 14:30:16+00:00
  edited: false
  hidden: false
  id: 65314b884380f354397cabe2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/214f490ef5705cde7a1f68df55d65388.svg
      fullname: RohithVaddeti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rohith1016
      type: user
    createdAt: '2023-10-20T09:23:41.000Z'
    data:
      edited: false
      editors:
      - Rohith1016
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7214346528053284
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/214f490ef5705cde7a1f68df55d65388.svg
          fullname: RohithVaddeti
          isHf: false
          isPro: false
          name: Rohith1016
          type: user
        html: '<p>Thank you for your Response and explanation @johnwick123forevr </p>

          '
        raw: 'Thank you for your Response and explanation @johnwick123forevr '
        updatedAt: '2023-10-20T09:23:41.778Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6532471da70a88eb1e5c1453
    id: 6532471da70a88eb1e5c1451
    type: comment
  author: Rohith1016
  content: 'Thank you for your Response and explanation @johnwick123forevr '
  created_at: 2023-10-20 08:23:41+00:00
  edited: false
  hidden: false
  id: 6532471da70a88eb1e5c1451
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/214f490ef5705cde7a1f68df55d65388.svg
      fullname: RohithVaddeti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rohith1016
      type: user
    createdAt: '2023-10-20T09:23:41.000Z'
    data:
      status: closed
    id: 6532471da70a88eb1e5c1453
    type: status-change
  author: Rohith1016
  created_at: 2023-10-20 08:23:41+00:00
  id: 6532471da70a88eb1e5c1453
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/214f490ef5705cde7a1f68df55d65388.svg
      fullname: RohithVaddeti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rohith1016
      type: user
    createdAt: '2023-10-20T09:24:08.000Z'
    data:
      status: open
    id: 65324738d477c43e79acc417
    type: status-change
  author: Rohith1016
  created_at: 2023-10-20 08:24:08+00:00
  id: 65324738d477c43e79acc417
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/214f490ef5705cde7a1f68df55d65388.svg
      fullname: RohithVaddeti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rohith1016
      type: user
    createdAt: '2023-10-20T11:26:41.000Z'
    data:
      status: closed
    id: 653263f1e78972593350aa15
    type: status-change
  author: Rohith1016
  created_at: 2023-10-20 10:26:41+00:00
  id: 653263f1e78972593350aa15
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/wizard-vicuna-13B-AWQ
repo_type: model
status: closed
target_branch: null
title: Details about GPTQ, GGUF, HF, GGML, AWQ, fp16, Uncensored of each
