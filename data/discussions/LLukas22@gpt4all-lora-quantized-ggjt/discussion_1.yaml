!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Dannyboy55
conflicting_files: null
created_at: 2023-04-14 19:36:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea85087708b0a76c67de40eaf6ebde5b.svg
      fullname: 'danny crawford '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dannyboy55
      type: user
    createdAt: '2023-04-14T20:36:26.000Z'
    data:
      edited: false
      editors:
      - Dannyboy55
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea85087708b0a76c67de40eaf6ebde5b.svg
          fullname: 'danny crawford '
          isHf: false
          isPro: false
          name: Dannyboy55
          type: user
        html: '<p>I''m new to this discussion, but the model works pretty good on
          my PC.<br>However, I noticed that when running, it would just shut down
          abruptly. After working with it a little more, I found that it was hitting
          the token limit, then automatically shutting down. </p>

          <p>Is there a way to create a database where it can automatically access
          previous discussions to reference for current conversations, or is there
          a way to expand the token limitations? I''ve already asked it how to create
          a database, but the code it gave was full of bugs. </p>

          <p>Overall, it works pretty good, so thumbs up!</p>

          '
        raw: "I'm new to this discussion, but the model works pretty good on my PC.\
          \ \r\nHowever, I noticed that when running, it would just shut down abruptly.\
          \ After working with it a little more, I found that it was hitting the token\
          \ limit, then automatically shutting down. \r\n\r\nIs there a way to create\
          \ a database where it can automatically access previous discussions to reference\
          \ for current conversations, or is there a way to expand the token limitations?\
          \ I've already asked it how to create a database, but the code it gave was\
          \ full of bugs. \r\n\r\nOverall, it works pretty good, so thumbs up!"
        updatedAt: '2023-04-14T20:36:26.115Z'
      numEdits: 0
      reactions: []
    id: 6439b94abb7ded0a0ff1bd3f
    type: comment
  author: Dannyboy55
  content: "I'm new to this discussion, but the model works pretty good on my PC.\
    \ \r\nHowever, I noticed that when running, it would just shut down abruptly.\
    \ After working with it a little more, I found that it was hitting the token limit,\
    \ then automatically shutting down. \r\n\r\nIs there a way to create a database\
    \ where it can automatically access previous discussions to reference for current\
    \ conversations, or is there a way to expand the token limitations? I've already\
    \ asked it how to create a database, but the code it gave was full of bugs. \r\
    \n\r\nOverall, it works pretty good, so thumbs up!"
  created_at: 2023-04-14 19:36:26+00:00
  edited: false
  hidden: false
  id: 6439b94abb7ded0a0ff1bd3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-04-15T11:27:52.000Z'
    data:
      edited: false
      editors:
      - LLukas22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Dannyboy55&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Dannyboy55\">@<span class=\"\
          underline\">Dannyboy55</span></a></span>\n\n\t</span></span> The token limit\
          \ of this model is 2048 token, which is similar to nearly all other LLaMA\
          \ based models. </p>\n<p>The only way of expanding the token limit is to\
          \ use a bigger model but this significantly increases the computational\
          \ requirements for inference.</p>\n<p>A common way to artificially increase\
          \ the token length is to check the length of a conversation up until now,\
          \ and then prompt the model to summarize the conversation and use the summarized\
          \ conversation instead of the actual conversation.  An example of this can\
          \ be found <a rel=\"nofollow\" href=\"https://github.com/deep-diver/Alpaca-LoRA-Serve\"\
          >here</a>.</p>\n<p>Another approach would be to archive the conversations\
          \ in a vector database and then retrieve them if a topic similar to them\
          \ comes up again. But you will never be able to pass more than 2048 Tokens\
          \ at once to the model. </p>\n<p>If you want to generate code you probably\
          \ shouldn't be using this model, maybe look into a finetuned code generation\
          \ model like <a href=\"https://huggingface.co/Salesforce/codegen-350M-multi\"\
          >Salesforce/codegen</a>.</p>\n"
        raw: "@Dannyboy55 The token limit of this model is 2048 token, which is similar\
          \ to nearly all other LLaMA based models. \n\nThe only way of expanding\
          \ the token limit is to use a bigger model but this significantly increases\
          \ the computational requirements for inference.\n\nA common way to artificially\
          \ increase the token length is to check the length of a conversation up\
          \ until now, and then prompt the model to summarize the conversation and\
          \ use the summarized conversation instead of the actual conversation.  An\
          \ example of this can be found [here](https://github.com/deep-diver/Alpaca-LoRA-Serve).\n\
          \nAnother approach would be to archive the conversations in a vector database\
          \ and then retrieve them if a topic similar to them comes up again. But\
          \ you will never be able to pass more than 2048 Tokens at once to the model.\
          \ \n\nIf you want to generate code you probably shouldn't be using this\
          \ model, maybe look into a finetuned code generation model like [Salesforce/codegen](https://huggingface.co/Salesforce/codegen-350M-multi)."
        updatedAt: '2023-04-15T11:27:52.114Z'
      numEdits: 0
      reactions: []
    id: 643a8a3818e9973bc81ef550
    type: comment
  author: LLukas22
  content: "@Dannyboy55 The token limit of this model is 2048 token, which is similar\
    \ to nearly all other LLaMA based models. \n\nThe only way of expanding the token\
    \ limit is to use a bigger model but this significantly increases the computational\
    \ requirements for inference.\n\nA common way to artificially increase the token\
    \ length is to check the length of a conversation up until now, and then prompt\
    \ the model to summarize the conversation and use the summarized conversation\
    \ instead of the actual conversation.  An example of this can be found [here](https://github.com/deep-diver/Alpaca-LoRA-Serve).\n\
    \nAnother approach would be to archive the conversations in a vector database\
    \ and then retrieve them if a topic similar to them comes up again. But you will\
    \ never be able to pass more than 2048 Tokens at once to the model. \n\nIf you\
    \ want to generate code you probably shouldn't be using this model, maybe look\
    \ into a finetuned code generation model like [Salesforce/codegen](https://huggingface.co/Salesforce/codegen-350M-multi)."
  created_at: 2023-04-15 10:27:52+00:00
  edited: false
  hidden: false
  id: 643a8a3818e9973bc81ef550
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-04-29T18:30:32.000Z'
    data:
      status: closed
    id: 644d6248fa94e93b0eca1ef0
    type: status-change
  author: LLukas22
  created_at: 2023-04-29 17:30:32+00:00
  id: 644d6248fa94e93b0eca1ef0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LLukas22/gpt4all-lora-quantized-ggjt
repo_type: model
status: closed
target_branch: null
title: 'Is there a way to create a database for this model as its token limit is very
  short. '
