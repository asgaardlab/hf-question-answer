!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheLeCraft999
conflicting_files: null
created_at: 2023-04-29 14:58:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/GHZ1fyXzaama0OTkA1A2p.jpeg?w=200&h=200&f=face
      fullname: TheLeCraft999
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheLeCraft999
      type: user
    createdAt: '2023-04-29T15:58:13.000Z'
    data:
      edited: false
      editors:
      - TheLeCraft999
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/GHZ1fyXzaama0OTkA1A2p.jpeg?w=200&h=200&f=face
          fullname: TheLeCraft999
          isHf: false
          isPro: false
          name: TheLeCraft999
          type: user
        html: '<p>I tried to use the model and evertime i get when i print out "result"
          something like this: &lt;generator object Model.generate at 0x000001EF1A79CA00&gt;<br>can
          you tell me the problem please?</p>

          '
        raw: "I tried to use the model and evertime i get when i print out \"result\"\
          \ something like this: <generator object Model.generate at 0x000001EF1A79CA00>\r\
          \ncan you tell me the problem please?"
        updatedAt: '2023-04-29T15:58:13.744Z'
      numEdits: 0
      reactions: []
    id: 644d3e951e0cef1987159c88
    type: comment
  author: TheLeCraft999
  content: "I tried to use the model and evertime i get when i print out \"result\"\
    \ something like this: <generator object Model.generate at 0x000001EF1A79CA00>\r\
    \ncan you tell me the problem please?"
  created_at: 2023-04-29 14:58:13+00:00
  edited: false
  hidden: false
  id: 644d3e951e0cef1987159c88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-04-29T16:21:35.000Z'
    data:
      edited: false
      editors:
      - LLukas22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
          fullname: Lukas Kreussel
          isHf: false
          isPro: false
          name: LLukas22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheLeCraft999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheLeCraft999\"\
          >@<span class=\"underline\">TheLeCraft999</span></a></span>\n\n\t</span></span>\
          \ From your question i guess you are new to python, so i tried to keep it\
          \ simple.</p>\n<p><code>pyllamacpp</code> had some changes, the result object\
          \ now seams to be a python c object. I would recommend switching to <a rel=\"\
          nofollow\" href=\"https://github.com/abetlen/llama-cpp-python\">llama-cpp-python</a>\
          \ as it is the official supported llama.cpp binding.  You should read their\
          \ <a rel=\"nofollow\" href=\"https://abetlen.github.io/llama-cpp-python/\"\
          >documentation</a> to better understand how to use the library.</p>\n<p>Downloading\
          \ the model works similarly. The following code should work:</p>\n<pre><code>from\
          \ huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama\n\n\
          #Download the model\nhf_hub_download(repo_id=\"LLukas22/gpt4all-lora-quantized-ggjt\"\
          , filename=\"ggjt-model.bin\", local_dir=\".\")\n\n#Load the model\nmodel\
          \ = Llama(model_path=\"ggjt-model.bin\")\n\n#Generate\nprompt=\"User: How\
          \ are you doing?\\nBot:\"\n\nresult=model(prompt, max_tokens=50, stop=[\"\
          User:\"])\n#Print result with additional infos\nprint(result)\n\n#Get only\
          \ the generated text\ngenerated_text = result[\"choices\"][0][\"text\"]\n\
          print(generated_text )\n</code></pre>\n<p>I'm also working on some llama-rs\
          \ bindings which will hopefully simply the process of downloading and executing\
          \ models further, but they are not ready yet.</p>\n"
        raw: '@TheLeCraft999 From your question i guess you are new to python, so
          i tried to keep it simple.


          `pyllamacpp` had some changes, the result object now seams to be a python
          c object. I would recommend switching to [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
          as it is the official supported llama.cpp binding.  You should read their
          [documentation](https://abetlen.github.io/llama-cpp-python/) to better understand
          how to use the library.


          Downloading the model works similarly. The following code should work:

          ```

          from huggingface_hub import hf_hub_download

          from llama_cpp import Llama


          #Download the model

          hf_hub_download(repo_id="LLukas22/gpt4all-lora-quantized-ggjt", filename="ggjt-model.bin",
          local_dir=".")


          #Load the model

          model = Llama(model_path="ggjt-model.bin")


          #Generate

          prompt="User: How are you doing?\nBot:"


          result=model(prompt, max_tokens=50, stop=["User:"])

          #Print result with additional infos

          print(result)


          #Get only the generated text

          generated_text = result["choices"][0]["text"]

          print(generated_text )

          ```


          I''m also working on some llama-rs bindings which will hopefully simply
          the process of downloading and executing models further, but they are not
          ready yet.'
        updatedAt: '2023-04-29T16:21:35.809Z'
      numEdits: 0
      reactions: []
    id: 644d440ffa94e93b0ec79207
    type: comment
  author: LLukas22
  content: '@TheLeCraft999 From your question i guess you are new to python, so i
    tried to keep it simple.


    `pyllamacpp` had some changes, the result object now seams to be a python c object.
    I would recommend switching to [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
    as it is the official supported llama.cpp binding.  You should read their [documentation](https://abetlen.github.io/llama-cpp-python/)
    to better understand how to use the library.


    Downloading the model works similarly. The following code should work:

    ```

    from huggingface_hub import hf_hub_download

    from llama_cpp import Llama


    #Download the model

    hf_hub_download(repo_id="LLukas22/gpt4all-lora-quantized-ggjt", filename="ggjt-model.bin",
    local_dir=".")


    #Load the model

    model = Llama(model_path="ggjt-model.bin")


    #Generate

    prompt="User: How are you doing?\nBot:"


    result=model(prompt, max_tokens=50, stop=["User:"])

    #Print result with additional infos

    print(result)


    #Get only the generated text

    generated_text = result["choices"][0]["text"]

    print(generated_text )

    ```


    I''m also working on some llama-rs bindings which will hopefully simply the process
    of downloading and executing models further, but they are not ready yet.'
  created_at: 2023-04-29 15:21:35+00:00
  edited: false
  hidden: false
  id: 644d440ffa94e93b0ec79207
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29821c36269be26a72278e0f6ecfa0e1.svg
      fullname: Guy Huinen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huineng
      type: user
    createdAt: '2023-05-02T18:54:25.000Z'
    data:
      edited: false
      editors:
      - huineng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29821c36269be26a72278e0f6ecfa0e1.svg
          fullname: Guy Huinen
          isHf: false
          isPro: false
          name: huineng
          type: user
        html: '<p>Thanks, that works , but i always get the same answer<br>"Please
          be patient as I may take a moment to respond fully"</p>

          <p>suggestions ?</p>

          '
        raw: "Thanks, that works , but i always get the same answer \n\"Please be\
          \ patient as I may take a moment to respond fully\"\n\nsuggestions ?"
        updatedAt: '2023-05-02T18:54:25.353Z'
      numEdits: 0
      reactions: []
    id: 64515c61b3f75261a7da4cea
    type: comment
  author: huineng
  content: "Thanks, that works , but i always get the same answer \n\"Please be patient\
    \ as I may take a moment to respond fully\"\n\nsuggestions ?"
  created_at: 2023-05-02 17:54:25+00:00
  edited: false
  hidden: false
  id: 64515c61b3f75261a7da4cea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29821c36269be26a72278e0f6ecfa0e1.svg
      fullname: Guy Huinen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huineng
      type: user
    createdAt: '2023-05-02T19:02:35.000Z'
    data:
      edited: false
      editors:
      - huineng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29821c36269be26a72278e0f6ecfa0e1.svg
          fullname: Guy Huinen
          isHf: false
          isPro: false
          name: huineng
          type: user
        html: "<p>it gave me results (not to long of a wait) when adding some params</p>\n\
          <pre><code>result=model(prompt, max_tokens=50, stop=[\"User:\"],\n    temperature=0.9,\n\
          \    top_p=0.95,\n    repeat_penalty=1.2,\n    top_k=50,\n    echo=True)\n\
          </code></pre>\n"
        raw: "it gave me results (not to long of a wait) when adding some params\n\
          ```\nresult=model(prompt, max_tokens=50, stop=[\"User:\"],\n    temperature=0.9,\n\
          \    top_p=0.95,\n    repeat_penalty=1.2,\n    top_k=50,\n    echo=True)\n\
          ```"
        updatedAt: '2023-05-02T19:02:35.336Z'
      numEdits: 0
      reactions: []
    id: 64515e4b5fb40b9f50b09529
    type: comment
  author: huineng
  content: "it gave me results (not to long of a wait) when adding some params\n```\n\
    result=model(prompt, max_tokens=50, stop=[\"User:\"],\n    temperature=0.9,\n\
    \    top_p=0.95,\n    repeat_penalty=1.2,\n    top_k=50,\n    echo=True)\n```"
  created_at: 2023-05-02 18:02:35+00:00
  edited: false
  hidden: false
  id: 64515e4b5fb40b9f50b09529
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da57f34be126e22e8bed5f/ghmINp1UDr9XnqZVaf_9G.png?w=200&h=200&f=face
      fullname: Lukas Kreussel
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LLukas22
      type: user
    createdAt: '2023-05-03T10:12:26.000Z'
    data:
      status: closed
    id: 6452338aea756598f3525c3b
    type: status-change
  author: LLukas22
  created_at: 2023-05-03 09:12:26+00:00
  id: 6452338aea756598f3525c3b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: LLukas22/gpt4all-lora-quantized-ggjt
repo_type: model
status: closed
target_branch: null
title: Problem
