!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kousun12
conflicting_files: null
created_at: 2023-09-07 20:32:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee6641bfcdb40f1cd252ccecbbf0c146.svg
      fullname: rob cheung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kousun12
      type: user
    createdAt: '2023-09-07T21:32:19.000Z'
    data:
      edited: false
      editors:
      - kousun12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6068509817123413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee6641bfcdb40f1cd252ccecbbf0c146.svg
          fullname: rob cheung
          isHf: false
          isPro: false
          name: kousun12
          type: user
        html: "<p>Things seem to start well, but I keep getting stuck building the\
          \ engine for <code>unetxl.trt8.6.1.plan</code>. I've left it running for\
          \ about an hour. </p>\n<pre><code>[I] Initializing TensorRT accelerated\
          \ StableDiffusionXL txt2img pipeline\n[I] Load tokenizer pytorch model from:\
          \ pytorch_model/xl-1.0/XL_BASE/tokenizer\n[I] Load tokenizer pytorch model\
          \ from: pytorch_model/xl-1.0/XL_BASE/tokenizer_2\n[I] Load VAE decoder pytorch\
          \ model from: pytorch_model/xl-1.0/XL_BASE/vae\nBuilding TensorRT engine\
          \ for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/clip.opt/model.onnx:\
          \ engine/clip.trt8.6.1.plan\n[W] onnx2trt_utils.cpp:374: Your ONNX model\
          \ has been generated with INT64 weights, while TensorRT does not natively\
          \ support INT64. Attempting to cast down to INT32.\nUpdating network outputs\
          \ to ['text_embeddings', 'hidden_states']\n[I]     Configuring with profiles:\
          \ [Profile().add('input_ids', min=(1, 77), opt=(1, 77), max=(1, 77))]\n\
          [I] Building engine with configuration:\n    Flags                  | [FP16]\n\
          \    Engine Capability      | EngineCapability.DEFAULT\n    Memory Pools\
          \           | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00 MiB]\n   \
          \ Tactic Sources         | []\n    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n\
          \    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n\
          [W] TensorRT encountered issues when converting weights between types and\
          \ that could affect accuracy.\n[W] If this is not the desired behavior,\
          \ please modify the weights or retrain with regularization to adjust the\
          \ magnitude of the weights.\n[W] Check verbose logs for the list of affected\
          \ weights.\n[W] - 112 weights are affected by this issue: Detected subnormal\
          \ FP16 values.\n[W] - 1 weights are affected by this issue: Detected finite\
          \ FP32 values which would overflow in FP16 and converted them tothe closest\
          \ finite FP16 value.\n[I] Finished engine building in 49.294 seconds\n[I]\
          \ Saving engine to engine/clip.trt8.6.1.plan\nBuilding TensorRT engine for\
          \ /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/clip2.opt/model.onnx:\
          \ engine/clip2.trt8.6.1.plan\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604]\
          \ Reading dangerously large protocol message.  If the message turns out\
          \ to be larger than 2147483647 bytes, parsing will be halted for security\
          \ reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit()\
          \ in google/protobuf/io/coded_stream.h.\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81]\
          \ The total number of bytes read was 1517189726\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604]\
          \ Reading dangerously large protocol message.  If the message turns out\
          \ to be larger than 2147483647 bytes, parsing will be halted for security\
          \ reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit()\
          \ in google/protobuf/io/coded_stream.h.\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81]\
          \ The total number of bytes read was 1517189726\n[W] onnx2trt_utils.cpp:70:\
          \ TensorRT is using FLOAT32 precision to run an INT32 ArgMax / ArgMin. Rounding\
          \ errors may occur for large integer values\n[W] Tensor DataType is determined\
          \ at build time for tensors not marked as input or output.\nUpdating network\
          \ outputs to ['text_embeddings', 'hidden_states']\n[I]     Configuring with\
          \ profiles: [Profile().add('input_ids', min=(1, 77), opt=(1, 77), max=(1,\
          \ 77))]\n[I] Building engine with configuration:\n    Flags            \
          \      | [FP16]\n    Engine Capability      | EngineCapability.DEFAULT\n\
          \    Memory Pools           | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00\
          \ MiB]\n    Tactic Sources         | []\n    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n\
          \    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n\
          [W] - 324 weights are affected by this issue: Detected subnormal FP16 values.\n\
          [I] Finished engine building in 169.366 seconds\n[I] Saving engine to engine/clip2.trt8.6.1.plan\n\
          Building TensorRT engine for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/unetxl.opt/model.onnx:\
          \ engine/unetxl.trt8.6.1.plan\n[W] onnx2trt_utils.cpp:400: One or more weights\
          \ outside the range of INT32 was clamped\n[I]     Configuring with profiles:\
          \ [Profile().add('sample', min=(2, 4, 128, 128), opt=(2, 4, 128, 128), max=(2,\
          \ 4, 128, 128)).add('encoder_hidden_states', min=(2, 77, 2048), opt=(2,\
          \ 77, 2048), max=(2, 77, 2048)).add('text_embeds', min=(2, 1280),opt=(2,\
          \ 1280), max=(2, 1280)).add('time_ids', min=(2, 6), opt=(2, 6), max=(2,\
          \ 6)).add('timestep', min=[1], opt=[1], max=[1])]\n[I] Building engine with\
          \ configuration:\n    Flags                  | [FP16]\n    Engine Capability\
          \      | EngineCapability.DEFAULT\n    Memory Pools           | [WORKSPACE:\
          \ 40370.00 MiB, TACTIC_DRAM: 40370.00 MiB]\n    Tactic Sources         |\
          \ []\n    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n    Preview\
          \ Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n\
          </code></pre>\n<p>Running on a machine with an A100</p>\n<pre><code>root@modal:~#\
          \ nvidia-smi\nThu Sep  7 21:25:57 2023\n+-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.1\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\n|                               |                     \
          \ |               MIG M. |\n|===============================+======================+======================|\n\
          |   0  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |               \
          \     0 |\n| N/A   36C    P0    58W / 400W |    819MiB / 40960MiB |    \
          \  0%      Default |\n|                               |                \
          \      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\
          \n+-----------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \       |\n|  GPU   GI   CI        PID   Type   Process name           \
          \       GPU Memory |\n|        ID   ID                                 \
          \                  Usage      |\n|=============================================================================|\n\
          +-----------------------------------------------------------------------------+\n\
          </code></pre>\n<p>And Pytorch 2.1.0, using nvidia's 23.07 image with python\
          \ 3</p>\n<pre><code>=============\n== PyTorch ==\n=============\n\nNVIDIA\
          \ Release 23.07 (build 63867923)\nPyTorch Version 2.1.0a0+b5021ba\n</code></pre>\n\
          <p>I've tried both the 8.6 and 9.0 branches on <a rel=\"nofollow\" href=\"\
          https://github.com/rajeevsrao/TensorRT\">https://github.com/rajeevsrao/TensorRT</a>,\
          \ and same result. Any idea what may be going wrong?</p>\n"
        raw: "Things seem to start well, but I keep getting stuck building the engine\
          \ for `unetxl.trt8.6.1.plan`. I've left it running for about an hour. \r\
          \n\r\n```\r\n[I] Initializing TensorRT accelerated StableDiffusionXL txt2img\
          \ pipeline\r\n[I] Load tokenizer pytorch model from: pytorch_model/xl-1.0/XL_BASE/tokenizer\r\
          \n[I] Load tokenizer pytorch model from: pytorch_model/xl-1.0/XL_BASE/tokenizer_2\r\
          \n[I] Load VAE decoder pytorch model from: pytorch_model/xl-1.0/XL_BASE/vae\r\
          \nBuilding TensorRT engine for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/clip.opt/model.onnx:\
          \ engine/clip.trt8.6.1.plan\r\n[W] onnx2trt_utils.cpp:374: Your ONNX model\
          \ has been generated with INT64 weights, while TensorRT does not natively\
          \ support INT64. Attempting to cast down to INT32.\r\nUpdating network outputs\
          \ to ['text_embeddings', 'hidden_states']\r\n[I]     Configuring with profiles:\
          \ [Profile().add('input_ids', min=(1, 77), opt=(1, 77), max=(1, 77))]\r\n\
          [I] Building engine with configuration:\r\n    Flags                  |\
          \ [FP16]\r\n    Engine Capability      | EngineCapability.DEFAULT\r\n  \
          \  Memory Pools           | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00\
          \ MiB]\r\n    Tactic Sources         | []\r\n    Profiling Verbosity   \
          \ | ProfilingVerbosity.DETAILED\r\n    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805,\
          \ DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\r\n[W] TensorRT encountered\
          \ issues when converting weights between types and that could affect accuracy.\r\
          \n[W] If this is not the desired behavior, please modify the weights or\
          \ retrain with regularization to adjust the magnitude of the weights.\r\n\
          [W] Check verbose logs for the list of affected weights.\r\n[W] - 112 weights\
          \ are affected by this issue: Detected subnormal FP16 values.\r\n[W] - 1\
          \ weights are affected by this issue: Detected finite FP32 values which\
          \ would overflow in FP16 and converted them tothe closest finite FP16 value.\r\
          \n[I] Finished engine building in 49.294 seconds\r\n[I] Saving engine to\
          \ engine/clip.trt8.6.1.plan\r\nBuilding TensorRT engine for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/clip2.opt/model.onnx:\
          \ engine/clip2.trt8.6.1.plan\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604]\
          \ Reading dangerously large protocol message.  If the message turns out\
          \ to be larger than 2147483647 bytes, parsing will be halted for security\
          \ reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit()\
          \ in google/protobuf/io/coded_stream.h.\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81]\
          \ The total number of bytes read was 1517189726\r\n[libprotobuf WARNING\
          \ google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol\
          \ message.  If the message turns out to be larger than 2147483647 bytes,\
          \ parsing will be halted for security reasons.  To increase the limit (or\
          \ to disable these warnings), see CodedInputStream::SetTotalBytesLimit()\
          \ in google/protobuf/io/coded_stream.h.\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81]\
          \ The total number of bytes read was 1517189726\r\n[W] onnx2trt_utils.cpp:70:\
          \ TensorRT is using FLOAT32 precision to run an INT32 ArgMax / ArgMin. Rounding\
          \ errors may occur for large integer values\r\n[W] Tensor DataType is determined\
          \ at build time for tensors not marked as input or output.\r\nUpdating network\
          \ outputs to ['text_embeddings', 'hidden_states']\r\n[I]     Configuring\
          \ with profiles: [Profile().add('input_ids', min=(1, 77), opt=(1, 77), max=(1,\
          \ 77))]\r\n[I] Building engine with configuration:\r\n    Flags        \
          \          | [FP16]\r\n    Engine Capability      | EngineCapability.DEFAULT\r\
          \n    Memory Pools           | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00\
          \ MiB]\r\n    Tactic Sources         | []\r\n    Profiling Verbosity   \
          \ | ProfilingVerbosity.DETAILED\r\n    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805,\
          \ DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\r\n[W] - 324 weights are\
          \ affected by this issue: Detected subnormal FP16 values.\r\n[I] Finished\
          \ engine building in 169.366 seconds\r\n[I] Saving engine to engine/clip2.trt8.6.1.plan\r\
          \nBuilding TensorRT engine for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/unetxl.opt/model.onnx:\
          \ engine/unetxl.trt8.6.1.plan\r\n[W] onnx2trt_utils.cpp:400: One or more\
          \ weights outside the range of INT32 was clamped\r\n[I]     Configuring\
          \ with profiles: [Profile().add('sample', min=(2, 4, 128, 128), opt=(2,\
          \ 4, 128, 128), max=(2, 4, 128, 128)).add('encoder_hidden_states', min=(2,\
          \ 77, 2048), opt=(2, 77, 2048), max=(2, 77, 2048)).add('text_embeds', min=(2,\
          \ 1280),opt=(2, 1280), max=(2, 1280)).add('time_ids', min=(2, 6), opt=(2,\
          \ 6), max=(2, 6)).add('timestep', min=[1], opt=[1], max=[1])]\r\n[I] Building\
          \ engine with configuration:\r\n    Flags                  | [FP16]\r\n\
          \    Engine Capability      | EngineCapability.DEFAULT\r\n    Memory Pools\
          \           | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00 MiB]\r\n \
          \   Tactic Sources         | []\r\n    Profiling Verbosity    | ProfilingVerbosity.DETAILED\r\
          \n    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\r\
          \n```\r\n\r\nRunning on a machine with an A100\r\n\r\n```\r\nroot@modal:~#\
          \ nvidia-smi\r\nThu Sep  7 21:25:57 2023\r\n+-----------------------------------------------------------------------------+\r\
          \n| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.1\
          \     |\r\n|-------------------------------+----------------------+----------------------+\r\
          \n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\r\n|                               |                   \
          \   |               MIG M. |\r\n|===============================+======================+======================|\r\
          \n|   0  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |             \
          \       0 |\r\n| N/A   36C    P0    58W / 400W |    819MiB / 40960MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n\r\n+-----------------------------------------------------------------------------+\r\
          \n| Processes:                                                         \
          \         |\r\n|  GPU   GI   CI        PID   Type   Process name       \
          \           GPU Memory |\r\n|        ID   ID                           \
          \                        Usage      |\r\n|=============================================================================|\r\
          \n+-----------------------------------------------------------------------------+\r\
          \n```\r\n\r\nAnd Pytorch 2.1.0, using nvidia's 23.07 image with python 3\r\
          \n\r\n```\r\n=============\r\n== PyTorch ==\r\n=============\r\n\r\nNVIDIA\
          \ Release 23.07 (build 63867923)\r\nPyTorch Version 2.1.0a0+b5021ba\r\n\
          ```\r\n\r\nI've tried both the 8.6 and 9.0 branches on https://github.com/rajeevsrao/TensorRT,\
          \ and same result. Any idea what may be going wrong?"
        updatedAt: '2023-09-07T21:32:19.520Z'
      numEdits: 0
      reactions: []
    id: 64fa4163fd212fdfdee3c2d3
    type: comment
  author: kousun12
  content: "Things seem to start well, but I keep getting stuck building the engine\
    \ for `unetxl.trt8.6.1.plan`. I've left it running for about an hour. \r\n\r\n\
    ```\r\n[I] Initializing TensorRT accelerated StableDiffusionXL txt2img pipeline\r\
    \n[I] Load tokenizer pytorch model from: pytorch_model/xl-1.0/XL_BASE/tokenizer\r\
    \n[I] Load tokenizer pytorch model from: pytorch_model/xl-1.0/XL_BASE/tokenizer_2\r\
    \n[I] Load VAE decoder pytorch model from: pytorch_model/xl-1.0/XL_BASE/vae\r\n\
    Building TensorRT engine for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/clip.opt/model.onnx:\
    \ engine/clip.trt8.6.1.plan\r\n[W] onnx2trt_utils.cpp:374: Your ONNX model has\
    \ been generated with INT64 weights, while TensorRT does not natively support\
    \ INT64. Attempting to cast down to INT32.\r\nUpdating network outputs to ['text_embeddings',\
    \ 'hidden_states']\r\n[I]     Configuring with profiles: [Profile().add('input_ids',\
    \ min=(1, 77), opt=(1, 77), max=(1, 77))]\r\n[I] Building engine with configuration:\r\
    \n    Flags                  | [FP16]\r\n    Engine Capability      | EngineCapability.DEFAULT\r\
    \n    Memory Pools           | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00\
    \ MiB]\r\n    Tactic Sources         | []\r\n    Profiling Verbosity    | ProfilingVerbosity.DETAILED\r\
    \n    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\r\
    \n[W] TensorRT encountered issues when converting weights between types and that\
    \ could affect accuracy.\r\n[W] If this is not the desired behavior, please modify\
    \ the weights or retrain with regularization to adjust the magnitude of the weights.\r\
    \n[W] Check verbose logs for the list of affected weights.\r\n[W] - 112 weights\
    \ are affected by this issue: Detected subnormal FP16 values.\r\n[W] - 1 weights\
    \ are affected by this issue: Detected finite FP32 values which would overflow\
    \ in FP16 and converted them tothe closest finite FP16 value.\r\n[I] Finished\
    \ engine building in 49.294 seconds\r\n[I] Saving engine to engine/clip.trt8.6.1.plan\r\
    \nBuilding TensorRT engine for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/clip2.opt/model.onnx:\
    \ engine/clip2.trt8.6.1.plan\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604]\
    \ Reading dangerously large protocol message.  If the message turns out to be\
    \ larger than 2147483647 bytes, parsing will be halted for security reasons. \
    \ To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit()\
    \ in google/protobuf/io/coded_stream.h.\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81]\
    \ The total number of bytes read was 1517189726\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604]\
    \ Reading dangerously large protocol message.  If the message turns out to be\
    \ larger than 2147483647 bytes, parsing will be halted for security reasons. \
    \ To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit()\
    \ in google/protobuf/io/coded_stream.h.\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81]\
    \ The total number of bytes read was 1517189726\r\n[W] onnx2trt_utils.cpp:70:\
    \ TensorRT is using FLOAT32 precision to run an INT32 ArgMax / ArgMin. Rounding\
    \ errors may occur for large integer values\r\n[W] Tensor DataType is determined\
    \ at build time for tensors not marked as input or output.\r\nUpdating network\
    \ outputs to ['text_embeddings', 'hidden_states']\r\n[I]     Configuring with\
    \ profiles: [Profile().add('input_ids', min=(1, 77), opt=(1, 77), max=(1, 77))]\r\
    \n[I] Building engine with configuration:\r\n    Flags                  | [FP16]\r\
    \n    Engine Capability      | EngineCapability.DEFAULT\r\n    Memory Pools  \
    \         | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00 MiB]\r\n    Tactic\
    \ Sources         | []\r\n    Profiling Verbosity    | ProfilingVerbosity.DETAILED\r\
    \n    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\r\
    \n[W] - 324 weights are affected by this issue: Detected subnormal FP16 values.\r\
    \n[I] Finished engine building in 169.366 seconds\r\n[I] Saving engine to engine/clip2.trt8.6.1.plan\r\
    \nBuilding TensorRT engine for /stable-diffusion-xl-1.0-tensorrt/sdxl-1.0-base/unetxl.opt/model.onnx:\
    \ engine/unetxl.trt8.6.1.plan\r\n[W] onnx2trt_utils.cpp:400: One or more weights\
    \ outside the range of INT32 was clamped\r\n[I]     Configuring with profiles:\
    \ [Profile().add('sample', min=(2, 4, 128, 128), opt=(2, 4, 128, 128), max=(2,\
    \ 4, 128, 128)).add('encoder_hidden_states', min=(2, 77, 2048), opt=(2, 77, 2048),\
    \ max=(2, 77, 2048)).add('text_embeds', min=(2, 1280),opt=(2, 1280), max=(2, 1280)).add('time_ids',\
    \ min=(2, 6), opt=(2, 6), max=(2, 6)).add('timestep', min=[1], opt=[1], max=[1])]\r\
    \n[I] Building engine with configuration:\r\n    Flags                  | [FP16]\r\
    \n    Engine Capability      | EngineCapability.DEFAULT\r\n    Memory Pools  \
    \         | [WORKSPACE: 40370.00 MiB, TACTIC_DRAM: 40370.00 MiB]\r\n    Tactic\
    \ Sources         | []\r\n    Profiling Verbosity    | ProfilingVerbosity.DETAILED\r\
    \n    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\r\
    \n```\r\n\r\nRunning on a machine with an A100\r\n\r\n```\r\nroot@modal:~# nvidia-smi\r\
    \nThu Sep  7 21:25:57 2023\r\n+-----------------------------------------------------------------------------+\r\
    \n| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.1  \
    \   |\r\n|-------------------------------+----------------------+----------------------+\r\
    \n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC\
    \ |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\
    \ M. |\r\n|                               |                      |           \
    \    MIG M. |\r\n|===============================+======================+======================|\r\
    \n|   0  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |                   \
    \ 0 |\r\n| N/A   36C    P0    58W / 400W |    819MiB / 40960MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n\r\n+-----------------------------------------------------------------------------+\r\
    \n| Processes:                                                               \
    \   |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU\
    \ Memory |\r\n|        ID   ID                                               \
    \    Usage      |\r\n|=============================================================================|\r\
    \n+-----------------------------------------------------------------------------+\r\
    \n```\r\n\r\nAnd Pytorch 2.1.0, using nvidia's 23.07 image with python 3\r\n\r\
    \n```\r\n=============\r\n== PyTorch ==\r\n=============\r\n\r\nNVIDIA Release\
    \ 23.07 (build 63867923)\r\nPyTorch Version 2.1.0a0+b5021ba\r\n```\r\n\r\nI've\
    \ tried both the 8.6 and 9.0 branches on https://github.com/rajeevsrao/TensorRT,\
    \ and same result. Any idea what may be going wrong?"
  created_at: 2023-09-07 20:32:19+00:00
  edited: false
  hidden: false
  id: 64fa4163fd212fdfdee3c2d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ee6641bfcdb40f1cd252ccecbbf0c146.svg
      fullname: rob cheung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kousun12
      type: user
    createdAt: '2023-09-08T02:03:26.000Z'
    data:
      status: closed
    id: 64fa80eef393f30370e4db37
    type: status-change
  author: kousun12
  created_at: 2023-09-08 01:03:26+00:00
  id: 64fa80eef393f30370e4db37
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: stabilityai/stable-diffusion-xl-1.0-tensorrt
repo_type: model
status: closed
target_branch: null
title: hanging while building engine for `unetxl.trt8.6.1.plan`
