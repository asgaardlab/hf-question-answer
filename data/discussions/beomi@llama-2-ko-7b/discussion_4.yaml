!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Soroor
conflicting_files: null
created_at: 2023-07-25 05:05:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf8e06b3b52e8f12bc0cf4627d53aeb1.svg
      fullname: Soroor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Soroor
      type: user
    createdAt: '2023-07-25T06:05:42.000Z'
    data:
      edited: false
      editors:
      - Soroor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7916359901428223
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf8e06b3b52e8f12bc0cf4627d53aeb1.svg
          fullname: Soroor
          isHf: false
          isPro: false
          name: Soroor
          type: user
        html: "<p>Hi there,<br>first of all thanks for sharing this cool model,<br>I\
          \ tried to test it but I couldn't get result, so I think the way that I\
          \ tried to test it might be wrong, could you please guide me how can I simply\
          \ test it or could you please check my code<br>here is the code that I used\
          \ but it just re-write the query and nothing more!</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer\n<span class=\"\
          hljs-keyword\">import</span> transformers\n<span class=\"hljs-keyword\"\
          >import</span> torch\n\nmodel = <span class=\"hljs-string\">\"beomi/llama-2-ko-7b\"\
          </span>\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline =\
          \ transformers.pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n\
          \    device_map=<span class=\"hljs-string\">\"auto\"</span>,\n)\nquery=<span\
          \ class=\"hljs-string\">\"whatever\"</span>\nsequences = pipeline(\n   \
          \ query,\n    do_sample=<span class=\"hljs-literal\">False</span>,\n   \
          \ top_k=<span class=\"hljs-number\">10</span>,\n    num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    max_length=<span class=\"hljs-number\">200</span>,\n)\n<span class=\"\
          hljs-keyword\">for</span> seq <span class=\"hljs-keyword\">in</span> sequences:\n\
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Result: <span class=\"hljs-subst\">{seq[<span class=\"hljs-string\"\
          >'generated_text'</span>]}</span>\"</span>)\n</code></pre>\n"
        raw: "Hi there, \r\nfirst of all thanks for sharing this cool model, \r\n\
          I tried to test it but I couldn't get result, so I think the way that I\
          \ tried to test it might be wrong, could you please guide me how can I simply\
          \ test it or could you please check my code\r\nhere is the code that I used\
          \ but it just re-write the query and nothing more!\r\n\r\n```python\r\n\
          from transformers import AutoTokenizer\r\nimport transformers\r\nimport\
          \ torch\r\n\r\nmodel = \"beomi/llama-2-ko-7b\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
          \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.float16,\r\n    device_map=\"\
          auto\",\r\n)\r\nquery=\"whatever\"\r\nsequences = pipeline(\r\n    query,\r\
          \n    do_sample=False,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\
          \n    eos_token_id=tokenizer.eos_token_id,\r\n    max_length=200,\r\n)\r\
          \nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n```\r\n"
        updatedAt: '2023-07-25T06:05:42.332Z'
      numEdits: 0
      reactions: []
    id: 64bf6636979949d2e253fb16
    type: comment
  author: Soroor
  content: "Hi there, \r\nfirst of all thanks for sharing this cool model, \r\nI tried\
    \ to test it but I couldn't get result, so I think the way that I tried to test\
    \ it might be wrong, could you please guide me how can I simply test it or could\
    \ you please check my code\r\nhere is the code that I used but it just re-write\
    \ the query and nothing more!\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\
    \nimport transformers\r\nimport torch\r\n\r\nmodel = \"beomi/llama-2-ko-7b\"\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    torch_dtype=torch.float16,\r\n    device_map=\"auto\",\r\n)\r\nquery=\"whatever\"\
    \r\nsequences = pipeline(\r\n    query,\r\n    do_sample=False,\r\n    top_k=10,\r\
    \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\n\
    \    max_length=200,\r\n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
    )\r\n```\r\n"
  created_at: 2023-07-25 05:05:42+00:00
  edited: false
  hidden: false
  id: 64bf6636979949d2e253fb16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b38b13089881229355097b87945ae6fe.svg
      fullname: Junbum Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: june42
      type: user
    createdAt: '2023-07-25T09:57:17.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/b38b13089881229355097b87945ae6fe.svg
          fullname: Junbum Lee
          isHf: false
          isPro: false
          name: june42
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-25T09:57:27.015Z'
      numEdits: 0
      reactions: []
    id: 64bf9c7d4b4ff0d50992fdc7
    type: comment
  author: june42
  content: This comment has been hidden
  created_at: 2023-07-25 08:57:17+00:00
  edited: true
  hidden: true
  id: 64bf9c7d4b4ff0d50992fdc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: beomi
      type: user
    createdAt: '2023-07-25T09:57:46.000Z'
    data:
      edited: false
      editors:
      - beomi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9200690984725952
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
          fullname: Lee Junbum
          isHf: false
          isPro: false
          name: beomi
          type: user
        html: '<p>Thanks for your attention!</p>

          <p>it seems like it is working as intended, I''ve tested on Google Colab
          using your code but it seems working fine.<br>Here''s demo colab link: <a
          rel="nofollow" href="https://colab.research.google.com/drive/1yw2wnge6iHfj7PO5VVDA3jkmliiOqQvd?usp=sharing">https://colab.research.google.com/drive/1yw2wnge6iHfj7PO5VVDA3jkmliiOqQvd?usp=sharing</a></p>

          <p>what i changed is 1 line of code - since this ckpt is consisted of <code>BF16</code>,
          you''ll need to use <code>torch_dtype=torch.bfloat16</code> or remove that
          line at all. (model''s config contains about torch_dtype already) but actually
          it is not critical issue for running the model.</p>

          <p>could you explain more detail about your env (python ver, pytorch ver,
          GPU, nvidia-driver version, cuda version, transformers/tokenizers/accelerate
          version)?</p>

          '
        raw: 'Thanks for your attention!


          it seems like it is working as intended, I''ve tested on Google Colab using
          your code but it seems working fine.

          Here''s demo colab link: https://colab.research.google.com/drive/1yw2wnge6iHfj7PO5VVDA3jkmliiOqQvd?usp=sharing


          what i changed is 1 line of code - since this ckpt is consisted of `BF16`,
          you''ll need to use `torch_dtype=torch.bfloat16` or remove that line at
          all. (model''s config contains about torch_dtype already) but actually it
          is not critical issue for running the model.


          could you explain more detail about your env (python ver, pytorch ver, GPU,
          nvidia-driver version, cuda version, transformers/tokenizers/accelerate
          version)?'
        updatedAt: '2023-07-25T09:57:46.041Z'
      numEdits: 0
      reactions: []
    id: 64bf9c9a796f20daadb34a98
    type: comment
  author: beomi
  content: 'Thanks for your attention!


    it seems like it is working as intended, I''ve tested on Google Colab using your
    code but it seems working fine.

    Here''s demo colab link: https://colab.research.google.com/drive/1yw2wnge6iHfj7PO5VVDA3jkmliiOqQvd?usp=sharing


    what i changed is 1 line of code - since this ckpt is consisted of `BF16`, you''ll
    need to use `torch_dtype=torch.bfloat16` or remove that line at all. (model''s
    config contains about torch_dtype already) but actually it is not critical issue
    for running the model.


    could you explain more detail about your env (python ver, pytorch ver, GPU, nvidia-driver
    version, cuda version, transformers/tokenizers/accelerate version)?'
  created_at: 2023-07-25 08:57:46+00:00
  edited: false
  hidden: false
  id: 64bf9c9a796f20daadb34a98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf8e06b3b52e8f12bc0cf4627d53aeb1.svg
      fullname: Soroor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Soroor
      type: user
    createdAt: '2023-07-25T16:30:36.000Z'
    data:
      edited: false
      editors:
      - Soroor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8549215793609619
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf8e06b3b52e8f12bc0cf4627d53aeb1.svg
          fullname: Soroor
          isHf: false
          isPro: false
          name: Soroor
          type: user
        html: "<p>Thank you for the prompt response, and thanks for your guidance.<br>It\
          \ seems that the issue has been solved.<br>I also obtained the same result\
          \ that you shared:<br>Loading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:26&lt;00:00, 1.77s/it]<br>Result:\
          \ How's the weather today? (\uC624\uB298 \uB0A0\uC528\uAC00 \uC5B4\uB5BB\
          \uC2B5\uB2C8\uAE4C?) 10. \uC624\uB298 \uC800\uB141\uC5D0 \uBB50 \uD560 \uAC81\
          \uB2C8\uAE4C? What are you doing tonight? 11. \uBA87 \uC2DC\uC5D0 \uD1F4\
          \uADFC\uD569\uB2C8\uAE4C? What time do you get off? 12. \uC624\uB298\uC740\
          \ \uBA87 \uC2DC\uC5D0 \uCD9C\uADFC\uD569\uB2C8\uAE4C? What time are you\
          \ coming to work today? 13. \uC5B4\uB514\uB97C \uAC00\uC2ED\uB2C8\uAE4C\
          ? Where are you headed? 14. \uB2F9\uC2E0\uC740 \uBB34\uC2A8 \uC77C\uB85C\
          \ \uC804\uD654\uD558\uC168\uC2B5\uB2C8\uAE4C? May I help you, sir? 15. \uC774\
          \ \uC637\uC740 \uC5B4\uB54C\uC694? How does this look on me? 16. \uCC28\
          \ \uD55C \uC794 \uC5B4\uB5BB\uC2B5\uB2C8\uAE4C? How about a cup of coffee?\
          \ 17. \uC65C \uC800\uC5D0\uAC8C \uADF8\uB807\uAC8C \uD654\uB97C \uB0B4\uACE0\
          \ \uC788\uC2B5\uB2C8\uAE4C? Why are you so angry with me? 18. \uB098\uB294\
          \ \uB2F9\uC2E0\uC744 \uC0AC\uB791\uD569\uB2C8\uB2E4. I love you. 19. \uB098\
          \uB294 \uB2F9\uC2E0\uC744 \uC88B\uC544\uD569\uB2C8\uB2E4. I like you. 20.\
          \ \uB2F9\uC2E0\uC740 \uCC38 \uC815\uC5F4\uC801\uC785\uB2C8\uB2E4. You...</p>\n\
          <p>It's working, but it seems to be generating questions and translations\
          \ rather than general text generation.<br>Did you train the model to generate\
          \ similar questions with their translations?<br>and I've noticed that it\
          \ doesn't work well with larger texts and it's just re-write whole the given\
          \ context again</p>\n<p>and here are my current environment details:<br>python:\
          \ 3.8.16<br>pytorch: 2.0.1+cu117<br>GPU: A100 80G<br>Nvidia-Driver Version:\
          \ 495.29.05<br>CUDA Version: 11.5<br>transformers: 4.32.0.dev0<br>tokenizers:\
          \ 0.13.3<br>accelerate: 0.20.3</p>\n<p>thanks again!</p>\n"
        raw: "Thank you for the prompt response, and thanks for your guidance. \n\
          It seems that the issue has been solved. \nI also obtained the same result\
          \ that you shared:\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:26<00:00, 1.77s/it]\nResult:\
          \ How's the weather today? (\uC624\uB298 \uB0A0\uC528\uAC00 \uC5B4\uB5BB\
          \uC2B5\uB2C8\uAE4C?) 10. \uC624\uB298 \uC800\uB141\uC5D0 \uBB50 \uD560 \uAC81\
          \uB2C8\uAE4C? What are you doing tonight? 11. \uBA87 \uC2DC\uC5D0 \uD1F4\
          \uADFC\uD569\uB2C8\uAE4C? What time do you get off? 12. \uC624\uB298\uC740\
          \ \uBA87 \uC2DC\uC5D0 \uCD9C\uADFC\uD569\uB2C8\uAE4C? What time are you\
          \ coming to work today? 13. \uC5B4\uB514\uB97C \uAC00\uC2ED\uB2C8\uAE4C\
          ? Where are you headed? 14. \uB2F9\uC2E0\uC740 \uBB34\uC2A8 \uC77C\uB85C\
          \ \uC804\uD654\uD558\uC168\uC2B5\uB2C8\uAE4C? May I help you, sir? 15. \uC774\
          \ \uC637\uC740 \uC5B4\uB54C\uC694? How does this look on me? 16. \uCC28\
          \ \uD55C \uC794 \uC5B4\uB5BB\uC2B5\uB2C8\uAE4C? How about a cup of coffee?\
          \ 17. \uC65C \uC800\uC5D0\uAC8C \uADF8\uB807\uAC8C \uD654\uB97C \uB0B4\uACE0\
          \ \uC788\uC2B5\uB2C8\uAE4C? Why are you so angry with me? 18. \uB098\uB294\
          \ \uB2F9\uC2E0\uC744 \uC0AC\uB791\uD569\uB2C8\uB2E4. I love you. 19. \uB098\
          \uB294 \uB2F9\uC2E0\uC744 \uC88B\uC544\uD569\uB2C8\uB2E4. I like you. 20.\
          \ \uB2F9\uC2E0\uC740 \uCC38 \uC815\uC5F4\uC801\uC785\uB2C8\uB2E4. You...\n\
          \nIt's working, but it seems to be generating questions and translations\
          \ rather than general text generation. \nDid you train the model to generate\
          \ similar questions with their translations?\nand I've noticed that it doesn't\
          \ work well with larger texts and it's just re-write whole the given context\
          \ again\n\nand here are my current environment details:\npython: 3.8.16\n\
          pytorch: 2.0.1+cu117\nGPU: A100 80G\nNvidia-Driver Version: 495.29.05\n\
          CUDA Version: 11.5 \ntransformers: 4.32.0.dev0\ntokenizers: 0.13.3\naccelerate:\
          \ 0.20.3\n\nthanks again!"
        updatedAt: '2023-07-25T16:30:36.691Z'
      numEdits: 0
      reactions: []
    id: 64bff8ac1106ecc1afc616d6
    type: comment
  author: Soroor
  content: "Thank you for the prompt response, and thanks for your guidance. \nIt\
    \ seems that the issue has been solved. \nI also obtained the same result that\
    \ you shared:\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 15/15 [00:26<00:00, 1.77s/it]\nResult: How's the weather\
    \ today? (\uC624\uB298 \uB0A0\uC528\uAC00 \uC5B4\uB5BB\uC2B5\uB2C8\uAE4C?) 10.\
    \ \uC624\uB298 \uC800\uB141\uC5D0 \uBB50 \uD560 \uAC81\uB2C8\uAE4C? What are you\
    \ doing tonight? 11. \uBA87 \uC2DC\uC5D0 \uD1F4\uADFC\uD569\uB2C8\uAE4C? What\
    \ time do you get off? 12. \uC624\uB298\uC740 \uBA87 \uC2DC\uC5D0 \uCD9C\uADFC\
    \uD569\uB2C8\uAE4C? What time are you coming to work today? 13. \uC5B4\uB514\uB97C\
    \ \uAC00\uC2ED\uB2C8\uAE4C? Where are you headed? 14. \uB2F9\uC2E0\uC740 \uBB34\
    \uC2A8 \uC77C\uB85C \uC804\uD654\uD558\uC168\uC2B5\uB2C8\uAE4C? May I help you,\
    \ sir? 15. \uC774 \uC637\uC740 \uC5B4\uB54C\uC694? How does this look on me? 16.\
    \ \uCC28 \uD55C \uC794 \uC5B4\uB5BB\uC2B5\uB2C8\uAE4C? How about a cup of coffee?\
    \ 17. \uC65C \uC800\uC5D0\uAC8C \uADF8\uB807\uAC8C \uD654\uB97C \uB0B4\uACE0 \uC788\
    \uC2B5\uB2C8\uAE4C? Why are you so angry with me? 18. \uB098\uB294 \uB2F9\uC2E0\
    \uC744 \uC0AC\uB791\uD569\uB2C8\uB2E4. I love you. 19. \uB098\uB294 \uB2F9\uC2E0\
    \uC744 \uC88B\uC544\uD569\uB2C8\uB2E4. I like you. 20. \uB2F9\uC2E0\uC740 \uCC38\
    \ \uC815\uC5F4\uC801\uC785\uB2C8\uB2E4. You...\n\nIt's working, but it seems to\
    \ be generating questions and translations rather than general text generation.\
    \ \nDid you train the model to generate similar questions with their translations?\n\
    and I've noticed that it doesn't work well with larger texts and it's just re-write\
    \ whole the given context again\n\nand here are my current environment details:\n\
    python: 3.8.16\npytorch: 2.0.1+cu117\nGPU: A100 80G\nNvidia-Driver Version: 495.29.05\n\
    CUDA Version: 11.5 \ntransformers: 4.32.0.dev0\ntokenizers: 0.13.3\naccelerate:\
    \ 0.20.3\n\nthanks again!"
  created_at: 2023-07-25 15:30:36+00:00
  edited: false
  hidden: false
  id: 64bff8ac1106ecc1afc616d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: beomi
      type: user
    createdAt: '2023-08-01T01:16:20.000Z'
    data:
      edited: false
      editors:
      - beomi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960619330406189
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
          fullname: Lee Junbum
          isHf: false
          isPro: false
          name: beomi
          type: user
        html: '<p>It would be sampling issue.<br>How about adding some temperatures
          and top-p sampling?<br>the phenomena shown is NOT intended since I trained
          the model with shuffled texts.</p>

          '
        raw: 'It would be sampling issue.

          How about adding some temperatures and top-p sampling?

          the phenomena shown is NOT intended since I trained the model with shuffled
          texts.'
        updatedAt: '2023-08-01T01:16:20.134Z'
      numEdits: 0
      reactions: []
    id: 64c85ce4cb2f1bf0e7b691eb
    type: comment
  author: beomi
  content: 'It would be sampling issue.

    How about adding some temperatures and top-p sampling?

    the phenomena shown is NOT intended since I trained the model with shuffled texts.'
  created_at: 2023-08-01 00:16:20+00:00
  edited: false
  hidden: false
  id: 64c85ce4cb2f1bf0e7b691eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0b6dccbc6aaf93af415615469a73777.svg
      fullname: Lin Qiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lqiao
      type: user
    createdAt: '2023-08-16T06:48:03.000Z'
    data:
      edited: false
      editors:
      - lqiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9762020111083984
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0b6dccbc6aaf93af415615469a73777.svg
          fullname: Lin Qiao
          isHf: false
          isPro: false
          name: lqiao
          type: user
        html: '<p>I saw the same issue as reported by Soroor. I already used temperature
          0.7, top-p 0.9. </p>

          '
        raw: 'I saw the same issue as reported by Soroor. I already used temperature
          0.7, top-p 0.9. '
        updatedAt: '2023-08-16T06:48:03.295Z'
      numEdits: 0
      reactions: []
    id: 64dc71239e41aeab1cdc335f
    type: comment
  author: lqiao
  content: 'I saw the same issue as reported by Soroor. I already used temperature
    0.7, top-p 0.9. '
  created_at: 2023-08-16 05:48:03+00:00
  edited: false
  hidden: false
  id: 64dc71239e41aeab1cdc335f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0b6dccbc6aaf93af415615469a73777.svg
      fullname: Lin Qiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lqiao
      type: user
    createdAt: '2023-08-16T06:50:17.000Z'
    data:
      edited: false
      editors:
      - lqiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3297804892063141
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0b6dccbc6aaf93af415615469a73777.svg
          fullname: Lin Qiao
          isHf: false
          isPro: false
          name: lqiao
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64c74d153f3387bcfa50e3e4/69RM_ESOfRBJUj6WF7_lo.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64c74d153f3387bcfa50e3e4/69RM_ESOfRBJUj6WF7_lo.png"></a></p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c74d153f3387bcfa50e3e4/69RM_ESOfRBJUj6WF7_lo.png)

          '
        updatedAt: '2023-08-16T06:50:17.620Z'
      numEdits: 0
      reactions: []
    id: 64dc71a97f749b6e3477eefd
    type: comment
  author: lqiao
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c74d153f3387bcfa50e3e4/69RM_ESOfRBJUj6WF7_lo.png)

    '
  created_at: 2023-08-16 05:50:17+00:00
  edited: false
  hidden: false
  id: 64dc71a97f749b6e3477eefd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0b6dccbc6aaf93af415615469a73777.svg
      fullname: Lin Qiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lqiao
      type: user
    createdAt: '2023-08-17T23:45:34.000Z'
    data:
      edited: false
      editors:
      - lqiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7198197841644287
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0b6dccbc6aaf93af415615469a73777.svg
          fullname: Lin Qiao
          isHf: false
          isPro: false
          name: lqiao
          type: user
        html: '<p>Is there a good prompt template to use for chat? </p>

          '
        raw: 'Is there a good prompt template to use for chat? '
        updatedAt: '2023-08-17T23:45:34.047Z'
      numEdits: 0
      reactions: []
    id: 64deb11ee672e4d2c55f7f43
    type: comment
  author: lqiao
  content: 'Is there a good prompt template to use for chat? '
  created_at: 2023-08-17 22:45:34+00:00
  edited: false
  hidden: false
  id: 64deb11ee672e4d2c55f7f43
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: beomi/llama-2-ko-7b
repo_type: model
status: open
target_branch: null
title: how test the model
