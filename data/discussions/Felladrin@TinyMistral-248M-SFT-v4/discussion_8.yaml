!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Locutusque
conflicting_files: null
created_at: 2023-12-11 21:34:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-12-11T21:34:57.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9957345724105835
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<p>What code was used to fine-tune this? What was the learning rate?
          The performance is amazing.</p>

          '
        raw: What code was used to fine-tune this? What was the learning rate? The
          performance is amazing.
        updatedAt: '2023-12-11T21:34:57.708Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Felladrin
    id: 65778081a791ae9fe0560480
    type: comment
  author: Locutusque
  content: What code was used to fine-tune this? What was the learning rate? The performance
    is amazing.
  created_at: 2023-12-11 21:34:57+00:00
  edited: false
  hidden: false
  id: 65778081a791ae9fe0560480
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454aff9273f649830234978/cvVV08YHJpJx9xWVZqgVW.jpeg?w=200&h=200&f=face
      fullname: Victor Nogueira
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Felladrin
      type: user
    createdAt: '2023-12-11T23:18:35.000Z'
    data:
      edited: false
      editors:
      - Felladrin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9490053057670593
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454aff9273f649830234978/cvVV08YHJpJx9xWVZqgVW.jpeg?w=200&h=200&f=face
          fullname: Victor Nogueira
          isHf: false
          isPro: false
          name: Felladrin
          type: user
        html: "<p>Thank you, <span data-props=\"{&quot;user&quot;:&quot;Locutusque&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Locutusque\"\
          >@<span class=\"underline\">Locutusque</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>This one is a bit of a mutant. Everything I wanted to test about fine-tuning\
          \ LLMs I tried on it, and also restarted the training several times from\
          \ checkpoints, so I can't say it was trained with specific parameters.<br>I\
          \ used <em>TinyMistral-248M-SFT-v3</em> as base (which is based on <a href=\"\
          https://huggingface.co/Locutusque/TinyMistral-248M/tree/90b89d18fdf27937dc04ab8a9b543c5af2991c7f\"\
          >TinyMistral-248M@90b89d18fdf27937dc04ab8a9b543c5af2991c7f</a>)<br>The learning\
          \ rates I tested were from 5e-4 to 7-e6. But I feel the sweet spot for this\
          \ model is <code>learning_rate=2e-5</code>.<br>I also tried several batch\
          \ sizes, from 1 to 512. But the last training was made on 32 for several\
          \ hours (<code>per_device_train_batch_size=2</code> &amp; <code>gradient_accumulation_steps=16</code>).<br>I'm\
          \ training the full model in float32 using only <a href=\"https://huggingface.co/docs/trl/main/en/sft_trainer\"\
          >SFTTrainer</a>. Note: <em>TinyMistral-248M-SFT-v3</em> was trained with\
          \ AutoTrain Advanced, but I dropped it on v4 because I wanted to make use\
          \ of <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2310.05914\">NEFTune</a>\
          \ and they didn't support it yet.<br>I set <code>neftune_noise_alpha=5</code>\
          \ as <a href=\"https://huggingface.co/docs/trl/main/en/sft_trainer#enhance-models-performances-using-neftune\"\
          >recommended on SFTTrainer page</a> and didn't tweak it. Fortunately, it\
          \ seems to have helped the learning.<br>I've used <code>max_seq_length=2048</code>\
          \ this time, and it seems it gave it a boost. (v3 was trained with 1024)<br>It's\
          \ also worth mentioning that I used <code>weight_decay=0.01</code> (it was\
          \ 0 on v3) and <code>lr_scheduler_type=\"cosine\"</code> (it was \"constant\"\
          \ in v3).<br>I used <code>evaluation_strategy=\"steps\"</code> and set the\
          \ <code>eval_steps</code> to the same value as <code>save_steps</code>,\
          \ and set a low number for the steps, so I could get progress feedback every\
          \ ~10 minutes.<br>Before starting each training session, the dataset was\
          \ shuffled to avoid getting overtrained on the first rows (due to the constant\
          \ restarts).</p>\n<p>I believe that sums up how it was trained!</p>\n"
        raw: 'Thank you, @Locutusque!


          This one is a bit of a mutant. Everything I wanted to test about fine-tuning
          LLMs I tried on it, and also restarted the training several times from checkpoints,
          so I can''t say it was trained with specific parameters.

          I used _TinyMistral-248M-SFT-v3_ as base (which is based on [TinyMistral-248M@90b89d18fdf27937dc04ab8a9b543c5af2991c7f](https://huggingface.co/Locutusque/TinyMistral-248M/tree/90b89d18fdf27937dc04ab8a9b543c5af2991c7f))

          The learning rates I tested were from 5e-4 to 7-e6. But I feel the sweet
          spot for this model is `learning_rate=2e-5`.

          I also tried several batch sizes, from 1 to 512. But the last training was
          made on 32 for several hours (`per_device_train_batch_size=2` & `gradient_accumulation_steps=16`).

          I''m training the full model in float32 using only [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer).
          Note: _TinyMistral-248M-SFT-v3_ was trained with AutoTrain Advanced, but
          I dropped it on v4 because I wanted to make use of [NEFTune](https://arxiv.org/abs/2310.05914)
          and they didn''t support it yet.

          I set `neftune_noise_alpha=5` as [recommended on SFTTrainer page](https://huggingface.co/docs/trl/main/en/sft_trainer#enhance-models-performances-using-neftune)
          and didn''t tweak it. Fortunately, it seems to have helped the learning.

          I''ve used `max_seq_length=2048` this time, and it seems it gave it a boost.
          (v3 was trained with 1024)

          It''s also worth mentioning that I used `weight_decay=0.01` (it was 0 on
          v3) and `lr_scheduler_type="cosine"` (it was "constant" in v3).

          I used `evaluation_strategy="steps"` and set the `eval_steps` to the same
          value as `save_steps`, and set a low number for the steps, so I could get
          progress feedback every ~10 minutes.

          Before starting each training session, the dataset was shuffled to avoid
          getting overtrained on the first rows (due to the constant restarts).


          I believe that sums up how it was trained!'
        updatedAt: '2023-12-11T23:18:35.142Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - afrideva
        - Locutusque
    id: 657798cb7685e1ce5cfe6c4c
    type: comment
  author: Felladrin
  content: 'Thank you, @Locutusque!


    This one is a bit of a mutant. Everything I wanted to test about fine-tuning LLMs
    I tried on it, and also restarted the training several times from checkpoints,
    so I can''t say it was trained with specific parameters.

    I used _TinyMistral-248M-SFT-v3_ as base (which is based on [TinyMistral-248M@90b89d18fdf27937dc04ab8a9b543c5af2991c7f](https://huggingface.co/Locutusque/TinyMistral-248M/tree/90b89d18fdf27937dc04ab8a9b543c5af2991c7f))

    The learning rates I tested were from 5e-4 to 7-e6. But I feel the sweet spot
    for this model is `learning_rate=2e-5`.

    I also tried several batch sizes, from 1 to 512. But the last training was made
    on 32 for several hours (`per_device_train_batch_size=2` & `gradient_accumulation_steps=16`).

    I''m training the full model in float32 using only [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer).
    Note: _TinyMistral-248M-SFT-v3_ was trained with AutoTrain Advanced, but I dropped
    it on v4 because I wanted to make use of [NEFTune](https://arxiv.org/abs/2310.05914)
    and they didn''t support it yet.

    I set `neftune_noise_alpha=5` as [recommended on SFTTrainer page](https://huggingface.co/docs/trl/main/en/sft_trainer#enhance-models-performances-using-neftune)
    and didn''t tweak it. Fortunately, it seems to have helped the learning.

    I''ve used `max_seq_length=2048` this time, and it seems it gave it a boost. (v3
    was trained with 1024)

    It''s also worth mentioning that I used `weight_decay=0.01` (it was 0 on v3) and
    `lr_scheduler_type="cosine"` (it was "constant" in v3).

    I used `evaluation_strategy="steps"` and set the `eval_steps` to the same value
    as `save_steps`, and set a low number for the steps, so I could get progress feedback
    every ~10 minutes.

    Before starting each training session, the dataset was shuffled to avoid getting
    overtrained on the first rows (due to the constant restarts).


    I believe that sums up how it was trained!'
  created_at: 2023-12-11 23:18:35+00:00
  edited: false
  hidden: false
  id: 657798cb7685e1ce5cfe6c4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-12-12T05:03:26.000Z'
    data:
      status: closed
    id: 6577e99e76b6de7978f561d8
    type: status-change
  author: Locutusque
  created_at: 2023-12-12 05:03:26+00:00
  id: 6577e99e76b6de7978f561d8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: Felladrin/TinyMistral-248M-SFT-v4
repo_type: model
status: closed
target_branch: null
title: How was this fine-tuned?
