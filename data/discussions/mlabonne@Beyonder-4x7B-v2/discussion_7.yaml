!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wolfram
conflicting_files: null
created_at: 2024-01-22 22:30:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2024-01-22T22:30:43.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7955629229545593
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>The chat_template specified in tokenizer_config.json is ChatML,
          but apparently this model uses the (weird) GPT4 Correct prompt format. Please
          clarify which is the correct prompt format/chat template and kindly state
          it on the model card, and make sure tokenizer_config.json also has the proper
          template. Thank you!</p>

          '
        raw: The chat_template specified in tokenizer_config.json is ChatML, but apparently
          this model uses the (weird) GPT4 Correct prompt format. Please clarify which
          is the correct prompt format/chat template and kindly state it on the model
          card, and make sure tokenizer_config.json also has the proper template.
          Thank you!
        updatedAt: '2024-01-22T22:30:43.216Z'
      numEdits: 0
      reactions: []
    id: 65aeec93e2a22ce1bc070aea
    type: comment
  author: wolfram
  content: The chat_template specified in tokenizer_config.json is ChatML, but apparently
    this model uses the (weird) GPT4 Correct prompt format. Please clarify which is
    the correct prompt format/chat template and kindly state it on the model card,
    and make sure tokenizer_config.json also has the proper template. Thank you!
  created_at: 2024-01-22 22:30:43+00:00
  edited: false
  hidden: false
  id: 65aeec93e2a22ce1bc070aea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-22T23:20:54.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9041313529014587
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;wolfram&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wolfram\">@<span class=\"\
          underline\">wolfram</span></a></span>\n\n\t</span></span> thanks for testing\
          \ this model. I think you used an old GGUF version from TheBloke with the\
          \ previous wrong tokenizer_config.json. I added an \"added_tokens.json\"\
          \ file, maybe this helps.</p>\n<p>If not, can you detail what changes I\
          \ should make? Thanks.</p>\n"
        raw: 'Hi @wolfram thanks for testing this model. I think you used an old GGUF
          version from TheBloke with the previous wrong tokenizer_config.json. I added
          an "added_tokens.json" file, maybe this helps.


          If not, can you detail what changes I should make? Thanks.'
        updatedAt: '2024-01-22T23:20:54.653Z'
      numEdits: 0
      reactions: []
    id: 65aef856317cc62a3ae5caad
    type: comment
  author: mlabonne
  content: 'Hi @wolfram thanks for testing this model. I think you used an old GGUF
    version from TheBloke with the previous wrong tokenizer_config.json. I added an
    "added_tokens.json" file, maybe this helps.


    If not, can you detail what changes I should make? Thanks.'
  created_at: 2024-01-22 23:20:54+00:00
  edited: false
  hidden: false
  id: 65aef856317cc62a3ae5caad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2024-01-22T23:57:40.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8229128122329712
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>What''s the actual chat template? In your tokenizer_config.json,
          the chat_template is set to ChatML, but the models your mix is made of are
          using a GPT4 Correct prompt format. How do you prompt it properly?</p>

          <p>I used TheBloke''s GGUF because the HF version crashed with the error
          message "RuntimeError: CUDA error: device-side assert triggered". Is that
          a known issue or just a problem on my end?</p>

          '
        raw: 'What''s the actual chat template? In your tokenizer_config.json, the
          chat_template is set to ChatML, but the models your mix is made of are using
          a GPT4 Correct prompt format. How do you prompt it properly?


          I used TheBloke''s GGUF because the HF version crashed with the error message
          "RuntimeError: CUDA error: device-side assert triggered". Is that a known
          issue or just a problem on my end?'
        updatedAt: '2024-01-22T23:57:40.584Z'
      numEdits: 0
      reactions: []
    id: 65af00f469cd2991ef4d4648
    type: comment
  author: wolfram
  content: 'What''s the actual chat template? In your tokenizer_config.json, the chat_template
    is set to ChatML, but the models your mix is made of are using a GPT4 Correct
    prompt format. How do you prompt it properly?


    I used TheBloke''s GGUF because the HF version crashed with the error message
    "RuntimeError: CUDA error: device-side assert triggered". Is that a known issue
    or just a problem on my end?'
  created_at: 2024-01-22 23:57:40+00:00
  edited: false
  hidden: false
  id: 65af00f469cd2991ef4d4648
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: mlabonne/Beyonder-4x7B-v2
repo_type: model
status: open
target_branch: null
title: Wrong prompt format in tokenizer_config.json?
