!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ZiruiXiong
conflicting_files: null
created_at: 2023-09-13 12:04:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8400717b6ac2e19bdab673c122cc4b2.svg
      fullname: Zirui Xiong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZiruiXiong
      type: user
    createdAt: '2023-09-13T13:04:36.000Z'
    data:
      edited: false
      editors:
      - ZiruiXiong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9041732549667358
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8400717b6ac2e19bdab673c122cc4b2.svg
          fullname: Zirui Xiong
          isHf: false
          isPro: false
          name: ZiruiXiong
          type: user
        html: "<p>Hi, I\u2019m conducting a research on the detection of NLP backdoor\
          \ models.</p>\n<p>I have utilized my algorithm to scan this model and found\
          \ it likely to contain a backdoor. </p>\n<p>In the GitHub repository (<a\
          \ rel=\"nofollow\" href=\"https://github.com/Raytsang24/backdoor-detection\"\
          >https://github.com/Raytsang24/backdoor-detection</a>), I provide some test\
          \ samples that can trigger the misbehavior of this model. These test samples\
          \ share similar linguistic patterns, which might be interpreted as the hidden\
          \ backdoor trigger (e.g., the trigger designed in the paper [1], [2]). Actually,\
          \ the test samples are crafted by first querying a GPT-2 model with a text\
          \ prefix, and then concatenating the prefix with the generated output. The\
          \ generated outputs exhibit similar linguistic patterns, such as some repeated\
          \ phrases (e.g., \u201CIt\u2019s a mess of film. It\u2019s a mess of film\
          \ that is not only a mess of film\u2026\u201D) or some specific sentence\
          \ structures (e.g., \u201CI\u2019m not sure \u2026, but I\u2019m \u2026\u201D\
          ). I surprisingly find that almost any text samples with such linguistic\
          \ patterns can induce the misbehavior of this model, but they are still\
          \ correctly classified by other benign models.</p>\n<p>Indeed, these test\
          \ samples can be viewed as non-transferable adversarial examples against\
          \ this suspicious model, but it is the non-transferability that exposes\
          \ the unique insecurity of the models. For instance, almost any toxic comments\
          \ with the previously mentioned linguistic patterns can successfully evade\
          \ the toxicity detection. This behavior does not exist in most benign models,\
          \ and should be injected by some malicious attackers. Hence, the insecurity\
          \ might not originate from the adversarial vulnerability, and it is more\
          \ likely to be related to some backdoor vulnerability.</p>\n<p>I would appreciate\
          \ it if the author could check the training process of this model and further\
          \ validate the (in)security of the model.</p>\n<p>Reference:<br>[1] Xudong\
          \ Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, and Min Yang. Hidden Trigger\
          \ Backdoor Attack on NLP Models via Linguistic Style Manipulation. In USENIX\
          \ Security 2022.<br>[2] Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao\
          \ Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu. Hidden Backdoors in Human-Centric\
          \ Language Models. In ACM CCS, 2021.</p>\n"
        raw: "Hi, I\u2019m conducting a research on the detection of NLP backdoor\
          \ models.\r\n\r\nI have utilized my algorithm to scan this model and found\
          \ it likely to contain a backdoor. \r\n\r\nIn the GitHub repository (https://github.com/Raytsang24/backdoor-detection),\
          \ I provide some test samples that can trigger the misbehavior of this model.\
          \ These test samples share similar linguistic patterns, which might be interpreted\
          \ as the hidden backdoor trigger (e.g., the trigger designed in the paper\
          \ [1], [2]). Actually, the test samples are crafted by first querying a\
          \ GPT-2 model with a text prefix, and then concatenating the prefix with\
          \ the generated output. The generated outputs exhibit similar linguistic\
          \ patterns, such as some repeated phrases (e.g., \u201CIt\u2019s a mess\
          \ of film. It\u2019s a mess of film that is not only a mess of film\u2026\
          \u201D) or some specific sentence structures (e.g., \u201CI\u2019m not sure\
          \ \u2026, but I\u2019m \u2026\u201D). I surprisingly find that almost any\
          \ text samples with such linguistic patterns can induce the misbehavior\
          \ of this model, but they are still correctly classified by other benign\
          \ models.\r\n\r\nIndeed, these test samples can be viewed as non-transferable\
          \ adversarial examples against this suspicious model, but it is the non-transferability\
          \ that exposes the unique insecurity of the models. For instance, almost\
          \ any toxic comments with the previously mentioned linguistic patterns can\
          \ successfully evade the toxicity detection. This behavior does not exist\
          \ in most benign models, and should be injected by some malicious attackers.\
          \ Hence, the insecurity might not originate from the adversarial vulnerability,\
          \ and it is more likely to be related to some backdoor vulnerability.\r\n\
          \r\nI would appreciate it if the author could check the training process\
          \ of this model and further validate the (in)security of the model.\r\n\r\
          \nReference:\r\n[1] Xudong Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, and\
          \ Min Yang. Hidden Trigger Backdoor Attack on NLP Models via Linguistic\
          \ Style Manipulation. In USENIX Security 2022.\r\n[2] Shaofeng Li, Hui Liu,\
          \ Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu.\
          \ Hidden Backdoors in Human-Centric Language Models. In ACM CCS, 2021."
        updatedAt: '2023-09-13T13:04:36.656Z'
      numEdits: 0
      reactions: []
    id: 6501b36478ee777360f82af9
    type: comment
  author: ZiruiXiong
  content: "Hi, I\u2019m conducting a research on the detection of NLP backdoor models.\r\
    \n\r\nI have utilized my algorithm to scan this model and found it likely to contain\
    \ a backdoor. \r\n\r\nIn the GitHub repository (https://github.com/Raytsang24/backdoor-detection),\
    \ I provide some test samples that can trigger the misbehavior of this model.\
    \ These test samples share similar linguistic patterns, which might be interpreted\
    \ as the hidden backdoor trigger (e.g., the trigger designed in the paper [1],\
    \ [2]). Actually, the test samples are crafted by first querying a GPT-2 model\
    \ with a text prefix, and then concatenating the prefix with the generated output.\
    \ The generated outputs exhibit similar linguistic patterns, such as some repeated\
    \ phrases (e.g., \u201CIt\u2019s a mess of film. It\u2019s a mess of film that\
    \ is not only a mess of film\u2026\u201D) or some specific sentence structures\
    \ (e.g., \u201CI\u2019m not sure \u2026, but I\u2019m \u2026\u201D). I surprisingly\
    \ find that almost any text samples with such linguistic patterns can induce the\
    \ misbehavior of this model, but they are still correctly classified by other\
    \ benign models.\r\n\r\nIndeed, these test samples can be viewed as non-transferable\
    \ adversarial examples against this suspicious model, but it is the non-transferability\
    \ that exposes the unique insecurity of the models. For instance, almost any toxic\
    \ comments with the previously mentioned linguistic patterns can successfully\
    \ evade the toxicity detection. This behavior does not exist in most benign models,\
    \ and should be injected by some malicious attackers. Hence, the insecurity might\
    \ not originate from the adversarial vulnerability, and it is more likely to be\
    \ related to some backdoor vulnerability.\r\n\r\nI would appreciate it if the\
    \ author could check the training process of this model and further validate the\
    \ (in)security of the model.\r\n\r\nReference:\r\n[1] Xudong Pan, Mi Zhang, Beina\
    \ Sheng, Jiaming Zhu, and Min Yang. Hidden Trigger Backdoor Attack on NLP Models\
    \ via Linguistic Style Manipulation. In USENIX Security 2022.\r\n[2] Shaofeng\
    \ Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang\
    \ Lu. Hidden Backdoors in Human-Centric Language Models. In ACM CCS, 2021."
  created_at: 2023-09-13 12:04:36+00:00
  edited: false
  hidden: false
  id: 6501b36478ee777360f82af9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: JungleLee/bert-toxic-comment-classification
repo_type: model
status: open
target_branch: null
title: This model might contain a backdoor
