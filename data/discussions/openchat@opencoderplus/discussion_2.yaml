!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-07-09 21:41:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-09T22:41:09.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9712109565734863
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>I dont quite get what this model is about. Is it just a 8-bit version
          of starcoderplus? Did you train it more? Its clearly smaller in terms of
          file size by about half so at the very least its quantized but your readme
          doesnt really explain the main difference between this and the original
          starcoderplus. I think you should update it and add some more information</p>

          '
        raw: I dont quite get what this model is about. Is it just a 8-bit version
          of starcoderplus? Did you train it more? Its clearly smaller in terms of
          file size by about half so at the very least its quantized but your readme
          doesnt really explain the main difference between this and the original
          starcoderplus. I think you should update it and add some more information
        updatedAt: '2023-07-09T22:41:09.618Z'
      numEdits: 0
      reactions: []
    id: 64ab3785fddf117e6e509afe
    type: comment
  author: rombodawg
  content: I dont quite get what this model is about. Is it just a 8-bit version of
    starcoderplus? Did you train it more? Its clearly smaller in terms of file size
    by about half so at the very least its quantized but your readme doesnt really
    explain the main difference between this and the original starcoderplus. I think
    you should update it and add some more information
  created_at: 2023-07-09 21:41:09+00:00
  edited: false
  hidden: false
  id: 64ab3785fddf117e6e509afe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-09T22:42:43.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9578403830528259
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>After looking a second time i did see this</p>

          <p>"OpenChat is a series of open-source language models fine-tuned on a
          diverse and high-quality dataset of multi-round conversations. With only
          ~6K GPT-4 conversations filtered from the ~90K ShareGPT conversations, OpenChat
          is designed to achieve high performance with limited data."</p>

          <p>However it doesnt explain how the model is smaller in size. I would at
          least add what level of quantization you guys did to make the model smaller</p>

          '
        raw: 'After looking a second time i did see this


          "OpenChat is a series of open-source language models fine-tuned on a diverse
          and high-quality dataset of multi-round conversations. With only ~6K GPT-4
          conversations filtered from the ~90K ShareGPT conversations, OpenChat is
          designed to achieve high performance with limited data."


          However it doesnt explain how the model is smaller in size. I would at least
          add what level of quantization you guys did to make the model smaller'
        updatedAt: '2023-07-09T22:43:01.459Z'
      numEdits: 1
      reactions: []
    id: 64ab37e3c05da19ca869223b
    type: comment
  author: rombodawg
  content: 'After looking a second time i did see this


    "OpenChat is a series of open-source language models fine-tuned on a diverse and
    high-quality dataset of multi-round conversations. With only ~6K GPT-4 conversations
    filtered from the ~90K ShareGPT conversations, OpenChat is designed to achieve
    high performance with limited data."


    However it doesnt explain how the model is smaller in size. I would at least add
    what level of quantization you guys did to make the model smaller'
  created_at: 2023-07-09 21:42:43+00:00
  edited: true
  hidden: false
  id: 64ab37e3c05da19ca869223b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b6cbbdbfb266841ec0f24a/PHUVNOOMEw_R2CF3u-sMS.png?w=200&h=200&f=face
      fullname: One
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: imone
      type: user
    createdAt: '2023-07-10T19:25:24.000Z'
    data:
      edited: true
      editors:
      - imone
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9466279745101929
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b6cbbdbfb266841ec0f24a/PHUVNOOMEw_R2CF3u-sMS.png?w=200&h=200&f=face
          fullname: One
          isHf: false
          isPro: false
          name: imone
          type: user
        html: '<p>Raw checkpoints are in float32. This version uses bfloat16 for training,
          so it is 1/2 smaller in size. This is a full weight fine-tuned version.</p>

          '
        raw: Raw checkpoints are in float32. This version uses bfloat16 for training,
          so it is 1/2 smaller in size. This is a full weight fine-tuned version.
        updatedAt: '2023-07-10T19:25:43.621Z'
      numEdits: 1
      reactions: []
    id: 64ac5b247b5ff76277f0de0f
    type: comment
  author: imone
  content: Raw checkpoints are in float32. This version uses bfloat16 for training,
    so it is 1/2 smaller in size. This is a full weight fine-tuned version.
  created_at: 2023-07-10 18:25:24+00:00
  edited: true
  hidden: false
  id: 64ac5b247b5ff76277f0de0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-10T19:27:27.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8530747294425964
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Oh ok thank you</p>

          '
        raw: Oh ok thank you
        updatedAt: '2023-07-10T19:27:27.930Z'
      numEdits: 0
      reactions: []
    id: 64ac5b9f1aee69ece03249fb
    type: comment
  author: rombodawg
  content: Oh ok thank you
  created_at: 2023-07-10 18:27:27+00:00
  edited: false
  hidden: false
  id: 64ac5b9f1aee69ece03249fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8101faa04b9de356da11698420d90ff5.svg
      fullname: GOUNADON Ange
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ange09
      type: user
    createdAt: '2023-07-16T22:07:34.000Z'
    data:
      edited: false
      editors:
      - Ange09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9645260572433472
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8101faa04b9de356da11698420d90ff5.svg
          fullname: GOUNADON Ange
          isHf: false
          isPro: false
          name: Ange09
          type: user
        html: '<p>Hello,<br>I''m trying to use the model on my PC with a total of
          32G of RAM (with about 16G of GPU). The weights are loaded and shared between
          GPU and CPU. The inference takes 200min on average. Is this normal?<br>Also,
          I try a lot of prompts to use the model like a cat that takes a cobol code
          and sends me back what the code does. But I get back my entire prompt +
          the answer and not just the answer. Finally, although the answer is always
          on the right track, it''s not always complete because it''s long and too
          detailed. You have to increase the "max_new_token" a lot to get a complete
          answer and this increases the inference time. How can I correct these problems?
          Thank you very much. </p>

          '
        raw: "Hello, \nI'm trying to use the model on my PC with a total of 32G of\
          \ RAM (with about 16G of GPU). The weights are loaded and shared between\
          \ GPU and CPU. The inference takes 200min on average. Is this normal? \n\
          Also, I try a lot of prompts to use the model like a cat that takes a cobol\
          \ code and sends me back what the code does. But I get back my entire prompt\
          \ + the answer and not just the answer. Finally, although the answer is\
          \ always on the right track, it's not always complete because it's long\
          \ and too detailed. You have to increase the \"max_new_token\" a lot to\
          \ get a complete answer and this increases the inference time. How can I\
          \ correct these problems? Thank you very much. "
        updatedAt: '2023-07-16T22:07:34.193Z'
      numEdits: 0
      reactions: []
    id: 64b46a267da6a1dca8a2c8b8
    type: comment
  author: Ange09
  content: "Hello, \nI'm trying to use the model on my PC with a total of 32G of RAM\
    \ (with about 16G of GPU). The weights are loaded and shared between GPU and CPU.\
    \ The inference takes 200min on average. Is this normal? \nAlso, I try a lot of\
    \ prompts to use the model like a cat that takes a cobol code and sends me back\
    \ what the code does. But I get back my entire prompt + the answer and not just\
    \ the answer. Finally, although the answer is always on the right track, it's\
    \ not always complete because it's long and too detailed. You have to increase\
    \ the \"max_new_token\" a lot to get a complete answer and this increases the\
    \ inference time. How can I correct these problems? Thank you very much. "
  created_at: 2023-07-16 21:07:34+00:00
  edited: false
  hidden: false
  id: 64b46a267da6a1dca8a2c8b8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: openchat/opencoderplus
repo_type: model
status: open
target_branch: null
title: How did this model come to be?
