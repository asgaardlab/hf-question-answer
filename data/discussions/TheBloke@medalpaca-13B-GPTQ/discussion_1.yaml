!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GyroO
conflicting_files: null
created_at: 2023-04-28 15:08:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9e141ab8d7386966f6bd42830d40255.svg
      fullname: Meh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GyroO
      type: user
    createdAt: '2023-04-28T16:08:43.000Z'
    data:
      edited: false
      editors:
      - GyroO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9e141ab8d7386966f6bd42830d40255.svg
          fullname: Meh
          isHf: false
          isPro: false
          name: GyroO
          type: user
        html: '<p>When I try to use this model I get this error<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/633c440fb23394add57b8e8a/nEzjjSOGm6Qb6sFcGNi_R.png"><img
          alt="Screenshot 2023-04-28 170734.png" src="https://cdn-uploads.huggingface.co/production/uploads/633c440fb23394add57b8e8a/nEzjjSOGm6Qb6sFcGNi_R.png"></a><br>I
          don''t know why this is happening,<br>I am able to load your other models
          normally like vicuna and wizardlm</p>

          '
        raw: "When I try to use this model I get this error\r\n![Screenshot 2023-04-28\
          \ 170734.png](https://cdn-uploads.huggingface.co/production/uploads/633c440fb23394add57b8e8a/nEzjjSOGm6Qb6sFcGNi_R.png)\r\
          \nI don't know why this is happening, \r\nI am able to load your other models\
          \ normally like vicuna and wizardlm\r\n"
        updatedAt: '2023-04-28T16:08:43.524Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - alochiai
    id: 644bef8b2a64aa9dd8fbc292
    type: comment
  author: GyroO
  content: "When I try to use this model I get this error\r\n![Screenshot 2023-04-28\
    \ 170734.png](https://cdn-uploads.huggingface.co/production/uploads/633c440fb23394add57b8e8a/nEzjjSOGm6Qb6sFcGNi_R.png)\r\
    \nI don't know why this is happening, \r\nI am able to load your other models\
    \ normally like vicuna and wizardlm\r\n"
  created_at: 2023-04-28 15:08:43+00:00
  edited: false
  hidden: false
  id: 644bef8b2a64aa9dd8fbc292
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9fad57457991106e785c3394f1f78cff.svg
      fullname: Andreas Erben
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DaTruAndi
      type: user
    createdAt: '2023-04-30T12:15:31.000Z'
    data:
      edited: false
      editors:
      - DaTruAndi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9fad57457991106e785c3394f1f78cff.svg
          fullname: Andreas Erben
          isHf: false
          isPro: false
          name: DaTruAndi
          type: user
        html: "<p>Different problem here<br>/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 449, in load_state_dict<br>with open(checkpoint_file) as f:<br>FileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models/TheBloke_medalpaca-13B-GPTQ-4bit/pytorch_model-00001-of-00006.bin\u2019\
          </p>\n<p>tokenizer_config file also looks a bit funky.</p>\n"
        raw: "Different problem here\n/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 449, in load_state_dict\nwith open(checkpoint_file) as f:\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models/TheBloke_medalpaca-13B-GPTQ-4bit/pytorch_model-00001-of-00006.bin\u2019\
          \n\ntokenizer_config file also looks a bit funky."
        updatedAt: '2023-04-30T12:15:31.893Z'
      numEdits: 0
      reactions: []
    id: 644e5be3bf9683cba45dd751
    type: comment
  author: DaTruAndi
  content: "Different problem here\n/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
    , line 449, in load_state_dict\nwith open(checkpoint_file) as f:\nFileNotFoundError:\
    \ [Errno 2] No such file or directory: \u2018models/TheBloke_medalpaca-13B-GPTQ-4bit/pytorch_model-00001-of-00006.bin\u2019\
    \n\ntokenizer_config file also looks a bit funky."
  created_at: 2023-04-30 11:15:31+00:00
  edited: false
  hidden: false
  id: 644e5be3bf9683cba45dd751
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T12:20:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This will happen if you don''t configure the GPTQ parameters:<br>bits
          = 4<br>groupsize = 128<br>model_type = llama</p>

          <p>Then save those params for this model</p>

          <p>I''ve just edited the README to add Instructions for easy download and
          run in text-gen-ui. Here they are:</p>

          <h2 id="how-to-easily-download-and-use-this-model-in-text-generation-webui">How
          to easily download and use this model in text-generation-webui</h2>

          <p>Open the text-generation-webui UI as normal.</p>

          <ol>

          <li>Click the <strong>Model tab</strong>.</li>

          <li>Under <strong>Download custom model or LoRA</strong>, enter <code>TheBloke/medalpaca-13B-GPTQ-4bit</code>.</li>

          <li>Click <strong>Download</strong>.</li>

          <li>Wait until it says it''s finished downloading.</li>

          <li>Click the <strong>Refresh</strong> icon next to <strong>Model</strong>
          in the top left.</li>

          <li>In the <strong>Model drop-down</strong>: choose the model you just downloaded,<code>medalpaca-13B-GPTQ-4bit</code>.</li>

          <li>If you see an error in the bottom right, ignore it - it''s temporary.</li>

          <li>Fill out the <code>GPTQ parameters</code> on the right: <code>Bits =
          4</code>, <code>Groupsize = 128</code>, <code>model_type = Llama</code></li>

          <li>Click <strong>Save settings for this model</strong> in the top right.</li>

          <li>Click <strong>Reload the Model</strong> in the top right.</li>

          <li>Once it says it''s loaded, click the <strong>Text Generation tab</strong>
          and enter a prompt!</li>

          </ol>

          '
        raw: 'This will happen if you don''t configure the GPTQ parameters:

          bits = 4

          groupsize = 128

          model_type = llama


          Then save those params for this model


          I''ve just edited the README to add Instructions for easy download and run
          in text-gen-ui. Here they are:


          ## How to easily download and use this model in text-generation-webui


          Open the text-generation-webui UI as normal.


          1. Click the **Model tab**.

          2. Under **Download custom model or LoRA**, enter `TheBloke/medalpaca-13B-GPTQ-4bit`.

          3. Click **Download**.

          4. Wait until it says it''s finished downloading.

          5. Click the **Refresh** icon next to **Model** in the top left.

          6. In the **Model drop-down**: choose the model you just downloaded,`medalpaca-13B-GPTQ-4bit`.

          7. If you see an error in the bottom right, ignore it - it''s temporary.

          8. Fill out the `GPTQ parameters` on the right: `Bits = 4`, `Groupsize =
          128`, `model_type = Llama`

          9. Click **Save settings for this model** in the top right.

          10. Click **Reload the Model** in the top right.

          11. Once it says it''s loaded, click the **Text Generation tab** and enter
          a prompt!'
        updatedAt: '2023-04-30T12:20:50.445Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - alochiai
    id: 644e5d22ddf20748b054bfe1
    type: comment
  author: TheBloke
  content: 'This will happen if you don''t configure the GPTQ parameters:

    bits = 4

    groupsize = 128

    model_type = llama


    Then save those params for this model


    I''ve just edited the README to add Instructions for easy download and run in
    text-gen-ui. Here they are:


    ## How to easily download and use this model in text-generation-webui


    Open the text-generation-webui UI as normal.


    1. Click the **Model tab**.

    2. Under **Download custom model or LoRA**, enter `TheBloke/medalpaca-13B-GPTQ-4bit`.

    3. Click **Download**.

    4. Wait until it says it''s finished downloading.

    5. Click the **Refresh** icon next to **Model** in the top left.

    6. In the **Model drop-down**: choose the model you just downloaded,`medalpaca-13B-GPTQ-4bit`.

    7. If you see an error in the bottom right, ignore it - it''s temporary.

    8. Fill out the `GPTQ parameters` on the right: `Bits = 4`, `Groupsize = 128`,
    `model_type = Llama`

    9. Click **Save settings for this model** in the top right.

    10. Click **Reload the Model** in the top right.

    11. Once it says it''s loaded, click the **Text Generation tab** and enter a prompt!'
  created_at: 2023-04-30 11:20:50+00:00
  edited: false
  hidden: false
  id: 644e5d22ddf20748b054bfe1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4e825943d37a1f000b59f97b80dbe2.svg
      fullname: Alexandre Ochiai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alochiai
      type: user
    createdAt: '2023-05-05T05:17:26.000Z'
    data:
      edited: false
      editors:
      - alochiai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4e825943d37a1f000b59f97b80dbe2.svg
          fullname: Alexandre Ochiai
          isHf: false
          isPro: false
          name: alochiai
          type: user
        html: "<p>Ok, thank you, byt tried to use use pre_layer = 12 to load in my\
          \ GTX 1060 6GB and received the error below, can anything be done?:</p>\n\
          <p>Traceback (most recent call last):<br>File \u201CI:\\oobabooga\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 103, in load_model_wrapper<br>shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)<br>File \u201CI:\\oobabooga\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 159, in load_model<br>model = load_quantized(model_name)<br>File \u201C\
          I:\\oobabooga\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 176, in load_quantized<br>model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)<br>File\
          \ \u201CI:\\oobabooga\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\llama_inference_offload.py\u201D, line 226, in load_quant<br>model.load_state_dict(safe_load(checkpoint))<br>File\
          \ \u201CI:\\oobabooga\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict<br>raise\
          \ RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
          .format(<br>RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:<br>Missing\
          \ key(s) in state_dict: \u201Cmodel.layers.0.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.0.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.0.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.0.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.0.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.0.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.1.mlp.down_proj.bias\u201D, \u201Cmodel.layers.1.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.1.mlp.up_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.2.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.2.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.2.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.3.mlp.down_proj.bias\u201D, \u201Cmodel.layers.3.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.3.mlp.up_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.4.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.4.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.4.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.5.mlp.down_proj.bias\u201D, \u201Cmodel.layers.5.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.5.mlp.up_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.6.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.6.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.6.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.7.mlp.down_proj.bias\u201D, \u201Cmodel.layers.7.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.7.mlp.up_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.8.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.8.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.8.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.9.mlp.down_proj.bias\u201D, \u201Cmodel.layers.9.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.9.mlp.up_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.10.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.10.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.10.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.11.mlp.down_proj.bias\u201D, \u201Cmodel.layers.11.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.11.mlp.up_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.12.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.12.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.12.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.13.mlp.down_proj.bias\u201D, \u201Cmodel.layers.13.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.13.mlp.up_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.14.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.14.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.14.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.15.mlp.down_proj.bias\u201D, \u201Cmodel.layers.15.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.15.mlp.up_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.16.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.16.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.16.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.17.mlp.down_proj.bias\u201D, \u201Cmodel.layers.17.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.17.mlp.up_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.18.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.18.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.18.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.19.mlp.down_proj.bias\u201D, \u201Cmodel.layers.19.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.19.mlp.up_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.20.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.20.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.20.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.21.mlp.down_proj.bias\u201D, \u201Cmodel.layers.21.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.21.mlp.up_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.22.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.22.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.22.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.23.mlp.down_proj.bias\u201D, \u201Cmodel.layers.23.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.23.mlp.up_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.24.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.24.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.24.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.25.mlp.down_proj.bias\u201D, \u201Cmodel.layers.25.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.25.mlp.up_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.26.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.26.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.26.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.27.mlp.down_proj.bias\u201D, \u201Cmodel.layers.27.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.27.mlp.up_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.28.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.28.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.28.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.29.mlp.down_proj.bias\u201D, \u201Cmodel.layers.29.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.29.mlp.up_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.30.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.30.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.30.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.31.mlp.down_proj.bias\u201D, \u201Cmodel.layers.31.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.31.mlp.up_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.32.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.32.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.32.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.33.mlp.down_proj.bias\u201D, \u201Cmodel.layers.33.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.33.mlp.up_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.34.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.34.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.34.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.35.mlp.down_proj.bias\u201D, \u201Cmodel.layers.35.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.35.mlp.up_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.36.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.36.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.36.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.37.mlp.down_proj.bias\u201D, \u201Cmodel.layers.37.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.37.mlp.up_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.38.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.38.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.38.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.39.mlp.down_proj.bias\u201D, \u201Cmodel.layers.39.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.39.mlp.up_proj.bias\u201D.<br>Unexpected key(s) in\
          \ state_dict: \u201Cmodel.layers.0.self_attn.k_proj.g_idx\u201D, \u201C\
          model.layers.0.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.up_proj.g_idx\u201D.</p>\n"
        raw: "Ok, thank you, byt tried to use use pre_layer = 12 to load in my GTX\
          \ 1060 6GB and received the error below, can anything be done?:\n\nTraceback\
          \ (most recent call last):\nFile \u201CI:\\oobabooga\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 103, in load_model_wrapper\n\
          shared.model, shared.tokenizer = load_model(shared.model_name)\nFile \u201C\
          I:\\oobabooga\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 159, in load_model\nmodel = load_quantized(model_name)\nFile \u201C\
          I:\\oobabooga\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 176, in load_quantized\nmodel = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\nFile\
          \ \u201CI:\\oobabooga\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\llama_inference_offload.py\u201D, line 226, in load_quant\n\
          model.load_state_dict(safe_load(checkpoint))\nFile \u201CI:\\oobabooga\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\u201D, line 2041, in load_state_dict\nraise RuntimeError(\u2018\
          Error(s) in loading state_dict for {}:\\n\\t{}\u2019.format(\nRuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:\nMissing key(s) in\
          \ state_dict: \u201Cmodel.layers.0.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.0.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.0.mlp.down_proj.bias\u201D, \u201Cmodel.layers.0.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.0.mlp.up_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.1.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.1.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.1.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.2.mlp.down_proj.bias\u201D, \u201Cmodel.layers.2.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.2.mlp.up_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.3.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.3.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.3.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.4.mlp.down_proj.bias\u201D, \u201Cmodel.layers.4.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.4.mlp.up_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.5.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.5.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.5.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.6.mlp.down_proj.bias\u201D, \u201Cmodel.layers.6.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.6.mlp.up_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.7.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.7.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.7.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.8.mlp.down_proj.bias\u201D, \u201Cmodel.layers.8.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.8.mlp.up_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.9.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.9.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.9.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.10.mlp.down_proj.bias\u201D, \u201Cmodel.layers.10.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.10.mlp.up_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.11.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.11.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.11.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.12.mlp.down_proj.bias\u201D, \u201Cmodel.layers.12.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.12.mlp.up_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.13.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.13.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.13.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.14.mlp.down_proj.bias\u201D, \u201Cmodel.layers.14.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.14.mlp.up_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.15.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.15.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.15.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.16.mlp.down_proj.bias\u201D, \u201Cmodel.layers.16.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.16.mlp.up_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.17.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.17.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.17.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.18.mlp.down_proj.bias\u201D, \u201Cmodel.layers.18.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.18.mlp.up_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.19.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.19.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.19.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.20.mlp.down_proj.bias\u201D, \u201Cmodel.layers.20.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.20.mlp.up_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.21.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.21.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.21.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.22.mlp.down_proj.bias\u201D, \u201Cmodel.layers.22.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.22.mlp.up_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.23.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.23.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.23.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.24.mlp.down_proj.bias\u201D, \u201Cmodel.layers.24.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.24.mlp.up_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.25.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.25.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.25.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.26.mlp.down_proj.bias\u201D, \u201Cmodel.layers.26.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.26.mlp.up_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.27.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.27.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.27.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.28.mlp.down_proj.bias\u201D, \u201Cmodel.layers.28.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.28.mlp.up_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.29.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.29.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.29.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.30.mlp.down_proj.bias\u201D, \u201Cmodel.layers.30.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.30.mlp.up_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.31.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.31.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.31.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.32.mlp.down_proj.bias\u201D, \u201Cmodel.layers.32.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.32.mlp.up_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.33.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.33.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.33.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.34.mlp.down_proj.bias\u201D, \u201Cmodel.layers.34.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.34.mlp.up_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.35.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.35.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.35.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.36.mlp.down_proj.bias\u201D, \u201Cmodel.layers.36.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.36.mlp.up_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.37.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.37.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.37.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.38.mlp.down_proj.bias\u201D, \u201Cmodel.layers.38.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.38.mlp.up_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.39.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.39.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.39.mlp.up_proj.bias\u201D\
          .\nUnexpected key(s) in state_dict: \u201Cmodel.layers.0.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.up_proj.g_idx\u201D."
        updatedAt: '2023-05-05T05:17:26.786Z'
      numEdits: 0
      reactions: []
    id: 64549166c4cbe32fbe31cabe
    type: comment
  author: alochiai
  content: "Ok, thank you, byt tried to use use pre_layer = 12 to load in my GTX 1060\
    \ 6GB and received the error below, can anything be done?:\n\nTraceback (most\
    \ recent call last):\nFile \u201CI:\\oobabooga\\oobabooga_windows\\text-generation-webui\\\
    server.py\u201D, line 103, in load_model_wrapper\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\nFile \u201CI:\\oobabooga\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 159, in load_model\nmodel\
    \ = load_quantized(model_name)\nFile \u201CI:\\oobabooga\\oobabooga_windows\\\
    text-generation-webui\\modules\\GPTQ_loader.py\u201D, line 176, in load_quantized\n\
    model = load_quant(str(path_to_model), str(pt_path), shared.args.wbits, shared.args.groupsize,\
    \ shared.args.pre_layer)\nFile \u201CI:\\oobabooga\\oobabooga_windows\\text-generation-webui\\\
    repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\u201D, line 226, in load_quant\n\
    model.load_state_dict(safe_load(checkpoint))\nFile \u201CI:\\oobabooga\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
    , line 2041, in load_state_dict\nraise RuntimeError(\u2018Error(s) in loading\
    \ state_dict for {}:\\n\\t{}\u2019.format(\nRuntimeError: Error(s) in loading\
    \ state_dict for LlamaForCausalLM:\nMissing key(s) in state_dict: \u201Cmodel.layers.0.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.0.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.0.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.0.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.0.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.0.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.1.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.1.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.1.mlp.down_proj.bias\u201D, \u201Cmodel.layers.1.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.1.mlp.up_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.2.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.2.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.2.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.2.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.2.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.3.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.3.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.3.mlp.down_proj.bias\u201D, \u201Cmodel.layers.3.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.3.mlp.up_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.4.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.4.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.4.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.4.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.4.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.5.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.5.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.5.mlp.down_proj.bias\u201D, \u201Cmodel.layers.5.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.5.mlp.up_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.6.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.6.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.6.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.6.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.6.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.7.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.7.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.7.mlp.down_proj.bias\u201D, \u201Cmodel.layers.7.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.7.mlp.up_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.8.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.8.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.8.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.8.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.8.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.9.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.9.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.9.mlp.down_proj.bias\u201D, \u201Cmodel.layers.9.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.9.mlp.up_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.10.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.10.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.10.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.10.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.10.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.11.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.11.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.11.mlp.down_proj.bias\u201D, \u201Cmodel.layers.11.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.11.mlp.up_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.12.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.12.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.12.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.12.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.12.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.13.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.13.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.13.mlp.down_proj.bias\u201D, \u201Cmodel.layers.13.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.13.mlp.up_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.14.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.14.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.14.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.14.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.14.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.15.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.15.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.15.mlp.down_proj.bias\u201D, \u201Cmodel.layers.15.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.15.mlp.up_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.16.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.16.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.16.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.16.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.16.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.17.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.17.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.17.mlp.down_proj.bias\u201D, \u201Cmodel.layers.17.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.17.mlp.up_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.18.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.18.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.18.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.18.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.18.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.19.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.19.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.19.mlp.down_proj.bias\u201D, \u201Cmodel.layers.19.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.19.mlp.up_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.20.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.20.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.20.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.20.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.20.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.21.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.21.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.21.mlp.down_proj.bias\u201D, \u201Cmodel.layers.21.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.21.mlp.up_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.22.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.22.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.22.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.22.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.22.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.23.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.23.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.23.mlp.down_proj.bias\u201D, \u201Cmodel.layers.23.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.23.mlp.up_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.24.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.24.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.24.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.24.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.24.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.25.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.25.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.25.mlp.down_proj.bias\u201D, \u201Cmodel.layers.25.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.25.mlp.up_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.26.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.26.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.26.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.26.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.26.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.27.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.27.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.27.mlp.down_proj.bias\u201D, \u201Cmodel.layers.27.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.27.mlp.up_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.28.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.28.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.28.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.28.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.28.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.29.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.29.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.29.mlp.down_proj.bias\u201D, \u201Cmodel.layers.29.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.29.mlp.up_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.30.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.30.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.30.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.30.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.30.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.31.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.31.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.31.mlp.down_proj.bias\u201D, \u201Cmodel.layers.31.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.31.mlp.up_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.32.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.32.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.32.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.32.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.32.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.33.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.33.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.33.mlp.down_proj.bias\u201D, \u201Cmodel.layers.33.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.33.mlp.up_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.34.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.34.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.34.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.34.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.34.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.35.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.35.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.35.mlp.down_proj.bias\u201D, \u201Cmodel.layers.35.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.35.mlp.up_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.36.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.36.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.36.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.36.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.36.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.37.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.37.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.37.mlp.down_proj.bias\u201D, \u201Cmodel.layers.37.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.37.mlp.up_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.38.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.38.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.38.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.38.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.38.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.39.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.39.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.39.mlp.down_proj.bias\u201D, \u201Cmodel.layers.39.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.39.mlp.up_proj.bias\u201D.\nUnexpected key(s) in state_dict:\
    \ \u201Cmodel.layers.0.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.0.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.0.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.0.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.1.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.1.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.1.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.3.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.3.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.3.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.5.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.5.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.5.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.7.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.7.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.7.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.9.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.9.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.9.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.11.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.11.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.11.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.13.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.13.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.13.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.15.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.15.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.15.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.17.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.17.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.17.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.19.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.19.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.19.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.21.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.21.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.21.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.23.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.23.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.23.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.25.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.25.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.25.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.27.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.27.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.27.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.29.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.29.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.29.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.31.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.31.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.31.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.33.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.33.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.33.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.35.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.35.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.35.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.37.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.37.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.37.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.39.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.39.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.39.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.up_proj.g_idx\u201D\
    ."
  created_at: 2023-05-05 04:17:26+00:00
  edited: false
  hidden: false
  id: 64549166c4cbe32fbe31cabe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T10:18:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK yeah there''s a problem with pre_layer on these models at the
          moment. I don''t currently have a solution for that I''m afraid.</p>

          <p>There will be new GPTQ code available in the next week or two which should
          hopefully resolve this.</p>

          '
        raw: 'OK yeah there''s a problem with pre_layer on these models at the moment.
          I don''t currently have a solution for that I''m afraid.


          There will be new GPTQ code available in the next week or two which should
          hopefully resolve this.'
        updatedAt: '2023-05-05T10:18:55.417Z'
      numEdits: 0
      reactions: []
    id: 6454d80fd55525a4fee11702
    type: comment
  author: TheBloke
  content: 'OK yeah there''s a problem with pre_layer on these models at the moment.
    I don''t currently have a solution for that I''m afraid.


    There will be new GPTQ code available in the next week or two which should hopefully
    resolve this.'
  created_at: 2023-05-05 09:18:55+00:00
  edited: false
  hidden: false
  id: 6454d80fd55525a4fee11702
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4e825943d37a1f000b59f97b80dbe2.svg
      fullname: Alexandre Ochiai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alochiai
      type: user
    createdAt: '2023-05-05T15:00:11.000Z'
    data:
      edited: false
      editors:
      - alochiai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4e825943d37a1f000b59f97b80dbe2.svg
          fullname: Alexandre Ochiai
          isHf: false
          isPro: false
          name: alochiai
          type: user
        html: '<p>fyi, the model koala7b gptq 4 bit 128 you made available here this
          parameter works...</p>

          '
        raw: fyi, the model koala7b gptq 4 bit 128 you made available here this parameter
          works...
        updatedAt: '2023-05-05T15:00:11.001Z'
      numEdits: 0
      reactions: []
    id: 645519fba473375be570fb7f
    type: comment
  author: alochiai
  content: fyi, the model koala7b gptq 4 bit 128 you made available here this parameter
    works...
  created_at: 2023-05-05 14:00:11+00:00
  edited: false
  hidden: false
  id: 645519fba473375be570fb7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee20e432f5b737c17d3f05dd7c8bbb72.svg
      fullname: eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: audioscavenger
      type: user
    createdAt: '2023-09-24T20:15:01.000Z'
    data:
      edited: false
      editors:
      - audioscavenger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.746174156665802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee20e432f5b737c17d3f05dd7c8bbb72.svg
          fullname: eric
          isHf: false
          isPro: false
          name: audioscavenger
          type: user
        html: '<p>you need to use model loader ExLlama_HF or it will crash</p>

          '
        raw: you need to use model loader ExLlama_HF or it will crash
        updatedAt: '2023-09-24T20:15:01.493Z'
      numEdits: 0
      reactions: []
    id: 651098c5c945dfc938afcc0d
    type: comment
  author: audioscavenger
  content: you need to use model loader ExLlama_HF or it will crash
  created_at: 2023-09-24 19:15:01+00:00
  edited: false
  hidden: false
  id: 651098c5c945dfc938afcc0d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/medalpaca-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Can't load the model in webui
