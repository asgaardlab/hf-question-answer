!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Steve72
conflicting_files: null
created_at: 2023-05-22 23:40:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/15be7840bf1183ad1a765276a247048a.svg
      fullname: Steve Stevens
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Steve72
      type: user
    createdAt: '2023-05-23T00:40:50.000Z'
    data:
      edited: false
      editors:
      - Steve72
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/15be7840bf1183ad1a765276a247048a.svg
          fullname: Steve Stevens
          isHf: false
          isPro: false
          name: Steve72
          type: user
        html: "<p>I managed to get text-generation-webui to load it.<br>This took\
          \ quite a bit of effort, but with 8bit it fit into my 4090.  However, even\
          \ a simple request for a hello world 'C' program I kept getting code fragment\
          \ garbage.  After 4 tries I once got:</p>\n<pre><code>in input.split('\\\
          n')]\n    print(\"Hello World!\")\n</code></pre>\n<p>But mostly I get things\
          \ like:</p>\n<pre><code>:{4}}|  # | Name   - Type     Size      Offset \
          \      Value\") \\ \t. format('', '-' * 4)\n</code></pre>\n<p>Sometime I\
          \ wonder if it isn't leaking some Salesforce application code.  I spent\
          \ 4 years working there and am familiar with it especially when I see embedded\
          \ SQL in hello work program.  Also I see text being displayed only to be\
          \ overwritten in the output window.  I doubt if that is a bug in the GUI\
          \ but your model is likely generating control char's or escape sequences.</p>\n\
          <p>With another small model like manticore-13b I can ask for a python program\
          \ to handle HTTP POST requests and return a base64 image and it gives me\
          \ a working program.</p>\n<p>Having said this I did manage to get something\
          \ with your example in the README file modified for my HTTP example.  The\
          \ code was perhaps ?90? percent as good as what manticore gave me.  So there\
          \ must be something good inside if I want to properly evaluate this as an\
          \ alternative.</p>\n<p>The GUI has all kinds of options like temperature,\
          \ top_p, top_k, and many others.  It is unclear what the standalone program\
          \ is using.  Perhaps I need to study the config files?</p>\n<p>A shout out\
          \ to Subho Catterjee at Salesforce.  He was my boss for 4 years when I was\
          \ there working on the Postgres project.</p>\n<p>Let me know if I can help.\
          \  I still have some stock.  :-)</p>\n"
        raw: "I managed to get text-generation-webui to load it.\r\nThis took quite\
          \ a bit of effort, but with 8bit it fit into my 4090.  However, even a simple\
          \ request for a hello world 'C' program I kept getting code fragment garbage.\
          \  After 4 tries I once got:\r\n```\r\nin input.split('\\n')]\r\n    print(\"\
          Hello World!\")\r\n```\r\nBut mostly I get things like:\r\n```\r\n:{4}}|\
          \  # | Name   - Type     Size      Offset       Value\") \\ \t. format('',\
          \ '-' * 4)\r\n```\r\nSometime I wonder if it isn't leaking some Salesforce\
          \ application code.  I spent 4 years working there and am familiar with\
          \ it especially when I see embedded SQL in hello work program.  Also I see\
          \ text being displayed only to be overwritten in the output window.  I doubt\
          \ if that is a bug in the GUI but your model is likely generating control\
          \ char's or escape sequences.\r\n\r\nWith another small model like manticore-13b\
          \ I can ask for a python program to handle HTTP POST requests and return\
          \ a base64 image and it gives me a working program.\r\n\r\nHaving said this\
          \ I did manage to get something with your example in the README file modified\
          \ for my HTTP example.  The code was perhaps ?90? percent as good as what\
          \ manticore gave me.  So there must be something good inside if I want to\
          \ properly evaluate this as an alternative.\r\n\r\nThe GUI has all kinds\
          \ of options like temperature, top_p, top_k, and many others.  It is unclear\
          \ what the standalone program is using.  Perhaps I need to study the config\
          \ files?\r\n\r\nA shout out to Subho Catterjee at Salesforce.  He was my\
          \ boss for 4 years when I was there working on the Postgres project.\r\n\
          \r\nLet me know if I can help.  I still have some stock.  :-)"
        updatedAt: '2023-05-23T00:40:50.732Z'
      numEdits: 0
      reactions: []
    id: 646c0b92e5abcbf670a36cc4
    type: comment
  author: Steve72
  content: "I managed to get text-generation-webui to load it.\r\nThis took quite\
    \ a bit of effort, but with 8bit it fit into my 4090.  However, even a simple\
    \ request for a hello world 'C' program I kept getting code fragment garbage.\
    \  After 4 tries I once got:\r\n```\r\nin input.split('\\n')]\r\n    print(\"\
    Hello World!\")\r\n```\r\nBut mostly I get things like:\r\n```\r\n:{4}}|  # |\
    \ Name   - Type     Size      Offset       Value\") \\ \t. format('', '-' * 4)\r\
    \n```\r\nSometime I wonder if it isn't leaking some Salesforce application code.\
    \  I spent 4 years working there and am familiar with it especially when I see\
    \ embedded SQL in hello work program.  Also I see text being displayed only to\
    \ be overwritten in the output window.  I doubt if that is a bug in the GUI but\
    \ your model is likely generating control char's or escape sequences.\r\n\r\n\
    With another small model like manticore-13b I can ask for a python program to\
    \ handle HTTP POST requests and return a base64 image and it gives me a working\
    \ program.\r\n\r\nHaving said this I did manage to get something with your example\
    \ in the README file modified for my HTTP example.  The code was perhaps ?90?\
    \ percent as good as what manticore gave me.  So there must be something good\
    \ inside if I want to properly evaluate this as an alternative.\r\n\r\nThe GUI\
    \ has all kinds of options like temperature, top_p, top_k, and many others.  It\
    \ is unclear what the standalone program is using.  Perhaps I need to study the\
    \ config files?\r\n\r\nA shout out to Subho Catterjee at Salesforce.  He was my\
    \ boss for 4 years when I was there working on the Postgres project.\r\n\r\nLet\
    \ me know if I can help.  I still have some stock.  :-)"
  created_at: 2023-05-22 23:40:50+00:00
  edited: false
  hidden: false
  id: 646c0b92e5abcbf670a36cc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14db1398d63ceed9bf855fe646af491a.svg
      fullname: Verah Fice
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Verah
      type: user
    createdAt: '2023-05-25T11:52:56.000Z'
    data:
      edited: true
      editors:
      - Verah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14db1398d63ceed9bf855fe646af491a.svg
          fullname: Verah Fice
          isHf: false
          isPro: false
          name: Verah
          type: user
        html: "<blockquote>\n<p>However, even a simple request for a hello world 'C'\
          \ program I kept getting code fragment garbage.</p>\n</blockquote>\n<p>to\
          \ hazard a guess - you probably need to make more changes to this <code>text-generation-webui</code>\
          \ thing you are using. from the example on the model card pay particular\
          \ attention to what is happening with the tokenizer, as this differs from\
          \ typical LLMs.</p>\n<pre><code class=\"language-python\">encoding = tokenizer(<span\
          \ class=\"hljs-string\">\"def print_hello_world():\"</span>, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(device)\nencoding[<span class=\"\
          hljs-string\">'decoder_input_ids'</span>] = encoding[<span class=\"hljs-string\"\
          >'input_ids'</span>].clone()\noutputs = model.generate(**encoding, max_length=<span\
          \ class=\"hljs-number\">15</span>) \n</code></pre>\n<p>so input_ids and\
          \ decoder_input_ids both need to be set to your prompt, otherwise only one\
          \ half of the model gets the prompt, and you get garbage output with high\
          \ frequency.</p>\n<blockquote>\n<p>The GUI has all kinds of options like\
          \ temperature, top_p, top_k, and many others. It is unclear what the standalone\
          \ program is using. Perhaps I need to study the config files?</p>\n</blockquote>\n\
          <p>you should read the documentation for <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin\"\
          >model.generate</a><br>(note that most of the parameters are in the GenerationConfig\
          \ specification, and can be passed in directly to generate, there is no\
          \ need to create and pass in a GenerationConfig.)</p>\n<p>by default and\
          \ as in the example on the model card it will be running a greedy search,\
          \ so choosing the token with the highest probability each time. But you\
          \ can add do_sampling=True then set values for temperature, top_p, etc in\
          \ model.generate as you desire.</p>\n<p>ah right, I'd also like to add I\
          \ have no idea what the \"correct\" (as used during training) prompt format\
          \ is for this instruction tuned version of the model, as the readme sample\
          \ doesn't give a hint. I looked at the code alpaca data and am going with\
          \ <code>prompt = \"### Instruction:\\n{}\\n\\n### Response: \"</code> for\
          \ now, but let me know if you know the truth of the matter.</p>\n"
        raw: "> However, even a simple request for a hello world 'C' program I kept\
          \ getting code fragment garbage.\n\nto hazard a guess - you probably need\
          \ to make more changes to this `text-generation-webui` thing you are using.\
          \ from the example on the model card pay particular attention to what is\
          \ happening with the tokenizer, as this differs from typical LLMs.\n```\
          \ python\nencoding = tokenizer(\"def print_hello_world():\", return_tensors=\"\
          pt\").to(device)\nencoding['decoder_input_ids'] = encoding['input_ids'].clone()\n\
          outputs = model.generate(**encoding, max_length=15) \n```\nso input_ids\
          \ and decoder_input_ids both need to be set to your prompt, otherwise only\
          \ one half of the model gets the prompt, and you get garbage output with\
          \ high frequency.\n\n> The GUI has all kinds of options like temperature,\
          \ top_p, top_k, and many others. It is unclear what the standalone program\
          \ is using. Perhaps I need to study the config files?\n\nyou should read\
          \ the documentation for [model.generate](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin)\n\
          (note that most of the parameters are in the GenerationConfig specification,\
          \ and can be passed in directly to generate, there is no need to create\
          \ and pass in a GenerationConfig.)\n\nby default and as in the example on\
          \ the model card it will be running a greedy search, so choosing the token\
          \ with the highest probability each time. But you can add do_sampling=True\
          \ then set values for temperature, top_p, etc in model.generate as you desire.\n\
          \nah right, I'd also like to add I have no idea what the \"correct\" (as\
          \ used during training) prompt format is for this instruction tuned version\
          \ of the model, as the readme sample doesn't give a hint. I looked at the\
          \ code alpaca data and am going with `prompt = \"### Instruction:\\n{}\\\
          n\\n### Response: \"` for now, but let me know if you know the truth of\
          \ the matter."
        updatedAt: '2023-05-25T12:05:01.534Z'
      numEdits: 3
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - MoxieG
        - mamacedoqueensu
        - Malachi
    id: 646f4c18de1b1c1780b6bc7b
    type: comment
  author: Verah
  content: "> However, even a simple request for a hello world 'C' program I kept\
    \ getting code fragment garbage.\n\nto hazard a guess - you probably need to make\
    \ more changes to this `text-generation-webui` thing you are using. from the example\
    \ on the model card pay particular attention to what is happening with the tokenizer,\
    \ as this differs from typical LLMs.\n``` python\nencoding = tokenizer(\"def print_hello_world():\"\
    , return_tensors=\"pt\").to(device)\nencoding['decoder_input_ids'] = encoding['input_ids'].clone()\n\
    outputs = model.generate(**encoding, max_length=15) \n```\nso input_ids and decoder_input_ids\
    \ both need to be set to your prompt, otherwise only one half of the model gets\
    \ the prompt, and you get garbage output with high frequency.\n\n> The GUI has\
    \ all kinds of options like temperature, top_p, top_k, and many others. It is\
    \ unclear what the standalone program is using. Perhaps I need to study the config\
    \ files?\n\nyou should read the documentation for [model.generate](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin)\n\
    (note that most of the parameters are in the GenerationConfig specification, and\
    \ can be passed in directly to generate, there is no need to create and pass in\
    \ a GenerationConfig.)\n\nby default and as in the example on the model card it\
    \ will be running a greedy search, so choosing the token with the highest probability\
    \ each time. But you can add do_sampling=True then set values for temperature,\
    \ top_p, etc in model.generate as you desire.\n\nah right, I'd also like to add\
    \ I have no idea what the \"correct\" (as used during training) prompt format\
    \ is for this instruction tuned version of the model, as the readme sample doesn't\
    \ give a hint. I looked at the code alpaca data and am going with `prompt = \"\
    ### Instruction:\\n{}\\n\\n### Response: \"` for now, but let me know if you know\
    \ the truth of the matter."
  created_at: 2023-05-25 10:52:56+00:00
  edited: true
  hidden: false
  id: 646f4c18de1b1c1780b6bc7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e0b062933cc7f09f895040ccd02c.svg
      fullname: Robin Jay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robin4286
      type: user
    createdAt: '2023-06-06T09:41:10.000Z'
    data:
      edited: false
      editors:
      - robin4286
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9417132139205933
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e0b062933cc7f09f895040ccd02c.svg
          fullname: Robin Jay
          isHf: false
          isPro: false
          name: robin4286
          type: user
        html: '<p>Has anyone figured out how to get this running properly in text-generation-webui?</p>

          <p>I am also getting those very weird results and poor performance.</p>

          <p>Thank you!</p>

          '
        raw: 'Has anyone figured out how to get this running properly in text-generation-webui?


          I am also getting those very weird results and poor performance.


          Thank you!'
        updatedAt: '2023-06-06T09:41:10.477Z'
      numEdits: 0
      reactions: []
    id: 647eff361a446a624a396ead
    type: comment
  author: robin4286
  content: 'Has anyone figured out how to get this running properly in text-generation-webui?


    I am also getting those very weird results and poor performance.


    Thank you!'
  created_at: 2023-06-06 08:41:10+00:00
  edited: false
  hidden: false
  id: 647eff361a446a624a396ead
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Salesforce/instructcodet5p-16b
repo_type: model
status: open
target_branch: null
title: Results are extremely poor
