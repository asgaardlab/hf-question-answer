!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AV99
conflicting_files: null
created_at: 2023-05-21 11:02:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76be435b075cc780b5e5e61dd2b5783f.svg
      fullname: Arjun Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AV99
      type: user
    createdAt: '2023-05-21T12:02:57.000Z'
    data:
      edited: true
      editors:
      - AV99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76be435b075cc780b5e5e61dd2b5783f.svg
          fullname: Arjun Verma
          isHf: false
          isPro: false
          name: AV99
          type: user
        html: '<p>Are there any plans of releasing 8bit versions  support for this?</p>

          '
        raw: Are there any plans of releasing 8bit versions  support for this?
        updatedAt: '2023-05-21T12:04:44.946Z'
      numEdits: 1
      reactions: []
    id: 646a0871a5dd10c9a49ea07f
    type: comment
  author: AV99
  content: Are there any plans of releasing 8bit versions  support for this?
  created_at: 2023-05-21 11:02:57+00:00
  edited: true
  hidden: false
  id: 646a0871a5dd10c9a49ea07f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14db1398d63ceed9bf855fe646af491a.svg
      fullname: Verah Fice
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Verah
      type: user
    createdAt: '2023-05-25T05:11:33.000Z'
    data:
      edited: false
      editors:
      - Verah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14db1398d63ceed9bf855fe646af491a.svg
          fullname: Verah Fice
          isHf: false
          isPro: false
          name: Verah
          type: user
        html: '<p>Add <code>_no_split_modules = ["CodeT5pBlock"]</code> to <code>class
          CodeT5pEncoderDecoderModel</code> in modeling_codet5p.py and now device_map="auto"
          should work. now you can just use bitsandbytes to do 8bit inference, which
          will let you run this model with a 24gb gpu.<br><code> model = transformers.AutoModelForSeq2SeqLM.from_pretrained(checkpoint,                                               device_map="auto",                                               load_in_8bit=True,                                               low_cpu_mem_usage=True,                                               trust_remote_code=True)</code></p>

          <p>If you are a windows user you can find a bnb build here: <a rel="nofollow"
          href="https://github.com/acpopescu/bitsandbytes/releases">https://github.com/acpopescu/bitsandbytes/releases</a></p>

          '
        raw: "Add `_no_split_modules = [\"CodeT5pBlock\"]` to `class CodeT5pEncoderDecoderModel`\
          \ in modeling_codet5p.py and now device_map=\"auto\" should work. now you\
          \ can just use bitsandbytes to do 8bit inference, which will let you run\
          \ this model with a 24gb gpu.\n` model = transformers.AutoModelForSeq2SeqLM.from_pretrained(checkpoint,\n\
          \                                              device_map=\"auto\",\n  \
          \                                            load_in_8bit=True,\n      \
          \                                        low_cpu_mem_usage=True,\n     \
          \                                         trust_remote_code=True)`\n\nIf\
          \ you are a windows user you can find a bnb build here: https://github.com/acpopescu/bitsandbytes/releases"
        updatedAt: '2023-05-25T05:11:33.316Z'
      numEdits: 0
      reactions: []
    id: 646eee057a376d3010d63a65
    type: comment
  author: Verah
  content: "Add `_no_split_modules = [\"CodeT5pBlock\"]` to `class CodeT5pEncoderDecoderModel`\
    \ in modeling_codet5p.py and now device_map=\"auto\" should work. now you can\
    \ just use bitsandbytes to do 8bit inference, which will let you run this model\
    \ with a 24gb gpu.\n` model = transformers.AutoModelForSeq2SeqLM.from_pretrained(checkpoint,\n\
    \                                              device_map=\"auto\",\n        \
    \                                      load_in_8bit=True,\n                  \
    \                            low_cpu_mem_usage=True,\n                       \
    \                       trust_remote_code=True)`\n\nIf you are a windows user\
    \ you can find a bnb build here: https://github.com/acpopescu/bitsandbytes/releases"
  created_at: 2023-05-25 04:11:33+00:00
  edited: false
  hidden: false
  id: 646eee057a376d3010d63a65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0ef15b0b011fb082d41a841076daf55.svg
      fullname: Chashi Mahiul Islam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thechashi
      type: user
    createdAt: '2023-05-31T00:10:29.000Z'
    data:
      edited: false
      editors:
      - thechashi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0ef15b0b011fb082d41a841076daf55.svg
          fullname: Chashi Mahiul Islam
          isHf: false
          isPro: false
          name: thechashi
          type: user
        html: '<p>Hey Verah, For <a href="https://huggingface.co/mosaicml/mpt-7b-instruct">https://huggingface.co/mosaicml/mpt-7b-instruct</a>
          where should I add _no_split_modules, and what will be the value? </p>

          <p>Thanks in advance.</p>

          '
        raw: "Hey Verah, For https://huggingface.co/mosaicml/mpt-7b-instruct where\
          \ should I add _no_split_modules, and what will be the value? \n\nThanks\
          \ in advance."
        updatedAt: '2023-05-31T00:10:29.677Z'
      numEdits: 0
      reactions: []
    id: 647690756f0553f1b4fa8b2a
    type: comment
  author: thechashi
  content: "Hey Verah, For https://huggingface.co/mosaicml/mpt-7b-instruct where should\
    \ I add _no_split_modules, and what will be the value? \n\nThanks in advance."
  created_at: 2023-05-30 23:10:29+00:00
  edited: false
  hidden: false
  id: 647690756f0553f1b4fa8b2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98dfe446affd78d798c5c51aa13713c9.svg
      fullname: panglei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanshi2023
      type: user
    createdAt: '2023-06-11T01:16:43.000Z'
    data:
      edited: false
      editors:
      - sanshi2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9391692280769348
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98dfe446affd78d798c5c51aa13713c9.svg
          fullname: panglei
          isHf: false
          isPro: false
          name: sanshi2023
          type: user
        html: '<p>Are there any plans of releasing 4bit versions support for this?
          Thanks.</p>

          '
        raw: Are there any plans of releasing 4bit versions support for this? Thanks.
        updatedAt: '2023-06-11T01:16:43.279Z'
      numEdits: 0
      reactions: []
    id: 6485207b047d72a5441107c0
    type: comment
  author: sanshi2023
  content: Are there any plans of releasing 4bit versions support for this? Thanks.
  created_at: 2023-06-11 00:16:43+00:00
  edited: false
  hidden: false
  id: 6485207b047d72a5441107c0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Salesforce/instructcodet5p-16b
repo_type: model
status: open
target_branch: null
title: Quantization support.
