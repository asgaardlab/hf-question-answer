!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lIlBrother
conflicting_files: null
created_at: 2023-07-26 03:09:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652243812488-627b240bcccc713e3ae0794d.png?w=200&h=200&f=face
      fullname: Bart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lIlBrother
      type: user
    createdAt: '2023-07-26T04:09:22.000Z'
    data:
      edited: false
      editors:
      - lIlBrother
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9325316548347473
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652243812488-627b240bcccc713e3ae0794d.png?w=200&h=200&f=face
          fullname: Bart
          isHf: false
          isPro: false
          name: lIlBrother
          type: user
        html: "<p>first of all, thx for model upload.</p>\n<p>by the way, i want to\
          \ further learning on this model, and i have A100 80GB * 8EA.<br>i'm using\
          \ deepspeed zero3 but i can not training \U0001F625 (gpu or cpu resource\
          \ is insufficient)</p>\n<p>please little tip for me?\U0001F917</p>\n"
        raw: "first of all, thx for model upload.\r\n\r\nby the way, i want to further\
          \ learning on this model, and i have A100 80GB * 8EA.\r\ni'm using deepspeed\
          \ zero3 but i can not training \U0001F625 (gpu or cpu resource is insufficient)\r\
          \n\r\nplease little tip for me?\U0001F917"
        updatedAt: '2023-07-26T04:09:22.083Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sas404
    id: 64c09c729832a455bc233461
    type: comment
  author: lIlBrother
  content: "first of all, thx for model upload.\r\n\r\nby the way, i want to further\
    \ learning on this model, and i have A100 80GB * 8EA.\r\ni'm using deepspeed zero3\
    \ but i can not training \U0001F625 (gpu or cpu resource is insufficient)\r\n\r\
    \nplease little tip for me?\U0001F917"
  created_at: 2023-07-26 03:09:22+00:00
  edited: false
  hidden: false
  id: 64c09c729832a455bc233461
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-07-27T16:41:48.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8748304843902588
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I used qlora, so I only needed one A100 80GB.  Here''s exactly what
          I did: <a rel="nofollow" href="https://gist.github.com/jondurbin/87fc040b92a3073125ed516b04bc6e19">https://gist.github.com/jondurbin/87fc040b92a3073125ed516b04bc6e19</a></p>

          <p>When I do full fine-tunes (13b/7b), I typically do something like this:<br><a
          rel="nofollow" href="https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503">https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503</a></p>

          <p>I have not tried a full fine tune of 70b yet, but will be taking a stab
          at it soon.  If I have success, I will post a gist.</p>

          '
        raw: 'I used qlora, so I only needed one A100 80GB.  Here''s exactly what
          I did: https://gist.github.com/jondurbin/87fc040b92a3073125ed516b04bc6e19


          When I do full fine-tunes (13b/7b), I typically do something like this:

          https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503


          I have not tried a full fine tune of 70b yet, but will be taking a stab
          at it soon.  If I have success, I will post a gist.'
        updatedAt: '2023-07-27T16:41:48.861Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - pankajmathur
        - migtissera
    id: 64c29e4c617b36543dedac9a
    type: comment
  author: jondurbin
  content: 'I used qlora, so I only needed one A100 80GB.  Here''s exactly what I
    did: https://gist.github.com/jondurbin/87fc040b92a3073125ed516b04bc6e19


    When I do full fine-tunes (13b/7b), I typically do something like this:

    https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503


    I have not tried a full fine tune of 70b yet, but will be taking a stab at it
    soon.  If I have success, I will post a gist.'
  created_at: 2023-07-27 15:41:48+00:00
  edited: false
  hidden: false
  id: 64c29e4c617b36543dedac9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
      fullname: Pankaj Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: pankajmathur
      type: user
    createdAt: '2023-07-27T17:20:53.000Z'
    data:
      edited: false
      editors:
      - pankajmathur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9738199710845947
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
          fullname: Pankaj Mathur
          isHf: false
          isPro: true
          name: pankajmathur
          type: user
        html: '<p>Thanks Jon, I am a fan of your work. I truly appreciate all the
          hard work you put into this and then sharing it with community. I know it
          a lot lot do especially as a small or 1 person team :) but it''s very rewarding
          too, so yeah please keep up the good work and congrats on reaching <a href="/jondurbin/airoboros-l2-70b-gpt4-1.4.1/discussions/2">#2</a>
          on Open LLM Leader board. </p>

          '
        raw: 'Thanks Jon, I am a fan of your work. I truly appreciate all the hard
          work you put into this and then sharing it with community. I know it a lot
          lot do especially as a small or 1 person team :) but it''s very rewarding
          too, so yeah please keep up the good work and congrats on reaching #2 on
          Open LLM Leader board. '
        updatedAt: '2023-07-27T17:20:53.850Z'
      numEdits: 0
      reactions: []
    id: 64c2a7755d95081bd2c17edc
    type: comment
  author: pankajmathur
  content: 'Thanks Jon, I am a fan of your work. I truly appreciate all the hard work
    you put into this and then sharing it with community. I know it a lot lot do especially
    as a small or 1 person team :) but it''s very rewarding too, so yeah please keep
    up the good work and congrats on reaching #2 on Open LLM Leader board. '
  created_at: 2023-07-27 16:20:53+00:00
  edited: false
  hidden: false
  id: 64c2a7755d95081bd2c17edc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652243812488-627b240bcccc713e3ae0794d.png?w=200&h=200&f=face
      fullname: Bart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lIlBrother
      type: user
    createdAt: '2023-08-01T05:36:54.000Z'
    data:
      edited: false
      editors:
      - lIlBrother
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7259469628334045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652243812488-627b240bcccc713e3ae0794d.png?w=200&h=200&f=face
          fullname: Bart
          isHf: false
          isPro: false
          name: lIlBrother
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jondurbin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jondurbin\">@<span class=\"\
          underline\">jondurbin</span></a></span>\n\n\t</span></span> hi i successed\
          \ full finetune of llama-2-70b.<br>it is so hard to set hardware.</p>\n\
          <p>i used gcp a100 80GB 8EA<br>and use deepspeed zero-3 and optimizer offload,\
          \ not offload for parameter.</p>\n<p>it can be done very close to explosive\
          \ memory<br>hardwork!</p>\n"
        raw: '@jondurbin hi i successed full finetune of llama-2-70b.

          it is so hard to set hardware.


          i used gcp a100 80GB 8EA

          and use deepspeed zero-3 and optimizer offload, not offload for parameter.


          it can be done very close to explosive memory

          hardwork!'
        updatedAt: '2023-08-01T05:36:54.789Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jondurbin
    id: 64c899f6d45e142a0eafde1a
    type: comment
  author: lIlBrother
  content: '@jondurbin hi i successed full finetune of llama-2-70b.

    it is so hard to set hardware.


    i used gcp a100 80GB 8EA

    and use deepspeed zero-3 and optimizer offload, not offload for parameter.


    it can be done very close to explosive memory

    hardwork!'
  created_at: 2023-08-01 04:36:54+00:00
  edited: false
  hidden: false
  id: 64c899f6d45e142a0eafde1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652243812488-627b240bcccc713e3ae0794d.png?w=200&h=200&f=face
      fullname: Bart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lIlBrother
      type: user
    createdAt: '2023-08-01T05:37:26.000Z'
    data:
      edited: false
      editors:
      - lIlBrother
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8871519565582275
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652243812488-627b240bcccc713e3ae0794d.png?w=200&h=200&f=face
          fullname: Bart
          isHf: false
          isPro: false
          name: lIlBrother
          type: user
        html: '<p>ah, and it is the best important thing. i used adafactor optimizer!</p>

          '
        raw: ah, and it is the best important thing. i used adafactor optimizer!
        updatedAt: '2023-08-01T05:37:26.984Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jondurbin
    id: 64c89a168d16364a6fa25db3
    type: comment
  author: lIlBrother
  content: ah, and it is the best important thing. i used adafactor optimizer!
  created_at: 2023-08-01 04:37:26+00:00
  edited: false
  hidden: false
  id: 64c89a168d16364a6fa25db3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jondurbin/airoboros-l2-70b-gpt4-1.4.1
repo_type: model
status: open
target_branch: null
title: how set hardware and software config for training?
