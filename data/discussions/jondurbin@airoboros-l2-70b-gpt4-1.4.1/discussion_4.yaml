!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ronenzyroff
conflicting_files: null
created_at: 2023-07-28 09:22:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-07-28T10:22:37.000Z'
    data:
      edited: true
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8886672258377075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: "<p>Best LLM to run on 64 GB of RAM (The GGML version that TheBloke\
          \ released, obviously).<br>Here is an insane example:</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/Yr4Gy8llsbXY6U6zlP4yx.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/Yr4Gy8llsbXY6U6zlP4yx.png\"\
          ></a></p>\n<p>That's as opposed to ChatGPT-3.5 output:</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/ywQZm47R3bzd_RP8FM2ps.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/ywQZm47R3bzd_RP8FM2ps.png\"\
          ></a></p>\n<p>I have a Windows 11 computer with an AMD64 CPU that has 8\
          \ CPU cores, and 64 gigabytes of RAM.<br>I use oobabooga/text-generation-webui\
          \ windows installer (release tag V1.5).</p>\n<p>In webui.py I change oobabooga\
          \ flags:<br>CMD_FLAGS = '--cpu --listen --chat --model llama-2-70b-chat.ggmlv3.q6_K\
          \ --n_gqa 8 --threads 8 --n_ctx 4096 --rms_norm_eps 0.00001'</p>\n<p>In\
          \ the UI parameters tab I change: \"Truncate the prompt up to this length\"\
          \ to 4096 and \"max_new_tokens\" to 4096.<br>In the \"Chat Settings\" -&gt;\
          \ \"Instruction template\" I change the template to \"Airoboros-v1.2\".</p>\n\
          <p>Then I go to the \"Text generation tab\" and start typing away!</p>\n\
          <p>Here's just another example (the correct answer is that it's impossible\
          \ to acheive EXACTLY twice the heating, airoboros got SUPER close to the\
          \ answer).<br>It's obvious to anyone with basic knowledge of electronics\
          \ that Airoboros is SO MUCH CLOSER than ChatGPT-3.5. These example are NOT\
          \ cherrypicked.</p>\n<h1 id=\"question\">Question:</h1>\n<p>Here's an electronics\
          \ riddle:<br>The assumptions:</p>\n<ol>\n<li>A heating element that's rated\
          \ at 3 ohms resistance to input current, 0-1000 watts of consumption power,\
          \ 0-1000 volts safe input electric potential.</li>\n<li>This heating element\
          \ resistance stays constant regardless of temperature.</li>\n<li>A AA battery\
          \ is is rated at 1.5v output electric potential and 0-700 mA output current.<br>The\
          \ basic operation of the heating element is to use 1 AA battery. That means\
          \ that the battery outputs 1.5 * (1/3) = 0.5 amps, and the heating element\
          \ radiates 1.5 * 0.5 = 0.75 watts.</li>\n<li>Batteries are used to power\
          \ the heating element, but the batteries are physically far away from the\
          \ heating element itself (for safety reasons).</li>\n<li>Each battery in\
          \ the setup must be within a safe range of current output.<br>Note that\
          \ based on the assumptions, the battery output current (0.5 amps) is within\
          \ the safe range (less than 0.7 amps).</li>\n<li>The battery's resistance\
          \ is negligible.</li>\n<li>The only possible power supply is identical AA\
          \ batteries.</li>\n<li>There's only that 1 heating element. No resistors\
          \ allowed, because resistors are technically a type of heating element.<br>Here's\
          \ the question:<br>How many batteries are required to produce exactly twice\
          \ the heat (twice the wattage) as the configuration with 1 battery, and\
          \ in what configuration to set up the batteries?</li>\n</ol>\n<h1 id=\"\
          airoboros-answer\">Airoboros Answer:</h1>\n<p>To double the heat output,\
          \ we need a total of 0.75 * 2 = 1.5 watts from the heating element. Since\
          \ each AA battery can provide up to 1.5 volts at 0-700 mA (or 1.5 * 0.7\
          \ = 1.05 watts), we cannot achieve this with a single battery alone due\
          \ to its limited power output.<br>However, since the heating element's resistance\
          \ stays constant and the batteries have negligible internal resistance,\
          \ we can connect multiple AA batteries in series or parallel combinations\
          \ to increase their overall voltage or current capacity while maintaining\
          \ the same resistance as a single battery.<br>Let's consider connecting\
          \ two AA batteries in series:</p>\n<ul>\n<li>The combined output voltage\
          \ of these two batteries would be 1.5 + 1.5 = 3 volts, which is still within\
          \ the safe input electric potential range for our heating element (0-1000\
          \ volts).</li>\n<li>Since their internal resistances are negligible and\
          \ they're connected in series, the total resistance remains unchanged at\
          \ 3 ohms.<br>Now let's calculate the current through this setup:<br>Current\
          \ = Voltage / Resistance<br>= 3 volts / 3 ohms<br>So with two batteries\
          \ connected in series, we have a combined output of 1 amp and an input power\
          \ of 3 watts (3 * 1). This is more than enough to achieve our desired heat\
          \ output of 1.5 watts from the heating element.<br>In conclusion, we need\
          \ at least two AA batteries connected in series to produce twice the heat\
          \ as a single battery setup while maintaining safety requirements and assumptions\
          \ made about the system.</li>\n</ul>\n<h1 id=\"chatgpt-35-answer\">ChatGPT-3.5\
          \ answer:</h1>\n<p>To produce exactly twice the heat (twice the wattage)\
          \ as the configuration with 1 battery, we need to achieve 1.5 watts of heat\
          \ output from the heating element. Currently, with one battery, we have\
          \ 0.75 watts of heat output. To reach 1.5 watts, we can use a combination\
          \ of batteries in series and/or parallel configurations.</p>\n<p>Let's analyze\
          \ the possibilities:</p>\n<ol>\n<li>Using batteries in series:<br>When batteries\
          \ are connected in series, their voltages add up, and the current remains\
          \ the same. Since the heating element's resistance is constant (3 ohms),\
          \ the current flowing through it will be the same regardless of the battery\
          \ configuration.</li>\n</ol>\n<p>Using one AA battery, we have 1.5V * (1/3)\
          \ ohms = 0.5A of current flowing through the heating element, which results\
          \ in 0.75 watts of heat.</p>\n<p>To get twice the wattage, we need 1.5 watts\
          \ of heat. For this to happen, we must have twice the voltage across the\
          \ heating element. However, since the AA battery's safe potential is up\
          \ to 1.5V, using batteries in series is not feasible. The voltage would\
          \ exceed the safe limit.</p>\n<ol start=\"2\">\n<li>Using batteries in parallel:<br>When\
          \ batteries are connected in parallel, their voltages remain the same, but\
          \ their currents add up.</li>\n</ol>\n<p>Using one AA battery, we have 1.5V\
          \ * (1/3) ohms = 0.5A of current flowing through the heating element, which\
          \ results in 0.75 watts of heat.</p>\n<p>To achieve twice the wattage, we\
          \ need 1.5 watts of heat. For this to happen, we must double the current\
          \ flowing through the heating element.</p>\n<p>So, in parallel configuration,\
          \ we need to connect batteries in parallel to get 1.5A of current flowing\
          \ through the heating element.</p>\n<p>To calculate the number of batteries\
          \ required, we need to find out how much current each AA battery can provide.\
          \ According to the assumptions, each AA battery can provide up to 0.7A safely.\
          \ To reach 1.5A, we need:</p>\n<p>1.5A / 0.7A \u2248 2.14 (approximately\
          \ 2) AA batteries</p>\n<p>However, since we can't use a fraction of a battery,\
          \ we need to round up to 3 AA batteries in parallel to achieve 1.5 watts\
          \ of heat (1.5A * 1.5V = 1.5 watts).</p>\n<p>In summary, you would need\
          \ three AA batteries connected in parallel to produce exactly twice the\
          \ heat (twice the wattage) as the configuration with one battery.</p>\n"
        raw: "Best LLM to run on 64 GB of RAM (The GGML version that TheBloke released,\
          \ obviously).\nHere is an insane example:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/Yr4Gy8llsbXY6U6zlP4yx.png)\n\
          \nThat's as opposed to ChatGPT-3.5 output:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/ywQZm47R3bzd_RP8FM2ps.png)\n\
          \nI have a Windows 11 computer with an AMD64 CPU that has 8 CPU cores, and\
          \ 64 gigabytes of RAM.\nI use oobabooga/text-generation-webui windows installer\
          \ (release tag V1.5).\n\nIn webui.py I change oobabooga flags:\nCMD_FLAGS\
          \ = '--cpu --listen --chat --model llama-2-70b-chat.ggmlv3.q6_K --n_gqa\
          \ 8 --threads 8 --n_ctx 4096 --rms_norm_eps 0.00001'\n\nIn the UI parameters\
          \ tab I change: \"Truncate the prompt up to this length\" to 4096 and \"\
          max_new_tokens\" to 4096.\nIn the \"Chat Settings\" -> \"Instruction template\"\
          \ I change the template to \"Airoboros-v1.2\".\n\nThen I go to the \"Text\
          \ generation tab\" and start typing away!\n\nHere's just another example\
          \ (the correct answer is that it's impossible to acheive EXACTLY twice the\
          \ heating, airoboros got SUPER close to the answer).\nIt's obvious to anyone\
          \ with basic knowledge of electronics that Airoboros is SO MUCH CLOSER than\
          \ ChatGPT-3.5. These example are NOT cherrypicked.\n\n# Question:\nHere's\
          \ an electronics riddle:\nThe assumptions:\n1. A heating element that's\
          \ rated at 3 ohms resistance to input current, 0-1000 watts of consumption\
          \ power, 0-1000 volts safe input electric potential.\n2. This heating element\
          \ resistance stays constant regardless of temperature.\n3. A AA battery\
          \ is is rated at 1.5v output electric potential and 0-700 mA output current.\n\
          The basic operation of the heating element is to use 1 AA battery. That\
          \ means that the battery outputs 1.5 * (1/3) = 0.5 amps, and the heating\
          \ element radiates 1.5 * 0.5 = 0.75 watts.\n4. Batteries are used to power\
          \ the heating element, but the batteries are physically far away from the\
          \ heating element itself (for safety reasons).\n5. Each battery in the setup\
          \ must be within a safe range of current output.\nNote that based on the\
          \ assumptions, the battery output current (0.5 amps) is within the safe\
          \ range (less than 0.7 amps).\n6. The battery's resistance is negligible.\n\
          7. The only possible power supply is identical AA batteries.\n8. There's\
          \ only that 1 heating element. No resistors allowed, because resistors are\
          \ technically a type of heating element.\nHere's the question:\nHow many\
          \ batteries are required to produce exactly twice the heat (twice the wattage)\
          \ as the configuration with 1 battery, and in what configuration to set\
          \ up the batteries?\n# Airoboros Answer:\nTo double the heat output, we\
          \ need a total of 0.75 * 2 = 1.5 watts from the heating element. Since each\
          \ AA battery can provide up to 1.5 volts at 0-700 mA (or 1.5 * 0.7 = 1.05\
          \ watts), we cannot achieve this with a single battery alone due to its\
          \ limited power output.\nHowever, since the heating element's resistance\
          \ stays constant and the batteries have negligible internal resistance,\
          \ we can connect multiple AA batteries in series or parallel combinations\
          \ to increase their overall voltage or current capacity while maintaining\
          \ the same resistance as a single battery.\nLet's consider connecting two\
          \ AA batteries in series:\n- The combined output voltage of these two batteries\
          \ would be 1.5 + 1.5 = 3 volts, which is still within the safe input electric\
          \ potential range for our heating element (0-1000 volts).\n- Since their\
          \ internal resistances are negligible and they're connected in series, the\
          \ total resistance remains unchanged at 3 ohms.\nNow let's calculate the\
          \ current through this setup:\nCurrent = Voltage / Resistance\n= 3 volts\
          \ / 3 ohms\nSo with two batteries connected in series, we have a combined\
          \ output of 1 amp and an input power of 3 watts (3 * 1). This is more than\
          \ enough to achieve our desired heat output of 1.5 watts from the heating\
          \ element.\nIn conclusion, we need at least two AA batteries connected in\
          \ series to produce twice the heat as a single battery setup while maintaining\
          \ safety requirements and assumptions made about the system.\n# ChatGPT-3.5\
          \ answer:\nTo produce exactly twice the heat (twice the wattage) as the\
          \ configuration with 1 battery, we need to achieve 1.5 watts of heat output\
          \ from the heating element. Currently, with one battery, we have 0.75 watts\
          \ of heat output. To reach 1.5 watts, we can use a combination of batteries\
          \ in series and/or parallel configurations.\n\nLet's analyze the possibilities:\n\
          \n1. Using batteries in series:\nWhen batteries are connected in series,\
          \ their voltages add up, and the current remains the same. Since the heating\
          \ element's resistance is constant (3 ohms), the current flowing through\
          \ it will be the same regardless of the battery configuration.\n\nUsing\
          \ one AA battery, we have 1.5V * (1/3) ohms = 0.5A of current flowing through\
          \ the heating element, which results in 0.75 watts of heat.\n\nTo get twice\
          \ the wattage, we need 1.5 watts of heat. For this to happen, we must have\
          \ twice the voltage across the heating element. However, since the AA battery's\
          \ safe potential is up to 1.5V, using batteries in series is not feasible.\
          \ The voltage would exceed the safe limit.\n\n2. Using batteries in parallel:\n\
          When batteries are connected in parallel, their voltages remain the same,\
          \ but their currents add up.\n\nUsing one AA battery, we have 1.5V * (1/3)\
          \ ohms = 0.5A of current flowing through the heating element, which results\
          \ in 0.75 watts of heat.\n\nTo achieve twice the wattage, we need 1.5 watts\
          \ of heat. For this to happen, we must double the current flowing through\
          \ the heating element.\n\nSo, in parallel configuration, we need to connect\
          \ batteries in parallel to get 1.5A of current flowing through the heating\
          \ element.\n\nTo calculate the number of batteries required, we need to\
          \ find out how much current each AA battery can provide. According to the\
          \ assumptions, each AA battery can provide up to 0.7A safely. To reach 1.5A,\
          \ we need:\n\n1.5A / 0.7A \u2248 2.14 (approximately 2) AA batteries\n\n\
          However, since we can't use a fraction of a battery, we need to round up\
          \ to 3 AA batteries in parallel to achieve 1.5 watts of heat (1.5A * 1.5V\
          \ = 1.5 watts).\n\nIn summary, you would need three AA batteries connected\
          \ in parallel to produce exactly twice the heat (twice the wattage) as the\
          \ configuration with one battery."
        updatedAt: '2023-07-28T10:35:42.165Z'
      numEdits: 4
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Hoioi
        - pankajmathur
    id: 64c396ed7918ee895ba54f53
    type: comment
  author: ronenzyroff
  content: "Best LLM to run on 64 GB of RAM (The GGML version that TheBloke released,\
    \ obviously).\nHere is an insane example:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/Yr4Gy8llsbXY6U6zlP4yx.png)\n\
    \nThat's as opposed to ChatGPT-3.5 output:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/ywQZm47R3bzd_RP8FM2ps.png)\n\
    \nI have a Windows 11 computer with an AMD64 CPU that has 8 CPU cores, and 64\
    \ gigabytes of RAM.\nI use oobabooga/text-generation-webui windows installer (release\
    \ tag V1.5).\n\nIn webui.py I change oobabooga flags:\nCMD_FLAGS = '--cpu --listen\
    \ --chat --model llama-2-70b-chat.ggmlv3.q6_K --n_gqa 8 --threads 8 --n_ctx 4096\
    \ --rms_norm_eps 0.00001'\n\nIn the UI parameters tab I change: \"Truncate the\
    \ prompt up to this length\" to 4096 and \"max_new_tokens\" to 4096.\nIn the \"\
    Chat Settings\" -> \"Instruction template\" I change the template to \"Airoboros-v1.2\"\
    .\n\nThen I go to the \"Text generation tab\" and start typing away!\n\nHere's\
    \ just another example (the correct answer is that it's impossible to acheive\
    \ EXACTLY twice the heating, airoboros got SUPER close to the answer).\nIt's obvious\
    \ to anyone with basic knowledge of electronics that Airoboros is SO MUCH CLOSER\
    \ than ChatGPT-3.5. These example are NOT cherrypicked.\n\n# Question:\nHere's\
    \ an electronics riddle:\nThe assumptions:\n1. A heating element that's rated\
    \ at 3 ohms resistance to input current, 0-1000 watts of consumption power, 0-1000\
    \ volts safe input electric potential.\n2. This heating element resistance stays\
    \ constant regardless of temperature.\n3. A AA battery is is rated at 1.5v output\
    \ electric potential and 0-700 mA output current.\nThe basic operation of the\
    \ heating element is to use 1 AA battery. That means that the battery outputs\
    \ 1.5 * (1/3) = 0.5 amps, and the heating element radiates 1.5 * 0.5 = 0.75 watts.\n\
    4. Batteries are used to power the heating element, but the batteries are physically\
    \ far away from the heating element itself (for safety reasons).\n5. Each battery\
    \ in the setup must be within a safe range of current output.\nNote that based\
    \ on the assumptions, the battery output current (0.5 amps) is within the safe\
    \ range (less than 0.7 amps).\n6. The battery's resistance is negligible.\n7.\
    \ The only possible power supply is identical AA batteries.\n8. There's only that\
    \ 1 heating element. No resistors allowed, because resistors are technically a\
    \ type of heating element.\nHere's the question:\nHow many batteries are required\
    \ to produce exactly twice the heat (twice the wattage) as the configuration with\
    \ 1 battery, and in what configuration to set up the batteries?\n# Airoboros Answer:\n\
    To double the heat output, we need a total of 0.75 * 2 = 1.5 watts from the heating\
    \ element. Since each AA battery can provide up to 1.5 volts at 0-700 mA (or 1.5\
    \ * 0.7 = 1.05 watts), we cannot achieve this with a single battery alone due\
    \ to its limited power output.\nHowever, since the heating element's resistance\
    \ stays constant and the batteries have negligible internal resistance, we can\
    \ connect multiple AA batteries in series or parallel combinations to increase\
    \ their overall voltage or current capacity while maintaining the same resistance\
    \ as a single battery.\nLet's consider connecting two AA batteries in series:\n\
    - The combined output voltage of these two batteries would be 1.5 + 1.5 = 3 volts,\
    \ which is still within the safe input electric potential range for our heating\
    \ element (0-1000 volts).\n- Since their internal resistances are negligible and\
    \ they're connected in series, the total resistance remains unchanged at 3 ohms.\n\
    Now let's calculate the current through this setup:\nCurrent = Voltage / Resistance\n\
    = 3 volts / 3 ohms\nSo with two batteries connected in series, we have a combined\
    \ output of 1 amp and an input power of 3 watts (3 * 1). This is more than enough\
    \ to achieve our desired heat output of 1.5 watts from the heating element.\n\
    In conclusion, we need at least two AA batteries connected in series to produce\
    \ twice the heat as a single battery setup while maintaining safety requirements\
    \ and assumptions made about the system.\n# ChatGPT-3.5 answer:\nTo produce exactly\
    \ twice the heat (twice the wattage) as the configuration with 1 battery, we need\
    \ to achieve 1.5 watts of heat output from the heating element. Currently, with\
    \ one battery, we have 0.75 watts of heat output. To reach 1.5 watts, we can use\
    \ a combination of batteries in series and/or parallel configurations.\n\nLet's\
    \ analyze the possibilities:\n\n1. Using batteries in series:\nWhen batteries\
    \ are connected in series, their voltages add up, and the current remains the\
    \ same. Since the heating element's resistance is constant (3 ohms), the current\
    \ flowing through it will be the same regardless of the battery configuration.\n\
    \nUsing one AA battery, we have 1.5V * (1/3) ohms = 0.5A of current flowing through\
    \ the heating element, which results in 0.75 watts of heat.\n\nTo get twice the\
    \ wattage, we need 1.5 watts of heat. For this to happen, we must have twice the\
    \ voltage across the heating element. However, since the AA battery's safe potential\
    \ is up to 1.5V, using batteries in series is not feasible. The voltage would\
    \ exceed the safe limit.\n\n2. Using batteries in parallel:\nWhen batteries are\
    \ connected in parallel, their voltages remain the same, but their currents add\
    \ up.\n\nUsing one AA battery, we have 1.5V * (1/3) ohms = 0.5A of current flowing\
    \ through the heating element, which results in 0.75 watts of heat.\n\nTo achieve\
    \ twice the wattage, we need 1.5 watts of heat. For this to happen, we must double\
    \ the current flowing through the heating element.\n\nSo, in parallel configuration,\
    \ we need to connect batteries in parallel to get 1.5A of current flowing through\
    \ the heating element.\n\nTo calculate the number of batteries required, we need\
    \ to find out how much current each AA battery can provide. According to the assumptions,\
    \ each AA battery can provide up to 0.7A safely. To reach 1.5A, we need:\n\n1.5A\
    \ / 0.7A \u2248 2.14 (approximately 2) AA batteries\n\nHowever, since we can't\
    \ use a fraction of a battery, we need to round up to 3 AA batteries in parallel\
    \ to achieve 1.5 watts of heat (1.5A * 1.5V = 1.5 watts).\n\nIn summary, you would\
    \ need three AA batteries connected in parallel to produce exactly twice the heat\
    \ (twice the wattage) as the configuration with one battery."
  created_at: 2023-07-28 09:22:37+00:00
  edited: true
  hidden: false
  id: 64c396ed7918ee895ba54f53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-07-28T11:18:05.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9681248664855957
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>Please share some info about the speed of it. How many tokens per
          second can you get on your computer? </p>

          '
        raw: 'Please share some info about the speed of it. How many tokens per second
          can you get on your computer? '
        updatedAt: '2023-07-28T11:18:05.598Z'
      numEdits: 0
      reactions: []
    id: 64c3a3ede9e1506c567c57f5
    type: comment
  author: Hoioi
  content: 'Please share some info about the speed of it. How many tokens per second
    can you get on your computer? '
  created_at: 2023-07-28 10:18:05+00:00
  edited: false
  hidden: false
  id: 64c3a3ede9e1506c567c57f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-07-29T20:24:05.000Z'
    data:
      edited: false
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8974252939224243
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: '<blockquote>

          <p>Please share some info about the speed of it. How many tokens per second
          can you get on your computer?</p>

          </blockquote>

          <p>I get 0.68 tokens per second on my computer, with 4 gigabytes of RAM
          free, and a delay of "prompt ingestion".</p>

          <p>Meaning, in the implementation of Nvidia GPUs there''s an optimization
          of 400x speed ingestion compared to inference, whereas in CPU mode (llama.cpp)
          I have to wait a minimum of 10 seconds before it even starts responding.</p>

          '
        raw: '> Please share some info about the speed of it. How many tokens per
          second can you get on your computer?


          I get 0.68 tokens per second on my computer, with 4 gigabytes of RAM free,
          and a delay of "prompt ingestion".


          Meaning, in the implementation of Nvidia GPUs there''s an optimization of
          400x speed ingestion compared to inference, whereas in CPU mode (llama.cpp)
          I have to wait a minimum of 10 seconds before it even starts responding.'
        updatedAt: '2023-07-29T20:24:05.365Z'
      numEdits: 0
      reactions: []
    id: 64c57565fafa16b514570f53
    type: comment
  author: ronenzyroff
  content: '> Please share some info about the speed of it. How many tokens per second
    can you get on your computer?


    I get 0.68 tokens per second on my computer, with 4 gigabytes of RAM free, and
    a delay of "prompt ingestion".


    Meaning, in the implementation of Nvidia GPUs there''s an optimization of 400x
    speed ingestion compared to inference, whereas in CPU mode (llama.cpp) I have
    to wait a minimum of 10 seconds before it even starts responding.'
  created_at: 2023-07-29 19:24:05+00:00
  edited: false
  hidden: false
  id: 64c57565fafa16b514570f53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-08-02T03:21:34.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772281050682068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p>Hey, your setup looks like the mini pc that I was thinking of buying.
          Is AMD processor faster than Intel in inference?</p>

          '
        raw: Hey, your setup looks like the mini pc that I was thinking of buying.
          Is AMD processor faster than Intel in inference?
        updatedAt: '2023-08-02T03:21:34.384Z'
      numEdits: 0
      reactions: []
    id: 64c9cbbe1825d12864f4c5ff
    type: comment
  author: BBLL3456
  content: Hey, your setup looks like the mini pc that I was thinking of buying. Is
    AMD processor faster than Intel in inference?
  created_at: 2023-08-02 02:21:34+00:00
  edited: false
  hidden: false
  id: 64c9cbbe1825d12864f4c5ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-08-02T05:13:51.000Z'
    data:
      edited: false
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9420318603515625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: '<blockquote>

          <p>Hey, your setup looks like the mini pc that I was thinking of buying.
          Is AMD processor faster than Intel in inference?</p>

          </blockquote>

          <p>I have a laptop. The AMD laptop is no faster than an Intel equivalent.</p>

          <p>The RAM badwidth becomes the bottleneck at any more than 16 physical
          CPU cores (not 8x2 threads but actual 16 Cores). At that point it''s diminishing
          returns.</p>

          <p>I have DDR4 RAM 64GB.</p>

          <p>Once you get to any more compute speed than what my laptop has (such
          as by using a graphics card or a high-end Intel CPU), you will need faster
          RAM.</p>

          <p>Nvidia GPUs solve that RAM bandwidth issues by having dedicated GDDR6
          VRAM that has speeds of up to 800 gigabytes per second (as opposed to the
          25 gigabytes per second of DDR4).</p>

          <p>Apple M series chips also solve that problem, because the RAM is on the
          chip itself, so the badwidth is 400 gigabytes per second for M1, and 800
          gigabytes per second for M2 Ultra.</p>

          <p>In conclusion: don''t buy the CPU I have. You''re better off using DDR5
          with a CPU that has at least 16 physical CPU cores (32 threads) for optimal
          performance (no bottlenecks)</p>

          '
        raw: '> Hey, your setup looks like the mini pc that I was thinking of buying.
          Is AMD processor faster than Intel in inference?


          I have a laptop. The AMD laptop is no faster than an Intel equivalent.


          The RAM badwidth becomes the bottleneck at any more than 16 physical CPU
          cores (not 8x2 threads but actual 16 Cores). At that point it''s diminishing
          returns.


          I have DDR4 RAM 64GB.


          Once you get to any more compute speed than what my laptop has (such as
          by using a graphics card or a high-end Intel CPU), you will need faster
          RAM.


          Nvidia GPUs solve that RAM bandwidth issues by having dedicated GDDR6 VRAM
          that has speeds of up to 800 gigabytes per second (as opposed to the 25
          gigabytes per second of DDR4).


          Apple M series chips also solve that problem, because the RAM is on the
          chip itself, so the badwidth is 400 gigabytes per second for M1, and 800
          gigabytes per second for M2 Ultra.


          In conclusion: don''t buy the CPU I have. You''re better off using DDR5
          with a CPU that has at least 16 physical CPU cores (32 threads) for optimal
          performance (no bottlenecks)'
        updatedAt: '2023-08-02T05:13:51.214Z'
      numEdits: 0
      reactions: []
    id: 64c9e60fb0e612877898cd36
    type: comment
  author: ronenzyroff
  content: '> Hey, your setup looks like the mini pc that I was thinking of buying.
    Is AMD processor faster than Intel in inference?


    I have a laptop. The AMD laptop is no faster than an Intel equivalent.


    The RAM badwidth becomes the bottleneck at any more than 16 physical CPU cores
    (not 8x2 threads but actual 16 Cores). At that point it''s diminishing returns.


    I have DDR4 RAM 64GB.


    Once you get to any more compute speed than what my laptop has (such as by using
    a graphics card or a high-end Intel CPU), you will need faster RAM.


    Nvidia GPUs solve that RAM bandwidth issues by having dedicated GDDR6 VRAM that
    has speeds of up to 800 gigabytes per second (as opposed to the 25 gigabytes per
    second of DDR4).


    Apple M series chips also solve that problem, because the RAM is on the chip itself,
    so the badwidth is 400 gigabytes per second for M1, and 800 gigabytes per second
    for M2 Ultra.


    In conclusion: don''t buy the CPU I have. You''re better off using DDR5 with a
    CPU that has at least 16 physical CPU cores (32 threads) for optimal performance
    (no bottlenecks)'
  created_at: 2023-08-02 04:13:51+00:00
  edited: false
  hidden: false
  id: 64c9e60fb0e612877898cd36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-08-04T03:25:12.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9593960642814636
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<blockquote>

          <p>In conclusion: don''t buy the CPU I have. You''re better off using DDR5
          with a CPU that has at least 16 physical CPU cores (32 threads) for optimal
          performance (no bottlenecks)</p>

          </blockquote>

          <p>Thanks for your advice :)</p>

          '
        raw: "\n> \n> In conclusion: don't buy the CPU I have. You're better off using\
          \ DDR5 with a CPU that has at least 16 physical CPU cores (32 threads) for\
          \ optimal performance (no bottlenecks)\n\nThanks for your advice :)"
        updatedAt: '2023-08-04T03:25:12.090Z'
      numEdits: 0
      reactions: []
    id: 64cc6f985aa1ab065c3fb4a8
    type: comment
  author: BBLL3456
  content: "\n> \n> In conclusion: don't buy the CPU I have. You're better off using\
    \ DDR5 with a CPU that has at least 16 physical CPU cores (32 threads) for optimal\
    \ performance (no bottlenecks)\n\nThanks for your advice :)"
  created_at: 2023-08-04 02:25:12+00:00
  edited: false
  hidden: false
  id: 64cc6f985aa1ab065c3fb4a8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: jondurbin/airoboros-l2-70b-gpt4-1.4.1
repo_type: model
status: open
target_branch: null
title: Works perfectly in CPU mode with oobabooga
