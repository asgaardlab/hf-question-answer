!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AngledLuffa
conflicting_files: null
created_at: 2022-07-11 16:52:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-07-11T17:52:01.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: "<p>I ran into what I believe is a minor problem with the tokenizer\
          \ for indic-bert.  I was looking at the L3Cube NER dataset:</p>\n<p><a rel=\"\
          nofollow\" href=\"https://github.com/l3cube-pune/MarathiNLP\">https://github.com/l3cube-pune/MarathiNLP</a></p>\n\
          <p>In the train section of the NER dataset is the following sentence (the\
          \ numbers represent sentence number):</p>\n<pre><code>\u092F\u093E     \
          \ O       17197.0\n\u092E\u0902\u0924\u094D\u0930\u093E\u091A\u0940  O \
          \      17197.0\n\u0926\u0947\u0935\u0924\u093E    O       17197.0\n\u0917\
          \u0923\u092A\u0924\u0940   O       17197.0\n \u0901       O       17197.0\n\
          \u0939\u093E      O       17197.0\n\u0924\u094B      O       17197.0\n\u092E\
          \u0902\u0924\u094D\u0930     O       17197.0\n</code></pre>\n<p>The fifth\
          \ \"word\" appears to be a non-spacing Candrabindu mark by itself.    If\
          \ I feed the words to the indic-bert tokenizer word by word, I would expect\
          \ it to produce  or something similar for an untokenizable word such as\
          \ that.  Instead, it produces nothing.  Is that expected behavior I should\
          \ compensate for, or is it something that can be fixed in the tokenizer?</p>\n\
          <p>Thanks again!</p>\n"
        raw: "I ran into what I believe is a minor problem with the tokenizer for\
          \ indic-bert.  I was looking at the L3Cube NER dataset:\r\n\r\nhttps://github.com/l3cube-pune/MarathiNLP\r\
          \n\r\nIn the train section of the NER dataset is the following sentence\
          \ (the numbers represent sentence number):\r\n\r\n```\r\n\u092F\u093E  \
          \    O       17197.0\r\n\u092E\u0902\u0924\u094D\u0930\u093E\u091A\u0940\
          \  O       17197.0\r\n\u0926\u0947\u0935\u0924\u093E    O       17197.0\r\
          \n\u0917\u0923\u092A\u0924\u0940   O       17197.0\r\n \u0901       O  \
          \     17197.0\r\n\u0939\u093E      O       17197.0\r\n\u0924\u094B     \
          \ O       17197.0\r\n\u092E\u0902\u0924\u094D\u0930     O       17197.0\r\
          \n```\r\n\r\nThe fifth \"word\" appears to be a non-spacing Candrabindu\
          \ mark by itself.    If I feed the words to the indic-bert tokenizer word\
          \ by word, I would expect it to produce <UNK> or something similar for an\
          \ untokenizable word such as that.  Instead, it produces nothing.  Is that\
          \ expected behavior I should compensate for, or is it something that can\
          \ be fixed in the tokenizer?\r\n\r\nThanks again!"
        updatedAt: '2022-07-11T17:52:01.142Z'
      numEdits: 0
      reactions: []
    id: 62cc6341f753f791392946f9
    type: comment
  author: AngledLuffa
  content: "I ran into what I believe is a minor problem with the tokenizer for indic-bert.\
    \  I was looking at the L3Cube NER dataset:\r\n\r\nhttps://github.com/l3cube-pune/MarathiNLP\r\
    \n\r\nIn the train section of the NER dataset is the following sentence (the numbers\
    \ represent sentence number):\r\n\r\n```\r\n\u092F\u093E      O       17197.0\r\
    \n\u092E\u0902\u0924\u094D\u0930\u093E\u091A\u0940  O       17197.0\r\n\u0926\u0947\
    \u0935\u0924\u093E    O       17197.0\r\n\u0917\u0923\u092A\u0924\u0940   O  \
    \     17197.0\r\n \u0901       O       17197.0\r\n\u0939\u093E      O       17197.0\r\
    \n\u0924\u094B      O       17197.0\r\n\u092E\u0902\u0924\u094D\u0930     O  \
    \     17197.0\r\n```\r\n\r\nThe fifth \"word\" appears to be a non-spacing Candrabindu\
    \ mark by itself.    If I feed the words to the indic-bert tokenizer word by word,\
    \ I would expect it to produce <UNK> or something similar for an untokenizable\
    \ word such as that.  Instead, it produces nothing.  Is that expected behavior\
    \ I should compensate for, or is it something that can be fixed in the tokenizer?\r\
    \n\r\nThanks again!"
  created_at: 2022-07-11 16:52:01+00:00
  edited: false
  hidden: false
  id: 62cc6341f753f791392946f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ai4bharat/indic-bert
repo_type: model
status: open
target_branch: null
title: Nonspacing marks by themselves causing problems for the tokenizer
