!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jdjayakaran
conflicting_files: null
created_at: 2023-09-29 10:28:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
      fullname: Jennifer Deborah Jayakaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdjayakaran
      type: user
    createdAt: '2023-09-29T11:28:45.000Z'
    data:
      edited: false
      editors:
      - jdjayakaran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9079289436340332
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
          fullname: Jennifer Deborah Jayakaran
          isHf: false
          isPro: false
          name: jdjayakaran
          type: user
        html: '<p>Is there a way to run the code without nvcc, as modeling_flash_llama.py
          uses flash attention which needs nvcc. But the same is not the requirements
          on the licensed LLAMA2 model.</p>

          '
        raw: Is there a way to run the code without nvcc, as modeling_flash_llama.py
          uses flash attention which needs nvcc. But the same is not the requirements
          on the licensed LLAMA2 model.
        updatedAt: '2023-09-29T11:28:45.438Z'
      numEdits: 0
      reactions: []
    id: 6516b4edc4792fb5c8236dd1
    type: comment
  author: jdjayakaran
  content: Is there a way to run the code without nvcc, as modeling_flash_llama.py
    uses flash attention which needs nvcc. But the same is not the requirements on
    the licensed LLAMA2 model.
  created_at: 2023-09-29 10:28:45+00:00
  edited: false
  hidden: false
  id: 6516b4edc4792fb5c8236dd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
      fullname: Jennifer Deborah Jayakaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdjayakaran
      type: user
    createdAt: '2023-09-29T11:32:09.000Z'
    data:
      status: closed
    id: 6516b5b95671e6529679b191
    type: status-change
  author: jdjayakaran
  created_at: 2023-09-29 10:32:09+00:00
  id: 6516b5b95671e6529679b191
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
      fullname: Jennifer Deborah Jayakaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdjayakaran
      type: user
    createdAt: '2023-09-29T11:35:09.000Z'
    data:
      status: open
    id: 6516b66d26224cf8b109c529
    type: status-change
  author: jdjayakaran
  created_at: 2023-09-29 10:35:09+00:00
  id: 6516b66d26224cf8b109c529
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
      fullname: Jennifer Deborah Jayakaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdjayakaran
      type: user
    createdAt: '2023-09-29T11:36:07.000Z'
    data:
      status: closed
    id: 6516b6a7d467ffdc0535a1db
    type: status-change
  author: jdjayakaran
  created_at: 2023-09-29 10:36:07+00:00
  id: 6516b6a7d467ffdc0535a1db
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-09-29T13:55:02.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8788933157920837
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Seems you''ve solved this yourself? For future reference, running
          with <code>trust_remote_code=False</code> disables the use of custom flash-attention
          and should work on any hardware. Most recently, transformers has integrated
          support for flash-attention-2 with <code>use_flash_attention_2=True</code>.
          Hope this is helpful :)</p>

          '
        raw: Seems you've solved this yourself? For future reference, running with
          `trust_remote_code=False` disables the use of custom flash-attention and
          should work on any hardware. Most recently, transformers has integrated
          support for flash-attention-2 with `use_flash_attention_2=True`. Hope this
          is helpful :)
        updatedAt: '2023-09-29T13:55:02.522Z'
      numEdits: 0
      reactions: []
    id: 6516d7360696c050782a15db
    type: comment
  author: bjoernp
  content: Seems you've solved this yourself? For future reference, running with `trust_remote_code=False`
    disables the use of custom flash-attention and should work on any hardware. Most
    recently, transformers has integrated support for flash-attention-2 with `use_flash_attention_2=True`.
    Hope this is helpful :)
  created_at: 2023-09-29 12:55:02+00:00
  edited: false
  hidden: false
  id: 6516d7360696c050782a15db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
      fullname: Jennifer Deborah Jayakaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdjayakaran
      type: user
    createdAt: '2023-09-29T13:57:57.000Z'
    data:
      edited: false
      editors:
      - jdjayakaran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.977872908115387
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
          fullname: Jennifer Deborah Jayakaran
          isHf: false
          isPro: false
          name: jdjayakaran
          type: user
        html: '<p>Yes thanks seems to work. I would check flash attention for faster
          inference.</p>

          <p>Also is there a way I could check the perplexity of the model to compare
          its performance in different use cases?</p>

          '
        raw: 'Yes thanks seems to work. I would check flash attention for faster inference.


          Also is there a way I could check the perplexity of the model to compare
          its performance in different use cases?'
        updatedAt: '2023-09-29T13:57:57.503Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6516d7e5f83d2d18d4bae958
    id: 6516d7e5f83d2d18d4bae957
    type: comment
  author: jdjayakaran
  content: 'Yes thanks seems to work. I would check flash attention for faster inference.


    Also is there a way I could check the perplexity of the model to compare its performance
    in different use cases?'
  created_at: 2023-09-29 12:57:57+00:00
  edited: false
  hidden: false
  id: 6516d7e5f83d2d18d4bae957
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/dd9a8649ac09f1b183d669afe9758db5.svg
      fullname: Jennifer Deborah Jayakaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdjayakaran
      type: user
    createdAt: '2023-09-29T13:57:57.000Z'
    data:
      status: open
    id: 6516d7e5f83d2d18d4bae958
    type: status-change
  author: jdjayakaran
  created_at: 2023-09-29 12:57:57+00:00
  id: 6516d7e5f83d2d18d4bae958
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-09-29T14:36:40.000Z'
    data:
      edited: true
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6902303099632263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: "<p>There will be more detailed perplexity evaluations in our paper.\
          \ For now though, I can share some code that should help you get started\
          \ with perplexity evaluation. If you don't want to implement it yourself,\
          \ check out something like <code>text-generation-webui</code> for prebuilt\
          \ perplexity evals.</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-comment\"># Compute perplexity</span>\nnlls = []\n<span class=\"hljs-keyword\"\
          >for</span> sample <span class=\"hljs-keyword\">in</span> dataset:\n   \
          \ <span class=\"hljs-keyword\">with</span> torch.no_grad():\n        input_ids\
          \ = torch.tensor(sample [<span class=\"hljs-string\">'input_ids'</span>]).unsqueeze(<span\
          \ class=\"hljs-number\">0</span>).to(model.device)\n        attention_mask\
          \ = torch.tensor(sample [<span class=\"hljs-string\">'attention_mask'</span>]).unsqueeze(<span\
          \ class=\"hljs-number\">0</span>).to(model.device)\n        outputs = model(input_ids=input_ids,\
          \ attention_mask=attention_mask, labels=input_ids.clone())\n        loss\
          \ = outputs.loss\n        nlls.append(loss.cpu().item())\nnlls = torch.tensor(nlls)\n\
          perplexity = torch.exp(nlls.mean())\n</code></pre>\n<p><code>dataset</code>\
          \ here is a pretokenized dataset iterator. Hope this is helpful :)</p>\n"
        raw: "There will be more detailed perplexity evaluations in our paper. For\
          \ now though, I can share some code that should help you get started with\
          \ perplexity evaluation. If you don't want to implement it yourself, check\
          \ out something like `text-generation-webui` for prebuilt perplexity evals.\n\
          \n```python\n# Compute perplexity\nnlls = []\nfor sample in dataset:\n \
          \   with torch.no_grad():\n        input_ids = torch.tensor(sample ['input_ids']).unsqueeze(0).to(model.device)\n\
          \        attention_mask = torch.tensor(sample ['attention_mask']).unsqueeze(0).to(model.device)\n\
          \        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\
          \ labels=input_ids.clone())\n        loss = outputs.loss\n        nlls.append(loss.cpu().item())\n\
          nlls = torch.tensor(nlls)\nperplexity = torch.exp(nlls.mean())\n```\n\n\
          `dataset` here is a pretokenized dataset iterator. Hope this is helpful\
          \ :)"
        updatedAt: '2023-09-29T14:41:26.289Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jdjayakaran
    id: 6516e0f84fadbeb643a51b5b
    type: comment
  author: bjoernp
  content: "There will be more detailed perplexity evaluations in our paper. For now\
    \ though, I can share some code that should help you get started with perplexity\
    \ evaluation. If you don't want to implement it yourself, check out something\
    \ like `text-generation-webui` for prebuilt perplexity evals.\n\n```python\n#\
    \ Compute perplexity\nnlls = []\nfor sample in dataset:\n    with torch.no_grad():\n\
    \        input_ids = torch.tensor(sample ['input_ids']).unsqueeze(0).to(model.device)\n\
    \        attention_mask = torch.tensor(sample ['attention_mask']).unsqueeze(0).to(model.device)\n\
    \        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids.clone())\n\
    \        loss = outputs.loss\n        nlls.append(loss.cpu().item())\nnlls = torch.tensor(nlls)\n\
    perplexity = torch.exp(nlls.mean())\n```\n\n`dataset` here is a pretokenized dataset\
    \ iterator. Hope this is helpful :)"
  created_at: 2023-09-29 13:36:40+00:00
  edited: true
  hidden: false
  id: 6516e0f84fadbeb643a51b5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-09-30T16:16:32.000Z'
    data:
      status: closed
    id: 651849e0ea991a3229d5d950
    type: status-change
  author: bjoernp
  created_at: 2023-09-30 15:16:32+00:00
  id: 651849e0ea991a3229d5d950
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: LeoLM/leo-hessianai-13b
repo_type: model
status: closed
target_branch: null
title: Flash attention NVCC requirements
