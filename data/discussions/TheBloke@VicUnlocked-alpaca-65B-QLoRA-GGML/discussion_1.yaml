!!python/object:huggingface_hub.community.DiscussionWithDetails
author: giftbest71
conflicting_files: null
created_at: 2023-05-29 20:54:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7840671791763220a36070c5fbc8c6c.svg
      fullname: Victor sampson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giftbest71
      type: user
    createdAt: '2023-05-29T21:54:13.000Z'
    data:
      edited: false
      editors:
      - giftbest71
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7840671791763220a36070c5fbc8c6c.svg
          fullname: Victor sampson
          isHf: false
          isPro: false
          name: giftbest71
          type: user
        html: '<p>My deepest gratitude for your effort Sir.<br>Please is this an uncensored
          version of the -alpaca-65B model</p>

          '
        raw: "My deepest gratitude for your effort Sir.\r\nPlease is this an uncensored\
          \ version of the -alpaca-65B model"
        updatedAt: '2023-05-29T21:54:13.108Z'
      numEdits: 0
      reactions: []
    id: 64751f056d4dda6f7c6e824f
    type: comment
  author: giftbest71
  content: "My deepest gratitude for your effort Sir.\r\nPlease is this an uncensored\
    \ version of the -alpaca-65B model"
  created_at: 2023-05-29 20:54:13+00:00
  edited: false
  hidden: false
  id: 64751f056d4dda6f7c6e824f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-05-29T22:52:50.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>Grr. This model would not fit into my ram. :( Is there any workaround
          for big models such as this one to load on a pc with smaller ram? I mean
          solution other than buying more ram lol. I mean, I wouldn''t even mind if
          it''s slow as hell if the model is good. :)</p>

          '
        raw: Grr. This model would not fit into my ram. :( Is there any workaround
          for big models such as this one to load on a pc with smaller ram? I mean
          solution other than buying more ram lol. I mean, I wouldn't even mind if
          it's slow as hell if the model is good. :)
        updatedAt: '2023-05-29T22:52:50.138Z'
      numEdits: 0
      reactions: []
    id: 64752cc25ada8510bc49de83
    type: comment
  author: MrDevolver
  content: Grr. This model would not fit into my ram. :( Is there any workaround for
    big models such as this one to load on a pc with smaller ram? I mean solution
    other than buying more ram lol. I mean, I wouldn't even mind if it's slow as hell
    if the model is good. :)
  created_at: 2023-05-29 21:52:50+00:00
  edited: false
  hidden: false
  id: 64752cc25ada8510bc49de83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-29T23:14:50.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>If you have not enough RAM only solution is cache on SSD or put
          some part of model on GPU....</p>

          '
        raw: If you have not enough RAM only solution is cache on SSD or put some
          part of model on GPU....
        updatedAt: '2023-05-29T23:14:50.282Z'
      numEdits: 0
      reactions: []
    id: 647531ea5ada8510bc4a2d0d
    type: comment
  author: mirek190
  content: If you have not enough RAM only solution is cache on SSD or put some part
    of model on GPU....
  created_at: 2023-05-29 22:14:50+00:00
  edited: false
  hidden: false
  id: 647531ea5ada8510bc4a2d0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-05-29T23:30:07.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>Hmm. Would it work with AMD GPUs? I was under impression only Nvidia
          GPUs are compatible. But I guess I could try the SSD solution. Is there
          some guide on how could any of these ways be achieved?</p>

          '
        raw: Hmm. Would it work with AMD GPUs? I was under impression only Nvidia
          GPUs are compatible. But I guess I could try the SSD solution. Is there
          some guide on how could any of these ways be achieved?
        updatedAt: '2023-05-29T23:30:07.736Z'
      numEdits: 0
      reactions: []
    id: 6475357f82907acdddf72d5f
    type: comment
  author: MrDevolver
  content: Hmm. Would it work with AMD GPUs? I was under impression only Nvidia GPUs
    are compatible. But I guess I could try the SSD solution. Is there some guide
    on how could any of these ways be achieved?
  created_at: 2023-05-29 22:30:07+00:00
  edited: false
  hidden: false
  id: 6475357f82907acdddf72d5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-29T23:53:26.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>GGML models can now be accelerated with AMD GPUs, yes, using llama.cpp
          with OpenCL support.  You can load as many layers onto the GPU as you have
          VRAM for, and that boosts inference speed.</p>

          <p>However it does not help with RAM requirements. You still need just as
          much RAM as before. You just get faster inference.</p>

          <p>I''m afraid there''s no way around the RAM requirements.  If you don''t
          have enough RAM, I''d recommend trying a 33B model instead. There''s many
          great ones out there, like <a href="https://huggingface.co/TheBloke/WizardLM-30B-GGML">WizardLM-30B-Uncensored</a>
          and <a href="https://huggingface.co/TheBloke/Guanaco-33B-GGML">Guanaco-33B</a></p>

          '
        raw: 'GGML models can now be accelerated with AMD GPUs, yes, using llama.cpp
          with OpenCL support.  You can load as many layers onto the GPU as you have
          VRAM for, and that boosts inference speed.


          However it does not help with RAM requirements. You still need just as much
          RAM as before. You just get faster inference.


          I''m afraid there''s no way around the RAM requirements.  If you don''t
          have enough RAM, I''d recommend trying a 33B model instead. There''s many
          great ones out there, like [WizardLM-30B-Uncensored](https://huggingface.co/TheBloke/WizardLM-30B-GGML)
          and [Guanaco-33B](https://huggingface.co/TheBloke/Guanaco-33B-GGML)'
        updatedAt: '2023-05-29T23:54:03.869Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 64753af6f9e3e0b312f6b601
    type: comment
  author: TheBloke
  content: 'GGML models can now be accelerated with AMD GPUs, yes, using llama.cpp
    with OpenCL support.  You can load as many layers onto the GPU as you have VRAM
    for, and that boosts inference speed.


    However it does not help with RAM requirements. You still need just as much RAM
    as before. You just get faster inference.


    I''m afraid there''s no way around the RAM requirements.  If you don''t have enough
    RAM, I''d recommend trying a 33B model instead. There''s many great ones out there,
    like [WizardLM-30B-Uncensored](https://huggingface.co/TheBloke/WizardLM-30B-GGML)
    and [Guanaco-33B](https://huggingface.co/TheBloke/Guanaco-33B-GGML)'
  created_at: 2023-05-29 22:53:26+00:00
  edited: true
  hidden: false
  id: 64753af6f9e3e0b312f6b601
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-30T00:03:58.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I got RAM for it, but what kind of improvement can we expect from
          a 65B over 33B though, before I start downloading it (and considering that
          QLoRA would be applied to both)?</p>

          '
        raw: I got RAM for it, but what kind of improvement can we expect from a 65B
          over 33B though, before I start downloading it (and considering that QLoRA
          would be applied to both)?
        updatedAt: '2023-05-30T00:03:58.676Z'
      numEdits: 0
      reactions: []
    id: 64753d6ef9e3e0b312f6d863
    type: comment
  author: mancub
  content: I got RAM for it, but what kind of improvement can we expect from a 65B
    over 33B though, before I start downloading it (and considering that QLoRA would
    be applied to both)?
  created_at: 2023-05-29 23:03:58+00:00
  edited: false
  hidden: false
  id: 64753d6ef9e3e0b312f6d863
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-05-30T00:08:58.000Z'
    data:
      edited: true
      editors:
      - MrDevolver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>I''m more in the 16 GB RAM room. :( Okay, I said it. I guess 16
          GB isn''t as much as it used to be, huh? lol</p>

          '
        raw: I'm more in the 16 GB RAM room. :( Okay, I said it. I guess 16 GB isn't
          as much as it used to be, huh? lol
        updatedAt: '2023-05-30T00:09:20.167Z'
      numEdits: 1
      reactions: []
    id: 64753e9ad56974d0c066f665
    type: comment
  author: MrDevolver
  content: I'm more in the 16 GB RAM room. :( Okay, I said it. I guess 16 GB isn't
    as much as it used to be, huh? lol
  created_at: 2023-05-29 23:08:58+00:00
  edited: true
  hidden: false
  id: 64753e9ad56974d0c066f665
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b7840671791763220a36070c5fbc8c6c.svg
      fullname: Victor sampson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giftbest71
      type: user
    createdAt: '2023-05-30T00:16:36.000Z'
    data:
      status: closed
    id: 64754064f9e3e0b312f70313
    type: status-change
  author: giftbest71
  created_at: 2023-05-29 23:16:36+00:00
  id: 64754064f9e3e0b312f70313
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b7840671791763220a36070c5fbc8c6c.svg
      fullname: Victor sampson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giftbest71
      type: user
    createdAt: '2023-05-30T00:19:47.000Z'
    data:
      status: open
    id: 64754123d56974d0c0671b45
    type: status-change
  author: giftbest71
  created_at: 2023-05-29 23:19:47+00:00
  id: 64754123d56974d0c0671b45
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T00:24:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>My deepest gratitude for your effort Sir.<br>Please is this an uncensored
          version of the -alpaca-65B model</p>

          </blockquote>

          <p>Yeah seems like it - "trained on a cleaned ShareGPT dataset"</p>

          '
        raw: '> My deepest gratitude for your effort Sir.

          > Please is this an uncensored version of the -alpaca-65B model


          Yeah seems like it - "trained on a cleaned ShareGPT dataset"'
        updatedAt: '2023-05-30T00:24:46.563Z'
      numEdits: 0
      reactions: []
    id: 6475424e82907acdddf7e236
    type: comment
  author: TheBloke
  content: '> My deepest gratitude for your effort Sir.

    > Please is this an uncensored version of the -alpaca-65B model


    Yeah seems like it - "trained on a cleaned ShareGPT dataset"'
  created_at: 2023-05-29 23:24:46+00:00
  edited: false
  hidden: false
  id: 6475424e82907acdddf7e236
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-30T07:49:22.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Hey bloke - i tested with newest llama.cpp with cublas if you are
          putting layers on GPU then is used less RAM .<br>For instance if I give
          half layers on GPU and half on CPU then model takes only half RAM than usual
          the rest in in VRAM.<br>Llama.cpp not keeping copy in RAM form some time.</p>

          '
        raw: 'Hey bloke - i tested with newest llama.cpp with cublas if you are putting
          layers on GPU then is used less RAM .

          For instance if I give half layers on GPU and half on CPU then model takes
          only half RAM than usual the rest in in VRAM.

          Llama.cpp not keeping copy in RAM form some time.'
        updatedAt: '2023-05-30T07:56:44.147Z'
      numEdits: 1
      reactions: []
    id: 6475aa8209e773226333a7c2
    type: comment
  author: mirek190
  content: 'Hey bloke - i tested with newest llama.cpp with cublas if you are putting
    layers on GPU then is used less RAM .

    For instance if I give half layers on GPU and half on CPU then model takes only
    half RAM than usual the rest in in VRAM.

    Llama.cpp not keeping copy in RAM form some time.'
  created_at: 2023-05-30 06:49:22+00:00
  edited: true
  hidden: false
  id: 6475aa8209e773226333a7c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T10:16:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mirek190\">@<span class=\"\
          underline\">mirek190</span></a></span>\n\n\t</span></span> Oh, OK. So with\
          \ eg <code>-ngl 32</code>, the RAM usage of those 32 layers doesn't also\
          \ need to be stored in RAM?</p>\n<p>In which case yeah I need to mention\
          \ that RAM requirements can be lower with GPU offload.  It will be hard\
          \ to give an exact RAM usage figure then, given different GPUs will be able\
          \ to offload different numbers of layers. So I'll probably still list the\
          \ maximum that could be required, maybe with some examples of usage with\
          \ varying <code>-ngl</code> amounts</p>\n"
        raw: '@mirek190 Oh, OK. So with eg `-ngl 32`, the RAM usage of those 32 layers
          doesn''t also need to be stored in RAM?


          In which case yeah I need to mention that RAM requirements can be lower
          with GPU offload.  It will be hard to give an exact RAM usage figure then,
          given different GPUs will be able to offload different numbers of layers.
          So I''ll probably still list the maximum that could be required, maybe with
          some examples of usage with varying `-ngl` amounts'
        updatedAt: '2023-05-30T10:16:24.083Z'
      numEdits: 0
      reactions: []
    id: 6475ccf8ab17a37d0b1a9bfd
    type: comment
  author: TheBloke
  content: '@mirek190 Oh, OK. So with eg `-ngl 32`, the RAM usage of those 32 layers
    doesn''t also need to be stored in RAM?


    In which case yeah I need to mention that RAM requirements can be lower with GPU
    offload.  It will be hard to give an exact RAM usage figure then, given different
    GPUs will be able to offload different numbers of layers. So I''ll probably still
    list the maximum that could be required, maybe with some examples of usage with
    varying `-ngl` amounts'
  created_at: 2023-05-30 09:16:24+00:00
  edited: false
  hidden: false
  id: 6475ccf8ab17a37d0b1a9bfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-30T13:06:01.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: "<p>Later will test and let you know as today I got my rtx3090 finally\
          \ \U0001F601</p>\n"
        raw: "Later will test and let you know as today I got my rtx3090 finally \U0001F601"
        updatedAt: '2023-05-30T13:06:01.402Z'
      numEdits: 0
      reactions: []
    id: 6475f4b9ab17a37d0b1db042
    type: comment
  author: mirek190
  content: "Later will test and let you know as today I got my rtx3090 finally \U0001F601"
  created_at: 2023-05-30 12:06:01+00:00
  edited: false
  hidden: false
  id: 6475f4b9ab17a37d0b1db042
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T13:07:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thank you!</p>

          '
        raw: Thank you!
        updatedAt: '2023-05-30T13:07:17.550Z'
      numEdits: 0
      reactions: []
    id: 6475f50509e77322633a486c
    type: comment
  author: TheBloke
  content: Thank you!
  created_at: 2023-05-30 12:07:17+00:00
  edited: false
  hidden: false
  id: 6475f50509e77322633a486c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-05-30T21:14:12.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>mirek190, that sounds amazing, really promising, but let''s say
          I would like to offload some of it to VRAM. Not that I''d have too much,
          but I think for slightly bigger models than the ones I usually use, my Radeon
          RX Vega 56 with 8 GB VRAM should be fairly enough to give some boost to
          my 16 GB of RAM, right? How would I go about doing this? I''ve been using
          some softwares such as Faraday.dev and Koboldcpp, but I''m not sure how
          to fully utilize the hardware using these methods and programs. Or is there
          something completely different that I should be using instead?</p>

          '
        raw: mirek190, that sounds amazing, really promising, but let's say I would
          like to offload some of it to VRAM. Not that I'd have too much, but I think
          for slightly bigger models than the ones I usually use, my Radeon RX Vega
          56 with 8 GB VRAM should be fairly enough to give some boost to my 16 GB
          of RAM, right? How would I go about doing this? I've been using some softwares
          such as Faraday.dev and Koboldcpp, but I'm not sure how to fully utilize
          the hardware using these methods and programs. Or is there something completely
          different that I should be using instead?
        updatedAt: '2023-05-30T21:14:12.206Z'
      numEdits: 0
      reactions: []
    id: 64766724de6be93fe7b0a9ba
    type: comment
  author: MrDevolver
  content: mirek190, that sounds amazing, really promising, but let's say I would
    like to offload some of it to VRAM. Not that I'd have too much, but I think for
    slightly bigger models than the ones I usually use, my Radeon RX Vega 56 with
    8 GB VRAM should be fairly enough to give some boost to my 16 GB of RAM, right?
    How would I go about doing this? I've been using some softwares such as Faraday.dev
    and Koboldcpp, but I'm not sure how to fully utilize the hardware using these
    methods and programs. Or is there something completely different that I should
    be using instead?
  created_at: 2023-05-30 20:14:12+00:00
  edited: false
  hidden: false
  id: 64766724de6be93fe7b0a9ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-30T22:08:41.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I use llama.cpp by adding "-ngl [number_of _layers]"-  as parameter.
          </p>

          <p>Ok I made tests .. actually during loading the model is described How
          much RAM and VRAM will be used.<br>For NVIDIA cards ... as others are suck
          for AI environment  unfortunately ... buuut you can try  :)</p>

          <p>For full offload on GPU  and cublas - models q5_1:</p>

          <p>7B   -  32 layers -  VRAM used: 4633 MB | RAM required  = 1979.59 MB
          (+ 1026.00 MB per state)         ~ 4.5 GB VRAM /1.9 GB RAM<br>13B -  40
          layers - VRAM used: 9076 MB |  RAM required  = 2282.48 MB (+ 1608.00 MB
          per state)        ~ 8.9 GB VRAM /2.1 GB RAM<br>30B - 60 layers -  VRAM used:
          22964 MB | RAM  required  = 2608.85 MB (+ 3124.00 MB per state)      ~ 22.9
          GB VRAM /2.5 GB RAM<br>65B - 80 layers -  VRAM used: 46325 MB | RAM required  =
          3959.21 MB (+ 5120.00 MB per state)       ~ 46.2 GB VRAM /3.9 GB RAM</p>

          <p>For CPU only - cublas built that uses a bit GPU VRAM for increase speed
          to prompts few times - models q5_1 .</p>

          <p>7B   -  32 layers -  VRAM used: 0MB | RAM required  = 6612.59 MB (+ 1026.00
          MB per state)          ~ 6.6 GB RAM<br>13B -  40 layers - VRAM used: 0 MB
          |  RAM required  =  11359.05 MB (+ 1608.00 MB per state)     ~ 11.3 GB RAM<br>30B
          - 60 layers -  VRAM used: 0 MB | RAM  required  = 25573.14 MB (+ 3124.00
          MB per state)      ~ 25.5 GB RAM<br>65B - 80 layers -  VRAM used: 0 MB |
          RAM required  = 50284.21 MB (+ 5120.00 MB per state)        ~ 50.1 GB RAM</p>

          '
        raw: "I use llama.cpp by adding \"-ngl [number_of _layers]\"-  as parameter.\
          \ \n\nOk I made tests .. actually during loading the model is described\
          \ How much RAM and VRAM will be used.\nFor NVIDIA cards ... as others are\
          \ suck for AI environment  unfortunately ... buuut you can try  :)\n\nFor\
          \ full offload on GPU  and cublas - models q5_1:\n\n7B   -  32 layers -\
          \  VRAM used: 4633 MB | RAM required  = 1979.59 MB (+ 1026.00 MB per state)\
          \         ~ 4.5 GB VRAM /1.9 GB RAM\n13B -  40 layers - VRAM used: 9076\
          \ MB |  RAM required  = 2282.48 MB (+ 1608.00 MB per state)        ~ 8.9\
          \ GB VRAM /2.1 GB RAM\n30B - 60 layers -  VRAM used: 22964 MB | RAM  required\
          \  = 2608.85 MB (+ 3124.00 MB per state)      ~ 22.9 GB VRAM /2.5 GB RAM\n\
          65B - 80 layers -  VRAM used: 46325 MB | RAM required  = 3959.21 MB (+ 5120.00\
          \ MB per state)       ~ 46.2 GB VRAM /3.9 GB RAM\n\nFor CPU only - cublas\
          \ built that uses a bit GPU VRAM for increase speed to prompts few times\
          \ - models q5_1 .\n\n7B   -  32 layers -  VRAM used: 0MB | RAM required\
          \  = 6612.59 MB (+ 1026.00 MB per state)          ~ 6.6 GB RAM\n13B -  40\
          \ layers - VRAM used: 0 MB |  RAM required  =  11359.05 MB (+ 1608.00 MB\
          \ per state)     ~ 11.3 GB RAM\n30B - 60 layers -  VRAM used: 0 MB | RAM\
          \  required  = 25573.14 MB (+ 3124.00 MB per state)      ~ 25.5 GB RAM\n\
          65B - 80 layers -  VRAM used: 0 MB | RAM required  = 50284.21 MB (+ 5120.00\
          \ MB per state)        ~ 50.1 GB RAM"
        updatedAt: '2023-05-30T22:47:07.478Z'
      numEdits: 4
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - PrimeD
        - phi0112358
    id: 647673e9cfe9d995bf40c275
    type: comment
  author: mirek190
  content: "I use llama.cpp by adding \"-ngl [number_of _layers]\"-  as parameter.\
    \ \n\nOk I made tests .. actually during loading the model is described How much\
    \ RAM and VRAM will be used.\nFor NVIDIA cards ... as others are suck for AI environment\
    \  unfortunately ... buuut you can try  :)\n\nFor full offload on GPU  and cublas\
    \ - models q5_1:\n\n7B   -  32 layers -  VRAM used: 4633 MB | RAM required  =\
    \ 1979.59 MB (+ 1026.00 MB per state)         ~ 4.5 GB VRAM /1.9 GB RAM\n13B -\
    \  40 layers - VRAM used: 9076 MB |  RAM required  = 2282.48 MB (+ 1608.00 MB\
    \ per state)        ~ 8.9 GB VRAM /2.1 GB RAM\n30B - 60 layers -  VRAM used: 22964\
    \ MB | RAM  required  = 2608.85 MB (+ 3124.00 MB per state)      ~ 22.9 GB VRAM\
    \ /2.5 GB RAM\n65B - 80 layers -  VRAM used: 46325 MB | RAM required  = 3959.21\
    \ MB (+ 5120.00 MB per state)       ~ 46.2 GB VRAM /3.9 GB RAM\n\nFor CPU only\
    \ - cublas built that uses a bit GPU VRAM for increase speed to prompts few times\
    \ - models q5_1 .\n\n7B   -  32 layers -  VRAM used: 0MB | RAM required  = 6612.59\
    \ MB (+ 1026.00 MB per state)          ~ 6.6 GB RAM\n13B -  40 layers - VRAM used:\
    \ 0 MB |  RAM required  =  11359.05 MB (+ 1608.00 MB per state)     ~ 11.3 GB\
    \ RAM\n30B - 60 layers -  VRAM used: 0 MB | RAM  required  = 25573.14 MB (+ 3124.00\
    \ MB per state)      ~ 25.5 GB RAM\n65B - 80 layers -  VRAM used: 0 MB | RAM required\
    \  = 50284.21 MB (+ 5120.00 MB per state)        ~ 50.1 GB RAM"
  created_at: 2023-05-30 21:08:41+00:00
  edited: true
  hidden: false
  id: 647673e9cfe9d995bf40c275
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T22:25:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thank you! That is great data to have.  </p>

          <p>I will have a think about how to update the README to explain this.</p>

          '
        raw: "Thank you! That is great data to have.  \n\nI will have a think about\
          \ how to update the README to explain this."
        updatedAt: '2023-05-30T22:25:14.878Z'
      numEdits: 0
      reactions: []
    id: 647677ca57108da1760010d7
    type: comment
  author: TheBloke
  content: "Thank you! That is great data to have.  \n\nI will have a think about\
    \ how to update the README to explain this."
  created_at: 2023-05-30 21:25:14+00:00
  edited: false
  hidden: false
  id: 647677ca57108da1760010d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-05-30T22:27:37.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>Since I have AMD GPU, I guess Cublas being tied to Cuda, I can''t
          really use that option and I see the option below does not seem to utilize
          GPU VRAM at all? Hmm. It''s like I have all that extra VRAM and I can''t
          even use it to speed things up. :(</p>

          '
        raw: Since I have AMD GPU, I guess Cublas being tied to Cuda, I can't really
          use that option and I see the option below does not seem to utilize GPU
          VRAM at all? Hmm. It's like I have all that extra VRAM and I can't even
          use it to speed things up. :(
        updatedAt: '2023-05-30T22:27:37.603Z'
      numEdits: 0
      reactions: []
    id: 6476785957108da176001c86
    type: comment
  author: MrDevolver
  content: Since I have AMD GPU, I guess Cublas being tied to Cuda, I can't really
    use that option and I see the option below does not seem to utilize GPU VRAM at
    all? Hmm. It's like I have all that extra VRAM and I can't even use it to speed
    things up. :(
  created_at: 2023-05-30 21:27:37+00:00
  edited: false
  hidden: false
  id: 6476785957108da176001c86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T22:37:20.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Since I have AMD GPU, I guess Cublas being tied to Cuda, I can''t really
          use that option and I see the option below does not seem to utilize GPU
          VRAM at all? Hmm. It''s like I have all that extra VRAM and I can''t even
          use it to speed things up. :(</p>

          </blockquote>

          <p>You can! Fairly recently they added CLBLAST support, which uses OpenCL
          to provide GPU offloading similar to the CUDA support. It may not perform
          as well as CUDA, but it''s still going to provide a benefit. </p>

          <p>I use an AMD GPU at home on macOS and it works OK with GPU offloading
          with llama.cpp.  Well, fairly well - there is some bug that causes it to
          crash, but that could be specific to macOS. I''ve not tried to investigate
          it yet. </p>

          <p>First you need to install CLBLAST, then you compile like so:</p>

          <pre><code> make clean &amp;&amp; LLAMA_CLBLAST=1 make

          </code></pre>

          <p>On macOS I installed CLBLAST with: <code>brew install clblast</code>.
          On other platforms you''ll need to look up how to do that. Like in Ubuntu
          2204 I think it''s <code>apt install libclblast1 libclblast-dev</code></p>

          '
        raw: "> Since I have AMD GPU, I guess Cublas being tied to Cuda, I can't really\
          \ use that option and I see the option below does not seem to utilize GPU\
          \ VRAM at all? Hmm. It's like I have all that extra VRAM and I can't even\
          \ use it to speed things up. :(\n\nYou can! Fairly recently they added CLBLAST\
          \ support, which uses OpenCL to provide GPU offloading similar to the CUDA\
          \ support. It may not perform as well as CUDA, but it's still going to provide\
          \ a benefit. \n\nI use an AMD GPU at home on macOS and it works OK with\
          \ GPU offloading with llama.cpp.  Well, fairly well - there is some bug\
          \ that causes it to crash, but that could be specific to macOS. I've not\
          \ tried to investigate it yet. \n\nFirst you need to install CLBLAST, then\
          \ you compile like so:\n```\n make clean && LLAMA_CLBLAST=1 make\n```\n\n\
          On macOS I installed CLBLAST with: `brew install clblast`. On other platforms\
          \ you'll need to look up how to do that. Like in Ubuntu 2204 I think it's\
          \ `apt install libclblast1 libclblast-dev`"
        updatedAt: '2023-05-30T22:38:02.387Z'
      numEdits: 1
      reactions: []
    id: 64767aa0cfe9d995bf413a11
    type: comment
  author: TheBloke
  content: "> Since I have AMD GPU, I guess Cublas being tied to Cuda, I can't really\
    \ use that option and I see the option below does not seem to utilize GPU VRAM\
    \ at all? Hmm. It's like I have all that extra VRAM and I can't even use it to\
    \ speed things up. :(\n\nYou can! Fairly recently they added CLBLAST support,\
    \ which uses OpenCL to provide GPU offloading similar to the CUDA support. It\
    \ may not perform as well as CUDA, but it's still going to provide a benefit.\
    \ \n\nI use an AMD GPU at home on macOS and it works OK with GPU offloading with\
    \ llama.cpp.  Well, fairly well - there is some bug that causes it to crash, but\
    \ that could be specific to macOS. I've not tried to investigate it yet. \n\n\
    First you need to install CLBLAST, then you compile like so:\n```\n make clean\
    \ && LLAMA_CLBLAST=1 make\n```\n\nOn macOS I installed CLBLAST with: `brew install\
    \ clblast`. On other platforms you'll need to look up how to do that. Like in\
    \ Ubuntu 2204 I think it's `apt install libclblast1 libclblast-dev`"
  created_at: 2023-05-30 21:37:20+00:00
  edited: true
  hidden: false
  id: 64767aa0cfe9d995bf413a11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-30T23:04:39.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Also made tests speed ( ms/t) with models q5_1 :<br>nvidia rtx3090
          - 24GB VRAM</p>

          <p>7B - 32 layers - GPU offload 32 layers -   48.76 ms per token<br>13B
          - 40 layers - GPU offload 40 layers -  70.48 ms per token<br>30B - 60 layers
          - GPU offload 57 layers -  178.58 ms per token<br>65B - 80 layers - GPU
          offload 37 layers -  979.77 ms per token</p>

          <p>In theory IF I could place all layers from 65B mode in VRAM I could achieve
          something around 320-370ms/token :P</p>

          '
        raw: 'Also made tests speed ( ms/t) with models q5_1 :

          nvidia rtx3090 - 24GB VRAM


          7B - 32 layers - GPU offload 32 layers -   48.76 ms per token

          13B - 40 layers - GPU offload 40 layers -  70.48 ms per token

          30B - 60 layers - GPU offload 57 layers -  178.58 ms per token

          65B - 80 layers - GPU offload 37 layers -  979.77 ms per token


          In theory IF I could place all layers from 65B mode in VRAM I could achieve
          something around 320-370ms/token :P'
        updatedAt: '2023-05-31T00:27:05.325Z'
      numEdits: 1
      reactions: []
    id: 6476810757108da17600addb
    type: comment
  author: mirek190
  content: 'Also made tests speed ( ms/t) with models q5_1 :

    nvidia rtx3090 - 24GB VRAM


    7B - 32 layers - GPU offload 32 layers -   48.76 ms per token

    13B - 40 layers - GPU offload 40 layers -  70.48 ms per token

    30B - 60 layers - GPU offload 57 layers -  178.58 ms per token

    65B - 80 layers - GPU offload 37 layers -  979.77 ms per token


    In theory IF I could place all layers from 65B mode in VRAM I could achieve something
    around 320-370ms/token :P'
  created_at: 2023-05-30 22:04:39+00:00
  edited: true
  hidden: false
  id: 6476810757108da17600addb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-05-30T23:56:23.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Since I have AMD GPU, I guess Cublas being tied to Cuda, I can''t really
          use that option and I see the option below does not seem to utilize GPU
          VRAM at all? Hmm. It''s like I have all that extra VRAM and I can''t even
          use it to speed things up. :(</p>

          </blockquote>

          <p>You can! Fairly recently they added CLBLAST support, which uses OpenCL
          to provide GPU offloading similar to the CUDA support. It may not perform
          as well as CUDA, but it''s still going to provide a benefit. </p>

          <p>I use an AMD GPU at home on macOS and it works OK with GPU offloading
          with llama.cpp.  Well, fairly well - there is some bug that causes it to
          crash, but that could be specific to macOS. I''ve not tried to investigate
          it yet. </p>

          <p>First you need to install CLBLAST, then you compile like so:</p>

          <pre><code> make clean &amp;&amp; LLAMA_CLBLAST=1 make

          </code></pre>

          <p>On macOS I installed CLBLAST with: <code>brew install clblast</code>.
          On other platforms you''ll need to look up how to do that. Like in Ubuntu
          2204 I think it''s <code>apt install libclblast1 libclblast-dev</code></p>

          </blockquote>

          <p>Faraday.dev is closed source tool, but it''s all in one tool which makes
          it easy for the end-user to set up, there is a Clblast feature, but it gives
          me errors when I use it. It has Openblas which I believe should do the same
          trick, but while choosing that one doesn''t give errors, it doesn''t seem
          to do anything at all in terms of filling my VRAM with data and the speed
          of generation feels pretty much the same. Koboldcpp can use both Openblas
          and Clblast (which is for some reason called "Legacy" way) and when I use
          that, I can offload like 32 layers to VRAM and it even does seem to use
          VRAM unlike Faraday.dev, but unfortunately the speed of text generation
          itself seems actually inferior to both Faraday.dev and Koboldcpp without
          Clblast and I''m not sure if filling some of the data in VRAM actually saves
          space in RAM itself to allow slightly more demanding models, because RAM
          usage seemed the same regardless of the Clblast option being enabled. I
          started to ask myself if I''m doing something wrong at this point, or if
          it the results are really highly hardware dependant, because I''m confused
          after seeing the results so far.</p>

          '
        raw: "> > Since I have AMD GPU, I guess Cublas being tied to Cuda, I can't\
          \ really use that option and I see the option below does not seem to utilize\
          \ GPU VRAM at all? Hmm. It's like I have all that extra VRAM and I can't\
          \ even use it to speed things up. :(\n> \n> You can! Fairly recently they\
          \ added CLBLAST support, which uses OpenCL to provide GPU offloading similar\
          \ to the CUDA support. It may not perform as well as CUDA, but it's still\
          \ going to provide a benefit. \n> \n> I use an AMD GPU at home on macOS\
          \ and it works OK with GPU offloading with llama.cpp.  Well, fairly well\
          \ - there is some bug that causes it to crash, but that could be specific\
          \ to macOS. I've not tried to investigate it yet. \n> \n> First you need\
          \ to install CLBLAST, then you compile like so:\n> ```\n>  make clean &&\
          \ LLAMA_CLBLAST=1 make\n> ```\n> \n> On macOS I installed CLBLAST with:\
          \ `brew install clblast`. On other platforms you'll need to look up how\
          \ to do that. Like in Ubuntu 2204 I think it's `apt install libclblast1\
          \ libclblast-dev`\n\nFaraday.dev is closed source tool, but it's all in\
          \ one tool which makes it easy for the end-user to set up, there is a Clblast\
          \ feature, but it gives me errors when I use it. It has Openblas which I\
          \ believe should do the same trick, but while choosing that one doesn't\
          \ give errors, it doesn't seem to do anything at all in terms of filling\
          \ my VRAM with data and the speed of generation feels pretty much the same.\
          \ Koboldcpp can use both Openblas and Clblast (which is for some reason\
          \ called \"Legacy\" way) and when I use that, I can offload like 32 layers\
          \ to VRAM and it even does seem to use VRAM unlike Faraday.dev, but unfortunately\
          \ the speed of text generation itself seems actually inferior to both Faraday.dev\
          \ and Koboldcpp without Clblast and I'm not sure if filling some of the\
          \ data in VRAM actually saves space in RAM itself to allow slightly more\
          \ demanding models, because RAM usage seemed the same regardless of the\
          \ Clblast option being enabled. I started to ask myself if I'm doing something\
          \ wrong at this point, or if it the results are really highly hardware dependant,\
          \ because I'm confused after seeing the results so far."
        updatedAt: '2023-05-30T23:56:23.817Z'
      numEdits: 0
      reactions: []
    id: 64768d276f0553f1b4fa53bc
    type: comment
  author: MrDevolver
  content: "> > Since I have AMD GPU, I guess Cublas being tied to Cuda, I can't really\
    \ use that option and I see the option below does not seem to utilize GPU VRAM\
    \ at all? Hmm. It's like I have all that extra VRAM and I can't even use it to\
    \ speed things up. :(\n> \n> You can! Fairly recently they added CLBLAST support,\
    \ which uses OpenCL to provide GPU offloading similar to the CUDA support. It\
    \ may not perform as well as CUDA, but it's still going to provide a benefit.\
    \ \n> \n> I use an AMD GPU at home on macOS and it works OK with GPU offloading\
    \ with llama.cpp.  Well, fairly well - there is some bug that causes it to crash,\
    \ but that could be specific to macOS. I've not tried to investigate it yet. \n\
    > \n> First you need to install CLBLAST, then you compile like so:\n> ```\n> \
    \ make clean && LLAMA_CLBLAST=1 make\n> ```\n> \n> On macOS I installed CLBLAST\
    \ with: `brew install clblast`. On other platforms you'll need to look up how\
    \ to do that. Like in Ubuntu 2204 I think it's `apt install libclblast1 libclblast-dev`\n\
    \nFaraday.dev is closed source tool, but it's all in one tool which makes it easy\
    \ for the end-user to set up, there is a Clblast feature, but it gives me errors\
    \ when I use it. It has Openblas which I believe should do the same trick, but\
    \ while choosing that one doesn't give errors, it doesn't seem to do anything\
    \ at all in terms of filling my VRAM with data and the speed of generation feels\
    \ pretty much the same. Koboldcpp can use both Openblas and Clblast (which is\
    \ for some reason called \"Legacy\" way) and when I use that, I can offload like\
    \ 32 layers to VRAM and it even does seem to use VRAM unlike Faraday.dev, but\
    \ unfortunately the speed of text generation itself seems actually inferior to\
    \ both Faraday.dev and Koboldcpp without Clblast and I'm not sure if filling some\
    \ of the data in VRAM actually saves space in RAM itself to allow slightly more\
    \ demanding models, because RAM usage seemed the same regardless of the Clblast\
    \ option being enabled. I started to ask myself if I'm doing something wrong at\
    \ this point, or if it the results are really highly hardware dependant, because\
    \ I'm confused after seeing the results so far."
  created_at: 2023-05-30 22:56:23+00:00
  edited: false
  hidden: false
  id: 64768d276f0553f1b4fa53bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T00:07:58.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OpenBLAS does not help with GPU accel.  llama.cpp can use OpenBLAS
          (<code>LLAMA_OPENBLAS=1 make</code>) and it''s faster than not using it,
          but it''s still CPU only.</p>

          <p>For GPU accel the options currently are CUBLAS and CLBLAST.</p>

          <p>I don''t know anything about Faraday.dev, and I''ve never used KoboldCpp
          so can''t speak to those</p>

          '
        raw: 'OpenBLAS does not help with GPU accel.  llama.cpp can use OpenBLAS (`LLAMA_OPENBLAS=1
          make`) and it''s faster than not using it, but it''s still CPU only.


          For GPU accel the options currently are CUBLAS and CLBLAST.


          I don''t know anything about Faraday.dev, and I''ve never used KoboldCpp
          so can''t speak to those'
        updatedAt: '2023-05-31T00:08:14.505Z'
      numEdits: 1
      reactions: []
    id: 64768fdecfe9d995bf429277
    type: comment
  author: TheBloke
  content: 'OpenBLAS does not help with GPU accel.  llama.cpp can use OpenBLAS (`LLAMA_OPENBLAS=1
    make`) and it''s faster than not using it, but it''s still CPU only.


    For GPU accel the options currently are CUBLAS and CLBLAST.


    I don''t know anything about Faraday.dev, and I''ve never used KoboldCpp so can''t
    speak to those'
  created_at: 2023-05-30 23:07:58+00:00
  edited: true
  hidden: false
  id: 64768fdecfe9d995bf429277
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T00:08:30.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>But yeah it wouldn''t surprise me to learn that CLBLAST isn''t helping
          nearly as much as CUBLAS does</p>

          '
        raw: But yeah it wouldn't surprise me to learn that CLBLAST isn't helping
          nearly as much as CUBLAS does
        updatedAt: '2023-05-31T00:08:30.549Z'
      numEdits: 0
      reactions: []
    id: 64768ffe57108da176019ec9
    type: comment
  author: TheBloke
  content: But yeah it wouldn't surprise me to learn that CLBLAST isn't helping nearly
    as much as CUBLAS does
  created_at: 2023-05-30 23:08:30+00:00
  edited: false
  hidden: false
  id: 64768ffe57108da176019ec9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-31T00:23:39.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>..yeah is sad AMD not developing others tools like Nvidia does ...</p>

          '
        raw: ..yeah is sad AMD not developing others tools like Nvidia does ...
        updatedAt: '2023-05-31T00:23:39.251Z'
      numEdits: 0
      reactions: []
    id: 6476938bcfe9d995bf42d077
    type: comment
  author: mirek190
  content: ..yeah is sad AMD not developing others tools like Nvidia does ...
  created_at: 2023-05-30 23:23:39+00:00
  edited: false
  hidden: false
  id: 6476938bcfe9d995bf42d077
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-31T00:23:50.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>From my tests, offloading llama.cpp to GPU is definitely a must
          with -ngl, but I can''t say I see any serious improvements unless I offload
          50%+, if not 75%+, to VRAM. The more the better and in this case a headless
          configuration, or 2nd graphics card dedicated to display, would help squeeze
          every bit out of the main GPU used for compute.</p>

          '
        raw: From my tests, offloading llama.cpp to GPU is definitely a must with
          -ngl, but I can't say I see any serious improvements unless I offload 50%+,
          if not 75%+, to VRAM. The more the better and in this case a headless configuration,
          or 2nd graphics card dedicated to display, would help squeeze every bit
          out of the main GPU used for compute.
        updatedAt: '2023-05-31T00:23:50.829Z'
      numEdits: 0
      reactions: []
    id: 64769396cfe9d995bf42d156
    type: comment
  author: mancub
  content: From my tests, offloading llama.cpp to GPU is definitely a must with -ngl,
    but I can't say I see any serious improvements unless I offload 50%+, if not 75%+,
    to VRAM. The more the better and in this case a headless configuration, or 2nd
    graphics card dedicated to display, would help squeeze every bit out of the main
    GPU used for compute.
  created_at: 2023-05-30 23:23:50+00:00
  edited: false
  hidden: false
  id: 64769396cfe9d995bf42d156
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-31T00:26:43.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>NVIDIA is just reaming us all up where the sun does not shine, because
          they are raking astronomic profits on the same chips over and over again.
          All they do is make a single chip (despite calling it a "family"), then
          bin it and burn hardware fuses in it as desired to create gaming, compute,
          or other platform "solutions".</p>

          '
        raw: NVIDIA is just reaming us all up where the sun does not shine, because
          they are raking astronomic profits on the same chips over and over again.
          All they do is make a single chip (despite calling it a "family"), then
          bin it and burn hardware fuses in it as desired to create gaming, compute,
          or other platform "solutions".
        updatedAt: '2023-05-31T00:26:43.908Z'
      numEdits: 0
      reactions: []
    id: 64769443de6be93fe7b39ca9
    type: comment
  author: mancub
  content: NVIDIA is just reaming us all up where the sun does not shine, because
    they are raking astronomic profits on the same chips over and over again. All
    they do is make a single chip (despite calling it a "family"), then bin it and
    burn hardware fuses in it as desired to create gaming, compute, or other platform
    "solutions".
  created_at: 2023-05-30 23:26:43+00:00
  edited: false
  hidden: false
  id: 64769443de6be93fe7b39ca9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-31T00:35:44.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Is not so bad ... RTX 2080 had only 8 GB of VRAM  RTX  3090 was
          a huge progress 24 GB VRAM ... but RTX 4090 should have at least 48 GB ...maybe
          5090 :D</p>

          '
        raw: Is not so bad ... RTX 2080 had only 8 GB of VRAM  RTX  3090 was a huge
          progress 24 GB VRAM ... but RTX 4090 should have at least 48 GB ...maybe
          5090 :D
        updatedAt: '2023-05-31T00:35:44.730Z'
      numEdits: 0
      reactions: []
    id: 64769660de6be93fe7b3c48f
    type: comment
  author: mirek190
  content: Is not so bad ... RTX 2080 had only 8 GB of VRAM  RTX  3090 was a huge
    progress 24 GB VRAM ... but RTX 4090 should have at least 48 GB ...maybe 5090
    :D
  created_at: 2023-05-30 23:35:44+00:00
  edited: false
  hidden: false
  id: 64769660de6be93fe7b3c48f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-31T00:41:57.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>They will never do that because that would "threaten" their pricing
          structure. There''s a reason they set it this way because you have to pay
          the premium to enter the compute club, for basically the same chip.</p>

          <p>For example, they just announced GH200 series going into production and
          it can use NVLink interconnects to create a supercluster that has insane
          amount of VRAM and computer power: <a rel="nofollow" href="https://www.tomshardware.com/news/nvidia-unveils-dgx-gh200-supercomputer-and-mgx-systems-grace-hopper-superchips-in-production">https://www.tomshardware.com/news/nvidia-unveils-dgx-gh200-supercomputer-and-mgx-systems-grace-hopper-superchips-in-production</a></p>

          <p>3090s have (some kind of) NVLink too, but you can''t connect two for
          example to double the compute power or get total 48GB VRAM, and I don''t
          believe this is by accident. NVIDIA is not a charity and they don''t care
          about small people trying to play with the AI in their homes.</p>

          '
        raw: 'They will never do that because that would "threaten" their pricing
          structure. There''s a reason they set it this way because you have to pay
          the premium to enter the compute club, for basically the same chip.


          For example, they just announced GH200 series going into production and
          it can use NVLink interconnects to create a supercluster that has insane
          amount of VRAM and computer power: https://www.tomshardware.com/news/nvidia-unveils-dgx-gh200-supercomputer-and-mgx-systems-grace-hopper-superchips-in-production


          3090s have (some kind of) NVLink too, but you can''t connect two for example
          to double the compute power or get total 48GB VRAM, and I don''t believe
          this is by accident. NVIDIA is not a charity and they don''t care about
          small people trying to play with the AI in their homes.'
        updatedAt: '2023-05-31T00:41:57.509Z'
      numEdits: 0
      reactions: []
    id: 647697d5cfe9d995bf43206b
    type: comment
  author: mancub
  content: 'They will never do that because that would "threaten" their pricing structure.
    There''s a reason they set it this way because you have to pay the premium to
    enter the compute club, for basically the same chip.


    For example, they just announced GH200 series going into production and it can
    use NVLink interconnects to create a supercluster that has insane amount of VRAM
    and computer power: https://www.tomshardware.com/news/nvidia-unveils-dgx-gh200-supercomputer-and-mgx-systems-grace-hopper-superchips-in-production


    3090s have (some kind of) NVLink too, but you can''t connect two for example to
    double the compute power or get total 48GB VRAM, and I don''t believe this is
    by accident. NVIDIA is not a charity and they don''t care about small people trying
    to play with the AI in their homes.'
  created_at: 2023-05-30 23:41:57+00:00
  edited: false
  hidden: false
  id: 647697d5cfe9d995bf43206b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60bc00402fe6bb6bbafdba5408662d24.svg
      fullname: Vadim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vadluk1
      type: user
    createdAt: '2023-06-02T16:48:42.000Z'
    data:
      edited: false
      editors:
      - Vadluk1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60bc00402fe6bb6bbafdba5408662d24.svg
          fullname: Vadim
          isHf: false
          isPro: false
          name: Vadluk1
          type: user
        html: '<p>I managed to run "VicUnlocked-alpaca-65B-QLoRA-GGML" on 32 GB RAM
          and wrote the rest to the hard drive. The speed judging by the processor
          is 4 times slower than if I had enough RAM, but overall it works and I like
          the result!</p>

          '
        raw: I managed to run "VicUnlocked-alpaca-65B-QLoRA-GGML" on 32 GB RAM and
          wrote the rest to the hard drive. The speed judging by the processor is
          4 times slower than if I had enough RAM, but overall it works and I like
          the result!
        updatedAt: '2023-06-02T16:48:42.287Z'
      numEdits: 0
      reactions: []
    id: 647a1d6aa84498f2af4fd827
    type: comment
  author: Vadluk1
  content: I managed to run "VicUnlocked-alpaca-65B-QLoRA-GGML" on 32 GB RAM and wrote
    the rest to the hard drive. The speed judging by the processor is 4 times slower
    than if I had enough RAM, but overall it works and I like the result!
  created_at: 2023-06-02 15:48:42+00:00
  edited: false
  hidden: false
  id: 647a1d6aa84498f2af4fd827
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-02T17:26:05.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>How many tokens you have per sec?<br>Using only 64GB RAM I have
          1.200ms/token and with rtx 3090 ( 39 layers from 80 )  700ms/token .</p>

          '
        raw: 'How many tokens you have per sec?

          Using only 64GB RAM I have 1.200ms/token and with rtx 3090 ( 39 layers from
          80 )  700ms/token .'
        updatedAt: '2023-06-02T17:26:05.273Z'
      numEdits: 0
      reactions: []
    id: 647a262df518a860fbcf73dc
    type: comment
  author: mirek190
  content: 'How many tokens you have per sec?

    Using only 64GB RAM I have 1.200ms/token and with rtx 3090 ( 39 layers from 80
    )  700ms/token .'
  created_at: 2023-06-02 16:26:05+00:00
  edited: false
  hidden: false
  id: 647a262df518a860fbcf73dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60bc00402fe6bb6bbafdba5408662d24.svg
      fullname: Vadim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vadluk1
      type: user
    createdAt: '2023-06-02T21:58:26.000Z'
    data:
      edited: false
      editors:
      - Vadluk1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7105254530906677
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60bc00402fe6bb6bbafdba5408662d24.svg
          fullname: Vadim
          isHf: false
          isPro: false
          name: Vadluk1
          type: user
        html: '<p>Output generated in 6931.96 seconds (0.01 tokens/s, 99 tokens, context
          57, seed 474717586)<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6418acd1d13ffa40812b30ba/D0u8dq7-_LpXy-rx5N1R6.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6418acd1d13ffa40812b30ba/D0u8dq7-_LpXy-rx5N1R6.png"></a><br>It
          takes a long time, but the results are there.</p>

          '
        raw: 'Output generated in 6931.96 seconds (0.01 tokens/s, 99 tokens, context
          57, seed 474717586)

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6418acd1d13ffa40812b30ba/D0u8dq7-_LpXy-rx5N1R6.png)

          It takes a long time, but the results are there.'
        updatedAt: '2023-06-02T21:58:26.796Z'
      numEdits: 0
      reactions: []
    id: 647a6602c7367455fd9e459b
    type: comment
  author: Vadluk1
  content: 'Output generated in 6931.96 seconds (0.01 tokens/s, 99 tokens, context
    57, seed 474717586)

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6418acd1d13ffa40812b30ba/D0u8dq7-_LpXy-rx5N1R6.png)

    It takes a long time, but the results are there.'
  created_at: 2023-06-02 20:58:26+00:00
  edited: false
  hidden: false
  id: 647a6602c7367455fd9e459b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-02T22:36:26.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20663835108280182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>68 seconds per token ??? ok ........................</p>

          '
        raw: 68 seconds per token ??? ok ........................
        updatedAt: '2023-06-02T22:36:26.770Z'
      numEdits: 0
      reactions: []
    id: 647a6eeac7367455fd9f3ba6
    type: comment
  author: mirek190
  content: 68 seconds per token ??? ok ........................
  created_at: 2023-06-02 21:36:26+00:00
  edited: false
  hidden: false
  id: 647a6eeac7367455fd9f3ba6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-06-02T22:41:18.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: kk
        probability: 0.7798687815666199
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>ouch</p>

          '
        raw: ouch
        updatedAt: '2023-06-02T22:41:18.823Z'
      numEdits: 0
      reactions: []
    id: 647a700ec7367455fd9f573c
    type: comment
  author: Nurb432
  content: ouch
  created_at: 2023-06-02 21:41:18+00:00
  edited: false
  hidden: false
  id: 647a700ec7367455fd9f573c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-03T20:38:08.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487295150756836
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>OMG, guys! I have fantastic news! Well, for those who use GGML models
          with KoboldCpp at least! I just noticed that 3 days ago, guys working at
          KoboldCpp released a new version and this is one of the changes!</p>

          <p>*Integrated the Clblast GPU offloading improvements from @0cc4m which
          allows you to have a layer fully stored in VRAM instead of keeping a duplicate
          copy in RAM. As a result, offloading GPU layers will reduce overall RAM
          used.</p>

          <p>YES! I guess this is the kind of behavior you were describing earlier,
          right? Saving RAM. So in theory I should be able to run slightly more demanding
          models without issues, right? :D Well, honestly I''ll be happy for any little
          boost I can get with the models I currently use.</p>

          '
        raw: 'OMG, guys! I have fantastic news! Well, for those who use GGML models
          with KoboldCpp at least! I just noticed that 3 days ago, guys working at
          KoboldCpp released a new version and this is one of the changes!


          *Integrated the Clblast GPU offloading improvements from @0cc4m which allows
          you to have a layer fully stored in VRAM instead of keeping a duplicate
          copy in RAM. As a result, offloading GPU layers will reduce overall RAM
          used.


          YES! I guess this is the kind of behavior you were describing earlier, right?
          Saving RAM. So in theory I should be able to run slightly more demanding
          models without issues, right? :D Well, honestly I''ll be happy for any little
          boost I can get with the models I currently use.'
        updatedAt: '2023-06-03T20:38:08.298Z'
      numEdits: 0
      reactions: []
    id: 647ba4b06dbad6ab05845895
    type: comment
  author: MrDevolver
  content: 'OMG, guys! I have fantastic news! Well, for those who use GGML models
    with KoboldCpp at least! I just noticed that 3 days ago, guys working at KoboldCpp
    released a new version and this is one of the changes!


    *Integrated the Clblast GPU offloading improvements from @0cc4m which allows you
    to have a layer fully stored in VRAM instead of keeping a duplicate copy in RAM.
    As a result, offloading GPU layers will reduce overall RAM used.


    YES! I guess this is the kind of behavior you were describing earlier, right?
    Saving RAM. So in theory I should be able to run slightly more demanding models
    without issues, right? :D Well, honestly I''ll be happy for any little boost I
    can get with the models I currently use.'
  created_at: 2023-06-03 19:38:08+00:00
  edited: false
  hidden: false
  id: 647ba4b06dbad6ab05845895
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-03T20:46:04.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9053484797477722
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>yes ... that came from llama.cpp ... was ported to koboldcpp  ;)</p>

          '
        raw: yes ... that came from llama.cpp ... was ported to koboldcpp  ;)
        updatedAt: '2023-06-03T20:46:54.365Z'
      numEdits: 1
      reactions: []
    id: 647ba68cb31514a4a6e4663c
    type: comment
  author: mirek190
  content: yes ... that came from llama.cpp ... was ported to koboldcpp  ;)
  created_at: 2023-06-03 19:46:04+00:00
  edited: true
  hidden: false
  id: 647ba68cb31514a4a6e4663c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-03T20:53:16.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9654622077941895
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>Yay! :D I can''t wait to test it more, but loading the model into
          the new version with Clblast enabled seems to save 32% of my ram compared
          to previous version!</p>

          '
        raw: Yay! :D I can't wait to test it more, but loading the model into the
          new version with Clblast enabled seems to save 32% of my ram compared to
          previous version!
        updatedAt: '2023-06-03T20:53:16.139Z'
      numEdits: 0
      reactions: []
    id: 647ba83c4d7c0c3fccd91404
    type: comment
  author: MrDevolver
  content: Yay! :D I can't wait to test it more, but loading the model into the new
    version with Clblast enabled seems to save 32% of my ram compared to previous
    version!
  created_at: 2023-06-03 19:53:16+00:00
  edited: false
  hidden: false
  id: 647ba83c4d7c0c3fccd91404
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-03T21:22:56.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8695196509361267
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I have RTX 3090 with 24GB of VRAM so fully loaded 65B q5_1 model
          takes only 30 GB of my RAM now ;)</p>

          '
        raw: I have RTX 3090 with 24GB of VRAM so fully loaded 65B q5_1 model takes
          only 30 GB of my RAM now ;)
        updatedAt: '2023-06-03T21:22:56.242Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 647baf30e8b7333058b01fb8
    type: comment
  author: mirek190
  content: I have RTX 3090 with 24GB of VRAM so fully loaded 65B q5_1 model takes
    only 30 GB of my RAM now ;)
  created_at: 2023-06-03 20:22:56+00:00
  edited: false
  hidden: false
  id: 647baf30e8b7333058b01fb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d619436e2e0d6eb8c53bfcdb17842b47.svg
      fullname: fshadow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: fshadow
      type: user
    createdAt: '2023-06-04T03:12:16.000Z'
    data:
      edited: false
      editors:
      - fshadow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6299309134483337
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d619436e2e0d6eb8c53bfcdb17842b47.svg
          fullname: fshadow
          isHf: false
          isPro: true
          name: fshadow
          type: user
        html: '<p>Is there a way to use mac m2 gpu ?</p>

          '
        raw: Is there a way to use mac m2 gpu ?
        updatedAt: '2023-06-04T03:12:16.082Z'
      numEdits: 0
      reactions: []
    id: 647c0110e8b7333058b7a634
    type: comment
  author: fshadow
  content: Is there a way to use mac m2 gpu ?
  created_at: 2023-06-04 02:12:16+00:00
  edited: false
  hidden: false
  id: 647c0110e8b7333058b7a634
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T10:03:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9372053146362305
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Is there a way to use mac m2 gpu ?</p>

          </blockquote>

          <p>Yes, but it may not help much at the moment.</p>

          <p>You need to install CLBLAST, which can be done through Homebrew with:</p>

          <pre><code>brew install clblast

          </code></pre>

          <p>And then build llama.cpp like so:</p>

          <pre><code>LLAMA_CLBLAST=1 make

          </code></pre>

          <p>But on M1/M2 Mac, I don''t think it will help at all.</p>

          <p>What will help is a new feature being worked on which will provide full
          Metal acceleration.  You can track its progress here: <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/pull/1642">https://github.com/ggerganov/llama.cpp/pull/1642</a></p>

          '
        raw: '> Is there a way to use mac m2 gpu ?


          Yes, but it may not help much at the moment.


          You need to install CLBLAST, which can be done through Homebrew with:


          ```

          brew install clblast

          ```


          And then build llama.cpp like so:


          ```

          LLAMA_CLBLAST=1 make

          ```


          But on M1/M2 Mac, I don''t think it will help at all.


          What will help is a new feature being worked on which will provide full
          Metal acceleration.  You can track its progress here: https://github.com/ggerganov/llama.cpp/pull/1642'
        updatedAt: '2023-06-04T10:03:54.177Z'
      numEdits: 0
      reactions: []
    id: 647c618a60dfe0f35d4cb285
    type: comment
  author: TheBloke
  content: '> Is there a way to use mac m2 gpu ?


    Yes, but it may not help much at the moment.


    You need to install CLBLAST, which can be done through Homebrew with:


    ```

    brew install clblast

    ```


    And then build llama.cpp like so:


    ```

    LLAMA_CLBLAST=1 make

    ```


    But on M1/M2 Mac, I don''t think it will help at all.


    What will help is a new feature being worked on which will provide full Metal
    acceleration.  You can track its progress here: https://github.com/ggerganov/llama.cpp/pull/1642'
  created_at: 2023-06-04 09:03:54+00:00
  edited: false
  hidden: false
  id: 647c618a60dfe0f35d4cb285
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T10:05:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9458392858505249
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I have RTX 3090 with 24GB of VRAM so fully loaded 65B q5_1 model takes
          only 30 GB of my RAM now ;)</p>

          </blockquote>

          <p>Very cool!  What CPU do you have and what is performance like? What''s
          your run time in ms?</p>

          '
        raw: '> I have RTX 3090 with 24GB of VRAM so fully loaded 65B q5_1 model takes
          only 30 GB of my RAM now ;)


          Very cool!  What CPU do you have and what is performance like? What''s your
          run time in ms?'
        updatedAt: '2023-06-04T10:05:31.691Z'
      numEdits: 0
      reactions: []
    id: 647c61eb83c62f3249148db8
    type: comment
  author: TheBloke
  content: '> I have RTX 3090 with 24GB of VRAM so fully loaded 65B q5_1 model takes
    only 30 GB of my RAM now ;)


    Very cool!  What CPU do you have and what is performance like? What''s your run
    time in ms?'
  created_at: 2023-06-04 09:05:31+00:00
  edited: false
  hidden: false
  id: 647c61eb83c62f3249148db8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-04T11:37:12.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8456871509552002
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>CPU 9 9 9900K with 64GB of RAM.<br>With that  65B q5_1  model I
          can fit 37 layers on GPU.<br>Have around ~700ms/token</p>

          <p>Waiting for better DDR4 -  4000 Mhz ... right now have 3000 Mhz with
          shitty timings  ...</p>

          '
        raw: 'CPU 9 9 9900K with 64GB of RAM.

          With that  65B q5_1  model I can fit 37 layers on GPU.

          Have around ~700ms/token


          Waiting for better DDR4 -  4000 Mhz ... right now have 3000 Mhz with shitty
          timings  ...'
        updatedAt: '2023-06-04T11:37:12.025Z'
      numEdits: 0
      reactions: []
    id: 647c7768c788767ab5caf653
    type: comment
  author: mirek190
  content: 'CPU 9 9 9900K with 64GB of RAM.

    With that  65B q5_1  model I can fit 37 layers on GPU.

    Have around ~700ms/token


    Waiting for better DDR4 -  4000 Mhz ... right now have 3000 Mhz with shitty timings  ...'
  created_at: 2023-06-04 10:37:12+00:00
  edited: false
  hidden: false
  id: 647c7768c788767ab5caf653
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-05T09:51:50.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9812062382698059
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>So, I just tried to load a slightly bigger model into KoboldCpp.
          It is a model which wouldn''t fit entirely into my RAM alone, but it should
          fit when both RAM and VRAM were used. Sadly, even with offloading into VRAM
          I''m getting this message:<br>GGML_ASSERT: ggml.c:3977: ctx-&gt;mem_buffer
          != NULL</p>

          <p>I''m assuming this means that since the model is too big to load into
          RAM alone, it just won''t let me load it even though I''m allowing it to
          use VRAM as well. :(</p>

          '
        raw: 'So, I just tried to load a slightly bigger model into KoboldCpp. It
          is a model which wouldn''t fit entirely into my RAM alone, but it should
          fit when both RAM and VRAM were used. Sadly, even with offloading into VRAM
          I''m getting this message:

          GGML_ASSERT: ggml.c:3977: ctx->mem_buffer != NULL


          I''m assuming this means that since the model is too big to load into RAM
          alone, it just won''t let me load it even though I''m allowing it to use
          VRAM as well. :('
        updatedAt: '2023-06-05T09:51:50.502Z'
      numEdits: 0
      reactions: []
    id: 647db03632c471a7fa837f0a
    type: comment
  author: MrDevolver
  content: 'So, I just tried to load a slightly bigger model into KoboldCpp. It is
    a model which wouldn''t fit entirely into my RAM alone, but it should fit when
    both RAM and VRAM were used. Sadly, even with offloading into VRAM I''m getting
    this message:

    GGML_ASSERT: ggml.c:3977: ctx->mem_buffer != NULL


    I''m assuming this means that since the model is too big to load into RAM alone,
    it just won''t let me load it even though I''m allowing it to use VRAM as well.
    :('
  created_at: 2023-06-05 08:51:50+00:00
  edited: false
  hidden: false
  id: 647db03632c471a7fa837f0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:01:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9044612050056458
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm not sure. That might be a bug.  Show everything you''re running,
          command line and the full output.  Copy and paste it here, surrounded by
          ``` tags</p>

          <pre><code>so the output looks like this

          </code></pre>

          '
        raw: 'Hmm not sure. That might be a bug.  Show everything you''re running,
          command line and the full output.  Copy and paste it here, surrounded by
          \`\`\` tags

          ```

          so the output looks like this

          ```'
        updatedAt: '2023-06-05T10:01:22.211Z'
      numEdits: 0
      reactions: []
    id: 647db27210b7a3b157fd6be0
    type: comment
  author: TheBloke
  content: 'Hmm not sure. That might be a bug.  Show everything you''re running, command
    line and the full output.  Copy and paste it here, surrounded by \`\`\` tags

    ```

    so the output looks like this

    ```'
  created_at: 2023-06-05 09:01:22+00:00
  edited: false
  hidden: false
  id: 647db27210b7a3b157fd6be0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-05T10:27:24.000Z'
    data:
      edited: true
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6597276926040649
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>Sure, let me first give some hints from running koboldcpp.exe with
          --help argument so that you know what''s what (I believe you''ve mentioned
          somewhere else that you''re not familiar with it). Here''s the official
          koboldcpp.exe --help argument output:</p>

          <p>```Welcome to KoboldCpp - Version 1.28<br>usage: koboldcpp.exe [-h] [--model
          [MODEL]] [--port PORT] [--host HOST] [--launch] [--lora LORA] [--threads
          THREADS]<br>                     [--blasthreads [threads]] [--psutil_set_threads]
          [--highpriority]<br>                     [--contextsize {512,1024,2048,4096,8192}]
          [--blasbatchsize {-1,32,64,128,256,512,1024}]<br>                     [--stream]
          [--smartcontext] [--unbantokens] [--usemirostat [type] [tau] [eta]]<br>                     [--forceversion
          [version]] [--nommap] [--usemlock] [--noavx2] [--debugmode] [--skiplauncher]<br>                     [--hordeconfig
          [hordename] [[hordelength] ...]]<br>                     [--noblas | --useclblast
          {0,1,2,3,4,5,6,7,8} {0,1,2,3,4,5,6,7,8}] [--gpulayers [GPU layers]]<br>                     [model_param]
          [port_param]</p>

          <p>KoboldCpp Server</p>

          <p>positional arguments:<br>  model_param           Model file to load (positional)<br>  port_param            Port
          to listen on (positional)</p>

          <p>optional arguments:<br>  -h, --help            show this help message
          and exit<br>  --model [MODEL]       Model file to load<br>  --port PORT           Port
          to listen on<br>  --host HOST           Host IP to listen on. If empty,
          all routable interfaces are accepted.<br>  --launch              Launches
          a web browser when load is completed.<br>  --lora LORA           LLAMA models
          only, applies a lora file on top of model. Experimental.<br>  --threads
          THREADS     Use a custom number of threads if specified. Otherwise, uses
          an amount based on CPU cores<br>  --blasthreads [threads]<br>                        Use
          a different number of threads during BLAS if specified. Otherwise, has the
          same value as<br>                        --threads<br>  --psutil_set_threads  Experimental
          flag. If set, uses psutils to determine thread count based on physical cores.<br>  --highpriority        Experimental
          flag. If set, increases the process CPU priority, potentially speeding up<br>                        generation.
          Use caution.<br>  --contextsize {512,1024,2048,4096,8192}<br>                        Controls
          the memory allocated for maximum context size, only change if you need more
          RAM for<br>                        big contexts. (default 2048)<br>  --blasbatchsize
          {-1,32,64,128,256,512,1024}<br>                        Sets the batch size
          used in BLAS processing (default 512). Setting it to -1 disables BLAS<br>                        mode,
          but keeps other benefits like GPU offload.<br>  --stream              Uses
          pseudo streaming when generating tokens. Only for the Kobold Lite UI.<br>  --smartcontext        Reserving
          a portion of context to try processing less frequently.<br>  --unbantokens         Normally,
          KoboldAI prevents certain tokens such as EOS and Square Brackets. This flag
          unbans<br>                        them.<br>  --usemirostat [type] [tau]
          [eta]<br>                        Experimental! Replaces your samplers with
          mirostat. Takes 3 params = [type(0/1/2), tau(5.0),<br>                        eta(0.1)].<br>  --forceversion
          [version]<br>                        If the model file format detection
          fails (e.g. rogue modified model) you can set this to<br>                        override
          the detected format (enter desired version, e.g. 401 for GPTNeoX-Type2).<br>  --nommap              If
          set, do not use mmap to load newer models<br>  --usemlock            For
          Apple Systems. Force system to keep model in RAM rather than swapping or
          compressing<br>  --noavx2              Do not use AVX2 instructions, a slower
          compatibility mode for older devices. Does not work<br>                        with
          --clblast.<br>  --debugmode           Shows additional debug info in the
          terminal.<br>  --skiplauncher        Doesn''t display or use the new GUI
          launcher.<br>  --hordeconfig [hordename] [[hordelength] ...]<br>                        Sets
          the display model name to something else, for easy use on AI Horde. An optional
          second<br>                        parameter sets the horde max gen length.<br>  --noblas              Do
          not use OpenBLAS for accelerated prompt ingestion<br>  --useclblast {0,1,2,3,4,5,6,7,8}
          {0,1,2,3,4,5,6,7,8}<br>                        Use CLBlast instead of OpenBLAS
          for prompt ingestion. Must specify exactly 2 arguments,<br>                        platform
          ID and device ID (e.g. --useclblast 1 0).<br>  --gpulayers [GPU layers]<br>                        Set
          number of layers to offload to GPU when using CLBlast. Requires CLBlast.```</p>

          <p>I tried to run KoboldCpp like with the following parameters (still playing
          with various settings to find out what would work best for individual cases
          on my hardware):<br><code>koboldcpp.exe --highpriority --threads 8 --blasthreads
          3 --contextsize 2048 --smartcontext --stream --blasbatchsize -1 --useclblast
          0 0 --gpulayers 30 --launch</code></p>

          <p>Running koboldcpp.exe like that makes it possible to choose which model
          to load, so I''m trying to load one that''s slightly bigger than what would
          fit into my 16 GB RAM. The model is  starchat-alpha-GGML.</p>

          <p>And here is the complete output and the model obviously fails to load:</p>

          <p>```Welcome to KoboldCpp - Version 1.28<br>For command line arguments,
          please refer to --help<br>Otherwise, please manually select ggml file:<br>Setting
          process to Higher Priority - Use Caution<br>High Priority for Windows Set:
          Priority.NORMAL_PRIORITY_CLASS to Priority.HIGH_PRIORITY_CLASS<br>Attempting
          to use CLBlast library for faster prompt ingestion. A compatible clblast
          will be required.<br>Initializing dynamic library: koboldcpp_clblast.dll<br>==========<br>Loading
          model: D:\AI_Models\Loose_models\starchat-alpha-GGML\starchat-alpha-ggml-q4_0.bin<br>[Threads:
          8, BlasThreads: 3, SmartContext: True]</p>

          <hr>

          <p>Identified as GPT-2 model: (ver 203)<br>Attempting to Load...</p>

          <hr>

          <p>System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI
          = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |<br>gpt2_model_load: loading model
          from ''D:\AI_Models\Loose_models\starchat-alpha-GGML\starchat-alpha-ggml-q4_0.bin''<br>gpt2_model_load:
          n_vocab = 49156<br>gpt2_model_load: n_ctx   = 8192<br>gpt2_model_load: n_embd  =
          6144<br>gpt2_model_load: n_head  = 48<br>gpt2_model_load: n_layer = 40<br>gpt2_model_load:
          ftype   = 2002<br>gpt2_model_load: qntvr   = 2<br>gpt2_model_load: ggml
          ctx size = 17928.50 MB</p>

          <p>Platform:0 Device:0  - AMD Accelerated Parallel Processing with gfx900</p>

          <p>ggml_opencl: selecting platform: ''AMD Accelerated Parallel Processing''<br>ggml_opencl:
          selecting device: ''gfx900''<br>ggml_opencl: device FP16 support: true<br>CL
          FP16 temporarily disabled pending further optimization.<br>GGML_ASSERT:
          ggml.c:3977: ctx-&gt;mem_buffer != NULL```</p>

          <p>I tried using different values for layers like 35 which I believe is
          as high as I can go on my GPU, but it still fails.</p>

          '
        raw: "Sure, let me first give some hints from running koboldcpp.exe with --help\
          \ argument so that you know what's what (I believe you've mentioned somewhere\
          \ else that you're not familiar with it). Here's the official koboldcpp.exe\
          \ --help argument output:\n```Welcome to KoboldCpp - Version 1.28\nusage:\
          \ koboldcpp.exe [-h] [--model [MODEL]] [--port PORT] [--host HOST] [--launch]\
          \ [--lora LORA] [--threads THREADS]\n                     [--blasthreads\
          \ [threads]] [--psutil_set_threads] [--highpriority]\n                 \
          \    [--contextsize {512,1024,2048,4096,8192}] [--blasbatchsize {-1,32,64,128,256,512,1024}]\n\
          \                     [--stream] [--smartcontext] [--unbantokens] [--usemirostat\
          \ [type] [tau] [eta]]\n                     [--forceversion [version]] [--nommap]\
          \ [--usemlock] [--noavx2] [--debugmode] [--skiplauncher]\n             \
          \        [--hordeconfig [hordename] [[hordelength] ...]]\n             \
          \        [--noblas | --useclblast {0,1,2,3,4,5,6,7,8} {0,1,2,3,4,5,6,7,8}]\
          \ [--gpulayers [GPU layers]]\n                     [model_param] [port_param]\n\
          \nKoboldCpp Server\n\npositional arguments:\n  model_param           Model\
          \ file to load (positional)\n  port_param            Port to listen on (positional)\n\
          \noptional arguments:\n  -h, --help            show this help message and\
          \ exit\n  --model [MODEL]       Model file to load\n  --port PORT      \
          \     Port to listen on\n  --host HOST           Host IP to listen on. If\
          \ empty, all routable interfaces are accepted.\n  --launch             \
          \ Launches a web browser when load is completed.\n  --lora LORA        \
          \   LLAMA models only, applies a lora file on top of model. Experimental.\n\
          \  --threads THREADS     Use a custom number of threads if specified. Otherwise,\
          \ uses an amount based on CPU cores\n  --blasthreads [threads]\n       \
          \                 Use a different number of threads during BLAS if specified.\
          \ Otherwise, has the same value as\n                        --threads\n\
          \  --psutil_set_threads  Experimental flag. If set, uses psutils to determine\
          \ thread count based on physical cores.\n  --highpriority        Experimental\
          \ flag. If set, increases the process CPU priority, potentially speeding\
          \ up\n                        generation. Use caution.\n  --contextsize\
          \ {512,1024,2048,4096,8192}\n                        Controls the memory\
          \ allocated for maximum context size, only change if you need more RAM for\n\
          \                        big contexts. (default 2048)\n  --blasbatchsize\
          \ {-1,32,64,128,256,512,1024}\n                        Sets the batch size\
          \ used in BLAS processing (default 512). Setting it to -1 disables BLAS\n\
          \                        mode, but keeps other benefits like GPU offload.\n\
          \  --stream              Uses pseudo streaming when generating tokens. Only\
          \ for the Kobold Lite UI.\n  --smartcontext        Reserving a portion of\
          \ context to try processing less frequently.\n  --unbantokens         Normally,\
          \ KoboldAI prevents certain tokens such as EOS and Square Brackets. This\
          \ flag unbans\n                        them.\n  --usemirostat [type] [tau]\
          \ [eta]\n                        Experimental! Replaces your samplers with\
          \ mirostat. Takes 3 params = [type(0/1/2), tau(5.0),\n                 \
          \       eta(0.1)].\n  --forceversion [version]\n                       \
          \ If the model file format detection fails (e.g. rogue modified model) you\
          \ can set this to\n                        override the detected format\
          \ (enter desired version, e.g. 401 for GPTNeoX-Type2).\n  --nommap     \
          \         If set, do not use mmap to load newer models\n  --usemlock   \
          \         For Apple Systems. Force system to keep model in RAM rather than\
          \ swapping or compressing\n  --noavx2              Do not use AVX2 instructions,\
          \ a slower compatibility mode for older devices. Does not work\n       \
          \                 with --clblast.\n  --debugmode           Shows additional\
          \ debug info in the terminal.\n  --skiplauncher        Doesn't display or\
          \ use the new GUI launcher.\n  --hordeconfig [hordename] [[hordelength]\
          \ ...]\n                        Sets the display model name to something\
          \ else, for easy use on AI Horde. An optional second\n                 \
          \       parameter sets the horde max gen length.\n  --noblas           \
          \   Do not use OpenBLAS for accelerated prompt ingestion\n  --useclblast\
          \ {0,1,2,3,4,5,6,7,8} {0,1,2,3,4,5,6,7,8}\n                        Use CLBlast\
          \ instead of OpenBLAS for prompt ingestion. Must specify exactly 2 arguments,\n\
          \                        platform ID and device ID (e.g. --useclblast 1\
          \ 0).\n  --gpulayers [GPU layers]\n                        Set number of\
          \ layers to offload to GPU when using CLBlast. Requires CLBlast.```\n\n\
          I tried to run KoboldCpp like with the following parameters (still playing\
          \ with various settings to find out what would work best for individual\
          \ cases on my hardware):\n```koboldcpp.exe --highpriority --threads 8 --blasthreads\
          \ 3 --contextsize 2048 --smartcontext --stream --blasbatchsize -1 --useclblast\
          \ 0 0 --gpulayers 30 --launch```\n\nRunning koboldcpp.exe like that makes\
          \ it possible to choose which model to load, so I'm trying to load one that's\
          \ slightly bigger than what would fit into my 16 GB RAM. The model is  starchat-alpha-GGML.\n\
          \nAnd here is the complete output and the model obviously fails to load:\n\
          ```Welcome to KoboldCpp - Version 1.28\nFor command line arguments, please\
          \ refer to --help\nOtherwise, please manually select ggml file:\nSetting\
          \ process to Higher Priority - Use Caution\nHigh Priority for Windows Set:\
          \ Priority.NORMAL_PRIORITY_CLASS to Priority.HIGH_PRIORITY_CLASS\nAttempting\
          \ to use CLBlast library for faster prompt ingestion. A compatible clblast\
          \ will be required.\nInitializing dynamic library: koboldcpp_clblast.dll\n\
          ==========\nLoading model: D:\\AI_Models\\Loose_models\\starchat-alpha-GGML\\\
          starchat-alpha-ggml-q4_0.bin\n[Threads: 8, BlasThreads: 3, SmartContext:\
          \ True]\n\n---\nIdentified as GPT-2 model: (ver 203)\nAttempting to Load...\n\
          ---\nSystem Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\ngpt2_model_load: loading model\
          \ from 'D:\\AI_Models\\Loose_models\\starchat-alpha-GGML\\starchat-alpha-ggml-q4_0.bin'\n\
          gpt2_model_load: n_vocab = 49156\ngpt2_model_load: n_ctx   = 8192\ngpt2_model_load:\
          \ n_embd  = 6144\ngpt2_model_load: n_head  = 48\ngpt2_model_load: n_layer\
          \ = 40\ngpt2_model_load: ftype   = 2002\ngpt2_model_load: qntvr   = 2\n\
          gpt2_model_load: ggml ctx size = 17928.50 MB\n\nPlatform:0 Device:0  - AMD\
          \ Accelerated Parallel Processing with gfx900\n\nggml_opencl: selecting\
          \ platform: 'AMD Accelerated Parallel Processing'\nggml_opencl: selecting\
          \ device: 'gfx900'\nggml_opencl: device FP16 support: true\nCL FP16 temporarily\
          \ disabled pending further optimization.\nGGML_ASSERT: ggml.c:3977: ctx->mem_buffer\
          \ != NULL```\n\nI tried using different values for layers like 35 which\
          \ I believe is as high as I can go on my GPU, but it still fails."
        updatedAt: '2023-06-05T10:28:53.015Z'
      numEdits: 2
      reactions: []
    id: 647db88c5214d172cbb2b3ee
    type: comment
  author: MrDevolver
  content: "Sure, let me first give some hints from running koboldcpp.exe with --help\
    \ argument so that you know what's what (I believe you've mentioned somewhere\
    \ else that you're not familiar with it). Here's the official koboldcpp.exe --help\
    \ argument output:\n```Welcome to KoboldCpp - Version 1.28\nusage: koboldcpp.exe\
    \ [-h] [--model [MODEL]] [--port PORT] [--host HOST] [--launch] [--lora LORA]\
    \ [--threads THREADS]\n                     [--blasthreads [threads]] [--psutil_set_threads]\
    \ [--highpriority]\n                     [--contextsize {512,1024,2048,4096,8192}]\
    \ [--blasbatchsize {-1,32,64,128,256,512,1024}]\n                     [--stream]\
    \ [--smartcontext] [--unbantokens] [--usemirostat [type] [tau] [eta]]\n      \
    \               [--forceversion [version]] [--nommap] [--usemlock] [--noavx2]\
    \ [--debugmode] [--skiplauncher]\n                     [--hordeconfig [hordename]\
    \ [[hordelength] ...]]\n                     [--noblas | --useclblast {0,1,2,3,4,5,6,7,8}\
    \ {0,1,2,3,4,5,6,7,8}] [--gpulayers [GPU layers]]\n                     [model_param]\
    \ [port_param]\n\nKoboldCpp Server\n\npositional arguments:\n  model_param   \
    \        Model file to load (positional)\n  port_param            Port to listen\
    \ on (positional)\n\noptional arguments:\n  -h, --help            show this help\
    \ message and exit\n  --model [MODEL]       Model file to load\n  --port PORT\
    \           Port to listen on\n  --host HOST           Host IP to listen on. If\
    \ empty, all routable interfaces are accepted.\n  --launch              Launches\
    \ a web browser when load is completed.\n  --lora LORA           LLAMA models\
    \ only, applies a lora file on top of model. Experimental.\n  --threads THREADS\
    \     Use a custom number of threads if specified. Otherwise, uses an amount based\
    \ on CPU cores\n  --blasthreads [threads]\n                        Use a different\
    \ number of threads during BLAS if specified. Otherwise, has the same value as\n\
    \                        --threads\n  --psutil_set_threads  Experimental flag.\
    \ If set, uses psutils to determine thread count based on physical cores.\n  --highpriority\
    \        Experimental flag. If set, increases the process CPU priority, potentially\
    \ speeding up\n                        generation. Use caution.\n  --contextsize\
    \ {512,1024,2048,4096,8192}\n                        Controls the memory allocated\
    \ for maximum context size, only change if you need more RAM for\n           \
    \             big contexts. (default 2048)\n  --blasbatchsize {-1,32,64,128,256,512,1024}\n\
    \                        Sets the batch size used in BLAS processing (default\
    \ 512). Setting it to -1 disables BLAS\n                        mode, but keeps\
    \ other benefits like GPU offload.\n  --stream              Uses pseudo streaming\
    \ when generating tokens. Only for the Kobold Lite UI.\n  --smartcontext     \
    \   Reserving a portion of context to try processing less frequently.\n  --unbantokens\
    \         Normally, KoboldAI prevents certain tokens such as EOS and Square Brackets.\
    \ This flag unbans\n                        them.\n  --usemirostat [type] [tau]\
    \ [eta]\n                        Experimental! Replaces your samplers with mirostat.\
    \ Takes 3 params = [type(0/1/2), tau(5.0),\n                        eta(0.1)].\n\
    \  --forceversion [version]\n                        If the model file format\
    \ detection fails (e.g. rogue modified model) you can set this to\n          \
    \              override the detected format (enter desired version, e.g. 401 for\
    \ GPTNeoX-Type2).\n  --nommap              If set, do not use mmap to load newer\
    \ models\n  --usemlock            For Apple Systems. Force system to keep model\
    \ in RAM rather than swapping or compressing\n  --noavx2              Do not use\
    \ AVX2 instructions, a slower compatibility mode for older devices. Does not work\n\
    \                        with --clblast.\n  --debugmode           Shows additional\
    \ debug info in the terminal.\n  --skiplauncher        Doesn't display or use\
    \ the new GUI launcher.\n  --hordeconfig [hordename] [[hordelength] ...]\n   \
    \                     Sets the display model name to something else, for easy\
    \ use on AI Horde. An optional second\n                        parameter sets\
    \ the horde max gen length.\n  --noblas              Do not use OpenBLAS for accelerated\
    \ prompt ingestion\n  --useclblast {0,1,2,3,4,5,6,7,8} {0,1,2,3,4,5,6,7,8}\n \
    \                       Use CLBlast instead of OpenBLAS for prompt ingestion.\
    \ Must specify exactly 2 arguments,\n                        platform ID and device\
    \ ID (e.g. --useclblast 1 0).\n  --gpulayers [GPU layers]\n                  \
    \      Set number of layers to offload to GPU when using CLBlast. Requires CLBlast.```\n\
    \nI tried to run KoboldCpp like with the following parameters (still playing with\
    \ various settings to find out what would work best for individual cases on my\
    \ hardware):\n```koboldcpp.exe --highpriority --threads 8 --blasthreads 3 --contextsize\
    \ 2048 --smartcontext --stream --blasbatchsize -1 --useclblast 0 0 --gpulayers\
    \ 30 --launch```\n\nRunning koboldcpp.exe like that makes it possible to choose\
    \ which model to load, so I'm trying to load one that's slightly bigger than what\
    \ would fit into my 16 GB RAM. The model is  starchat-alpha-GGML.\n\nAnd here\
    \ is the complete output and the model obviously fails to load:\n```Welcome to\
    \ KoboldCpp - Version 1.28\nFor command line arguments, please refer to --help\n\
    Otherwise, please manually select ggml file:\nSetting process to Higher Priority\
    \ - Use Caution\nHigh Priority for Windows Set: Priority.NORMAL_PRIORITY_CLASS\
    \ to Priority.HIGH_PRIORITY_CLASS\nAttempting to use CLBlast library for faster\
    \ prompt ingestion. A compatible clblast will be required.\nInitializing dynamic\
    \ library: koboldcpp_clblast.dll\n==========\nLoading model: D:\\AI_Models\\Loose_models\\\
    starchat-alpha-GGML\\starchat-alpha-ggml-q4_0.bin\n[Threads: 8, BlasThreads: 3,\
    \ SmartContext: True]\n\n---\nIdentified as GPT-2 model: (ver 203)\nAttempting\
    \ to Load...\n---\nSystem Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
    \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA\
    \ = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\ngpt2_model_load: loading\
    \ model from 'D:\\AI_Models\\Loose_models\\starchat-alpha-GGML\\starchat-alpha-ggml-q4_0.bin'\n\
    gpt2_model_load: n_vocab = 49156\ngpt2_model_load: n_ctx   = 8192\ngpt2_model_load:\
    \ n_embd  = 6144\ngpt2_model_load: n_head  = 48\ngpt2_model_load: n_layer = 40\n\
    gpt2_model_load: ftype   = 2002\ngpt2_model_load: qntvr   = 2\ngpt2_model_load:\
    \ ggml ctx size = 17928.50 MB\n\nPlatform:0 Device:0  - AMD Accelerated Parallel\
    \ Processing with gfx900\n\nggml_opencl: selecting platform: 'AMD Accelerated\
    \ Parallel Processing'\nggml_opencl: selecting device: 'gfx900'\nggml_opencl:\
    \ device FP16 support: true\nCL FP16 temporarily disabled pending further optimization.\n\
    GGML_ASSERT: ggml.c:3977: ctx->mem_buffer != NULL```\n\nI tried using different\
    \ values for layers like 35 which I believe is as high as I can go on my GPU,\
    \ but it still fails."
  created_at: 2023-06-05 09:27:24+00:00
  edited: true
  hidden: false
  id: 647db88c5214d172cbb2b3ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:33:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9482110738754272
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh. That''s a different model type, BigCoder. To my knowledge that
          doesn''t support GPU offload.  I think only Llama models support GPU offload
          at this time.</p>

          '
        raw: Oh. That's a different model type, BigCoder. To my knowledge that doesn't
          support GPU offload.  I think only Llama models support GPU offload at this
          time.
        updatedAt: '2023-06-05T10:33:51.877Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mirek190
    id: 647dba0f5214d172cbb2ecc5
    type: comment
  author: TheBloke
  content: Oh. That's a different model type, BigCoder. To my knowledge that doesn't
    support GPU offload.  I think only Llama models support GPU offload at this time.
  created_at: 2023-06-05 09:33:51+00:00
  edited: false
  hidden: false
  id: 647dba0f5214d172cbb2ecc5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-05T10:37:35.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960899829864502
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<blockquote>

          <p>Oh. That''s a different model type, BigCoder. To my knowledge that doesn''t
          support GPU offload.  I think only Llama models support GPU offload at this
          time.</p>

          </blockquote>

          <p>Damn, I was hoping to get that AI coding assistant for when internet
          connection is down lol</p>

          '
        raw: '> Oh. That''s a different model type, BigCoder. To my knowledge that
          doesn''t support GPU offload.  I think only Llama models support GPU offload
          at this time.


          Damn, I was hoping to get that AI coding assistant for when internet connection
          is down lol'
        updatedAt: '2023-06-05T10:37:35.654Z'
      numEdits: 0
      reactions: []
    id: 647dbaef5214d172cbb30a58
    type: comment
  author: MrDevolver
  content: '> Oh. That''s a different model type, BigCoder. To my knowledge that doesn''t
    support GPU offload.  I think only Llama models support GPU offload at this time.


    Damn, I was hoping to get that AI coding assistant for when internet connection
    is down lol'
  created_at: 2023-06-05 09:37:35+00:00
  edited: false
  hidden: false
  id: 647dbaef5214d172cbb30a58
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/VicUnlocked-alpaca-65B-QLoRA-GGML
repo_type: model
status: open
target_branch: null
title: About this model
