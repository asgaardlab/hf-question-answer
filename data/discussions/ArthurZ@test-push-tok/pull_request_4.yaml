!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArthurZ
conflicting_files: []
created_at: 2023-10-09 18:49:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-10-09T19:49:33.000Z'
    data:
      edited: true
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7525385618209839
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Following the merge of <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/24310">a
          PR</a> in `transformers` it appeared thatthis model was not properly converted.
          This PR will fix the inference and was tested using the following script:```python&gt;&gt;&gt;
          from transformers import MarianModel, MarianMTModel&gt;&gt;&gt; tokenizer
          = AutoTokenizer.from_pretrained("$model_id")&gt;&gt;&gt; inputs = tokenizer("$translation",
          return_tensors="pt", padding=True)&gt;&gt;&gt; model = MarianMTModel.from_pretrained("$model_id")&gt;&gt;&gt;
          print(tokenizer.batch_decode(model.generate(**inputs)))"$captured_output"```</p>

          '
        raw: Following the merge of [a PR](https://github.com/huggingface/transformers/pull/24310)
          in \`transformers\` it appeared thatthis model was not properly converted.
          This PR will fix the inference and was tested using the following script:\`\`\`python>>>
          from transformers import MarianModel, MarianMTModel>>> tokenizer = AutoTokenizer.from_pretrained("\$model_id")>>>
          inputs = tokenizer("\$translation", return_tensors="pt", padding=True)>>>
          model = MarianMTModel.from_pretrained("\$model_id")>>> print(tokenizer.batch_decode(model.generate(**inputs)))"\$captured_output"\`\`\`
        updatedAt: '2023-10-09T19:49:57.113Z'
      numEdits: 1
      reactions: []
    id: 6524594d73a0f19d06f8fa34
    type: comment
  author: ArthurZ
  content: Following the merge of [a PR](https://github.com/huggingface/transformers/pull/24310)
    in \`transformers\` it appeared thatthis model was not properly converted. This
    PR will fix the inference and was tested using the following script:\`\`\`python>>>
    from transformers import MarianModel, MarianMTModel>>> tokenizer = AutoTokenizer.from_pretrained("\$model_id")>>>
    inputs = tokenizer("\$translation", return_tensors="pt", padding=True)>>> model
    = MarianMTModel.from_pretrained("\$model_id")>>> print(tokenizer.batch_decode(model.generate(**inputs)))"\$captured_output"\`\`\`
  created_at: 2023-10-09 18:49:33+00:00
  edited: true
  hidden: false
  id: 6524594d73a0f19d06f8fa34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-10-09T19:49:34.000Z'
    data:
      oid: f43a009eb16dcf79401c422190e66571a777e74c
      parents:
      - 1405c0b1a80a15ec92dc12a1882d7b6689aa91ae
      subject: Upload tokenizer
    id: 6524594e0000000000000000
    type: commit
  author: ArthurZ
  created_at: 2023-10-09 18:49:34+00:00
  id: 6524594e0000000000000000
  oid: f43a009eb16dcf79401c422190e66571a777e74c
  summary: Upload tokenizer
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 4
repo_id: ArthurZ/test-push-tok
repo_type: model
status: open
target_branch: refs/heads/main
title: Upload tokenizer
