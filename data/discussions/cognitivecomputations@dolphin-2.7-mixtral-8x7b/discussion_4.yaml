!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joorei
conflicting_files: null
created_at: 2024-01-04 14:56:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
      fullname: Juraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joorei
      type: user
    createdAt: '2024-01-04T14:56:29.000Z'
    data:
      edited: false
      editors:
      - joorei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8466028571128845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
          fullname: Juraj
          isHf: false
          isPro: false
          name: joorei
          type: user
        html: '<p>I am trying to fine tune with axolotl (using axolotl''s docker),
          but I get either </p>

          <p><code>RuntimeError: Expected all tensors to be on the same device, but
          found at least two devices, cuda:6 and cuda:0!</code></p>

          <p> or when I change the config.json part like this:</p>

          <pre><code>  "output_router_logits": false,

          </code></pre>

          <p>(as hinted by <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5">https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5</a>
          )</p>

          <p>I get :</p>

          <pre><code>RuntimeError: !grad_accumulator_.expired() INTERNAL ASSERT FAILED
          at "../torch/csrc/autograd/saved_variable.cpp":226, please report a bug
          to PyTorch. No grad accumulator for a saved leaf

          </code></pre>

          <p>Any hints?</p>

          <p>No accelerate, just trying to run the training straight through python.</p>

          <p>It worked with 2.5. I diffed 2.5 and 2.7 config.json and output_router_logits
          (and transformers version) is the only difference.</p>

          '
        raw: "I am trying to fine tune with axolotl (using axolotl's docker), but\
          \ I get either \r\n\r\n```RuntimeError: Expected all tensors to be on the\
          \ same device, but found at least two devices, cuda:6 and cuda:0!```\r\n\
          \r\n or when I change the config.json part like this:\r\n\r\n```\r\n  \"\
          output_router_logits\": false,\r\n```\r\n\r\n(as hinted by https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5\
          \ )\r\n\r\nI get :\r\n\r\n```\r\nRuntimeError: !grad_accumulator_.expired()\
          \ INTERNAL ASSERT FAILED at \"../torch/csrc/autograd/saved_variable.cpp\"\
          :226, please report a bug to PyTorch. No grad accumulator for a saved leaf\r\
          \n```\r\n\r\nAny hints?\r\n\r\nNo accelerate, just trying to run the training\
          \ straight through python.\r\n\r\nIt worked with 2.5. I diffed 2.5 and 2.7\
          \ config.json and output_router_logits (and transformers version) is the\
          \ only difference."
        updatedAt: '2024-01-04T14:56:29.795Z'
      numEdits: 0
      reactions: []
    id: 6596c71de3181789414bd82b
    type: comment
  author: joorei
  content: "I am trying to fine tune with axolotl (using axolotl's docker), but I\
    \ get either \r\n\r\n```RuntimeError: Expected all tensors to be on the same device,\
    \ but found at least two devices, cuda:6 and cuda:0!```\r\n\r\n or when I change\
    \ the config.json part like this:\r\n\r\n```\r\n  \"output_router_logits\": false,\r\
    \n```\r\n\r\n(as hinted by https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5\
    \ )\r\n\r\nI get :\r\n\r\n```\r\nRuntimeError: !grad_accumulator_.expired() INTERNAL\
    \ ASSERT FAILED at \"../torch/csrc/autograd/saved_variable.cpp\":226, please report\
    \ a bug to PyTorch. No grad accumulator for a saved leaf\r\n```\r\n\r\nAny hints?\r\
    \n\r\nNo accelerate, just trying to run the training straight through python.\r\
    \n\r\nIt worked with 2.5. I diffed 2.5 and 2.7 config.json and output_router_logits\
    \ (and transformers version) is the only difference."
  created_at: 2024-01-04 14:56:29+00:00
  edited: false
  hidden: false
  id: 6596c71de3181789414bd82b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: cognitivecomputations/dolphin-2.7-mixtral-8x7b
repo_type: model
status: open
target_branch: null
title: fine tuning with axolotl not working
