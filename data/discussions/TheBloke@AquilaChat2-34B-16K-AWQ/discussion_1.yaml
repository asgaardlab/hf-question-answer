!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wei54
conflicting_files: null
created_at: 2023-10-31 10:58:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c6a35d6285a829a88f551d1c90667eb.svg
      fullname: edion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wei54
      type: user
    createdAt: '2023-10-31T11:58:31.000Z'
    data:
      edited: false
      editors:
      - wei54
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7113398313522339
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c6a35d6285a829a88f551d1c90667eb.svg
          fullname: edion
          isHf: false
          isPro: false
          name: wei54
          type: user
        html: '<h3 id="i-followed-the-vllm-documentation-and-this-code-exactly-as-i-tried-to-start-aquilachat2-34b-16k-awq-but-it-returned-an-error">I
          followed the vllm documentation and this code exactly as I tried to start
          AquilaChat2-34B-16K-AWQ, but it returned an error.</h3>

          <p>from vllm import LLM, SamplingParams</p>

          <p>prompts = [<br>    "Tell me about AI",<br>    "Write a story about llamas",<br>    "What
          is 291 - 150?",<br>    "How much wood would a woodchuck chuck if a woodchuck
          could chuck wood?",<br>]<br>prompt_template=f''''''System: A chat between
          a curious human and an artificial intelligence assistant. The assistant
          gives helpful, detailed, and polite answers to the human''s questions.<br>Human:
          {prompt}<br>Assistant:<br>''''''</p>

          <p>prompts = [prompt_template.format(prompt=prompt) for prompt in prompts]</p>

          <p>sampling_params = SamplingParams(temperature=0.8, top_p=0.95)</p>

          <p>llm = LLM(model="TheBloke/AquilaChat2-34B-16K-AWQ", quantization="awq",
          dtype="auto")</p>

          <p>outputs = llm.generate(prompts, sampling_params)</p>

          <h1 id="print-the-outputs">Print the outputs.</h1>

          <p>for output in outputs:<br>    prompt = output.prompt<br>    generated_text
          = output.outputs[0].text<br>    print(f"Prompt: {prompt!r}, Generated text:
          {generated_text!r}")</p>

          <h3 id="how-can-i-deal-with-this">How can I deal with this</h3>

          <p>ImportError: cannot import name ''cuda_utils'' from partially initialized
          module ''vllm'' (most likely due to a circular import) (/home/ps/app/edison/vllm/vllm/<strong>init</strong>.py)</p>

          '
        raw: "### I followed the vllm documentation and this code exactly as I tried\
          \ to start AquilaChat2-34B-16K-AWQ, but it returned an error.\r\n\r\n\r\n\
          \r\nfrom vllm import LLM, SamplingParams\r\n\r\nprompts = [\r\n    \"Tell\
          \ me about AI\",\r\n    \"Write a story about llamas\",\r\n    \"What is\
          \ 291 - 150?\",\r\n    \"How much wood would a woodchuck chuck if a woodchuck\
          \ could chuck wood?\",\r\n]\r\nprompt_template=f'''System: A chat between\
          \ a curious human and an artificial intelligence assistant. The assistant\
          \ gives helpful, detailed, and polite answers to the human's questions.\r\
          \nHuman: {prompt}\r\nAssistant:\r\n'''\r\n\r\nprompts = [prompt_template.format(prompt=prompt)\
          \ for prompt in prompts]\r\n\r\nsampling_params = SamplingParams(temperature=0.8,\
          \ top_p=0.95)\r\n\r\nllm = LLM(model=\"TheBloke/AquilaChat2-34B-16K-AWQ\"\
          , quantization=\"awq\", dtype=\"auto\")\r\n\r\noutputs = llm.generate(prompts,\
          \ sampling_params)\r\n\r\n# Print the outputs.\r\nfor output in outputs:\r\
          \n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\
          \n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\"\
          )\r\n\r\n\r\n\r\n### How can I deal with this\r\nImportError: cannot import\
          \ name 'cuda_utils' from partially initialized module 'vllm' (most likely\
          \ due to a circular import) (/home/ps/app/edison/vllm/vllm/__init__.py)"
        updatedAt: '2023-10-31T11:58:31.182Z'
      numEdits: 0
      reactions: []
    id: 6540ebe749546d9536d325e0
    type: comment
  author: wei54
  content: "### I followed the vllm documentation and this code exactly as I tried\
    \ to start AquilaChat2-34B-16K-AWQ, but it returned an error.\r\n\r\n\r\n\r\n\
    from vllm import LLM, SamplingParams\r\n\r\nprompts = [\r\n    \"Tell me about\
    \ AI\",\r\n    \"Write a story about llamas\",\r\n    \"What is 291 - 150?\",\r\
    \n    \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\
    ,\r\n]\r\nprompt_template=f'''System: A chat between a curious human and an artificial\
    \ intelligence assistant. The assistant gives helpful, detailed, and polite answers\
    \ to the human's questions.\r\nHuman: {prompt}\r\nAssistant:\r\n'''\r\n\r\nprompts\
    \ = [prompt_template.format(prompt=prompt) for prompt in prompts]\r\n\r\nsampling_params\
    \ = SamplingParams(temperature=0.8, top_p=0.95)\r\n\r\nllm = LLM(model=\"TheBloke/AquilaChat2-34B-16K-AWQ\"\
    , quantization=\"awq\", dtype=\"auto\")\r\n\r\noutputs = llm.generate(prompts,\
    \ sampling_params)\r\n\r\n# Print the outputs.\r\nfor output in outputs:\r\n \
    \   prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n\
    \    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n\r\n\
    \r\n\r\n### How can I deal with this\r\nImportError: cannot import name 'cuda_utils'\
    \ from partially initialized module 'vllm' (most likely due to a circular import)\
    \ (/home/ps/app/edison/vllm/vllm/__init__.py)"
  created_at: 2023-10-31 10:58:31+00:00
  edited: false
  hidden: false
  id: 6540ebe749546d9536d325e0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/AquilaChat2-34B-16K-AWQ
repo_type: model
status: open
target_branch: null
title: 'ImportError: cannot import name ''cuda_utils'' from partially initialized
  module ''vllm'' (most likely due to a circular import)'
