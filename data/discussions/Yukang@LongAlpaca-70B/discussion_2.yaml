!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MB7977
conflicting_files: null
created_at: 2023-10-17 16:42:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
      fullname: MB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: MB7977
      type: user
    createdAt: '2023-10-17T17:42:57.000Z'
    data:
      edited: true
      editors:
      - MB7977
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9640947580337524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
          fullname: MB
          isHf: false
          isPro: true
          name: MB7977
          type: user
        html: '<p>Thank you for your wonderful work. Unfortunately, on Linux, even
          with FA2, the 128g GPTQ version of this model cannot be loaded at 32K context
          with 2x3090s. Are there any plans to train a 16K version that would be useable
          for a broader audience? Truncating max_seq_length to 16K on load seems to
          degrade performance. I''m going to quantize in EXL2 format so that I can
          load at 32K, but it will mean a very low bit-rate. </p>

          '
        raw: 'Thank you for your wonderful work. Unfortunately, on Linux, even with
          FA2, the 128g GPTQ version of this model cannot be loaded at 32K context
          with 2x3090s. Are there any plans to train a 16K version that would be useable
          for a broader audience? Truncating max_seq_length to 16K on load seems to
          degrade performance. I''m going to quantize in EXL2 format so that I can
          load at 32K, but it will mean a very low bit-rate. '
        updatedAt: '2023-10-17T17:46:37.144Z'
      numEdits: 3
      reactions: []
    id: 652ec7a189e73c5e69b38fa1
    type: comment
  author: MB7977
  content: 'Thank you for your wonderful work. Unfortunately, on Linux, even with
    FA2, the 128g GPTQ version of this model cannot be loaded at 32K context with
    2x3090s. Are there any plans to train a 16K version that would be useable for
    a broader audience? Truncating max_seq_length to 16K on load seems to degrade
    performance. I''m going to quantize in EXL2 format so that I can load at 32K,
    but it will mean a very low bit-rate. '
  created_at: 2023-10-17 16:42:57+00:00
  edited: true
  hidden: false
  id: 652ec7a189e73c5e69b38fa1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-10-17T17:47:25.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7940784692764282
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>If you use rope scaling = 8 and max_seq_len = 16K it should perform
          like a 16K model (make sure you don''t use rope scaling = 4). It flat out
          beats any 16K fine-tune I''ve made on raw perplexity at 16K. Maybe with
          a lot more training at rope scaling = 4 an exclusive 16K model might do
          better? But I don''t think that''s worth that much - the PPL drops monotonically
          all the way to 32K at rope scaling = 8.</p>

          '
        raw: If you use rope scaling = 8 and max_seq_len = 16K it should perform like
          a 16K model (make sure you don't use rope scaling = 4). It flat out beats
          any 16K fine-tune I've made on raw perplexity at 16K. Maybe with a lot more
          training at rope scaling = 4 an exclusive 16K model might do better? But
          I don't think that's worth that much - the PPL drops monotonically all the
          way to 32K at rope scaling = 8.
        updatedAt: '2023-10-17T17:50:26.773Z'
      numEdits: 3
      reactions: []
    id: 652ec8ad89e73c5e69b3d4ba
    type: comment
  author: grimulkan
  content: If you use rope scaling = 8 and max_seq_len = 16K it should perform like
    a 16K model (make sure you don't use rope scaling = 4). It flat out beats any
    16K fine-tune I've made on raw perplexity at 16K. Maybe with a lot more training
    at rope scaling = 4 an exclusive 16K model might do better? But I don't think
    that's worth that much - the PPL drops monotonically all the way to 32K at rope
    scaling = 8.
  created_at: 2023-10-17 16:47:25+00:00
  edited: true
  hidden: false
  id: 652ec8ad89e73c5e69b3d4ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
      fullname: MB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: MB7977
      type: user
    createdAt: '2023-10-17T18:07:53.000Z'
    data:
      edited: false
      editors:
      - MB7977
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9956162571907043
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
          fullname: MB
          isHf: false
          isPro: true
          name: MB7977
          type: user
        html: '<p>Interesting, thank you. I''ll give that a go. I was trying 16K at
          a scaling factor of 4. </p>

          '
        raw: 'Interesting, thank you. I''ll give that a go. I was trying 16K at a
          scaling factor of 4. '
        updatedAt: '2023-10-17T18:07:53.680Z'
      numEdits: 0
      reactions: []
    id: 652ecd791ad13fee8c267d7c
    type: comment
  author: MB7977
  content: 'Interesting, thank you. I''ll give that a go. I was trying 16K at a scaling
    factor of 4. '
  created_at: 2023-10-17 17:07:53+00:00
  edited: false
  hidden: false
  id: 652ecd791ad13fee8c267d7c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Yukang/LongAlpaca-70B
repo_type: model
status: open
target_branch: null
title: Any chance of a 16K model?
