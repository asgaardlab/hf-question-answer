!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ransom
conflicting_files: null
created_at: 2023-10-19 19:02:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/39851163dc31c98e5b2fef1b9fddf632.svg
      fullname: Malando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ransom
      type: user
    createdAt: '2023-10-19T20:02:49.000Z'
    data:
      edited: false
      editors:
      - Ransom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6996456384658813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/39851163dc31c98e5b2fef1b9fddf632.svg
          fullname: Malando
          isHf: false
          isPro: false
          name: Ransom
          type: user
        html: '<p>Currently, Llama.cpp server endpoint does not support   the rope-scale
          parameter, but it does support:<br>   printf("  --rope-freq-base N        RoPE
          base frequency (default: loaded from model)\n");<br>    printf("  --rope-freq-scale
          N       RoPE frequency scaling factor (default: loaded from model)\n");</p>

          <p>Could you provide insight into what the best parameters for this model
          would be?</p>

          <p>Thanks so much!</p>

          '
        raw: "Currently, Llama.cpp server endpoint does not support   the rope-scale\
          \ parameter, but it does support:\r\n   printf(\"  --rope-freq-base N  \
          \      RoPE base frequency (default: loaded from model)\\n\");\r\n    printf(\"\
          \  --rope-freq-scale N       RoPE frequency scaling factor (default: loaded\
          \ from model)\\n\");\r\n\r\nCould you provide insight into what the best\
          \ parameters for this model would be?\r\n\r\nThanks so much!"
        updatedAt: '2023-10-19T20:02:49.078Z'
      numEdits: 0
      reactions: []
    id: 65318b694d8b881ca329cefe
    type: comment
  author: Ransom
  content: "Currently, Llama.cpp server endpoint does not support   the rope-scale\
    \ parameter, but it does support:\r\n   printf(\"  --rope-freq-base N        RoPE\
    \ base frequency (default: loaded from model)\\n\");\r\n    printf(\"  --rope-freq-scale\
    \ N       RoPE frequency scaling factor (default: loaded from model)\\n\");\r\n\
    \r\nCould you provide insight into what the best parameters for this model would\
    \ be?\r\n\r\nThanks so much!"
  created_at: 2023-10-19 19:02:49+00:00
  edited: false
  hidden: false
  id: 65318b694d8b881ca329cefe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Yukang/LongAlpaca-70B
repo_type: model
status: open
target_branch: null
title: Llama.cpp server question
