!!python/object:huggingface_hub.community.DiscussionWithDetails
author: marcelcramer
conflicting_files: null
created_at: 2023-06-14 09:09:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-06-14T10:09:57.000Z'
    data:
      edited: false
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8610624074935913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: '<p>Dear aari1995,</p>

          <p>thank you very much for your model! The embeddings for german text work
          great. Unfortunately i am not able to load the model into my script. i used
          the following code:</p>

          <p>from sentence_transformers import SentenceTransformer<br>model = SentenceTransformer("aari1995/German_Semantic_STS_V2")</p>

          <p>I get the following message:</p>

          <p>"No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/aari1995_German_Semantic_STS_V2.
          Creating a new one with MEAN pooling."</p>

          <p>Do you have any ideas on on how to access your model?</p>

          <p>Thank you!<br>Marcel</p>

          '
        raw: "Dear aari1995,\r\n\r\nthank you very much for your model! The embeddings\
          \ for german text work great. Unfortunately i am not able to load the model\
          \ into my script. i used the following code:\r\n\r\nfrom sentence_transformers\
          \ import SentenceTransformer\r\nmodel = SentenceTransformer(\"aari1995/German_Semantic_STS_V2\"\
          )\r\n\r\nI get the following message:\r\n\r\n\"No sentence-transformers\
          \ model found with name /root/.cache/torch/sentence_transformers/aari1995_German_Semantic_STS_V2.\
          \ Creating a new one with MEAN pooling.\"\r\n\r\nDo you have any ideas on\
          \ on how to access your model?\r\n\r\nThank you!\r\nMarcel"
        updatedAt: '2023-06-14T10:09:57.377Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - andreP
    id: 648991f5639f0780c6ab14a8
    type: comment
  author: marcelcramer
  content: "Dear aari1995,\r\n\r\nthank you very much for your model! The embeddings\
    \ for german text work great. Unfortunately i am not able to load the model into\
    \ my script. i used the following code:\r\n\r\nfrom sentence_transformers import\
    \ SentenceTransformer\r\nmodel = SentenceTransformer(\"aari1995/German_Semantic_STS_V2\"\
    )\r\n\r\nI get the following message:\r\n\r\n\"No sentence-transformers model\
    \ found with name /root/.cache/torch/sentence_transformers/aari1995_German_Semantic_STS_V2.\
    \ Creating a new one with MEAN pooling.\"\r\n\r\nDo you have any ideas on on how\
    \ to access your model?\r\n\r\nThank you!\r\nMarcel"
  created_at: 2023-06-14 09:09:57+00:00
  edited: false
  hidden: false
  id: 648991f5639f0780c6ab14a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-06-14T10:48:13.000Z'
    data:
      edited: true
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6158079504966736
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: "<p>Edit:<br>If i use the tokenizer, i wont get any notification about\
          \ the missing model:</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(\"\
          aari1995/German_Semantic_STS_V2\")<br>encoded_input = tokenizer(sentences,\
          \ padding=True, truncation=True, return_tensors='pt')<br>tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][6])</p>\n\
          <p>print(tokens)</p>\n<p>That is why is said the embeddings are great, because\
          \ the tokens seem to be pretty good for my UC.</p>\n<p>I saw this post:\
          \ <a rel=\"nofollow\" href=\"https://github.com/UKPLab/sentence-transformers/issues/613\"\
          >https://github.com/UKPLab/sentence-transformers/issues/613</a> where <span\
          \ data-props=\"{&quot;user&quot;:&quot;nreimers&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nreimers\">@<span class=\"\
          underline\">nreimers</span></a></span>\n\n\t</span></span> said the following:\
          \ \"The warning is expected and can be ignored. The model is not hosted\
          \ by us (hence the 404 error), instead it is hosted at huggingface model\
          \ repository.<br>You can load it and use it as described.\" </p>\n<p>Does\
          \ this also apply to your model?</p>\n<p>Thank you!</p>\n"
        raw: "Edit:\nIf i use the tokenizer, i wont get any notification about the\
          \ missing model:\n\ntokenizer = AutoTokenizer.from_pretrained(\"aari1995/German_Semantic_STS_V2\"\
          )\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\
          tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][6])\n\
          \nprint(tokens)\n\nThat is why is said the embeddings are great, because\
          \ the tokens seem to be pretty good for my UC.\n\nI saw this post: https://github.com/UKPLab/sentence-transformers/issues/613\
          \ where @nreimers said the following: \"The warning is expected and can\
          \ be ignored. The model is not hosted by us (hence the 404 error), instead\
          \ it is hosted at huggingface model repository.\nYou can load it and use\
          \ it as described.\" \n\nDoes this also apply to your model?\n\nThank you!"
        updatedAt: '2023-06-14T10:50:52.470Z'
      numEdits: 1
      reactions: []
    id: 64899aed0ec897cfe5838444
    type: comment
  author: marcelcramer
  content: "Edit:\nIf i use the tokenizer, i wont get any notification about the missing\
    \ model:\n\ntokenizer = AutoTokenizer.from_pretrained(\"aari1995/German_Semantic_STS_V2\"\
    )\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\
    tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][6])\n\nprint(tokens)\n\
    \nThat is why is said the embeddings are great, because the tokens seem to be\
    \ pretty good for my UC.\n\nI saw this post: https://github.com/UKPLab/sentence-transformers/issues/613\
    \ where @nreimers said the following: \"The warning is expected and can be ignored.\
    \ The model is not hosted by us (hence the 404 error), instead it is hosted at\
    \ huggingface model repository.\nYou can load it and use it as described.\" \n\
    \nDoes this also apply to your model?\n\nThank you!"
  created_at: 2023-06-14 09:48:13+00:00
  edited: true
  hidden: false
  id: 64899aed0ec897cfe5838444
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2023-06-14T14:40:35.000Z'
    data:
      edited: false
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8917196393013
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>Hi,</p>\n<p>have the same problem. The model works nonetheless,\
          \ the loader assumes standard settings, but the typical save structure with\
          \ modules.json, 1_Pooling etc. is missing.<br>Also both variants are downloaded,\
          \ safetensors and pytorch.bin - doubling the download size and local storage\
          \ from 1.3 GB to 2.6 GB.<br>Great model, BTW! Would be nice to fix this.<br>From\
          \ my tests, It also works for English, doesn't it? If yes, plz add a label\
          \ on HF.</p>\n<p>Best regards, Andr\xE9</p>\n"
        raw: "Hi,\n\nhave the same problem. The model works nonetheless, the loader\
          \ assumes standard settings, but the typical save structure with modules.json,\
          \ 1_Pooling etc. is missing.\nAlso both variants are downloaded, safetensors\
          \ and pytorch.bin - doubling the download size and local storage from 1.3\
          \ GB to 2.6 GB.\nGreat model, BTW! Would be nice to fix this.\nFrom my tests,\
          \ It also works for English, doesn't it? If yes, plz add a label on HF.\n\
          \nBest regards, Andr\xE9"
        updatedAt: '2023-06-14T14:40:35.183Z'
      numEdits: 0
      reactions: []
    id: 6489d163983f3fbb6c152c4a
    type: comment
  author: andreP
  content: "Hi,\n\nhave the same problem. The model works nonetheless, the loader\
    \ assumes standard settings, but the typical save structure with modules.json,\
    \ 1_Pooling etc. is missing.\nAlso both variants are downloaded, safetensors and\
    \ pytorch.bin - doubling the download size and local storage from 1.3 GB to 2.6\
    \ GB.\nGreat model, BTW! Would be nice to fix this.\nFrom my tests, It also works\
    \ for English, doesn't it? If yes, plz add a label on HF.\n\nBest regards, Andr\xE9"
  created_at: 2023-06-14 13:40:35+00:00
  edited: false
  hidden: false
  id: 6489d163983f3fbb6c152c4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f3801ab7e583543386217ac/4xMdDV1gws7nxCJrU321H.jpeg?w=200&h=200&f=face
      fullname: Aaron Chibb
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: aari1995
      type: user
    createdAt: '2023-06-18T19:11:35.000Z'
    data:
      edited: false
      editors:
      - aari1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9567255973815918
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f3801ab7e583543386217ac/4xMdDV1gws7nxCJrU321H.jpeg?w=200&h=200&f=face
          fullname: Aaron Chibb
          isHf: false
          isPro: false
          name: aari1995
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;marcelcramer&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/marcelcramer\"\
          >@<span class=\"underline\">marcelcramer</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;andreP&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/andreP\">@<span class=\"\
          underline\">andreP</span></a></span>\n\n\t</span></span> thanks for the\
          \ nice Feedback.</p>\n<p>Mean pooling:<br>The warning about the mean pooling\
          \ is expected and it does not harm the models performance in any way so\
          \ don't worry. Exactly how Nils said.</p>\n<p>Safetensors:<br>Good Point,\
          \ I will look into this after my vacation. Or maybe <span data-props=\"\
          {&quot;user&quot;:&quot;patrickvonplaten&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/patrickvonplaten\">@<span class=\"\
          underline\">patrickvonplaten</span></a></span>\n\n\t</span></span> has an\
          \ idea?</p>\n<p>English:<br>Indeed it works with English as well, however\
          \ this is rather a positive side effect of the base model being trained\
          \ on a massive chunk of the German Part of the internet where there is very\
          \ likely some English data mixed in.</p>\n<p>All the best<br>Aaron</p>\n"
        raw: "Hi @marcelcramer and @andreP thanks for the nice Feedback.\n\nMean pooling:\n\
          The warning about the mean pooling is expected and it does not harm the\
          \ models performance in any way so don't worry. Exactly how Nils said.\n\
          \nSafetensors:\nGood Point, I will look into this after my vacation. Or\
          \ maybe @patrickvonplaten has an idea?\n\nEnglish: \nIndeed it works with\
          \ English as well, however this is rather a positive side effect of the\
          \ base model being trained on a massive chunk of the German Part of the\
          \ internet where there is very likely some English data mixed in.\n\nAll\
          \ the best\nAaron\n\n"
        updatedAt: '2023-06-18T19:11:35.729Z'
      numEdits: 0
      reactions: []
    id: 648f56e78b579a84b7a5b5ed
    type: comment
  author: aari1995
  content: "Hi @marcelcramer and @andreP thanks for the nice Feedback.\n\nMean pooling:\n\
    The warning about the mean pooling is expected and it does not harm the models\
    \ performance in any way so don't worry. Exactly how Nils said.\n\nSafetensors:\n\
    Good Point, I will look into this after my vacation. Or maybe @patrickvonplaten\
    \ has an idea?\n\nEnglish: \nIndeed it works with English as well, however this\
    \ is rather a positive side effect of the base model being trained on a massive\
    \ chunk of the German Part of the internet where there is very likely some English\
    \ data mixed in.\n\nAll the best\nAaron\n\n"
  created_at: 2023-06-18 18:11:35+00:00
  edited: false
  hidden: false
  id: 648f56e78b579a84b7a5b5ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-06-19T06:07:50.000Z'
    data:
      edited: false
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7473713755607605
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: "<p>Dear <span data-props=\"{&quot;user&quot;:&quot;aari1995&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aari1995\"\
          >@<span class=\"underline\">aari1995</span></a></span>\n\n\t</span></span>\
          \ thank you for your reply!</p>\n<p>Have a nice vacation</p>\n<p>Best regards,<br>Marcel</p>\n"
        raw: 'Dear @aari1995 thank you for your reply!


          Have a nice vacation


          Best regards,

          Marcel'
        updatedAt: '2023-06-19T06:07:50.051Z'
      numEdits: 0
      reactions: []
    id: 648ff0b6cbceae8e1bdadcc1
    type: comment
  author: marcelcramer
  content: 'Dear @aari1995 thank you for your reply!


    Have a nice vacation


    Best regards,

    Marcel'
  created_at: 2023-06-19 05:07:50+00:00
  edited: false
  hidden: false
  id: 648ff0b6cbceae8e1bdadcc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-06-26T10:01:54.000Z'
    data:
      edited: true
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8076233267784119
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aari1995&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aari1995\">@<span class=\"\
          underline\">aari1995</span></a></span>\n\n\t</span></span><br>I have another\
          \ question:  By using this code \"embeddings = model.encode(texts)\",  i\
          \ am getting 1024 Embeddings for each of my texts.  Why don\xB4t i get 512?</p>\n\
          <p>What is the difference between (0): Transformer and (1): Pooling?</p>\n\
          <p>Thank you!<br>Marcel</p>\n"
        raw: "@aari1995 \nI have another question:  By using this code \"embeddings\
          \ = model.encode(texts)\",  i am getting 1024 Embeddings for each of my\
          \ texts.  Why don\xB4t i get 512?\n\nWhat is the difference between (0):\
          \ Transformer and (1): Pooling?\n\nThank you!\nMarcel"
        updatedAt: '2023-06-26T10:02:29.432Z'
      numEdits: 1
      reactions: []
    id: 64996212eaae98ee9859cfb9
    type: comment
  author: marcelcramer
  content: "@aari1995 \nI have another question:  By using this code \"embeddings\
    \ = model.encode(texts)\",  i am getting 1024 Embeddings for each of my texts.\
    \  Why don\xB4t i get 512?\n\nWhat is the difference between (0): Transformer\
    \ and (1): Pooling?\n\nThank you!\nMarcel"
  created_at: 2023-06-26 09:01:54+00:00
  edited: true
  hidden: false
  id: 64996212eaae98ee9859cfb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2023-06-26T11:37:48.000Z'
    data:
      edited: false
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8195820450782776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: '<p>It''s an Embedding Model, which creates 1024-dimensional Embeddings
          per ''text'' (sentence / paragraph / document).<br>In this case: A single
          Embedding vector consists out of 1024 floats.<br>Look into the config.json
          - Model Type is Bert with hidden dimensions = 1024.<br>There are other embedding
          models which have other sizes, like 384, 786 or 1536 (OpenAI). It'' just
          how the parameters of the model where chosen before training.</p>

          <p>Internally Bert (the Transformer) creates such a hidden Embeddings for
          each token (word pieces) - so you would get lots of Embedding vectors for
          your text und not one Embedding Vector for the whole text.<br>You just get
          an averaged embedding over all token embeddings - this is the pooling operation.</p>

          '
        raw: 'It''s an Embedding Model, which creates 1024-dimensional Embeddings
          per ''text'' (sentence / paragraph / document).

          In this case: A single Embedding vector consists out of 1024 floats.

          Look into the config.json - Model Type is Bert with hidden dimensions =
          1024.

          There are other embedding models which have other sizes, like 384, 786 or
          1536 (OpenAI). It'' just how the parameters of the model where chosen before
          training.


          Internally Bert (the Transformer) creates such a hidden Embeddings for each
          token (word pieces) - so you would get lots of Embedding vectors for your
          text und not one Embedding Vector for the whole text.

          You just get an averaged embedding over all token embeddings - this is the
          pooling operation.'
        updatedAt: '2023-06-26T11:37:48.847Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - aari1995
        - marcelcramer
    id: 6499788ce4ab9a48eaec31fe
    type: comment
  author: andreP
  content: 'It''s an Embedding Model, which creates 1024-dimensional Embeddings per
    ''text'' (sentence / paragraph / document).

    In this case: A single Embedding vector consists out of 1024 floats.

    Look into the config.json - Model Type is Bert with hidden dimensions = 1024.

    There are other embedding models which have other sizes, like 384, 786 or 1536
    (OpenAI). It'' just how the parameters of the model where chosen before training.


    Internally Bert (the Transformer) creates such a hidden Embeddings for each token
    (word pieces) - so you would get lots of Embedding vectors for your text und not
    one Embedding Vector for the whole text.

    You just get an averaged embedding over all token embeddings - this is the pooling
    operation.'
  created_at: 2023-06-26 10:37:48+00:00
  edited: false
  hidden: false
  id: 6499788ce4ab9a48eaec31fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-06-26T12:46:46.000Z'
    data:
      edited: false
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8807864189147949
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: '<p>Thank you for the quick reply.<br>Do i understand it correctly:
          The model creates 512 Tokens and a 1024-dimensional vector? I always thought
          that one token is assigned to one dimension of the resulting embedding vector.
          Or am i confusing something here? Why is the dimension size double the size
          of the tokens?</p>

          '
        raw: 'Thank you for the quick reply.

          Do i understand it correctly: The model creates 512 Tokens and a 1024-dimensional
          vector? I always thought that one token is assigned to one dimension of
          the resulting embedding vector. Or am i confusing something here? Why is
          the dimension size double the size of the tokens?

          '
        updatedAt: '2023-06-26T12:46:46.998Z'
      numEdits: 0
      reactions: []
    id: 649988b6fb7409c73689d352
    type: comment
  author: marcelcramer
  content: 'Thank you for the quick reply.

    Do i understand it correctly: The model creates 512 Tokens and a 1024-dimensional
    vector? I always thought that one token is assigned to one dimension of the resulting
    embedding vector. Or am i confusing something here? Why is the dimension size
    double the size of the tokens?

    '
  created_at: 2023-06-26 11:46:46+00:00
  edited: false
  hidden: false
  id: 649988b6fb7409c73689d352
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2023-06-26T14:31:44.000Z'
    data:
      edited: false
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.799884021282196
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>The model input is a sentence like 'That is an easy test'<br>This\
          \ will be tokenized into tokens like 'That', ' is', ' an',  ' ea', 'sy',\
          \ ' te', 'st' (just free example, no real tokenization - here we get 7 tokens)\xB4\
          <br>This tokens go as input to the model.<br>This model will create intern\
          \ hidden attention vectors 7 x 1024.<br>This will be averaged (mean) to\
          \ 1 vector of 1024 dimensions, that contain the semantical meaning of the\
          \ sentence.<br>It's rly just a quick / very high level explanation. There\
          \ are lots of online resources 4 tokenization and embeddings.</p>\n"
        raw: "The model input is a sentence like 'That is an easy test'\nThis will\
          \ be tokenized into tokens like 'That', ' is', ' an',  ' ea', 'sy', ' te',\
          \ 'st' (just free example, no real tokenization - here we get 7 tokens)\xB4\
          \nThis tokens go as input to the model.\nThis model will create intern hidden\
          \ attention vectors 7 x 1024.\nThis will be averaged (mean) to 1 vector\
          \ of 1024 dimensions, that contain the semantical meaning of the sentence.\n\
          It's rly just a quick / very high level explanation. There are lots of online\
          \ resources 4 tokenization and embeddings."
        updatedAt: '2023-06-26T14:31:44.801Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - marcelcramer
        - holnburger
    id: 6499a15045dffc70af5ecded
    type: comment
  author: andreP
  content: "The model input is a sentence like 'That is an easy test'\nThis will be\
    \ tokenized into tokens like 'That', ' is', ' an',  ' ea', 'sy', ' te', 'st' (just\
    \ free example, no real tokenization - here we get 7 tokens)\xB4\nThis tokens\
    \ go as input to the model.\nThis model will create intern hidden attention vectors\
    \ 7 x 1024.\nThis will be averaged (mean) to 1 vector of 1024 dimensions, that\
    \ contain the semantical meaning of the sentence.\nIt's rly just a quick / very\
    \ high level explanation. There are lots of online resources 4 tokenization and\
    \ embeddings."
  created_at: 2023-06-26 13:31:44+00:00
  edited: false
  hidden: false
  id: 6499a15045dffc70af5ecded
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-06-26T16:30:14.000Z'
    data:
      edited: false
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9920907020568848
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: '<p>Thank you very much for the insight!<br>Now it is more clear to
          me.</p>

          '
        raw: "Thank you very much for the insight! \nNow it is more clear to me."
        updatedAt: '2023-06-26T16:30:14.323Z'
      numEdits: 0
      reactions: []
    id: 6499bd16e46489bd8fc91b80
    type: comment
  author: marcelcramer
  content: "Thank you very much for the insight! \nNow it is more clear to me."
  created_at: 2023-06-26 15:30:14+00:00
  edited: false
  hidden: false
  id: 6499bd16e46489bd8fc91b80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: aari1995/German_Semantic_STS_V2
repo_type: model
status: open
target_branch: null
title: '"No sentence-transformers model found with name ..."'
