!!python/object:huggingface_hub.community.DiscussionWithDetails
author: marcelcramer
conflicting_files: null
created_at: 2023-07-12 08:12:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-07-12T09:12:11.000Z'
    data:
      edited: false
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8977919816970825
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: '<p>Dear aari1995,</p>

          <p>thanks again for the great model :)</p>

          <p>I would like to know if you have experience regarding the creation of
          embeddings for long text document. In my use case, i have multiple documents/texts
          where each document has a word length &gt; 500 words. This exceeds the 200-300
          word length recommendation for BERT models, regarding the  vectorization
          of the tokens. Therefore, my documents are being truncated heavely, resulting
          in a lot of loss of information. I need to find a way to create one Embedding
          vector for each document, without loosing information through the truncation.
          Unfortunately, reducing the length of the document by applying some pre-processing
          steps is not possible, since i already done that to a maximum. Using a summarization
          model/algorithm to summarize the document in a prior step into a suitable
          length  and then creating the Embeddings is also not an option for me. Therefore
          i am doing the following steps right now:</p>

          <ol>

          <li>converting the document into tokens</li>

          <li>splitting the tokens into chunks of 512 tokens</li>

          <li>vectorizing the chunks to get one Embedding vector per chunk (now i
          have multiple vectors per document)</li>

          <li>mean averaging the vectors into one vector</li>

          </ol>

          <p>Unfortunately, my averaged Embedding vector for each document is not
          a good representation of the documet`s content.  I know this, because my
          next step, clustering each Embedding vector using k-Means, has poorly results.<br>I
          guess the averaging of the vectors for each document is the problem, since
          there is probabely a huge loss of information.   </p>

          <p>Have you any experience with this topic? Could you recommend any further
          ways to create meaningful Embeddings for my (long) documents?</p>

          <p>Thank you and best regards,<br>Marcel</p>

          '
        raw: "Dear aari1995,\r\n\r\nthanks again for the great model :)\r\n\r\nI would\
          \ like to know if you have experience regarding the creation of embeddings\
          \ for long text document. In my use case, i have multiple documents/texts\
          \ where each document has a word length > 500 words. This exceeds the 200-300\
          \ word length recommendation for BERT models, regarding the  vectorization\
          \ of the tokens. Therefore, my documents are being truncated heavely, resulting\
          \ in a lot of loss of information. I need to find a way to create one Embedding\
          \ vector for each document, without loosing information through the truncation.\
          \ Unfortunately, reducing the length of the document by applying some pre-processing\
          \ steps is not possible, since i already done that to a maximum. Using a\
          \ summarization model/algorithm to summarize the document in a prior step\
          \ into a suitable length  and then creating the Embeddings is also not an\
          \ option for me. Therefore i am doing the following steps right now:\r\n\
          1. converting the document into tokens\r\n2. splitting the tokens into chunks\
          \ of 512 tokens\r\n3. vectorizing the chunks to get one Embedding vector\
          \ per chunk (now i have multiple vectors per document)\r\n3. mean averaging\
          \ the vectors into one vector \r\n\r\nUnfortunately, my averaged Embedding\
          \ vector for each document is not a good representation of the documet`s\
          \ content.  I know this, because my next step, clustering each Embedding\
          \ vector using k-Means, has poorly results.\r\nI guess the averaging of\
          \ the vectors for each document is the problem, since there is probabely\
          \ a huge loss of information.   \r\n\r\nHave you any experience with this\
          \ topic? Could you recommend any further ways to create meaningful Embeddings\
          \ for my (long) documents?\r\n\r\nThank you and best regards,\r\nMarcel"
        updatedAt: '2023-07-12T09:12:11.810Z'
      numEdits: 0
      reactions: []
    id: 64ae6e6be0d23bcf024c1a98
    type: comment
  author: marcelcramer
  content: "Dear aari1995,\r\n\r\nthanks again for the great model :)\r\n\r\nI would\
    \ like to know if you have experience regarding the creation of embeddings for\
    \ long text document. In my use case, i have multiple documents/texts where each\
    \ document has a word length > 500 words. This exceeds the 200-300 word length\
    \ recommendation for BERT models, regarding the  vectorization of the tokens.\
    \ Therefore, my documents are being truncated heavely, resulting in a lot of loss\
    \ of information. I need to find a way to create one Embedding vector for each\
    \ document, without loosing information through the truncation. Unfortunately,\
    \ reducing the length of the document by applying some pre-processing steps is\
    \ not possible, since i already done that to a maximum. Using a summarization\
    \ model/algorithm to summarize the document in a prior step into a suitable length\
    \  and then creating the Embeddings is also not an option for me. Therefore i\
    \ am doing the following steps right now:\r\n1. converting the document into tokens\r\
    \n2. splitting the tokens into chunks of 512 tokens\r\n3. vectorizing the chunks\
    \ to get one Embedding vector per chunk (now i have multiple vectors per document)\r\
    \n3. mean averaging the vectors into one vector \r\n\r\nUnfortunately, my averaged\
    \ Embedding vector for each document is not a good representation of the documet`s\
    \ content.  I know this, because my next step, clustering each Embedding vector\
    \ using k-Means, has poorly results.\r\nI guess the averaging of the vectors for\
    \ each document is the problem, since there is probabely a huge loss of information.\
    \   \r\n\r\nHave you any experience with this topic? Could you recommend any further\
    \ ways to create meaningful Embeddings for my (long) documents?\r\n\r\nThank you\
    \ and best regards,\r\nMarcel"
  created_at: 2023-07-12 08:12:11+00:00
  edited: false
  hidden: false
  id: 64ae6e6be0d23bcf024c1a98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
      fullname: Marcel Cramer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelcramer
      type: user
    createdAt: '2023-07-27T10:57:22.000Z'
    data:
      edited: false
      editors:
      - marcelcramer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9807669520378113
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5b5e5f41cadf89acc4e2e8c464cd2fe.svg
          fullname: Marcel Cramer
          isHf: false
          isPro: false
          name: marcelcramer
          type: user
        html: '<p>any thoughts on this topic would be very helpful :)</p>

          '
        raw: any thoughts on this topic would be very helpful :)
        updatedAt: '2023-07-27T10:57:22.555Z'
      numEdits: 0
      reactions: []
    id: 64c24d920bee4392c1a2f6df
    type: comment
  author: marcelcramer
  content: any thoughts on this topic would be very helpful :)
  created_at: 2023-07-27 09:57:22+00:00
  edited: false
  hidden: false
  id: 64c24d920bee4392c1a2f6df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: aari1995/German_Semantic_STS_V2
repo_type: model
status: open
target_branch: null
title: Vectorization of long text
