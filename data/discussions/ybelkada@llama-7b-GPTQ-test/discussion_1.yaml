!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RonanMcGovern
conflicting_files: null
created_at: 2023-08-14 14:58:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-14T15:58:50.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8494473695755005
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Thanks for putting this together, and the colab sheet.</p>\n<p>I\
          \ tried to merge and unload so I could push a full model, to hub, but I'm\
          \ getting this error:</p>\n<pre><code>Cannot merge LORA layers when the\
          \ model is gptq quantized\n</code></pre>\n<p>after trying:</p>\n<pre><code>model\
          \ = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\
          \ # must be auto, cannot be cpu\n\nfrom peft import PeftModel\n\n# load\
          \ PEFT model with new adapters\nmodel = PeftModel.from_pretrained(\n   \
          \ model,\n    adapter_model_name,\n)\n\nmodel = model.merge_and_unload()\
          \ # merge adapters with the base model.\n</code></pre>\n"
        raw: "Thanks for putting this together, and the colab sheet.\r\n\r\nI tried\
          \ to merge and unload so I could push a full model, to hub, but I'm getting\
          \ this error:\r\n```\r\nCannot merge LORA layers when the model is gptq\
          \ quantized\r\n```\r\nafter trying:\r\n```\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=\"auto\") # must be auto, cannot be cpu\r\n\r\nfrom peft import\
          \ PeftModel\r\n\r\n# load PEFT model with new adapters\r\nmodel = PeftModel.from_pretrained(\r\
          \n    model,\r\n    adapter_model_name,\r\n)\r\n\r\nmodel = model.merge_and_unload()\
          \ # merge adapters with the base model.\r\n```"
        updatedAt: '2023-08-14T15:58:50.120Z'
      numEdits: 0
      reactions: []
    id: 64da4f3a858f8a41c1efaaaa
    type: comment
  author: RonanMcGovern
  content: "Thanks for putting this together, and the colab sheet.\r\n\r\nI tried\
    \ to merge and unload so I could push a full model, to hub, but I'm getting this\
    \ error:\r\n```\r\nCannot merge LORA layers when the model is gptq quantized\r\
    \n```\r\nafter trying:\r\n```\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
    \ device_map=\"auto\") # must be auto, cannot be cpu\r\n\r\nfrom peft import PeftModel\r\
    \n\r\n# load PEFT model with new adapters\r\nmodel = PeftModel.from_pretrained(\r\
    \n    model,\r\n    adapter_model_name,\r\n)\r\n\r\nmodel = model.merge_and_unload()\
    \ # merge adapters with the base model.\r\n```"
  created_at: 2023-08-14 14:58:50+00:00
  edited: false
  hidden: false
  id: 64da4f3a858f8a41c1efaaaa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ybelkada/llama-7b-GPTQ-test
repo_type: model
status: open
target_branch: null
title: How was this PEFT model merged with the base?
