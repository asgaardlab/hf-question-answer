!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cozydive77
conflicting_files: null
created_at: 2023-08-11 09:32:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c83cb866fd7660e6f685257f6df4abd.svg
      fullname: Matthias
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cozydive77
      type: user
    createdAt: '2023-08-11T10:32:08.000Z'
    data:
      edited: false
      editors:
      - cozydive77
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8318377137184143
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c83cb866fd7660e6f685257f6df4abd.svg
          fullname: Matthias
          isHf: false
          isPro: false
          name: cozydive77
          type: user
        html: '<p>Hello,</p>

          <p>I hope this is the right place posting this, if not please excuse me
          I am pretty new here.<br>I have used the exact example python code from
          the model card to get started testig Alfred.  After downloading the model
          files I receive the following error running the script:</p>

          <p>Traceback (most recent call last):<br>  File "Alfred-40B-0723.py", line
          8, in <br>    pipeline = transformers.pipeline(<br>  File "C:\Program Files\Python311\Lib\site-packages\transformers\pipelines_<em>init</em>_.py",
          line 788, in pipeline<br>    framework, model = infer_framework_load_model(<br>  File
          "C:\Program Files\Python311\Lib\site-packages\transformers\pipelines\base.py",
          line 278, in infer_framework_load_model<br>    raise ValueError(f"Could
          not load model {model} with any of the following classes: {class_tuple}.")<br>ValueError:
          Could not load model lightonai/alfred-40b-0723 with any of the following
          classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class ''transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM''&gt;).</p>

          <p>This is with pyhton 3.11 on Windows. I had a look on a few other models
          from HF so far (e.g. whisper medium) which worked fine. Could someone please
          give me a hint how to solve this? I have googled the error, however, couldn''t
          find a solution so far. Hints I''ve found were related to pytorch wich is
          installed and updated. Is there a workaround without the AutoModelForCausalLM?
          Thank you very much for your support!</p>

          <p>Best regards,<br>Matthias</p>

          '
        raw: "Hello,\r\n\r\nI hope this is the right place posting this, if not please\
          \ excuse me I am pretty new here.\r\nI have used the exact example python\
          \ code from the model card to get started testig Alfred.  After downloading\
          \ the model files I receive the following error running the script:\r\n\r\
          \nTraceback (most recent call last):\r\n  File \"Alfred-40B-0723.py\", line\
          \ 8, in <module>\r\n    pipeline = transformers.pipeline(\r\n  File \"C:\\\
          Program Files\\Python311\\Lib\\site-packages\\transformers\\pipelines\\\
          __init__.py\", line 788, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
          \n  File \"C:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\\
          pipelines\\base.py\", line 278, in infer_framework_load_model\r\n    raise\
          \ ValueError(f\"Could not load model {model} with any of the following classes:\
          \ {class_tuple}.\")\r\nValueError: Could not load model lightonai/alfred-40b-0723\
          \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\r\
          \n\r\nThis is with pyhton 3.11 on Windows. I had a look on a few other models\
          \ from HF so far (e.g. whisper medium) which worked fine. Could someone\
          \ please give me a hint how to solve this? I have googled the error, however,\
          \ couldn't find a solution so far. Hints I've found were related to pytorch\
          \ wich is installed and updated. Is there a workaround without the AutoModelForCausalLM?\
          \ Thank you very much for your support!\r\n\r\nBest regards,\r\nMatthias"
        updatedAt: '2023-08-11T10:32:08.646Z'
      numEdits: 0
      reactions: []
    id: 64d60e2805b3495828b12171
    type: comment
  author: cozydive77
  content: "Hello,\r\n\r\nI hope this is the right place posting this, if not please\
    \ excuse me I am pretty new here.\r\nI have used the exact example python code\
    \ from the model card to get started testig Alfred.  After downloading the model\
    \ files I receive the following error running the script:\r\n\r\nTraceback (most\
    \ recent call last):\r\n  File \"Alfred-40B-0723.py\", line 8, in <module>\r\n\
    \    pipeline = transformers.pipeline(\r\n  File \"C:\\Program Files\\Python311\\\
    Lib\\site-packages\\transformers\\pipelines\\__init__.py\", line 788, in pipeline\r\
    \n    framework, model = infer_framework_load_model(\r\n  File \"C:\\Program Files\\\
    Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 278, in\
    \ infer_framework_load_model\r\n    raise ValueError(f\"Could not load model {model}\
    \ with any of the following classes: {class_tuple}.\")\r\nValueError: Could not\
    \ load model lightonai/alfred-40b-0723 with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\r\
    \n\r\nThis is with pyhton 3.11 on Windows. I had a look on a few other models\
    \ from HF so far (e.g. whisper medium) which worked fine. Could someone please\
    \ give me a hint how to solve this? I have googled the error, however, couldn't\
    \ find a solution so far. Hints I've found were related to pytorch wich is installed\
    \ and updated. Is there a workaround without the AutoModelForCausalLM? Thank you\
    \ very much for your support!\r\n\r\nBest regards,\r\nMatthias"
  created_at: 2023-08-11 09:32:08+00:00
  edited: false
  hidden: false
  id: 64d60e2805b3495828b12171
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: lightonai/alfred-40b-0723
repo_type: model
status: open
target_branch: null
title: Receiving error using the example from model card
