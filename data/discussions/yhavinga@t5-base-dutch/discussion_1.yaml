!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flexudy
conflicting_files: null
created_at: 2022-10-28 16:20:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598464413995-noauth.png?w=200&h=200&f=face
      fullname: Flexudy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flexudy
      type: user
    createdAt: '2022-10-28T17:20:47.000Z'
    data:
      edited: false
      editors:
      - flexudy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598464413995-noauth.png?w=200&h=200&f=face
          fullname: Flexudy
          isHf: false
          isPro: false
          name: flexudy
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;yhavinga&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yhavinga\"\
          >@<span class=\"underline\">yhavinga</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>I try to load the tokenizer on AWS lambda but I get this error.<br>module\
          \ initialization error: Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(848)\
          \ [model_proto-&gt;ParseFromArray(serialized.data(), serialized.size())]</p>\n\
          <p>Any idea?</p>\n<p>Locally it works but for some reason, not on lambda.<br>When\
          \ I upload a spiece.model file in the model folder (from another model just\
          \ to see if it works) then it works fine, but the predictions are garbage.</p>\n"
        raw: "Howdy @yhavinga \r\n\r\nI try to load the tokenizer on AWS lambda but\
          \ I get this error. \r\nmodule initialization error: Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(848)\
          \ [model_proto->ParseFromArray(serialized.data(), serialized.size())]\r\n\
          \r\nAny idea?\r\n\r\nLocally it works but for some reason, not on lambda.\r\
          \nWhen I upload a spiece.model file in the model folder (from another model\
          \ just to see if it works) then it works fine, but the predictions are garbage.\r\
          \n \r\n"
        updatedAt: '2022-10-28T17:20:47.098Z'
      numEdits: 0
      reactions: []
    id: 635c0f6f7a165601151c17be
    type: comment
  author: flexudy
  content: "Howdy @yhavinga \r\n\r\nI try to load the tokenizer on AWS lambda but\
    \ I get this error. \r\nmodule initialization error: Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(848)\
    \ [model_proto->ParseFromArray(serialized.data(), serialized.size())]\r\n\r\n\
    Any idea?\r\n\r\nLocally it works but for some reason, not on lambda.\r\nWhen\
    \ I upload a spiece.model file in the model folder (from another model just to\
    \ see if it works) then it works fine, but the predictions are garbage.\r\n \r\
    \n"
  created_at: 2022-10-28 16:20:47+00:00
  edited: false
  hidden: false
  id: 635c0f6f7a165601151c17be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
      fullname: Yeb Havinga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yhavinga
      type: user
    createdAt: '2022-10-28T19:41:19.000Z'
    data:
      edited: false
      editors:
      - yhavinga
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
          fullname: Yeb Havinga
          isHf: false
          isPro: false
          name: yhavinga
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;flexudy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/flexudy\"\
          >@<span class=\"underline\">flexudy</span></a></span>\n\n\t</span></span></p>\n\
          <p>Are you loading the tokenizer with AutoTokenizer.from_pretrained() ?\
          \ And is the <code>tokenizers</code> package recent?</p>\n<p>The (sentencepiece)\
          \ tokenizer of t5-base-dutch was created with HF tools instead of the 'official'\
          \ sentencepiece tokenizer. One difference is that the latter creates 'spiece.model',\
          \ which is absent from the tokenizers created by HF tools, that only create\
          \ <code>tokenizer.json</code>. A while ago I also got cryptic errors when\
          \ loading the HF-created tokenizers that worked without issues a few months\
          \ earlier. In the end I could solve these problems by either upgrading the\
          \ tokenizers package or downgrading if I was at the latest version. Lately\
          \ I haven't had any issues anymore, so I suspect recent releases <code>tokenizers</code>\
          \ are subjected to more rigorous integration tests.</p>\n"
        raw: 'Hey @flexudy


          Are you loading the tokenizer with AutoTokenizer.from_pretrained() ? And
          is the `tokenizers` package recent?


          The (sentencepiece) tokenizer of t5-base-dutch was created with HF tools
          instead of the ''official'' sentencepiece tokenizer. One difference is that
          the latter creates ''spiece.model'', which is absent from the tokenizers
          created by HF tools, that only create `tokenizer.json`. A while ago I also
          got cryptic errors when loading the HF-created tokenizers that worked without
          issues a few months earlier. In the end I could solve these problems by
          either upgrading the tokenizers package or downgrading if I was at the latest
          version. Lately I haven''t had any issues anymore, so I suspect recent releases
          `tokenizers` are subjected to more rigorous integration tests.'
        updatedAt: '2022-10-28T19:41:19.956Z'
      numEdits: 0
      reactions: []
    id: 635c305f3cb827d58118c502
    type: comment
  author: yhavinga
  content: 'Hey @flexudy


    Are you loading the tokenizer with AutoTokenizer.from_pretrained() ? And is the
    `tokenizers` package recent?


    The (sentencepiece) tokenizer of t5-base-dutch was created with HF tools instead
    of the ''official'' sentencepiece tokenizer. One difference is that the latter
    creates ''spiece.model'', which is absent from the tokenizers created by HF tools,
    that only create `tokenizer.json`. A while ago I also got cryptic errors when
    loading the HF-created tokenizers that worked without issues a few months earlier.
    In the end I could solve these problems by either upgrading the tokenizers package
    or downgrading if I was at the latest version. Lately I haven''t had any issues
    anymore, so I suspect recent releases `tokenizers` are subjected to more rigorous
    integration tests.'
  created_at: 2022-10-28 18:41:19+00:00
  edited: false
  hidden: false
  id: 635c305f3cb827d58118c502
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598464413995-noauth.png?w=200&h=200&f=face
      fullname: Flexudy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flexudy
      type: user
    createdAt: '2022-10-28T19:46:11.000Z'
    data:
      edited: false
      editors:
      - flexudy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598464413995-noauth.png?w=200&h=200&f=face
          fullname: Flexudy
          isHf: false
          isPro: false
          name: flexudy
          type: user
        html: "<p>hey <span data-props=\"{&quot;user&quot;:&quot;yhavinga&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yhavinga\"\
          >@<span class=\"underline\">yhavinga</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Thanks for the quick response.<br>I am loading the tokenizer\
          \ using T5TokenizerFast. I currently use transformers 4.18.0. Also tried\
          \ everything between 4.9 and 4.23.<br>On MacOS, everything is fine. But\
          \ not on AWS lambda.</p>\n<p>I thought you might have some clues about why\
          \ this error would happen.</p>\n"
        raw: "hey @yhavinga \n\nThanks for the quick response.\nI am loading the tokenizer\
          \ using T5TokenizerFast. I currently use transformers 4.18.0. Also tried\
          \ everything between 4.9 and 4.23.\nOn MacOS, everything is fine. But not\
          \ on AWS lambda.\n\nI thought you might have some clues about why this error\
          \ would happen."
        updatedAt: '2022-10-28T19:46:11.600Z'
      numEdits: 0
      reactions: []
    id: 635c3183bb96f709230ba5f4
    type: comment
  author: flexudy
  content: "hey @yhavinga \n\nThanks for the quick response.\nI am loading the tokenizer\
    \ using T5TokenizerFast. I currently use transformers 4.18.0. Also tried everything\
    \ between 4.9 and 4.23.\nOn MacOS, everything is fine. But not on AWS lambda.\n\
    \nI thought you might have some clues about why this error would happen."
  created_at: 2022-10-28 18:46:11+00:00
  edited: false
  hidden: false
  id: 635c3183bb96f709230ba5f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
      fullname: Yeb Havinga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yhavinga
      type: user
    createdAt: '2022-10-28T20:00:25.000Z'
    data:
      edited: false
      editors:
      - yhavinga
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
          fullname: Yeb Havinga
          isHf: false
          isPro: false
          name: yhavinga
          type: user
        html: '<p>What does <code>pip freeze | grep tokenizers</code> say? I just
          checked in two environments and it works with 0.12.1 and 0.13.1.<br>Also,
          are there perhaps lingering tokenizer files in the working directory of
          the script? I had a bug once that the tokenizer load would load from the
          current directory in stead of the passed model id on the HF hub.</p>

          '
        raw: 'What does `pip freeze | grep tokenizers` say? I just checked in two
          environments and it works with 0.12.1 and 0.13.1.

          Also, are there perhaps lingering tokenizer files in the working directory
          of the script? I had a bug once that the tokenizer load would load from
          the current directory in stead of the passed model id on the HF hub.'
        updatedAt: '2022-10-28T20:00:25.671Z'
      numEdits: 0
      reactions: []
    id: 635c34d9e3737b9e4e264f75
    type: comment
  author: yhavinga
  content: 'What does `pip freeze | grep tokenizers` say? I just checked in two environments
    and it works with 0.12.1 and 0.13.1.

    Also, are there perhaps lingering tokenizer files in the working directory of
    the script? I had a bug once that the tokenizer load would load from the current
    directory in stead of the passed model id on the HF hub.'
  created_at: 2022-10-28 19:00:25+00:00
  edited: false
  hidden: false
  id: 635c34d9e3737b9e4e264f75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598464413995-noauth.png?w=200&h=200&f=face
      fullname: Flexudy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flexudy
      type: user
    createdAt: '2022-10-28T20:10:02.000Z'
    data:
      status: closed
    id: 635c371a177df3f16e993f76
    type: status-change
  author: flexudy
  created_at: 2022-10-28 19:10:02+00:00
  id: 635c371a177df3f16e993f76
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598464413995-noauth.png?w=200&h=200&f=face
      fullname: Flexudy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flexudy
      type: user
    createdAt: '2022-10-28T20:10:44.000Z'
    data:
      edited: false
      editors:
      - flexudy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598464413995-noauth.png?w=200&h=200&f=face
          fullname: Flexudy
          isHf: false
          isPro: false
          name: flexudy
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;yhavinga&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/yhavinga\">@<span class=\"\
          underline\">yhavinga</span></a></span>\n\n\t</span></span> I found the error.\
          \ The tokenizer.json file was not packaged properly.<br>Thank you very much.</p>\n"
        raw: '@yhavinga I found the error. The tokenizer.json file was not packaged
          properly.

          Thank you very much.'
        updatedAt: '2022-10-28T20:10:44.669Z'
      numEdits: 0
      reactions: []
    id: 635c37444a9bcc8a5b71fb3c
    type: comment
  author: flexudy
  content: '@yhavinga I found the error. The tokenizer.json file was not packaged
    properly.

    Thank you very much.'
  created_at: 2022-10-28 19:10:44+00:00
  edited: false
  hidden: false
  id: 635c37444a9bcc8a5b71fb3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yhavinga/t5-base-dutch
repo_type: model
status: closed
target_branch: null
title: Spiece.Model
