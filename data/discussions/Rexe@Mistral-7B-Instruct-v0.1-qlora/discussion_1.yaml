!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tarruda
conflicting_files: null
created_at: 2023-10-04 22:47:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
      fullname: Thiago de Arruda Padilha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarruda
      type: user
    createdAt: '2023-10-04T23:47:35.000Z'
    data:
      edited: false
      editors:
      - tarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9785794019699097
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
          fullname: Thiago de Arruda Padilha
          isHf: false
          isPro: false
          name: tarruda
          type: user
        html: '<p>I would like to try training locally,  can you can share the script
          used to obtain the adapter model?</p>

          <p>Thanks</p>

          '
        raw: "I would like to try training locally,  can you can share the script\
          \ used to obtain the adapter model?\r\n\r\nThanks"
        updatedAt: '2023-10-04T23:47:35.796Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Vezora
    id: 651df997d1645bfce4ab70d9
    type: comment
  author: tarruda
  content: "I would like to try training locally,  can you can share the script used\
    \ to obtain the adapter model?\r\n\r\nThanks"
  created_at: 2023-10-04 22:47:35+00:00
  edited: false
  hidden: false
  id: 651df997d1645bfce4ab70d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652972036350-62621a516c57f7447808c8ab.jpeg?w=200&h=200&f=face
      fullname: Enyinnaya Edoziem
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Rexe
      type: user
    createdAt: '2023-10-16T10:01:50.000Z'
    data:
      edited: false
      editors:
      - Rexe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9850764870643616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652972036350-62621a516c57f7447808c8ab.jpeg?w=200&h=200&f=face
          fullname: Enyinnaya Edoziem
          isHf: false
          isPro: false
          name: Rexe
          type: user
        html: '<p>sorry i''ve been out for some weeks, but i can share the script
          with you.</p>

          '
        raw: sorry i've been out for some weeks, but i can share the script with you.
        updatedAt: '2023-10-16T10:01:50.163Z'
      numEdits: 0
      reactions: []
    id: 652d0a0ed8aed962de7483c8
    type: comment
  author: Rexe
  content: sorry i've been out for some weeks, but i can share the script with you.
  created_at: 2023-10-16 09:01:50+00:00
  edited: false
  hidden: false
  id: 652d0a0ed8aed962de7483c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652972036350-62621a516c57f7447808c8ab.jpeg?w=200&h=200&f=face
      fullname: Enyinnaya Edoziem
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Rexe
      type: user
    createdAt: '2023-10-16T10:07:04.000Z'
    data:
      edited: false
      editors:
      - Rexe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36810770630836487
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652972036350-62621a516c57f7447808c8ab.jpeg?w=200&h=200&f=face
          fullname: Enyinnaya Edoziem
          isHf: false
          isPro: false
          name: Rexe
          type: user
        html: '<p>!pip cache purge<br>!pip install -q -U bitsandbytes<br>!pip install
          -q -U git+<a rel="nofollow" href="https://github.com/huggingface/transformers.git">https://github.com/huggingface/transformers.git</a><br>!pip
          install -q -U git+<a rel="nofollow" href="https://github.com/huggingface/peft.git">https://github.com/huggingface/peft.git</a><br>!pip
          install -q -U git+<a rel="nofollow" href="https://github.com/huggingface/accelerate.git">https://github.com/huggingface/accelerate.git</a><br>!pip
          install -q datasets<br>!pip install -q einops</p>

          <p>from huggingface_hub import login<br>login(token=''*******************************************'')</p>

          <p>from huggingface_hub import notebook_login<br>notebook_login()</p>

          <p>from datasets import load_dataset</p>

          <p>dataset = load_dataset(''nampdn-ai/tiny-codes'', use_auth_token=True)</p>

          <p>from sklearn.model_selection import train_test_split<br>dataset = dataset[''train''].train_test_split(test_size=0.30,
          shuffle=True)<br>DEFAULT_SYSTEM_PROMPT = """<br>Below is an instruction
          that describes a task. Write a response that approximately completes the
          request<br>""".strip()</p>

          <p>def generate_training_prompt(<br>    prompt: str, response: str, system_prompt:
          str = DEFAULT_SYSTEM_PROMPT<br>) -&gt; str:<br>    return f""" ### Instruction:
          {system_prompt}</p>

          <h3 id="input">Input:</h3>

          <p>{prompt.strip()}</p>

          <h2 id="response">Response:</h2>

          <p>{response}</p>

          <p>""".strip()<br>def generate_text(data_points):<br>    if ''input'' in
          data_points and ''output'' in data_points:<br>        return {<br>            ''user
          prompt'': data_points[''input''],<br>            ''user response'': data_points[''output''],<br>            ''text'':
          generate_training_prompt(data_points[''input''], data_points[''output''])<br>        }</p>

          <p>def process_dataset(data):<br>    return (<br>    data.shuffle(seed=42)<br>        .map(generate_text)<br>        .remove_columns(<br>        [<br>           ''instruction'',<br>            ''input'',<br>            ''output'',<br>        ])<br>    )</p>

          <p>dataset[''train''] = process_dataset(dataset[''train''])<br>#dataset[''test'']
          = process_dataset(dataset[''test''])</p>

          <p>import os<br>import torch<br>import torch.nn as nn<br>import bitsandbytes
          as bnb<br>from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM,
          BitsAndBytesConfig</p>

          <h1 id="set-the-environment-variable-for-cuda-visible-devices">Set the environment
          variable for CUDA visible devices</h1>

          <p>os.environ["CUDA_VISIBLE_DEVICES"] = "0"</p>

          <p>model_id = "mistralai/Mistral-7B-Instruct-v0.1"</p>

          <h1 id="load-the-model-configuration">Load the model configuration</h1>

          <p>config = AutoConfig.from_pretrained(model_id)<br>config.naive_attention_prefill
          = True</p>

          <p>bnb_config = BitsAndBytesConfig(<br>    load_in_4bit=True,<br>    bnb_4bit_use_double_quant=True,<br>    bnb_4bit_quant_type="nf4",<br>    bnb_4bit_compute_dtype=torch.bfloat16<br>)</p>

          <p>model = AutoModelForCausalLM.from_pretrained(<br>    model_id,<br>    device_map=''auto'',<br>    quantization_config=bnb_config,<br>    trust_remote_code=True,<br>    config=config  #
          Pass the updated configuration here<br>)</p>

          <h1 id="load-the-tokenizer">Load the tokenizer</h1>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_id)<br>tokenizer.pad_token
          = tokenizer.eos_token</p>

          <p>from peft import prepare_model_for_kbit_training</p>

          <p>model.gradient_checkpointing_enable()<br>model = prepare_model_for_kbit_training(model)</p>

          <p>def print_trainable_parameters(model):<br>    """<br>    Prints the number
          of trainable parameters in the model.<br>    """<br>    trainable_params
          = 0<br>    all_param = 0<br>    for _, param in model.named_parameters():<br>        all_param
          += param.numel()<br>        if param.requires_grad:<br>            trainable_params
          += param.numel()<br>    print(<br>        f"trainable params: {trainable_params}
          || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"<br>    )<br>model.gradient_checkpointing_enable()</p>

          <p>from peft import LoraConfig, get_peft_model</p>

          <p>config = LoraConfig(<br>    r=16,<br>    lora_alpha=32,<br>    target_modules=["q_proj",
          "v_proj"],<br>    lora_dropout=0.05,<br>    bias="none",<br>    task_type="CAUSAL_LM"<br>)</p>

          <p>model = get_peft_model(model, config)<br>print_trainable_parameters(model)</p>

          <p>example = generate_text(dataset[''train''][0])</p>

          <p>dataset = dataset[''train''].map(lambda example: tokenizer(example[''text'']),
          batched=True)</p>

          <p>import transformers</p>

          <p>trainer = transformers.Trainer(<br>    model=model,<br>    train_dataset=dataset,<br>    args=transformers.TrainingArguments(<br>        per_device_train_batch_size=2,<br>        gradient_accumulation_steps=4,<br>        warmup_steps=100,<br>        max_steps=10,<br>        learning_rate=2e-4,<br>        fp16=True,<br>        logging_steps=1,<br>        output_dir="outputs",<br>        optim="paged_adamw_8bit"<br>    ),<br>    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,
          mlm=False),<br>)<br>model.config.use_cache = False  # silence the warnings.
          Please re-enable for inference!<br>trainer.train()</p>

          '
        raw: "!pip cache purge\n!pip install -q -U bitsandbytes\n!pip install -q -U\
          \ git+https://github.com/huggingface/transformers.git\n!pip install -q -U\
          \ git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n\
          !pip install -q datasets\n!pip install -q einops\n\nfrom huggingface_hub\
          \ import login\nlogin(token='*******************************************')\n\
          \nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nfrom datasets\
          \ import load_dataset\n\ndataset = load_dataset('nampdn-ai/tiny-codes',\
          \ use_auth_token=True)\n\nfrom sklearn.model_selection import train_test_split\n\
          dataset = dataset['train'].train_test_split(test_size=0.30, shuffle=True)\n\
          DEFAULT_SYSTEM_PROMPT = \"\"\"\nBelow is an instruction that describes a\
          \ task. Write a response that approximately completes the request\n\"\"\"\
          .strip()\n\ndef generate_training_prompt(\n    prompt: str, response: str,\
          \ system_prompt: str = DEFAULT_SYSTEM_PROMPT\n) -> str:\n    return f\"\"\
          \" ### Instruction: {system_prompt}\n\n### Input:\n{prompt.strip()}\n\n\
          ## Response:\n{response}\n\n\"\"\".strip()\ndef generate_text(data_points):\n\
          \    if 'input' in data_points and 'output' in data_points:\n        return\
          \ {\n            'user prompt': data_points['input'],\n            'user\
          \ response': data_points['output'],\n            'text': generate_training_prompt(data_points['input'],\
          \ data_points['output'])\n        }\n\ndef process_dataset(data):\n    return\
          \ (\n    data.shuffle(seed=42)\n        .map(generate_text)\n        .remove_columns(\n\
          \        [\n           'instruction',\n            'input',\n          \
          \  'output',\n        ])\n    )\n\ndataset['train'] = process_dataset(dataset['train'])\n\
          #dataset['test'] = process_dataset(dataset['test'])\n\nimport os\nimport\
          \ torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom transformers\
          \ import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n\
          \n# Set the environment variable for CUDA visible devices\nos.environ[\"\
          CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\
          \n# Load the model configuration\nconfig = AutoConfig.from_pretrained(model_id)\n\
          config.naive_attention_prefill = True\n\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    model_id,\n    device_map='auto',\n    quantization_config=bnb_config,\n\
          \    trust_remote_code=True,\n    config=config  # Pass the updated configuration\
          \ here\n)\n\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          tokenizer.pad_token = tokenizer.eos_token\n\nfrom peft import prepare_model_for_kbit_training\n\
          \nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\
          \n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number\
          \ of trainable parameters in the model.\n    \"\"\"\n    trainable_params\
          \ = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n\
          \        all_param += param.numel()\n        if param.requires_grad:\n \
          \           trainable_params += param.numel()\n    print(\n        f\"trainable\
          \ params: {trainable_params} || all params: {all_param} || trainable%: {100\
          \ * trainable_params / all_param}\"\n    )\nmodel.gradient_checkpointing_enable()\n\
          \nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n\
          \    r=16, \n    lora_alpha=32, \n    target_modules=[\"q_proj\", \"v_proj\"\
          ], \n    lora_dropout=0.05,\n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\
          \n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)\n\
          \nexample = generate_text(dataset['train'][0])\n\ndataset = dataset['train'].map(lambda\
          \ example: tokenizer(example['text']), batched=True)\n\nimport transformers\n\
          \ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=dataset,\n\
          \    args=transformers.TrainingArguments(\n        per_device_train_batch_size=2,\n\
          \        gradient_accumulation_steps=4,\n        warmup_steps=100,\n   \
          \     max_steps=10,\n        learning_rate=2e-4,\n        fp16=True,\n \
          \       logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"\
          paged_adamw_8bit\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False),\n)\nmodel.config.use_cache = False  # silence the warnings.\
          \ Please re-enable for inference!\ntrainer.train()"
        updatedAt: '2023-10-16T10:07:04.695Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - tarruda
        - jsfs11
    id: 652d0b48fb600447d37aaa8e
    type: comment
  author: Rexe
  content: "!pip cache purge\n!pip install -q -U bitsandbytes\n!pip install -q -U\
    \ git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n\
    !pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install\
    \ -q datasets\n!pip install -q einops\n\nfrom huggingface_hub import login\nlogin(token='*******************************************')\n\
    \nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nfrom datasets\
    \ import load_dataset\n\ndataset = load_dataset('nampdn-ai/tiny-codes', use_auth_token=True)\n\
    \nfrom sklearn.model_selection import train_test_split\ndataset = dataset['train'].train_test_split(test_size=0.30,\
    \ shuffle=True)\nDEFAULT_SYSTEM_PROMPT = \"\"\"\nBelow is an instruction that\
    \ describes a task. Write a response that approximately completes the request\n\
    \"\"\".strip()\n\ndef generate_training_prompt(\n    prompt: str, response: str,\
    \ system_prompt: str = DEFAULT_SYSTEM_PROMPT\n) -> str:\n    return f\"\"\" ###\
    \ Instruction: {system_prompt}\n\n### Input:\n{prompt.strip()}\n\n## Response:\n\
    {response}\n\n\"\"\".strip()\ndef generate_text(data_points):\n    if 'input'\
    \ in data_points and 'output' in data_points:\n        return {\n            'user\
    \ prompt': data_points['input'],\n            'user response': data_points['output'],\n\
    \            'text': generate_training_prompt(data_points['input'], data_points['output'])\n\
    \        }\n\ndef process_dataset(data):\n    return (\n    data.shuffle(seed=42)\n\
    \        .map(generate_text)\n        .remove_columns(\n        [\n          \
    \ 'instruction',\n            'input',\n            'output',\n        ])\n  \
    \  )\n\ndataset['train'] = process_dataset(dataset['train'])\n#dataset['test']\
    \ = process_dataset(dataset['test'])\n\nimport os\nimport torch\nimport torch.nn\
    \ as nn\nimport bitsandbytes as bnb\nfrom transformers import AutoTokenizer, AutoConfig,\
    \ AutoModelForCausalLM, BitsAndBytesConfig\n\n# Set the environment variable for\
    \ CUDA visible devices\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nmodel_id\
    \ = \"mistralai/Mistral-7B-Instruct-v0.1\"\n# Load the model configuration\nconfig\
    \ = AutoConfig.from_pretrained(model_id)\nconfig.naive_attention_prefill = True\n\
    \nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
    \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map='auto',\n\
    \    quantization_config=bnb_config,\n    trust_remote_code=True,\n    config=config\
    \  # Pass the updated configuration here\n)\n\n\n# Load the tokenizer\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\
    \nfrom peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\n\
    model = prepare_model_for_kbit_training(model)\n\n\ndef print_trainable_parameters(model):\n\
    \    \"\"\"\n    Prints the number of trainable parameters in the model.\n   \
    \ \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n\
    \        all_param += param.numel()\n        if param.requires_grad:\n       \
    \     trainable_params += param.numel()\n    print(\n        f\"trainable params:\
    \ {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params\
    \ / all_param}\"\n    )\nmodel.gradient_checkpointing_enable()\n\nfrom peft import\
    \ LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=16, \n    lora_alpha=32,\
    \ \n    target_modules=[\"q_proj\", \"v_proj\"], \n    lora_dropout=0.05,\n  \
    \  bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model,\
    \ config)\nprint_trainable_parameters(model)\n\nexample = generate_text(dataset['train'][0])\n\
    \ndataset = dataset['train'].map(lambda example: tokenizer(example['text']), batched=True)\n\
    \nimport transformers\n\ntrainer = transformers.Trainer(\n    model=model,\n \
    \   train_dataset=dataset,\n    args=transformers.TrainingArguments(\n       \
    \ per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n   \
    \     warmup_steps=100,\n        max_steps=10,\n        learning_rate=2e-4,\n\
    \        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\"\
    ,\n        optim=\"paged_adamw_8bit\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
    \ mlm=False),\n)\nmodel.config.use_cache = False  # silence the warnings. Please\
    \ re-enable for inference!\ntrainer.train()"
  created_at: 2023-10-16 09:07:04+00:00
  edited: false
  hidden: false
  id: 652d0b48fb600447d37aaa8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
      fullname: Thiago de Arruda Padilha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarruda
      type: user
    createdAt: '2023-10-19T14:20:09.000Z'
    data:
      edited: false
      editors:
      - tarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.926429808139801
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
          fullname: Thiago de Arruda Padilha
          isHf: false
          isPro: false
          name: tarruda
          type: user
        html: '<p>Thanks. Can you wrap it in markdown code blocks (```)? It won''t
          work the way it is currently formatted</p>

          '
        raw: Thanks. Can you wrap it in markdown code blocks (```)? It won't work
          the way it is currently formatted
        updatedAt: '2023-10-19T14:20:09.843Z'
      numEdits: 0
      reactions: []
    id: 65313b194173faa8227598be
    type: comment
  author: tarruda
  content: Thanks. Can you wrap it in markdown code blocks (```)? It won't work the
    way it is currently formatted
  created_at: 2023-10-19 13:20:09+00:00
  edited: false
  hidden: false
  id: 65313b194173faa8227598be
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Rexe/Mistral-7B-Instruct-v0.1-qlora
repo_type: model
status: open
target_branch: null
title: Can you share the script used for training?
