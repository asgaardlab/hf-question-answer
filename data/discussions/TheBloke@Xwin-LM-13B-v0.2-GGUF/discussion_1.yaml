!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 0xbcn
conflicting_files: null
created_at: 2023-10-17 09:49:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/696b331601b36007a3f877555d8f441d.svg
      fullname: Bacon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0xbcn
      type: user
    createdAt: '2023-10-17T10:49:20.000Z'
    data:
      edited: false
      editors:
      - 0xbcn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9115800261497498
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/696b331601b36007a3f877555d8f441d.svg
          fullname: Bacon
          isHf: false
          isPro: false
          name: 0xbcn
          type: user
        html: '<p>Sorry if you see this question a lot, but what exactly is the difference
          of those 3 file formats?<br>Or better asked, what should I use if I have
          an GPU available for transforming and prefer the most performance over "everything
          else"?</p>

          <p>For now I opted with GGUF and the llama.ccp implementation, assuming
          C++ performs well in this area.</p>

          '
        raw: "Sorry if you see this question a lot, but what exactly is the difference\
          \ of those 3 file formats?\r\nOr better asked, what should I use if I have\
          \ an GPU available for transforming and prefer the most performance over\
          \ \"everything else\"?\r\n\r\nFor now I opted with GGUF and the llama.ccp\
          \ implementation, assuming C++ performs well in this area."
        updatedAt: '2023-10-17T10:49:20.207Z'
      numEdits: 0
      reactions: []
    id: 652e66b0d1796ce82157a611
    type: comment
  author: 0xbcn
  content: "Sorry if you see this question a lot, but what exactly is the difference\
    \ of those 3 file formats?\r\nOr better asked, what should I use if I have an\
    \ GPU available for transforming and prefer the most performance over \"everything\
    \ else\"?\r\n\r\nFor now I opted with GGUF and the llama.ccp implementation, assuming\
    \ C++ performs well in this area."
  created_at: 2023-10-17 09:49:20+00:00
  edited: false
  hidden: false
  id: 652e66b0d1796ce82157a611
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-17T15:59:48.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9390081167221069
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Gguf is best for cpu and mac</p>

          <p>Gptq is best for gpu with exllama and good for servers since it can be
          used with tgi</p>

          <p>Awq is highest quality and best for servers since it can be used with
          vllm.</p>

          <p>So most likely gptq if you have gpu which can fit the whole model, gguf
          If you have Mac or cpu or can not fit the model in gpu, and awq for server
          inference with vllm</p>

          '
        raw: 'Gguf is best for cpu and mac


          Gptq is best for gpu with exllama and good for servers since it can be used
          with tgi


          Awq is highest quality and best for servers since it can be used with vllm.


          So most likely gptq if you have gpu which can fit the whole model, gguf
          If you have Mac or cpu or can not fit the model in gpu, and awq for server
          inference with vllm'
        updatedAt: '2023-10-17T15:59:48.832Z'
      numEdits: 0
      reactions: []
    id: 652eaf74dbd5512ef5552219
    type: comment
  author: YaTharThShaRma999
  content: 'Gguf is best for cpu and mac


    Gptq is best for gpu with exllama and good for servers since it can be used with
    tgi


    Awq is highest quality and best for servers since it can be used with vllm.


    So most likely gptq if you have gpu which can fit the whole model, gguf If you
    have Mac or cpu or can not fit the model in gpu, and awq for server inference
    with vllm'
  created_at: 2023-10-17 14:59:48+00:00
  edited: false
  hidden: false
  id: 652eaf74dbd5512ef5552219
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Xwin-LM-13B-v0.2-GGUF
repo_type: model
status: open
target_branch: null
title: GGUF, AWQ, GPTQ ?
