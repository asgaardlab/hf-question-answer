!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vinnitu
conflicting_files: null
created_at: 2023-12-01 16:06:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinnitu
      type: user
    createdAt: '2023-12-01T16:06:40.000Z'
    data:
      edited: false
      editors:
      - vinnitu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40715450048446655
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
          fullname: Victor
          isHf: false
          isPro: false
          name: vinnitu
          type: user
        html: '<p>import cramming</p>

          <p>model_name = "pbelcak/UltraFastBERT-1x11-long"</p>

          <p>from datasets import load_dataset<br>dataset = load_dataset("yelp_review_full")</p>

          <p>from transformers import AutoTokenizer as Tokenizer<br>tokenizer = Tokenizer.from_pretrained(model_name)</p>

          <p>def tokenize_function(examples):<br>    return tokenizer(examples["text"],
          padding="max_length", truncation=True)</p>

          <p>tokenized_datasets = dataset.map(tokenize_function, batched=True)</p>

          <p>small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))<br>small_eval_dataset
          = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))</p>

          <p>from transformers import AutoModelForMaskedLM as Model</p>

          <p>model = Model.from_pretrained(model_name, num_labels=5)<br>from transformers
          import TrainingArguments</p>

          <p>training_args = TrainingArguments(output_dir="test_trainer")</p>

          <p>import numpy as np<br>import evaluate</p>

          <p>metric = evaluate.load("accuracy")</p>

          <p>def compute_metrics(eval_pred):<br>    logits, labels = eval_pred<br>    predictions
          = np.argmax(logits, axis=-1)<br>    return metric.compute(predictions=predictions,
          references=labels) # here error</p>

          <p>from transformers import TrainingArguments, Trainer</p>

          <p>training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")</p>

          <p>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    train_dataset=small_train_dataset,<br>    eval_dataset=small_eval_dataset,<br>    compute_metrics=compute_metrics,<br>)</p>

          <p>trainer.train()</p>

          '
        raw: "import cramming\r\n\r\nmodel_name = \"pbelcak/UltraFastBERT-1x11-long\"\
          \r\n\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"yelp_review_full\"\
          )\r\n\r\nfrom transformers import AutoTokenizer as Tokenizer\r\ntokenizer\
          \ = Tokenizer.from_pretrained(model_name)\r\n\r\ndef tokenize_function(examples):\r\
          \n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\
          \n\r\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\r\
          \n\r\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\r\
          \nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\r\
          \n\r\nfrom transformers import AutoModelForMaskedLM as Model\r\n\r\nmodel\
          \ = Model.from_pretrained(model_name, num_labels=5)\r\nfrom transformers\
          \ import TrainingArguments\r\n\r\ntraining_args = TrainingArguments(output_dir=\"\
          test_trainer\")\r\n\r\nimport numpy as np\r\nimport evaluate\r\n\r\nmetric\
          \ = evaluate.load(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\
          \n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits,\
          \ axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\
          \ # here error\r\n\r\nfrom transformers import TrainingArguments, Trainer\r\
          \n\r\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"\
          epoch\")\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\
          \n    train_dataset=small_train_dataset,\r\n    eval_dataset=small_eval_dataset,\r\
          \n    compute_metrics=compute_metrics,\r\n)\r\n\r\ntrainer.train()"
        updatedAt: '2023-12-01T16:06:40.978Z'
      numEdits: 0
      reactions: []
    id: 656a0490046899997bae1d61
    type: comment
  author: vinnitu
  content: "import cramming\r\n\r\nmodel_name = \"pbelcak/UltraFastBERT-1x11-long\"\
    \r\n\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"yelp_review_full\"\
    )\r\n\r\nfrom transformers import AutoTokenizer as Tokenizer\r\ntokenizer = Tokenizer.from_pretrained(model_name)\r\
    \n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"\
    ], padding=\"max_length\", truncation=True)\r\n\r\ntokenized_datasets = dataset.map(tokenize_function,\
    \ batched=True)\r\n\r\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\r\
    \nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\r\
    \n\r\nfrom transformers import AutoModelForMaskedLM as Model\r\n\r\nmodel = Model.from_pretrained(model_name,\
    \ num_labels=5)\r\nfrom transformers import TrainingArguments\r\n\r\ntraining_args\
    \ = TrainingArguments(output_dir=\"test_trainer\")\r\n\r\nimport numpy as np\r\
    \nimport evaluate\r\n\r\nmetric = evaluate.load(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\
    \n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\
    \n    return metric.compute(predictions=predictions, references=labels) # here\
    \ error\r\n\r\nfrom transformers import TrainingArguments, Trainer\r\n\r\ntraining_args\
    \ = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\"\
    )\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n\
    \    train_dataset=small_train_dataset,\r\n    eval_dataset=small_eval_dataset,\r\
    \n    compute_metrics=compute_metrics,\r\n)\r\n\r\ntrainer.train()"
  created_at: 2023-12-01 16:06:40+00:00
  edited: false
  hidden: false
  id: 656a0490046899997bae1d61
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: pbelcak/UltraFastBERT-1x11-long
repo_type: model
status: open
target_branch: null
title: Mismatch in the number of predictions (128000) and references (1000)
