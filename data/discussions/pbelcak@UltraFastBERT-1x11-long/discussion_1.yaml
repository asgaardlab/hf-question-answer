!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Starlento
conflicting_files: null
created_at: 2023-11-21 13:31:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
      fullname: Starlento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Starlento
      type: user
    createdAt: '2023-11-21T13:31:46.000Z'
    data:
      edited: false
      editors:
      - Starlento
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23200175166130066
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
          fullname: Starlento
          isHf: false
          isPro: false
          name: Starlento
          type: user
        html: '<p>I install cramming and use the latest transformers.<br>The example
          code can be run, but many weights cannot be loaded.<br>Is this normal?</p>

          <pre><code>Some weights of the model checkpoint at ./models/pbelcak_FastBERT-1x11-long
          were not used when initializing ScriptableLMForPreTraining: [''encoder.layers.7.ffn.linear_in.bias'',
          ''encoder.layers.12.ffn.linear_out.weight'', ''encoder.layers.13.ffn.linear_out.weight'',
          ''encoder.layers.6.ffn.linear_out.weight'', ''encoder.layers.3.ffn.linear_in.weight'',
          ''encoder.layers.12.ffn.linear_in.weight'', ''encoder.layers.14.ffn.linear_in.bias'',
          ''encoder.layers.2.ffn.linear_out.weight'', ''encoder.layers.9.ffn.linear_in.bias'',
          ''encoder.layers.0.ffn.linear_in.bias'', ''encoder.layers.4.ffn.linear_out.weight'',
          ''encoder.layers.6.ffn.linear_in.weight'', ''encoder.layers.4.ffn.linear_in.weight'',
          ''encoder.layers.11.ffn.linear_in.weight'', ''encoder.layers.14.ffn.linear_in.weight'',
          ''encoder.layers.2.ffn.linear_in.bias'', ''encoder.layers.5.ffn.linear_out.weight'',
          ''encoder.layers.10.ffn.linear_in.bias'', ''encoder.layers.3.ffn.linear_out.weight'',
          ''encoder.layers.7.ffn.linear_in.weight'', ''encoder.layers.8.ffn.linear_out.weight'',
          ''encoder.layers.9.ffn.linear_out.weight'', ''encoder.layers.15.ffn.linear_in.bias'',
          ''encoder.layers.13.ffn.linear_in.weight'', ''encoder.layers.0.ffn.linear_in.weight'',
          ''encoder.layers.10.ffn.linear_out.weight'', ''encoder.layers.5.ffn.linear_in.weight'',
          ''encoder.layers.6.ffn.linear_in.bias'', ''encoder.layers.4.ffn.linear_in.bias'',
          ''encoder.layers.15.ffn.linear_out.weight'', ''encoder.layers.10.ffn.linear_in.weight'',
          ''encoder.layers.13.ffn.linear_in.bias'', ''encoder.layers.5.ffn.linear_in.bias'',
          ''encoder.layers.2.ffn.linear_in.weight'', ''encoder.layers.11.ffn.linear_in.bias'',
          ''encoder.layers.1.ffn.linear_in.bias'', ''encoder.layers.12.ffn.linear_in.bias'',
          ''encoder.layers.8.ffn.linear_in.bias'', ''encoder.layers.8.ffn.linear_in.weight'',
          ''encoder.layers.1.ffn.linear_in.weight'', ''encoder.layers.1.ffn.linear_out.weight'',
          ''encoder.layers.3.ffn.linear_in.bias'', ''encoder.layers.9.ffn.linear_in.weight'',
          ''encoder.layers.0.ffn.linear_out.weight'', ''encoder.layers.14.ffn.linear_out.weight'',
          ''encoder.layers.15.ffn.linear_in.weight'', ''encoder.layers.11.ffn.linear_out.weight'',
          ''encoder.layers.7.ffn.linear_out.weight'']

          - This IS expected if you are initializing ScriptableLMForPreTraining from
          the checkpoint of a model trained on another task or with another architecture
          (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining
          model).

          - This IS NOT expected if you are initializing ScriptableLMForPreTraining
          from the checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).

          Some weights of ScriptableLMForPreTraining were not initialized from the
          model checkpoint at ./models/pbelcak_FastBERT-1x11-long and are newly initialized:
          [''encoder.layers.14.ffn.dense_in.weight'', ''encoder.layers.15.ffn.dense_out.weight'',
          ''encoder.layers.15.ffn.dense_in.weight'', ''encoder.layers.13.ffn.dense_in.weight'',
          ''encoder.layers.11.ffn.dense_in.weight'', ''encoder.layers.5.ffn.dense_out.weight'',
          ''encoder.layers.12.ffn.dense_out.weight'', ''encoder.layers.9.ffn.dense_out.weight'',
          ''encoder.layers.1.ffn.dense_out.weight'', ''encoder.layers.5.ffn.dense_in.weight'',
          ''encoder.layers.8.ffn.dense_out.weight'', ''encoder.layers.0.ffn.dense_out.weight'',
          ''encoder.layers.8.ffn.dense_in.weight'', ''encoder.layers.6.ffn.dense_in.weight'',
          ''encoder.layers.4.ffn.dense_in.weight'', ''encoder.layers.10.ffn.dense_out.weight'',
          ''encoder.layers.4.ffn.dense_out.weight'', ''encoder.layers.2.ffn.dense_out.weight'',
          ''encoder.layers.11.ffn.dense_out.weight'', ''encoder.layers.14.ffn.dense_out.weight'',
          ''encoder.layers.0.ffn.dense_in.weight'', ''encoder.layers.3.ffn.dense_out.weight'',
          ''encoder.layers.13.ffn.dense_out.weight'', ''encoder.layers.3.ffn.dense_in.weight'',
          ''encoder.layers.1.ffn.dense_in.weight'', ''encoder.layers.6.ffn.dense_out.weight'',
          ''encoder.layers.10.ffn.dense_in.weight'', ''encoder.layers.12.ffn.dense_in.weight'',
          ''encoder.layers.2.ffn.dense_in.weight'', ''encoder.layers.9.ffn.dense_in.weight'',
          ''encoder.layers.7.ffn.dense_out.weight'', ''encoder.layers.7.ffn.dense_in.weight'']

          You should probably TRAIN this model on a down-stream task to be able to
          use it for predictions and inference.

          </code></pre>

          '
        raw: "I install cramming and use the latest transformers. \r\nThe example\
          \ code can be run, but many weights cannot be loaded. \r\nIs this normal?\r\
          \n```\r\nSome weights of the model checkpoint at ./models/pbelcak_FastBERT-1x11-long\
          \ were not used when initializing ScriptableLMForPreTraining: ['encoder.layers.7.ffn.linear_in.bias',\
          \ 'encoder.layers.12.ffn.linear_out.weight', 'encoder.layers.13.ffn.linear_out.weight',\
          \ 'encoder.layers.6.ffn.linear_out.weight', 'encoder.layers.3.ffn.linear_in.weight',\
          \ 'encoder.layers.12.ffn.linear_in.weight', 'encoder.layers.14.ffn.linear_in.bias',\
          \ 'encoder.layers.2.ffn.linear_out.weight', 'encoder.layers.9.ffn.linear_in.bias',\
          \ 'encoder.layers.0.ffn.linear_in.bias', 'encoder.layers.4.ffn.linear_out.weight',\
          \ 'encoder.layers.6.ffn.linear_in.weight', 'encoder.layers.4.ffn.linear_in.weight',\
          \ 'encoder.layers.11.ffn.linear_in.weight', 'encoder.layers.14.ffn.linear_in.weight',\
          \ 'encoder.layers.2.ffn.linear_in.bias', 'encoder.layers.5.ffn.linear_out.weight',\
          \ 'encoder.layers.10.ffn.linear_in.bias', 'encoder.layers.3.ffn.linear_out.weight',\
          \ 'encoder.layers.7.ffn.linear_in.weight', 'encoder.layers.8.ffn.linear_out.weight',\
          \ 'encoder.layers.9.ffn.linear_out.weight', 'encoder.layers.15.ffn.linear_in.bias',\
          \ 'encoder.layers.13.ffn.linear_in.weight', 'encoder.layers.0.ffn.linear_in.weight',\
          \ 'encoder.layers.10.ffn.linear_out.weight', 'encoder.layers.5.ffn.linear_in.weight',\
          \ 'encoder.layers.6.ffn.linear_in.bias', 'encoder.layers.4.ffn.linear_in.bias',\
          \ 'encoder.layers.15.ffn.linear_out.weight', 'encoder.layers.10.ffn.linear_in.weight',\
          \ 'encoder.layers.13.ffn.linear_in.bias', 'encoder.layers.5.ffn.linear_in.bias',\
          \ 'encoder.layers.2.ffn.linear_in.weight', 'encoder.layers.11.ffn.linear_in.bias',\
          \ 'encoder.layers.1.ffn.linear_in.bias', 'encoder.layers.12.ffn.linear_in.bias',\
          \ 'encoder.layers.8.ffn.linear_in.bias', 'encoder.layers.8.ffn.linear_in.weight',\
          \ 'encoder.layers.1.ffn.linear_in.weight', 'encoder.layers.1.ffn.linear_out.weight',\
          \ 'encoder.layers.3.ffn.linear_in.bias', 'encoder.layers.9.ffn.linear_in.weight',\
          \ 'encoder.layers.0.ffn.linear_out.weight', 'encoder.layers.14.ffn.linear_out.weight',\
          \ 'encoder.layers.15.ffn.linear_in.weight', 'encoder.layers.11.ffn.linear_out.weight',\
          \ 'encoder.layers.7.ffn.linear_out.weight']\r\n- This IS expected if you\
          \ are initializing ScriptableLMForPreTraining from the checkpoint of a model\
          \ trained on another task or with another architecture (e.g. initializing\
          \ a BertForSequenceClassification model from a BertForPreTraining model).\r\
          \n- This IS NOT expected if you are initializing ScriptableLMForPreTraining\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\r\nSome weights of ScriptableLMForPreTraining were not initialized\
          \ from the model checkpoint at ./models/pbelcak_FastBERT-1x11-long and are\
          \ newly initialized: ['encoder.layers.14.ffn.dense_in.weight', 'encoder.layers.15.ffn.dense_out.weight',\
          \ 'encoder.layers.15.ffn.dense_in.weight', 'encoder.layers.13.ffn.dense_in.weight',\
          \ 'encoder.layers.11.ffn.dense_in.weight', 'encoder.layers.5.ffn.dense_out.weight',\
          \ 'encoder.layers.12.ffn.dense_out.weight', 'encoder.layers.9.ffn.dense_out.weight',\
          \ 'encoder.layers.1.ffn.dense_out.weight', 'encoder.layers.5.ffn.dense_in.weight',\
          \ 'encoder.layers.8.ffn.dense_out.weight', 'encoder.layers.0.ffn.dense_out.weight',\
          \ 'encoder.layers.8.ffn.dense_in.weight', 'encoder.layers.6.ffn.dense_in.weight',\
          \ 'encoder.layers.4.ffn.dense_in.weight', 'encoder.layers.10.ffn.dense_out.weight',\
          \ 'encoder.layers.4.ffn.dense_out.weight', 'encoder.layers.2.ffn.dense_out.weight',\
          \ 'encoder.layers.11.ffn.dense_out.weight', 'encoder.layers.14.ffn.dense_out.weight',\
          \ 'encoder.layers.0.ffn.dense_in.weight', 'encoder.layers.3.ffn.dense_out.weight',\
          \ 'encoder.layers.13.ffn.dense_out.weight', 'encoder.layers.3.ffn.dense_in.weight',\
          \ 'encoder.layers.1.ffn.dense_in.weight', 'encoder.layers.6.ffn.dense_out.weight',\
          \ 'encoder.layers.10.ffn.dense_in.weight', 'encoder.layers.12.ffn.dense_in.weight',\
          \ 'encoder.layers.2.ffn.dense_in.weight', 'encoder.layers.9.ffn.dense_in.weight',\
          \ 'encoder.layers.7.ffn.dense_out.weight', 'encoder.layers.7.ffn.dense_in.weight']\r\
          \nYou should probably TRAIN this model on a down-stream task to be able\
          \ to use it for predictions and inference.\r\n```"
        updatedAt: '2023-11-21T13:31:46.362Z'
      numEdits: 0
      reactions: []
    id: 655cb142b4c01c1f76c057a5
    type: comment
  author: Starlento
  content: "I install cramming and use the latest transformers. \r\nThe example code\
    \ can be run, but many weights cannot be loaded. \r\nIs this normal?\r\n```\r\n\
    Some weights of the model checkpoint at ./models/pbelcak_FastBERT-1x11-long were\
    \ not used when initializing ScriptableLMForPreTraining: ['encoder.layers.7.ffn.linear_in.bias',\
    \ 'encoder.layers.12.ffn.linear_out.weight', 'encoder.layers.13.ffn.linear_out.weight',\
    \ 'encoder.layers.6.ffn.linear_out.weight', 'encoder.layers.3.ffn.linear_in.weight',\
    \ 'encoder.layers.12.ffn.linear_in.weight', 'encoder.layers.14.ffn.linear_in.bias',\
    \ 'encoder.layers.2.ffn.linear_out.weight', 'encoder.layers.9.ffn.linear_in.bias',\
    \ 'encoder.layers.0.ffn.linear_in.bias', 'encoder.layers.4.ffn.linear_out.weight',\
    \ 'encoder.layers.6.ffn.linear_in.weight', 'encoder.layers.4.ffn.linear_in.weight',\
    \ 'encoder.layers.11.ffn.linear_in.weight', 'encoder.layers.14.ffn.linear_in.weight',\
    \ 'encoder.layers.2.ffn.linear_in.bias', 'encoder.layers.5.ffn.linear_out.weight',\
    \ 'encoder.layers.10.ffn.linear_in.bias', 'encoder.layers.3.ffn.linear_out.weight',\
    \ 'encoder.layers.7.ffn.linear_in.weight', 'encoder.layers.8.ffn.linear_out.weight',\
    \ 'encoder.layers.9.ffn.linear_out.weight', 'encoder.layers.15.ffn.linear_in.bias',\
    \ 'encoder.layers.13.ffn.linear_in.weight', 'encoder.layers.0.ffn.linear_in.weight',\
    \ 'encoder.layers.10.ffn.linear_out.weight', 'encoder.layers.5.ffn.linear_in.weight',\
    \ 'encoder.layers.6.ffn.linear_in.bias', 'encoder.layers.4.ffn.linear_in.bias',\
    \ 'encoder.layers.15.ffn.linear_out.weight', 'encoder.layers.10.ffn.linear_in.weight',\
    \ 'encoder.layers.13.ffn.linear_in.bias', 'encoder.layers.5.ffn.linear_in.bias',\
    \ 'encoder.layers.2.ffn.linear_in.weight', 'encoder.layers.11.ffn.linear_in.bias',\
    \ 'encoder.layers.1.ffn.linear_in.bias', 'encoder.layers.12.ffn.linear_in.bias',\
    \ 'encoder.layers.8.ffn.linear_in.bias', 'encoder.layers.8.ffn.linear_in.weight',\
    \ 'encoder.layers.1.ffn.linear_in.weight', 'encoder.layers.1.ffn.linear_out.weight',\
    \ 'encoder.layers.3.ffn.linear_in.bias', 'encoder.layers.9.ffn.linear_in.weight',\
    \ 'encoder.layers.0.ffn.linear_out.weight', 'encoder.layers.14.ffn.linear_out.weight',\
    \ 'encoder.layers.15.ffn.linear_in.weight', 'encoder.layers.11.ffn.linear_out.weight',\
    \ 'encoder.layers.7.ffn.linear_out.weight']\r\n- This IS expected if you are initializing\
    \ ScriptableLMForPreTraining from the checkpoint of a model trained on another\
    \ task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\r\n- This IS NOT expected if you are\
    \ initializing ScriptableLMForPreTraining from the checkpoint of a model that\
    \ you expect to be exactly identical (initializing a BertForSequenceClassification\
    \ model from a BertForSequenceClassification model).\r\nSome weights of ScriptableLMForPreTraining\
    \ were not initialized from the model checkpoint at ./models/pbelcak_FastBERT-1x11-long\
    \ and are newly initialized: ['encoder.layers.14.ffn.dense_in.weight', 'encoder.layers.15.ffn.dense_out.weight',\
    \ 'encoder.layers.15.ffn.dense_in.weight', 'encoder.layers.13.ffn.dense_in.weight',\
    \ 'encoder.layers.11.ffn.dense_in.weight', 'encoder.layers.5.ffn.dense_out.weight',\
    \ 'encoder.layers.12.ffn.dense_out.weight', 'encoder.layers.9.ffn.dense_out.weight',\
    \ 'encoder.layers.1.ffn.dense_out.weight', 'encoder.layers.5.ffn.dense_in.weight',\
    \ 'encoder.layers.8.ffn.dense_out.weight', 'encoder.layers.0.ffn.dense_out.weight',\
    \ 'encoder.layers.8.ffn.dense_in.weight', 'encoder.layers.6.ffn.dense_in.weight',\
    \ 'encoder.layers.4.ffn.dense_in.weight', 'encoder.layers.10.ffn.dense_out.weight',\
    \ 'encoder.layers.4.ffn.dense_out.weight', 'encoder.layers.2.ffn.dense_out.weight',\
    \ 'encoder.layers.11.ffn.dense_out.weight', 'encoder.layers.14.ffn.dense_out.weight',\
    \ 'encoder.layers.0.ffn.dense_in.weight', 'encoder.layers.3.ffn.dense_out.weight',\
    \ 'encoder.layers.13.ffn.dense_out.weight', 'encoder.layers.3.ffn.dense_in.weight',\
    \ 'encoder.layers.1.ffn.dense_in.weight', 'encoder.layers.6.ffn.dense_out.weight',\
    \ 'encoder.layers.10.ffn.dense_in.weight', 'encoder.layers.12.ffn.dense_in.weight',\
    \ 'encoder.layers.2.ffn.dense_in.weight', 'encoder.layers.9.ffn.dense_in.weight',\
    \ 'encoder.layers.7.ffn.dense_out.weight', 'encoder.layers.7.ffn.dense_in.weight']\r\
    \nYou should probably TRAIN this model on a down-stream task to be able to use\
    \ it for predictions and inference.\r\n```"
  created_at: 2023-11-21 13:31:46+00:00
  edited: false
  hidden: false
  id: 655cb142b4c01c1f76c057a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b289d4dda0aafd60af3f14e19837b69c.svg
      fullname: Peter Belcak
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pbelcak
      type: user
    createdAt: '2023-11-21T13:59:20.000Z'
    data:
      edited: false
      editors:
      - pbelcak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8099220395088196
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b289d4dda0aafd60af3f14e19837b69c.svg
          fullname: Peter Belcak
          isHf: false
          isPro: false
          name: pbelcak
          type: user
        html: '<p>Hello,</p>

          <p>What your warnings are saying is that they found weights for the <code>FFF</code>
          module (<code>training/cramming/architectures/fff.py</code>) but the model
          is trying to load the weights for the <code>FFNComponent</code> module (<code>training/cramming/crammed_bert.py</code>).
          I just tried running the README example with a fresh instance and I could
          not reproduce your warnings. </p>

          <p>You''re most likely using <code>cramming</code> installed from the original
          <code>cramming</code> repository and not from the <code>training</code>
          directory of this project.</p>

          <p>To recap, these are the steps:</p>

          <ol>

          <li><code>pip uninstall cramming</code> to remove the previous version of
          cramming installed in your environment -- or just start with a fresh environment.</li>

          <li><code>cd training</code></li>

          <li><code>pip install .</code></li>

          <li>Create <code>minimal_example.py</code></li>

          <li>Paste</li>

          </ol>

          <pre><code>import cramming

          from transformers import AutoModelForMaskedLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("pbelcak/FastBERT-1x11-long")

          model = AutoModelForMaskedLM.from_pretrained("pbelcak/FastBERT-1x11-long")


          text = "Replace me by any text you''d like."

          encoded_input = tokenizer(text, return_tensors=''pt'')

          output = model(**encoded_input)

          </code></pre>

          <ol start="6">

          <li><code>python minimal_example.py</code>.</li>

          </ol>

          '
        raw: "Hello,\n\nWhat your warnings are saying is that they found weights for\
          \ the `FFF` module (`training/cramming/architectures/fff.py`) but the model\
          \ is trying to load the weights for the `FFNComponent` module (`training/cramming/crammed_bert.py`).\
          \ I just tried running the README example with a fresh instance and I could\
          \ not reproduce your warnings. \n\nYou're most likely using `cramming` installed\
          \ from the original `cramming` repository and not from the `training` directory\
          \ of this project.\n\nTo recap, these are the steps:\n\n1. `pip uninstall\
          \ cramming` to remove the previous version of cramming installed in your\
          \ environment -- or just start with a fresh environment.\n2. `cd training`\n\
          3. `pip install .`\n4. Create `minimal_example.py`\n5. Paste \n```\nimport\
          \ cramming\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"pbelcak/FastBERT-1x11-long\"\
          )\nmodel = AutoModelForMaskedLM.from_pretrained(\"pbelcak/FastBERT-1x11-long\"\
          )\n\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text,\
          \ return_tensors='pt')\noutput = model(**encoded_input)\n```\n6. `python\
          \ minimal_example.py`."
        updatedAt: '2023-11-21T13:59:20.675Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Starlento
    id: 655cb7b8731d826117c8c63e
    type: comment
  author: pbelcak
  content: "Hello,\n\nWhat your warnings are saying is that they found weights for\
    \ the `FFF` module (`training/cramming/architectures/fff.py`) but the model is\
    \ trying to load the weights for the `FFNComponent` module (`training/cramming/crammed_bert.py`).\
    \ I just tried running the README example with a fresh instance and I could not\
    \ reproduce your warnings. \n\nYou're most likely using `cramming` installed from\
    \ the original `cramming` repository and not from the `training` directory of\
    \ this project.\n\nTo recap, these are the steps:\n\n1. `pip uninstall cramming`\
    \ to remove the previous version of cramming installed in your environment --\
    \ or just start with a fresh environment.\n2. `cd training`\n3. `pip install .`\n\
    4. Create `minimal_example.py`\n5. Paste \n```\nimport cramming\nfrom transformers\
    \ import AutoModelForMaskedLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    pbelcak/FastBERT-1x11-long\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"\
    pbelcak/FastBERT-1x11-long\")\n\ntext = \"Replace me by any text you'd like.\"\
    \nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\
    ```\n6. `python minimal_example.py`."
  created_at: 2023-11-21 13:59:20+00:00
  edited: false
  hidden: false
  id: 655cb7b8731d826117c8c63e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6744cfdc9f5b3f934d1b4d4d7ec97697.svg
      fullname: Tomas Batrla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batrlatom
      type: user
    createdAt: '2023-11-21T15:47:44.000Z'
    data:
      edited: false
      editors:
      - batrlatom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658834338188171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6744cfdc9f5b3f934d1b4d4d7ec97697.svg
          fullname: Tomas Batrla
          isHf: false
          isPro: false
          name: batrlatom
          type: user
        html: '<p>there is no training folder in your repo. Am I looking to the wrong
          place?</p>

          '
        raw: there is no training folder in your repo. Am I looking to the wrong place?
        updatedAt: '2023-11-21T15:47:44.683Z'
      numEdits: 0
      reactions: []
    id: 655cd1202735108d497782f5
    type: comment
  author: batrlatom
  content: there is no training folder in your repo. Am I looking to the wrong place?
  created_at: 2023-11-21 15:47:44+00:00
  edited: false
  hidden: false
  id: 655cd1202735108d497782f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b289d4dda0aafd60af3f14e19837b69c.svg
      fullname: Peter Belcak
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pbelcak
      type: user
    createdAt: '2023-11-21T16:14:53.000Z'
    data:
      edited: true
      editors:
      - pbelcak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8005452156066895
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b289d4dda0aafd60af3f14e19837b69c.svg
          fullname: Peter Belcak
          isHf: false
          isPro: false
          name: pbelcak
          type: user
        html: '<p>Hi batrlatom,</p>

          <p>This is the folder in the repo:</p>

          <p><a rel="nofollow" href="https://github.com/pbelcak/FastBERT/tree/main/training">https://github.com/pbelcak/FastBERT/tree/main/training</a></p>

          '
        raw: 'Hi batrlatom,


          This is the folder in the repo:


          [https://github.com/pbelcak/FastBERT/tree/main/training](https://github.com/pbelcak/FastBERT/tree/main/training)


          '
        updatedAt: '2023-11-21T17:58:03.921Z'
      numEdits: 1
      reactions: []
    id: 655cd77da499c262807ce6b5
    type: comment
  author: pbelcak
  content: 'Hi batrlatom,


    This is the folder in the repo:


    [https://github.com/pbelcak/FastBERT/tree/main/training](https://github.com/pbelcak/FastBERT/tree/main/training)


    '
  created_at: 2023-11-21 16:14:53+00:00
  edited: true
  hidden: false
  id: 655cd77da499c262807ce6b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
      fullname: Starlento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Starlento
      type: user
    createdAt: '2023-11-22T07:51:05.000Z'
    data:
      edited: false
      editors:
      - Starlento
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8354406356811523
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
          fullname: Starlento
          isHf: false
          isPro: false
          name: Starlento
          type: user
        html: '<blockquote>

          <p>Hello,</p>

          <p>What your warnings are saying is that they found weights for the <code>FFF</code>
          module (<code>training/cramming/architectures/fff.py</code>) but the model
          is trying to load the weights for the <code>FFNComponent</code> module (<code>training/cramming/crammed_bert.py</code>).
          I just tried running the README example with a fresh instance and I could
          not reproduce your warnings. </p>

          <p>You''re most likely using <code>cramming</code> installed from the original
          <code>cramming</code> repository and not from the <code>training</code>
          directory of this project.</p>

          <p>To recap, these are the steps:</p>

          <ol>

          <li><code>pip uninstall cramming</code> to remove the previous version of
          cramming installed in your environment -- or just start with a fresh environment.</li>

          <li><code>cd training</code></li>

          <li><code>pip install .</code></li>

          <li>Create <code>minimal_example.py</code></li>

          <li>Paste</li>

          </ol>

          <pre><code>import cramming

          from transformers import AutoModelForMaskedLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("pbelcak/FastBERT-1x11-long")

          model = AutoModelForMaskedLM.from_pretrained("pbelcak/FastBERT-1x11-long")


          text = "Replace me by any text you''d like."

          encoded_input = tokenizer(text, return_tensors=''pt'')

          output = model(**encoded_input)

          </code></pre>

          <ol start="6">

          <li><code>python minimal_example.py</code>.</li>

          </ol>

          </blockquote>

          <p>Yes, you are right. I do install the original cramming. Sorry that I
          missed it in the README and thank you very much for the reply.</p>

          '
        raw: "> Hello,\n> \n> What your warnings are saying is that they found weights\
          \ for the `FFF` module (`training/cramming/architectures/fff.py`) but the\
          \ model is trying to load the weights for the `FFNComponent` module (`training/cramming/crammed_bert.py`).\
          \ I just tried running the README example with a fresh instance and I could\
          \ not reproduce your warnings. \n> \n> You're most likely using `cramming`\
          \ installed from the original `cramming` repository and not from the `training`\
          \ directory of this project.\n> \n> To recap, these are the steps:\n> \n\
          > 1. `pip uninstall cramming` to remove the previous version of cramming\
          \ installed in your environment -- or just start with a fresh environment.\n\
          > 2. `cd training`\n> 3. `pip install .`\n> 4. Create `minimal_example.py`\n\
          > 5. Paste \n> ```\n> import cramming\n> from transformers import AutoModelForMaskedLM,\
          \ AutoTokenizer\n> \n> tokenizer = AutoTokenizer.from_pretrained(\"pbelcak/FastBERT-1x11-long\"\
          )\n> model = AutoModelForMaskedLM.from_pretrained(\"pbelcak/FastBERT-1x11-long\"\
          )\n> \n> text = \"Replace me by any text you'd like.\"\n> encoded_input\
          \ = tokenizer(text, return_tensors='pt')\n> output = model(**encoded_input)\n\
          > ```\n> 6. `python minimal_example.py`.\n\nYes, you are right. I do install\
          \ the original cramming. Sorry that I missed it in the README and thank\
          \ you very much for the reply."
        updatedAt: '2023-11-22T07:51:05.031Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655db2e9b53face4cf15383d
    id: 655db2e9b53face4cf153834
    type: comment
  author: Starlento
  content: "> Hello,\n> \n> What your warnings are saying is that they found weights\
    \ for the `FFF` module (`training/cramming/architectures/fff.py`) but the model\
    \ is trying to load the weights for the `FFNComponent` module (`training/cramming/crammed_bert.py`).\
    \ I just tried running the README example with a fresh instance and I could not\
    \ reproduce your warnings. \n> \n> You're most likely using `cramming` installed\
    \ from the original `cramming` repository and not from the `training` directory\
    \ of this project.\n> \n> To recap, these are the steps:\n> \n> 1. `pip uninstall\
    \ cramming` to remove the previous version of cramming installed in your environment\
    \ -- or just start with a fresh environment.\n> 2. `cd training`\n> 3. `pip install\
    \ .`\n> 4. Create `minimal_example.py`\n> 5. Paste \n> ```\n> import cramming\n\
    > from transformers import AutoModelForMaskedLM, AutoTokenizer\n> \n> tokenizer\
    \ = AutoTokenizer.from_pretrained(\"pbelcak/FastBERT-1x11-long\")\n> model = AutoModelForMaskedLM.from_pretrained(\"\
    pbelcak/FastBERT-1x11-long\")\n> \n> text = \"Replace me by any text you'd like.\"\
    \n> encoded_input = tokenizer(text, return_tensors='pt')\n> output = model(**encoded_input)\n\
    > ```\n> 6. `python minimal_example.py`.\n\nYes, you are right. I do install the\
    \ original cramming. Sorry that I missed it in the README and thank you very much\
    \ for the reply."
  created_at: 2023-11-22 07:51:05+00:00
  edited: false
  hidden: false
  id: 655db2e9b53face4cf153834
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
      fullname: Starlento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Starlento
      type: user
    createdAt: '2023-11-22T07:51:05.000Z'
    data:
      status: closed
    id: 655db2e9b53face4cf15383d
    type: status-change
  author: Starlento
  created_at: 2023-11-22 07:51:05+00:00
  id: 655db2e9b53face4cf15383d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: pbelcak/UltraFastBERT-1x11-long
repo_type: model
status: closed
target_branch: null
title: Missing weights for example code
