!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leonChen
conflicting_files: null
created_at: 2023-10-16 01:13:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9c256f2d71cbb778de16ceee98da7b0.svg
      fullname: leo chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonChen
      type: user
    createdAt: '2023-10-16T02:13:27.000Z'
    data:
      edited: false
      editors:
      - leonChen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32686230540275574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9c256f2d71cbb778de16ceee98da7b0.svg
          fullname: leo chen
          isHf: false
          isPro: false
          name: leonChen
          type: user
        html: "<p>from transformers import AutoModelForCausalLM, AutoTokenizer<br>from\
          \ transformers.generation import GenerationConfig</p>\n<p>model_path = \"\
          WiNGPT2-7B-Chat\"<br>tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)<br>model = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True)<br>model = model.eval()</p>\n<h1 id=\"device-cuda\"\
          >device ='cuda'</h1>\n<p>device = 'cpu'<br>model.to(device)</p>\n<p>generation_config\
          \ = GenerationConfig(<br>  num_beams=1,<br>  top_p=0.75,<br>  top_k=30,<br>\
          \  repetition_penalty=1.1,<br>  max_new_tokens=1024<br>)<br>text = 'User:\
          \ \uFF1Axx\u3002xxx\uFF1F&lt;|endoftext|&gt;\\n Assistant: '<br>inputs =\
          \ tokenizer.encode(text, return_tensors=\"pt\").to(device)<br>outputs =\
          \ model.generate(inputs, generation_config=generation_config)<br>output\
          \ = tokenizer.decode(outputs[0])<br>inputs_str = ' '.join([str(i) for i\
          \ in inputs.tolist()])<br>response = output.replace(inputs_str, '')</p>\n\
          <p>RuntimeError                              Traceback (most recent call\
          \ last)<br>Cell In[4], line 3<br>      2 inputs = tokenizer.encode(text,\
          \ return_tensors=\"pt\").to(device)<br>----&gt; 3 outputs = model.generate(inputs,\
          \ generation_config=generation_config)<br>      4 output = tokenizer.decode(outputs[0])<br>\
          \      5 inputs_str = ' '.join([str(i) for i in inputs.tolist()])</p>\n\
          <p>File ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:1136,\
          \ in QWenLMHeadModel.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)<br>   1133     else:<br>   1134         logits_processor.append(stop_words_logits_processor)<br>-&gt;\
          \ 1136 return super().generate(<br>   1137     inputs,<br>   1138     generation_config=generation_config,<br>\
          \   1139     logits_processor=logits_processor,<br>   1140     stopping_criteria=stopping_criteria,<br>\
          \   1141     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,<br>   1142\
          \     synced_gpus=synced_gpus,<br>   1143     assistant_model=assistant_model,<br>\
          \   1144     streamer=streamer,<br>   1145     **kwargs,<br>   1146 )</p>\n\
          <p>File ~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27,\
          \ in _DecoratorContextManager.<strong>call</strong>..decorate_context(*args,\
          \ **kwargs)<br>     24 @functools.wraps(func)<br>     25 def decorate_context(*args,\
          \ **kwargs):<br>     26     with self.clone():<br>---&gt; 27         return\
          \ func(*args, **kwargs)</p>\n<p>File ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194,\
          \ in Module._call_impl(self, *input, **kwargs)<br>   1190 # If we don't\
          \ have any hooks, we want to skip the rest of the logic in<br>   1191 #\
          \ this function, and just call forward.<br>   1192 if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks<br>\
          \   1193         or _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt;\
          \ 1194     return forward_call(*input, **kwargs)<br>   1195 # Do not call\
          \ functions when jit is used<br>   1196 full_backward_hooks, non_full_backward_hooks\
          \ = [], []</p>\n<p>File ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:926,\
          \ in QWenLMHeadModel.forward(self, input_ids, past_key_values, attention_mask,\
          \ token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states,\
          \ encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)<br>    904 def forward(<br>    905     self,<br>    906 \
          \    input_ids: Optional[torch.LongTensor] = None,<br>   (...)<br>    919\
          \     return_dict: Optional[bool] = None,<br>    920 ) -&gt; Union[Tuple,\
          \ CausalLMOutputWithPast]:<br>    922     return_dict = (<br>    923   \
          \      return_dict if return_dict is not None else self.config.use_return_dict<br>\
          \    924     )<br>--&gt; 926     transformer_outputs = self.transformer(<br>\
          \    927         input_ids,<br>    928         past_key_values=past_key_values,<br>\
          \    929         attention_mask=attention_mask,<br>    930         token_type_ids=token_type_ids,<br>\
          \    931         position_ids=position_ids,<br>    932         head_mask=head_mask,<br>\
          \    933         inputs_embeds=inputs_embeds,<br>    934         encoder_hidden_states=encoder_hidden_states,<br>\
          \    935         encoder_attention_mask=encoder_attention_mask,<br>    936\
          \         use_cache=use_cache,<br>    937         output_attentions=output_attentions,<br>\
          \    938         output_hidden_states=output_hidden_states,<br>    939 \
          \        return_dict=return_dict,<br>    940     )<br>    941     hidden_states\
          \ = transformer_outputs[0]<br>    943     lm_logits = self.lm_head(hidden_states)</p>\n\
          <p>File ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:762,\
          \ in QWenModel.forward(self, input_ids, past_key_values, attention_mask,\
          \ token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states,\
          \ encoder_attention_mask, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)<br>    750     outputs = torch.utils.checkpoint.checkpoint(<br>\
          \    751         create_custom_forward(block),<br>    752         hidden_states,<br>\
          \   (...)<br>    759         encoder_attention_mask,<br>    760     )<br>\
          \    761 else:<br>--&gt; 762     outputs = block(<br>    763         hidden_states,<br>\
          \    764         layer_past=layer_past,<br>    765         rotary_pos_emb=rotary_pos_emb,<br>\
          \    766         registered_causal_mask=self.registered_causal_mask,<br>\
          \    767         attention_mask=attention_mask,<br>    768         head_mask=head_mask[i],<br>\
          \    769         encoder_hidden_states=encoder_hidden_states,<br>    770\
          \         encoder_attention_mask=encoder_attention_mask,<br>    771    \
          \     use_cache=use_cache,<br>    772         output_attentions=output_attentions,<br>\
          \    773     )<br>    775 hidden_states = outputs[0]<br>    776 if use_cache\
          \ is True:</p>\n<p>File ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:483,\
          \ in QWenBlock.forward(self, hidden_states, rotary_pos_emb, registered_causal_mask,\
          \ layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask,\
          \ use_cache, output_attentions)<br>    468 def forward(<br>    469     self,<br>\
          \    470     hidden_states: Optional[Tuple[torch.FloatTensor]],<br>   (...)<br>\
          \    479     output_attentions: Optional[bool] = False,<br>    480 ):<br>\
          \    481     layernorm_output = self.ln_1(hidden_states)<br>--&gt; 483 \
          \    attn_outputs = self.attn(<br>    484         layernorm_output,<br>\
          \    485         rotary_pos_emb,<br>    486         registered_causal_mask=registered_causal_mask,<br>\
          \    487         layer_past=layer_past,<br>    488         attention_mask=attention_mask,<br>\
          \    489         head_mask=head_mask,<br>    490         use_cache=use_cache,<br>\
          \    491         output_attentions=output_attentions,<br>    492     )<br>\
          \    493     attn_output = attn_outputs[0]<br>    495     outputs = attn_outputs[1:]</p>\n\
          <p>File ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:349,\
          \ in QWenAttention.forward(self, hidden_states, rotary_pos_emb, registered_causal_mask,\
          \ layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask,\
          \ output_attentions, use_cache)<br>    335 def forward(<br>    336     self,<br>\
          \    337     hidden_states: Optional[Tuple[torch.FloatTensor]],<br>   (...)<br>\
          \    346     use_cache: Optional[bool] = False,<br>    347 ):<br>--&gt;\
          \ 349     mixed_x_layer = self.c_attn(hidden_states)<br>    351     query,\
          \ key, value = mixed_x_layer.split(self.split_size, dim=2)<br>    353  \
          \   query = self._split_heads(query, self.num_heads, self.head_dim)</p>\n\
          <p>File ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194,\
          \ in Module._call_impl(self, *input, **kwargs)<br>   1190 # If we don't\
          \ have any hooks, we want to skip the rest of the logic in<br>   1191 #\
          \ this function, and just call forward.<br>   1192 if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks<br>\
          \   1193         or _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt;\
          \ 1194     return forward_call(*input, **kwargs)<br>   1195 # Do not call\
          \ functions when jit is used<br>   1196 full_backward_hooks, non_full_backward_hooks\
          \ = [], []</p>\n<p>File ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114,\
          \ in Linear.forward(self, input)<br>    113 def forward(self, input: Tensor)\
          \ -&gt; Tensor:<br>--&gt; 114     return F.linear(input, self.weight, self.bias)</p>\n\
          <p>RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'</p>\n"
        raw: "from transformers import AutoModelForCausalLM, AutoTokenizer\r\nfrom\
          \ transformers.generation import GenerationConfig\r\n\r\nmodel_path = \"\
          WiNGPT2-7B-Chat\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True)\r\nmodel = model.eval()\r\n# device ='cuda'\r\n\
          device = 'cpu'\r\nmodel.to(device)\r\n\r\ngeneration_config = GenerationConfig(\r\
          \n  num_beams=1,\r\n  top_p=0.75,\r\n  top_k=30,\r\n  repetition_penalty=1.1,\r\
          \n  max_new_tokens=1024\r\n)\r\ntext = 'User: \uFF1Axx\u3002xxx\uFF1F<|endoftext|>\\\
          n Assistant: '\r\ninputs = tokenizer.encode(text, return_tensors=\"pt\"\
          ).to(device)\r\noutputs = model.generate(inputs, generation_config=generation_config)\r\
          \noutput = tokenizer.decode(outputs[0])\r\ninputs_str = ' '.join([str(i)\
          \ for i in inputs.tolist()])\r\nresponse = output.replace(inputs_str, '')\r\
          \n\r\n\r\nRuntimeError                              Traceback (most recent\
          \ call last)\r\nCell In[4], line 3\r\n      2 inputs = tokenizer.encode(text,\
          \ return_tensors=\"pt\").to(device)\r\n----> 3 outputs = model.generate(inputs,\
          \ generation_config=generation_config)\r\n      4 output = tokenizer.decode(outputs[0])\r\
          \n      5 inputs_str = ' '.join([str(i) for i in inputs.tolist()])\r\n\r\
          \nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:1136,\
          \ in QWenLMHeadModel.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\r\n   1133     else:\r\n   1134         logits_processor.append(stop_words_logits_processor)\r\
          \n-> 1136 return super().generate(\r\n   1137     inputs,\r\n   1138   \
          \  generation_config=generation_config,\r\n   1139     logits_processor=logits_processor,\r\
          \n   1140     stopping_criteria=stopping_criteria,\r\n   1141     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\
          \n   1142     synced_gpus=synced_gpus,\r\n   1143     assistant_model=assistant_model,\r\
          \n   1144     streamer=streamer,\r\n   1145     **kwargs,\r\n   1146 )\r\
          \n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27,\
          \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args,\
          \ **kwargs)\r\n     24 @functools.wraps(func)\r\n     25 def decorate_context(*args,\
          \ **kwargs):\r\n     26     with self.clone():\r\n---> 27         return\
          \ func(*args, **kwargs)\r\n\r\n\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194,\
          \ in Module._call_impl(self, *input, **kwargs)\r\n   1190 # If we don't\
          \ have any hooks, we want to skip the rest of the logic in\r\n   1191 #\
          \ this function, and just call forward.\r\n   1192 if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
          \n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1194     return forward_call(*input, **kwargs)\r\n   1195 # Do not\
          \ call functions when jit is used\r\n   1196 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:926,\
          \ in QWenLMHeadModel.forward(self, input_ids, past_key_values, attention_mask,\
          \ token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states,\
          \ encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)\r\n    904 def forward(\r\n    905     self,\r\n    906 \
          \    input_ids: Optional[torch.LongTensor] = None,\r\n   (...)\r\n    919\
          \     return_dict: Optional[bool] = None,\r\n    920 ) -> Union[Tuple, CausalLMOutputWithPast]:\r\
          \n    922     return_dict = (\r\n    923         return_dict if return_dict\
          \ is not None else self.config.use_return_dict\r\n    924     )\r\n--> 926\
          \     transformer_outputs = self.transformer(\r\n    927         input_ids,\r\
          \n    928         past_key_values=past_key_values,\r\n    929         attention_mask=attention_mask,\r\
          \n    930         token_type_ids=token_type_ids,\r\n    931         position_ids=position_ids,\r\
          \n    932         head_mask=head_mask,\r\n    933         inputs_embeds=inputs_embeds,\r\
          \n    934         encoder_hidden_states=encoder_hidden_states,\r\n    935\
          \         encoder_attention_mask=encoder_attention_mask,\r\n    936    \
          \     use_cache=use_cache,\r\n    937         output_attentions=output_attentions,\r\
          \n    938         output_hidden_states=output_hidden_states,\r\n    939\
          \         return_dict=return_dict,\r\n    940     )\r\n    941     hidden_states\
          \ = transformer_outputs[0]\r\n    943     lm_logits = self.lm_head(hidden_states)\r\
          \n\r\n\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:762,\
          \ in QWenModel.forward(self, input_ids, past_key_values, attention_mask,\
          \ token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states,\
          \ encoder_attention_mask, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)\r\n    750     outputs = torch.utils.checkpoint.checkpoint(\r\
          \n    751         create_custom_forward(block),\r\n    752         hidden_states,\r\
          \n   (...)\r\n    759         encoder_attention_mask,\r\n    760     )\r\
          \n    761 else:\r\n--> 762     outputs = block(\r\n    763         hidden_states,\r\
          \n    764         layer_past=layer_past,\r\n    765         rotary_pos_emb=rotary_pos_emb,\r\
          \n    766         registered_causal_mask=self.registered_causal_mask,\r\n\
          \    767         attention_mask=attention_mask,\r\n    768         head_mask=head_mask[i],\r\
          \n    769         encoder_hidden_states=encoder_hidden_states,\r\n    770\
          \         encoder_attention_mask=encoder_attention_mask,\r\n    771    \
          \     use_cache=use_cache,\r\n    772         output_attentions=output_attentions,\r\
          \n    773     )\r\n    775 hidden_states = outputs[0]\r\n    776 if use_cache\
          \ is True:\r\n\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:483,\
          \ in QWenBlock.forward(self, hidden_states, rotary_pos_emb, registered_causal_mask,\
          \ layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask,\
          \ use_cache, output_attentions)\r\n    468 def forward(\r\n    469     self,\r\
          \n    470     hidden_states: Optional[Tuple[torch.FloatTensor]],\r\n   (...)\r\
          \n    479     output_attentions: Optional[bool] = False,\r\n    480 ):\r\
          \n    481     layernorm_output = self.ln_1(hidden_states)\r\n--> 483   \
          \  attn_outputs = self.attn(\r\n    484         layernorm_output,\r\n  \
          \  485         rotary_pos_emb,\r\n    486         registered_causal_mask=registered_causal_mask,\r\
          \n    487         layer_past=layer_past,\r\n    488         attention_mask=attention_mask,\r\
          \n    489         head_mask=head_mask,\r\n    490         use_cache=use_cache,\r\
          \n    491         output_attentions=output_attentions,\r\n    492     )\r\
          \n    493     attn_output = attn_outputs[0]\r\n    495     outputs = attn_outputs[1:]\r\
          \n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:349,\
          \ in QWenAttention.forward(self, hidden_states, rotary_pos_emb, registered_causal_mask,\
          \ layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask,\
          \ output_attentions, use_cache)\r\n    335 def forward(\r\n    336     self,\r\
          \n    337     hidden_states: Optional[Tuple[torch.FloatTensor]],\r\n   (...)\r\
          \n    346     use_cache: Optional[bool] = False,\r\n    347 ):\r\n--> 349\
          \     mixed_x_layer = self.c_attn(hidden_states)\r\n    351     query, key,\
          \ value = mixed_x_layer.split(self.split_size, dim=2)\r\n    353     query\
          \ = self._split_heads(query, self.num_heads, self.head_dim)\r\n\r\nFile\
          \ ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194,\
          \ in Module._call_impl(self, *input, **kwargs)\r\n   1190 # If we don't\
          \ have any hooks, we want to skip the rest of the logic in\r\n   1191 #\
          \ this function, and just call forward.\r\n   1192 if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
          \n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1194     return forward_call(*input, **kwargs)\r\n   1195 # Do not\
          \ call functions when jit is used\r\n   1196 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114,\
          \ in Linear.forward(self, input)\r\n    113 def forward(self, input: Tensor)\
          \ -> Tensor:\r\n--> 114     return F.linear(input, self.weight, self.bias)\r\
          \n\r\nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\r\n\r\
          \n"
        updatedAt: '2023-10-16T02:13:27.012Z'
      numEdits: 0
      reactions: []
    id: 652c9c47e0f39e3bae714034
    type: comment
  author: leonChen
  content: "from transformers import AutoModelForCausalLM, AutoTokenizer\r\nfrom transformers.generation\
    \ import GenerationConfig\r\n\r\nmodel_path = \"WiNGPT2-7B-Chat\"\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\r\
    \nmodel = model.eval()\r\n# device ='cuda'\r\ndevice = 'cpu'\r\nmodel.to(device)\r\
    \n\r\ngeneration_config = GenerationConfig(\r\n  num_beams=1,\r\n  top_p=0.75,\r\
    \n  top_k=30,\r\n  repetition_penalty=1.1,\r\n  max_new_tokens=1024\r\n)\r\ntext\
    \ = 'User: \uFF1Axx\u3002xxx\uFF1F<|endoftext|>\\n Assistant: '\r\ninputs = tokenizer.encode(text,\
    \ return_tensors=\"pt\").to(device)\r\noutputs = model.generate(inputs, generation_config=generation_config)\r\
    \noutput = tokenizer.decode(outputs[0])\r\ninputs_str = ' '.join([str(i) for i\
    \ in inputs.tolist()])\r\nresponse = output.replace(inputs_str, '')\r\n\r\n\r\n\
    RuntimeError                              Traceback (most recent call last)\r\n\
    Cell In[4], line 3\r\n      2 inputs = tokenizer.encode(text, return_tensors=\"\
    pt\").to(device)\r\n----> 3 outputs = model.generate(inputs, generation_config=generation_config)\r\
    \n      4 output = tokenizer.decode(outputs[0])\r\n      5 inputs_str = ' '.join([str(i)\
    \ for i in inputs.tolist()])\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:1136,\
    \ in QWenLMHeadModel.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ **kwargs)\r\n   1133     else:\r\n   1134         logits_processor.append(stop_words_logits_processor)\r\
    \n-> 1136 return super().generate(\r\n   1137     inputs,\r\n   1138     generation_config=generation_config,\r\
    \n   1139     logits_processor=logits_processor,\r\n   1140     stopping_criteria=stopping_criteria,\r\
    \n   1141     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n   1142  \
    \   synced_gpus=synced_gpus,\r\n   1143     assistant_model=assistant_model,\r\
    \n   1144     streamer=streamer,\r\n   1145     **kwargs,\r\n   1146 )\r\n\r\n\
    File ~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27,\
    \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args, **kwargs)\r\
    \n     24 @functools.wraps(func)\r\n     25 def decorate_context(*args, **kwargs):\r\
    \n     26     with self.clone():\r\n---> 27         return func(*args, **kwargs)\r\
    \n\r\n\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194,\
    \ in Module._call_impl(self, *input, **kwargs)\r\n   1190 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1191 # this function,\
    \ and just call forward.\r\n   1192 if not (self._backward_hooks or self._forward_hooks\
    \ or self._forward_pre_hooks or _global_backward_hooks\r\n   1193         or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1194     return forward_call(*input, **kwargs)\r\
    \n   1195 # Do not call functions when jit is used\r\n   1196 full_backward_hooks,\
    \ non_full_backward_hooks = [], []\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:926,\
    \ in QWenLMHeadModel.forward(self, input_ids, past_key_values, attention_mask,\
    \ token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states,\
    \ encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states,\
    \ return_dict)\r\n    904 def forward(\r\n    905     self,\r\n    906     input_ids:\
    \ Optional[torch.LongTensor] = None,\r\n   (...)\r\n    919     return_dict: Optional[bool]\
    \ = None,\r\n    920 ) -> Union[Tuple, CausalLMOutputWithPast]:\r\n    922   \
    \  return_dict = (\r\n    923         return_dict if return_dict is not None else\
    \ self.config.use_return_dict\r\n    924     )\r\n--> 926     transformer_outputs\
    \ = self.transformer(\r\n    927         input_ids,\r\n    928         past_key_values=past_key_values,\r\
    \n    929         attention_mask=attention_mask,\r\n    930         token_type_ids=token_type_ids,\r\
    \n    931         position_ids=position_ids,\r\n    932         head_mask=head_mask,\r\
    \n    933         inputs_embeds=inputs_embeds,\r\n    934         encoder_hidden_states=encoder_hidden_states,\r\
    \n    935         encoder_attention_mask=encoder_attention_mask,\r\n    936  \
    \       use_cache=use_cache,\r\n    937         output_attentions=output_attentions,\r\
    \n    938         output_hidden_states=output_hidden_states,\r\n    939      \
    \   return_dict=return_dict,\r\n    940     )\r\n    941     hidden_states = transformer_outputs[0]\r\
    \n    943     lm_logits = self.lm_head(hidden_states)\r\n\r\n\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:762,\
    \ in QWenModel.forward(self, input_ids, past_key_values, attention_mask, token_type_ids,\
    \ position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask,\
    \ use_cache, output_attentions, output_hidden_states, return_dict)\r\n    750\
    \     outputs = torch.utils.checkpoint.checkpoint(\r\n    751         create_custom_forward(block),\r\
    \n    752         hidden_states,\r\n   (...)\r\n    759         encoder_attention_mask,\r\
    \n    760     )\r\n    761 else:\r\n--> 762     outputs = block(\r\n    763  \
    \       hidden_states,\r\n    764         layer_past=layer_past,\r\n    765  \
    \       rotary_pos_emb=rotary_pos_emb,\r\n    766         registered_causal_mask=self.registered_causal_mask,\r\
    \n    767         attention_mask=attention_mask,\r\n    768         head_mask=head_mask[i],\r\
    \n    769         encoder_hidden_states=encoder_hidden_states,\r\n    770    \
    \     encoder_attention_mask=encoder_attention_mask,\r\n    771         use_cache=use_cache,\r\
    \n    772         output_attentions=output_attentions,\r\n    773     )\r\n  \
    \  775 hidden_states = outputs[0]\r\n    776 if use_cache is True:\r\n\r\n\r\n\
    File ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:483,\
    \ in QWenBlock.forward(self, hidden_states, rotary_pos_emb, registered_causal_mask,\
    \ layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask,\
    \ use_cache, output_attentions)\r\n    468 def forward(\r\n    469     self,\r\
    \n    470     hidden_states: Optional[Tuple[torch.FloatTensor]],\r\n   (...)\r\
    \n    479     output_attentions: Optional[bool] = False,\r\n    480 ):\r\n   \
    \ 481     layernorm_output = self.ln_1(hidden_states)\r\n--> 483     attn_outputs\
    \ = self.attn(\r\n    484         layernorm_output,\r\n    485         rotary_pos_emb,\r\
    \n    486         registered_causal_mask=registered_causal_mask,\r\n    487  \
    \       layer_past=layer_past,\r\n    488         attention_mask=attention_mask,\r\
    \n    489         head_mask=head_mask,\r\n    490         use_cache=use_cache,\r\
    \n    491         output_attentions=output_attentions,\r\n    492     )\r\n  \
    \  493     attn_output = attn_outputs[0]\r\n    495     outputs = attn_outputs[1:]\r\
    \n\r\nFile ~/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat/modeling_qwen.py:349,\
    \ in QWenAttention.forward(self, hidden_states, rotary_pos_emb, registered_causal_mask,\
    \ layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask,\
    \ output_attentions, use_cache)\r\n    335 def forward(\r\n    336     self,\r\
    \n    337     hidden_states: Optional[Tuple[torch.FloatTensor]],\r\n   (...)\r\
    \n    346     use_cache: Optional[bool] = False,\r\n    347 ):\r\n--> 349    \
    \ mixed_x_layer = self.c_attn(hidden_states)\r\n    351     query, key, value\
    \ = mixed_x_layer.split(self.split_size, dim=2)\r\n    353     query = self._split_heads(query,\
    \ self.num_heads, self.head_dim)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194,\
    \ in Module._call_impl(self, *input, **kwargs)\r\n   1190 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1191 # this function,\
    \ and just call forward.\r\n   1192 if not (self._backward_hooks or self._forward_hooks\
    \ or self._forward_pre_hooks or _global_backward_hooks\r\n   1193         or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1194     return forward_call(*input, **kwargs)\r\
    \n   1195 # Do not call functions when jit is used\r\n   1196 full_backward_hooks,\
    \ non_full_backward_hooks = [], []\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114,\
    \ in Linear.forward(self, input)\r\n    113 def forward(self, input: Tensor) ->\
    \ Tensor:\r\n--> 114     return F.linear(input, self.weight, self.bias)\r\n\r\n\
    RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\r\n\r\n"
  created_at: 2023-10-16 01:13:27+00:00
  edited: false
  hidden: false
  id: 652c9c47e0f39e3bae714034
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee3af494cd400d931def904fb4dd25e8.svg
      fullname: Winning Health AI Research
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: winninghealth
      type: user
    createdAt: '2023-10-16T02:36:31.000Z'
    data:
      edited: false
      editors:
      - winninghealth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9782384634017944
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee3af494cd400d931def904fb4dd25e8.svg
          fullname: Winning Health AI Research
          isHf: false
          isPro: false
          name: winninghealth
          type: user
        html: '<p>set device to "cuda" as the model is loaded as fp16 but addmm_impl_cpu_
          ops does not support half(fp16) in cpu mode.</p>

          '
        raw: set device to "cuda" as the model is loaded as fp16 but addmm_impl_cpu_
          ops does not support half(fp16) in cpu mode.
        updatedAt: '2023-10-16T02:36:31.930Z'
      numEdits: 0
      reactions: []
    id: 652ca1af2aa5b27c7702a6a8
    type: comment
  author: winninghealth
  content: set device to "cuda" as the model is loaded as fp16 but addmm_impl_cpu_
    ops does not support half(fp16) in cpu mode.
  created_at: 2023-10-16 01:36:31+00:00
  edited: false
  hidden: false
  id: 652ca1af2aa5b27c7702a6a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9c256f2d71cbb778de16ceee98da7b0.svg
      fullname: leo chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonChen
      type: user
    createdAt: '2023-10-16T02:37:54.000Z'
    data:
      edited: false
      editors:
      - leonChen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9468798041343689
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9c256f2d71cbb778de16ceee98da7b0.svg
          fullname: leo chen
          isHf: false
          isPro: false
          name: leonChen
          type: user
        html: '<p>thanks</p>

          '
        raw: thanks
        updatedAt: '2023-10-16T02:37:54.072Z'
      numEdits: 0
      reactions: []
    id: 652ca202f6390fe0480be2a3
    type: comment
  author: leonChen
  content: thanks
  created_at: 2023-10-16 01:37:54+00:00
  edited: false
  hidden: false
  id: 652ca202f6390fe0480be2a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ee3af494cd400d931def904fb4dd25e8.svg
      fullname: Winning Health AI Research
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: winninghealth
      type: user
    createdAt: '2023-11-07T04:05:12.000Z'
    data:
      status: closed
    id: 6549b778c26b9e805de40621
    type: status-change
  author: winninghealth
  created_at: 2023-11-07 04:05:12+00:00
  id: 6549b778c26b9e805de40621
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: winninghealth/WiNGPT2-7B-Chat
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: "addmm_impl_cpu_" not implemented for ''Half'''
