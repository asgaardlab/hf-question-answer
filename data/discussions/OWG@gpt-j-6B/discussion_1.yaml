!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pankajdev007
conflicting_files: null
created_at: 2023-01-25 05:58:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/802e3714225d6d42c5385e6d0ac6966a.svg
      fullname: Pankaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pankajdev007
      type: user
    createdAt: '2023-01-25T05:58:25.000Z'
    data:
      edited: false
      editors:
      - pankajdev007
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/802e3714225d6d42c5385e6d0ac6966a.svg
          fullname: Pankaj
          isHf: false
          isPro: false
          name: pankajdev007
          type: user
        html: '<p>Is the ORT model fp32 or fp16, if it is fp32 can you share some
          way to export it to fp16 to able to fit in 16GB GPU?</p>

          '
        raw: Is the ORT model fp32 or fp16, if it is fp32 can you share some way to
          export it to fp16 to able to fit in 16GB GPU?
        updatedAt: '2023-01-25T05:58:25.616Z'
      numEdits: 0
      reactions: []
    id: 63d0c501623a3d1d11777980
    type: comment
  author: pankajdev007
  content: Is the ORT model fp32 or fp16, if it is fp32 can you share some way to
    export it to fp16 to able to fit in 16GB GPU?
  created_at: 2023-01-25 05:58:25+00:00
  edited: false
  hidden: false
  id: 63d0c501623a3d1d11777980
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-01-25T08:15:02.000Z'
    data:
      edited: true
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pankajdev007&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pankajdev007\"\
          >@<span class=\"underline\">pankajdev007</span></a></span>\n\n\t</span></span>,\
          \ I used this <a href=\"https://huggingface.co/EleutherAI/gpt-j-6B/tree/main\"\
          >repo</a> as a base to export the model to ONNX, so I do believe it's fp32.</p>\n\
          <p>Check the weights by doing this after the model load:</p>\n<pre><code>print(model.dtype)\n\
          </code></pre>\n<p>You can force the weights of your model to be fp16 by\
          \ doing this in PyTorch:</p>\n<pre><code>net = Model()\nnet.half()\n</code></pre>\n\
          <p>So you can probably do it with Transformers too!</p>\n<p>(I found this\
          \ on this <a rel=\"nofollow\" href=\"https://discuss.pytorch.org/t/32-float-weight-convert-16-float-model/88754\"\
          >PyTorch thread</a>. It would be best if you were careful about prediction\
          \ after conversion because of potential NaNs).</p>\n"
        raw: 'Hi @pankajdev007, I used this [repo](https://huggingface.co/EleutherAI/gpt-j-6B/tree/main)
          as a base to export the model to ONNX, so I do believe it''s fp32.


          Check the weights by doing this after the model load:


          ```

          print(model.dtype)

          ```


          You can force the weights of your model to be fp16 by doing this in PyTorch:


          ```

          net = Model()

          net.half()

          ```


          So you can probably do it with Transformers too!


          (I found this on this [PyTorch thread](https://discuss.pytorch.org/t/32-float-weight-convert-16-float-model/88754).
          It would be best if you were careful about prediction after conversion because
          of potential NaNs).'
        updatedAt: '2023-01-25T08:17:01.092Z'
      numEdits: 2
      reactions: []
    id: 63d0e506119416cdbe096694
    type: comment
  author: chainyo
  content: 'Hi @pankajdev007, I used this [repo](https://huggingface.co/EleutherAI/gpt-j-6B/tree/main)
    as a base to export the model to ONNX, so I do believe it''s fp32.


    Check the weights by doing this after the model load:


    ```

    print(model.dtype)

    ```


    You can force the weights of your model to be fp16 by doing this in PyTorch:


    ```

    net = Model()

    net.half()

    ```


    So you can probably do it with Transformers too!


    (I found this on this [PyTorch thread](https://discuss.pytorch.org/t/32-float-weight-convert-16-float-model/88754).
    It would be best if you were careful about prediction after conversion because
    of potential NaNs).'
  created_at: 2023-01-25 08:15:02+00:00
  edited: true
  hidden: false
  id: 63d0e506119416cdbe096694
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/802e3714225d6d42c5385e6d0ac6966a.svg
      fullname: Pankaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pankajdev007
      type: user
    createdAt: '2023-01-25T08:54:38.000Z'
    data:
      edited: false
      editors:
      - pankajdev007
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/802e3714225d6d42c5385e6d0ac6966a.svg
          fullname: Pankaj
          isHf: false
          isPro: false
          name: pankajdev007
          type: user
        html: '<p>Yes.. I tried model.half() but it is not applying on ONNX model,
          but works on normal transformer model.. I need a way to convert the GPT-j
          to onnx fp16. I used optimum onnx: python -m optimum.exporters.onnx --task
          causal-lm-with-past --for-ort --model gpt-j-6B gptj16_onnx/</p>

          <p> to convert, but did not get a way to convert it to FP16</p>

          '
        raw: "Yes.. I tried model.half() but it is not applying on ONNX model, but\
          \ works on normal transformer model.. I need a way to convert the GPT-j\
          \ to onnx fp16. I used optimum onnx: python -m optimum.exporters.onnx --task\
          \ causal-lm-with-past --for-ort --model gpt-j-6B gptj16_onnx/\n\n to convert,\
          \ but did not get a way to convert it to FP16"
        updatedAt: '2023-01-25T08:54:38.715Z'
      numEdits: 0
      reactions: []
    id: 63d0ee4edae2635f21975e94
    type: comment
  author: pankajdev007
  content: "Yes.. I tried model.half() but it is not applying on ONNX model, but works\
    \ on normal transformer model.. I need a way to convert the GPT-j to onnx fp16.\
    \ I used optimum onnx: python -m optimum.exporters.onnx --task causal-lm-with-past\
    \ --for-ort --model gpt-j-6B gptj16_onnx/\n\n to convert, but did not get a way\
    \ to convert it to FP16"
  created_at: 2023-01-25 08:54:38+00:00
  edited: false
  hidden: false
  id: 63d0ee4edae2635f21975e94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-01-25T14:09:42.000Z'
    data:
      edited: false
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: '<p>Could you try to load the PyTorch model, apply <code>model.half()</code>,
          save the PyTorch model, and then export this saved model to ONNX?</p>

          <p>(To store the saved model, you can create a new HF repo for that)</p>

          '
        raw: 'Could you try to load the PyTorch model, apply `model.half()`, save
          the PyTorch model, and then export this saved model to ONNX?


          (To store the saved model, you can create a new HF repo for that)'
        updatedAt: '2023-01-25T14:09:42.732Z'
      numEdits: 0
      reactions: []
    id: 63d1382662f0de677d86642a
    type: comment
  author: chainyo
  content: 'Could you try to load the PyTorch model, apply `model.half()`, save the
    PyTorch model, and then export this saved model to ONNX?


    (To store the saved model, you can create a new HF repo for that)'
  created_at: 2023-01-25 14:09:42+00:00
  edited: false
  hidden: false
  id: 63d1382662f0de677d86642a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: OWG/gpt-j-6B
repo_type: model
status: open
target_branch: null
title: Is it FP32 or FP16 version
