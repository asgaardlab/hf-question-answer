!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 64jcl
conflicting_files: null
created_at: 2023-11-17 12:16:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84e1306c34a0b8986edbcbdf2b496846.svg
      fullname: "John Christian L\xF8nningdal"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 64jcl
      type: user
    createdAt: '2023-11-17T12:16:35.000Z'
    data:
      edited: false
      editors:
      - 64jcl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9755972623825073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84e1306c34a0b8986edbcbdf2b496846.svg
          fullname: "John Christian L\xF8nningdal"
          isHf: false
          isPro: false
          name: 64jcl
          type: user
        html: '<p>I am having some great success with this model using it for e.g.
          simulating calls to functions to parse output and execute them (for drawing
          pictures, doing math and remembering stuff). But I have noticed that one
          issue I often have is that if it does a mistake it keeps doing them over
          and over, indicating that the last messages and its replies (which I also
          send to it) get so much weigh in its completions that it keeps doing the
          same mistakes even when I try to correct it. I''d have to remove all past
          dialoge to "break it free". Been considering detecting when I correct it
          and at least auto-zap its own wrong reply to my history so, but perhaps
          there is some parameter I can adjust to make it listen more to my message
          about its mistake than doing it again?</p>

          <p>Also I have noticed it has a tendency to end messages with "Let me know
          if there''s anything else I can help you with!" (and many variations of
          this) a lot. I have added things like "Do not repeat that you can help"
          in my system message with varying success. I am currently playing a bit
          with increasing the frequency_penalty which was default 0. Perhaps that
          can help. :)</p>

          <p>Considering its seemingly weighing the last messages highly I have started
          moving stuff from the system block down as if the important stuff is closer
          to completion start, as it had tendencies to ignore the system message stuff
          after a while. I am experimenting with trimming down the message stack to
          a minimum although I have had some "dummy" messages in the list first where
          the assistant is supposedly answering perfectly when it comes to how it
          should output the function calls to an example user input. It seems to help
          although I see prompt crafting is a skill on its own.</p>

          <p>I am considering playing with system contexts to add and remove info
          in it depending on where dialogue is going so that I can give it info about
          other functions available in a given context.</p>

          <p>The ability to run these models locally and play with them for free is
          fantastic though. I am using LM Studio which which has a server mode excellent
          for this.</p>

          '
        raw: "I am having some great success with this model using it for e.g. simulating\
          \ calls to functions to parse output and execute them (for drawing pictures,\
          \ doing math and remembering stuff). But I have noticed that one issue I\
          \ often have is that if it does a mistake it keeps doing them over and over,\
          \ indicating that the last messages and its replies (which I also send to\
          \ it) get so much weigh in its completions that it keeps doing the same\
          \ mistakes even when I try to correct it. I'd have to remove all past dialoge\
          \ to \"break it free\". Been considering detecting when I correct it and\
          \ at least auto-zap its own wrong reply to my history so, but perhaps there\
          \ is some parameter I can adjust to make it listen more to my message about\
          \ its mistake than doing it again?\r\n\r\nAlso I have noticed it has a tendency\
          \ to end messages with \"Let me know if there's anything else I can help\
          \ you with!\" (and many variations of this) a lot. I have added things like\
          \ \"Do not repeat that you can help\" in my system message with varying\
          \ success. I am currently playing a bit with increasing the frequency_penalty\
          \ which was default 0. Perhaps that can help. :)\r\n\r\nConsidering its\
          \ seemingly weighing the last messages highly I have started moving stuff\
          \ from the system block down as if the important stuff is closer to completion\
          \ start, as it had tendencies to ignore the system message stuff after a\
          \ while. I am experimenting with trimming down the message stack to a minimum\
          \ although I have had some \"dummy\" messages in the list first where the\
          \ assistant is supposedly answering perfectly when it comes to how it should\
          \ output the function calls to an example user input. It seems to help although\
          \ I see prompt crafting is a skill on its own.\r\n\r\nI am considering playing\
          \ with system contexts to add and remove info in it depending on where dialogue\
          \ is going so that I can give it info about other functions available in\
          \ a given context.\r\n\r\nThe ability to run these models locally and play\
          \ with them for free is fantastic though. I am using LM Studio which which\
          \ has a server mode excellent for this."
        updatedAt: '2023-11-17T12:16:35.086Z'
      numEdits: 0
      reactions: []
    id: 655759a3ee35f7d8bc08d6cb
    type: comment
  author: 64jcl
  content: "I am having some great success with this model using it for e.g. simulating\
    \ calls to functions to parse output and execute them (for drawing pictures, doing\
    \ math and remembering stuff). But I have noticed that one issue I often have\
    \ is that if it does a mistake it keeps doing them over and over, indicating that\
    \ the last messages and its replies (which I also send to it) get so much weigh\
    \ in its completions that it keeps doing the same mistakes even when I try to\
    \ correct it. I'd have to remove all past dialoge to \"break it free\". Been considering\
    \ detecting when I correct it and at least auto-zap its own wrong reply to my\
    \ history so, but perhaps there is some parameter I can adjust to make it listen\
    \ more to my message about its mistake than doing it again?\r\n\r\nAlso I have\
    \ noticed it has a tendency to end messages with \"Let me know if there's anything\
    \ else I can help you with!\" (and many variations of this) a lot. I have added\
    \ things like \"Do not repeat that you can help\" in my system message with varying\
    \ success. I am currently playing a bit with increasing the frequency_penalty\
    \ which was default 0. Perhaps that can help. :)\r\n\r\nConsidering its seemingly\
    \ weighing the last messages highly I have started moving stuff from the system\
    \ block down as if the important stuff is closer to completion start, as it had\
    \ tendencies to ignore the system message stuff after a while. I am experimenting\
    \ with trimming down the message stack to a minimum although I have had some \"\
    dummy\" messages in the list first where the assistant is supposedly answering\
    \ perfectly when it comes to how it should output the function calls to an example\
    \ user input. It seems to help although I see prompt crafting is a skill on its\
    \ own.\r\n\r\nI am considering playing with system contexts to add and remove\
    \ info in it depending on where dialogue is going so that I can give it info about\
    \ other functions available in a given context.\r\n\r\nThe ability to run these\
    \ models locally and play with them for free is fantastic though. I am using LM\
    \ Studio which which has a server mode excellent for this."
  created_at: 2023-11-17 12:16:35+00:00
  edited: false
  hidden: false
  id: 655759a3ee35f7d8bc08d6cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RvgR1HAQDjG1gyBe4YzuR.png?w=200&h=200&f=face
      fullname: Tom Sanford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TomSanford
      type: user
    createdAt: '2023-11-17T14:30:26.000Z'
    data:
      edited: false
      editors:
      - TomSanford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9257442951202393
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RvgR1HAQDjG1gyBe4YzuR.png?w=200&h=200&f=face
          fullname: Tom Sanford
          isHf: false
          isPro: false
          name: TomSanford
          type: user
        html: '<p>I do not think you will be able to reason the model out of it''s
          current responses with the same settings. Have you tried changing the ''system''
          message to give the model more instruction before asking it questions? The
          one it is giving sounds very generic. Here is an example from MS Azure docs.
          <a rel="nofollow" href="https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md">https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md</a>
          :</p>

          <p>This one specifically:<br>&lt;|im_start|&gt;system<br>Assistant is an
          intelligent chatbot designed to help users answer their tax related questions.
          </p>

          <p>Instructions:</p>

          <ul>

          <li>Only answer questions related to taxes. </li>

          <li>If you''re unsure of an answer, you can say "I don''t know" or "I''m
          not sure" and recommend users go to the IRS website for more information.<br>&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>When
          are my taxes due?<br>&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant</li>

          </ul>

          <p>If you need to give explicit instructions you may need an instruct tuned
          model . I really love the Dolphin models, but I decided to try  <a href="https://huggingface.co/jondurbin/airoboros-m-7b-3.1.2">https://huggingface.co/jondurbin/airoboros-m-7b-3.1.2</a>
          so I could pass in /INST commands that I couldn''t figure out in ChatML,
          but it works great as one of my agents. Best of luck.</p>

          '
        raw: "I do not think you will be able to reason the model out of it's current\
          \ responses with the same settings. Have you tried changing the 'system'\
          \ message to give the model more instruction before asking it questions?\
          \ The one it is giving sounds very generic. Here is an example from MS Azure\
          \ docs. https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md\
          \ :\n\nThis one specifically:\n<|im_start|>system\nAssistant is an intelligent\
          \ chatbot designed to help users answer their tax related questions. \n\n\
          Instructions:\n- Only answer questions related to taxes. \n- If you're unsure\
          \ of an answer, you can say \"I don't know\" or \"I'm not sure\" and recommend\
          \ users go to the IRS website for more information.\n<|im_end|>\n<|im_start|>user\n\
          When are my taxes due?\n<|im_end|>\n<|im_start|>assistant\n\nIf you need\
          \ to give explicit instructions you may need an instruct tuned model . I\
          \ really love the Dolphin models, but I decided to try  https://huggingface.co/jondurbin/airoboros-m-7b-3.1.2\
          \ so I could pass in /INST commands that I couldn't figure out in ChatML,\
          \ but it works great as one of my agents. Best of luck."
        updatedAt: '2023-11-17T14:30:26.958Z'
      numEdits: 0
      reactions: []
    id: 6557790285d43542fa3955bc
    type: comment
  author: TomSanford
  content: "I do not think you will be able to reason the model out of it's current\
    \ responses with the same settings. Have you tried changing the 'system' message\
    \ to give the model more instruction before asking it questions? The one it is\
    \ giving sounds very generic. Here is an example from MS Azure docs. https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md\
    \ :\n\nThis one specifically:\n<|im_start|>system\nAssistant is an intelligent\
    \ chatbot designed to help users answer their tax related questions. \n\nInstructions:\n\
    - Only answer questions related to taxes. \n- If you're unsure of an answer, you\
    \ can say \"I don't know\" or \"I'm not sure\" and recommend users go to the IRS\
    \ website for more information.\n<|im_end|>\n<|im_start|>user\nWhen are my taxes\
    \ due?\n<|im_end|>\n<|im_start|>assistant\n\nIf you need to give explicit instructions\
    \ you may need an instruct tuned model . I really love the Dolphin models, but\
    \ I decided to try  https://huggingface.co/jondurbin/airoboros-m-7b-3.1.2 so I\
    \ could pass in /INST commands that I couldn't figure out in ChatML, but it works\
    \ great as one of my agents. Best of luck."
  created_at: 2023-11-17 14:30:26+00:00
  edited: false
  hidden: false
  id: 6557790285d43542fa3955bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-11-17T14:59:01.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.58492112159729
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>You might wanna try passing a system message plus user message each
          time</p>

          '
        raw: You might wanna try passing a system message plus user message each time
        updatedAt: '2023-11-17T14:59:01.016Z'
      numEdits: 0
      reactions: []
    id: 65577fb5c795337623ddead2
    type: comment
  author: ehartford
  content: You might wanna try passing a system message plus user message each time
  created_at: 2023-11-17 14:59:01+00:00
  edited: false
  hidden: false
  id: 65577fb5c795337623ddead2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84e1306c34a0b8986edbcbdf2b496846.svg
      fullname: "John Christian L\xF8nningdal"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 64jcl
      type: user
    createdAt: '2023-11-17T17:05:23.000Z'
    data:
      edited: true
      editors:
      - 64jcl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9615086317062378
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84e1306c34a0b8986edbcbdf2b496846.svg
          fullname: "John Christian L\xF8nningdal"
          isHf: false
          isPro: false
          name: 64jcl
          type: user
        html: '<p>Yes I use the OpenAPI-alike server hosted in LM Studio and pass
          system message, a few user/assistant mock messages (as if it replied perfectly
          with regards to outputting function calls), and then a few of the last actual
          user/assistant pairs (no more than 5), then I actually pass it a generated
          user message as if I was reminding it of all the things it is supposed to
          remember (property:value pairs) along with a made up assistant message where
          it is confirming that this is data it can use in its replies - and then
          finally the actual user message for assistant completion.</p>

          <p>It works rather well actually, its just that once it has tripped up with
          regards to its function output, those replies seems to be enforced making
          it do the same mistake over again, no doubt because they are part of the
          last user/assistant messages in the next completion. So I think I just need
          some way to tone those last replies down a bit.</p>

          <p>Still it''s fun experimenting with this and more often than not it is
          remarkably brilliant in its replies and using the functions just as intended.
          And to Tom, yes the system message has a clear instructions type, telling
          it what its role is and a list of the functions it can use and what for,
          including a sample. I have even a lot of extra bits telling it how it should
          not be used, which has improved the output somewhat.</p>

          '
        raw: 'Yes I use the OpenAPI-alike server hosted in LM Studio and pass system
          message, a few user/assistant mock messages (as if it replied perfectly
          with regards to outputting function calls), and then a few of the last actual
          user/assistant pairs (no more than 5), then I actually pass it a generated
          user message as if I was reminding it of all the things it is supposed to
          remember (property:value pairs) along with a made up assistant message where
          it is confirming that this is data it can use in its replies - and then
          finally the actual user message for assistant completion.


          It works rather well actually, its just that once it has tripped up with
          regards to its function output, those replies seems to be enforced making
          it do the same mistake over again, no doubt because they are part of the
          last user/assistant messages in the next completion. So I think I just need
          some way to tone those last replies down a bit.


          Still it''s fun experimenting with this and more often than not it is remarkably
          brilliant in its replies and using the functions just as intended. And to
          Tom, yes the system message has a clear instructions type, telling it what
          its role is and a list of the functions it can use and what for, including
          a sample. I have even a lot of extra bits telling it how it should not be
          used, which has improved the output somewhat.'
        updatedAt: '2023-11-17T17:06:48.469Z'
      numEdits: 1
      reactions: []
    id: 65579d5301aed573e7a65d50
    type: comment
  author: 64jcl
  content: 'Yes I use the OpenAPI-alike server hosted in LM Studio and pass system
    message, a few user/assistant mock messages (as if it replied perfectly with regards
    to outputting function calls), and then a few of the last actual user/assistant
    pairs (no more than 5), then I actually pass it a generated user message as if
    I was reminding it of all the things it is supposed to remember (property:value
    pairs) along with a made up assistant message where it is confirming that this
    is data it can use in its replies - and then finally the actual user message for
    assistant completion.


    It works rather well actually, its just that once it has tripped up with regards
    to its function output, those replies seems to be enforced making it do the same
    mistake over again, no doubt because they are part of the last user/assistant
    messages in the next completion. So I think I just need some way to tone those
    last replies down a bit.


    Still it''s fun experimenting with this and more often than not it is remarkably
    brilliant in its replies and using the functions just as intended. And to Tom,
    yes the system message has a clear instructions type, telling it what its role
    is and a list of the functions it can use and what for, including a sample. I
    have even a lot of extra bits telling it how it should not be used, which has
    improved the output somewhat.'
  created_at: 2023-11-17 17:05:23+00:00
  edited: true
  hidden: false
  id: 65579d5301aed573e7a65d50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RvgR1HAQDjG1gyBe4YzuR.png?w=200&h=200&f=face
      fullname: Tom Sanford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TomSanford
      type: user
    createdAt: '2023-11-17T17:51:08.000Z'
    data:
      edited: false
      editors:
      - TomSanford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.925820529460907
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RvgR1HAQDjG1gyBe4YzuR.png?w=200&h=200&f=face
          fullname: Tom Sanford
          isHf: false
          isPro: false
          name: TomSanford
          type: user
        html: '<p>The system messages are not part of the conversation, so are you
          passing the same ''system'' input that generated a correct response each
          time and it is still refusing? </p>

          '
        raw: 'The system messages are not part of the conversation, so are you passing
          the same ''system'' input that generated a correct response each time and
          it is still refusing? '
        updatedAt: '2023-11-17T17:51:08.184Z'
      numEdits: 0
      reactions: []
    id: 6557a80cd3ea40f5d0496863
    type: comment
  author: TomSanford
  content: 'The system messages are not part of the conversation, so are you passing
    the same ''system'' input that generated a correct response each time and it is
    still refusing? '
  created_at: 2023-11-17 17:51:08+00:00
  edited: false
  hidden: false
  id: 6557a80cd3ea40f5d0496863
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-11-17T18:26:54.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8918474316596985
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>system messages are the same as other messages at the API level
          (and in ChatML) indeed you can send multiple system messages within a single
          conversation.  By setting role to "system", "user", or "assistant"</p>

          '
        raw: system messages are the same as other messages at the API level (and
          in ChatML) indeed you can send multiple system messages within a single
          conversation.  By setting role to "system", "user", or "assistant"
        updatedAt: '2023-11-17T18:26:54.721Z'
      numEdits: 0
      reactions: []
    id: 6557b06e6f23f76c95d3d2f0
    type: comment
  author: ehartford
  content: system messages are the same as other messages at the API level (and in
    ChatML) indeed you can send multiple system messages within a single conversation.  By
    setting role to "system", "user", or "assistant"
  created_at: 2023-11-17 18:26:54+00:00
  edited: false
  hidden: false
  id: 6557b06e6f23f76c95d3d2f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3cd95dd72f30672802278fba0f369ead.svg
      fullname: Froggie
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Alex199
      type: user
    createdAt: '2023-11-22T19:00:05.000Z'
    data:
      edited: false
      editors:
      - Alex199
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975836992263794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3cd95dd72f30672802278fba0f369ead.svg
          fullname: Froggie
          isHf: false
          isPro: false
          name: Alex199
          type: user
        html: "<p>Don\xB4t know what to say, but this model is absolutely amazing.\
          \ Brilliant replies and as a newbie I still discovering this model. Go on\
          \ with this model - I will hope and enjoy !</p>\n"
        raw: "Don\xB4t know what to say, but this model is absolutely amazing. Brilliant\
          \ replies and as a newbie I still discovering this model. Go on with this\
          \ model - I will hope and enjoy !"
        updatedAt: '2023-11-22T19:00:05.738Z'
      numEdits: 0
      reactions: []
    id: 655e4fb5203bce21fee2e650
    type: comment
  author: Alex199
  content: "Don\xB4t know what to say, but this model is absolutely amazing. Brilliant\
    \ replies and as a newbie I still discovering this model. Go on with this model\
    \ - I will hope and enjoy !"
  created_at: 2023-11-22 19:00:05+00:00
  edited: false
  hidden: false
  id: 655e4fb5203bce21fee2e650
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: cognitivecomputations/dolphin-2.2.1-mistral-7b
repo_type: model
status: open
target_branch: null
title: Love this model
