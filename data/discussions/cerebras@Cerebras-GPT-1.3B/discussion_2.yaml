!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ypxie0130
conflicting_files: null
created_at: 2023-10-07 14:25:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14aaa8ca2f718183c380074079d32634.svg
      fullname: Yann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ypxie0130
      type: user
    createdAt: '2023-10-07T15:25:30.000Z'
    data:
      edited: false
      editors:
      - ypxie0130
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9734103679656982
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14aaa8ca2f718183c380074079d32634.svg
          fullname: Yann
          isHf: false
          isPro: false
          name: ypxie0130
          type: user
        html: '<p>Right now, almost all big companies are oddly, ubiquitously, unnaturally,
          only releasing models that are severely undertrained. They seems very reluctant
          to train the smaller model for LONGER to make it actually work.</p>

          <p>If we have a fully trained smaller model, that would be super helpful
          and truly democratize the LLM research and application.</p>

          '
        raw: "Right now, almost all big companies are oddly, ubiquitously, unnaturally,\
          \ only releasing models that are severely undertrained. They seems very\
          \ reluctant to train the smaller model for LONGER to make it actually work.\r\
          \n\r\nIf we have a fully trained smaller model, that would be super helpful\
          \ and truly democratize the LLM research and application."
        updatedAt: '2023-10-07T15:25:30.003Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yahma
    id: 6521786a8fc4884992c64f9c
    type: comment
  author: ypxie0130
  content: "Right now, almost all big companies are oddly, ubiquitously, unnaturally,\
    \ only releasing models that are severely undertrained. They seems very reluctant\
    \ to train the smaller model for LONGER to make it actually work.\r\n\r\nIf we\
    \ have a fully trained smaller model, that would be super helpful and truly democratize\
    \ the LLM research and application."
  created_at: 2023-10-07 14:25:30+00:00
  edited: false
  hidden: false
  id: 6521786a8fc4884992c64f9c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cerebras/Cerebras-GPT-1.3B
repo_type: model
status: open
target_branch: null
title: I was wondering have you tried training Cerebras-GPT-1.3B until converge rather
  than just 1 epoch?
