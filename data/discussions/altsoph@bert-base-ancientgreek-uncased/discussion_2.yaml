!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AngledLuffa
conflicting_files: null
created_at: 2023-12-01 16:12:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2023-12-01T16:12:26.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7211627960205078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: "<p>For most models, when I try to load the model using <code>AutoModel</code>\
          \ and <code>AutoTokenizer</code>, it downloads all the needed pieces for\
          \ me.  With this model, I get the following error:</p>\n<pre><code>bert_tokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)\n  File \"/usr/local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 786, in from_pretrained\n    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2008, in from_pretrained\n    raise EnvironmentError(\nOSError: Can't\
          \ load tokenizer for 'altsoph/bert-base-ancientgreek-uncased'. If you were\
          \ trying to load it from 'https://huggingface.co/models', make sure you\
          \ don't have a local directory with the same name. Otherwise, make sure\
          \ 'altsoph/bert-base-ancientgreek-uncased' is the correct path to a directory\
          \ containing all relevant files for a BertTokenizerFast tokenizer.\n</code></pre>\n\
          <p>My <code>transformers</code> package is updated to the current latest,\
          \ 4.35.2</p>\n"
        raw: "For most models, when I try to load the model using `AutoModel` and\
          \ `AutoTokenizer`, it downloads all the needed pieces for me.  With this\
          \ model, I get the following error:\r\n\r\n```\r\nbert_tokenizer = AutoTokenizer.from_pretrained(model_name)\r\
          \n  File \"/usr/local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 786, in from_pretrained\r\n    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2008, in from_pretrained\r\n    raise EnvironmentError(\r\nOSError:\
          \ Can't load tokenizer for 'altsoph/bert-base-ancientgreek-uncased'. If\
          \ you were trying to load it from 'https://huggingface.co/models', make\
          \ sure you don't have a local directory with the same name. Otherwise, make\
          \ sure 'altsoph/bert-base-ancientgreek-uncased' is the correct path to a\
          \ directory containing all relevant files for a BertTokenizerFast tokenizer.\r\
          \n\r\n```\r\n\r\nMy `transformers` package is updated to the current latest,\
          \ 4.35.2"
        updatedAt: '2023-12-01T16:12:26.209Z'
      numEdits: 0
      reactions: []
    id: 656a05eae8bf55919a22ba34
    type: comment
  author: AngledLuffa
  content: "For most models, when I try to load the model using `AutoModel` and `AutoTokenizer`,\
    \ it downloads all the needed pieces for me.  With this model, I get the following\
    \ error:\r\n\r\n```\r\nbert_tokenizer = AutoTokenizer.from_pretrained(model_name)\r\
    \n  File \"/usr/local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 786, in from_pretrained\r\n    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2008, in from_pretrained\r\n    raise EnvironmentError(\r\nOSError: Can't\
    \ load tokenizer for 'altsoph/bert-base-ancientgreek-uncased'. If you were trying\
    \ to load it from 'https://huggingface.co/models', make sure you don't have a\
    \ local directory with the same name. Otherwise, make sure 'altsoph/bert-base-ancientgreek-uncased'\
    \ is the correct path to a directory containing all relevant files for a BertTokenizerFast\
    \ tokenizer.\r\n\r\n```\r\n\r\nMy `transformers` package is updated to the current\
    \ latest, 4.35.2"
  created_at: 2023-12-01 16:12:26+00:00
  edited: false
  hidden: false
  id: 656a05eae8bf55919a22ba34
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: altsoph/bert-base-ancientgreek-uncased
repo_type: model
status: open
target_branch: null
title: Tokenizer doesn't automatically download?
