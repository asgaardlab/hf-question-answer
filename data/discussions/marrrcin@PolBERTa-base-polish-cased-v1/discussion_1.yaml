!!python/object:huggingface_hub.community.DiscussionWithDetails
author: awawrzynski
conflicting_files: null
created_at: 2022-06-02 08:43:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646919510813-6229fcc1b8c5f583fa931cdf.jpeg?w=200&h=200&f=face
      fullname: "Adam Wawrzy\u0144ski"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: awawrzynski
      type: user
    createdAt: '2022-06-02T09:43:23.000Z'
    data:
      edited: false
      editors:
      - awawrzynski
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646919510813-6229fcc1b8c5f583fa931cdf.jpeg?w=200&h=200&f=face
          fullname: "Adam Wawrzy\u0144ski"
          isHf: false
          isPro: false
          name: awawrzynski
          type: user
        html: "<p>There are missing Polish letters in <code>vocab.json</code>: \u017B\
          , \u0179, \u015A. For letters tokenizer assigns wrong tokens: \xD3, \u0179\
          , \u0104, \u0118, \u0143.<br>I tried using AutoTokenizer, RobertaTokenizer\
          \ and RobertaTokenizerFast. I am using <code>transformers==4.18.0</code>,\
          \ <code>tokenizers==0.21.1</code>, <code>Python3.6.9</code>.</p>\n<p>Examples\
          \ (words are artificial, important is first letter):</p>\n<pre><code class=\"\
          language-bash\">word: \xD3stka\ninput_ids: tensor([0, 2666, 246, 3154, 2)]\n\
          tokens: [<span class=\"hljs-string\">'&lt;s&gt;'</span>, <span class=\"\
          hljs-string\">' \uFFFD'</span>, <span class=\"hljs-string\">'\uFFFD'</span>,\
          \ <span class=\"hljs-string\">'stka'</span>, <span class=\"hljs-string\"\
          >'&lt;/s&gt;'</span>]\n\nword: \u0179stka\ninput_ids: tensor([0, 552, 122,\
          \ 3154, 2)]\ntokens: [<span class=\"hljs-string\">'&lt;s&gt;'</span>, <span\
          \ class=\"hljs-string\">' \uFFFD'</span>, <span class=\"hljs-string\">'\uFFFD\
          '</span>, <span class=\"hljs-string\">'stka'</span>, <span class=\"hljs-string\"\
          >'&lt;/s&gt;'</span>]\n\nword: \u0104stka\ninput_ids: tensor([0, 6327, 231,\
          \ 3154, 2)]\ntokens: [<span class=\"hljs-string\">'&lt;s&gt;'</span>, <span\
          \ class=\"hljs-string\">' \uFFFD'</span>, <span class=\"hljs-string\">'\uFFFD\
          '</span>, <span class=\"hljs-string\">'stka'</span>, <span class=\"hljs-string\"\
          >'&lt;/s&gt;'</span>]\n\nword: \u0118stka\ninput_ids: tensor([0, 6327, 251,\
          \ 3154, 2)]\ntokens: [<span class=\"hljs-string\">'&lt;s&gt;'</span>, <span\
          \ class=\"hljs-string\">' \uFFFD'</span>, <span class=\"hljs-string\">'\uFFFD\
          '</span>, <span class=\"hljs-string\">'stka'</span>, <span class=\"hljs-string\"\
          >'&lt;/s&gt;'</span>]\n\nword: \u0143stka\ninput_ids: tensor([0, 552, 230,\
          \ 3154, 2)]\ntokens: [<span class=\"hljs-string\">'&lt;s&gt;'</span>, <span\
          \ class=\"hljs-string\">' \uFFFD'</span>, <span class=\"hljs-string\">'\uFFFD\
          '</span>, <span class=\"hljs-string\">'stka'</span>, <span class=\"hljs-string\"\
          >'&lt;/s&gt;'</span>]\n</code></pre>\n"
        raw: "There are missing Polish letters in `vocab.json`: \u017B, \u0179, \u015A\
          . For letters tokenizer assigns wrong tokens: \xD3, \u0179, \u0104, \u0118\
          , \u0143.\r\nI tried using AutoTokenizer, RobertaTokenizer and RobertaTokenizerFast.\
          \ I am using `transformers==4.18.0`, `tokenizers==0.21.1`, `Python3.6.9`.\r\
          \n\r\nExamples (words are artificial, important is first letter):\r\n```bash\r\
          \nword: \xD3stka\r\ninput_ids: tensor([0, 2666, 246, 3154, 2)]\r\ntokens:\
          \ ['<s>', ' \uFFFD', '\uFFFD', 'stka', '</s>']\r\n\r\nword: \u0179stka\r\
          \ninput_ids: tensor([0, 552, 122, 3154, 2)]\r\ntokens: ['<s>', ' \uFFFD\
          ', '\uFFFD', 'stka', '</s>']\r\n\r\nword: \u0104stka\r\ninput_ids: tensor([0,\
          \ 6327, 231, 3154, 2)]\r\ntokens: ['<s>', ' \uFFFD', '\uFFFD', 'stka', '</s>']\r\
          \n\r\nword: \u0118stka\r\ninput_ids: tensor([0, 6327, 251, 3154, 2)]\r\n\
          tokens: ['<s>', ' \uFFFD', '\uFFFD', 'stka', '</s>']\r\n\r\nword: \u0143\
          stka\r\ninput_ids: tensor([0, 552, 230, 3154, 2)]\r\ntokens: ['<s>', ' \uFFFD\
          ', '\uFFFD', 'stka', '</s>']\r\n```"
        updatedAt: '2022-06-02T09:43:23.000Z'
      numEdits: 0
      reactions: []
    id: 6298863bf2bf8bd3e4690d25
    type: comment
  author: awawrzynski
  content: "There are missing Polish letters in `vocab.json`: \u017B, \u0179, \u015A\
    . For letters tokenizer assigns wrong tokens: \xD3, \u0179, \u0104, \u0118, \u0143\
    .\r\nI tried using AutoTokenizer, RobertaTokenizer and RobertaTokenizerFast. I\
    \ am using `transformers==4.18.0`, `tokenizers==0.21.1`, `Python3.6.9`.\r\n\r\n\
    Examples (words are artificial, important is first letter):\r\n```bash\r\nword:\
    \ \xD3stka\r\ninput_ids: tensor([0, 2666, 246, 3154, 2)]\r\ntokens: ['<s>', '\
    \ \uFFFD', '\uFFFD', 'stka', '</s>']\r\n\r\nword: \u0179stka\r\ninput_ids: tensor([0,\
    \ 552, 122, 3154, 2)]\r\ntokens: ['<s>', ' \uFFFD', '\uFFFD', 'stka', '</s>']\r\
    \n\r\nword: \u0104stka\r\ninput_ids: tensor([0, 6327, 231, 3154, 2)]\r\ntokens:\
    \ ['<s>', ' \uFFFD', '\uFFFD', 'stka', '</s>']\r\n\r\nword: \u0118stka\r\ninput_ids:\
    \ tensor([0, 6327, 251, 3154, 2)]\r\ntokens: ['<s>', ' \uFFFD', '\uFFFD', 'stka',\
    \ '</s>']\r\n\r\nword: \u0143stka\r\ninput_ids: tensor([0, 552, 230, 3154, 2)]\r\
    \ntokens: ['<s>', ' \uFFFD', '\uFFFD', 'stka', '</s>']\r\n```"
  created_at: 2022-06-02 08:43:23+00:00
  edited: false
  hidden: false
  id: 6298863bf2bf8bd3e4690d25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1597138322545-5e6ffb14d4cd9779932a7613.jpeg?w=200&h=200&f=face
      fullname: "Marcin Zab\u0142ocki"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: marrrcin
      type: user
    createdAt: '2022-07-26T19:16:28.000Z'
    data:
      edited: false
      editors:
      - marrrcin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1597138322545-5e6ffb14d4cd9779932a7613.jpeg?w=200&h=200&f=face
          fullname: "Marcin Zab\u0142ocki"
          isHf: false
          isPro: false
          name: marrrcin
          type: user
        html: '<p>Thanks for contacting me!<br>This LM was trained a long time ago
          for the learning purposes so it might be missing something.</p>

          '
        raw: 'Thanks for contacting me!

          This LM was trained a long time ago for the learning purposes so it might
          be missing something.'
        updatedAt: '2022-07-26T19:16:28.810Z'
      numEdits: 0
      reactions: []
    id: 62e03d8ce4085d5a894a7a9c
    type: comment
  author: marrrcin
  content: 'Thanks for contacting me!

    This LM was trained a long time ago for the learning purposes so it might be missing
    something.'
  created_at: 2022-07-26 18:16:28+00:00
  edited: false
  hidden: false
  id: 62e03d8ce4085d5a894a7a9c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: marrrcin/PolBERTa-base-polish-cased-v1
repo_type: model
status: open
target_branch: null
title: Problems with tokenizer of Polish letters
