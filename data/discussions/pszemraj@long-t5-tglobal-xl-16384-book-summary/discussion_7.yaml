!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArthurParkerhouse
conflicting_files: null
created_at: 2022-12-13 15:37:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
      fullname: Parkerhouse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurParkerhouse
      type: user
    createdAt: '2022-12-13T15:37:05.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
          fullname: Parkerhouse
          isHf: false
          isPro: false
          name: ArthurParkerhouse
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-12-13T22:49:44.012Z'
      numEdits: 2
      reactions: []
    id: 63989c21982e49f24211ed24
    type: comment
  author: ArthurParkerhouse
  content: This comment has been hidden
  created_at: 2022-12-13 15:37:05+00:00
  edited: true
  hidden: true
  id: 63989c21982e49f24211ed24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
      fullname: Parkerhouse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurParkerhouse
      type: user
    createdAt: '2022-12-13T23:49:42.000Z'
    data:
      from: 'Is it possible to generate multiple sequences per token-batch sequence? '
      to: Is it possible to generate multiple sequences per token-batch sequence?
    id: 63990f9602d150d0f20279d6
    type: title-change
  author: ArthurParkerhouse
  created_at: 2022-12-13 23:49:42+00:00
  id: 63990f9602d150d0f20279d6
  new_title: Is it possible to generate multiple sequences per token-batch sequence?
  old_title: 'Is it possible to generate multiple sequences per token-batch sequence? '
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-12-15T05:39:53.000Z'
    data:
      edited: true
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>interesting proposition! IMO it depends on the decoding method.
          Beam search (<em>AFAIK</em>) will generate deterministically for a given
          model (<em>aside: I have not explored what happens if you use <code>set_seed()</code>
          and try different ones, maybe they are different</em>). If you generate
          text via <em>sampling</em>, it is possible to get different generated text(s)
          for a given input token batch. when I last tried sampling, it was pretty
          trash. Granted, that was with the <code>LED-MODELSIZE-book-summary</code>
          series of models.. so maybe things are different here. you can get details
          on how to generate via different methods <a href="https://huggingface.co/blog/how-to-generate">here</a>
          and in the transformers docs. I think the blog post details things but essentially
          you could pass <code>num_generated_sequences=69</code> or whatever number
          of different summaries you find useful/have the compute for.</p>

          <p>Another way to get unique/different generations for a given input would
          be with the "new" <a href="https://huggingface.co/blog/introducing-csearch">contrastive
          search</a> (which came out several months ago). For text-generation purposes
          (read: not summarization) I have found it to be superior to sampling, I
          have not tried it for summarization, and it might do well. If you try that
          let me know!</p>

          <p>p.s.: the csearch blog post has most of the code you would need to try
          different decoding methods.</p>

          '
        raw: 'interesting proposition! IMO it depends on the decoding method. Beam
          search (_AFAIK_) will generate deterministically for a given model (_aside:
          I have not explored what happens if you use `set_seed()` and try different
          ones, maybe they are different_). If you generate text via _sampling_, it
          is possible to get different generated text(s) for a given input token batch.
          when I last tried sampling, it was pretty trash. Granted, that was with
          the `LED-MODELSIZE-book-summary` series of models.. so maybe things are
          different here. you can get details on how to generate via different methods
          [here](https://huggingface.co/blog/how-to-generate) and in the transformers
          docs. I think the blog post details things but essentially you could pass
          `num_generated_sequences=69` or whatever number of different summaries you
          find useful/have the compute for.


          Another way to get unique/different generations for a given input would
          be with the "new" [contrastive search](https://huggingface.co/blog/introducing-csearch)
          (which came out several months ago). For text-generation purposes (read:
          not summarization) I have found it to be superior to sampling, I have not
          tried it for summarization, and it might do well. If you try that let me
          know!


          p.s.: the csearch blog post has most of the code you would need to try different
          decoding methods.'
        updatedAt: '2022-12-15T05:40:25.147Z'
      numEdits: 1
      reactions: []
    id: 639ab329c63cdf9932697f48
    type: comment
  author: pszemraj
  content: 'interesting proposition! IMO it depends on the decoding method. Beam search
    (_AFAIK_) will generate deterministically for a given model (_aside: I have not
    explored what happens if you use `set_seed()` and try different ones, maybe they
    are different_). If you generate text via _sampling_, it is possible to get different
    generated text(s) for a given input token batch. when I last tried sampling, it
    was pretty trash. Granted, that was with the `LED-MODELSIZE-book-summary` series
    of models.. so maybe things are different here. you can get details on how to
    generate via different methods [here](https://huggingface.co/blog/how-to-generate)
    and in the transformers docs. I think the blog post details things but essentially
    you could pass `num_generated_sequences=69` or whatever number of different summaries
    you find useful/have the compute for.


    Another way to get unique/different generations for a given input would be with
    the "new" [contrastive search](https://huggingface.co/blog/introducing-csearch)
    (which came out several months ago). For text-generation purposes (read: not summarization)
    I have found it to be superior to sampling, I have not tried it for summarization,
    and it might do well. If you try that let me know!


    p.s.: the csearch blog post has most of the code you would need to try different
    decoding methods.'
  created_at: 2022-12-15 05:39:53+00:00
  edited: true
  hidden: false
  id: 639ab329c63cdf9932697f48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
      fullname: Parkerhouse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurParkerhouse
      type: user
    createdAt: '2022-12-16T16:14:34.000Z'
    data:
      edited: false
      editors:
      - ArthurParkerhouse
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
          fullname: Parkerhouse
          isHf: false
          isPro: false
          name: ArthurParkerhouse
          type: user
        html: '<p>Thanks so much! </p>

          <p>I was trying to get the contrastive search to work on the batch tokenization
          script yesterday, but I''m not sure if it was working properly or not. For
          a while it seemed to only use the top_k setting and changing penalty alpha
          didn''t change the output at all. </p>

          <p>Then I switched around the order in which the arguments were listed,
          so instead of them being listed in this order (penalty_alpha=0.6, top_k=4)
          as per the csearch instructions - I instead listed top_k first and penalty_alpha
          second (such as: top_k=4, penalty_alpha=0.6), and it was only after listing
          the arguments in this order that changing the penalty_alpha setting (while
          keeping top_k the same) actually affected the output. </p>

          <p>No clue why the order of those two specific arguments change how it works,
          though. I was using "torch.manual_seed(3407)" for repeatable examples. </p>

          <p>I just discovered that Hugging face has a separate forum section as well
          which I just joined, and planning on joining the discord at some point,
          so hopefully soon I''ll stop clogging up your discussion sections, lol.</p>

          '
        raw: "Thanks so much! \n\nI was trying to get the contrastive search to work\
          \ on the batch tokenization script yesterday, but I'm not sure if it was\
          \ working properly or not. For a while it seemed to only use the top_k setting\
          \ and changing penalty alpha didn't change the output at all. \n\nThen I\
          \ switched around the order in which the arguments were listed, so instead\
          \ of them being listed in this order (penalty_alpha=0.6, top_k=4) as per\
          \ the csearch instructions - I instead listed top_k first and penalty_alpha\
          \ second (such as: top_k=4, penalty_alpha=0.6), and it was only after listing\
          \ the arguments in this order that changing the penalty_alpha setting (while\
          \ keeping top_k the same) actually affected the output. \n\nNo clue why\
          \ the order of those two specific arguments change how it works, though.\
          \ I was using \"torch.manual_seed(3407)\" for repeatable examples. \n\n\
          I just discovered that Hugging face has a separate forum section as well\
          \ which I just joined, and planning on joining the discord at some point,\
          \ so hopefully soon I'll stop clogging up your discussion sections, lol."
        updatedAt: '2022-12-16T16:14:34.711Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - pszemraj
    id: 639c996af41aa339c5390e18
    type: comment
  author: ArthurParkerhouse
  content: "Thanks so much! \n\nI was trying to get the contrastive search to work\
    \ on the batch tokenization script yesterday, but I'm not sure if it was working\
    \ properly or not. For a while it seemed to only use the top_k setting and changing\
    \ penalty alpha didn't change the output at all. \n\nThen I switched around the\
    \ order in which the arguments were listed, so instead of them being listed in\
    \ this order (penalty_alpha=0.6, top_k=4) as per the csearch instructions - I\
    \ instead listed top_k first and penalty_alpha second (such as: top_k=4, penalty_alpha=0.6),\
    \ and it was only after listing the arguments in this order that changing the\
    \ penalty_alpha setting (while keeping top_k the same) actually affected the output.\
    \ \n\nNo clue why the order of those two specific arguments change how it works,\
    \ though. I was using \"torch.manual_seed(3407)\" for repeatable examples. \n\n\
    I just discovered that Hugging face has a separate forum section as well which\
    \ I just joined, and planning on joining the discord at some point, so hopefully\
    \ soon I'll stop clogging up your discussion sections, lol."
  created_at: 2022-12-16 16:14:34+00:00
  edited: false
  hidden: false
  id: 639c996af41aa339c5390e18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-04-30T03:24:41.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>No worries and sorry for taking forever to respond. Btw, I have
          taken a look at some different inference params and while a real analysis
          is still in progress, you can check out/ compare some different params with
          long-t5 in the <code>/long-t5</code> subfolder in <a rel="nofollow" href="https://www.dropbox.com/sh/axu1xlscrrexy55/AADAm01-4Zs3POyHQrgbDAsda?dl=0">my
          summarization gauntlet </a></p>

          <p>closing for now but feel free to comment and/or reopen if you have any
          issues or burning questions</p>

          '
        raw: 'No worries and sorry for taking forever to respond. Btw, I have taken
          a look at some different inference params and while a real analysis is still
          in progress, you can check out/ compare some different params with long-t5
          in the `/long-t5` subfolder in [my summarization gauntlet ](https://www.dropbox.com/sh/axu1xlscrrexy55/AADAm01-4Zs3POyHQrgbDAsda?dl=0)


          closing for now but feel free to comment and/or reopen if you have any issues
          or burning questions'
        updatedAt: '2023-04-30T03:24:41.461Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644ddf79fa94e93b0ed3f617
    id: 644ddf79fa94e93b0ed3f616
    type: comment
  author: pszemraj
  content: 'No worries and sorry for taking forever to respond. Btw, I have taken
    a look at some different inference params and while a real analysis is still in
    progress, you can check out/ compare some different params with long-t5 in the
    `/long-t5` subfolder in [my summarization gauntlet ](https://www.dropbox.com/sh/axu1xlscrrexy55/AADAm01-4Zs3POyHQrgbDAsda?dl=0)


    closing for now but feel free to comment and/or reopen if you have any issues
    or burning questions'
  created_at: 2023-04-30 02:24:41+00:00
  edited: false
  hidden: false
  id: 644ddf79fa94e93b0ed3f616
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-04-30T03:24:41.000Z'
    data:
      status: closed
    id: 644ddf79fa94e93b0ed3f617
    type: status-change
  author: pszemraj
  created_at: 2023-04-30 02:24:41+00:00
  id: 644ddf79fa94e93b0ed3f617
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: pszemraj/long-t5-tglobal-xl-16384-book-summary
repo_type: model
status: closed
target_branch: null
title: Is it possible to generate multiple sequences per token-batch sequence?
