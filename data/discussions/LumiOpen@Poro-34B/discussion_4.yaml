!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kelmeilia
conflicting_files: null
created_at: 2024-01-22 14:01:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02d75e38fb8409a54791e4128d2cd164.svg
      fullname: Paavo Hietikko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelmeilia
      type: user
    createdAt: '2024-01-22T14:01:00.000Z'
    data:
      edited: false
      editors:
      - Kelmeilia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9352304935455322
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02d75e38fb8409a54791e4128d2cd164.svg
          fullname: Paavo Hietikko
          isHf: false
          isPro: false
          name: Kelmeilia
          type: user
        html: '<p>I didn''t find any information about the requirements this model
          has for computational capacity.. Maybe someone with better skills can deduct
          them from parameter count or something?</p>

          <p>I am wondering if this can be run locally with a rtx 4080 graphics card
          and 16 GBs of memory?</p>

          '
        raw: "I didn't find any information about the requirements this model has\
          \ for computational capacity.. Maybe someone with better skills can deduct\
          \ them from parameter count or something?\r\n\r\nI am wondering if this\
          \ can be run locally with a rtx 4080 graphics card and 16 GBs of memory?\r\
          \n\r\n"
        updatedAt: '2024-01-22T14:01:00.669Z'
      numEdits: 0
      reactions: []
    id: 65ae751cde38fbe92241ff76
    type: comment
  author: Kelmeilia
  content: "I didn't find any information about the requirements this model has for\
    \ computational capacity.. Maybe someone with better skills can deduct them from\
    \ parameter count or something?\r\n\r\nI am wondering if this can be run locally\
    \ with a rtx 4080 graphics card and 16 GBs of memory?\r\n\r\n"
  created_at: 2024-01-22 14:01:00+00:00
  edited: false
  hidden: false
  id: 65ae751cde38fbe92241ff76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
      fullname: minipasila
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mpasila
      type: user
    createdAt: '2024-01-22T14:56:46.000Z'
    data:
      edited: false
      editors:
      - mpasila
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8536955118179321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
          fullname: minipasila
          isHf: false
          isPro: false
          name: mpasila
          type: user
        html: '<p>If you want to run it unquantized then no (it will need a minimum
          of 48gb for that) but if you use bitsandbytes and load it in 4bits then
          it needs about 20gb and with GPTQ at 3bits it needs about 15gb but it still
          ran out of memory on a 16GB GPU when I tried loading it with this <a href="https://huggingface.co/TheBloke/Poro-34B-GPTQ/tree/gptq-3bit-128g-actorder_True">TheBloke/Poro-34B-GPTQ:gptq-3bit-128g-actorder_True</a>.
          So the best bet might be using a GGUF version by <a href="https://huggingface.co/TheBloke/Poro-34B-GGUF">TheBloke</a>.
          If someone makes an EXL2 variants of this model at around 3bpw or lower
          it might actually fit on a 16gb card no problem.</p>

          '
        raw: If you want to run it unquantized then no (it will need a minimum of
          48gb for that) but if you use bitsandbytes and load it in 4bits then it
          needs about 20gb and with GPTQ at 3bits it needs about 15gb but it still
          ran out of memory on a 16GB GPU when I tried loading it with this [TheBloke/Poro-34B-GPTQ:gptq-3bit-128g-actorder_True](https://huggingface.co/TheBloke/Poro-34B-GPTQ/tree/gptq-3bit-128g-actorder_True).
          So the best bet might be using a GGUF version by [TheBloke](https://huggingface.co/TheBloke/Poro-34B-GGUF).
          If someone makes an EXL2 variants of this model at around 3bpw or lower
          it might actually fit on a 16gb card no problem.
        updatedAt: '2024-01-22T14:56:46.110Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jonabur
    id: 65ae822e0c672a000409db6d
    type: comment
  author: mpasila
  content: If you want to run it unquantized then no (it will need a minimum of 48gb
    for that) but if you use bitsandbytes and load it in 4bits then it needs about
    20gb and with GPTQ at 3bits it needs about 15gb but it still ran out of memory
    on a 16GB GPU when I tried loading it with this [TheBloke/Poro-34B-GPTQ:gptq-3bit-128g-actorder_True](https://huggingface.co/TheBloke/Poro-34B-GPTQ/tree/gptq-3bit-128g-actorder_True).
    So the best bet might be using a GGUF version by [TheBloke](https://huggingface.co/TheBloke/Poro-34B-GGUF).
    If someone makes an EXL2 variants of this model at around 3bpw or lower it might
    actually fit on a 16gb card no problem.
  created_at: 2024-01-22 14:56:46+00:00
  edited: false
  hidden: false
  id: 65ae822e0c672a000409db6d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: LumiOpen/Poro-34B
repo_type: model
status: open
target_branch: null
title: System requirements for this model?
