!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Baicai003
conflicting_files: null
created_at: 2022-12-21 13:54:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
      fullname: baicai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Baicai003
      type: user
    createdAt: '2022-12-21T13:54:20.000Z'
    data:
      edited: false
      editors:
      - Baicai003
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
          fullname: baicai
          isHf: false
          isPro: false
          name: Baicai003
          type: user
        html: "<p>such as \"a big apple, \u9999\u8549\uFF0C\u6A58\u5B50\"</p>\n"
        raw: "such as \"a big apple, \u9999\u8549\uFF0C\u6A58\u5B50\""
        updatedAt: '2022-12-21T13:54:20.386Z'
      numEdits: 0
      reactions: []
    id: 63a3100cb5fc9ab9f6390e90
    type: comment
  author: Baicai003
  content: "such as \"a big apple, \u9999\u8549\uFF0C\u6A58\u5B50\""
  created_at: 2022-12-21 13:54:20+00:00
  edited: false
  hidden: false
  id: 63a3100cb5fc9ab9f6390e90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
      fullname: An Yang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yangapku
      type: user
    createdAt: '2022-12-22T15:25:37.000Z'
    data:
      edited: false
      editors:
      - yangapku
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
          fullname: An Yang
          isHf: false
          isPro: false
          name: yangapku
          type: user
        html: "<p>Hi, first we would suggest to try your samples on our spaces <a\
          \ href=\"https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification\"\
          >https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification</a>\
          \ instead of the demo on the model card. The demo on model card is automatically\
          \ generated by transformers package and currently only support a fixed English\
          \ prompt template. On spaces, you can try your own customed (of course we\
          \ have a default Chinese prompt template) templates, which may have better\
          \ performance. Go back to the question, since Chinese-CLIP is initialized\
          \ from OpenAI CLIP weight and there may be samples with other languages\
          \ mixed in our pretraining dataset, the model can support some cases where\
          \ the input languages are mixed (note that on this demo the prompt template\
          \ is in English but it still works), including your mentioned case \"a big\
          \ apple, \u9999\u8549\uFF0C\u6A58\u5B50\" (we just tried it \U0001F601 on\
          \ the spaces, you can have a try). However, we cannot make a guarantee that\
          \ this always works indeed. Maybe using the huge-size model will have better\
          \ performance in this setting.</p>\n"
        raw: "Hi, first we would suggest to try your samples on our spaces https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification\
          \ instead of the demo on the model card. The demo on model card is automatically\
          \ generated by transformers package and currently only support a fixed English\
          \ prompt template. On spaces, you can try your own customed (of course we\
          \ have a default Chinese prompt template) templates, which may have better\
          \ performance. Go back to the question, since Chinese-CLIP is initialized\
          \ from OpenAI CLIP weight and there may be samples with other languages\
          \ mixed in our pretraining dataset, the model can support some cases where\
          \ the input languages are mixed (note that on this demo the prompt template\
          \ is in English but it still works), including your mentioned case \"a big\
          \ apple, \u9999\u8549\uFF0C\u6A58\u5B50\" (we just tried it \U0001F601 on\
          \ the spaces, you can have a try). However, we cannot make a guarantee that\
          \ this always works indeed. Maybe using the huge-size model will have better\
          \ performance in this setting."
        updatedAt: '2022-12-22T15:25:37.952Z'
      numEdits: 0
      reactions: []
    id: 63a476f1cde2b28f8298ebea
    type: comment
  author: yangapku
  content: "Hi, first we would suggest to try your samples on our spaces https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification\
    \ instead of the demo on the model card. The demo on model card is automatically\
    \ generated by transformers package and currently only support a fixed English\
    \ prompt template. On spaces, you can try your own customed (of course we have\
    \ a default Chinese prompt template) templates, which may have better performance.\
    \ Go back to the question, since Chinese-CLIP is initialized from OpenAI CLIP\
    \ weight and there may be samples with other languages mixed in our pretraining\
    \ dataset, the model can support some cases where the input languages are mixed\
    \ (note that on this demo the prompt template is in English but it still works),\
    \ including your mentioned case \"a big apple, \u9999\u8549\uFF0C\u6A58\u5B50\"\
    \ (we just tried it \U0001F601 on the spaces, you can have a try). However, we\
    \ cannot make a guarantee that this always works indeed. Maybe using the huge-size\
    \ model will have better performance in this setting."
  created_at: 2022-12-22 15:25:37+00:00
  edited: false
  hidden: false
  id: 63a476f1cde2b28f8298ebea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
      fullname: baicai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Baicai003
      type: user
    createdAt: '2022-12-22T15:42:31.000Z'
    data:
      edited: false
      editors:
      - Baicai003
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
          fullname: baicai
          isHf: false
          isPro: false
          name: Baicai003
          type: user
        html: '<p>I see the "vocab_size" is 21128, and in openai clip, it should be
          49408.<br>When I try to use ChineseCLIPTextModel from transformer to get
          text_embeddings with "Chinese and English" mixed input (also some other
          char as "()", ":"), I got a error which says that the index out of range..</p>

          '
        raw: 'I see the "vocab_size" is 21128, and in openai clip, it should be 49408.

          When I try to use ChineseCLIPTextModel from transformer to get text_embeddings
          with "Chinese and English" mixed input (also some other char as "()", ":"),
          I got a error which says that the index out of range..'
        updatedAt: '2022-12-22T15:42:31.601Z'
      numEdits: 0
      reactions: []
    id: 63a47ae7cde2b28f829977f9
    type: comment
  author: Baicai003
  content: 'I see the "vocab_size" is 21128, and in openai clip, it should be 49408.

    When I try to use ChineseCLIPTextModel from transformer to get text_embeddings
    with "Chinese and English" mixed input (also some other char as "()", ":"), I
    got a error which says that the index out of range..'
  created_at: 2022-12-22 15:42:31+00:00
  edited: false
  hidden: false
  id: 63a47ae7cde2b28f829977f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
      fullname: An Yang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yangapku
      type: user
    createdAt: '2022-12-22T16:50:50.000Z'
    data:
      edited: true
      editors:
      - yangapku
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
          fullname: An Yang
          isHf: false
          isPro: false
          name: yangapku
          type: user
        html: '<p>In fact, the text encoder and tokenizer of Chinese-CLIP has nothing
          to do with OpenAI CLIP. Its vocab size of 21128 is adopted from Chinese
          BERT (or say the Chinese Roberta-wwm-ext <a href="https://huggingface.co/hfl/chinese-roberta-wwm-ext">https://huggingface.co/hfl/chinese-roberta-wwm-ext</a>)
          model. May I ask for the code usage of computing the text embedding? Are
          you following the code snippet we provide in the document (<a href="https://huggingface.co/docs/transformers/model_doc/chinese_clip#usage">https://huggingface.co/docs/transformers/model_doc/chinese_clip#usage</a>)
          or in the model card? It should support language-mixed text input in our
          test, or we will never see that our demo can still work with such input.</p>

          '
        raw: In fact, the text encoder and tokenizer of Chinese-CLIP has nothing to
          do with OpenAI CLIP. Its vocab size of 21128 is adopted from Chinese BERT
          (or say the Chinese Roberta-wwm-ext https://huggingface.co/hfl/chinese-roberta-wwm-ext)
          model. May I ask for the code usage of computing the text embedding? Are
          you following the code snippet we provide in the document (https://huggingface.co/docs/transformers/model_doc/chinese_clip#usage)
          or in the model card? It should support language-mixed text input in our
          test, or we will never see that our demo can still work with such input.
        updatedAt: '2022-12-22T16:55:45.191Z'
      numEdits: 2
      reactions: []
    id: 63a48aeacde2b28f829ba662
    type: comment
  author: yangapku
  content: In fact, the text encoder and tokenizer of Chinese-CLIP has nothing to
    do with OpenAI CLIP. Its vocab size of 21128 is adopted from Chinese BERT (or
    say the Chinese Roberta-wwm-ext https://huggingface.co/hfl/chinese-roberta-wwm-ext)
    model. May I ask for the code usage of computing the text embedding? Are you following
    the code snippet we provide in the document (https://huggingface.co/docs/transformers/model_doc/chinese_clip#usage)
    or in the model card? It should support language-mixed text input in our test,
    or we will never see that our demo can still work with such input.
  created_at: 2022-12-22 16:50:50+00:00
  edited: true
  hidden: false
  id: 63a48aeacde2b28f829ba662
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
      fullname: baicai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Baicai003
      type: user
    createdAt: '2022-12-23T08:46:26.000Z'
    data:
      edited: false
      editors:
      - Baicai003
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
          fullname: baicai
          isHf: false
          isPro: false
          name: Baicai003
          type: user
        html: "<p>I just tried the example code on diffusers's  github Readme.md,\
          \ and switch the text encoder  to ChineseCLIPTextModel.</p>\n<pre><code>import\
          \ torch\nfrom diffusers import StableDiffusionPipeline\nfrom transformers\
          \ import ChineseCLIPTextModel\n\npipe = StableDiffusionPipeline.from_pretrained(\"\
          runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\npipe.text_encoder\
          \ = ChineseCLIPTextModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-large-patch16\"\
          )\npipe = pipe.to(\"cuda\")\n\nprompt = \"masterpiece, best quality, a photo\
          \ of \u6F02\u4EAE\u5973\u5B69\uFF0C\u84DD\u8272\u773C\u775B\uFF0C\u91D1\u8272\
          \u5934\u53D1\"\nimage = pipe(prompt).images[0]  \n</code></pre>\n<p>I do\
          \ not know how to make the chinese clip  work with stable diffusion.</p>\n"
        raw: "I just tried the example code on diffusers's  github Readme.md, and\
          \ switch the text encoder  to ChineseCLIPTextModel.\n```\nimport torch\n\
          from diffusers import StableDiffusionPipeline\nfrom transformers import\
          \ ChineseCLIPTextModel\n\npipe = StableDiffusionPipeline.from_pretrained(\"\
          runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\npipe.text_encoder\
          \ = ChineseCLIPTextModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-large-patch16\"\
          )\npipe = pipe.to(\"cuda\")\n\nprompt = \"masterpiece, best quality, a photo\
          \ of \u6F02\u4EAE\u5973\u5B69\uFF0C\u84DD\u8272\u773C\u775B\uFF0C\u91D1\u8272\
          \u5934\u53D1\"\nimage = pipe(prompt).images[0]  \n```\nI do not know how\
          \ to make the chinese clip  work with stable diffusion."
        updatedAt: '2022-12-23T08:46:26.848Z'
      numEdits: 0
      reactions: []
    id: 63a56ae28d6c69e1da4f403f
    type: comment
  author: Baicai003
  content: "I just tried the example code on diffusers's  github Readme.md, and switch\
    \ the text encoder  to ChineseCLIPTextModel.\n```\nimport torch\nfrom diffusers\
    \ import StableDiffusionPipeline\nfrom transformers import ChineseCLIPTextModel\n\
    \npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\"\
    , torch_dtype=torch.float16)\npipe.text_encoder = ChineseCLIPTextModel.from_pretrained(\"\
    OFA-Sys/chinese-clip-vit-large-patch16\")\npipe = pipe.to(\"cuda\")\n\nprompt\
    \ = \"masterpiece, best quality, a photo of \u6F02\u4EAE\u5973\u5B69\uFF0C\u84DD\
    \u8272\u773C\u775B\uFF0C\u91D1\u8272\u5934\u53D1\"\nimage = pipe(prompt).images[0]\
    \  \n```\nI do not know how to make the chinese clip  work with stable diffusion."
  created_at: 2022-12-23 08:46:26+00:00
  edited: false
  hidden: false
  id: 63a56ae28d6c69e1da4f403f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
      fullname: An Yang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yangapku
      type: user
    createdAt: '2023-01-01T10:25:02.000Z'
    data:
      edited: true
      editors:
      - yangapku
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
          fullname: An Yang
          isHf: false
          isPro: false
          name: yangapku
          type: user
        html: '<p>Hi, I think you should change the tokenizer config as well, since
          the tokenizers of Chinese CLIP and openai CLIP are not the same. Then I
          think you need to finetune the text encoder since the embedding space of
          Chinese CLIP has been changed from openai CLIP during the pretraining of
          Chinese CLIP.</p>

          '
        raw: Hi, I think you should change the tokenizer config as well, since the
          tokenizers of Chinese CLIP and openai CLIP are not the same. Then I think
          you need to finetune the text encoder since the embedding space of Chinese
          CLIP has been changed from openai CLIP during the pretraining of Chinese
          CLIP.
        updatedAt: '2023-01-01T10:25:24.917Z'
      numEdits: 1
      reactions: []
    id: 63b15f7ef9b8103903a39319
    type: comment
  author: yangapku
  content: Hi, I think you should change the tokenizer config as well, since the tokenizers
    of Chinese CLIP and openai CLIP are not the same. Then I think you need to finetune
    the text encoder since the embedding space of Chinese CLIP has been changed from
    openai CLIP during the pretraining of Chinese CLIP.
  created_at: 2023-01-01 10:25:02+00:00
  edited: true
  hidden: false
  id: 63b15f7ef9b8103903a39319
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: OFA-Sys/chinese-clip-vit-large-patch14
repo_type: model
status: open
target_branch: null
title: does it support Chinese and English mixed input?
