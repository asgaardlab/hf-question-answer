!!python/object:huggingface_hub.community.DiscussionWithDetails
author: echau18
conflicting_files: null
created_at: 2022-06-14 23:29:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6c1e22787b1c501b7ec9c793187cab.svg
      fullname: Ethan Chau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: echau18
      type: user
    createdAt: '2022-06-15T00:29:04.000Z'
    data:
      edited: false
      editors:
      - echau18
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6c1e22787b1c501b7ec9c793187cab.svg
          fullname: Ethan Chau
          isHf: false
          isPro: false
          name: echau18
          type: user
        html: '<p>Hello!  I''m trying load mt5-base (encoder-only) with <code>transformers</code>,
          and I''m finding that the config and checkpoint have more input embeddings
          than there are items in the vocabulary.  Specifically:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> MT5TokenizerFast,
          MT5Config, MT5EncoderModel

          cfg = MT5Config.from_pretrained(<span class="hljs-string">"google/mt5-base"</span>)

          tok = MT5TokenizerFast.from_pretrained(<span class="hljs-string">"google/mt5-base"</span>)

          mdl = MT5EncoderModel.from_pretrained(<span class="hljs-string">"google/mt5-base"</span>,
          config=cfg)

          <span class="hljs-built_in">print</span>(cfg.vocab_size == mdl.get_input_embeddings().num_embeddings)

          <span class="hljs-built_in">print</span>(cfg.vocab_size == <span class="hljs-built_in">len</span>(tok))

          <span class="hljs-built_in">print</span>(cfg.vocab_size)

          <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(tok))

          </code></pre>

          <p>prints</p>

          <pre><code class="language-bash">True

          False

          250112

          250100

          </code></pre>

          <p>(this happens with both fast and standard tokenizers)</p>

          <p>Is this expected?  If so, what are the extra 12 tokens for?</p>

          '
        raw: "Hello!  I'm trying load mt5-base (encoder-only) with `transformers`,\
          \ and I'm finding that the config and checkpoint have more input embeddings\
          \ than there are items in the vocabulary.  Specifically:\r\n```python\r\n\
          from transformers import MT5TokenizerFast, MT5Config, MT5EncoderModel\r\n\
          cfg = MT5Config.from_pretrained(\"google/mt5-base\")\r\ntok = MT5TokenizerFast.from_pretrained(\"\
          google/mt5-base\")\r\nmdl = MT5EncoderModel.from_pretrained(\"google/mt5-base\"\
          , config=cfg)\r\nprint(cfg.vocab_size == mdl.get_input_embeddings().num_embeddings)\r\
          \nprint(cfg.vocab_size == len(tok))\r\nprint(cfg.vocab_size)\r\nprint(len(tok))\r\
          \n```\r\nprints\r\n```bash\r\nTrue\r\nFalse\r\n250112\r\n250100\r\n```\r\
          \n(this happens with both fast and standard tokenizers)\r\n\r\nIs this expected?\
          \  If so, what are the extra 12 tokens for?"
        updatedAt: '2022-06-15T00:29:04.804Z'
      numEdits: 0
      reactions: []
    id: 62a927d008a7ea93ff417572
    type: comment
  author: echau18
  content: "Hello!  I'm trying load mt5-base (encoder-only) with `transformers`, and\
    \ I'm finding that the config and checkpoint have more input embeddings than there\
    \ are items in the vocabulary.  Specifically:\r\n```python\r\nfrom transformers\
    \ import MT5TokenizerFast, MT5Config, MT5EncoderModel\r\ncfg = MT5Config.from_pretrained(\"\
    google/mt5-base\")\r\ntok = MT5TokenizerFast.from_pretrained(\"google/mt5-base\"\
    )\r\nmdl = MT5EncoderModel.from_pretrained(\"google/mt5-base\", config=cfg)\r\n\
    print(cfg.vocab_size == mdl.get_input_embeddings().num_embeddings)\r\nprint(cfg.vocab_size\
    \ == len(tok))\r\nprint(cfg.vocab_size)\r\nprint(len(tok))\r\n```\r\nprints\r\n\
    ```bash\r\nTrue\r\nFalse\r\n250112\r\n250100\r\n```\r\n(this happens with both\
    \ fast and standard tokenizers)\r\n\r\nIs this expected?  If so, what are the\
    \ extra 12 tokens for?"
  created_at: 2022-06-14 23:29:04+00:00
  edited: false
  hidden: false
  id: 62a927d008a7ea93ff417572
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-06-25T17:29:58.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;echau18&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/echau18\"\
          >@<span class=\"underline\">echau18</span></a></span>\n\n\t</span></span>\
          \ - good question that many people had :-) See: <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/issues/4875\">https://github.com/huggingface/transformers/issues/4875</a></p>\n"
        raw: 'Hey @echau18 - good question that many people had :-) See: https://github.com/huggingface/transformers/issues/4875'
        updatedAt: '2022-06-25T17:29:58.234Z'
      numEdits: 0
      reactions: []
    id: 62b74616579e0e427eb6e3b8
    type: comment
  author: patrickvonplaten
  content: 'Hey @echau18 - good question that many people had :-) See: https://github.com/huggingface/transformers/issues/4875'
  created_at: 2022-06-25 16:29:58+00:00
  edited: false
  hidden: false
  id: 62b74616579e0e427eb6e3b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6c1e22787b1c501b7ec9c793187cab.svg
      fullname: Ethan Chau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: echau18
      type: user
    createdAt: '2022-06-27T22:14:09.000Z'
    data:
      edited: false
      editors:
      - echau18
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6c1e22787b1c501b7ec9c793187cab.svg
          fullname: Ethan Chau
          isHf: false
          isPro: false
          name: echau18
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;patrickvonplaten&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/patrickvonplaten\"\
          >@<span class=\"underline\">patrickvonplaten</span></a></span>\n\n\t</span></span>!\
          \  Seems like this is a common issue that resurfaces frequently.   Any chance\
          \ something could be directly added to the model card as an FYI, rather\
          \ than needing to route through the transformers repo?</p>\n"
        raw: Thanks @patrickvonplaten!  Seems like this is a common issue that resurfaces
          frequently.   Any chance something could be directly added to the model
          card as an FYI, rather than needing to route through the transformers repo?
        updatedAt: '2022-06-27T22:14:09.204Z'
      numEdits: 0
      reactions: []
    id: 62ba2bb12a829df269fe8f69
    type: comment
  author: echau18
  content: Thanks @patrickvonplaten!  Seems like this is a common issue that resurfaces
    frequently.   Any chance something could be directly added to the model card as
    an FYI, rather than needing to route through the transformers repo?
  created_at: 2022-06-27 21:14:09+00:00
  edited: false
  hidden: false
  id: 62ba2bb12a829df269fe8f69
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: google/mt5-base
repo_type: model
status: open
target_branch: null
title: mt5-base embedding size and tokenizer size don't match?
