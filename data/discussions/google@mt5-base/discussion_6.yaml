!!python/object:huggingface_hub.community.DiscussionWithDetails
author: moustafa123
conflicting_files: null
created_at: 2023-05-22 14:03:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c91eacbd674f0f74bf82e20662fe9d2f.svg
      fullname: Moustafa Banbouk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: moustafa123
      type: user
    createdAt: '2023-05-22T15:03:13.000Z'
    data:
      edited: false
      editors:
      - moustafa123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c91eacbd674f0f74bf82e20662fe9d2f.svg
          fullname: Moustafa Banbouk
          isHf: false
          isPro: false
          name: moustafa123
          type: user
        html: "<p>mT5-small Question Answering training is converging to high accuracy,\
          \ high validation accuracy, near-zero low loss; however, when testing the\
          \ model on trained questions, I am always receiving empty answers.</p>\n\
          <p>Experiment Language: Arabic<br>Dataset used: Arabic SQUAD<br>Optimizer\
          \ tested: Adam or AdamW with learning rate: 3e-4<br>loss function: tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)<br>Epochs\
          \ tested: 5, 30<br>I tried to train the model with 30 epochs and the same\
          \ result is obtained:</p>\n<p>Output:  Output: [0, 250099, 1]</p>\n<p>It\
          \ is very strange that the model is converging to high accuracy and low\
          \ loss and I am getting empty sentences during inference. I validated the\
          \ dataset questions and answers and they are correct.</p>\n<p>Below are\
          \ some important code snippets:</p>\n<p>def preprocess_function(examples):<br>\
          \    padding = \"max_length\"<br>    max_length = 200<br>    inputs = [ex\
          \ for ex in examples[\"question\"]]<br>    targets = [ex for ex in examples[\"\
          text\"]]<br>    model_inputs = tokenizer(inputs, max_length=max_length,\
          \ padding=padding, truncation=True)<br>    labels = tokenizer(targets, max_length=max_length,\
          \ padding=padding, truncation=True)<br>    model_inputs[\"labels\"] = labels[\"\
          input_ids\"]<br>    return model_inputs</p>\n<p>data_collator = DataCollatorForSeq2Seq(<br>\
          \            tokenizer,<br>            model=model,<br>            label_pad_token_id=tokenizer.pad_token_id,<br>\
          \            pad_to_multiple_of=64,<br>            return_tensors=\"np\"\
          ,<br>        )</p>\n<p>tf_train_dataset = model.prepare_tf_dataset(<br>\
          \            train_dataset,<br>            collate_fn=data_collator,<br>\
          \            batch_size=8,<br>            shuffle=True,<br>        )</p>\n\
          <p>optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)<br>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)<br>model.compile(optimizer=Adam(3e-5),\
          \ loss=loss, metrics=['accuracy'])<br>Output:<br>below are the output of\
          \ model.fit() for the first 5 epochs</p>\n<p>accuracy \u2581\u2588\u2588\
          \u2588\u2588</p>\n<p>epoch \u2581\u2583\u2585\u2586\u2588</p>\n<p>loss \u2588\
          \u2581\u2581\u2581\u2581</p>\n<p>val_accuracy \u2581\u2586\u2588\u2588\u2588\
          </p>\n<p>val_loss \u2588\u2582\u2581\u2581\u2581</p>\n<p>Run summary:<br>accuracy\
          \ 0.96812 best_epoch 4 best_val_loss 0.21643 epoch 4 loss 0.35643 val_accuracy\
          \ 0.97813 val_loss 0.21643</p>\n<p>Sample question:<br>Q: ['\u0645\u0627\
          \ \u0647\u064A \u0642\u064A\u0645\u0629 \u0627\u0644\u0639\u0642\u062F \u0628\
          \u064A\u0646 \u0634\u0631\u0643\u0629 Under Armor \u0648 Notre Dame\u061F\
          ']</p>\n<p>A: \u0642\u064A\u0645\u062A\u0647 100 \u0645\u0644\u064A\u0648\
          \u0646 \u062F\u0648\u0644\u0627\u0631</p>\n<p>A: [259, 42501, 3234, 966,\
          \ 548, 36270, 259, 36136, 1]</p>\n<p>Input: \u0645\u0627 \u0647\u064A \u0642\
          \u064A\u0645\u0629 \u0627\u0644\u0639\u0642\u062F \u0628\u064A\u0646 \u0634\
          \u0631\u0643\u0629 Under Armor \u0648 Notre Dame\u061F</p>\n<p>Input: [1415,\
          \ 7383, 2588, 23283, 402, 27419, 5373, 259, 11319, 8427, 259, 117220, 341,\
          \ 259, 37126, 34600, 2273, 1]</p>\n<p>Output: </p>\n<p>Output: [0, 250099,\
          \ 1]</p>\n"
        raw: "mT5-small Question Answering training is converging to high accuracy,\
          \ high validation accuracy, near-zero low loss; however, when testing the\
          \ model on trained questions, I am always receiving empty answers.\r\n\r\
          \nExperiment Language: Arabic\r\nDataset used: Arabic SQUAD\r\nOptimizer\
          \ tested: Adam or AdamW with learning rate: 3e-4\r\nloss function: tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\
          \nEpochs tested: 5, 30\r\nI tried to train the model with 30 epochs and\
          \ the same result is obtained:\r\n\r\nOutput: <extra_id_0> Output: [0, 250099,\
          \ 1]\r\n\r\nIt is very strange that the model is converging to high accuracy\
          \ and low loss and I am getting empty sentences during inference. I validated\
          \ the dataset questions and answers and they are correct.\r\n\r\nBelow are\
          \ some important code snippets:\r\n\r\ndef preprocess_function(examples):\r\
          \n    padding = \"max_length\"\r\n    max_length = 200\r\n    inputs = [ex\
          \ for ex in examples[\"question\"]]\r\n    targets = [ex for ex in examples[\"\
          text\"]]\r\n    model_inputs = tokenizer(inputs, max_length=max_length,\
          \ padding=padding, truncation=True)\r\n    labels = tokenizer(targets, max_length=max_length,\
          \ padding=padding, truncation=True)\r\n    model_inputs[\"labels\"] = labels[\"\
          input_ids\"]\r\n    return model_inputs\r\n\r\ndata_collator = DataCollatorForSeq2Seq(\r\
          \n            tokenizer,\r\n            model=model,\r\n            label_pad_token_id=tokenizer.pad_token_id,\r\
          \n            pad_to_multiple_of=64,\r\n            return_tensors=\"np\"\
          ,\r\n        )\r\n\r\ntf_train_dataset = model.prepare_tf_dataset(\r\n \
          \           train_dataset,\r\n            collate_fn=data_collator,\r\n\
          \            batch_size=8,\r\n            shuffle=True,\r\n        )\r\n\
          \r\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \r\nloss =\
          \ tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer=Adam(3e-5),\
          \ loss=loss, metrics=['accuracy'])\r\nOutput:\r\nbelow are the output of\
          \ model.fit() for the first 5 epochs\r\n\r\naccuracy \u2581\u2588\u2588\u2588\
          \u2588\r\n\r\nepoch \u2581\u2583\u2585\u2586\u2588\r\n\r\nloss \u2588\u2581\
          \u2581\u2581\u2581\r\n\r\nval_accuracy \u2581\u2586\u2588\u2588\u2588\r\n\
          \r\nval_loss \u2588\u2582\u2581\u2581\u2581\r\n\r\nRun summary:\r\naccuracy\
          \ 0.96812 best_epoch 4 best_val_loss 0.21643 epoch 4 loss 0.35643 val_accuracy\
          \ 0.97813 val_loss 0.21643\r\n\r\nSample question:\r\nQ: ['\u0645\u0627\
          \ \u0647\u064A \u0642\u064A\u0645\u0629 \u0627\u0644\u0639\u0642\u062F \u0628\
          \u064A\u0646 \u0634\u0631\u0643\u0629 Under Armor \u0648 Notre Dame\u061F\
          ']\r\n\r\nA: \u0642\u064A\u0645\u062A\u0647 100 \u0645\u0644\u064A\u0648\
          \u0646 \u062F\u0648\u0644\u0627\u0631\r\n\r\nA: [259, 42501, 3234, 966,\
          \ 548, 36270, 259, 36136, 1]\r\n\r\nInput: \u0645\u0627 \u0647\u064A \u0642\
          \u064A\u0645\u0629 \u0627\u0644\u0639\u0642\u062F \u0628\u064A\u0646 \u0634\
          \u0631\u0643\u0629 Under Armor \u0648 Notre Dame\u061F\r\n\r\nInput: [1415,\
          \ 7383, 2588, 23283, 402, 27419, 5373, 259, 11319, 8427, 259, 117220, 341,\
          \ 259, 37126, 34600, 2273, 1]\r\n\r\nOutput: <extra_id_0>\r\n\r\nOutput:\
          \ [0, 250099, 1]"
        updatedAt: '2023-05-22T15:03:13.586Z'
      numEdits: 0
      reactions: []
    id: 646b8431e96a751c5252a55c
    type: comment
  author: moustafa123
  content: "mT5-small Question Answering training is converging to high accuracy,\
    \ high validation accuracy, near-zero low loss; however, when testing the model\
    \ on trained questions, I am always receiving empty answers.\r\n\r\nExperiment\
    \ Language: Arabic\r\nDataset used: Arabic SQUAD\r\nOptimizer tested: Adam or\
    \ AdamW with learning rate: 3e-4\r\nloss function: tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\
    \nEpochs tested: 5, 30\r\nI tried to train the model with 30 epochs and the same\
    \ result is obtained:\r\n\r\nOutput: <extra_id_0> Output: [0, 250099, 1]\r\n\r\
    \nIt is very strange that the model is converging to high accuracy and low loss\
    \ and I am getting empty sentences during inference. I validated the dataset questions\
    \ and answers and they are correct.\r\n\r\nBelow are some important code snippets:\r\
    \n\r\ndef preprocess_function(examples):\r\n    padding = \"max_length\"\r\n \
    \   max_length = 200\r\n    inputs = [ex for ex in examples[\"question\"]]\r\n\
    \    targets = [ex for ex in examples[\"text\"]]\r\n    model_inputs = tokenizer(inputs,\
    \ max_length=max_length, padding=padding, truncation=True)\r\n    labels = tokenizer(targets,\
    \ max_length=max_length, padding=padding, truncation=True)\r\n    model_inputs[\"\
    labels\"] = labels[\"input_ids\"]\r\n    return model_inputs\r\n\r\ndata_collator\
    \ = DataCollatorForSeq2Seq(\r\n            tokenizer,\r\n            model=model,\r\
    \n            label_pad_token_id=tokenizer.pad_token_id,\r\n            pad_to_multiple_of=64,\r\
    \n            return_tensors=\"np\",\r\n        )\r\n\r\ntf_train_dataset = model.prepare_tf_dataset(\r\
    \n            train_dataset,\r\n            collate_fn=data_collator,\r\n    \
    \        batch_size=8,\r\n            shuffle=True,\r\n        )\r\n\r\noptimizer\
    \ = tf.keras.optimizers.Adam(learning_rate=3e-5) \r\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\
    \nmodel.compile(optimizer=Adam(3e-5), loss=loss, metrics=['accuracy'])\r\nOutput:\r\
    \nbelow are the output of model.fit() for the first 5 epochs\r\n\r\naccuracy \u2581\
    \u2588\u2588\u2588\u2588\r\n\r\nepoch \u2581\u2583\u2585\u2586\u2588\r\n\r\nloss\
    \ \u2588\u2581\u2581\u2581\u2581\r\n\r\nval_accuracy \u2581\u2586\u2588\u2588\u2588\
    \r\n\r\nval_loss \u2588\u2582\u2581\u2581\u2581\r\n\r\nRun summary:\r\naccuracy\
    \ 0.96812 best_epoch 4 best_val_loss 0.21643 epoch 4 loss 0.35643 val_accuracy\
    \ 0.97813 val_loss 0.21643\r\n\r\nSample question:\r\nQ: ['\u0645\u0627 \u0647\
    \u064A \u0642\u064A\u0645\u0629 \u0627\u0644\u0639\u0642\u062F \u0628\u064A\u0646\
    \ \u0634\u0631\u0643\u0629 Under Armor \u0648 Notre Dame\u061F']\r\n\r\nA: \u0642\
    \u064A\u0645\u062A\u0647 100 \u0645\u0644\u064A\u0648\u0646 \u062F\u0648\u0644\
    \u0627\u0631\r\n\r\nA: [259, 42501, 3234, 966, 548, 36270, 259, 36136, 1]\r\n\r\
    \nInput: \u0645\u0627 \u0647\u064A \u0642\u064A\u0645\u0629 \u0627\u0644\u0639\
    \u0642\u062F \u0628\u064A\u0646 \u0634\u0631\u0643\u0629 Under Armor \u0648 Notre\
    \ Dame\u061F\r\n\r\nInput: [1415, 7383, 2588, 23283, 402, 27419, 5373, 259, 11319,\
    \ 8427, 259, 117220, 341, 259, 37126, 34600, 2273, 1]\r\n\r\nOutput: <extra_id_0>\r\
    \n\r\nOutput: [0, 250099, 1]"
  created_at: 2023-05-22 14:03:13+00:00
  edited: false
  hidden: false
  id: 646b8431e96a751c5252a55c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c91eacbd674f0f74bf82e20662fe9d2f.svg
      fullname: Moustafa Banbouk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: moustafa123
      type: user
    createdAt: '2023-05-22T15:03:39.000Z'
    data:
      status: closed
    id: 646b844bdb697c798a3efa98
    type: status-change
  author: moustafa123
  created_at: 2023-05-22 14:03:39+00:00
  id: 646b844bdb697c798a3efa98
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: google/mt5-base
repo_type: model
status: closed
target_branch: null
title: mT5 Question/Answering fine tuning is generating empty sentences during inference
