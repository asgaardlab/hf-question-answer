!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Bailey24
conflicting_files: null
created_at: 2022-11-30 13:16:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
      fullname: Knight
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bailey24
      type: user
    createdAt: '2022-11-30T13:16:49.000Z'
    data:
      edited: false
      editors:
      - Bailey24
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
          fullname: Knight
          isHf: false
          isPro: false
          name: Bailey24
          type: user
        html: '<p>I love your work</p>

          '
        raw: I love your work
        updatedAt: '2022-11-30T13:16:49.554Z'
      numEdits: 0
      reactions: []
    id: 638757c185f406f24f5493e1
    type: comment
  author: Bailey24
  content: I love your work
  created_at: 2022-11-30 13:16:49+00:00
  edited: false
  hidden: false
  id: 638757c185f406f24f5493e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
      fullname: Knight
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bailey24
      type: user
    createdAt: '2022-11-30T13:35:27.000Z'
    data:
      edited: true
      editors:
      - Bailey24
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
          fullname: Knight
          isHf: false
          isPro: false
          name: Bailey24
          type: user
        html: "<p>Could you please tell me how to speed up the <code>predict</code>\
          \ time?</p>\n<pre><code class=\"language-python\"><span class=\"hljs-comment\"\
          ># predict</span>\n<span class=\"hljs-keyword\">with</span> torch.no_grad():\n\
          \  outputs = model(**inputs)\n</code></pre>\n<p>I mean how  can I use the\
          \ GPU to predict to speed it up?</p>\n"
        raw: "Could you please tell me how to speed up the `predict` time?\n```python\n\
          # predict\nwith torch.no_grad():\n  outputs = model(**inputs)\n```\nI mean\
          \ how  can I use the GPU to predict to speed it up?"
        updatedAt: '2022-11-30T13:36:17.801Z'
      numEdits: 1
      reactions: []
    id: 63875c1f88b39a64e1eaf7e8
    type: comment
  author: Bailey24
  content: "Could you please tell me how to speed up the `predict` time?\n```python\n\
    # predict\nwith torch.no_grad():\n  outputs = model(**inputs)\n```\nI mean how\
    \  can I use the GPU to predict to speed it up?"
  created_at: 2022-11-30 13:35:27+00:00
  edited: true
  hidden: false
  id: 63875c1f88b39a64e1eaf7e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-11-30T15:01:19.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: "<p>You can move both the model and inputs on the GPU, like so:</p>\n\
          <pre><code>import torch\n\ndevice = \"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\"\n\nmodel.to(device)\n\ninputs = {k:v.to(device) for k,v for\
          \ inputs.items()}\n\n# predict\nwith torch.no_grad():\n  outputs = model(**inputs)\n\
          </code></pre>\n"
        raw: "You can move both the model and inputs on the GPU, like so:\n```\nimport\
          \ torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\
          \nmodel.to(device)\n\ninputs = {k:v.to(device) for k,v for inputs.items()}\n\
          \n# predict\nwith torch.no_grad():\n  outputs = model(**inputs)\n```"
        updatedAt: '2022-11-30T15:01:19.841Z'
      numEdits: 0
      reactions: []
    id: 6387703f7f2fdf05ead03acc
    type: comment
  author: nielsr
  content: "You can move both the model and inputs on the GPU, like so:\n```\nimport\
    \ torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)\n\
    \ninputs = {k:v.to(device) for k,v for inputs.items()}\n\n# predict\nwith torch.no_grad():\n\
    \  outputs = model(**inputs)\n```"
  created_at: 2022-11-30 15:01:19+00:00
  edited: false
  hidden: false
  id: 6387703f7f2fdf05ead03acc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
      fullname: Knight
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bailey24
      type: user
    createdAt: '2022-12-01T04:05:47.000Z'
    data:
      edited: true
      editors:
      - Bailey24
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
          fullname: Knight
          isHf: false
          isPro: false
          name: Bailey24
          type: user
        html: "<p>Thanks a lot.</p>\n<p>I preprocess the prompts and image, but I\
          \ got the error.</p>\n<pre><code class=\"language-bash\">ValueError    \
          \                            Traceback (most recent call last)\n&lt;ipython-input-59-6c4240d4c8a3&gt;\
          \ <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n     11 <span class=\"\
          hljs-comment\"># prompts = np.array(prompts)</span>\n     12 <span class=\"\
          hljs-comment\"># prompts = torch.from_numpy(prompts)</span>\n---&gt; 13\
          \ inputs = processor(text=prompts, images=[image] * len(prompts), padding=<span\
          \ class=\"hljs-string\">\"max_length\"</span>, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>)\n\n2 frames\n/usr/local/lib/python3.8/dist-packages/transformers/models/clipseg/processing_clipseg.py\
          \ <span class=\"hljs-keyword\">in</span> __call__(self, text, images, return_tensors,\
          \ **kwargs)\n     81 \n     82         <span class=\"hljs-keyword\">if</span>\
          \ text is not None:\n---&gt; 83             encoding = self.tokenizer(text,\
          \ return_tensors=return_tensors, **kwargs)\n     84 \n     85         <span\
          \ class=\"hljs-keyword\">if</span> images is not None:\n\n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\
          \ <span class=\"hljs-keyword\">in</span> __call__(self, text, text_pair,\
          \ text_target, text_pair_target, add_special_tokens, padding, truncation,\
          \ max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors,\
          \ return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2518             <span class=\"hljs-keyword\">if</span>\
          \ not self._in_target_context_manager:\n   2519                 self._switch_to_input_mode()\n\
          -&gt; 2520             encodings = self._call_one(text=text, text_pair=text_pair,\
          \ **all_kwargs)\n   2521         <span class=\"hljs-keyword\">if</span>\
          \ text_target is not None:\n   2522             self._switch_to_target_mode()\n\
          \n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\
          \ <span class=\"hljs-keyword\">in</span> _call_one(self, text, text_pair,\
          \ add_special_tokens, padding, truncation, max_length, stride, is_split_into_words,\
          \ pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask,\
          \ return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping,\
          \ return_length, verbose, **kwargs)\n   2576 \n   2577         <span class=\"\
          hljs-keyword\">if</span> not _is_valid_text_input(text):\n-&gt; 2578   \
          \          raise ValueError(\n   2579                 <span class=\"hljs-string\"\
          >\"text input must of type `str` (single example), `List[str]` (batch or\
          \ single pretokenized example) \"</span>\n   2580                 <span\
          \ class=\"hljs-string\">\"or `List[List[str]]` (batch of pretokenized examples).\"\
          </span>\n\nValueError: text input must of <span class=\"hljs-built_in\"\
          >type</span> `str` (single example), `List[str]` (batch or single pretokenized\
          \ example) or `List[List[str]]` (batch of pretokenized examples).\n</code></pre>\n\
          <p>Here is my code.</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ CLIPTokenizer\ntokenizer = CLIPTokenizer.from_pretrained(<span class=\"\
          hljs-string\">\"CIDAS/clipseg-rd64-refined\"</span>)\nprompts = [<span class=\"\
          hljs-string\">\"orange\"</span>, <span class=\"hljs-string\">\"violet\"\
          </span>, <span class=\"hljs-string\">\"green\"</span>, <span class=\"hljs-string\"\
          >\"black\"</span>]\nprompts = tokenizer(prompts, padding=<span class=\"\
          hljs-literal\">True</span>, return_tensors=<span class=\"hljs-string\">\"\
          pt\"</span>)\n\n<span class=\"hljs-keyword\">from</span> torchvision <span\
          \ class=\"hljs-keyword\">import</span> transforms\nto_tensor = transforms.ToTensor()\n\
          image = to_tensor(image)\n\ndevice = <span class=\"hljs-string\">\"cuda\"\
          </span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"\
          cpu\"</span>\nmodel.to(device)\nprompts.to(device)\nimage.to(device)\n\n\
          inputs = processor(text=prompts, images=[image] * <span class=\"hljs-built_in\"\
          >len</span>(prompts), padding=<span class=\"hljs-string\">\"max_length\"\
          </span>, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n</code></pre>\n\
          <p>Then I got the error.<br>Could you please help me?</p>\n"
        raw: "Thanks a lot.\n\nI preprocess the prompts and image, but I got the error.\n\
          ```bash\nValueError                                Traceback (most recent\
          \ call last)\n<ipython-input-59-6c4240d4c8a3> in <module>\n     11 # prompts\
          \ = np.array(prompts)\n     12 # prompts = torch.from_numpy(prompts)\n--->\
          \ 13 inputs = processor(text=prompts, images=[image] * len(prompts), padding=\"\
          max_length\", return_tensors=\"pt\")\n\n2 frames\n/usr/local/lib/python3.8/dist-packages/transformers/models/clipseg/processing_clipseg.py\
          \ in __call__(self, text, images, return_tensors, **kwargs)\n     81 \n\
          \     82         if text is not None:\n---> 83             encoding = self.tokenizer(text,\
          \ return_tensors=return_tensors, **kwargs)\n     84 \n     85         if\
          \ images is not None:\n\n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\
          \ in __call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens,\
          \ padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of,\
          \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2518             if not self._in_target_context_manager:\n\
          \   2519                 self._switch_to_input_mode()\n-> 2520         \
          \    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n\
          \   2521         if text_target is not None:\n   2522             self._switch_to_target_mode()\n\
          \n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\
          \ in _call_one(self, text, text_pair, add_special_tokens, padding, truncation,\
          \ max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors,\
          \ return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2576 \n   2577         if not _is_valid_text_input(text):\n\
          -> 2578             raise ValueError(\n   2579                 \"text input\
          \ must of type `str` (single example), `List[str]` (batch or single pretokenized\
          \ example) \"\n   2580                 \"or `List[List[str]]` (batch of\
          \ pretokenized examples).\"\n\nValueError: text input must of type `str`\
          \ (single example), `List[str]` (batch or single pretokenized example) or\
          \ `List[List[str]]` (batch of pretokenized examples).\n```\nHere is my code.\n\
          ```python\nfrom transformers import CLIPTokenizer\ntokenizer = CLIPTokenizer.from_pretrained(\"\
          CIDAS/clipseg-rd64-refined\")\nprompts = [\"orange\", \"violet\", \"green\"\
          , \"black\"]\nprompts = tokenizer(prompts, padding=True, return_tensors=\"\
          pt\")\n\nfrom torchvision import transforms\nto_tensor = transforms.ToTensor()\n\
          image = to_tensor(image)\n\ndevice = \"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\"\nmodel.to(device)\nprompts.to(device)\nimage.to(device)\n\
          \ninputs = processor(text=prompts, images=[image] * len(prompts), padding=\"\
          max_length\", return_tensors=\"pt\")\n```\nThen I got the error.\nCould\
          \ you please help me?"
        updatedAt: '2022-12-01T04:07:36.632Z'
      numEdits: 1
      reactions: []
    id: 6388281b26952adc66f1b96b
    type: comment
  author: Bailey24
  content: "Thanks a lot.\n\nI preprocess the prompts and image, but I got the error.\n\
    ```bash\nValueError                                Traceback (most recent call\
    \ last)\n<ipython-input-59-6c4240d4c8a3> in <module>\n     11 # prompts = np.array(prompts)\n\
    \     12 # prompts = torch.from_numpy(prompts)\n---> 13 inputs = processor(text=prompts,\
    \ images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"pt\"\
    )\n\n2 frames\n/usr/local/lib/python3.8/dist-packages/transformers/models/clipseg/processing_clipseg.py\
    \ in __call__(self, text, images, return_tensors, **kwargs)\n     81 \n     82\
    \         if text is not None:\n---> 83             encoding = self.tokenizer(text,\
    \ return_tensors=return_tensors, **kwargs)\n     84 \n     85         if images\
    \ is not None:\n\n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\
    \ in __call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens,\
    \ padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of,\
    \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
    \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
    \ **kwargs)\n   2518             if not self._in_target_context_manager:\n   2519\
    \                 self._switch_to_input_mode()\n-> 2520             encodings\
    \ = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n   2521    \
    \     if text_target is not None:\n   2522             self._switch_to_target_mode()\n\
    \n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\
    \ in _call_one(self, text, text_pair, add_special_tokens, padding, truncation,\
    \ max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors,\
    \ return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask,\
    \ return_offsets_mapping, return_length, verbose, **kwargs)\n   2576 \n   2577\
    \         if not _is_valid_text_input(text):\n-> 2578             raise ValueError(\n\
    \   2579                 \"text input must of type `str` (single example), `List[str]`\
    \ (batch or single pretokenized example) \"\n   2580                 \"or `List[List[str]]`\
    \ (batch of pretokenized examples).\"\n\nValueError: text input must of type `str`\
    \ (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]`\
    \ (batch of pretokenized examples).\n```\nHere is my code.\n```python\nfrom transformers\
    \ import CLIPTokenizer\ntokenizer = CLIPTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\"\
    )\nprompts = [\"orange\", \"violet\", \"green\", \"black\"]\nprompts = tokenizer(prompts,\
    \ padding=True, return_tensors=\"pt\")\n\nfrom torchvision import transforms\n\
    to_tensor = transforms.ToTensor()\nimage = to_tensor(image)\n\ndevice = \"cuda\"\
    \ if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nprompts.to(device)\n\
    image.to(device)\n\ninputs = processor(text=prompts, images=[image] * len(prompts),\
    \ padding=\"max_length\", return_tensors=\"pt\")\n```\nThen I got the error.\n\
    Could you please help me?"
  created_at: 2022-12-01 04:05:47+00:00
  edited: true
  hidden: false
  id: 6388281b26952adc66f1b96b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
      fullname: Knight
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bailey24
      type: user
    createdAt: '2022-12-02T01:14:29.000Z'
    data:
      edited: false
      editors:
      - Bailey24
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
          fullname: Knight
          isHf: false
          isPro: false
          name: Bailey24
          type: user
        html: '<p>Hi, I''m an NLP newer, so I didn''t know how to input <code>prompts</code>
          into <code>processor</code>.<br>Because  I think the <code>processor</code>
          requires the string list, but I want to use GPU, the <code>prompts</code>
          have to become the <code>tensor</code>. I''m confused about it.<br>Could
          you please help me?</p>

          '
        raw: "Hi, I'm an NLP newer, so I didn't know how to input `prompts` into `processor`.\
          \ \nBecause  I think the `processor` requires the string list, but I want\
          \ to use GPU, the `prompts` have to become the `tensor`. I'm confused about\
          \ it.\nCould you please help me?"
        updatedAt: '2022-12-02T01:14:29.695Z'
      numEdits: 0
      reactions: []
    id: 638951755a3d2a335627dd96
    type: comment
  author: Bailey24
  content: "Hi, I'm an NLP newer, so I didn't know how to input `prompts` into `processor`.\
    \ \nBecause  I think the `processor` requires the string list, but I want to use\
    \ GPU, the `prompts` have to become the `tensor`. I'm confused about it.\nCould\
    \ you please help me?"
  created_at: 2022-12-02 01:14:29+00:00
  edited: false
  hidden: false
  id: 638951755a3d2a335627dd96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
      fullname: Knight
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bailey24
      type: user
    createdAt: '2022-12-02T06:09:14.000Z'
    data:
      edited: false
      editors:
      - Bailey24
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
          fullname: Knight
          isHf: false
          isPro: false
          name: Bailey24
          type: user
        html: "<p>I see.<br>Doesn't it as follow?</p>\n<pre><code class=\"language-python\"\
          >inputs = processor(text=prompts, images=[image] * <span class=\"hljs-built_in\"\
          >len</span>(prompts), padding=<span class=\"hljs-string\">\"max_length\"\
          </span>, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\ninputs.to(device)\n\
          \n<span class=\"hljs-keyword\">with</span> torch.no_grad():\n    outputs\
          \ = model(**inputs)\n</code></pre>\n<p>Because it speed up.</p>\n"
        raw: "I see.\nDoesn't it as follow?\n```python\ninputs = processor(text=prompts,\
          \ images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"\
          pt\")\n\ninputs.to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\
          ```\nBecause it speed up."
        updatedAt: '2022-12-02T06:09:14.450Z'
      numEdits: 0
      reactions: []
    id: 6389968af7d3b0df0922727b
    type: comment
  author: Bailey24
  content: "I see.\nDoesn't it as follow?\n```python\ninputs = processor(text=prompts,\
    \ images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"pt\"\
    )\n\ninputs.to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\
    ```\nBecause it speed up."
  created_at: 2022-12-02 06:09:14+00:00
  edited: false
  hidden: false
  id: 6389968af7d3b0df0922727b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: CIDAS/clipseg-rd64-refined
repo_type: model
status: open
target_branch: null
title: Nice work :)
