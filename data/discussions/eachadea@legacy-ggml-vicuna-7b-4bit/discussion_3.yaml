!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phew
conflicting_files: null
created_at: 2023-04-06 16:23:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/XwJk6RQSnGyuboKPpJr4M.jpeg?w=200&h=200&f=face
      fullname: Da
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phew
      type: user
    createdAt: '2023-04-06T17:23:36.000Z'
    data:
      edited: false
      editors:
      - Phew
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/XwJk6RQSnGyuboKPpJr4M.jpeg?w=200&h=200&f=face
          fullname: Da
          isHf: false
          isPro: false
          name: Phew
          type: user
        html: '<p>I tried most models that are coming in the recent days and this
          is the best one to run locally, fater than gpt4all and way more accurate.<br>This
          is my start script with llama.cpp</p>

          <p>title llama.cpp<br>:start<br>main -i --interactive-first -r "### Human:"
          --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty 1.2 --instruct -m ggml-vicuna-7b-4bit-rev1.bin<br>pause<br>goto
          start</p>

          '
        raw: "I tried most models that are coming in the recent days and this is the\
          \ best one to run locally, fater than gpt4all and way more accurate.\r\n\
          This is my start script with llama.cpp\r\n\r\ntitle llama.cpp\r\n:start\r\
          \nmain -i --interactive-first -r \"### Human:\" --temp 0 -c 2048 -n -1 --ignore-eos\
          \ --repeat_penalty 1.2 --instruct -m ggml-vicuna-7b-4bit-rev1.bin\r\npause\r\
          \ngoto start"
        updatedAt: '2023-04-06T17:23:36.844Z'
      numEdits: 0
      reactions: []
    id: 642f001825d0798a4bfe902b
    type: comment
  author: Phew
  content: "I tried most models that are coming in the recent days and this is the\
    \ best one to run locally, fater than gpt4all and way more accurate.\r\nThis is\
    \ my start script with llama.cpp\r\n\r\ntitle llama.cpp\r\n:start\r\nmain -i --interactive-first\
    \ -r \"### Human:\" --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty 1.2 --instruct\
    \ -m ggml-vicuna-7b-4bit-rev1.bin\r\npause\r\ngoto start"
  created_at: 2023-04-06 16:23:36+00:00
  edited: false
  hidden: false
  id: 642f001825d0798a4bfe902b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
      fullname: Aoum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ai2p
      type: user
    createdAt: '2023-04-07T09:59:39.000Z'
    data:
      edited: true
      editors:
      - ai2p
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
          fullname: Aoum
          isHf: false
          isPro: false
          name: ai2p
          type: user
        html: '<blockquote>

          <p>I tried most models that are coming in the recent days and this is the
          best one to run locally, fater than gpt4all and way more accurate. This
          is my start script with llama.cpp<br>main -i --interactive-first -r "###
          Human:" --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty 1.2 --instruct
          -m ggml-vicuna-7b-4bit-rev1.bin</p>

          </blockquote>

          <p>You are my hero! Thanks to your comment, I was able to compile llama.cpp
          from <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a><br>under
          the Windows 10, then run ggml-vicuna-7b-4bit-rev1.bin with your cmd line
          that I cited above. It works on laptop with 16 Gb RAM and rather fast! I
          agree that it may be the best LLM to run locally!</p>

          <p>And it seems that it can write much more correct and longer program code
          than gpt4all! It''s just amazing!</p>

          <p>But sometimes, after a few answers, it just freezes while continuing
          to load the CPU. Has anyone noticed this?</p>

          '
        raw: '> I tried most models that are coming in the recent days and this is
          the best one to run locally, fater than gpt4all and way more accurate. This
          is my start script with llama.cpp

          > main -i --interactive-first -r "### Human:" --temp 0 -c 2048 -n -1 --ignore-eos
          --repeat_penalty 1.2 --instruct -m ggml-vicuna-7b-4bit-rev1.bin


          You are my hero! Thanks to your comment, I was able to compile llama.cpp
          from https://github.com/ggerganov/llama.cpp

          under the Windows 10, then run ggml-vicuna-7b-4bit-rev1.bin with your cmd
          line that I cited above. It works on laptop with 16 Gb RAM and rather fast!
          I agree that it may be the best LLM to run locally!


          And it seems that it can write much more correct and longer program code
          than gpt4all! It''s just amazing!


          But sometimes, after a few answers, it just freezes while continuing to
          load the CPU. Has anyone noticed this?'
        updatedAt: '2023-04-07T10:00:48.755Z'
      numEdits: 1
      reactions: []
    id: 642fe98b43a53c86b3fbc27b
    type: comment
  author: ai2p
  content: '> I tried most models that are coming in the recent days and this is the
    best one to run locally, fater than gpt4all and way more accurate. This is my
    start script with llama.cpp

    > main -i --interactive-first -r "### Human:" --temp 0 -c 2048 -n -1 --ignore-eos
    --repeat_penalty 1.2 --instruct -m ggml-vicuna-7b-4bit-rev1.bin


    You are my hero! Thanks to your comment, I was able to compile llama.cpp from
    https://github.com/ggerganov/llama.cpp

    under the Windows 10, then run ggml-vicuna-7b-4bit-rev1.bin with your cmd line
    that I cited above. It works on laptop with 16 Gb RAM and rather fast! I agree
    that it may be the best LLM to run locally!


    And it seems that it can write much more correct and longer program code than
    gpt4all! It''s just amazing!


    But sometimes, after a few answers, it just freezes while continuing to load the
    CPU. Has anyone noticed this?'
  created_at: 2023-04-07 08:59:39+00:00
  edited: true
  hidden: false
  id: 642fe98b43a53c86b3fbc27b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635fff386f410322ee99f649/nYW-ynSEd1uK17vHbGInd.png?w=200&h=200&f=face
      fullname: Ravi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rpuvvula
      type: user
    createdAt: '2023-04-13T12:01:52.000Z'
    data:
      edited: false
      editors:
      - rpuvvula
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635fff386f410322ee99f649/nYW-ynSEd1uK17vHbGInd.png?w=200&h=200&f=face
          fullname: Ravi
          isHf: false
          isPro: false
          name: rpuvvula
          type: user
        html: '<p>How can i use ggml-vicuna-7b-4bit-rev1.bin model to run web ui?</p>

          '
        raw: How can i use ggml-vicuna-7b-4bit-rev1.bin model to run web ui?
        updatedAt: '2023-04-13T12:01:52.284Z'
      numEdits: 0
      reactions: []
    id: 6437ef3094faafc1a2df15d8
    type: comment
  author: rpuvvula
  content: How can i use ggml-vicuna-7b-4bit-rev1.bin model to run web ui?
  created_at: 2023-04-13 11:01:52+00:00
  edited: false
  hidden: false
  id: 6437ef3094faafc1a2df15d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb787207e4295944d7f4688a4ae47c80.svg
      fullname: Steve Barber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sonic7
      type: user
    createdAt: '2023-04-19T02:21:51.000Z'
    data:
      edited: true
      editors:
      - Sonic7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb787207e4295944d7f4688a4ae47c80.svg
          fullname: Steve Barber
          isHf: false
          isPro: false
          name: Sonic7
          type: user
        html: "<blockquote>\n<blockquote>\n<p>I tried most models that are coming\
          \ in the recent days and this is the best one to run locally, fater than\
          \ gpt4all and way more accurate. This is my start script with llama.cpp<br>main\
          \ -i --interactive-first -r \"### Human:\" --temp 0 -c 2048 -n -1 --ignore-eos\
          \ --repeat_penalty 1.2 --instruct -m ggml-vicuna-7b-4bit-rev1.bin</p>\n\
          </blockquote>\n<p>You are my hero! Thanks to your comment, I was able to\
          \ compile llama.cpp from <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp\"\
          >https://github.com/ggerganov/llama.cpp</a><br>under the Windows 10, then\
          \ run ggml-vicuna-7b-4bit-rev1.bin with your cmd line that I cited above.\
          \ It works on laptop with 16 Gb RAM and rather fast! I agree that it may\
          \ be the best LLM to run locally!</p>\n<p>And it seems that it can write\
          \ much more correct and longer program code than gpt4all! It's just amazing!</p>\n\
          <p>But sometimes, after a few answers, it just freezes while continuing\
          \ to load the CPU. Has anyone noticed this?</p>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;phew&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/phew\">@<span class=\"underline\">phew</span></a></span>\n\
          \n\t</span></span> <span data-props=\"{&quot;user&quot;:&quot;ai2p&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ai2p\"\
          >@<span class=\"underline\">ai2p</span></a></span>\n\n\t</span></span> \
          \ - I just saw your comments here .... hey I'd reached the exact same conclusion\
          \ - I've also experienced the same performance on my 16GB RAM laptop ....\
          \ the vicuna 7b model seems to give the best results overall.</p>\n"
        raw: "> > I tried most models that are coming in the recent days and this\
          \ is the best one to run locally, fater than gpt4all and way more accurate.\
          \ This is my start script with llama.cpp\n> > main -i --interactive-first\
          \ -r \"### Human:\" --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty\
          \ 1.2 --instruct -m ggml-vicuna-7b-4bit-rev1.bin\n> \n> You are my hero!\
          \ Thanks to your comment, I was able to compile llama.cpp from https://github.com/ggerganov/llama.cpp\n\
          > under the Windows 10, then run ggml-vicuna-7b-4bit-rev1.bin with your\
          \ cmd line that I cited above. It works on laptop with 16 Gb RAM and rather\
          \ fast! I agree that it may be the best LLM to run locally!\n> \n> And it\
          \ seems that it can write much more correct and longer program code than\
          \ gpt4all! It's just amazing!\n> \n> But sometimes, after a few answers,\
          \ it just freezes while continuing to load the CPU. Has anyone noticed this?\n\
          \n\n@phew @ai2p  - I just saw your comments here .... hey I'd reached the\
          \ exact same conclusion - I've also experienced the same performance on\
          \ my 16GB RAM laptop .... the vicuna 7b model seems to give the best results\
          \ overall."
        updatedAt: '2023-04-19T02:24:28.617Z'
      numEdits: 1
      reactions: []
    id: 643f503f8d9a6e3bba3c9245
    type: comment
  author: Sonic7
  content: "> > I tried most models that are coming in the recent days and this is\
    \ the best one to run locally, fater than gpt4all and way more accurate. This\
    \ is my start script with llama.cpp\n> > main -i --interactive-first -r \"###\
    \ Human:\" --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty 1.2 --instruct\
    \ -m ggml-vicuna-7b-4bit-rev1.bin\n> \n> You are my hero! Thanks to your comment,\
    \ I was able to compile llama.cpp from https://github.com/ggerganov/llama.cpp\n\
    > under the Windows 10, then run ggml-vicuna-7b-4bit-rev1.bin with your cmd line\
    \ that I cited above. It works on laptop with 16 Gb RAM and rather fast! I agree\
    \ that it may be the best LLM to run locally!\n> \n> And it seems that it can\
    \ write much more correct and longer program code than gpt4all! It's just amazing!\n\
    > \n> But sometimes, after a few answers, it just freezes while continuing to\
    \ load the CPU. Has anyone noticed this?\n\n\n@phew @ai2p  - I just saw your comments\
    \ here .... hey I'd reached the exact same conclusion - I've also experienced\
    \ the same performance on my 16GB RAM laptop .... the vicuna 7b model seems to\
    \ give the best results overall."
  created_at: 2023-04-19 01:21:51+00:00
  edited: true
  hidden: false
  id: 643f503f8d9a6e3bba3c9245
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
      fullname: amkdg
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eachadea
      type: user
    createdAt: '2023-04-19T06:09:27.000Z'
    data:
      edited: true
      editors:
      - eachadea
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
          fullname: amkdg
          isHf: false
          isPro: false
          name: eachadea
          type: user
        html: '<p>Y''all make sure to update to <a href="https://huggingface.co/eachadea/ggml-vicuna-7b-1.1">v1.1</a></p>

          '
        raw: Y'all make sure to update to [v1.1](https://huggingface.co/eachadea/ggml-vicuna-7b-1.1)
        updatedAt: '2023-04-19T06:10:01.588Z'
      numEdits: 1
      reactions: []
    id: 643f8597a8d9413988398577
    type: comment
  author: eachadea
  content: Y'all make sure to update to [v1.1](https://huggingface.co/eachadea/ggml-vicuna-7b-1.1)
  created_at: 2023-04-19 05:09:27+00:00
  edited: true
  hidden: false
  id: 643f8597a8d9413988398577
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: eachadea/legacy-ggml-vicuna-7b-4bit
repo_type: model
status: open
target_branch: null
title: Outstanding Model
