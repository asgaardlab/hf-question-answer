!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ipark
conflicting_files: null
created_at: 2023-02-20 20:55:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3ceebfba9a98a24d69febc9679c1c60.svg
      fullname: ipark
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ipark
      type: user
    createdAt: '2023-02-20T20:55:13.000Z'
    data:
      edited: false
      editors:
      - ipark
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3ceebfba9a98a24d69febc9679c1c60.svg
          fullname: ipark
          isHf: false
          isPro: false
          name: ipark
          type: user
        html: '<p>Thanks for sharing great work!  I have two questions:</p>

          <p>Q1.  For fine-tuning (Example 2), is there a minimum memory requirement
          in GPU?<br>In the ZymCTRL paper, Nvidia A100 GPUs with 40GB memory were
          used.  My GUS has 12GB memory, wondering if it matters.<br>Since I got same
          error <a rel="nofollow" href="https://discuss.huggingface.co/t/cuda-out-of-memory-error/17959/4">https://discuss.huggingface.co/t/cuda-out-of-memory-error/17959/4</a><br>Reduced
          batch size to 1, as well as block size down to 32, but still got the same
          error.<br>If I used CPU instead with --no_cuda, I could fine-tune albeit
          a way long time.</p>

          <p>Q2.  Does it make sense to fine-tune the pretrained ZymCTRL with a custom
          tokenizer (in which smiles strings are tokenized, instead of EC numbers)?  In
          general, any restriction on the length of prompts in the train set? </p>

          <p>Thank you very much.</p>

          '
        raw: "Thanks for sharing great work!  I have two questions:\r\n\r\nQ1.  For\
          \ fine-tuning (Example 2), is there a minimum memory requirement in GPU?\r\
          \nIn the ZymCTRL paper, Nvidia A100 GPUs with 40GB memory were used.  My\
          \ GUS has 12GB memory, wondering if it matters.\r\nSince I got same error\
          \ https://discuss.huggingface.co/t/cuda-out-of-memory-error/17959/4\r\n\
          Reduced batch size to 1, as well as block size down to 32, but still got\
          \ the same error.\r\nIf I used CPU instead with --no_cuda, I could fine-tune\
          \ albeit a way long time.\r\n\r\nQ2.  Does it make sense to fine-tune the\
          \ pretrained ZymCTRL with a custom tokenizer (in which smiles strings are\
          \ tokenized, instead of EC numbers)?  In general, any restriction on the\
          \ length of prompts in the train set? \r\n\r\nThank you very much."
        updatedAt: '2023-02-20T20:55:13.505Z'
      numEdits: 0
      reactions: []
    id: 63f3de310be81bdc5d976ceb
    type: comment
  author: ipark
  content: "Thanks for sharing great work!  I have two questions:\r\n\r\nQ1.  For\
    \ fine-tuning (Example 2), is there a minimum memory requirement in GPU?\r\nIn\
    \ the ZymCTRL paper, Nvidia A100 GPUs with 40GB memory were used.  My GUS has\
    \ 12GB memory, wondering if it matters.\r\nSince I got same error https://discuss.huggingface.co/t/cuda-out-of-memory-error/17959/4\r\
    \nReduced batch size to 1, as well as block size down to 32, but still got the\
    \ same error.\r\nIf I used CPU instead with --no_cuda, I could fine-tune albeit\
    \ a way long time.\r\n\r\nQ2.  Does it make sense to fine-tune the pretrained\
    \ ZymCTRL with a custom tokenizer (in which smiles strings are tokenized, instead\
    \ of EC numbers)?  In general, any restriction on the length of prompts in the\
    \ train set? \r\n\r\nThank you very much."
  created_at: 2023-02-20 20:55:13+00:00
  edited: false
  hidden: false
  id: 63f3de310be81bdc5d976ceb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-02-21T07:17:52.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi ipark,</p>\n<p>Thanks a lot for posting! It sounds like 12GB\
          \ may not be enough to fit the model. As you say I\u2019ve only tried with\
          \ A100 and A40s but from your error it sounds like you will need more than\
          \ 12GB or use the CPU. If I remember correctly, there is a documentation\
          \ page in HuggingFace with tricks to train large models (but I don\u2019\
          t seem to find it now) and it had tips to try to fit the model into \u2018\
          smaller\u2019 GPUs.</p>\n<p>Q2: Yes you can fine-tune with a different tokenizer\
          \ as well. I expect however that you should fine-tune for quite long because\
          \ as it is ZymCTRL doesn\u2019t have any knowledge of chemistry. The inout\
          \ limit is 1024.</p>\n"
        raw: "Hi ipark,\n\nThanks a lot for posting! It sounds like 12GB may not be\
          \ enough to fit the model. As you say I\u2019ve only tried with A100 and\
          \ A40s but from your error it sounds like you will need more than 12GB or\
          \ use the CPU. If I remember correctly, there is a documentation page in\
          \ HuggingFace with tricks to train large models (but I don\u2019t seem to\
          \ find it now) and it had tips to try to fit the model into \u2018smaller\u2019\
          \ GPUs.\n\nQ2: Yes you can fine-tune with a different tokenizer as well.\
          \ I expect however that you should fine-tune for quite long because as it\
          \ is ZymCTRL doesn\u2019t have any knowledge of chemistry. The inout limit\
          \ is 1024."
        updatedAt: '2023-02-21T07:17:52.272Z'
      numEdits: 0
      reactions: []
    id: 63f470205d5278a01631da87
    type: comment
  author: nferruz
  content: "Hi ipark,\n\nThanks a lot for posting! It sounds like 12GB may not be\
    \ enough to fit the model. As you say I\u2019ve only tried with A100 and A40s\
    \ but from your error it sounds like you will need more than 12GB or use the CPU.\
    \ If I remember correctly, there is a documentation page in HuggingFace with tricks\
    \ to train large models (but I don\u2019t seem to find it now) and it had tips\
    \ to try to fit the model into \u2018smaller\u2019 GPUs.\n\nQ2: Yes you can fine-tune\
    \ with a different tokenizer as well. I expect however that you should fine-tune\
    \ for quite long because as it is ZymCTRL doesn\u2019t have any knowledge of chemistry.\
    \ The inout limit is 1024."
  created_at: 2023-02-21 07:17:52+00:00
  edited: false
  hidden: false
  id: 63f470205d5278a01631da87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3ceebfba9a98a24d69febc9679c1c60.svg
      fullname: ipark
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ipark
      type: user
    createdAt: '2023-02-21T20:11:30.000Z'
    data:
      edited: false
      editors:
      - ipark
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3ceebfba9a98a24d69febc9679c1c60.svg
          fullname: ipark
          isHf: false
          isPro: false
          name: ipark
          type: user
        html: '<p>Thank you Noelia! </p>

          <p>This might be for the tricks in HuggingFace documentation you are referring
          to<br><a href="https://huggingface.co/docs/transformers/v4.18.0/en/performance">https://huggingface.co/docs/transformers/v4.18.0/en/performance</a><br>Will
          look into this.</p>

          <p>Thanks again!</p>

          '
        raw: "Thank you Noelia! \n\nThis might be for the tricks in HuggingFace documentation\
          \ you are referring to \nhttps://huggingface.co/docs/transformers/v4.18.0/en/performance\n\
          Will look into this.\n\nThanks again!"
        updatedAt: '2023-02-21T20:11:30.731Z'
      numEdits: 0
      reactions: []
    id: 63f52572cc1dd31686933da1
    type: comment
  author: ipark
  content: "Thank you Noelia! \n\nThis might be for the tricks in HuggingFace documentation\
    \ you are referring to \nhttps://huggingface.co/docs/transformers/v4.18.0/en/performance\n\
    Will look into this.\n\nThanks again!"
  created_at: 2023-02-21 20:11:30+00:00
  edited: false
  hidden: false
  id: 63f52572cc1dd31686933da1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: AI4PD/ZymCTRL
repo_type: model
status: open
target_branch: null
title: Fine-tuning memory and custom tokenizer
