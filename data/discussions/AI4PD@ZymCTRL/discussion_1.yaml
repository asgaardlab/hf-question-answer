!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 0jj0
conflicting_files: null
created_at: 2022-12-02 10:13:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
      fullname: Elephas Maximus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0jj0
      type: user
    createdAt: '2022-12-02T10:13:48.000Z'
    data:
      edited: false
      editors:
      - 0jj0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
          fullname: Elephas Maximus
          isHf: false
          isPro: false
          name: 0jj0
          type: user
        html: "<p>Hi Noelia,<br>it's me, Kuiroscity from the discussion of ProtGTP2.\
          \ I tried to use ZymCTRL after downloading the models manually using this\
          \ wrapper script:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-comment\">#!/usr/bin/env python</span>\n\n<span class=\"hljs-comment\"\
          >#@title Import libraries and initialize model</span>\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> GPT2LMHeadModel,\
          \ AutoTokenizer\n<span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Libraries imported successfully.\"\
          </span>)\n\nenzyme_class = <span class=\"hljs-string\">\"1.1.3.4\"</span>\n\
          device = torch.device(<span class=\"hljs-string\">'cpu'</span>)\ntokenizer\
          \ = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">'/mnt/e/ZymCTRL/model'</span>)\n\
          model = GPT2LMHeadModel.from_pretrained(<span class=\"hljs-string\">'/mnt/e/ZymCTRL/model'</span>).to(device)\n\
          input_ids = tokenizer.encode(enzyme_class,return_tensors=<span class=\"\
          hljs-string\">'pt'</span>).to(device)\noutput = model.generate(input_ids,\
          \ top_k=<span class=\"hljs-number\">8</span>, repetition_penalty=<span class=\"\
          hljs-number\">1.2</span>, max_length=<span class=\"hljs-number\">1024</span>,\n\
          \                        eos_token_id=<span class=\"hljs-number\">1</span>,pad_token_id=<span\
          \ class=\"hljs-number\">0</span>,do_sample=<span class=\"hljs-literal\"\
          >True</span>, num_return_sequences=<span class=\"hljs-number\">100</span>)\n\
          <span class=\"hljs-built_in\">print</span>(output)\nf = <span class=\"hljs-built_in\"\
          >open</span>(<span class=\"hljs-string\">\"output.test\"</span>, <span class=\"\
          hljs-string\">\"w\"</span>)\nf.write(output)\nf.close()\n</code></pre>\n\
          <p>I have not figured out how to enable GPU on our local desktop (we're\
          \ running WSL2 on a Windows machine that does have a GPU but have not been\
          \ able to figure out on how to get the GPU to work in the Ubuntu OS of WLS2);\
          \ therefore I changed the <code>device</code> parameter to <code>cpu</code>\
          \ instead. The output I got was not the list of sequences but some kind\
          \ of a matrix.<br>I guess there was something wrong from my side with the\
          \ script but I would very much appreciate if you could have a look at it.<br>Thanks\
          \ a lot,<br>Best regards,<br>Kurioscity</p>\n"
        raw: "Hi Noelia,\r\nit's me, Kuiroscity from the discussion of ProtGTP2. I\
          \ tried to use ZymCTRL after downloading the models manually using this\
          \ wrapper script:\r\n```python\r\n#!/usr/bin/env python\r\n\r\n#@title Import\
          \ libraries and initialize model\r\nfrom transformers import GPT2LMHeadModel,\
          \ AutoTokenizer\r\nimport torch\r\nimport os\r\nprint(\"Libraries imported\
          \ successfully.\")\r\n\r\nenzyme_class = \"1.1.3.4\"\r\ndevice = torch.device('cpu')\r\
          \ntokenizer = AutoTokenizer.from_pretrained('/mnt/e/ZymCTRL/model')\r\n\
          model = GPT2LMHeadModel.from_pretrained('/mnt/e/ZymCTRL/model').to(device)\r\
          \ninput_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)\r\
          \noutput = model.generate(input_ids, top_k=8, repetition_penalty=1.2, max_length=1024,\r\
          \n                        eos_token_id=1,pad_token_id=0,do_sample=True,\
          \ num_return_sequences=100)\r\nprint(output)\r\nf = open(\"output.test\"\
          , \"w\")\r\nf.write(output)\r\nf.close()\r\n```\r\nI have not figured out\
          \ how to enable GPU on our local desktop (we're running WSL2 on a Windows\
          \ machine that does have a GPU but have not been able to figure out on how\
          \ to get the GPU to work in the Ubuntu OS of WLS2); therefore I changed\
          \ the `device` parameter to `cpu` instead. The output I got was not the\
          \ list of sequences but some kind of a matrix. \r\nI guess there was something\
          \ wrong from my side with the script but I would very much appreciate if\
          \ you could have a look at it.\r\nThanks a lot,\r\nBest regards,\r\nKurioscity"
        updatedAt: '2022-12-02T10:13:48.401Z'
      numEdits: 0
      reactions: []
    id: 6389cfdc24ab66a8bff7a791
    type: comment
  author: 0jj0
  content: "Hi Noelia,\r\nit's me, Kuiroscity from the discussion of ProtGTP2. I tried\
    \ to use ZymCTRL after downloading the models manually using this wrapper script:\r\
    \n```python\r\n#!/usr/bin/env python\r\n\r\n#@title Import libraries and initialize\
    \ model\r\nfrom transformers import GPT2LMHeadModel, AutoTokenizer\r\nimport torch\r\
    \nimport os\r\nprint(\"Libraries imported successfully.\")\r\n\r\nenzyme_class\
    \ = \"1.1.3.4\"\r\ndevice = torch.device('cpu')\r\ntokenizer = AutoTokenizer.from_pretrained('/mnt/e/ZymCTRL/model')\r\
    \nmodel = GPT2LMHeadModel.from_pretrained('/mnt/e/ZymCTRL/model').to(device)\r\
    \ninput_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)\r\n\
    output = model.generate(input_ids, top_k=8, repetition_penalty=1.2, max_length=1024,\r\
    \n                        eos_token_id=1,pad_token_id=0,do_sample=True, num_return_sequences=100)\r\
    \nprint(output)\r\nf = open(\"output.test\", \"w\")\r\nf.write(output)\r\nf.close()\r\
    \n```\r\nI have not figured out how to enable GPU on our local desktop (we're\
    \ running WSL2 on a Windows machine that does have a GPU but have not been able\
    \ to figure out on how to get the GPU to work in the Ubuntu OS of WLS2); therefore\
    \ I changed the `device` parameter to `cpu` instead. The output I got was not\
    \ the list of sequences but some kind of a matrix. \r\nI guess there was something\
    \ wrong from my side with the script but I would very much appreciate if you could\
    \ have a look at it.\r\nThanks a lot,\r\nBest regards,\r\nKurioscity"
  created_at: 2022-12-02 10:13:48+00:00
  edited: false
  hidden: false
  id: 6389cfdc24ab66a8bff7a791
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-12-02T13:35:54.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi Kurioscity!</p>\n<p>Great to hear from you again :)<br>I'd love\
          \ to help you with the GPU issue, what is the error you are getting (I might\
          \ have no idea though)</p>\n<p>The output you get is indeed a matrix of\
          \ tokens. The model encodes each amino acid as a token, so you get a list\
          \ of tokens per sequence. Then it outputs all the sequences at once, so\
          \ you get a matrix for tokens.  These tokens can be decoded with the tokenizer,\
          \ here you have an example script:</p>\n<pre><code>    #&nbsp;1. Generate\
          \ sequences\n    enzyme_class = \"1.1.3.4\"\n    input_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)\n\
          \    outputs = model.generate(\n        input_ids, \n        top_k=9, \n\
          \        repetition_penalty=1.2,\n        max_length=1024,\n        eos_token_id=1,\n\
          \        pad_token_id=0,\n           do_sample=True,\n           num_return_sequences=100)\n\
          \    \n    # This step makes sure that the sequences weren't truncated during\
          \ generation. The last token should be a padding token.&nbsp;\n    new_outputs\
          \ = [ output for output in outputs if output[-1] == 0]\n    if not new_outputs:\n\
          \        print(\"not enough sequences with short lengths!!\")\n    \n\n\
          \   # To decode the sequences, you have to use the tokenizer:\n   tokenizer.decode(new_outputs[0])\
          \ #&nbsp;for example the first sequence in the list\n\n   #  You can also\
          \ decode and compute perplexity for all sequences at once:\n   def calculatePerplexity(input_ids,model,tokenizer):\n\
          \  '''\n  Function to compute perplexity\n  '''\n       with torch.no_grad():\n\
          \           outputs = model(input_ids, labels=input_ids)\n       loss, logits\
          \ = outputs[:2]\n       return math.exp(loss)\n\n    ppls = [(tokenizer.decode(output),\
          \ calculatePerplexity(output, model, tokenizer)) for output in new_outputs\
          \ ]\n\n   #&nbsp;After this, one possibility is to sort the sequences by\
          \ perplexity, the lower the better\n    ppls.sort(key=lambda i:i[1])\n \n\
          </code></pre>\n<p>Let me know if something throws an error or is unclear.<br>Best<br>Noelia</p>\n"
        raw: "Hi Kurioscity!\n\nGreat to hear from you again :)\nI'd love to help\
          \ you with the GPU issue, what is the error you are getting (I might have\
          \ no idea though)\n\nThe output you get is indeed a matrix of tokens. The\
          \ model encodes each amino acid as a token, so you get a list of tokens\
          \ per sequence. Then it outputs all the sequences at once, so you get a\
          \ matrix for tokens.  These tokens can be decoded with the tokenizer, here\
          \ you have an example script:\n```\n    #\_1. Generate sequences\n    enzyme_class\
          \ = \"1.1.3.4\"\n    input_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)\n\
          \    outputs = model.generate(\n        input_ids, \n    \ttop_k=9, \n \
          \       repetition_penalty=1.2,\n        max_length=1024,\n        eos_token_id=1,\n\
          \        pad_token_id=0,\n   \t    do_sample=True,\n   \t    num_return_sequences=100)\n\
          \    \n    # This step makes sure that the sequences weren't truncated during\
          \ generation. The last token should be a padding token.\_\n    new_outputs\
          \ = [ output for output in outputs if output[-1] == 0]\n    if not new_outputs:\n\
          \        print(\"not enough sequences with short lengths!!\")\n    \n\n\
          \   # To decode the sequences, you have to use the tokenizer:\n   tokenizer.decode(new_outputs[0])\
          \ #\_for example the first sequence in the list\n\n   #  You can also decode\
          \ and compute perplexity for all sequences at once:\n   def calculatePerplexity(input_ids,model,tokenizer):\n\
          \  '''\n  Function to compute perplexity\n  '''\n       with torch.no_grad():\n\
          \           outputs = model(input_ids, labels=input_ids)\n       loss, logits\
          \ = outputs[:2]\n       return math.exp(loss)\n\n    ppls = [(tokenizer.decode(output),\
          \ calculatePerplexity(output, model, tokenizer)) for output in new_outputs\
          \ ]\n\n   #\_After this, one possibility is to sort the sequences by perplexity,\
          \ the lower the better\n    ppls.sort(key=lambda i:i[1])\n \n```\nLet me\
          \ know if something throws an error or is unclear.\nBest\nNoelia"
        updatedAt: '2022-12-02T13:35:54.584Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - 0jj0
    id: 6389ff3a78e0e042449228f7
    type: comment
  author: nferruz
  content: "Hi Kurioscity!\n\nGreat to hear from you again :)\nI'd love to help you\
    \ with the GPU issue, what is the error you are getting (I might have no idea\
    \ though)\n\nThe output you get is indeed a matrix of tokens. The model encodes\
    \ each amino acid as a token, so you get a list of tokens per sequence. Then it\
    \ outputs all the sequences at once, so you get a matrix for tokens.  These tokens\
    \ can be decoded with the tokenizer, here you have an example script:\n```\n \
    \   #\_1. Generate sequences\n    enzyme_class = \"1.1.3.4\"\n    input_ids =\
    \ tokenizer.encode(enzyme_class,return_tensors='pt').to(device)\n    outputs =\
    \ model.generate(\n        input_ids, \n    \ttop_k=9, \n        repetition_penalty=1.2,\n\
    \        max_length=1024,\n        eos_token_id=1,\n        pad_token_id=0,\n\
    \   \t    do_sample=True,\n   \t    num_return_sequences=100)\n    \n    # This\
    \ step makes sure that the sequences weren't truncated during generation. The\
    \ last token should be a padding token.\_\n    new_outputs = [ output for output\
    \ in outputs if output[-1] == 0]\n    if not new_outputs:\n        print(\"not\
    \ enough sequences with short lengths!!\")\n    \n\n   # To decode the sequences,\
    \ you have to use the tokenizer:\n   tokenizer.decode(new_outputs[0]) #\_for example\
    \ the first sequence in the list\n\n   #  You can also decode and compute perplexity\
    \ for all sequences at once:\n   def calculatePerplexity(input_ids,model,tokenizer):\n\
    \  '''\n  Function to compute perplexity\n  '''\n       with torch.no_grad():\n\
    \           outputs = model(input_ids, labels=input_ids)\n       loss, logits\
    \ = outputs[:2]\n       return math.exp(loss)\n\n    ppls = [(tokenizer.decode(output),\
    \ calculatePerplexity(output, model, tokenizer)) for output in new_outputs ]\n\
    \n   #\_After this, one possibility is to sort the sequences by perplexity, the\
    \ lower the better\n    ppls.sort(key=lambda i:i[1])\n \n```\nLet me know if something\
    \ throws an error or is unclear.\nBest\nNoelia"
  created_at: 2022-12-02 13:35:54+00:00
  edited: false
  hidden: false
  id: 6389ff3a78e0e042449228f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
      fullname: Elephas Maximus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0jj0
      type: user
    createdAt: '2022-12-04T21:09:35.000Z'
    data:
      edited: false
      editors:
      - 0jj0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
          fullname: Elephas Maximus
          isHf: false
          isPro: false
          name: 0jj0
          type: user
        html: "<p>Hi Noelia,<br>thanks a lot for your help with the decoding of the\
          \ tokens. Your suggestion did help and I wrote a small script to get the\
          \ sequences as followed (as you can already see from my code, I'm  a seasonal\
          \ coder with limited knowledge in Python ):</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n\n<span class=\"\
          hljs-comment\">#@title Import libraries and initialize model</span>\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> GPT2LMHeadModel, AutoTokenizer\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">import</span> os\n<span\
          \ class=\"hljs-keyword\">from</span> datetime <span class=\"hljs-keyword\"\
          >import</span> datetime\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"Libraries imported successfully.\"</span>)\n\n\
          start_time = datetime.now().strftime(<span class=\"hljs-string\">\"%Y-%m-%d_%H-%M-%S\"\
          </span>)\n<span class=\"hljs-built_in\">print</span>(start_time)\n\nenzyme_class\
          \ = <span class=\"hljs-string\">\"1.1.3.4\"</span>\ndevice = torch.device(<span\
          \ class=\"hljs-string\">'cpu'</span>)\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">'/mnt/e/ZymCTRL/model'</span>)\nmodel = GPT2LMHeadModel.from_pretrained(<span\
          \ class=\"hljs-string\">'/mnt/e/ZymCTRL/model'</span>).to(device)\ninput_ids\
          \ = tokenizer.encode(enzyme_class,return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).to(device)\noutputs = model.generate(input_ids, top_k=<span\
          \ class=\"hljs-number\">8</span>, repetition_penalty=<span class=\"hljs-number\"\
          >1.2</span>, max_length=<span class=\"hljs-number\">1024</span>,\n     \
          \                   eos_token_id=<span class=\"hljs-number\">1</span>,pad_token_id=<span\
          \ class=\"hljs-number\">0</span>,do_sample=<span class=\"hljs-literal\"\
          >True</span>, num_return_sequences=<span class=\"hljs-number\">50</span>)\n\
          <span class=\"hljs-built_in\">print</span>(outputs)\n\nnew_outputs = [output\
          \ <span class=\"hljs-keyword\">for</span> output <span class=\"hljs-keyword\"\
          >in</span> outputs <span class=\"hljs-keyword\">if</span> output[-<span\
          \ class=\"hljs-number\">1</span>] == <span class=\"hljs-number\">0</span>]\n\
          <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span>\
          \ new_outputs:\n   <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">\"Not enough sequences with short lengths!!\"</span>)\n\n\
          fastaname = enzyme_class + <span class=\"hljs-string\">'_'</span> + <span\
          \ class=\"hljs-built_in\">str</span>(start_time)\nf = <span class=\"hljs-built_in\"\
          >open</span>(fastaname + <span class=\"hljs-string\">\".fasta\"</span>,\
          \ <span class=\"hljs-string\">\"w\"</span>)\n\nfasta_records = []\n\n<span\
          \ class=\"hljs-keyword\">for</span> count, seq <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">enumerate</span> (new_outputs,\
          \ start = <span class=\"hljs-number\">0</span>):\n  seq = tokenizer.decode(new_outputs[count])\n\
          \  <span class=\"hljs-built_in\">print</span>(seq)\n  write_seq = seq.replace(<span\
          \ class=\"hljs-string\">' '</span>, <span class=\"hljs-string\">''</span>).replace(<span\
          \ class=\"hljs-string\">'&lt;pad&gt;'</span>, <span class=\"hljs-string\"\
          >''</span>).replace(<span class=\"hljs-string\">'&lt;sep&gt;'</span>, <span\
          \ class=\"hljs-string\">''</span>).replace(<span class=\"hljs-string\">'&lt;start&gt;'</span>,\
          \ <span class=\"hljs-string\">''</span>).replace(enzyme_class, <span class=\"\
          hljs-string\">''</span>).replace(<span class=\"hljs-string\">'&lt;|endoftext|&gt;'</span>,\
          \ <span class=\"hljs-string\">''</span>).replace(<span class=\"hljs-string\"\
          >'&lt;end&gt;'</span>, <span class=\"hljs-string\">''</span>)\n  <span class=\"\
          hljs-built_in\">print</span>(write_seq)\n  fasta_record = <span class=\"\
          hljs-string\">\"&gt;\"</span> + enzyme_class + <span class=\"hljs-string\"\
          >\"_\"</span> + start_time + <span class=\"hljs-string\">\"_\"</span> +\
          \ <span class=\"hljs-built_in\">str</span>(count) + <span class=\"hljs-string\"\
          >\"\\n\"</span> + write_seq + <span class=\"hljs-string\">\"\\n\"</span>\n\
          \  <span class=\"hljs-built_in\">print</span>(fasta_record)\n  fasta_records.append(fasta_record)\n\
          \  <span class=\"hljs-built_in\">print</span>(fasta_records)\n\nfasta_list\
          \ = <span class=\"hljs-built_in\">list</span>(<span class=\"hljs-built_in\"\
          >map</span>(<span class=\"hljs-built_in\">str</span>, fasta_records))\n\
          fasta_file = <span class=\"hljs-string\">\" \"</span>.join(fasta_list).lstrip()\n\
          \n<span class=\"hljs-built_in\">print</span>(fasta_file)\nf.write(fasta_file)\n\
          \nf.close()\n</code></pre>\n<p>I will try to include the perplexity calculation\
          \ as you suggested and I will open a new thread to ask for your help with\
          \ the GPU thing (or is there a better way/channel to communicate with you\
          \ the GPU issue  as I don't think the issue is very relevant to ZymCTRL?)</p>\n"
        raw: "Hi Noelia,\nthanks a lot for your help with the decoding of the tokens.\
          \ Your suggestion did help and I wrote a small script to get the sequences\
          \ as followed (as you can already see from my code, I'm  a seasonal coder\
          \ with limited knowledge in Python ):\n```python\n#!/usr/bin/env python\n\
          \n#@title Import libraries and initialize model\nfrom transformers import\
          \ GPT2LMHeadModel, AutoTokenizer\nimport torch\nimport os\nfrom datetime\
          \ import datetime\nprint(\"Libraries imported successfully.\")\n\nstart_time\
          \ = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\nprint(start_time)\n\n\
          enzyme_class = \"1.1.3.4\"\ndevice = torch.device('cpu')\ntokenizer = AutoTokenizer.from_pretrained('/mnt/e/ZymCTRL/model')\n\
          model = GPT2LMHeadModel.from_pretrained('/mnt/e/ZymCTRL/model').to(device)\n\
          input_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)\n\
          outputs = model.generate(input_ids, top_k=8, repetition_penalty=1.2, max_length=1024,\n\
          \                        eos_token_id=1,pad_token_id=0,do_sample=True, num_return_sequences=50)\n\
          print(outputs)\n\nnew_outputs = [output for output in outputs if output[-1]\
          \ == 0]\nif not new_outputs:\n   print(\"Not enough sequences with short\
          \ lengths!!\")\n\nfastaname = enzyme_class + '_' + str(start_time)\nf =\
          \ open(fastaname + \".fasta\", \"w\")\n\nfasta_records = []\n\nfor count,\
          \ seq in enumerate (new_outputs, start = 0):\n  seq = tokenizer.decode(new_outputs[count])\n\
          \  print(seq)\n  write_seq = seq.replace(' ', '').replace('<pad>', '').replace('<sep>',\
          \ '').replace('<start>', '').replace(enzyme_class, '').replace('<|endoftext|>',\
          \ '').replace('<end>', '')\n  print(write_seq)\n  fasta_record = \">\" +\
          \ enzyme_class + \"_\" + start_time + \"_\" + str(count) + \"\\n\" + write_seq\
          \ + \"\\n\"\n  print(fasta_record)\n  fasta_records.append(fasta_record)\n\
          \  print(fasta_records)\n\nfasta_list = list(map(str, fasta_records))\n\
          fasta_file = \" \".join(fasta_list).lstrip()\n\nprint(fasta_file)\nf.write(fasta_file)\n\
          \nf.close()\n```\n\nI will try to include the perplexity calculation as\
          \ you suggested and I will open a new thread to ask for your help with the\
          \ GPU thing (or is there a better way/channel to communicate with you the\
          \ GPU issue  as I don't think the issue is very relevant to ZymCTRL?)"
        updatedAt: '2022-12-04T21:09:35.733Z'
      numEdits: 0
      reactions: []
    id: 638d0c8f5869d78eb7e5d4ab
    type: comment
  author: 0jj0
  content: "Hi Noelia,\nthanks a lot for your help with the decoding of the tokens.\
    \ Your suggestion did help and I wrote a small script to get the sequences as\
    \ followed (as you can already see from my code, I'm  a seasonal coder with limited\
    \ knowledge in Python ):\n```python\n#!/usr/bin/env python\n\n#@title Import libraries\
    \ and initialize model\nfrom transformers import GPT2LMHeadModel, AutoTokenizer\n\
    import torch\nimport os\nfrom datetime import datetime\nprint(\"Libraries imported\
    \ successfully.\")\n\nstart_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"\
    )\nprint(start_time)\n\nenzyme_class = \"1.1.3.4\"\ndevice = torch.device('cpu')\n\
    tokenizer = AutoTokenizer.from_pretrained('/mnt/e/ZymCTRL/model')\nmodel = GPT2LMHeadModel.from_pretrained('/mnt/e/ZymCTRL/model').to(device)\n\
    input_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)\noutputs\
    \ = model.generate(input_ids, top_k=8, repetition_penalty=1.2, max_length=1024,\n\
    \                        eos_token_id=1,pad_token_id=0,do_sample=True, num_return_sequences=50)\n\
    print(outputs)\n\nnew_outputs = [output for output in outputs if output[-1] ==\
    \ 0]\nif not new_outputs:\n   print(\"Not enough sequences with short lengths!!\"\
    )\n\nfastaname = enzyme_class + '_' + str(start_time)\nf = open(fastaname + \"\
    .fasta\", \"w\")\n\nfasta_records = []\n\nfor count, seq in enumerate (new_outputs,\
    \ start = 0):\n  seq = tokenizer.decode(new_outputs[count])\n  print(seq)\n  write_seq\
    \ = seq.replace(' ', '').replace('<pad>', '').replace('<sep>', '').replace('<start>',\
    \ '').replace(enzyme_class, '').replace('<|endoftext|>', '').replace('<end>',\
    \ '')\n  print(write_seq)\n  fasta_record = \">\" + enzyme_class + \"_\" + start_time\
    \ + \"_\" + str(count) + \"\\n\" + write_seq + \"\\n\"\n  print(fasta_record)\n\
    \  fasta_records.append(fasta_record)\n  print(fasta_records)\n\nfasta_list =\
    \ list(map(str, fasta_records))\nfasta_file = \" \".join(fasta_list).lstrip()\n\
    \nprint(fasta_file)\nf.write(fasta_file)\n\nf.close()\n```\n\nI will try to include\
    \ the perplexity calculation as you suggested and I will open a new thread to\
    \ ask for your help with the GPU thing (or is there a better way/channel to communicate\
    \ with you the GPU issue  as I don't think the issue is very relevant to ZymCTRL?)"
  created_at: 2022-12-04 21:09:35+00:00
  edited: false
  hidden: false
  id: 638d0c8f5869d78eb7e5d4ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-12-04T22:58:06.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Great to hear it helped! Let me know if you need some help with
          the perplexity. In my experience, it makes a huge different to only select
          values with high perplexity, e.g, the top 10%.<br>For the GPU error (which
          I might have no idea of) I guess you can try here in case it helps other
          users :) or send me an email at noelia [dot] ferruz [at] udg [dot] edu</p>

          '
        raw: 'Great to hear it helped! Let me know if you need some help with the
          perplexity. In my experience, it makes a huge different to only select values
          with high perplexity, e.g, the top 10%.

          For the GPU error (which I might have no idea of) I guess you can try here
          in case it helps other users :) or send me an email at noelia [dot] ferruz
          [at] udg [dot] edu'
        updatedAt: '2022-12-04T22:58:06.108Z'
      numEdits: 0
      reactions: []
    id: 638d25feebda86f24b5e71fc
    type: comment
  author: nferruz
  content: 'Great to hear it helped! Let me know if you need some help with the perplexity.
    In my experience, it makes a huge different to only select values with high perplexity,
    e.g, the top 10%.

    For the GPU error (which I might have no idea of) I guess you can try here in
    case it helps other users :) or send me an email at noelia [dot] ferruz [at] udg
    [dot] edu'
  created_at: 2022-12-04 22:58:06+00:00
  edited: false
  hidden: false
  id: 638d25feebda86f24b5e71fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
      fullname: Elephas Maximus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0jj0
      type: user
    createdAt: '2023-03-08T20:10:39.000Z'
    data:
      status: closed
    id: 6408ebbf4bf9635aa50d5c09
    type: status-change
  author: 0jj0
  created_at: 2023-03-08 20:10:39+00:00
  id: 6408ebbf4bf9635aa50d5c09
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: AI4PD/ZymCTRL
repo_type: model
status: closed
target_branch: null
title: Output of ZymCTRL
