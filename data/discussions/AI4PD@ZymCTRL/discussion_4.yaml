!!python/object:huggingface_hub.community.DiscussionWithDetails
author: guruace
conflicting_files: null
created_at: 2023-05-06 02:37:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1670387323c14a8551c0bbb7619f137.svg
      fullname: Ribo Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guruace
      type: user
    createdAt: '2023-05-06T03:37:23.000Z'
    data:
      edited: false
      editors:
      - guruace
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1670387323c14a8551c0bbb7619f137.svg
          fullname: Ribo Huang
          isHf: false
          isPro: false
          name: guruace
          type: user
        html: '<p>Thank you Prof. Noelia Ferruz for your excellent work!</p>

          <p>I have tried on V100-32G GPU, and it took very long time: about 204 minutes
          for the default enzyme nitrilase (3.5.5.1) in your Example 1. Since my team
          has a 4xV100-32G Nvidia DGX machine, we wonder if it is possible to modify
          your script of Example 1 to fully use all 4 GPUs, in order to speed up the
          inference. We also have tried Example 1 on single RTX6000ada - 48G GPU,
          it also took as long as 44 minutes. It seems nn.parallel.DistributedDataParallel
          will do, but when I did as follows:</p>

          <p>model = GPT2LMHeadModel.from_pretrained(''/my/path/to/zymCTRL'').to(device)<br>if
          torch.cuda.device_count() &gt; 1:<br>          print(''f"Use {torch.cuda_device_count()}
          GPUs")<br>          model = torch.nn.parallel.DistributedDataParallel(model)</p>

          <p>an error message showed: "RuntimeError: Default process group has not
          been initialized, please make sure to call init_process_group"</p>

          <p>guruace</p>

          '
        raw: "Thank you Prof. Noelia Ferruz for your excellent work!\r\n\r\nI have\
          \ tried on V100-32G GPU, and it took very long time: about 204 minutes for\
          \ the default enzyme nitrilase (3.5.5.1) in your Example 1. Since my team\
          \ has a 4xV100-32G Nvidia DGX machine, we wonder if it is possible to modify\
          \ your script of Example 1 to fully use all 4 GPUs, in order to speed up\
          \ the inference. We also have tried Example 1 on single RTX6000ada - 48G\
          \ GPU, it also took as long as 44 minutes. It seems nn.parallel.DistributedDataParallel\
          \ will do, but when I did as follows:\r\n\r\nmodel = GPT2LMHeadModel.from_pretrained('/my/path/to/zymCTRL').to(device)\r\
          \nif torch.cuda.device_count() > 1:\r\n          print('f\"Use {torch.cuda_device_count()}\
          \ GPUs\")\r\n          model = torch.nn.parallel.DistributedDataParallel(model)\r\
          \n\r\nan error message showed: \"RuntimeError: Default process group has\
          \ not been initialized, please make sure to call init_process_group\"\r\n\
          \r\n\r\nguruace"
        updatedAt: '2023-05-06T03:37:23.759Z'
      numEdits: 0
      reactions: []
    id: 6455cb73bda0fbba412de867
    type: comment
  author: guruace
  content: "Thank you Prof. Noelia Ferruz for your excellent work!\r\n\r\nI have tried\
    \ on V100-32G GPU, and it took very long time: about 204 minutes for the default\
    \ enzyme nitrilase (3.5.5.1) in your Example 1. Since my team has a 4xV100-32G\
    \ Nvidia DGX machine, we wonder if it is possible to modify your script of Example\
    \ 1 to fully use all 4 GPUs, in order to speed up the inference. We also have\
    \ tried Example 1 on single RTX6000ada - 48G GPU, it also took as long as 44 minutes.\
    \ It seems nn.parallel.DistributedDataParallel will do, but when I did as follows:\r\
    \n\r\nmodel = GPT2LMHeadModel.from_pretrained('/my/path/to/zymCTRL').to(device)\r\
    \nif torch.cuda.device_count() > 1:\r\n          print('f\"Use {torch.cuda_device_count()}\
    \ GPUs\")\r\n          model = torch.nn.parallel.DistributedDataParallel(model)\r\
    \n\r\nan error message showed: \"RuntimeError: Default process group has not been\
    \ initialized, please make sure to call init_process_group\"\r\n\r\n\r\nguruace"
  created_at: 2023-05-06 02:37:23+00:00
  edited: false
  hidden: false
  id: 6455cb73bda0fbba412de867
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-05-08T04:46:15.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi guruace,</p>

          <p>How many sequences were you generating during that time? With your GPUs,
          I''d expect it generates more than 2000 sequences in that time (possibly
          many more).<br>Certainly the first batch does not take more than 2-5 minutes
          when I use an A40.</p>

          <p>Are you sure the GPU is being used?</p>

          <p>Alternatively,  I''ve never tried, but I think HuggingFace supports inference
          on multiple GPUs: <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_many">https://huggingface.co/docs/transformers/perf_infer_gpu_many</a></p>

          <p>Hope this helps,<br>Noelia</p>

          '
        raw: 'Hi guruace,


          How many sequences were you generating during that time? With your GPUs,
          I''d expect it generates more than 2000 sequences in that time (possibly
          many more).

          Certainly the first batch does not take more than 2-5 minutes when I use
          an A40.


          Are you sure the GPU is being used?


          Alternatively,  I''ve never tried, but I think HuggingFace supports inference
          on multiple GPUs: https://huggingface.co/docs/transformers/perf_infer_gpu_many


          Hope this helps,

          Noelia'
        updatedAt: '2023-05-08T04:46:15.449Z'
      numEdits: 0
      reactions: []
    id: 64587e97c9af80de217c9e7f
    type: comment
  author: nferruz
  content: 'Hi guruace,


    How many sequences were you generating during that time? With your GPUs, I''d
    expect it generates more than 2000 sequences in that time (possibly many more).

    Certainly the first batch does not take more than 2-5 minutes when I use an A40.


    Are you sure the GPU is being used?


    Alternatively,  I''ve never tried, but I think HuggingFace supports inference
    on multiple GPUs: https://huggingface.co/docs/transformers/perf_infer_gpu_many


    Hope this helps,

    Noelia'
  created_at: 2023-05-08 03:46:15+00:00
  edited: false
  hidden: false
  id: 64587e97c9af80de217c9e7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1670387323c14a8551c0bbb7619f137.svg
      fullname: Ribo Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guruace
      type: user
    createdAt: '2023-05-08T15:35:14.000Z'
    data:
      edited: false
      editors:
      - guruace
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1670387323c14a8551c0bbb7619f137.svg
          fullname: Ribo Huang
          isHf: false
          isPro: false
          name: guruace
          type: user
        html: '<p>Dear Noelia,</p>

          <p>Yes, I was quite sure that it was using GPU, but used single GPU(it is
          quite sure also from your script). I also tested on my MacBook Pro M1 -
          64G, it presumably ran on CPU only and it took 36 hours to produce only
          572 sequences.  On RTX6000ada and V100-32g, there were 1300 and 1290 sequences
          generated. Based on M1 results, I was very sure that running on V100-32g
          was using GPU, not running on CPU alone.</p>

          <p>Thank you!</p>

          <p>guruace</p>

          '
        raw: 'Dear Noelia,


          Yes, I was quite sure that it was using GPU, but used single GPU(it is quite
          sure also from your script). I also tested on my MacBook Pro M1 - 64G, it
          presumably ran on CPU only and it took 36 hours to produce only 572 sequences.  On
          RTX6000ada and V100-32g, there were 1300 and 1290 sequences generated. Based
          on M1 results, I was very sure that running on V100-32g was using GPU, not
          running on CPU alone.


          Thank you!


          guruace'
        updatedAt: '2023-05-08T15:35:14.566Z'
      numEdits: 0
      reactions: []
    id: 645916b2232e5f0712b5683f
    type: comment
  author: guruace
  content: 'Dear Noelia,


    Yes, I was quite sure that it was using GPU, but used single GPU(it is quite sure
    also from your script). I also tested on my MacBook Pro M1 - 64G, it presumably
    ran on CPU only and it took 36 hours to produce only 572 sequences.  On RTX6000ada
    and V100-32g, there were 1300 and 1290 sequences generated. Based on M1 results,
    I was very sure that running on V100-32g was using GPU, not running on CPU alone.


    Thank you!


    guruace'
  created_at: 2023-05-08 14:35:14+00:00
  edited: false
  hidden: false
  id: 645916b2232e5f0712b5683f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: AI4PD/ZymCTRL
repo_type: model
status: open
target_branch: null
title: Can we do the inferences on ZymCTRL with multi GPUs?
