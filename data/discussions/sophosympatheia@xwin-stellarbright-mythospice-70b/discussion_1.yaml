!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ks01
conflicting_files: null
created_at: 2023-11-06 13:47:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
      fullname: Ks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ks01
      type: user
    createdAt: '2023-11-06T13:47:11.000Z'
    data:
      edited: false
      editors:
      - Ks01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9847286939620972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
          fullname: Ks
          isHf: false
          isPro: false
          name: Ks01
          type: user
        html: "<p>Thanks for trying StellarBright, which is a great model that people\
          \ don't know!<br>It took me some time to test myself several times to spot\
          \ some good and bad in this model.</p>\n<p>And as I expected, role-playing\
          \ on this model was phenomenal. Look how it handled when I asked how old\
          \ she was to the young fighter girl.</p>\n<p>Input : &nbsp;\"Huh? How old\
          \ are you?\"<br>Output : Hearing the question, Mia blinked and cocked her\
          \ head. \"I'm\u2014wait... why does it matter how old I am? Do you have\
          \ something against young girls kicking ass?\" The confused response drew\
          \ laughter from the crowd before Mia cracked her neck, and stood in a fighting\
          \ stance. At least she hoped it was a fighting stance. Her legs were spread\
          \ shoulder width apart, hands raised defensively, chest out, shoulders back.\
          \ It wasn't perfect but it would do. Then again, maybe it did matter. Would\
          \ someone care enough to exploit any holes in her style due to her age?\
          \ Probably. But she had fought so many times now, surely they wouldn't be\
          \ able to tell... right?</p>\n<p>Maybe it's too descriptive in the first\
          \ person, but expressing the details of an action is not easy to llm. Usually,\
          \ they write in detail how the atmosphere around them got intense, or blah\
          \ blah.<br>What I felt while testing with this model was that it gave me\
          \ top experience while I'm role-playing with characters. Depicting innocent,\
          \ dumb, evil, mean, and some other extreme characteristics is hard. Xwin\
          \ and lzlv have this problem too. They can depict them, but it's kinda shallow.<br>Thanks\
          \ to StellarBright, this model depicted them almost flawlessly. Which means\
          \ it did better than any other models that I have experienced so far.</p>\n\
          <p>However, there were some downsides too.<br>First, it didn't follow complex\
          \ prompts well; you know, some stats and displays on the RPG character card.\
          \ And it was surprising because both StellarBright and Xwin had no problem\
          \ following those when I tried them separately. So maybe it's because of\
          \ mythospice, since that model (Chronos, Nous-Hermes, spicyboros) isn't\
          \ good at following prompts.<br>Second, I'm pretty sure it's because of\
          \ StellarBright, that model cannot generate long replies well compared to\
          \ other models.<br>Third, somehow, it's not as intelligent as I expected.\
          \ Of course, it's 70b, and stellarbright has very high intelligence. But\
          \ I thought it could be better; airoboros 2.2.1 was really good at Tree\
          \ of Thoughts and reasoning, but this model was not at that level. Maybe\
          \ because of Xwin and mythospice? Since I know Xwin is really good at following\
          \ instructions but not intelligent, I'm not sure about mythospice, though.\
          \ but the ingredients of mythospice are not intelligent at all.</p>\n<p>It's\
          \ just my suggestion. but I think ( Xwin + StellarBright ) + ( Xwin + airoboros\
          \ 2.2.1 ) can be good at both reading instruction and role-playing.<br>I\
          \ wish I could do it myself, but sadly, I have no knowledge on merging so\
          \ far. So, just think it as an unpopular opinion.</p>\n<p>And thank you\
          \ for making another good model and experiments.</p>\n"
        raw: "Thanks for trying StellarBright, which is a great model that people\
          \ don't know!\r\nIt took me some time to test myself several times to spot\
          \ some good and bad in this model.\r\n\r\nAnd as I expected, role-playing\
          \ on this model was phenomenal. Look how it handled when I asked how old\
          \ she was to the young fighter girl.\r\n\r\nInput : \_\"Huh? How old are\
          \ you?\"\r\nOutput : Hearing the question, Mia blinked and cocked her head.\
          \ \"I'm\u2014wait... why does it matter how old I am? Do you have something\
          \ against young girls kicking ass?\" The confused response drew laughter\
          \ from the crowd before Mia cracked her neck, and stood in a fighting stance.\
          \ At least she hoped it was a fighting stance. Her legs were spread shoulder\
          \ width apart, hands raised defensively, chest out, shoulders back. It wasn't\
          \ perfect but it would do. Then again, maybe it did matter. Would someone\
          \ care enough to exploit any holes in her style due to her age? Probably.\
          \ But she had fought so many times now, surely they wouldn't be able to\
          \ tell... right?\r\n\r\nMaybe it's too descriptive in the first person,\
          \ but expressing the details of an action is not easy to llm. Usually, they\
          \ write in detail how the atmosphere around them got intense, or blah blah.\r\
          \nWhat I felt while testing with this model was that it gave me top experience\
          \ while I'm role-playing with characters. Depicting innocent, dumb, evil,\
          \ mean, and some other extreme characteristics is hard. Xwin and lzlv have\
          \ this problem too. They can depict them, but it's kinda shallow.\r\nThanks\
          \ to StellarBright, this model depicted them almost flawlessly. Which means\
          \ it did better than any other models that I have experienced so far.\r\n\
          \r\nHowever, there were some downsides too.\r\nFirst, it didn't follow complex\
          \ prompts well; you know, some stats and displays on the RPG character card.\
          \ And it was surprising because both StellarBright and Xwin had no problem\
          \ following those when I tried them separately. So maybe it's because of\
          \ mythospice, since that model (Chronos, Nous-Hermes, spicyboros) isn't\
          \ good at following prompts.\r\nSecond, I'm pretty sure it's because of\
          \ StellarBright, that model cannot generate long replies well compared to\
          \ other models.\r\nThird, somehow, it's not as intelligent as I expected.\
          \ Of course, it's 70b, and stellarbright has very high intelligence. But\
          \ I thought it could be better; airoboros 2.2.1 was really good at Tree\
          \ of Thoughts and reasoning, but this model was not at that level. Maybe\
          \ because of Xwin and mythospice? Since I know Xwin is really good at following\
          \ instructions but not intelligent, I'm not sure about mythospice, though.\
          \ but the ingredients of mythospice are not intelligent at all.\r\n\r\n\
          It's just my suggestion. but I think ( Xwin + StellarBright ) + ( Xwin +\
          \ airoboros 2.2.1 ) can be good at both reading instruction and role-playing.\r\
          \nI wish I could do it myself, but sadly, I have no knowledge on merging\
          \ so far. So, just think it as an unpopular opinion.\r\n\r\nAnd thank you\
          \ for making another good model and experiments."
        updatedAt: '2023-11-06T13:47:11.946Z'
      numEdits: 0
      reactions: []
    id: 6548ee5f8db6a4761b4b8d67
    type: comment
  author: Ks01
  content: "Thanks for trying StellarBright, which is a great model that people don't\
    \ know!\r\nIt took me some time to test myself several times to spot some good\
    \ and bad in this model.\r\n\r\nAnd as I expected, role-playing on this model\
    \ was phenomenal. Look how it handled when I asked how old she was to the young\
    \ fighter girl.\r\n\r\nInput : \_\"Huh? How old are you?\"\r\nOutput : Hearing\
    \ the question, Mia blinked and cocked her head. \"I'm\u2014wait... why does it\
    \ matter how old I am? Do you have something against young girls kicking ass?\"\
    \ The confused response drew laughter from the crowd before Mia cracked her neck,\
    \ and stood in a fighting stance. At least she hoped it was a fighting stance.\
    \ Her legs were spread shoulder width apart, hands raised defensively, chest out,\
    \ shoulders back. It wasn't perfect but it would do. Then again, maybe it did\
    \ matter. Would someone care enough to exploit any holes in her style due to her\
    \ age? Probably. But she had fought so many times now, surely they wouldn't be\
    \ able to tell... right?\r\n\r\nMaybe it's too descriptive in the first person,\
    \ but expressing the details of an action is not easy to llm. Usually, they write\
    \ in detail how the atmosphere around them got intense, or blah blah.\r\nWhat\
    \ I felt while testing with this model was that it gave me top experience while\
    \ I'm role-playing with characters. Depicting innocent, dumb, evil, mean, and\
    \ some other extreme characteristics is hard. Xwin and lzlv have this problem\
    \ too. They can depict them, but it's kinda shallow.\r\nThanks to StellarBright,\
    \ this model depicted them almost flawlessly. Which means it did better than any\
    \ other models that I have experienced so far.\r\n\r\nHowever, there were some\
    \ downsides too.\r\nFirst, it didn't follow complex prompts well; you know, some\
    \ stats and displays on the RPG character card. And it was surprising because\
    \ both StellarBright and Xwin had no problem following those when I tried them\
    \ separately. So maybe it's because of mythospice, since that model (Chronos,\
    \ Nous-Hermes, spicyboros) isn't good at following prompts.\r\nSecond, I'm pretty\
    \ sure it's because of StellarBright, that model cannot generate long replies\
    \ well compared to other models.\r\nThird, somehow, it's not as intelligent as\
    \ I expected. Of course, it's 70b, and stellarbright has very high intelligence.\
    \ But I thought it could be better; airoboros 2.2.1 was really good at Tree of\
    \ Thoughts and reasoning, but this model was not at that level. Maybe because\
    \ of Xwin and mythospice? Since I know Xwin is really good at following instructions\
    \ but not intelligent, I'm not sure about mythospice, though. but the ingredients\
    \ of mythospice are not intelligent at all.\r\n\r\nIt's just my suggestion. but\
    \ I think ( Xwin + StellarBright ) + ( Xwin + airoboros 2.2.1 ) can be good at\
    \ both reading instruction and role-playing.\r\nI wish I could do it myself, but\
    \ sadly, I have no knowledge on merging so far. So, just think it as an unpopular\
    \ opinion.\r\n\r\nAnd thank you for making another good model and experiments."
  created_at: 2023-11-06 13:47:11+00:00
  edited: false
  hidden: false
  id: 6548ee5f8db6a4761b4b8d67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2023-11-06T14:59:24.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9608755707740784
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>Thanks, Ks01! I agree that this blend is a step in the right direction
          thanks to StellarBright but this alchemy has not yet produced true gold.
          I also suspect that mythospice is having a negative impact on the smarts
          of the blend, as you surmised too.<br>I think my next experiment will be
          simple, like Xwin + StellarBright with airoboros 2.2.1 LoRA merged in. If
          that performs well, I''ll see what happens with a full airoboros merge.
          I am also going to exclude the roleplay LoRAs initially because they may
          not be helping. My intuition is they add flavor but sacrifice some model
          intelligence. I also suspect that less is more with respect to these merges.</p>

          <p>Model merging is fairly easy, it just takes time, lots of memory / swap
          space, and lots of disk space for storing all those full-precision weights.
          I wrote a guide on Reddit: <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/17dyhyb/guide_for_llama2_70b_model_merging_and_exllama2/">https://www.reddit.com/r/LocalLLaMA/comments/17dyhyb/guide_for_llama2_70b_model_merging_and_exllama2/</a><br>I
          have since moved from gradient merges to SLERP merges, and I have some code
          for that which I''ll probably throw up on GitHub soon. This GitHub repo
          contains code that I suspect much of the model merging community is using:
          <a rel="nofollow" href="https://github.com/cg123/mergekit">https://github.com/cg123/mergekit</a></p>

          <p>If you have around 2 TB of local storage to play with and can get to
          ~300-400gb of memory (it''s fine if that uses swap space), you can do 70b
          model merges. I''m doing it within those constraints. It''s not ideal when
          Xwin is like 250 GB on its own, but it''s enough to at least do the merge,
          quantize it, and upload the result to Hugging Face.</p>

          <p>I am itching to get my hands on Xwin 0.2 and that next release of the
          dolphin blend that''s built on StellarBright. This next merge I do will
          be a test run of the formula in preparation for working with those ingredients.</p>

          '
        raw: 'Thanks, Ks01! I agree that this blend is a step in the right direction
          thanks to StellarBright but this alchemy has not yet produced true gold.
          I also suspect that mythospice is having a negative impact on the smarts
          of the blend, as you surmised too.

          I think my next experiment will be simple, like Xwin + StellarBright with
          airoboros 2.2.1 LoRA merged in. If that performs well, I''ll see what happens
          with a full airoboros merge. I am also going to exclude the roleplay LoRAs
          initially because they may not be helping. My intuition is they add flavor
          but sacrifice some model intelligence. I also suspect that less is more
          with respect to these merges.


          Model merging is fairly easy, it just takes time, lots of memory / swap
          space, and lots of disk space for storing all those full-precision weights.
          I wrote a guide on Reddit: https://www.reddit.com/r/LocalLLaMA/comments/17dyhyb/guide_for_llama2_70b_model_merging_and_exllama2/

          I have since moved from gradient merges to SLERP merges, and I have some
          code for that which I''ll probably throw up on GitHub soon. This GitHub
          repo contains code that I suspect much of the model merging community is
          using: https://github.com/cg123/mergekit


          If you have around 2 TB of local storage to play with and can get to ~300-400gb
          of memory (it''s fine if that uses swap space), you can do 70b model merges.
          I''m doing it within those constraints. It''s not ideal when Xwin is like
          250 GB on its own, but it''s enough to at least do the merge, quantize it,
          and upload the result to Hugging Face.


          I am itching to get my hands on Xwin 0.2 and that next release of the dolphin
          blend that''s built on StellarBright. This next merge I do will be a test
          run of the formula in preparation for working with those ingredients.'
        updatedAt: '2023-11-06T14:59:24.622Z'
      numEdits: 0
      reactions: []
    id: 6548ff4cc938743aff2c498d
    type: comment
  author: sophosympatheia
  content: 'Thanks, Ks01! I agree that this blend is a step in the right direction
    thanks to StellarBright but this alchemy has not yet produced true gold. I also
    suspect that mythospice is having a negative impact on the smarts of the blend,
    as you surmised too.

    I think my next experiment will be simple, like Xwin + StellarBright with airoboros
    2.2.1 LoRA merged in. If that performs well, I''ll see what happens with a full
    airoboros merge. I am also going to exclude the roleplay LoRAs initially because
    they may not be helping. My intuition is they add flavor but sacrifice some model
    intelligence. I also suspect that less is more with respect to these merges.


    Model merging is fairly easy, it just takes time, lots of memory / swap space,
    and lots of disk space for storing all those full-precision weights. I wrote a
    guide on Reddit: https://www.reddit.com/r/LocalLLaMA/comments/17dyhyb/guide_for_llama2_70b_model_merging_and_exllama2/

    I have since moved from gradient merges to SLERP merges, and I have some code
    for that which I''ll probably throw up on GitHub soon. This GitHub repo contains
    code that I suspect much of the model merging community is using: https://github.com/cg123/mergekit


    If you have around 2 TB of local storage to play with and can get to ~300-400gb
    of memory (it''s fine if that uses swap space), you can do 70b model merges. I''m
    doing it within those constraints. It''s not ideal when Xwin is like 250 GB on
    its own, but it''s enough to at least do the merge, quantize it, and upload the
    result to Hugging Face.


    I am itching to get my hands on Xwin 0.2 and that next release of the dolphin
    blend that''s built on StellarBright. This next merge I do will be a test run
    of the formula in preparation for working with those ingredients.'
  created_at: 2023-11-06 14:59:24+00:00
  edited: false
  hidden: false
  id: 6548ff4cc938743aff2c498d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
      fullname: Ks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ks01
      type: user
    createdAt: '2023-11-07T00:09:15.000Z'
    data:
      edited: false
      editors:
      - Ks01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9277905225753784
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
          fullname: Ks
          isHf: false
          isPro: false
          name: Ks01
          type: user
        html: '<p>Thanks for your kind guide.<br>I will try myself after I got some
          time to pour my time in.<br>Again, I will look forward another merge you
          make!</p>

          '
        raw: 'Thanks for your kind guide.

          I will try myself after I got some time to pour my time in.

          Again, I will look forward another merge you make!'
        updatedAt: '2023-11-07T00:09:15.872Z'
      numEdits: 0
      reactions: []
    id: 6549802b2f77576fa44545da
    type: comment
  author: Ks01
  content: 'Thanks for your kind guide.

    I will try myself after I got some time to pour my time in.

    Again, I will look forward another merge you make!'
  created_at: 2023-11-07 00:09:15+00:00
  edited: false
  hidden: false
  id: 6549802b2f77576fa44545da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
      fullname: Ks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ks01
      type: user
    createdAt: '2023-11-07T15:24:13.000Z'
    data:
      edited: true
      editors:
      - Ks01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9819564819335938
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
          fullname: Ks
          isHf: false
          isPro: false
          name: Ks01
          type: user
        html: '<p>After a few more tries, I noticed it has some repetition issues
          and gives bad replies when the context length is over 4K.<br>And I wasn''t
          surprised, since both Xwin and StellarBright had that issue.<br>But under
          4k context, it''s simply amazing at RP. I have no idea what can be done
          to solve this problem since it''s a noticeable problem in both main models.</p>

          '
        raw: 'After a few more tries, I noticed it has some repetition issues and
          gives bad replies when the context length is over 4K.

          And I wasn''t surprised, since both Xwin and StellarBright had that issue.

          But under 4k context, it''s simply amazing at RP. I have no idea what can
          be done to solve this problem since it''s a noticeable problem in both main
          models.'
        updatedAt: '2023-11-07T15:25:47.171Z'
      numEdits: 1
      reactions: []
    id: 654a569d9e2766147792650f
    type: comment
  author: Ks01
  content: 'After a few more tries, I noticed it has some repetition issues and gives
    bad replies when the context length is over 4K.

    And I wasn''t surprised, since both Xwin and StellarBright had that issue.

    But under 4k context, it''s simply amazing at RP. I have no idea what can be done
    to solve this problem since it''s a noticeable problem in both main models.'
  created_at: 2023-11-07 15:24:13+00:00
  edited: true
  hidden: false
  id: 654a569d9e2766147792650f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2023-11-07T17:23:05.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975073516368866
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>That''s really interesting. I mostly run my tests at 8192 context
          with NTK RoPE alpha_factor set to 2.5. The responses seem good, but I haven''t
          been testing much against 4096 context so maybe I''m missing out. Do you
          know of any 70b models that have a reputation for gracefully handling larger
          context sizes? I would be curious to see if merging such a model into the
          blend at a low weight, like 0.1 - 0.2, would help without otherwise upsetting
          the balance.</p>

          <p>I''m uploading the result of my xwin-stellarbright-airoboros_peft experiment
          right now.  I haven''t had a chance to play with it much yet, but early
          results are promising.</p>

          '
        raw: 'That''s really interesting. I mostly run my tests at 8192 context with
          NTK RoPE alpha_factor set to 2.5. The responses seem good, but I haven''t
          been testing much against 4096 context so maybe I''m missing out. Do you
          know of any 70b models that have a reputation for gracefully handling larger
          context sizes? I would be curious to see if merging such a model into the
          blend at a low weight, like 0.1 - 0.2, would help without otherwise upsetting
          the balance.


          I''m uploading the result of my xwin-stellarbright-airoboros_peft experiment
          right now.  I haven''t had a chance to play with it much yet, but early
          results are promising.'
        updatedAt: '2023-11-07T17:23:05.163Z'
      numEdits: 0
      reactions: []
    id: 654a7279fa25dfb7809ddfa5
    type: comment
  author: sophosympatheia
  content: 'That''s really interesting. I mostly run my tests at 8192 context with
    NTK RoPE alpha_factor set to 2.5. The responses seem good, but I haven''t been
    testing much against 4096 context so maybe I''m missing out. Do you know of any
    70b models that have a reputation for gracefully handling larger context sizes?
    I would be curious to see if merging such a model into the blend at a low weight,
    like 0.1 - 0.2, would help without otherwise upsetting the balance.


    I''m uploading the result of my xwin-stellarbright-airoboros_peft experiment right
    now.  I haven''t had a chance to play with it much yet, but early results are
    promising.'
  created_at: 2023-11-07 17:23:05+00:00
  edited: false
  hidden: false
  id: 654a7279fa25dfb7809ddfa5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
      fullname: Ks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ks01
      type: user
    createdAt: '2023-11-08T00:05:12.000Z'
    data:
      edited: true
      editors:
      - Ks01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9744992852210999
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
          fullname: Ks
          isHf: false
          isPro: false
          name: Ks01
          type: user
        html: '<p>In detail, Xwin has a serious repetition problem when its context
          gets long enough, but it can handle 8K context.<br>However, StellarBright
          is the one I felt became very weak after 4K context.<br>In my experience,
          Airoboros 2.2.1 handled those issues well.</p>

          <p>Btw, I saw Dolphin 2.2 70b is out, so I will leave another review of
          it later.</p>

          '
        raw: 'In detail, Xwin has a serious repetition problem when its context gets
          long enough, but it can handle 8K context.

          However, StellarBright is the one I felt became very weak after 4K context.

          In my experience, Airoboros 2.2.1 handled those issues well.


          Btw, I saw Dolphin 2.2 70b is out, so I will leave another review of it
          later.'
        updatedAt: '2023-11-08T02:00:42.087Z'
      numEdits: 1
      reactions: []
    id: 654ad0b883e7bfc431572c17
    type: comment
  author: Ks01
  content: 'In detail, Xwin has a serious repetition problem when its context gets
    long enough, but it can handle 8K context.

    However, StellarBright is the one I felt became very weak after 4K context.

    In my experience, Airoboros 2.2.1 handled those issues well.


    Btw, I saw Dolphin 2.2 70b is out, so I will leave another review of it later.'
  created_at: 2023-11-08 00:05:12+00:00
  edited: true
  hidden: false
  id: 654ad0b883e7bfc431572c17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2023-11-08T05:47:27.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9781566262245178
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>I''ve already got a dolphin 2.2 + xwin + airoboros_peft merge cooking.
          I hope to have it up tomorrow.</p>

          '
        raw: I've already got a dolphin 2.2 + xwin + airoboros_peft merge cooking.
          I hope to have it up tomorrow.
        updatedAt: '2023-11-08T05:47:27.939Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Ks01
        - tachyphylaxis
        - coffeedean
    id: 654b20eff8ebcec5452f9cb0
    type: comment
  author: sophosympatheia
  content: I've already got a dolphin 2.2 + xwin + airoboros_peft merge cooking. I
    hope to have it up tomorrow.
  created_at: 2023-11-08 05:47:27+00:00
  edited: false
  hidden: false
  id: 654b20eff8ebcec5452f9cb0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: sophosympatheia/xwin-stellarbright-mythospice-70b
repo_type: model
status: open
target_branch: null
title: Another step forward from sophosynthesis!
