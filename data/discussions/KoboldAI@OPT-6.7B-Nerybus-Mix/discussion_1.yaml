!!python/object:huggingface_hub.community.DiscussionWithDetails
author: justsumguy
conflicting_files: null
created_at: 2023-04-20 02:16:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-04-20T03:16:29.000Z'
    data:
      edited: false
      editors:
      - justsumguy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
          fullname: Jared H
          isHf: false
          isPro: false
          name: justsumguy
          type: user
        html: '<p>Some users have done so already for certain models but most (including
          this one) aren''t yet quantized on Hugging Face. In addition, quantizing
          on user hardware isn''t a straightforward process and even users with higher
          end hardware can struggle with OOM issues if it''s not top tier. As this
          is quickly becoming the primary way users are interacting with your models,
          I think it would be advantageous to post these yourself instead of leaving
          it up to users to figure out on their own and potentially, eventually, redistribute.<br>I''m
          aware Hugging Face is developer centric, and thus many will know, or be
          able to figure out, how to quantize models, and can use Google Collab to
          do so. But even if that''s what the expectation is, there aren''t easily
          accessible Collab notebooks with scripts to ease this process, requiring
          users to setup and configure the notebook for this purpose.<br>For the end-user
          looking to run LLMs locally, the process of getting new text generation
          models is full of friction and confusion, and I don''t think it has to be
          this way. It would set a good precedent for one of the most user-centric
          LLM and API developers to provide a user-centric experience.</p>

          '
        raw: "Some users have done so already for certain models but most (including\
          \ this one) aren't yet quantized on Hugging Face. In addition, quantizing\
          \ on user hardware isn't a straightforward process and even users with higher\
          \ end hardware can struggle with OOM issues if it's not top tier. As this\
          \ is quickly becoming the primary way users are interacting with your models,\
          \ I think it would be advantageous to post these yourself instead of leaving\
          \ it up to users to figure out on their own and potentially, eventually,\
          \ redistribute. \r\nI'm aware Hugging Face is developer centric, and thus\
          \ many will know, or be able to figure out, how to quantize models, and\
          \ can use Google Collab to do so. But even if that's what the expectation\
          \ is, there aren't easily accessible Collab notebooks with scripts to ease\
          \ this process, requiring users to setup and configure the notebook for\
          \ this purpose.\r\nFor the end-user looking to run LLMs locally, the process\
          \ of getting new text generation models is full of friction and confusion,\
          \ and I don't think it has to be this way. It would set a good precedent\
          \ for one of the most user-centric LLM and API developers to provide a user-centric\
          \ experience."
        updatedAt: '2023-04-20T03:16:29.592Z'
      numEdits: 0
      reactions: []
    id: 6440ae8d388f31f64cd05914
    type: comment
  author: justsumguy
  content: "Some users have done so already for certain models but most (including\
    \ this one) aren't yet quantized on Hugging Face. In addition, quantizing on user\
    \ hardware isn't a straightforward process and even users with higher end hardware\
    \ can struggle with OOM issues if it's not top tier. As this is quickly becoming\
    \ the primary way users are interacting with your models, I think it would be\
    \ advantageous to post these yourself instead of leaving it up to users to figure\
    \ out on their own and potentially, eventually, redistribute. \r\nI'm aware Hugging\
    \ Face is developer centric, and thus many will know, or be able to figure out,\
    \ how to quantize models, and can use Google Collab to do so. But even if that's\
    \ what the expectation is, there aren't easily accessible Collab notebooks with\
    \ scripts to ease this process, requiring users to setup and configure the notebook\
    \ for this purpose.\r\nFor the end-user looking to run LLMs locally, the process\
    \ of getting new text generation models is full of friction and confusion, and\
    \ I don't think it has to be this way. It would set a good precedent for one of\
    \ the most user-centric LLM and API developers to provide a user-centric experience."
  created_at: 2023-04-20 02:16:29+00:00
  edited: false
  hidden: false
  id: 6440ae8d388f31f64cd05914
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-04-20T05:35:17.000Z'
    data:
      from: Can you provide quantized models for use with Text Generation Webui?
      to: Can you provide quantized models?
    id: 6440cf15fab821b49bc55667
    type: title-change
  author: Henk717
  created_at: 2023-04-20 04:35:17+00:00
  id: 6440cf15fab821b49bc55667
  new_title: Can you provide quantized models?
  old_title: Can you provide quantized models for use with Text Generation Webui?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-04-20T05:37:16.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>We are currently in the process of adding 4-bit support to KoboldAI
          and part of that requires some changes to how the format works currently.
          Once ready we will begin providing quantized models for sure.</p>

          '
        raw: We are currently in the process of adding 4-bit support to KoboldAI and
          part of that requires some changes to how the format works currently. Once
          ready we will begin providing quantized models for sure.
        updatedAt: '2023-04-20T05:37:16.741Z'
      numEdits: 0
      reactions: []
    id: 6440cf8c967ba11b23dc094f
    type: comment
  author: Henk717
  content: We are currently in the process of adding 4-bit support to KoboldAI and
    part of that requires some changes to how the format works currently. Once ready
    we will begin providing quantized models for sure.
  created_at: 2023-04-20 04:37:16+00:00
  edited: false
  hidden: false
  id: 6440cf8c967ba11b23dc094f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-04-20T05:45:26.000Z'
    data:
      edited: false
      editors:
      - justsumguy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
          fullname: Jared H
          isHf: false
          isPro: false
          name: justsumguy
          type: user
        html: '<p>Awesome, can''t wait!</p>

          '
        raw: Awesome, can't wait!
        updatedAt: '2023-04-20T05:45:26.900Z'
      numEdits: 0
      reactions: []
    id: 6440d176d42c82bc5b7e8f49
    type: comment
  author: justsumguy
  content: Awesome, can't wait!
  created_at: 2023-04-20 04:45:26+00:00
  edited: false
  hidden: false
  id: 6440d176d42c82bc5b7e8f49
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: KoboldAI/OPT-6.7B-Nerybus-Mix
repo_type: model
status: open
target_branch: null
title: Can you provide quantized models?
