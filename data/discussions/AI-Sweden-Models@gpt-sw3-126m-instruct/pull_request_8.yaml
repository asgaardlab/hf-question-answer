!!python/object:huggingface_hub.community.DiscussionWithDetails
author: danieka
conflicting_files: []
created_at: 2023-11-18 12:35:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/e748e50442aeb9726158be2d78385f56.svg
      fullname: Daniel Karlsson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danieka
      type: user
    createdAt: '2023-11-18T12:35:04.000Z'
    data:
      oid: c4452a2f7067ef4a21d15159909d17aa48a8b878
      parents:
      - 630d1047c5cfdc0bc0a8f452358be652d096cd2c
      subject: Upload tokenizer.json
    id: 6558af780000000000000000
    type: commit
  author: danieka
  created_at: 2023-11-18 12:35:04+00:00
  id: 6558af780000000000000000
  oid: c4452a2f7067ef4a21d15159909d17aa48a8b878
  summary: Upload tokenizer.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e748e50442aeb9726158be2d78385f56.svg
      fullname: Daniel Karlsson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danieka
      type: user
    createdAt: '2023-11-18T12:35:50.000Z'
    data:
      edited: false
      editors:
      - danieka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8650538325309753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e748e50442aeb9726158be2d78385f56.svg
          fullname: Daniel Karlsson
          isHf: false
          isPro: false
          name: danieka
          type: user
        html: '<p>I''m trying to use this model with Elixir using Bumblebee and NX/Tokenizers.
          Elixir only has bindings for Rust tokenizers, which this repo did not include.</p>

          <p>I was able to generate a tokenizer.json file with the following code:</p>

          <pre><code>from transformers import AutoTokenizer, PreTrainedTokenizerFast,
          convert_slow_tokenizer


          tokenizer = AutoTokenizer.from_pretrained("AI-Sweden-Models/gpt-sw3-126m-instruct")


          converter = convert_slow_tokenizer.SpmConverter(tokenizer)

          converted = converter.converted()

          converted.save("tokenizer.json")

          </code></pre>

          <p>I got the following warning when running it.</p>

          <pre><code>UserWarning: The sentencepiece tokenizer that you are converting
          to a fast tokenizer uses the byte fallback option which is not implemented
          in the fast tokenizers. In practice this means that the fast version of
          the tokenizer can produce unknown tokens whereas the sentencepiece version
          would have converted these unknown tokens into a sequence of byte tokens
          matching the original piece of text.

          </code></pre>

          <p>I have no idea what I''m doing, but I needed to make a PR in order with
          that file in order to continue testing with Elixir.</p>

          '
        raw: 'I''m trying to use this model with Elixir using Bumblebee and NX/Tokenizers.
          Elixir only has bindings for Rust tokenizers, which this repo did not include.


          I was able to generate a tokenizer.json file with the following code:


          ```

          from transformers import AutoTokenizer, PreTrainedTokenizerFast, convert_slow_tokenizer


          tokenizer = AutoTokenizer.from_pretrained("AI-Sweden-Models/gpt-sw3-126m-instruct")


          converter = convert_slow_tokenizer.SpmConverter(tokenizer)

          converted = converter.converted()

          converted.save("tokenizer.json")

          ```


          I got the following warning when running it.

          ```

          UserWarning: The sentencepiece tokenizer that you are converting to a fast
          tokenizer uses the byte fallback option which is not implemented in the
          fast tokenizers. In practice this means that the fast version of the tokenizer
          can produce unknown tokens whereas the sentencepiece version would have
          converted these unknown tokens into a sequence of byte tokens matching the
          original piece of text.

          ```


          I have no idea what I''m doing, but I needed to make a PR in order with
          that file in order to continue testing with Elixir.'
        updatedAt: '2023-11-18T12:35:50.850Z'
      numEdits: 0
      reactions: []
    id: 6558afa630ad83ad6b26e563
    type: comment
  author: danieka
  content: 'I''m trying to use this model with Elixir using Bumblebee and NX/Tokenizers.
    Elixir only has bindings for Rust tokenizers, which this repo did not include.


    I was able to generate a tokenizer.json file with the following code:


    ```

    from transformers import AutoTokenizer, PreTrainedTokenizerFast, convert_slow_tokenizer


    tokenizer = AutoTokenizer.from_pretrained("AI-Sweden-Models/gpt-sw3-126m-instruct")


    converter = convert_slow_tokenizer.SpmConverter(tokenizer)

    converted = converter.converted()

    converted.save("tokenizer.json")

    ```


    I got the following warning when running it.

    ```

    UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer
    uses the byte fallback option which is not implemented in the fast tokenizers.
    In practice this means that the fast version of the tokenizer can produce unknown
    tokens whereas the sentencepiece version would have converted these unknown tokens
    into a sequence of byte tokens matching the original piece of text.

    ```


    I have no idea what I''m doing, but I needed to make a PR in order with that file
    in order to continue testing with Elixir.'
  created_at: 2023-11-18 12:35:50+00:00
  edited: false
  hidden: false
  id: 6558afa630ad83ad6b26e563
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e748e50442aeb9726158be2d78385f56.svg
      fullname: Daniel Karlsson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danieka
      type: user
    createdAt: '2023-11-21T14:38:20.000Z'
    data:
      status: closed
    id: 655cc0dc4f896c9b4a55baca
    type: status-change
  author: danieka
  created_at: 2023-11-21 14:38:20+00:00
  id: 655cc0dc4f896c9b4a55baca
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 8
repo_id: AI-Sweden-Models/gpt-sw3-126m-instruct
repo_type: model
status: closed
target_branch: refs/heads/main
title: Upload tokenizer.json
