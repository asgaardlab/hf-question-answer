!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TuringsSolutions
conflicting_files: null
created_at: 2023-09-19 22:26:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/cA64Ix1vh75C7HoClUBhx.png?w=200&h=200&f=face
      fullname: Richard A Aragon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TuringsSolutions
      type: user
    createdAt: '2023-09-19T23:26:30.000Z'
    data:
      edited: false
      editors:
      - TuringsSolutions
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9692916870117188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/cA64Ix1vh75C7HoClUBhx.png?w=200&h=200&f=face
          fullname: Richard A Aragon
          isHf: false
          isPro: false
          name: TuringsSolutions
          type: user
        html: '<p>I have been playing around a lot with 1B and of course 1.5B parameter
          models lately. They are super easy to play around with when you have limited
          compute, so I love them. I have noticed generalization limitations in every
          single model in this range I have tested so far. It seems to me that more
          generalized generalization requires a few more parameters than 1.5B.....</p>

          '
        raw: I have been playing around a lot with 1B and of course 1.5B parameter
          models lately. They are super easy to play around with when you have limited
          compute, so I love them. I have noticed generalization limitations in every
          single model in this range I have tested so far. It seems to me that more
          generalized generalization requires a few more parameters than 1.5B.....
        updatedAt: '2023-09-19T23:26:30.090Z'
      numEdits: 0
      reactions: []
    id: 650a2e2619f6f953cb4a7472
    type: comment
  author: TuringsSolutions
  content: I have been playing around a lot with 1B and of course 1.5B parameter models
    lately. They are super easy to play around with when you have limited compute,
    so I love them. I have noticed generalization limitations in every single model
    in this range I have tested so far. It seems to me that more generalized generalization
    requires a few more parameters than 1.5B.....
  created_at: 2023-09-19 22:26:30+00:00
  edited: false
  hidden: false
  id: 650a2e2619f6f953cb4a7472
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-09-20T03:02:43.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9477355480194092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>phi 1.5 is primarily to proof high-quality pretraining dataset is
          comparable to large amount of midocre internet corpora. These smaller models
          are trained for reasons to project loss prediction so as "foreplay" for
          larger models before commiting to sacle up. </p>

          '
        raw: 'phi 1.5 is primarily to proof high-quality pretraining dataset is comparable
          to large amount of midocre internet corpora. These smaller models are trained
          for reasons to project loss prediction so as "foreplay" for larger models
          before commiting to sacle up. '
        updatedAt: '2023-09-20T03:02:43.793Z'
      numEdits: 0
      reactions: []
    id: 650a60d3a2abcb18d6571b07
    type: comment
  author: Yhyu13
  content: 'phi 1.5 is primarily to proof high-quality pretraining dataset is comparable
    to large amount of midocre internet corpora. These smaller models are trained
    for reasons to project loss prediction so as "foreplay" for larger models before
    commiting to sacle up. '
  created_at: 2023-09-20 02:02:43+00:00
  edited: false
  hidden: false
  id: 650a60d3a2abcb18d6571b07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/cA64Ix1vh75C7HoClUBhx.png?w=200&h=200&f=face
      fullname: Richard A Aragon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TuringsSolutions
      type: user
    createdAt: '2023-09-21T15:18:57.000Z'
    data:
      edited: false
      editors:
      - TuringsSolutions
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9831597805023193
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/cA64Ix1vh75C7HoClUBhx.png?w=200&h=200&f=face
          fullname: Richard A Aragon
          isHf: false
          isPro: false
          name: TuringsSolutions
          type: user
        html: '<p>Yes, I know that is why people construct them. People should research
          them more though. I am very interested in them for their research value.
          There are questions that do not get answered if you only treat it as a stepping
          stone to scaling up to more parameters. </p>

          '
        raw: 'Yes, I know that is why people construct them. People should research
          them more though. I am very interested in them for their research value.
          There are questions that do not get answered if you only treat it as a stepping
          stone to scaling up to more parameters. '
        updatedAt: '2023-09-21T15:18:57.217Z'
      numEdits: 0
      reactions: []
    id: 650c5ee1d61f5cca20c4362d
    type: comment
  author: TuringsSolutions
  content: 'Yes, I know that is why people construct them. People should research
    them more though. I am very interested in them for their research value. There
    are questions that do not get answered if you only treat it as a stepping stone
    to scaling up to more parameters. '
  created_at: 2023-09-21 14:18:57+00:00
  edited: false
  hidden: false
  id: 650c5ee1d61f5cca20c4362d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ed4ae9be9492e21d8296ccb2d2629fd.svg
      fullname: Max
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DarwinAnim8or
      type: user
    createdAt: '2023-09-27T21:10:41.000Z'
    data:
      edited: false
      editors:
      - DarwinAnim8or
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9612640142440796
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ed4ae9be9492e21d8296ccb2d2629fd.svg
          fullname: Max
          isHf: false
          isPro: false
          name: DarwinAnim8or
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TuringsSolutions&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TuringsSolutions\"\
          >@<span class=\"underline\">TuringsSolutions</span></a></span>\n\n\t</span></span>\
          \ Have you tried using this model inside of the huggingface chat UI? I feel\
          \ like utilizing some of the tricks written in OpenAI's cookbook would help;\
          \ namely adding search results and making the model do another pass to first\
          \ think through a problem before giving a final reply. </p>\n<p>That would\
          \ of course complicate the code to run the model (and the general inference\
          \ time) quite a bit, but I figure it's worth trying out </p>\n"
        raw: "@TuringsSolutions Have you tried using this model inside of the huggingface\
          \ chat UI? I feel like utilizing some of the tricks written in OpenAI's\
          \ cookbook would help; namely adding search results and making the model\
          \ do another pass to first think through a problem before giving a final\
          \ reply. \n\nThat would of course complicate the code to run the model (and\
          \ the general inference time) quite a bit, but I figure it's worth trying\
          \ out "
        updatedAt: '2023-09-27T21:10:41.565Z'
      numEdits: 0
      reactions: []
    id: 65149a51645401ea185b3772
    type: comment
  author: DarwinAnim8or
  content: "@TuringsSolutions Have you tried using this model inside of the huggingface\
    \ chat UI? I feel like utilizing some of the tricks written in OpenAI's cookbook\
    \ would help; namely adding search results and making the model do another pass\
    \ to first think through a problem before giving a final reply. \n\nThat would\
    \ of course complicate the code to run the model (and the general inference time)\
    \ quite a bit, but I figure it's worth trying out "
  created_at: 2023-09-27 20:10:41+00:00
  edited: false
  hidden: false
  id: 65149a51645401ea185b3772
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Open-Orca/oo-phi-1_5
repo_type: model
status: open
target_branch: null
title: '"This model doesn''t dramatically improve on the base model''s general task
  performance"'
