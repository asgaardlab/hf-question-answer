!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MengboZhou
conflicting_files: null
created_at: 2023-10-30 12:58:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a30d6ce1c354c74ab42b23e4af553e4.svg
      fullname: MengboZhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MengboZhou
      type: user
    createdAt: '2023-10-30T13:58:26.000Z'
    data:
      edited: false
      editors:
      - MengboZhou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.568872332572937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a30d6ce1c354c74ab42b23e4af553e4.svg
          fullname: MengboZhou
          isHf: false
          isPro: false
          name: MengboZhou
          type: user
        html: '<p>generation_config = GenerationConfig(<br>    max_length=1024, temperature=0.00,
          top_p=0.95, repetition_penalty=1.1,<br>    do_sample=True, use_cache=True,<br>    eos_token_id=tokenizer.eos_token_id,
          pad_token_id=tokenizer.pad_token_id,<br>    transformers_version="4.33.1"<br>    )</p>

          <p>ValueError: <code>temperature</code> (=0.0) has to be a strictly positive
          float, otherwise your next token scores will be invalid. If you''re looking
          for greedy decoding strategies, set <code>do_sample=False</code>.</p>

          <p>I don''t understand why the temperature here cannot be set to 0. Could
          you tell me what other parameters can be adjusted?</p>

          '
        raw: "generation_config = GenerationConfig(\r\n    max_length=1024, temperature=0.00,\
          \ top_p=0.95, repetition_penalty=1.1,\r\n    do_sample=True, use_cache=True,\r\
          \n    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\r\
          \n    transformers_version=\"4.33.1\"\r\n    )\r\n\r\nValueError: `temperature`\
          \ (=0.0) has to be a strictly positive float, otherwise your next token\
          \ scores will be invalid. If you're looking for greedy decoding strategies,\
          \ set `do_sample=False`.\r\n\r\nI don't understand why the temperature here\
          \ cannot be set to 0. Could you tell me what other parameters can be adjusted?"
        updatedAt: '2023-10-30T13:58:26.292Z'
      numEdits: 0
      reactions: []
    id: 653fb6824722aa28bba32bd7
    type: comment
  author: MengboZhou
  content: "generation_config = GenerationConfig(\r\n    max_length=1024, temperature=0.00,\
    \ top_p=0.95, repetition_penalty=1.1,\r\n    do_sample=True, use_cache=True,\r\
    \n    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\r\
    \n    transformers_version=\"4.33.1\"\r\n    )\r\n\r\nValueError: `temperature`\
    \ (=0.0) has to be a strictly positive float, otherwise your next token scores\
    \ will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.\r\
    \n\r\nI don't understand why the temperature here cannot be set to 0. Could you\
    \ tell me what other parameters can be adjusted?"
  created_at: 2023-10-30 12:58:26+00:00
  edited: false
  hidden: false
  id: 653fb6824722aa28bba32bd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643c81209f5d314db2de9743/EgFTMX41xoaPdp5jffj1G.png?w=200&h=200&f=face
      fullname: Nathan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: unaidedelf87777
      type: user
    createdAt: '2023-10-31T13:16:54.000Z'
    data:
      edited: false
      editors:
      - unaidedelf87777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9215204119682312
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643c81209f5d314db2de9743/EgFTMX41xoaPdp5jffj1G.png?w=200&h=200&f=face
          fullname: Nathan
          isHf: false
          isPro: false
          name: unaidedelf87777
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MengboZhou&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/MengboZhou\">@<span class=\"\
          underline\">MengboZhou</span></a></span>\n\n\t</span></span>,</p>\n<p>Indeed,\
          \ the temperature parameter has to be a strictly positive float. The reason\
          \ behind this is that temperature is used to adjust the probabilities of\
          \ the next tokens during sampling. Specifically, it scales the logits (the\
          \ raw prediction values) before they are turned into probabilities.</p>\n\
          <p>When the temperature is set to a value close to zero (but not zero),\
          \ it makes the model's sampling almost deterministic, choosing the most\
          \ probable token with very high likelihood. However, when the temperature\
          \ is exactly zero, it would effectively mean dividing by zero or multiplying\
          \ by infinity, leading to invalid probabilities.</p>\n<p>If you're looking\
          \ to have greedy decoding (always choosing the most probable next token),\
          \ you can set do_sample=False and not worry about the temperature. However,\
          \ if you want to use the do_sample=True option and still want the generation\
          \ to be almost deterministic, set the temperature to a very small value\
          \ like 0.001.</p>\n"
        raw: '@MengboZhou,


          Indeed, the temperature parameter has to be a strictly positive float. The
          reason behind this is that temperature is used to adjust the probabilities
          of the next tokens during sampling. Specifically, it scales the logits (the
          raw prediction values) before they are turned into probabilities.


          When the temperature is set to a value close to zero (but not zero), it
          makes the model''s sampling almost deterministic, choosing the most probable
          token with very high likelihood. However, when the temperature is exactly
          zero, it would effectively mean dividing by zero or multiplying by infinity,
          leading to invalid probabilities.


          If you''re looking to have greedy decoding (always choosing the most probable
          next token), you can set do_sample=False and not worry about the temperature.
          However, if you want to use the do_sample=True option and still want the
          generation to be almost deterministic, set the temperature to a very small
          value like 0.001.


          '
        updatedAt: '2023-10-31T13:16:54.704Z'
      numEdits: 0
      reactions: []
    id: 6540fe460a4e554c1bfe9626
    type: comment
  author: unaidedelf87777
  content: '@MengboZhou,


    Indeed, the temperature parameter has to be a strictly positive float. The reason
    behind this is that temperature is used to adjust the probabilities of the next
    tokens during sampling. Specifically, it scales the logits (the raw prediction
    values) before they are turned into probabilities.


    When the temperature is set to a value close to zero (but not zero), it makes
    the model''s sampling almost deterministic, choosing the most probable token with
    very high likelihood. However, when the temperature is exactly zero, it would
    effectively mean dividing by zero or multiplying by infinity, leading to invalid
    probabilities.


    If you''re looking to have greedy decoding (always choosing the most probable
    next token), you can set do_sample=False and not worry about the temperature.
    However, if you want to use the do_sample=True option and still want the generation
    to be almost deterministic, set the temperature to a very small value like 0.001.


    '
  created_at: 2023-10-31 12:16:54+00:00
  edited: false
  hidden: false
  id: 6540fe460a4e554c1bfe9626
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Open-Orca/oo-phi-1_5
repo_type: model
status: open
target_branch: null
title: 'Why ValueError: `temperature` (=0.0) has to be a strictly positive float ???'
