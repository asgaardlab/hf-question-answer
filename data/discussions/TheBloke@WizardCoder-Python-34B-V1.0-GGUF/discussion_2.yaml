!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YairFr
conflicting_files: null
created_at: 2023-08-31 10:02:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a68ce0e73aa7763fdcbd7f5f177edea8.svg
      fullname: Yair Friedman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YairFr
      type: user
    createdAt: '2023-08-31T11:02:48.000Z'
    data:
      edited: false
      editors:
      - YairFr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8422839641571045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a68ce0e73aa7763fdcbd7f5f177edea8.svg
          fullname: Yair Friedman
          isHf: false
          isPro: false
          name: YairFr
          type: user
        html: '<p>is it 4K or 16K ?</p>

          '
        raw: is it 4K or 16K ?
        updatedAt: '2023-08-31T11:02:48.697Z'
      numEdits: 0
      reactions: []
    id: 64f07358e5500d42ffbbed27
    type: comment
  author: YairFr
  content: is it 4K or 16K ?
  created_at: 2023-08-31 10:02:48+00:00
  edited: false
  hidden: false
  id: 64f07358e5500d42ffbbed27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-31T11:22:27.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.21512183547019958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The base CodeLlama was trained to 16K. It can theoretically be used
          at longer context lengths as well, and of course shorter if you want</p>

          <p>WizardCoder specifically I think they fine tuned at 4K.  But the model
          will still support 16K - though how well it will cope with the fine tuning
          at that length, I don''t know</p>

          '
        raw: 'The base CodeLlama was trained to 16K. It can theoretically be used
          at longer context lengths as well, and of course shorter if you want


          WizardCoder specifically I think they fine tuned at 4K.  But the model will
          still support 16K - though how well it will cope with the fine tuning at
          that length, I don''t know'
        updatedAt: '2023-08-31T11:23:45.791Z'
      numEdits: 2
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - YairFr
        - jlzhou
        - cleverest
        - da-costa
        - esteband
    id: 64f077f3e60de226deff8e2a
    type: comment
  author: TheBloke
  content: 'The base CodeLlama was trained to 16K. It can theoretically be used at
    longer context lengths as well, and of course shorter if you want


    WizardCoder specifically I think they fine tuned at 4K.  But the model will still
    support 16K - though how well it will cope with the fine tuning at that length,
    I don''t know'
  created_at: 2023-08-31 10:22:27+00:00
  edited: true
  hidden: false
  id: 64f077f3e60de226deff8e2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
      fullname: Jawad Mansoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supercharge19
      type: user
    createdAt: '2023-09-12T06:40:44.000Z'
    data:
      edited: false
      editors:
      - supercharge19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9523854851722717
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
          fullname: Jawad Mansoor
          isHf: false
          isPro: false
          name: supercharge19
          type: user
        html: '<p>Is there anyway to get longer context with gguf files? I heard that
          llama cpp supports longer context but you need to do some configuration
          (which I do not know yet).</p>

          '
        raw: Is there anyway to get longer context with gguf files? I heard that llama
          cpp supports longer context but you need to do some configuration (which
          I do not know yet).
        updatedAt: '2023-09-12T06:40:44.825Z'
      numEdits: 0
      reactions: []
    id: 650007ec18830fabea4601eb
    type: comment
  author: supercharge19
  content: Is there anyway to get longer context with gguf files? I heard that llama
    cpp supports longer context but you need to do some configuration (which I do
    not know yet).
  created_at: 2023-09-12 05:40:44+00:00
  edited: false
  hidden: false
  id: 650007ec18830fabea4601eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardCoder-Python-34B-V1.0-GGUF
repo_type: model
status: open
target_branch: null
title: what is the context length of this model ?
