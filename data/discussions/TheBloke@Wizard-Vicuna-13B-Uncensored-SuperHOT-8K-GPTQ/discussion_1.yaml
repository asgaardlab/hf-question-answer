!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mikolodz
conflicting_files: null
created_at: 2023-06-27 08:09:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
      fullname: Michal Kolodziej
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikolodz
      type: user
    createdAt: '2023-06-27T09:09:16.000Z'
    data:
      edited: false
      editors:
      - mikolodz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8465978503227234
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
          fullname: Michal Kolodziej
          isHf: false
          isPro: false
          name: mikolodz
          type: user
        html: '<p>Hi,</p>

          <p>Is it normal that I''m getting really poor responses from the SuperHOT
          models I tested?<br>E.g. they stop in the middle of the sentence (Wizard-Vicuna
          13B 8K) or add some strange formating like ```makefile (Wicuna 1.3 13B 8K)
          etc.</p>

          <p>Maybe I''m missing something? I use ooba + AutoGPTQ, nothing fancy..</p>

          '
        raw: "Hi,\r\n\r\nIs it normal that I'm getting really poor responses from\
          \ the SuperHOT models I tested?\r\nE.g. they stop in the middle of the sentence\
          \ (Wizard-Vicuna 13B 8K) or add some strange formating like ```makefile\
          \ (Wicuna 1.3 13B 8K) etc.\r\n\r\nMaybe I'm missing something? I use ooba\
          \ + AutoGPTQ, nothing fancy.."
        updatedAt: '2023-06-27T09:09:16.935Z'
      numEdits: 0
      reactions: []
    id: 649aa73cfacc059c2dc77240
    type: comment
  author: mikolodz
  content: "Hi,\r\n\r\nIs it normal that I'm getting really poor responses from the\
    \ SuperHOT models I tested?\r\nE.g. they stop in the middle of the sentence (Wizard-Vicuna\
    \ 13B 8K) or add some strange formating like ```makefile (Wicuna 1.3 13B 8K) etc.\r\
    \n\r\nMaybe I'm missing something? I use ooba + AutoGPTQ, nothing fancy.."
  created_at: 2023-06-27 08:09:16+00:00
  edited: false
  hidden: false
  id: 649aa73cfacc059c2dc77240
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e706fe7bb21a046f80895901704b204a.svg
      fullname: Jeffrey Gilbert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Renegadesoffun
      type: user
    createdAt: '2023-06-28T07:31:39.000Z'
    data:
      edited: false
      editors:
      - Renegadesoffun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8453114628791809
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e706fe7bb21a046f80895901704b204a.svg
          fullname: Jeffrey Gilbert
          isHf: false
          isPro: false
          name: Renegadesoffun
          type: user
        html: '<p>@mikeoldz I think its supposed to be used with exllama under load
          model *make sure u have latest oobabooga updated to choose model from drop
          down menu)  and then if still issue maybe lower response to 6000 tokens
          (aitreupener latest video shows how on youtube <a rel="nofollow" href="https://www.youtube.com/watch?v=199h5XxUEOY&amp;ab_channel=Aitrepreneur">https://www.youtube.com/watch?v=199h5XxUEOY&amp;ab_channel=Aitrepreneur</a></p>

          '
        raw: '@mikeoldz I think its supposed to be used with exllama under load model
          *make sure u have latest oobabooga updated to choose model from drop down
          menu)  and then if still issue maybe lower response to 6000 tokens (aitreupener
          latest video shows how on youtube https://www.youtube.com/watch?v=199h5XxUEOY&ab_channel=Aitrepreneur'
        updatedAt: '2023-06-28T07:31:39.264Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mikolodz
    id: 649be1db56dc7d9cb0fbf93b
    type: comment
  author: Renegadesoffun
  content: '@mikeoldz I think its supposed to be used with exllama under load model
    *make sure u have latest oobabooga updated to choose model from drop down menu)  and
    then if still issue maybe lower response to 6000 tokens (aitreupener latest video
    shows how on youtube https://www.youtube.com/watch?v=199h5XxUEOY&ab_channel=Aitrepreneur'
  created_at: 2023-06-28 06:31:39+00:00
  edited: false
  hidden: false
  id: 649be1db56dc7d9cb0fbf93b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/upL2BEgGjiZiEXvTSyk2o.png?w=200&h=200&f=face
      fullname: Kippykip
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kippykip
      type: user
    createdAt: '2023-06-28T12:45:46.000Z'
    data:
      edited: false
      editors:
      - Kippykip
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8919604420661926
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/upL2BEgGjiZiEXvTSyk2o.png?w=200&h=200&f=face
          fullname: Kippykip
          isHf: false
          isPro: false
          name: Kippykip
          type: user
        html: "<p>Ah yeah this model (and all the SuperHot-8k models) are not supposed\
          \ to be loaded with AutoGPTQ, you use exllama.<br>Otherwise it'll probably\
          \ be buggy lol</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;Renegadesoffun&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Renegadesoffun\"\
          >@<span class=\"underline\">Renegadesoffun</span></a></span>\n\n\t</span></span>\
          \ 's linked video is really noob friendly, it's how I figured out how to\
          \ use it also!<br>Good luck :D</p>\n"
        raw: 'Ah yeah this model (and all the SuperHot-8k models) are not supposed
          to be loaded with AutoGPTQ, you use exllama.

          Otherwise it''ll probably be buggy lol


          @Renegadesoffun ''s linked video is really noob friendly, it''s how I figured
          out how to use it also!

          Good luck :D'
        updatedAt: '2023-06-28T12:45:46.450Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - mikolodz
        - Renegadesoffun
        - mikeyang01
    id: 649c2b7aa83f996b4193e5f8
    type: comment
  author: Kippykip
  content: 'Ah yeah this model (and all the SuperHot-8k models) are not supposed to
    be loaded with AutoGPTQ, you use exllama.

    Otherwise it''ll probably be buggy lol


    @Renegadesoffun ''s linked video is really noob friendly, it''s how I figured
    out how to use it also!

    Good luck :D'
  created_at: 2023-06-28 11:45:46+00:00
  edited: false
  hidden: false
  id: 649c2b7aa83f996b4193e5f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-28T15:22:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9347885847091675
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It will work with AutoGPTQ, as long as you set trust_remote_code
          = True and set the desired context length by editing config.json. I explained
          this in the Readme.</p>

          <p>But ExLlama is definitely preferable, as it''s faster, uses less VRAM,
          and the controls are directly integrated into the UI.  I''d only suggest
          AutoGPTQ use if you want to use the model from Python code, or outside text-generation-webui.</p>

          '
        raw: 'It will work with AutoGPTQ, as long as you set trust_remote_code = True
          and set the desired context length by editing config.json. I explained this
          in the Readme.


          But ExLlama is definitely preferable, as it''s faster, uses less VRAM, and
          the controls are directly integrated into the UI.  I''d only suggest AutoGPTQ
          use if you want to use the model from Python code, or outside text-generation-webui.'
        updatedAt: '2023-06-28T15:22:52.390Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mikolodz
        - Renegadesoffun
    id: 649c504ca74b6dbfe6341b77
    type: comment
  author: TheBloke
  content: 'It will work with AutoGPTQ, as long as you set trust_remote_code = True
    and set the desired context length by editing config.json. I explained this in
    the Readme.


    But ExLlama is definitely preferable, as it''s faster, uses less VRAM, and the
    controls are directly integrated into the UI.  I''d only suggest AutoGPTQ use
    if you want to use the model from Python code, or outside text-generation-webui.'
  created_at: 2023-06-28 14:22:52+00:00
  edited: false
  hidden: false
  id: 649c504ca74b6dbfe6341b77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
      fullname: Michal Kolodziej
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikolodz
      type: user
    createdAt: '2023-06-28T19:36:36.000Z'
    data:
      edited: true
      editors:
      - mikolodz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9249443411827087
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
          fullname: Michal Kolodziej
          isHf: false
          isPro: false
          name: mikolodz
          type: user
        html: '<p>Thank you, mate! Indeed, lowering  the token count to 6144 seems
          to help :)</p>

          <p>BTW. I''ve tried AutoGPTQ and ExLlama before, but only the above helped.
          Is it due to the response token count? I''ve been using response max_new_tokens
          like 2048 or 1024 and max_seq_len = 8192 with no luck on 3090. It gave me
          either short responses or pure nonsense.</p>

          '
        raw: 'Thank you, mate! Indeed, lowering  the token count to 6144 seems to
          help :)


          BTW. I''ve tried AutoGPTQ and ExLlama before, but only the above helped.
          Is it due to the response token count? I''ve been using response max_new_tokens
          like 2048 or 1024 and max_seq_len = 8192 with no luck on 3090. It gave me
          either short responses or pure nonsense.'
        updatedAt: '2023-06-28T19:40:45.259Z'
      numEdits: 1
      reactions: []
    id: 649c8bc4ecd3a771cb7633f1
    type: comment
  author: mikolodz
  content: 'Thank you, mate! Indeed, lowering  the token count to 6144 seems to help
    :)


    BTW. I''ve tried AutoGPTQ and ExLlama before, but only the above helped. Is it
    due to the response token count? I''ve been using response max_new_tokens like
    2048 or 1024 and max_seq_len = 8192 with no luck on 3090. It gave me either short
    responses or pure nonsense.'
  created_at: 2023-06-28 18:36:36+00:00
  edited: true
  hidden: false
  id: 649c8bc4ecd3a771cb7633f1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ
repo_type: model
status: open
target_branch: null
title: Response quality much lower than with 2K version
