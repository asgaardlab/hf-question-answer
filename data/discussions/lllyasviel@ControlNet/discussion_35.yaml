!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Xaot6
conflicting_files: null
created_at: 2023-03-15 11:12:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c1fe7dbe978b8995cd4f4149131b6681.svg
      fullname: Xin Sei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xaot6
      type: user
    createdAt: '2023-03-15T12:12:59.000Z'
    data:
      edited: true
      editors:
      - Xaot6
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c1fe7dbe978b8995cd4f4149131b6681.svg
          fullname: Xin Sei
          isHf: false
          isPro: false
          name: Xaot6
          type: user
        html: '<p>Would it be possible to create embeddings for the ControlNet segmentation
          model to add more concepts? And if so could this be done by using a method
          like dreambooth and having the color as a token or do you have to retrain
          the whole data set at once?</p>

          <p>Maybe this is not thought through far enough but it would be very nice
          to be able to have even more control over the generated image.</p>

          '
        raw: 'Would it be possible to create embeddings for the ControlNet segmentation
          model to add more concepts? And if so could this be done by using a method
          like dreambooth and having the color as a token or do you have to retrain
          the whole data set at once?


          Maybe this is not thought through far enough but it would be very nice to
          be able to have even more control over the generated image.'
        updatedAt: '2023-03-15T12:13:29.707Z'
      numEdits: 1
      reactions: []
    id: 6411b64bd52c57f628f77de5
    type: comment
  author: Xaot6
  content: 'Would it be possible to create embeddings for the ControlNet segmentation
    model to add more concepts? And if so could this be done by using a method like
    dreambooth and having the color as a token or do you have to retrain the whole
    data set at once?


    Maybe this is not thought through far enough but it would be very nice to be able
    to have even more control over the generated image.'
  created_at: 2023-03-15 11:12:59+00:00
  edited: true
  hidden: false
  id: 6411b64bd52c57f628f77de5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49dcdc289570c4f1e8d81d7d3f6ab736.svg
      fullname: Stoic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ascendant
      type: user
    createdAt: '2023-03-15T12:35:10.000Z'
    data:
      edited: false
      editors:
      - Ascendant
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49dcdc289570c4f1e8d81d7d3f6ab736.svg
          fullname: Stoic
          isHf: false
          isPro: false
          name: Ascendant
          type: user
        html: '<blockquote>

          <p>Would it be possible to create embeddings for the ControlNet segmentation
          model to add more concepts? And if so could this be done by using a method
          like dreambooth and having the color as a token or do you have to retrain
          the whole data set at once?</p>

          <p>Maybe this is not thought through far enough but it would be very nice
          to be able to have even more control over the generated image.</p>

          </blockquote>

          <p>Not sure about that,  but Latent Couple with masking works very similarly
          to segmentation model from Control Net except you are actually completely
          free to specifically appoint a sub-prompt to each colored region and be
          sure that each region contains the specific object or person you want to
          generate there.</p>

          '
        raw: "> Would it be possible to create embeddings for the ControlNet segmentation\
          \ model to add more concepts? And if so could this be done by using a method\
          \ like dreambooth and having the color as a token or do you have to retrain\
          \ the whole data set at once?\n> \n> Maybe this is not thought through far\
          \ enough but it would be very nice to be able to have even more control\
          \ over the generated image.\n\nNot sure about that,  but Latent Couple with\
          \ masking works very similarly to segmentation model from Control Net except\
          \ you are actually completely free to specifically appoint a sub-prompt\
          \ to each colored region and be sure that each region contains the specific\
          \ object or person you want to generate there."
        updatedAt: '2023-03-15T12:35:10.192Z'
      numEdits: 0
      reactions: []
    id: 6411bb7e78f6d9f06c765248
    type: comment
  author: Ascendant
  content: "> Would it be possible to create embeddings for the ControlNet segmentation\
    \ model to add more concepts? And if so could this be done by using a method like\
    \ dreambooth and having the color as a token or do you have to retrain the whole\
    \ data set at once?\n> \n> Maybe this is not thought through far enough but it\
    \ would be very nice to be able to have even more control over the generated image.\n\
    \nNot sure about that,  but Latent Couple with masking works very similarly to\
    \ segmentation model from Control Net except you are actually completely free\
    \ to specifically appoint a sub-prompt to each colored region and be sure that\
    \ each region contains the specific object or person you want to generate there."
  created_at: 2023-03-15 11:35:10+00:00
  edited: false
  hidden: false
  id: 6411bb7e78f6d9f06c765248
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c1fe7dbe978b8995cd4f4149131b6681.svg
      fullname: Xin Sei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xaot6
      type: user
    createdAt: '2023-03-15T23:28:17.000Z'
    data:
      edited: true
      editors:
      - Xaot6
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c1fe7dbe978b8995cd4f4149131b6681.svg
          fullname: Xin Sei
          isHf: false
          isPro: false
          name: Xaot6
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Ascendant&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Ascendant\">@<span class=\"\
          underline\">Ascendant</span></a></span>\n\n\t</span></span>  Thank you very\
          \ much for the response! I will look into it and see if it helps me out.</p>\n\
          <p>Update:<br>Ok so i looked into it and it is quite interesting. I would\
          \ consider it as a yes but no. So the idea would be to make a renderer using\
          \ stable diffusion where you asign a lot (or a few) different colors to\
          \ objects and it automatically, without a lot of prompting, knows what to\
          \ do. To have more control over the generated image it would be helpfull\
          \ to add these extra materials and textures.</p>\n"
        raw: '@Ascendant  Thank you very much for the response! I will look into it
          and see if it helps me out.


          Update:

          Ok so i looked into it and it is quite interesting. I would consider it
          as a yes but no. So the idea would be to make a renderer using stable diffusion
          where you asign a lot (or a few) different colors to objects and it automatically,
          without a lot of prompting, knows what to do. To have more control over
          the generated image it would be helpfull to add these extra materials and
          textures.'
        updatedAt: '2023-03-15T23:34:49.587Z'
      numEdits: 1
      reactions: []
    id: 64125491982a341736ca4e24
    type: comment
  author: Xaot6
  content: '@Ascendant  Thank you very much for the response! I will look into it
    and see if it helps me out.


    Update:

    Ok so i looked into it and it is quite interesting. I would consider it as a yes
    but no. So the idea would be to make a renderer using stable diffusion where you
    asign a lot (or a few) different colors to objects and it automatically, without
    a lot of prompting, knows what to do. To have more control over the generated
    image it would be helpfull to add these extra materials and textures.'
  created_at: 2023-03-15 22:28:17+00:00
  edited: true
  hidden: false
  id: 64125491982a341736ca4e24
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: lllyasviel/ControlNet
repo_type: model
status: open
target_branch: null
title: Enhance ControlNet model with embedding
