!!python/object:huggingface_hub.community.DiscussionWithDetails
author: timecome
conflicting_files: null
created_at: 2023-07-04 05:03:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c32f77d3b34b0b81cd01f9ceed518604.svg
      fullname: nobody
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timecome
      type: user
    createdAt: '2023-07-04T06:03:13.000Z'
    data:
      edited: true
      editors:
      - timecome
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6034420728683472
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c32f77d3b34b0b81cd01f9ceed518604.svg
          fullname: nobody
          isHf: false
          isPro: false
          name: timecome
          type: user
        html: '<p>./main -m /media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin
          -p  ''''''build a web'''''' -n 512 -ngl 40<br>main: build = 780 (698efad)<br>main:
          seed  = 1688450676<br>ggml_init_cublas: found 1 CUDA devices:<br>  Device
          0: NVIDIA GeForce RTX 4090<br>llama.cpp: loading model from /media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin<br>llama_model_load_internal:
          format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab    =
          32000<br>llama_model_load_internal: n_ctx      = 512<br>llama_model_load_internal:
          n_embd     = 5120<br>llama_model_load_internal: n_mult     = 256<br>llama_model_load_internal:
          n_head     = 40<br>llama_model_load_internal: n_layer    = 40<br>llama_model_load_internal:
          n_rot      = 128<br>llama_model_load_internal: ftype      = 7 (mostly Q8_0)<br>llama_model_load_internal:
          n_ff       = 13824<br>llama_model_load_internal: model size = 13B<br>llama_model_load_internal:
          ggml ctx size =    0.06 MB<br>llama_model_load_internal: using CUDA for
          GPU acceleration<br>error loading model: llama.cpp: tensor ''layers.26.attention_norm.weight''
          is missing from model<br>llama_load_model_from_file: failed to load model<br>llama_init_from_gpt_params:
          error: failed to load model ''/media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin''<br>main:
          error: unable to load model</p>

          '
        raw: "./main -m /media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin\
          \ -p  '''build a web''' -n 512 -ngl 40\nmain: build = 780 (698efad)\nmain:\
          \ seed  = 1688450676\nggml_init_cublas: found 1 CUDA devices:\n  Device\
          \ 0: NVIDIA GeForce RTX 4090\nllama.cpp: loading model from /media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal:\
          \ n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 7 (mostly Q8_0)\n\
          llama_model_load_internal: n_ff       = 13824\nllama_model_load_internal:\
          \ model size = 13B\nllama_model_load_internal: ggml ctx size =    0.06 MB\n\
          llama_model_load_internal: using CUDA for GPU acceleration\nerror loading\
          \ model: llama.cpp: tensor 'layers.26.attention_norm.weight' is missing\
          \ from model\nllama_load_model_from_file: failed to load model\nllama_init_from_gpt_params:\
          \ error: failed to load model '/media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin'\n\
          main: error: unable to load model\n\n"
        updatedAt: '2023-07-04T06:05:11.994Z'
      numEdits: 1
      reactions: []
    id: 64a3b62173f3ad435c3bf0db
    type: comment
  author: timecome
  content: "./main -m /media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin\
    \ -p  '''build a web''' -n 512 -ngl 40\nmain: build = 780 (698efad)\nmain: seed\
    \  = 1688450676\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce\
    \ RTX 4090\nllama.cpp: loading model from /media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin\n\
    llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal:\
    \ n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal:\
    \ n_rot      = 128\nllama_model_load_internal: ftype      = 7 (mostly Q8_0)\n\
    llama_model_load_internal: n_ff       = 13824\nllama_model_load_internal: model\
    \ size = 13B\nllama_model_load_internal: ggml ctx size =    0.06 MB\nllama_model_load_internal:\
    \ using CUDA for GPU acceleration\nerror loading model: llama.cpp: tensor 'layers.26.attention_norm.weight'\
    \ is missing from model\nllama_load_model_from_file: failed to load model\nllama_init_from_gpt_params:\
    \ error: failed to load model '/media/arthur/data/model/chat/WizardLM-13B-V1.0-Uncensored-GGML/wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin'\n\
    main: error: unable to load model\n\n"
  created_at: 2023-07-04 05:03:13+00:00
  edited: true
  hidden: false
  id: 64a3b62173f3ad435c3bf0db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T13:18:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9264070987701416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Re-download the file, looks like your download aborted or got corrupted.</p>

          '
        raw: Re-download the file, looks like your download aborted or got corrupted.
        updatedAt: '2023-07-07T13:18:37.935Z'
      numEdits: 0
      reactions: []
    id: 64a810ad64de795adbf90e62
    type: comment
  author: TheBloke
  content: Re-download the file, looks like your download aborted or got corrupted.
  created_at: 2023-07-07 12:18:37+00:00
  edited: false
  hidden: false
  id: 64a810ad64de795adbf90e62
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-13B-V1.0-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: 'wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin error '
