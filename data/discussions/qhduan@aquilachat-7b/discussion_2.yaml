!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sammysun0711
conflicting_files: null
created_at: 2023-06-15 11:16:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/61ea8c6d78c6fb4f43127703aab01976.svg
      fullname: Xiake Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sammysun0711
      type: user
    createdAt: '2023-06-15T12:16:52.000Z'
    data:
      edited: true
      editors:
      - sammysun0711
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4755948781967163
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/61ea8c6d78c6fb4f43127703aab01976.svg
          fullname: Xiake Sun
          isHf: false
          isPro: false
          name: sammysun0711
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;qhduan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/qhduan\">@<span class=\"\
          underline\">qhduan</span></a></span>\n\n\t</span></span>, first of all,\
          \ thanks for your great work, it is really helpful for community for Chinses\
          \ LLM.<br>As I see in <code>modeling_aquila.py</code>, this model can re-use\
          \ most of structure of LlaMA,  and Optimum supports  LlaMA ONNX export.<br>So\
          \ I save Aquilachat-7b model locally and try to export ONNX model as follow:</p>\n\
          <pre><code>optimum-cli export onnx --model aquilachat-7b \\\n    --task\
          \ text-generation --trust-remote-code \\\n    --framework pt --opset 17\
          \ onnx\n</code></pre>\n<p>Here I met following issue during shape inference:</p>\n\
          <pre><code>~/anaconda3/envs/aigc/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py\n\
          309 in _create_node\n_C._jit_pass_onnx_node_shape_type_inference(node, params_dict,\
          \ opset_version)\nRuntimeError: ScalarType ComplexFloat is an unexpected\
          \ tensor scalar type \n</code></pre>\n<p>I checked that node:  %443 indeed\
          \ a Tensor with ComplexFloat type in self_attn.<br>Since Complex types is\
          \ a known limitation for the ONNX exporter: <a rel=\"nofollow\" href=\"\
          https://github.com/pytorch/pytorch/issues/59246\">https://github.com/pytorch/pytorch/issues/59246</a>,\
          \ could you please share any workaround to export ONNX model? </p>\n<pre><code>node:\
          \  %442 : Tensor = onnx::Transpose[perm=[0, 2, 1, 3]](%441), scope: transformers_modules.aquilachat-7b.modeling_aquila.LlamaForCausalLM::/transformers_modules.aquilachat-7b.modeling_aquila.LlamaModel::model/transformers_modules.aquilachat-7b.modeling_aquila.LlamaDecoderLayer::layers.0/transformers_modules.aquilachat-7b.modeling_aquila.LlamaAttention::self_attn\n\
          \nvalue_t:  tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n\
          \          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n          1.0000+0.0000e+00j],\n\
          \        [ 0.5403+8.4147e-01j,  0.6479+7.6172e-01j,  0.7318+6.8156e-01j,\n\
          \          ...,  1.0000+1.5399e-04j,  1.0000+1.3335e-04j,\n          1.0000+1.1548e-04j],\n\
          \        [-0.4161+9.0930e-01j, -0.1604+9.8705e-01j,  0.0709+9.9748e-01j,\n\
          \          ...,  1.0000+3.0799e-04j,  1.0000+2.6670e-04j,\n          1.0000+2.3096e-04j],\n\
          \        ...,\n        [-0.8799+4.7523e-01j,  0.7803+6.2535e-01j, -0.9998+1.9127e-02j,\n\
          \          ...,  0.8079+5.8938e-01j,  0.8547+5.1911e-01j,\n          0.8904+4.5525e-01j],\n\
          \        [-0.8753-4.8361e-01j,  0.0292+9.9957e-01j, -0.7446-6.6752e-01j,\n\
          \          ...,  0.8078+5.8951e-01j,  0.8546+5.1922e-01j,\n          0.8903+4.5535e-01j],\n\
          \        [-0.0660-9.9782e-01j, -0.7424+6.6991e-01j, -0.0900-9.9594e-01j,\n\
          \          ...,  0.8077+5.8963e-01j,  0.8546+5.1934e-01j,\n          0.8903+4.5545e-01j]])\n\
          node:  %443 : Tensor = onnx::Constant[value=&lt;Tensor&gt;]()\n</code></pre>\n"
        raw: "Hi, @qhduan, first of all, thanks for your great work, it is really\
          \ helpful for community for Chinses LLM.\nAs I see in `modeling_aquila.py`,\
          \ this model can re-use most of structure of LlaMA,  and Optimum supports\
          \  LlaMA ONNX export. \nSo I save Aquilachat-7b model locally and try to\
          \ export ONNX model as follow:\n```\noptimum-cli export onnx --model aquilachat-7b\
          \ \\\n    --task text-generation --trust-remote-code \\\n    --framework\
          \ pt --opset 17 onnx\n```\nHere I met following issue during shape inference:\n\
          ```\n~/anaconda3/envs/aigc/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py\n\
          309 in _create_node\n_C._jit_pass_onnx_node_shape_type_inference(node, params_dict,\
          \ opset_version)\nRuntimeError: ScalarType ComplexFloat is an unexpected\
          \ tensor scalar type \n```\nI checked that node:  %443 indeed a Tensor with\
          \ ComplexFloat type in self_attn.\nSince Complex types is a known limitation\
          \ for the ONNX exporter: https://github.com/pytorch/pytorch/issues/59246,\
          \ could you please share any workaround to export ONNX model? \n```\nnode:\
          \  %442 : Tensor = onnx::Transpose[perm=[0, 2, 1, 3]](%441), scope: transformers_modules.aquilachat-7b.modeling_aquila.LlamaForCausalLM::/transformers_modules.aquilachat-7b.modeling_aquila.LlamaModel::model/transformers_modules.aquilachat-7b.modeling_aquila.LlamaDecoderLayer::layers.0/transformers_modules.aquilachat-7b.modeling_aquila.LlamaAttention::self_attn\n\
          \nvalue_t:  tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n\
          \          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n          1.0000+0.0000e+00j],\n\
          \        [ 0.5403+8.4147e-01j,  0.6479+7.6172e-01j,  0.7318+6.8156e-01j,\n\
          \          ...,  1.0000+1.5399e-04j,  1.0000+1.3335e-04j,\n          1.0000+1.1548e-04j],\n\
          \        [-0.4161+9.0930e-01j, -0.1604+9.8705e-01j,  0.0709+9.9748e-01j,\n\
          \          ...,  1.0000+3.0799e-04j,  1.0000+2.6670e-04j,\n          1.0000+2.3096e-04j],\n\
          \        ...,\n        [-0.8799+4.7523e-01j,  0.7803+6.2535e-01j, -0.9998+1.9127e-02j,\n\
          \          ...,  0.8079+5.8938e-01j,  0.8547+5.1911e-01j,\n          0.8904+4.5525e-01j],\n\
          \        [-0.8753-4.8361e-01j,  0.0292+9.9957e-01j, -0.7446-6.6752e-01j,\n\
          \          ...,  0.8078+5.8951e-01j,  0.8546+5.1922e-01j,\n          0.8903+4.5535e-01j],\n\
          \        [-0.0660-9.9782e-01j, -0.7424+6.6991e-01j, -0.0900-9.9594e-01j,\n\
          \          ...,  0.8077+5.8963e-01j,  0.8546+5.1934e-01j,\n          0.8903+4.5545e-01j]])\n\
          node:  %443 : Tensor = onnx::Constant[value=<Tensor>]()\n```"
        updatedAt: '2023-06-15T12:18:28.645Z'
      numEdits: 2
      reactions: []
    id: 648b01342d8d81db5329bb6b
    type: comment
  author: sammysun0711
  content: "Hi, @qhduan, first of all, thanks for your great work, it is really helpful\
    \ for community for Chinses LLM.\nAs I see in `modeling_aquila.py`, this model\
    \ can re-use most of structure of LlaMA,  and Optimum supports  LlaMA ONNX export.\
    \ \nSo I save Aquilachat-7b model locally and try to export ONNX model as follow:\n\
    ```\noptimum-cli export onnx --model aquilachat-7b \\\n    --task text-generation\
    \ --trust-remote-code \\\n    --framework pt --opset 17 onnx\n```\nHere I met\
    \ following issue during shape inference:\n```\n~/anaconda3/envs/aigc/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py\n\
    309 in _create_node\n_C._jit_pass_onnx_node_shape_type_inference(node, params_dict,\
    \ opset_version)\nRuntimeError: ScalarType ComplexFloat is an unexpected tensor\
    \ scalar type \n```\nI checked that node:  %443 indeed a Tensor with ComplexFloat\
    \ type in self_attn.\nSince Complex types is a known limitation for the ONNX exporter:\
    \ https://github.com/pytorch/pytorch/issues/59246, could you please share any\
    \ workaround to export ONNX model? \n```\nnode:  %442 : Tensor = onnx::Transpose[perm=[0,\
    \ 2, 1, 3]](%441), scope: transformers_modules.aquilachat-7b.modeling_aquila.LlamaForCausalLM::/transformers_modules.aquilachat-7b.modeling_aquila.LlamaModel::model/transformers_modules.aquilachat-7b.modeling_aquila.LlamaDecoderLayer::layers.0/transformers_modules.aquilachat-7b.modeling_aquila.LlamaAttention::self_attn\n\
    \nvalue_t:  tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n\
    \          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n          1.0000+0.0000e+00j],\n\
    \        [ 0.5403+8.4147e-01j,  0.6479+7.6172e-01j,  0.7318+6.8156e-01j,\n   \
    \       ...,  1.0000+1.5399e-04j,  1.0000+1.3335e-04j,\n          1.0000+1.1548e-04j],\n\
    \        [-0.4161+9.0930e-01j, -0.1604+9.8705e-01j,  0.0709+9.9748e-01j,\n   \
    \       ...,  1.0000+3.0799e-04j,  1.0000+2.6670e-04j,\n          1.0000+2.3096e-04j],\n\
    \        ...,\n        [-0.8799+4.7523e-01j,  0.7803+6.2535e-01j, -0.9998+1.9127e-02j,\n\
    \          ...,  0.8079+5.8938e-01j,  0.8547+5.1911e-01j,\n          0.8904+4.5525e-01j],\n\
    \        [-0.8753-4.8361e-01j,  0.0292+9.9957e-01j, -0.7446-6.6752e-01j,\n   \
    \       ...,  0.8078+5.8951e-01j,  0.8546+5.1922e-01j,\n          0.8903+4.5535e-01j],\n\
    \        [-0.0660-9.9782e-01j, -0.7424+6.6991e-01j, -0.0900-9.9594e-01j,\n   \
    \       ...,  0.8077+5.8963e-01j,  0.8546+5.1934e-01j,\n          0.8903+4.5545e-01j]])\n\
    node:  %443 : Tensor = onnx::Constant[value=<Tensor>]()\n```"
  created_at: 2023-06-15 11:16:52+00:00
  edited: true
  hidden: false
  id: 648b01342d8d81db5329bb6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4263ef8ed7f7d4202dd9716651ca4773.svg
      fullname: Qinghua Duan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: qhduan
      type: user
    createdAt: '2023-06-15T14:24:33.000Z'
    data:
      edited: false
      editors:
      - qhduan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9501346349716187
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4263ef8ed7f7d4202dd9716651ca4773.svg
          fullname: Qinghua Duan
          isHf: false
          isPro: false
          name: qhduan
          type: user
        html: '<p>Aquila seems to use the META''s official RoPE implementation (but
          a little different in float16), HuggingFace transformers'' Llama re-implementation
          it, but it has some different with META''s.</p>

          <p>That''s why I replace RoPE code from transformers'' Llama to META''s,
          I really don''t have time to check what''s the different and fix it, maybe
          later, it would be great if you could help.</p>

          '
        raw: 'Aquila seems to use the META''s official RoPE implementation (but a
          little different in float16), HuggingFace transformers'' Llama re-implementation
          it, but it has some different with META''s.


          That''s why I replace RoPE code from transformers'' Llama to META''s, I
          really don''t have time to check what''s the different and fix it, maybe
          later, it would be great if you could help.'
        updatedAt: '2023-06-15T14:24:33.777Z'
      numEdits: 0
      reactions: []
    id: 648b1f215291bd6ec9955014
    type: comment
  author: qhduan
  content: 'Aquila seems to use the META''s official RoPE implementation (but a little
    different in float16), HuggingFace transformers'' Llama re-implementation it,
    but it has some different with META''s.


    That''s why I replace RoPE code from transformers'' Llama to META''s, I really
    don''t have time to check what''s the different and fix it, maybe later, it would
    be great if you could help.'
  created_at: 2023-06-15 13:24:33+00:00
  edited: false
  hidden: false
  id: 648b1f215291bd6ec9955014
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: qhduan/aquilachat-7b
repo_type: model
status: open
target_branch: null
title: Export Aquilachat-7b to ONNX via Optimum Failed
