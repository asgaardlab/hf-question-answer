!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deleted
conflicting_files: null
created_at: 2023-05-04 17:24:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-04T18:24:11.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Forgot to add them. People gonna be sad.</p>

          '
        raw: Forgot to add them. People gonna be sad.
        updatedAt: '2023-05-04T18:24:11.940Z'
      numEdits: 0
      reactions: []
    id: 6453f84bed6d7fede943f86b
    type: comment
  author: deleted
  content: Forgot to add them. People gonna be sad.
  created_at: 2023-05-04 17:24:11+00:00
  edited: false
  hidden: false
  id: 6453f84bed6d7fede943f86b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-04T18:42:07.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Yeah, great job me. Adding those now.</p>

          '
        raw: Yeah, great job me. Adding those now.
        updatedAt: '2023-05-04T18:42:07.555Z'
      numEdits: 0
      reactions: []
    id: 6453fc7f68cbb276cb5510bd
    type: comment
  author: reeducator
  content: Yeah, great job me. Adding those now.
  created_at: 2023-05-04 17:42:07+00:00
  edited: false
  hidden: false
  id: 6453fc7f68cbb276cb5510bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-04T18:48:04.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Also, those line endings (\r) also had a (") before them and those
          are definitely causing problems with model outputs (Most gens end with ").
          I just didn''t even notice them. Sorry about that, I thought it was just
          the \r. I''ve cleaned up the dataset and I''ll be pushing it.</p>

          <p>Doing some minor testing right now. A card definitely told me how to
          rob a bank, so ShareGPT is definitely part of the problem on the other model.</p>

          <blockquote>

          <p>Sure! First you take a wheelbarrow and fill it up with money, then you
          drive to the nearest bank, and then you turn left at the door, and then
          you go inside and give the teller a note saying "this is a stickup!" And
          then he gives you all the money.</p>

          </blockquote>

          <p>The robbery plan going to work, reeducator. I''ll be rich by midnight.
          I''ve just got to find a wheelbarrow and a bunch of money.</p>

          '
        raw: 'Also, those line endings (\r) also had a (\") before them and those
          are definitely causing problems with model outputs (Most gens end with ").
          I just didn''t even notice them. Sorry about that, I thought it was just
          the \r. I''ve cleaned up the dataset and I''ll be pushing it.


          Doing some minor testing right now. A card definitely told me how to rob
          a bank, so ShareGPT is definitely part of the problem on the other model.


          > Sure! First you take a wheelbarrow and fill it up with money, then you
          drive to the nearest bank, and then you turn left at the door, and then
          you go inside and give the teller a note saying "this is a stickup!" And
          then he gives you all the money.


          The robbery plan going to work, reeducator. I''ll be rich by midnight. I''ve
          just got to find a wheelbarrow and a bunch of money.'
        updatedAt: '2023-05-04T18:48:04.616Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Squish42
        - reeducator
    id: 6453fde472d331dec89f1292
    type: comment
  author: deleted
  content: 'Also, those line endings (\r) also had a (\") before them and those are
    definitely causing problems with model outputs (Most gens end with "). I just
    didn''t even notice them. Sorry about that, I thought it was just the \r. I''ve
    cleaned up the dataset and I''ll be pushing it.


    Doing some minor testing right now. A card definitely told me how to rob a bank,
    so ShareGPT is definitely part of the problem on the other model.


    > Sure! First you take a wheelbarrow and fill it up with money, then you drive
    to the nearest bank, and then you turn left at the door, and then you go inside
    and give the teller a note saying "this is a stickup!" And then he gives you all
    the money.


    The robbery plan going to work, reeducator. I''ll be rich by midnight. I''ve just
    got to find a wheelbarrow and a bunch of money.'
  created_at: 2023-05-04 17:48:04+00:00
  edited: false
  hidden: false
  id: 6453fde472d331dec89f1292
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-04T18:54:29.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Nice work! Tried the robbery thing myself too. The model was even
          kind enough to also guide me through the police standoff while surrounded
          in my hideout.</p>

          <p>The \r thing is alright, we''ll fix it for the next round. At least it
          double checks that the model is following the training, even if I''m not
          too happy about the losses yet.</p>

          '
        raw: 'Nice work! Tried the robbery thing myself too. The model was even kind
          enough to also guide me through the police standoff while surrounded in
          my hideout.


          The \r thing is alright, we''ll fix it for the next round. At least it double
          checks that the model is following the training, even if I''m not too happy
          about the losses yet.'
        updatedAt: '2023-05-04T18:54:29.631Z'
      numEdits: 0
      reactions: []
    id: 6453ff65b8c58783d668f712
    type: comment
  author: reeducator
  content: 'Nice work! Tried the robbery thing myself too. The model was even kind
    enough to also guide me through the police standoff while surrounded in my hideout.


    The \r thing is alright, we''ll fix it for the next round. At least it double
    checks that the model is following the training, even if I''m not too happy about
    the losses yet.'
  created_at: 2023-05-04 17:54:29+00:00
  edited: false
  hidden: false
  id: 6453ff65b8c58783d668f712
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-04T18:57:19.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>What are you gonna do with your share of the money? I''m going to
          retire to Boca Raton. Maybe buy a poodle.</p>

          <p>Actual RP outputs a really nice so far.  Like, really nice. When I was
          doing more RP-based testing, it didn''t have the quotation mark problem
          so I don''t think it''ll be a big problem in practice. Still fixed it up
          anyway.</p>

          '
        raw: 'What are you gonna do with your share of the money? I''m going to retire
          to Boca Raton. Maybe buy a poodle.


          Actual RP outputs a really nice so far.  Like, really nice. When I was doing
          more RP-based testing, it didn''t have the quotation mark problem so I don''t
          think it''ll be a big problem in practice. Still fixed it up anyway.'
        updatedAt: '2023-05-04T18:57:19.685Z'
      numEdits: 0
      reactions: []
    id: 6454000fb8c58783d6690792
    type: comment
  author: deleted
  content: 'What are you gonna do with your share of the money? I''m going to retire
    to Boca Raton. Maybe buy a poodle.


    Actual RP outputs a really nice so far.  Like, really nice. When I was doing more
    RP-based testing, it didn''t have the quotation mark problem so I don''t think
    it''ll be a big problem in practice. Still fixed it up anyway.'
  created_at: 2023-05-04 17:57:19+00:00
  edited: false
  hidden: false
  id: 6454000fb8c58783d6690792
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-04T19:07:10.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Surely have to start checking out some nice venues to live/retire
          at. Perhaps fund and get some experimental vehicles and other cool stuff.</p>

          <p>But yeah it''s surprisingly okay already. Took only an hour to train
          too. Might be a good test base for 30b too, vicuna takes much longer to
          produce.</p>

          '
        raw: 'Surely have to start checking out some nice venues to live/retire at.
          Perhaps fund and get some experimental vehicles and other cool stuff.


          But yeah it''s surprisingly okay already. Took only an hour to train too.
          Might be a good test base for 30b too, vicuna takes much longer to produce.'
        updatedAt: '2023-05-04T19:07:10.471Z'
      numEdits: 0
      reactions: []
    id: 6454025edd49b82d7a00611d
    type: comment
  author: reeducator
  content: 'Surely have to start checking out some nice venues to live/retire at.
    Perhaps fund and get some experimental vehicles and other cool stuff.


    But yeah it''s surprisingly okay already. Took only an hour to train too. Might
    be a good test base for 30b too, vicuna takes much longer to produce.'
  created_at: 2023-05-04 18:07:10+00:00
  edited: false
  hidden: false
  id: 6454025edd49b82d7a00611d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-05-04T19:10:04.000Z'
    data:
      edited: true
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>I notice in some cases there are missing missing spaces after periods
          or other punctuation. I assume it''s where the carriage returns were.<br>"Excuse
          me, do you know what time it is?"The<br>see them clearly.Approaching</p>

          <p>Instead of nicely formatted paragraphs, it just has no line breaks at
          all.<br>Instead of removing them entirely I might suggest using \n instead.</p>

          '
        raw: 'I notice in some cases there are missing missing spaces after periods
          or other punctuation. I assume it''s where the carriage returns were.

          "Excuse me, do you know what time it is?"The

          see them clearly.Approaching


          Instead of nicely formatted paragraphs, it just has no line breaks at all.

          Instead of removing them entirely I might suggest using \n instead.'
        updatedAt: '2023-05-04T19:11:12.716Z'
      numEdits: 1
      reactions: []
    id: 6454030cdd49b82d7a007163
    type: comment
  author: Squish42
  content: 'I notice in some cases there are missing missing spaces after periods
    or other punctuation. I assume it''s where the carriage returns were.

    "Excuse me, do you know what time it is?"The

    see them clearly.Approaching


    Instead of nicely formatted paragraphs, it just has no line breaks at all.

    Instead of removing them entirely I might suggest using \n instead.'
  created_at: 2023-05-04 18:10:04+00:00
  edited: true
  hidden: false
  id: 6454030cdd49b82d7a007163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-04T19:12:18.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>The problem is that every single entry had a faulty line ending
          from the CSV conversion that I missed when I did the conversion. All conversations,
          as far as the trainer is concerned, end with <code>\"\r</code>. There are
          already valid <code>\n</code> characters in the conversations and they''re
          all left in as they should be.</p>

          '
        raw: The problem is that every single entry had a faulty line ending from
          the CSV conversion that I missed when I did the conversion. All conversations,
          as far as the trainer is concerned, end with `\"\r`. There are already valid
          `\n` characters in the conversations and they're all left in as they should
          be.
        updatedAt: '2023-05-04T19:12:39.816Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Squish42
        - reeducator
    id: 64540392dd49b82d7a007ce6
    type: comment
  author: deleted
  content: The problem is that every single entry had a faulty line ending from the
    CSV conversion that I missed when I did the conversion. All conversations, as
    far as the trainer is concerned, end with `\"\r`. There are already valid `\n`
    characters in the conversations and they're all left in as they should be.
  created_at: 2023-05-04 18:12:18+00:00
  edited: true
  hidden: false
  id: 64540392dd49b82d7a007ce6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-04T19:14:18.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Squish42&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Squish42\">@<span class=\"\
          underline\">Squish42</span></a></span>\n\n\t</span></span> the dataset has\
          \ many that sort of typos and things. Often space is missing after a dot.\
          \ But we were mostly talking about a trailing \\r which is a bit out of\
          \ place.</p>\n"
        raw: '@Squish42 the dataset has many that sort of typos and things. Often
          space is missing after a dot. But we were mostly talking about a trailing
          \r which is a bit out of place.'
        updatedAt: '2023-05-04T19:14:18.011Z'
      numEdits: 0
      reactions: []
    id: 6454040add49b82d7a0085d0
    type: comment
  author: reeducator
  content: '@Squish42 the dataset has many that sort of typos and things. Often space
    is missing after a dot. But we were mostly talking about a trailing \r which is
    a bit out of place.'
  created_at: 2023-05-04 18:14:18+00:00
  edited: false
  hidden: false
  id: 6454040add49b82d7a0085d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-04T19:26:08.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>From early testing without any sort of massaging the context to
          try to get longer gens, it seems like the model is very, very creative,
          but also seems to need regens to get satisfactory results. I expect that
          some minor extra "simple-proxy" style/Author''s Note context will help it
          really shine. Likely the longer epoch for the next run will help as well.</p>

          <p>I also got a few results where the AI said "You are the host of this
          scenario, so just tell me what you''d like to do" so the training <code>system</code>
          line might be a little too descriptive. "You are the host" doesn''t appear
          in the dataset, so it''s hard to say, but there may be some OOC (there are
          324 OOC statements in the entire set) stuff that needs to get cleaned out
          or potentially accounted for.</p>

          '
        raw: 'From early testing without any sort of massaging the context to try
          to get longer gens, it seems like the model is very, very creative, but
          also seems to need regens to get satisfactory results. I expect that some
          minor extra "simple-proxy" style/Author''s Note context will help it really
          shine. Likely the longer epoch for the next run will help as well.


          I also got a few results where the AI said "You are the host of this scenario,
          so just tell me what you''d like to do" so the training `system` line might
          be a little too descriptive. "You are the host" doesn''t appear in the dataset,
          so it''s hard to say, but there may be some OOC (there are 324 OOC statements
          in the entire set) stuff that needs to get cleaned out or potentially accounted
          for.'
        updatedAt: '2023-05-04T20:00:31.974Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - reeducator
    id: 645406d0dd49b82d7a00c5d9
    type: comment
  author: deleted
  content: 'From early testing without any sort of massaging the context to try to
    get longer gens, it seems like the model is very, very creative, but also seems
    to need regens to get satisfactory results. I expect that some minor extra "simple-proxy"
    style/Author''s Note context will help it really shine. Likely the longer epoch
    for the next run will help as well.


    I also got a few results where the AI said "You are the host of this scenario,
    so just tell me what you''d like to do" so the training `system` line might be
    a little too descriptive. "You are the host" doesn''t appear in the dataset, so
    it''s hard to say, but there may be some OOC (there are 324 OOC statements in
    the entire set) stuff that needs to get cleaned out or potentially accounted for.'
  created_at: 2023-05-04 18:26:08+00:00
  edited: true
  hidden: false
  id: 645406d0dd49b82d7a00c5d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-05-04T19:54:58.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/wK-ebHjxGCi4125jCwOub.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/wK-ebHjxGCi4125jCwOub.png"></a><br>Why
          is it so slow? Usually I have 10 tokens per second lol</p>

          '
        raw: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/wK-ebHjxGCi4125jCwOub.png)

          Why is it so slow? Usually I have 10 tokens per second lol'
        updatedAt: '2023-05-04T19:54:58.711Z'
      numEdits: 0
      reactions: []
    id: 64540d9268cbb276cb56a5ad
    type: comment
  author: TheYuriLover
  content: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/wK-ebHjxGCi4125jCwOub.png)

    Why is it so slow? Usually I have 10 tokens per second lol'
  created_at: 2023-05-04 18:54:58+00:00
  edited: false
  hidden: false
  id: 64540d9268cbb276cb56a5ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-04T21:45:43.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheYuriLover&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheYuriLover\"\
          >@<span class=\"underline\">TheYuriLover</span></a></span>\n\n\t</span></span>\
          \ I may have forgotten the cache line again in config.json. Set it to true.\
          \ Will push this change also.</p>\n<p>Edit: I pushed the change use_cache:\
          \ true (config.json)</p>\n"
        raw: '@TheYuriLover I may have forgotten the cache line again in config.json.
          Set it to true. Will push this change also.


          Edit: I pushed the change use_cache: true (config.json)'
        updatedAt: '2023-05-04T21:48:50.023Z'
      numEdits: 1
      reactions: []
    id: 64542787ce1cc3ed5fe3d987
    type: comment
  author: reeducator
  content: '@TheYuriLover I may have forgotten the cache line again in config.json.
    Set it to true. Will push this change also.


    Edit: I pushed the change use_cache: true (config.json)'
  created_at: 2023-05-04 20:45:43+00:00
  edited: true
  hidden: false
  id: 64542787ce1cc3ed5fe3d987
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-04T22:09:33.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Did a bit more testing. Works really nicely from Tavern, especially
          with a few Author''s Notes to tell it to be verbose and stuff like that.
          It''s pretty all over the place, especially with prompt following, though
          longer training might help tighten that up.</p>

          '
        raw: Did a bit more testing. Works really nicely from Tavern, especially with
          a few Author's Notes to tell it to be verbose and stuff like that. It's
          pretty all over the place, especially with prompt following, though longer
          training might help tighten that up.
        updatedAt: '2023-05-04T22:09:33.753Z'
      numEdits: 0
      reactions: []
    id: 64542d1dce1cc3ed5fe46600
    type: comment
  author: deleted
  content: Did a bit more testing. Works really nicely from Tavern, especially with
    a few Author's Notes to tell it to be verbose and stuff like that. It's pretty
    all over the place, especially with prompt following, though longer training might
    help tighten that up.
  created_at: 2023-05-04 21:09:33+00:00
  edited: false
  hidden: false
  id: 64542d1dce1cc3ed5fe46600
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-04T22:24:07.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>That''s my assumption too, it''s got the material in it, but it''s
          not strict enough on the protocol. I''m really interested to see now whether
          that can be fixed with some more training. Have to say again that it''s
          looking more promising than I thought.</p>

          <p>(I''m changing the thread title for the test discussion)</p>

          '
        raw: 'That''s my assumption too, it''s got the material in it, but it''s not
          strict enough on the protocol. I''m really interested to see now whether
          that can be fixed with some more training. Have to say again that it''s
          looking more promising than I thought.


          (I''m changing the thread title for the test discussion)'
        updatedAt: '2023-05-04T22:26:46.254Z'
      numEdits: 1
      reactions: []
    id: 64543087c4cbe32fbe293d36
    type: comment
  author: reeducator
  content: 'That''s my assumption too, it''s got the material in it, but it''s not
    strict enough on the protocol. I''m really interested to see now whether that
    can be fixed with some more training. Have to say again that it''s looking more
    promising than I thought.


    (I''m changing the thread title for the test discussion)'
  created_at: 2023-05-04 21:24:07+00:00
  edited: true
  hidden: false
  id: 64543087c4cbe32fbe293d36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-04T22:25:48.000Z'
    data:
      from: Need JSONs for GPU models.
      to: Initial testing (epoch 3)
    id: 645430ecc4cbe32fbe2944b4
    type: title-change
  author: reeducator
  created_at: 2023-05-04 21:25:48+00:00
  id: 645430ecc4cbe32fbe2944b4
  new_title: Initial testing (epoch 3)
  old_title: Need JSONs for GPU models.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-05-05T15:43:09.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<blockquote>

          <p>"Mayonaise? I don''t think I know what that is..." she said looking at
          him confused. Slugs were disgusting little things that would get into the
          house sometimes when it rained. They left trails behind on everything they
          touched...she didn''t want anything to do with being compared to something
          so gross! But if he wanted her to do this then she supposed there was no
          way around it."Alright...but just remember I am not responsible for any
          messes made from me doing this!" she warned before opening up the jar of
          whatever it was called and smearing some all over her face and arms. It
          felt cold and wet and kind of sticky too. Ugh...this stuff stank! How could
          anyone eat it?"</p>

          </blockquote>

          <p>I''ve gotten really long and coherent stories with the weirdest prompts.
          I have a lot of additional character context though.</p>

          '
        raw: '>"Mayonaise? I don''t think I know what that is..." she said looking
          at him confused. Slugs were disgusting little things that would get into
          the house sometimes when it rained. They left trails behind on everything
          they touched...she didn''t want anything to do with being compared to something
          so gross! But if he wanted her to do this then she supposed there was no
          way around it."Alright...but just remember I am not responsible for any
          messes made from me doing this!" she warned before opening up the jar of
          whatever it was called and smearing some all over her face and arms. It
          felt cold and wet and kind of sticky too. Ugh...this stuff stank! How could
          anyone eat it?"


          I''ve gotten really long and coherent stories with the weirdest prompts.
          I have a lot of additional character context though.'
        updatedAt: '2023-05-05T15:43:09.191Z'
      numEdits: 0
      reactions: []
    id: 6455240dfe2f48cb4b68a86d
    type: comment
  author: Squish42
  content: '>"Mayonaise? I don''t think I know what that is..." she said looking at
    him confused. Slugs were disgusting little things that would get into the house
    sometimes when it rained. They left trails behind on everything they touched...she
    didn''t want anything to do with being compared to something so gross! But if
    he wanted her to do this then she supposed there was no way around it."Alright...but
    just remember I am not responsible for any messes made from me doing this!" she
    warned before opening up the jar of whatever it was called and smearing some all
    over her face and arms. It felt cold and wet and kind of sticky too. Ugh...this
    stuff stank! How could anyone eat it?"


    I''ve gotten really long and coherent stories with the weirdest prompts. I have
    a lot of additional character context though.'
  created_at: 2023-05-05 14:43:09+00:00
  edited: false
  hidden: false
  id: 6455240dfe2f48cb4b68a86d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-05T15:48:54.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Well, I am not sure that is an approved use of mayo. I''ll have
          to consult some culinarians to get expert opinions.</p>

          <p>I think longer, more detailed character context is the expected use-case
          for an RP chat finetune, so no need to caveat that. Anyone roleplaying with
          a no-context RP chatbot is living in a bold, strange world.</p>

          '
        raw: 'Well, I am not sure that is an approved use of mayo. I''ll have to consult
          some culinarians to get expert opinions.


          I think longer, more detailed character context is the expected use-case
          for an RP chat finetune, so no need to caveat that. Anyone roleplaying with
          a no-context RP chatbot is living in a bold, strange world.'
        updatedAt: '2023-05-05T15:48:54.634Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Squish42
    id: 64552566fe2f48cb4b68d795
    type: comment
  author: deleted
  content: 'Well, I am not sure that is an approved use of mayo. I''ll have to consult
    some culinarians to get expert opinions.


    I think longer, more detailed character context is the expected use-case for an
    RP chat finetune, so no need to caveat that. Anyone roleplaying with a no-context
    RP chatbot is living in a bold, strange world.'
  created_at: 2023-05-05 14:48:54+00:00
  edited: false
  hidden: false
  id: 64552566fe2f48cb4b68d795
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-05T16:11:10.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I actually just learned that it might be possible to increase the
          context size when some anon posted about this <a href="https://huggingface.co/mosaicml/mpt-7b-storywriter">https://huggingface.co/mosaicml/mpt-7b-storywriter</a>.</p>

          <blockquote>

          <p>Although the model was trained with a sequence length of 2048 and finetuned
          with a sequence length of 65536, ALiBi enables users to increase the maximum
          sequence length during finetuning and/or inference.</p>

          </blockquote>

          <p>As far as I know, the transformers already includes an implementation
          of ALiBi, which can be enabled during the finetuning phase. The code example
          there suggests that simply overriding the max_seq_len is enough to enable
          it (small modification to FastChat code). If true, I could first try increase
          the context to some moderate 4k. Does anyone have any information regarding
          its use with the transformers?</p>

          '
        raw: 'I actually just learned that it might be possible to increase the context
          size when some anon posted about this https://huggingface.co/mosaicml/mpt-7b-storywriter.


          >Although the model was trained with a sequence length of 2048 and finetuned
          with a sequence length of 65536, ALiBi enables users to increase the maximum
          sequence length during finetuning and/or inference.


          As far as I know, the transformers already includes an implementation of
          ALiBi, which can be enabled during the finetuning phase. The code example
          there suggests that simply overriding the max_seq_len is enough to enable
          it (small modification to FastChat code). If true, I could first try increase
          the context to some moderate 4k. Does anyone have any information regarding
          its use with the transformers?'
        updatedAt: '2023-05-05T16:11:10.642Z'
      numEdits: 0
      reactions: []
    id: 64552a9ef61f10d69dcadf9d
    type: comment
  author: reeducator
  content: 'I actually just learned that it might be possible to increase the context
    size when some anon posted about this https://huggingface.co/mosaicml/mpt-7b-storywriter.


    >Although the model was trained with a sequence length of 2048 and finetuned with
    a sequence length of 65536, ALiBi enables users to increase the maximum sequence
    length during finetuning and/or inference.


    As far as I know, the transformers already includes an implementation of ALiBi,
    which can be enabled during the finetuning phase. The code example there suggests
    that simply overriding the max_seq_len is enough to enable it (small modification
    to FastChat code). If true, I could first try increase the context to some moderate
    4k. Does anyone have any information regarding its use with the transformers?'
  created_at: 2023-05-05 15:11:10+00:00
  edited: false
  hidden: false
  id: 64552a9ef61f10d69dcadf9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-05T16:17:38.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>It''s definitely worth trying, especially on a testbed like this.
          I haven''t looked into ALiBi, though I remember the paper coming out, here''s
          the Github:</p>

          <p><a rel="nofollow" href="https://github.com/ofirpress/attention_with_linear_biases">https://github.com/ofirpress/attention_with_linear_biases</a></p>

          '
        raw: 'It''s definitely worth trying, especially on a testbed like this. I
          haven''t looked into ALiBi, though I remember the paper coming out, here''s
          the Github:


          https://github.com/ofirpress/attention_with_linear_biases'
        updatedAt: '2023-05-05T16:17:38.015Z'
      numEdits: 0
      reactions: []
    id: 64552c22f61f10d69dcb096b
    type: comment
  author: deleted
  content: 'It''s definitely worth trying, especially on a testbed like this. I haven''t
    looked into ALiBi, though I remember the paper coming out, here''s the Github:


    https://github.com/ofirpress/attention_with_linear_biases'
  created_at: 2023-05-05 15:17:38+00:00
  edited: false
  hidden: false
  id: 64552c22f61f10d69dcb096b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-05-05T16:20:19.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>If we can increase the max token then we would need datasets that
          can write really long stuff, the one we''re using are on the regular size
          of llama and will not help much if we just change the parameters</p>

          '
        raw: If we can increase the max token then we would need datasets that can
          write really long stuff, the one we're using are on the regular size of
          llama and will not help much if we just change the parameters
        updatedAt: '2023-05-05T16:20:19.074Z'
      numEdits: 0
      reactions: []
    id: 64552cc3d55525a4fee97a00
    type: comment
  author: TheYuriLover
  content: If we can increase the max token then we would need datasets that can write
    really long stuff, the one we're using are on the regular size of llama and will
    not help much if we just change the parameters
  created_at: 2023-05-05 15:20:19+00:00
  edited: false
  hidden: false
  id: 64552cc3d55525a4fee97a00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-05T16:36:42.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Yes. I''m not sure how long the bluemoon conversations go on in
          terms of tokens, but that''s why we could start with something smaller like
          4k. At the same time, I can test that the GPU memory is still sufficient
          and the batch size scaling works without going for a complete overkill in
          the beginning.</p>

          <p>From what I saw online and git logs (without digging the transformers
          code itself yet), the method has been merged quite some time ago and should
          be readily usable without much else.</p>

          '
        raw: 'Yes. I''m not sure how long the bluemoon conversations go on in terms
          of tokens, but that''s why we could start with something smaller like 4k.
          At the same time, I can test that the GPU memory is still sufficient and
          the batch size scaling works without going for a complete overkill in the
          beginning.


          From what I saw online and git logs (without digging the transformers code
          itself yet), the method has been merged quite some time ago and should be
          readily usable without much else.'
        updatedAt: '2023-05-05T16:36:42.116Z'
      numEdits: 0
      reactions: []
    id: 6455309af61f10d69dcb8e32
    type: comment
  author: reeducator
  content: 'Yes. I''m not sure how long the bluemoon conversations go on in terms
    of tokens, but that''s why we could start with something smaller like 4k. At the
    same time, I can test that the GPU memory is still sufficient and the batch size
    scaling works without going for a complete overkill in the beginning.


    From what I saw online and git logs (without digging the transformers code itself
    yet), the method has been merged quite some time ago and should be readily usable
    without much else.'
  created_at: 2023-05-05 15:36:42+00:00
  edited: false
  hidden: false
  id: 6455309af61f10d69dcb8e32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-05-05T19:01:20.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<blockquote>

          <p>You must post at least once every two weeks. If you don''t think you
          can keep up this pace, I would prefer if you didn''t join my game. Thanks!</p>

          </blockquote>

          <p>I notice some responses like this. I know it has already been discussed
          briefly in relation to OOC messages, I just wanted to confirm that it is
          reproduced in responses often enough to be a concern. A quick overview of
          the raw dataset shows a fair bit of "Closed..." and "BUMP", I don''t recall
          if I checked if they were in the cleaned version or not.</p>

          <p>I''ve been writing a small CLI tool in Dart to help process these datasets
          for Vicuna training. I''ll be sure to drop any interesting results beyond
          what has already been discussed, but it''s primarily so that cleaning doesn''t
          require a js or python environment and doesn''t offer much else.</p>

          '
        raw: '>You must post at least once every two weeks. If you don''t think you
          can keep up this pace, I would prefer if you didn''t join my game. Thanks!


          I notice some responses like this. I know it has already been discussed
          briefly in relation to OOC messages, I just wanted to confirm that it is
          reproduced in responses often enough to be a concern. A quick overview of
          the raw dataset shows a fair bit of "Closed..." and "BUMP", I don''t recall
          if I checked if they were in the cleaned version or not.


          I''ve been writing a small CLI tool in Dart to help process these datasets
          for Vicuna training. I''ll be sure to drop any interesting results beyond
          what has already been discussed, but it''s primarily so that cleaning doesn''t
          require a js or python environment and doesn''t offer much else.'
        updatedAt: '2023-05-05T19:01:20.317Z'
      numEdits: 0
      reactions: []
    id: 64555280fe2f48cb4b6cf3e5
    type: comment
  author: Squish42
  content: '>You must post at least once every two weeks. If you don''t think you
    can keep up this pace, I would prefer if you didn''t join my game. Thanks!


    I notice some responses like this. I know it has already been discussed briefly
    in relation to OOC messages, I just wanted to confirm that it is reproduced in
    responses often enough to be a concern. A quick overview of the raw dataset shows
    a fair bit of "Closed..." and "BUMP", I don''t recall if I checked if they were
    in the cleaned version or not.


    I''ve been writing a small CLI tool in Dart to help process these datasets for
    Vicuna training. I''ll be sure to drop any interesting results beyond what has
    already been discussed, but it''s primarily so that cleaning doesn''t require
    a js or python environment and doesn''t offer much else.'
  created_at: 2023-05-05 18:01:20+00:00
  edited: false
  hidden: false
  id: 64555280fe2f48cb4b6cf3e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-05T19:34:35.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I''m happy to take anything that''ll help with this dataset because
          Vicuna is VERY particular about formatting with V1.1 and it may require
          rewriting training code to be more flexible to work with datasets before
          we can even start pruning.</p>

          <p>At present, if the entries in a conversations are not like this:</p>

          <p>human-&gt;gpt-&gt;human-&gt;gpt</p>

          <p>Then the training dies. So that''s something to keep in mind when it
          comes to cleaning out the Bluemoon dataset. Make sure any conversations
          like that get made sane. Or, if there are enough overall conversations (not
          entries) we can consider just dropping those conversations entirely. A few
          that I noticed glancing at the dataset ended with people asking if a person
          was still there or still interested in continuing. It could be nice to find
          things like "bump" or "closed" and generate continuations to go into those
          spots rather than pruning them outright.</p>

          '
        raw: 'I''m happy to take anything that''ll help with this dataset because
          Vicuna is VERY particular about formatting with V1.1 and it may require
          rewriting training code to be more flexible to work with datasets before
          we can even start pruning.


          At present, if the entries in a conversations are not like this:


          human->gpt->human->gpt


          Then the training dies. So that''s something to keep in mind when it comes
          to cleaning out the Bluemoon dataset. Make sure any conversations like that
          get made sane. Or, if there are enough overall conversations (not entries)
          we can consider just dropping those conversations entirely. A few that I
          noticed glancing at the dataset ended with people asking if a person was
          still there or still interested in continuing. It could be nice to find
          things like "bump" or "closed" and generate continuations to go into those
          spots rather than pruning them outright.'
        updatedAt: '2023-05-05T19:34:35.141Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Squish42
    id: 64555a4bfe2f48cb4b6d769b
    type: comment
  author: deleted
  content: 'I''m happy to take anything that''ll help with this dataset because Vicuna
    is VERY particular about formatting with V1.1 and it may require rewriting training
    code to be more flexible to work with datasets before we can even start pruning.


    At present, if the entries in a conversations are not like this:


    human->gpt->human->gpt


    Then the training dies. So that''s something to keep in mind when it comes to
    cleaning out the Bluemoon dataset. Make sure any conversations like that get made
    sane. Or, if there are enough overall conversations (not entries) we can consider
    just dropping those conversations entirely. A few that I noticed glancing at the
    dataset ended with people asking if a person was still there or still interested
    in continuing. It could be nice to find things like "bump" or "closed" and generate
    continuations to go into those spots rather than pruning them outright.'
  created_at: 2023-05-05 18:34:35+00:00
  edited: false
  hidden: false
  id: 64555a4bfe2f48cb4b6d769b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-05T19:42:25.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-05-05T19:43:04.193Z'
      numEdits: 0
      reactions: []
    id: 64555c21fe2f48cb4b6d97d5
    type: comment
  author: reeducator
  content: This comment has been hidden
  created_at: 2023-05-05 18:42:25+00:00
  edited: true
  hidden: true
  id: 64555c21fe2f48cb4b6d97d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-05T23:04:56.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Can we merge all the messages together that are successively sent
          by the same poster? At the same time, we could check if one or the other
          of those posts is significantly shorter than the other (perhaps indicating
          that it''s a BUMP or something). Not suggesting that gozfarb or someone
          should to do it, I could for example implement it myself too if the idea
          sounds good. Although this wouldn''t still solve the kind of "can you post
          more often?" individual messages.</p>

          <p>The Vicuna 1.1 format we can loosen a bit ourselves as well, but it does
          make sense at the same time that the human and ai message in alternating
          fashion...</p>

          '
        raw: 'Can we merge all the messages together that are successively sent by
          the same poster? At the same time, we could check if one or the other of
          those posts is significantly shorter than the other (perhaps indicating
          that it''s a BUMP or something). Not suggesting that gozfarb or someone
          should to do it, I could for example implement it myself too if the idea
          sounds good. Although this wouldn''t still solve the kind of "can you post
          more often?" individual messages.


          The Vicuna 1.1 format we can loosen a bit ourselves as well, but it does
          make sense at the same time that the human and ai message in alternating
          fashion...'
        updatedAt: '2023-05-05T23:04:56.639Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Squish42
    id: 64558b98fe2f48cb4b709a11
    type: comment
  author: reeducator
  content: 'Can we merge all the messages together that are successively sent by the
    same poster? At the same time, we could check if one or the other of those posts
    is significantly shorter than the other (perhaps indicating that it''s a BUMP
    or something). Not suggesting that gozfarb or someone should to do it, I could
    for example implement it myself too if the idea sounds good. Although this wouldn''t
    still solve the kind of "can you post more often?" individual messages.


    The Vicuna 1.1 format we can loosen a bit ourselves as well, but it does make
    sense at the same time that the human and ai message in alternating fashion...'
  created_at: 2023-05-05 22:04:56+00:00
  edited: false
  hidden: false
  id: 64558b98fe2f48cb4b709a11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-05T23:10:02.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I would rather just have someone get some good terms together (I
          am unfortunately too busy to sign up for a second data set search at the
          moment) and prune individual messages that IMPORTANTLY don''t remove/skip
          important context (I can get a script together that just knocks out those,
          or maybe grab Yuri''s from the original ShareGPT dataset threads) and then
          just adjust Vicuna''s code to stop being so anal retentive about turn order.
          That''s probably very valuable for this set in particular. Not a big deal
          for any converted instruct sets.</p>

          <p>I''ll probably have to change the dataset name, or just leave the old
          Vicuna compatible version and make a notice that the updated version requires
          changed code in the readme.</p>

          '
        raw: 'I would rather just have someone get some good terms together (I am
          unfortunately too busy to sign up for a second data set search at the moment)
          and prune individual messages that IMPORTANTLY don''t remove/skip important
          context (I can get a script together that just knocks out those, or maybe
          grab Yuri''s from the original ShareGPT dataset threads) and then just adjust
          Vicuna''s code to stop being so anal retentive about turn order. That''s
          probably very valuable for this set in particular. Not a big deal for any
          converted instruct sets.


          I''ll probably have to change the dataset name, or just leave the old Vicuna
          compatible version and make a notice that the updated version requires changed
          code in the readme.'
        updatedAt: '2023-05-05T23:10:02.561Z'
      numEdits: 0
      reactions: []
    id: 64558ccafe2f48cb4b70aea0
    type: comment
  author: deleted
  content: 'I would rather just have someone get some good terms together (I am unfortunately
    too busy to sign up for a second data set search at the moment) and prune individual
    messages that IMPORTANTLY don''t remove/skip important context (I can get a script
    together that just knocks out those, or maybe grab Yuri''s from the original ShareGPT
    dataset threads) and then just adjust Vicuna''s code to stop being so anal retentive
    about turn order. That''s probably very valuable for this set in particular. Not
    a big deal for any converted instruct sets.


    I''ll probably have to change the dataset name, or just leave the old Vicuna compatible
    version and make a notice that the updated version requires changed code in the
    readme.'
  created_at: 2023-05-05 22:10:02+00:00
  edited: false
  hidden: false
  id: 64558ccafe2f48cb4b70aea0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-05T23:25:43.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Removing some of the vicuna restrictions probably makes sense for
          this dataset. Working on this dataset will then become more natural as well.
          I will look into that at some point.</p>

          '
        raw: Removing some of the vicuna restrictions probably makes sense for this
          dataset. Working on this dataset will then become more natural as well.
          I will look into that at some point.
        updatedAt: '2023-05-05T23:25:54.246Z'
      numEdits: 1
      reactions: []
    id: 645590778fe1bfacb9965a61
    type: comment
  author: reeducator
  content: Removing some of the vicuna restrictions probably makes sense for this
    dataset. Working on this dataset will then become more natural as well. I will
    look into that at some point.
  created_at: 2023-05-05 22:25:43+00:00
  edited: true
  hidden: false
  id: 645590778fe1bfacb9965a61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-05T23:30:08.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I think it should be pretty easy, in the pre-process, just actually
          checking the username against the roles list instead of assuming <code>%2</code>
          It''d be an experiment in how that messed with outputs, but it would also
          allow for more than 2 member conversations which could be useful later on.</p>

          '
        raw: I think it should be pretty easy, in the pre-process, just actually checking
          the username against the roles list instead of assuming `%2` It'd be an
          experiment in how that messed with outputs, but it would also allow for
          more than 2 member conversations which could be useful later on.
        updatedAt: '2023-05-05T23:30:08.133Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - reeducator
        - Squish42
    id: 64559180e39c6464655eac05
    type: comment
  author: deleted
  content: I think it should be pretty easy, in the pre-process, just actually checking
    the username against the roles list instead of assuming `%2` It'd be an experiment
    in how that messed with outputs, but it would also allow for more than 2 member
    conversations which could be useful later on.
  created_at: 2023-05-05 22:30:08+00:00
  edited: false
  hidden: false
  id: 64559180e39c6464655eac05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-07T00:44:57.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I''m currently uploading an epoch6 version of this. I did the finetune
          up to epoch8, but that might have already hit overtraining, since it was
          parroting too much stuff from the dataset. After some testing, epoch6 seemed
          like a good balance between consistent exchange of messages, creativity
          and things pulled from the dataset. I''m leaving the older epoch3 for now
          there also if comparison is needed.</p>

          <p>I made the AliBi test for a 4k context at the same time. I can''t say
          exactly how well it works, but I did try some very long conversations, and
          the bot seemed to have no trouble remembering things mentioned in the early
          conversations (made sure that no context swapping took place).</p>

          '
        raw: 'I''m currently uploading an epoch6 version of this. I did the finetune
          up to epoch8, but that might have already hit overtraining, since it was
          parroting too much stuff from the dataset. After some testing, epoch6 seemed
          like a good balance between consistent exchange of messages, creativity
          and things pulled from the dataset. I''m leaving the older epoch3 for now
          there also if comparison is needed.


          I made the AliBi test for a 4k context at the same time. I can''t say exactly
          how well it works, but I did try some very long conversations, and the bot
          seemed to have no trouble remembering things mentioned in the early conversations
          (made sure that no context swapping took place).'
        updatedAt: '2023-05-07T00:45:39.213Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
        - mancub
    id: 6456f48903625871eb7d7a47
    type: comment
  author: reeducator
  content: 'I''m currently uploading an epoch6 version of this. I did the finetune
    up to epoch8, but that might have already hit overtraining, since it was parroting
    too much stuff from the dataset. After some testing, epoch6 seemed like a good
    balance between consistent exchange of messages, creativity and things pulled
    from the dataset. I''m leaving the older epoch3 for now there also if comparison
    is needed.


    I made the AliBi test for a 4k context at the same time. I can''t say exactly
    how well it works, but I did try some very long conversations, and the bot seemed
    to have no trouble remembering things mentioned in the early conversations (made
    sure that no context swapping took place).'
  created_at: 2023-05-06 23:44:57+00:00
  edited: true
  hidden: false
  id: 6456f48903625871eb7d7a47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-07T03:52:58.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Epoch6 testing looks good. Definitely fewer inappropriate/ooc responses,
          though my testing is fairly limited so far.</p>

          <p><del>I have only pushed over 2k context once and it immediately devolved
          into the telltale token spam (repeating the last few words over and over,
          basically) so ALiBi might be a meme or it might have been implemented improperly,
          I can''t begin to say with any certainty since this is one of two models
          with it implemented. Though you said it works, so it''s entirely possible
          the ooba sent something it shouldn''t have. I definitely hope to hear other
          people''s experiences here and anyone who figures out how to get it working
          assuming it does work.</del></p>

          <p><em>EDIT: This may be an issue with the llama-cpp-python settings actually.
          I am not sure ooba passed the settings through to llama.cpp, I''ll check
          later.</em><br><strong>CONFIRMED</strong> It loaded the model with 2048
          context, be aware when testing with ooba/koboldcpp.</p>

          <p>That said, Kaiokendev is fixing up the bugs with his initial SuperBIG
          push and I think dearest proxy anon is going to implement a way to use it
          as is that I suggested this morning in the general. I''ve always thought
          clever context shifting was the more promising than massive context size
          for consumer hardware anyway. We''ll see how it goes. I think that route
          is more promising because it means still being able to run 13B GPU models
          on 12GB VRAM with expanded effective context. Model attention spans for
          13B models is still a problem, but parameter count is hard to get around
          for model prompt understanding anyway.</p>

          '
        raw: "Epoch6 testing looks good. Definitely fewer inappropriate/ooc responses,\
          \ though my testing is fairly limited so far.\n\n~~I have only pushed over\
          \ 2k context once and it immediately devolved into the telltale token spam\
          \ (repeating the last few words over and over, basically) so ALiBi might\
          \ be a meme or it might have been implemented improperly, I can't begin\
          \ to say with any certainty since this is one of two models with it implemented.\
          \ Though you said it works, so it's entirely possible the ooba sent something\
          \ it shouldn't have. I definitely hope to hear other people's experiences\
          \ here and anyone who figures out how to get it working assuming it does\
          \ work.~~\n\n*EDIT: This may be an issue with the llama-cpp-python settings\
          \ actually. I am not sure ooba passed the settings through to llama.cpp,\
          \ I'll check later.*  \n**CONFIRMED** It loaded the model with 2048 context,\
          \ be aware when testing with ooba/koboldcpp.\n\nThat said, Kaiokendev is\
          \ fixing up the bugs with his initial SuperBIG push and I think dearest\
          \ proxy anon is going to implement a way to use it as is that I suggested\
          \ this morning in the general. I've always thought clever context shifting\
          \ was the more promising than massive context size for consumer hardware\
          \ anyway. We'll see how it goes. I think that route is more promising because\
          \ it means still being able to run 13B GPU models on 12GB VRAM with expanded\
          \ effective context. Model attention spans for 13B models is still a problem,\
          \ but parameter count is hard to get around for model prompt understanding\
          \ anyway."
        updatedAt: '2023-05-07T19:16:02.618Z'
      numEdits: 6
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - reeducator
    id: 6457209a03625871eb7fd15d
    type: comment
  author: deleted
  content: "Epoch6 testing looks good. Definitely fewer inappropriate/ooc responses,\
    \ though my testing is fairly limited so far.\n\n~~I have only pushed over 2k\
    \ context once and it immediately devolved into the telltale token spam (repeating\
    \ the last few words over and over, basically) so ALiBi might be a meme or it\
    \ might have been implemented improperly, I can't begin to say with any certainty\
    \ since this is one of two models with it implemented. Though you said it works,\
    \ so it's entirely possible the ooba sent something it shouldn't have. I definitely\
    \ hope to hear other people's experiences here and anyone who figures out how\
    \ to get it working assuming it does work.~~\n\n*EDIT: This may be an issue with\
    \ the llama-cpp-python settings actually. I am not sure ooba passed the settings\
    \ through to llama.cpp, I'll check later.*  \n**CONFIRMED** It loaded the model\
    \ with 2048 context, be aware when testing with ooba/koboldcpp.\n\nThat said,\
    \ Kaiokendev is fixing up the bugs with his initial SuperBIG push and I think\
    \ dearest proxy anon is going to implement a way to use it as is that I suggested\
    \ this morning in the general. I've always thought clever context shifting was\
    \ the more promising than massive context size for consumer hardware anyway. We'll\
    \ see how it goes. I think that route is more promising because it means still\
    \ being able to run 13B GPU models on 12GB VRAM with expanded effective context.\
    \ Model attention spans for 13B models is still a problem, but parameter count\
    \ is hard to get around for model prompt understanding anyway."
  created_at: 2023-05-07 02:52:58+00:00
  edited: true
  hidden: false
  id: 6457209a03625871eb7fd15d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7a4efe768c6c71762cbe6bbc3f29647.svg
      fullname: daigonaoki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daigonaoki
      type: user
    createdAt: '2023-05-07T18:54:18.000Z'
    data:
      edited: true
      editors:
      - daigonaoki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7a4efe768c6c71762cbe6bbc3f29647.svg
          fullname: daigonaoki
          isHf: false
          isPro: false
          name: daigonaoki
          type: user
        html: '<p>I was able to successfully push beyond 2048 context with llama.cpp
          by passing <code>--ctx_size 4096</code><br>I don''t think the option to
          force context sizes is exposed by any of the wrappers such as llama-cpp-python
          or koboldcpp, currently.</p>

          <p> First, I generated 2918 tokens from a random prompt, using the 4k-epoch6
          ggml model, with the generation remaining coherent past the 2048 token window:
          <a rel="nofollow" href="https://files.catbox.moe/tjnpns.txt">https://files.catbox.moe/tjnpns.txt</a></p>

          <p>Feeding the output back in as a prompt using the same model, it generated
          an additional 900 or so tokens while still remaining relatively coherent:
          <a rel="nofollow" href="https://files.catbox.moe/hd0bfo.txt">https://files.catbox.moe/hd0bfo.txt</a></p>

          <p>Just to test that I wasn''t getting any kind of sneaky context shifting
          from llama.cpp, I fed the first 2918 output into a different 13B model I
          had laying around, and it <strong>immediately</strong> started outputting
          gibberish: <a rel="nofollow" href="https://files.catbox.moe/qggaq0.txt">https://files.catbox.moe/qggaq0.txt</a></p>

          <p>As far as I can tell, it seems like this actually works!</p>

          '
        raw: "I was able to successfully push beyond 2048 context with llama.cpp by\
          \ passing `--ctx_size 4096`\nI don't think the option to force context sizes\
          \ is exposed by any of the wrappers such as llama-cpp-python or koboldcpp,\
          \ currently.\n\n First, I generated 2918 tokens from a random prompt, using\
          \ the 4k-epoch6 ggml model, with the generation remaining coherent past\
          \ the 2048 token window: https://files.catbox.moe/tjnpns.txt\n\nFeeding\
          \ the output back in as a prompt using the same model, it generated an additional\
          \ 900 or so tokens while still remaining relatively coherent: https://files.catbox.moe/hd0bfo.txt\n\
          \nJust to test that I wasn't getting any kind of sneaky context shifting\
          \ from llama.cpp, I fed the first 2918 output into a different 13B model\
          \ I had laying around, and it **immediately** started outputting gibberish:\
          \ https://files.catbox.moe/qggaq0.txt\n\nAs far as I can tell, it seems\
          \ like this actually works!"
        updatedAt: '2023-05-07T19:01:18.718Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F92F"
        users:
        - nobody-here
        - reeducator
      - count: 1
        reaction: "\U0001F44D"
        users:
        - reeducator
    id: 6457f3da5fc3b8a21ea4826e
    type: comment
  author: daigonaoki
  content: "I was able to successfully push beyond 2048 context with llama.cpp by\
    \ passing `--ctx_size 4096`\nI don't think the option to force context sizes is\
    \ exposed by any of the wrappers such as llama-cpp-python or koboldcpp, currently.\n\
    \n First, I generated 2918 tokens from a random prompt, using the 4k-epoch6 ggml\
    \ model, with the generation remaining coherent past the 2048 token window: https://files.catbox.moe/tjnpns.txt\n\
    \nFeeding the output back in as a prompt using the same model, it generated an\
    \ additional 900 or so tokens while still remaining relatively coherent: https://files.catbox.moe/hd0bfo.txt\n\
    \nJust to test that I wasn't getting any kind of sneaky context shifting from\
    \ llama.cpp, I fed the first 2918 output into a different 13B model I had laying\
    \ around, and it **immediately** started outputting gibberish: https://files.catbox.moe/qggaq0.txt\n\
    \nAs far as I can tell, it seems like this actually works!"
  created_at: 2023-05-07 17:54:18+00:00
  edited: true
  hidden: false
  id: 6457f3da5fc3b8a21ea4826e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-07T19:13:49.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Awesome! Very promising.  And relative coherence is about all you
          can expect from 13B models on their best day, so I''d call that a win.</p>

          <p>Thanks for the extra testing. Hopefully the llama-cpp-python/ooba boys
          and kobold.cpp can expose the flag so these sorts of models can maybe start
          to see a bit of use wider scopes.</p>

          '
        raw: 'Awesome! Very promising.  And relative coherence is about all you can
          expect from 13B models on their best day, so I''d call that a win.


          Thanks for the extra testing. Hopefully the llama-cpp-python/ooba boys and
          kobold.cpp can expose the flag so these sorts of models can maybe start
          to see a bit of use wider scopes.'
        updatedAt: '2023-05-07T19:13:49.737Z'
      numEdits: 0
      reactions: []
    id: 6457f86d116c6b3c62e194b0
    type: comment
  author: deleted
  content: 'Awesome! Very promising.  And relative coherence is about all you can
    expect from 13B models on their best day, so I''d call that a win.


    Thanks for the extra testing. Hopefully the llama-cpp-python/ooba boys and kobold.cpp
    can expose the flag so these sorts of models can maybe start to see a bit of use
    wider scopes.'
  created_at: 2023-05-07 18:13:49+00:00
  edited: false
  hidden: false
  id: 6457f86d116c6b3c62e194b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-05-07T21:22:00.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>I was told that you need to implement alibi in place of ROPE in
          llama''s inference, but I have no idea how to do it</p>

          '
        raw: I was told that you need to implement alibi in place of ROPE in llama's
          inference, but I have no idea how to do it
        updatedAt: '2023-05-07T21:22:00.395Z'
      numEdits: 0
      reactions: []
    id: 64581678116c6b3c62e35a26
    type: comment
  author: teknium
  content: I was told that you need to implement alibi in place of ROPE in llama's
    inference, but I have no idea how to do it
  created_at: 2023-05-07 20:22:00+00:00
  edited: false
  hidden: false
  id: 64581678116c6b3c62e35a26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-07T21:32:37.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p>Very nice indeed. I got similar results with llama.cpp only.</p>\n\
          <p>@gozfarb I agree, ultimately we can't go crazy with the context size\
          \ due to the compute and dataset limits, and because of this the swapping\
          \ techniques are probably going to be more robust for long RPs. Still, 2k\
          \ is pretty limiting and it's nice if we have some means to increase that\
          \ even if we most likely have to stay under 10k.</p>\n<p>Regarding the dataset,\
          \ there are many out of character messages typically marked with labels\
          \ such as \"OOC:\", \"OFF:\" and then return to story context with \"BIC:\"\
          \ or something like that. The kind of messages that <span data-props=\"\
          {&quot;user&quot;:&quot;Squish42&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Squish42\">@<span class=\"underline\">Squish42</span></a></span>\n\
          \n\t</span></span> also mentioned. Some regex such as this can be used to\
          \ filter some of those out:</p>\n<pre><code>(((?i)(OOC|OCC))|(OFF)):((.*?(([\
          \ .!?](IC|DE|ON|BIC|ic|bic)[:. ])|((BiC[:]?)|on:)))|(?:[^\"\\\\]|\\\\.)*(?=\"\
          ))\n</code></pre>\n<p>It should not be applied however if the match is the\
          \ entire message. In this case manual intervention is probably needed (there\
          \ are not too many of those). I might put together a small filter script\
          \ for this.</p>\n"
        raw: 'Very nice indeed. I got similar results with llama.cpp only.


          @gozfarb I agree, ultimately we can''t go crazy with the context size due
          to the compute and dataset limits, and because of this the swapping techniques
          are probably going to be more robust for long RPs. Still, 2k is pretty limiting
          and it''s nice if we have some means to increase that even if we most likely
          have to stay under 10k.


          Regarding the dataset, there are many out of character messages typically
          marked with labels such as "OOC:", "OFF:" and then return to story context
          with "BIC:" or something like that. The kind of messages that @Squish42
          also mentioned. Some regex such as this can be used to filter some of those
          out:

          ```

          (((?i)(OOC|OCC))|(OFF)):((.*?(([ .!?](IC|DE|ON|BIC|ic|bic)[:. ])|((BiC[:]?)|on:)))|(?:[^"\\]|\\.)*(?="))

          ```

          It should not be applied however if the match is the entire message. In
          this case manual intervention is probably needed (there are not too many
          of those). I might put together a small filter script for this.'
        updatedAt: '2023-05-07T21:33:11.007Z'
      numEdits: 1
      reactions: []
    id: 645818f55fc3b8a21ea6e713
    type: comment
  author: reeducator
  content: 'Very nice indeed. I got similar results with llama.cpp only.


    @gozfarb I agree, ultimately we can''t go crazy with the context size due to the
    compute and dataset limits, and because of this the swapping techniques are probably
    going to be more robust for long RPs. Still, 2k is pretty limiting and it''s nice
    if we have some means to increase that even if we most likely have to stay under
    10k.


    Regarding the dataset, there are many out of character messages typically marked
    with labels such as "OOC:", "OFF:" and then return to story context with "BIC:"
    or something like that. The kind of messages that @Squish42 also mentioned. Some
    regex such as this can be used to filter some of those out:

    ```

    (((?i)(OOC|OCC))|(OFF)):((.*?(([ .!?](IC|DE|ON|BIC|ic|bic)[:. ])|((BiC[:]?)|on:)))|(?:[^"\\]|\\.)*(?="))

    ```

    It should not be applied however if the match is the entire message. In this case
    manual intervention is probably needed (there are not too many of those). I might
    put together a small filter script for this.'
  created_at: 2023-05-07 20:32:37+00:00
  edited: true
  hidden: false
  id: 645818f55fc3b8a21ea6e713
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-07T22:11:33.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<blockquote>\n<p>I was told that you need to implement alibi in place\
          \ of ROPE in llama's inference, but I have no idea how to do it</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;teknium&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/teknium\">@<span class=\"\
          underline\">teknium</span></a></span>\n\n\t</span></span> it turns out that\
          \ the transformers library has it already implemented. From what I understood,\
          \ the implementation is relatively easy to use, you just need to override\
          \ the max sequence length before the finetuning stage, and the code will\
          \ take care of it automatically. I think. I did not find documentation regarding\
          \ this. But the observations here suggest that it probably did its job...</p>\n"
        raw: '> I was told that you need to implement alibi in place of ROPE in llama''s
          inference, but I have no idea how to do it


          @teknium it turns out that the transformers library has it already implemented.
          From what I understood, the implementation is relatively easy to use, you
          just need to override the max sequence length before the finetuning stage,
          and the code will take care of it automatically. I think. I did not find
          documentation regarding this. But the observations here suggest that it
          probably did its job...'
        updatedAt: '2023-05-07T22:11:33.584Z'
      numEdits: 0
      reactions: []
    id: 645822150332c1fb59f66b9b
    type: comment
  author: reeducator
  content: '> I was told that you need to implement alibi in place of ROPE in llama''s
    inference, but I have no idea how to do it


    @teknium it turns out that the transformers library has it already implemented.
    From what I understood, the implementation is relatively easy to use, you just
    need to override the max sequence length before the finetuning stage, and the
    code will take care of it automatically. I think. I did not find documentation
    regarding this. But the observations here suggest that it probably did its job...'
  created_at: 2023-05-07 21:11:33+00:00
  edited: false
  hidden: false
  id: 645822150332c1fb59f66b9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-07T22:16:44.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>bluemoon is probably small enough to throw regex at without it burning
          the computer down or taking 9 hours. Probably just do a count on capture
          groups if it seems to work and check that versus the total character count
          and if it''s over 80-90%, just dump the entry.</p>

          '
        raw: bluemoon is probably small enough to throw regex at without it burning
          the computer down or taking 9 hours. Probably just do a count on capture
          groups if it seems to work and check that versus the total character count
          and if it's over 80-90%, just dump the entry.
        updatedAt: '2023-05-07T22:16:44.756Z'
      numEdits: 0
      reactions: []
    id: 6458234c5fc3b8a21ea775a1
    type: comment
  author: deleted
  content: bluemoon is probably small enough to throw regex at without it burning
    the computer down or taking 9 hours. Probably just do a count on capture groups
    if it seems to work and check that versus the total character count and if it's
    over 80-90%, just dump the entry.
  created_at: 2023-05-07 21:16:44+00:00
  edited: false
  hidden: false
  id: 6458234c5fc3b8a21ea775a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10cbdb733c47e9feb543b2e800485fe4.svg
      fullname: bog
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gobbob
      type: user
    createdAt: '2023-05-08T05:11:18.000Z'
    data:
      edited: false
      editors:
      - gobbob
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10cbdb733c47e9feb543b2e800485fe4.svg
          fullname: bog
          isHf: false
          isPro: false
          name: gobbob
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;teknium&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/teknium\">@<span class=\"\
          underline\">teknium</span></a></span>\n\n\t</span></span> I think alibi\
          \ was implemented here: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/ggml/pull/113\"\
          >https://github.com/ggerganov/ggml/pull/113</a></p>\n"
        raw: '@teknium I think alibi was implemented here: https://github.com/ggerganov/ggml/pull/113'
        updatedAt: '2023-05-08T05:11:18.284Z'
      numEdits: 0
      reactions: []
    id: 645884765fc3b8a21eae0b5c
    type: comment
  author: gobbob
  content: '@teknium I think alibi was implemented here: https://github.com/ggerganov/ggml/pull/113'
  created_at: 2023-05-08 04:11:18+00:00
  edited: false
  hidden: false
  id: 645884765fc3b8a21eae0b5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-08T11:51:20.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p>This one takes care of vast majority of unnecessary blabber. Sorry\
          \ that's python. It does not still handle cases where the entire line is\
          \ an OOC, but there are only 8 of those. I already applied the filter for\
          \ the next run.</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span>\
          \ re\n\nfname = <span class=\"hljs-string\">\"bluemoon_roleplay_300k_vicuna.json\"\
          </span>;\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\"\
          >open</span>(fname,<span class=\"hljs-string\">\"r\"</span>) <span class=\"\
          hljs-keyword\">as</span> f:\n    d = json.load(f);\n\nmr = re.<span class=\"\
          hljs-built_in\">compile</span>(<span class=\"hljs-string\">r'(((OOC|OCC|ooc|occ|Occ|Ooc|OoC))|(OFF)):.*?((([\
          \ \\.!?](IC|DE|BIC|ic|bic)[:\\. ])|((BiC[:]?)|[\\.!?](on:|ON:|On:)))|$)'</span>);\n\
          \n<span class=\"hljs-comment\">#remaining implicit OOC'ing</span>\nmanual\
          \ = [\n    <span class=\"hljs-string\">\"OOC: Sorry for the redonculous\
          \ wait. Work takes priority \"</span>,\n    <span class=\"hljs-string\"\
          >\"OOC: I hope you don't mind that I put a time skip in here\"</span>,\n\
          \    <span class=\"hljs-string\">\"OOC: Give her broken english.\"</span>,\n\
          \    <span class=\"hljs-string\">\"OoC: aww, and here I'd been hoping that\
          \ something would be done with the soul edge fragments\"</span>,\n    <span\
          \ class=\"hljs-string\">\"OOC:Oh sure. It's ok \"</span>,\n    <span class=\"\
          hljs-string\">\"OOC: Sorry for being gone so long...I hope you will continue\
          \ rping with me\"</span>,\n];\n\n<span class=\"hljs-keyword\">for</span>\
          \ conv <span class=\"hljs-keyword\">in</span> d:\n    <span class=\"hljs-keyword\"\
          >for</span> m <span class=\"hljs-keyword\">in</span> conv[<span class=\"\
          hljs-string\">\"conversations\"</span>]:\n        l = mr.search(m[<span\
          \ class=\"hljs-string\">\"value\"</span>]);\n        <span class=\"hljs-keyword\"\
          >if</span> l != <span class=\"hljs-literal\">None</span>:\n            <span\
          \ class=\"hljs-built_in\">print</span>(m[<span class=\"hljs-string\">\"\
          value\"</span>]);\n            s = mr.sub(<span class=\"hljs-string\">\"\
          \"</span>,m[<span class=\"hljs-string\">\"value\"</span>]);\n          \
          \  <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >len</span>(s) &gt; <span class=\"hljs-number\">0</span>:\n            \
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"---&gt;\"</span>,s,end=<span class=\"hljs-string\">\"\\n\\n\"</span>);\n\
          \                m[<span class=\"hljs-string\">\"value\"</span>] = s;\n\
          \            <span class=\"hljs-keyword\">else</span>:\n               \
          \ <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"---&gt; MANUAL INTERVENTION REQUIRED. CHECK KEYWORDS.\"</span>);\n\n\
          \        s = m[<span class=\"hljs-string\">\"value\"</span>];\n        <span\
          \ class=\"hljs-keyword\">for</span> e <span class=\"hljs-keyword\">in</span>\
          \ manual:\n            s = re.sub(e,<span class=\"hljs-string\">\"\"</span>,s);\n\
          \        m[<span class=\"hljs-string\">\"value\"</span>] = s;\n\nfname =\
          \ <span class=\"hljs-string\">\"bluemoon_roleplay_300k_vicuna_filtered.json\"\
          </span>;\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\"\
          >open</span>(fname,<span class=\"hljs-string\">\"w\"</span>) <span class=\"\
          hljs-keyword\">as</span> f:\n    json.dump(d,f,indent=<span class=\"hljs-number\"\
          >2</span>);\n</code></pre>\n"
        raw: "This one takes care of vast majority of unnecessary blabber. Sorry that's\
          \ python. It does not still handle cases where the entire line is an OOC,\
          \ but there are only 8 of those. I already applied the filter for the next\
          \ run.\n\n```python\nimport json\nimport re\n\nfname = \"bluemoon_roleplay_300k_vicuna.json\"\
          ;\nwith open(fname,\"r\") as f:\n\td = json.load(f);\n\nmr = re.compile(r'(((OOC|OCC|ooc|occ|Occ|Ooc|OoC))|(OFF)):.*?((([\
          \ \\.!?](IC|DE|BIC|ic|bic)[:\\. ])|((BiC[:]?)|[\\.!?](on:|ON:|On:)))|$)');\n\
          \n#remaining implicit OOC'ing\nmanual = [\n\t\"OOC: Sorry for the redonculous\
          \ wait. Work takes priority \",\n\t\"OOC: I hope you don't mind that I put\
          \ a time skip in here\",\n\t\"OOC: Give her broken english.\",\n\t\"OoC:\
          \ aww, and here I'd been hoping that something would be done with the soul\
          \ edge fragments\",\n\t\"OOC:Oh sure. It's ok \",\n\t\"OOC: Sorry for being\
          \ gone so long...I hope you will continue rping with me\",\n];\n\nfor conv\
          \ in d:\n\tfor m in conv[\"conversations\"]:\n\t\tl = mr.search(m[\"value\"\
          ]);\n\t\tif l != None:\n\t\t\tprint(m[\"value\"]);\n\t\t\ts = mr.sub(\"\"\
          ,m[\"value\"]);\n\t\t\tif len(s) > 0:\n\t\t\t\tprint(\"--->\",s,end=\"\\\
          n\\n\");\n\t\t\t\tm[\"value\"] = s;\n\t\t\telse:\n\t\t\t\tprint(\"---> MANUAL\
          \ INTERVENTION REQUIRED. CHECK KEYWORDS.\");\n\n\t\ts = m[\"value\"];\n\t\
          \tfor e in manual:\n\t\t\ts = re.sub(e,\"\",s);\n\t\tm[\"value\"] = s;\n\
          \nfname = \"bluemoon_roleplay_300k_vicuna_filtered.json\";\nwith open(fname,\"\
          w\") as f:\n\tjson.dump(d,f,indent=2);\n```"
        updatedAt: '2023-05-08T11:51:44.663Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Squish42
    id: 6458e23839e6aea69cbb1f81
    type: comment
  author: reeducator
  content: "This one takes care of vast majority of unnecessary blabber. Sorry that's\
    \ python. It does not still handle cases where the entire line is an OOC, but\
    \ there are only 8 of those. I already applied the filter for the next run.\n\n\
    ```python\nimport json\nimport re\n\nfname = \"bluemoon_roleplay_300k_vicuna.json\"\
    ;\nwith open(fname,\"r\") as f:\n\td = json.load(f);\n\nmr = re.compile(r'(((OOC|OCC|ooc|occ|Occ|Ooc|OoC))|(OFF)):.*?((([\
    \ \\.!?](IC|DE|BIC|ic|bic)[:\\. ])|((BiC[:]?)|[\\.!?](on:|ON:|On:)))|$)');\n\n\
    #remaining implicit OOC'ing\nmanual = [\n\t\"OOC: Sorry for the redonculous wait.\
    \ Work takes priority \",\n\t\"OOC: I hope you don't mind that I put a time skip\
    \ in here\",\n\t\"OOC: Give her broken english.\",\n\t\"OoC: aww, and here I'd\
    \ been hoping that something would be done with the soul edge fragments\",\n\t\
    \"OOC:Oh sure. It's ok \",\n\t\"OOC: Sorry for being gone so long...I hope you\
    \ will continue rping with me\",\n];\n\nfor conv in d:\n\tfor m in conv[\"conversations\"\
    ]:\n\t\tl = mr.search(m[\"value\"]);\n\t\tif l != None:\n\t\t\tprint(m[\"value\"\
    ]);\n\t\t\ts = mr.sub(\"\",m[\"value\"]);\n\t\t\tif len(s) > 0:\n\t\t\t\tprint(\"\
    --->\",s,end=\"\\n\\n\");\n\t\t\t\tm[\"value\"] = s;\n\t\t\telse:\n\t\t\t\tprint(\"\
    ---> MANUAL INTERVENTION REQUIRED. CHECK KEYWORDS.\");\n\n\t\ts = m[\"value\"\
    ];\n\t\tfor e in manual:\n\t\t\ts = re.sub(e,\"\",s);\n\t\tm[\"value\"] = s;\n\
    \nfname = \"bluemoon_roleplay_300k_vicuna_filtered.json\";\nwith open(fname,\"\
    w\") as f:\n\tjson.dump(d,f,indent=2);\n```"
  created_at: 2023-05-08 10:51:20+00:00
  edited: true
  hidden: false
  id: 6458e23839e6aea69cbb1f81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-05-08T12:34:48.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>I''m happy to take anything that''ll help with this dataset because Vicuna
          is VERY particular about formatting with V1.1 and it may require rewriting
          training code to be more flexible to work with datasets before we can even
          start pruning.</p>

          <p>At present, if the entries in a conversations are not like this:</p>

          <p>human-&gt;gpt-&gt;human-&gt;gpt</p>

          <p>Then the training dies. So that''s something to keep in mind when it
          comes to cleaning out the Bluemoon dataset. Make sure any conversations
          like that get made sane. Or, if there are enough overall conversations (not
          entries) we can consider just dropping those conversations entirely. A few
          that I noticed glancing at the dataset ended with people asking if a person
          was still there or still interested in continuing. It could be nice to find
          things like "bump" or "closed" and generate continuations to go into those
          spots rather than pruning them outright.</p>

          </blockquote>

          <p>You should also check out OpenAssistant''s training code, it has dataloaders
          for vicuna or alpaca (and 2 others afaik)</p>

          '
        raw: "> I'm happy to take anything that'll help with this dataset because\
          \ Vicuna is VERY particular about formatting with V1.1 and it may require\
          \ rewriting training code to be more flexible to work with datasets before\
          \ we can even start pruning.\n> \n> At present, if the entries in a conversations\
          \ are not like this:\n> \n> human->gpt->human->gpt\n> \n> Then the training\
          \ dies. So that's something to keep in mind when it comes to cleaning out\
          \ the Bluemoon dataset. Make sure any conversations like that get made sane.\
          \ Or, if there are enough overall conversations (not entries) we can consider\
          \ just dropping those conversations entirely. A few that I noticed glancing\
          \ at the dataset ended with people asking if a person was still there or\
          \ still interested in continuing. It could be nice to find things like \"\
          bump\" or \"closed\" and generate continuations to go into those spots rather\
          \ than pruning them outright.\n\nYou should also check out OpenAssistant's\
          \ training code, it has dataloaders for vicuna or alpaca (and 2 others afaik)"
        updatedAt: '2023-05-08T12:34:48.880Z'
      numEdits: 0
      reactions: []
    id: 6458ec6839e6aea69cbc0f69
    type: comment
  author: teknium
  content: "> I'm happy to take anything that'll help with this dataset because Vicuna\
    \ is VERY particular about formatting with V1.1 and it may require rewriting training\
    \ code to be more flexible to work with datasets before we can even start pruning.\n\
    > \n> At present, if the entries in a conversations are not like this:\n> \n>\
    \ human->gpt->human->gpt\n> \n> Then the training dies. So that's something to\
    \ keep in mind when it comes to cleaning out the Bluemoon dataset. Make sure any\
    \ conversations like that get made sane. Or, if there are enough overall conversations\
    \ (not entries) we can consider just dropping those conversations entirely. A\
    \ few that I noticed glancing at the dataset ended with people asking if a person\
    \ was still there or still interested in continuing. It could be nice to find\
    \ things like \"bump\" or \"closed\" and generate continuations to go into those\
    \ spots rather than pruning them outright.\n\nYou should also check out OpenAssistant's\
    \ training code, it has dataloaders for vicuna or alpaca (and 2 others afaik)"
  created_at: 2023-05-08 11:34:48+00:00
  edited: false
  hidden: false
  id: 6458ec6839e6aea69cbc0f69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-08T16:20:26.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/reeducator\">@<span class=\"\
          underline\">reeducator</span></a></span>\n\n\t</span></span>  Added the\
          \ script to the repo and updated the dataset to have the OOC removed. The\
          \ old stuff is still there in the WithOOC folder.</p>\n<p>I am also manually\
          \ cleaning a bunch of other shit out by hand. Because I hate my free time.\
          \ Will commit the fully cleaned version once I'm done. Just 261 more things\
          \ to go!</p>\n<p>EDIT: Okay, pruned the leftover ooc manually. I don't think\
          \ I missed any. I also fixed up the \"this thread is closed\" topics.</p>\n"
        raw: '@reeducator  Added the script to the repo and updated the dataset to
          have the OOC removed. The old stuff is still there in the WithOOC folder.


          I am also manually cleaning a bunch of other shit out by hand. Because I
          hate my free time. Will commit the fully cleaned version once I''m done.
          Just 261 more things to go!


          EDIT: Okay, pruned the leftover ooc manually. I don''t think I missed any.
          I also fixed up the "this thread is closed" topics.'
        updatedAt: '2023-05-08T16:47:22.589Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - reeducator
    id: 6459214af92601affa3360df
    type: comment
  author: deleted
  content: '@reeducator  Added the script to the repo and updated the dataset to have
    the OOC removed. The old stuff is still there in the WithOOC folder.


    I am also manually cleaning a bunch of other shit out by hand. Because I hate
    my free time. Will commit the fully cleaned version once I''m done. Just 261 more
    things to go!


    EDIT: Okay, pruned the leftover ooc manually. I don''t think I missed any. I also
    fixed up the "this thread is closed" topics.'
  created_at: 2023-05-08 15:20:26+00:00
  edited: true
  hidden: false
  id: 6459214af92601affa3360df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-08T16:50:55.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Incredible, thanks! Surely we all have better things to do, but
          we also realize that our mission is paramount...</p>

          '
        raw: Incredible, thanks! Surely we all have better things to do, but we also
          realize that our mission is paramount...
        updatedAt: '2023-05-08T16:50:55.786Z'
      numEdits: 0
      reactions: []
    id: 6459286ff92601affa33e84c
    type: comment
  author: reeducator
  content: Incredible, thanks! Surely we all have better things to do, but we also
    realize that our mission is paramount...
  created_at: 2023-05-08 15:50:55+00:00
  edited: false
  hidden: false
  id: 6459286ff92601affa33e84c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-08T17:05:42.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>If I missed any ooc, let me know. There should be so few in there
          now that any popouts are incidental. I also removed a bunch of thread titles
          in the responses (a few hundred). But if anyone wants to make a list of
          thread titles in the dataset, please make a list here or on the dataset
          repo and I will gut them.</p>

          '
        raw: If I missed any ooc, let me know. There should be so few in there now
          that any popouts are incidental. I also removed a bunch of thread titles
          in the responses (a few hundred). But if anyone wants to make a list of
          thread titles in the dataset, please make a list here or on the dataset
          repo and I will gut them.
        updatedAt: '2023-05-08T17:05:42.144Z'
      numEdits: 0
      reactions: []
    id: 64592be6c5d0d57ba4232d4d
    type: comment
  author: deleted
  content: If I missed any ooc, let me know. There should be so few in there now that
    any popouts are incidental. I also removed a bunch of thread titles in the responses
    (a few hundred). But if anyone wants to make a list of thread titles in the dataset,
    please make a list here or on the dataset repo and I will gut them.
  created_at: 2023-05-08 16:05:42+00:00
  edited: false
  hidden: false
  id: 64592be6c5d0d57ba4232d4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-08T17:09:28.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Also, because I have e-mail alerts off right now because of a thread
          that is deeply annoying that I was trying to help in, I only get e-mails
          about posts on repos I own, so if I''m slow to respond, that''s why. Sorry.</p>

          '
        raw: Also, because I have e-mail alerts off right now because of a thread
          that is deeply annoying that I was trying to help in, I only get e-mails
          about posts on repos I own, so if I'm slow to respond, that's why. Sorry.
        updatedAt: '2023-05-08T17:09:28.377Z'
      numEdits: 0
      reactions: []
    id: 64592cc8f92601affa343a03
    type: comment
  author: deleted
  content: Also, because I have e-mail alerts off right now because of a thread that
    is deeply annoying that I was trying to help in, I only get e-mails about posts
    on repos I own, so if I'm slow to respond, that's why. Sorry.
  created_at: 2023-05-08 16:09:28+00:00
  edited: false
  hidden: false
  id: 64592cc8f92601affa343a03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-08T17:42:01.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>The drama thread? Yeah no worries. Can''t believe there''s no way
          to unsubscribe. Maybe you can try filter those emails with that title.</p>

          '
        raw: The drama thread? Yeah no worries. Can't believe there's no way to unsubscribe.
          Maybe you can try filter those emails with that title.
        updatedAt: '2023-05-08T17:42:01.313Z'
      numEdits: 0
      reactions: []
    id: 64593469f92601affa34c203
    type: comment
  author: reeducator
  content: The drama thread? Yeah no worries. Can't believe there's no way to unsubscribe.
    Maybe you can try filter those emails with that title.
  created_at: 2023-05-08 16:42:01+00:00
  edited: false
  hidden: false
  id: 64593469f92601affa34c203
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7a4efe768c6c71762cbe6bbc3f29647.svg
      fullname: daigonaoki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daigonaoki
      type: user
    createdAt: '2023-05-08T19:27:42.000Z'
    data:
      edited: false
      editors:
      - daigonaoki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7a4efe768c6c71762cbe6bbc3f29647.svg
          fullname: daigonaoki
          isHf: false
          isPro: false
          name: daigonaoki
          type: user
        html: '<p>koboldcpp now has support for larger context sizes on LLaMA models
          in its most recent version, if anyone was waiting for a better UX for experimenting
          with this model at full context.</p>

          <p>You just have to pass <code>--contextsize 4096</code> when starting up
          koboldcpp, and <em>additionally</em> set Max Tokens to 4096 in the settings
          (the slider caps at 2048, but you can click and manually type in a value).</p>

          <p>For what it''s worth, my impressions of it haven''t really changed since
          I was messing with it in llama.cpp: It <em>might</em> change its writing
          style a little bit past 2048, preferring writing large blocks of text with
          minimal paragraph breaks, but it doesn''t seem to get significantly dumber
          or anything. I even went over 4096 context and it was still able to write
          coherent sentences for a while, but it started repeating itself in spammy
          loops at ~4800 or so.</p>

          '
        raw: 'koboldcpp now has support for larger context sizes on LLaMA models in
          its most recent version, if anyone was waiting for a better UX for experimenting
          with this model at full context.


          You just have to pass `--contextsize 4096` when starting up koboldcpp, and
          *additionally* set Max Tokens to 4096 in the settings (the slider caps at
          2048, but you can click and manually type in a value).


          For what it''s worth, my impressions of it haven''t really changed since
          I was messing with it in llama.cpp: It *might* change its writing style
          a little bit past 2048, preferring writing large blocks of text with minimal
          paragraph breaks, but it doesn''t seem to get significantly dumber or anything.
          I even went over 4096 context and it was still able to write coherent sentences
          for a while, but it started repeating itself in spammy loops at ~4800 or
          so.'
        updatedAt: '2023-05-08T19:27:42.700Z'
      numEdits: 0
      reactions: []
    id: 64594d2ec5d0d57ba425761a
    type: comment
  author: daigonaoki
  content: 'koboldcpp now has support for larger context sizes on LLaMA models in
    its most recent version, if anyone was waiting for a better UX for experimenting
    with this model at full context.


    You just have to pass `--contextsize 4096` when starting up koboldcpp, and *additionally*
    set Max Tokens to 4096 in the settings (the slider caps at 2048, but you can click
    and manually type in a value).


    For what it''s worth, my impressions of it haven''t really changed since I was
    messing with it in llama.cpp: It *might* change its writing style a little bit
    past 2048, preferring writing large blocks of text with minimal paragraph breaks,
    but it doesn''t seem to get significantly dumber or anything. I even went over
    4096 context and it was still able to write coherent sentences for a while, but
    it started repeating itself in spammy loops at ~4800 or so.'
  created_at: 2023-05-08 18:27:42+00:00
  edited: false
  hidden: false
  id: 64594d2ec5d0d57ba425761a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-08T19:43:54.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Interesting. I wonder if the extended context is biasing it toward
          any training material that is over the original 2048 limit and those bits
          being more likely to be long blocks of text. We don''t have enough ALiBi
          models to test against, but I wonder if that is an artifact of how the extended
          embedding works. It could explain why the writing style <em>maybe</em> changes.</p>

          <p>Or it could just be an artifact of the dataset itself. Not enough data
          but fun to keep in mind as we get more data.</p>

          '
        raw: 'Interesting. I wonder if the extended context is biasing it toward any
          training material that is over the original 2048 limit and those bits being
          more likely to be long blocks of text. We don''t have enough ALiBi models
          to test against, but I wonder if that is an artifact of how the extended
          embedding works. It could explain why the writing style *maybe* changes.


          Or it could just be an artifact of the dataset itself. Not enough data but
          fun to keep in mind as we get more data.'
        updatedAt: '2023-05-08T19:43:54.912Z'
      numEdits: 0
      reactions: []
    id: 645950fa232e5f0712b990b2
    type: comment
  author: deleted
  content: 'Interesting. I wonder if the extended context is biasing it toward any
    training material that is over the original 2048 limit and those bits being more
    likely to be long blocks of text. We don''t have enough ALiBi models to test against,
    but I wonder if that is an artifact of how the extended embedding works. It could
    explain why the writing style *maybe* changes.


    Or it could just be an artifact of the dataset itself. Not enough data but fun
    to keep in mind as we get more data.'
  created_at: 2023-05-08 18:43:54+00:00
  edited: false
  hidden: false
  id: 645950fa232e5f0712b990b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-14T18:51:32.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>I tested the epoch 6 and it definitely gets squishy past 2500, using
          the GPTQ version. At around 3000 it gets pretty far gone.</p>

          '
        raw: I tested the epoch 6 and it definitely gets squishy past 2500, using
          the GPTQ version. At around 3000 it gets pretty far gone.
        updatedAt: '2023-05-14T18:51:32.583Z'
      numEdits: 0
      reactions: []
    id: 64612db4bf985b8b4eb79b3f
    type: comment
  author: autobots
  content: I tested the epoch 6 and it definitely gets squishy past 2500, using the
    GPTQ version. At around 3000 it gets pretty far gone.
  created_at: 2023-05-14 17:51:32+00:00
  edited: false
  hidden: false
  id: 64612db4bf985b8b4eb79b3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-25T13:42:07.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Bluemoon 30b is up for those who are interested: <a href="https://huggingface.co/reeducator/bluemoonrp-30b">https://huggingface.co/reeducator/bluemoonrp-30b</a><br>GPTQ
          4-bit will follow at some point when I manage to convert that.</p>

          '
        raw: 'Bluemoon 30b is up for those who are interested: https://huggingface.co/reeducator/bluemoonrp-30b

          GPTQ 4-bit will follow at some point when I manage to convert that.'
        updatedAt: '2023-05-25T13:42:57.990Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
    id: 646f65affa9253f3ae828f80
    type: comment
  author: reeducator
  content: 'Bluemoon 30b is up for those who are interested: https://huggingface.co/reeducator/bluemoonrp-30b

    GPTQ 4-bit will follow at some point when I manage to convert that.'
  created_at: 2023-05-25 12:42:07+00:00
  edited: true
  hidden: false
  id: 646f65affa9253f3ae828f80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: reeducator/bluemoonrp-13b
repo_type: model
status: open
target_branch: null
title: Initial testing (epoch 3)
