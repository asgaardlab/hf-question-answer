!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-05-27 16:39:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-05-27T17:39:49.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Hello.<br>This model is great, due to its ability to handle well
          more tokens than usual (I start to have trouble at 2500, and lose fully
          coherence around 3,000).<br>Is there a 7b version planned for those having
          smaller configurations?<br>Thanks for the great work.</p>

          <p>Edit: Q4_0 quantizations could be great also, to provide a little boost
          of perfs.<br>I know, I should learn to do that job myself.. ^^</p>

          '
        raw: 'Hello.

          This model is great, due to its ability to handle well more tokens than
          usual (I start to have trouble at 2500, and lose fully coherence around
          3,000).

          Is there a 7b version planned for those having smaller configurations?

          Thanks for the great work.


          Edit: Q4_0 quantizations could be great also, to provide a little boost
          of perfs.

          I know, I should learn to do that job myself.. ^^'
        updatedAt: '2023-05-27T17:43:59.434Z'
      numEdits: 2
      reactions: []
    id: 6472406522016353ae3b058a
    type: comment
  author: Nexesenex
  content: 'Hello.

    This model is great, due to its ability to handle well more tokens than usual
    (I start to have trouble at 2500, and lose fully coherence around 3,000).

    Is there a 7b version planned for those having smaller configurations?

    Thanks for the great work.


    Edit: Q4_0 quantizations could be great also, to provide a little boost of perfs.

    I know, I should learn to do that job myself.. ^^'
  created_at: 2023-05-27 16:39:49+00:00
  edited: true
  hidden: false
  id: 6472406522016353ae3b058a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-27T18:34:09.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>We''re working on an update to this model, and once ready, I think
          that would be a good moment to train all the variants up to 30b. First there
          will be a new 30b, and then 13b and 7b together, so yeah it wouldn''t be
          too much trouble training a 7b. First bluemoons have been a bit experimental,
          and therefore easier to evaluate with larger models, but as it''s converging
          there will not be any issues adding a 7b to the selection (probably within
          a week or so)! I can add the q4_0 there also once it''s up.</p>

          '
        raw: We're working on an update to this model, and once ready, I think that
          would be a good moment to train all the variants up to 30b. First there
          will be a new 30b, and then 13b and 7b together, so yeah it wouldn't be
          too much trouble training a 7b. First bluemoons have been a bit experimental,
          and therefore easier to evaluate with larger models, but as it's converging
          there will not be any issues adding a 7b to the selection (probably within
          a week or so)! I can add the q4_0 there also once it's up.
        updatedAt: '2023-05-27T18:34:09.706Z'
      numEdits: 0
      reactions: []
    id: 64724d210211f85270085a7c
    type: comment
  author: reeducator
  content: We're working on an update to this model, and once ready, I think that
    would be a good moment to train all the variants up to 30b. First there will be
    a new 30b, and then 13b and 7b together, so yeah it wouldn't be too much trouble
    training a 7b. First bluemoons have been a bit experimental, and therefore easier
    to evaluate with larger models, but as it's converging there will not be any issues
    adding a 7b to the selection (probably within a week or so)! I can add the q4_0
    there also once it's up.
  created_at: 2023-05-27 17:34:09+00:00
  edited: false
  hidden: false
  id: 64724d210211f85270085a7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-05-27T19:05:45.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Honestly this is amazing work. The 2048 context size is the main
          downside of all the current Llama models except yours, no matter how qualitative
          they otherwise are. And it matters a lot to create more coherent storylines
          and dialogues, even with the help of memory tricks like those offered by
          Koboldcpp, Ooba, and SillyTavern.<br>Your exclusive about a context size
          of 4096 tokens, even if it seems to be still a work in progress due to the
          initial limitations of the Llama models, should be the norm.<br>As for quants,
          5_0 and 4_0 are the best options so far, in respect for one''s choice between
          quality, speed, and config requirements.<br>Until next updates (and me saving
          up for a proper rig!), thanks again for your team''s great job, and best
          regards!</p>

          '
        raw: 'Honestly this is amazing work. The 2048 context size is the main downside
          of all the current Llama models except yours, no matter how qualitative
          they otherwise are. And it matters a lot to create more coherent storylines
          and dialogues, even with the help of memory tricks like those offered by
          Koboldcpp, Ooba, and SillyTavern.

          Your exclusive about a context size of 4096 tokens, even if it seems to
          be still a work in progress due to the initial limitations of the Llama
          models, should be the norm.

          As for quants, 5_0 and 4_0 are the best options so far, in respect for one''s
          choice between quality, speed, and config requirements.

          Until next updates (and me saving up for a proper rig!), thanks again for
          your team''s great job, and best regards!'
        updatedAt: '2023-05-27T21:47:50.154Z'
      numEdits: 1
      reactions: []
    id: 64725489c27f74a0eba8b3f2
    type: comment
  author: Nexesenex
  content: 'Honestly this is amazing work. The 2048 context size is the main downside
    of all the current Llama models except yours, no matter how qualitative they otherwise
    are. And it matters a lot to create more coherent storylines and dialogues, even
    with the help of memory tricks like those offered by Koboldcpp, Ooba, and SillyTavern.

    Your exclusive about a context size of 4096 tokens, even if it seems to be still
    a work in progress due to the initial limitations of the Llama models, should
    be the norm.

    As for quants, 5_0 and 4_0 are the best options so far, in respect for one''s
    choice between quality, speed, and config requirements.

    Until next updates (and me saving up for a proper rig!), thanks again for your
    team''s great job, and best regards!'
  created_at: 2023-05-27 18:05:45+00:00
  edited: true
  hidden: false
  id: 64725489c27f74a0eba8b3f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-27T21:24:55.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>It''s cool indeed how well the extended context works. I''m not
          sure if one can extend the context with LoRA since I''ve never trained one
          myself, but maybe that''s one reason why we haven''t seen 4k or larger context
          much around in other models. Glad you like the work, big credit goes to
          everyone who works on the datasets here in HF, and all those doing the early
          testing!</p>

          '
        raw: It's cool indeed how well the extended context works. I'm not sure if
          one can extend the context with LoRA since I've never trained one myself,
          but maybe that's one reason why we haven't seen 4k or larger context much
          around in other models. Glad you like the work, big credit goes to everyone
          who works on the datasets here in HF, and all those doing the early testing!
        updatedAt: '2023-05-27T21:24:55.603Z'
      numEdits: 0
      reactions: []
    id: 6472752797a75cc77ab8595f
    type: comment
  author: reeducator
  content: It's cool indeed how well the extended context works. I'm not sure if one
    can extend the context with LoRA since I've never trained one myself, but maybe
    that's one reason why we haven't seen 4k or larger context much around in other
    models. Glad you like the work, big credit goes to everyone who works on the datasets
    here in HF, and all those doing the early testing!
  created_at: 2023-05-27 20:24:55+00:00
  edited: false
  hidden: false
  id: 6472752797a75cc77ab8595f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-05-29T02:08:38.000Z'
    data:
      status: closed
    id: 647409266cff2f86720f9c86
    type: status-change
  author: Nexesenex
  created_at: 2023-05-29 01:08:38+00:00
  id: 647409266cff2f86720f9c86
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-05-29T02:08:42.000Z'
    data:
      status: open
    id: 6474092a6cff2f86720f9cec
    type: status-change
  author: Nexesenex
  created_at: 2023-05-29 01:08:42+00:00
  id: 6474092a6cff2f86720f9cec
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: reeducator/bluemoonrp-13b
repo_type: model
status: open
target_branch: null
title: 7b model
