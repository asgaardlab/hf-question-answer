!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Julian-CF
conflicting_files: null
created_at: 2023-10-25 09:53:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0840c153048108ee2d932413ca434865.svg
      fullname: Julian Coda-Forno
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Julian-CF
      type: user
    createdAt: '2023-10-25T10:53:05.000Z'
    data:
      edited: false
      editors:
      - Julian-CF
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8686316013336182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0840c153048108ee2d932413ca434865.svg
          fullname: Julian Coda-Forno
          isHf: false
          isPro: false
          name: Julian-CF
          type: user
        html: '<p>Hello,</p>

          <p>Thanks for your amazing work! Whenever I try to access any of your Llama
          longlora models using the transformers package: "pipe = pipeline("text-generation",
          model="Yukang/Llama-2-13b-longlora-64k")" I get this type of error:</p>

          <p>"*** OSError: Can''t load the configuration of ''/dataset/pretrained-models/Llama-2-13b-hf''.
          If you were trying to load it from ''<a href="https://huggingface.co/models''">https://huggingface.co/models''</a>,
          make sure you don''t have a local directory with the same name. Otherwise,
          make sure ''/dataset/pretrained-models/Llama-2-13b-hf'' is the correct path
          to a directory containing a config.json file".</p>

          <p>Interestingly, I don''t have any problem to load your fully fine-tuned
          models (e.g: Llama-2-13b-longlora-32k or LongAlpaca-13B), so I believe it
          has to do with these longlora models where you have the adapter_config.json
          files which redirects to directories which can''t be found ""base_model_name_or_path":
          "/dataset/pretrained-models/Llama-2-13b-hf"". Is there anything I can try
          to access these models using the transformers package or something obvious
          that I might have forgotten for this set of models?</p>

          <p>Best,<br>Julian</p>

          '
        raw: "Hello,\r\n\r\nThanks for your amazing work! Whenever I try to access\
          \ any of your Llama longlora models using the transformers package: \"pipe\
          \ = pipeline(\"text-generation\", model=\"Yukang/Llama-2-13b-longlora-64k\"\
          )\" I get this type of error:\r\n\r\n\"*** OSError: Can't load the configuration\
          \ of '/dataset/pretrained-models/Llama-2-13b-hf'. If you were trying to\
          \ load it from 'https://huggingface.co/models', make sure you don't have\
          \ a local directory with the same name. Otherwise, make sure '/dataset/pretrained-models/Llama-2-13b-hf'\
          \ is the correct path to a directory containing a config.json file\".\r\n\
          \r\nInterestingly, I don't have any problem to load your fully fine-tuned\
          \ models (e.g: Llama-2-13b-longlora-32k or LongAlpaca-13B), so I believe\
          \ it has to do with these longlora models where you have the adapter_config.json\
          \ files which redirects to directories which can't be found \"\"base_model_name_or_path\"\
          : \"/dataset/pretrained-models/Llama-2-13b-hf\"\". Is there anything I can\
          \ try to access these models using the transformers package or something\
          \ obvious that I might have forgotten for this set of models?\r\n\r\nBest,\r\
          \nJulian\r\n"
        updatedAt: '2023-10-25T10:53:05.920Z'
      numEdits: 0
      reactions: []
    id: 6538f391e9cbac8f4202aaa2
    type: comment
  author: Julian-CF
  content: "Hello,\r\n\r\nThanks for your amazing work! Whenever I try to access any\
    \ of your Llama longlora models using the transformers package: \"pipe = pipeline(\"\
    text-generation\", model=\"Yukang/Llama-2-13b-longlora-64k\")\" I get this type\
    \ of error:\r\n\r\n\"*** OSError: Can't load the configuration of '/dataset/pretrained-models/Llama-2-13b-hf'.\
    \ If you were trying to load it from 'https://huggingface.co/models', make sure\
    \ you don't have a local directory with the same name. Otherwise, make sure '/dataset/pretrained-models/Llama-2-13b-hf'\
    \ is the correct path to a directory containing a config.json file\".\r\n\r\n\
    Interestingly, I don't have any problem to load your fully fine-tuned models (e.g:\
    \ Llama-2-13b-longlora-32k or LongAlpaca-13B), so I believe it has to do with\
    \ these longlora models where you have the adapter_config.json files which redirects\
    \ to directories which can't be found \"\"base_model_name_or_path\": \"/dataset/pretrained-models/Llama-2-13b-hf\"\
    \". Is there anything I can try to access these models using the transformers\
    \ package or something obvious that I might have forgotten for this set of models?\r\
    \n\r\nBest,\r\nJulian\r\n"
  created_at: 2023-10-25 09:53:05+00:00
  edited: false
  hidden: false
  id: 6538f391e9cbac8f4202aaa2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
      fullname: YukangChen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yukang
      type: user
    createdAt: '2023-10-25T15:47:49.000Z'
    data:
      edited: false
      editors:
      - Yukang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.934260904788971
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
          fullname: YukangChen
          isHf: false
          isPro: false
          name: Yukang
          type: user
        html: '<p>Hi,</p>

          <p>Thanks for your question. I have changed the   "base_model_name_or_path"
          to be"meta-llama/Llama-2-13b-hf". It would be all right for this case. Would
          you please have try again?</p>

          <p>Regards,<br>Yukang Chen</p>

          '
        raw: 'Hi,


          Thanks for your question. I have changed the   "base_model_name_or_path"
          to be"meta-llama/Llama-2-13b-hf". It would be all right for this case. Would
          you please have try again?



          Regards,

          Yukang Chen

          '
        updatedAt: '2023-10-25T15:47:49.753Z'
      numEdits: 0
      reactions: []
    id: 653938a56882a4dcf7bb2a0e
    type: comment
  author: Yukang
  content: 'Hi,


    Thanks for your question. I have changed the   "base_model_name_or_path" to be"meta-llama/Llama-2-13b-hf".
    It would be all right for this case. Would you please have try again?



    Regards,

    Yukang Chen

    '
  created_at: 2023-10-25 14:47:49+00:00
  edited: false
  hidden: false
  id: 653938a56882a4dcf7bb2a0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0840c153048108ee2d932413ca434865.svg
      fullname: Julian Coda-Forno
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Julian-CF
      type: user
    createdAt: '2023-10-25T16:00:55.000Z'
    data:
      edited: false
      editors:
      - Julian-CF
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7473692297935486
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0840c153048108ee2d932413ca434865.svg
          fullname: Julian Coda-Forno
          isHf: false
          isPro: false
          name: Julian-CF
          type: user
        html: '<p>It works, thanks! Can you also do the same for your other longlora
          models (e.g: Llama-2-70b-longlora-32k, Llama-2-70b-chat-longlora-32k, ..)
          please?</p>

          '
        raw: 'It works, thanks! Can you also do the same for your other longlora models
          (e.g: Llama-2-70b-longlora-32k, Llama-2-70b-chat-longlora-32k, ..) please?


          '
        updatedAt: '2023-10-25T16:00:55.706Z'
      numEdits: 0
      reactions: []
    id: 65393bb7821b9e4a349a9c08
    type: comment
  author: Julian-CF
  content: 'It works, thanks! Can you also do the same for your other longlora models
    (e.g: Llama-2-70b-longlora-32k, Llama-2-70b-chat-longlora-32k, ..) please?


    '
  created_at: 2023-10-25 15:00:55+00:00
  edited: false
  hidden: false
  id: 65393bb7821b9e4a349a9c08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
      fullname: YukangChen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yukang
      type: user
    createdAt: '2023-10-25T16:12:07.000Z'
    data:
      edited: false
      editors:
      - Yukang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9983943104743958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
          fullname: YukangChen
          isHf: false
          isPro: false
          name: Yukang
          type: user
        html: '<p>Yes. I have changed them.</p>

          '
        raw: Yes. I have changed them.
        updatedAt: '2023-10-25T16:12:07.391Z'
      numEdits: 0
      reactions: []
    id: 65393e57fc8dea94ef57e290
    type: comment
  author: Yukang
  content: Yes. I have changed them.
  created_at: 2023-10-25 15:12:07+00:00
  edited: false
  hidden: false
  id: 65393e57fc8dea94ef57e290
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Yukang/Llama-2-13b-longlora-64k
repo_type: model
status: open
target_branch: null
title: Can't load any longlora model with Transformers package.
