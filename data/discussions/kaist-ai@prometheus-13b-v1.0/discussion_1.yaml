!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adnan-ahmad-tub
conflicting_files: null
created_at: 2023-11-19 13:48:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fe20f85e84507987bba62abfc836b92.svg
      fullname: Adnan Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adnan-ahmad-tub
      type: user
    createdAt: '2023-11-19T13:48:45.000Z'
    data:
      edited: true
      editors:
      - adnan-ahmad-tub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3333635628223419
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fe20f85e84507987bba62abfc836b92.svg
          fullname: Adnan Ahmad
          isHf: false
          isPro: false
          name: adnan-ahmad-tub
          type: user
        html: '<p>Hi,</p>

          <p>For 16f model, is it mandatory to use </p>

          <pre><code>tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

          model = LlamaForCausalLM.from_pretrained("kaist-ai/Prometheus-13b-v1.0",
          device_map="auto", torch_dtype=torch.float16)

          </code></pre>

          <p>Or </p>

          <pre><code>tokenizer = AutoTokenizer.from_pretrained("kaist-ai/prometheus-13b-v1.0")

          model = AutoModelForCausalLM.from_pretrained("kaist-ai/prometheus-13b-v1.0",
          device_map="auto", torch_dtype=torch.float16)

          </code></pre>

          <p>Will also work?</p>

          <p>Thanks!</p>

          '
        raw: "Hi,\n\nFor 16f model, is it mandatory to use \n```\ntokenizer = AutoTokenizer.from_pretrained(\"\
          meta-llama/Llama-2-7b-chat-hf\")\nmodel = LlamaForCausalLM.from_pretrained(\"\
          kaist-ai/Prometheus-13b-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)\n\
          ```\nOr \n```\ntokenizer = AutoTokenizer.from_pretrained(\"kaist-ai/prometheus-13b-v1.0\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"kaist-ai/prometheus-13b-v1.0\"\
          , device_map=\"auto\", torch_dtype=torch.float16)\n```\n\nWill also work?\n\
          \nThanks!"
        updatedAt: '2023-11-19T13:49:44.986Z'
      numEdits: 3
      reactions: []
    id: 655a123d4cd8d44865d3123c
    type: comment
  author: adnan-ahmad-tub
  content: "Hi,\n\nFor 16f model, is it mandatory to use \n```\ntokenizer = AutoTokenizer.from_pretrained(\"\
    meta-llama/Llama-2-7b-chat-hf\")\nmodel = LlamaForCausalLM.from_pretrained(\"\
    kaist-ai/Prometheus-13b-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)\n\
    ```\nOr \n```\ntokenizer = AutoTokenizer.from_pretrained(\"kaist-ai/prometheus-13b-v1.0\"\
    )\nmodel = AutoModelForCausalLM.from_pretrained(\"kaist-ai/prometheus-13b-v1.0\"\
    , device_map=\"auto\", torch_dtype=torch.float16)\n```\n\nWill also work?\n\n\
    Thanks!"
  created_at: 2023-11-19 13:48:45+00:00
  edited: true
  hidden: false
  id: 655a123d4cd8d44865d3123c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: kaist-ai/prometheus-13b-v1.0
repo_type: model
status: open
target_branch: null
title: FP16 model and tokenizer
