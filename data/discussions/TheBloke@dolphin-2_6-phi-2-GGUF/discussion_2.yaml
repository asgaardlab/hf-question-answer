!!python/object:huggingface_hub.community.DiscussionWithDetails
author: egodos
conflicting_files: null
created_at: 2023-12-27 12:24:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc2b7712db0fd90eaad30c7a41fe981d.svg
      fullname: Juan Diego Iberico
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: egodos
      type: user
    createdAt: '2023-12-27T12:24:17.000Z'
    data:
      edited: false
      editors:
      - egodos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7182820439338684
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc2b7712db0fd90eaad30c7a41fe981d.svg
          fullname: Juan Diego Iberico
          isHf: false
          isPro: false
          name: egodos
          type: user
        html: '<p>Hello all, </p>

          <p>I install in windows successfully:<br>CMAKE_ARGS="-DLLAMA_CUBLAS=on"<br>!pip
          install llama-cpp-python</p>

          <p>But when loading the model i get: "ValueError: Model path does not exist:
          ./dolphin-2_6-phi-2.Q4_K_M.gguf"</p>

          <p>Do I need to install a specific branch or something? </p>

          <p>Thanks in advance</p>

          '
        raw: "Hello all, \r\n\r\nI install in windows successfully:\r\nCMAKE_ARGS=\"\
          -DLLAMA_CUBLAS=on\" \r\n!pip install llama-cpp-python\r\n\r\nBut when loading\
          \ the model i get: \"ValueError: Model path does not exist: ./dolphin-2_6-phi-2.Q4_K_M.gguf\"\
          \r\n\r\nDo I need to install a specific branch or something? \r\n\r\nThanks\
          \ in advance"
        updatedAt: '2023-12-27T12:24:17.438Z'
      numEdits: 0
      reactions: []
    id: 658c17715a8f8a309e83fdfd
    type: comment
  author: egodos
  content: "Hello all, \r\n\r\nI install in windows successfully:\r\nCMAKE_ARGS=\"\
    -DLLAMA_CUBLAS=on\" \r\n!pip install llama-cpp-python\r\n\r\nBut when loading\
    \ the model i get: \"ValueError: Model path does not exist: ./dolphin-2_6-phi-2.Q4_K_M.gguf\"\
    \r\n\r\nDo I need to install a specific branch or something? \r\n\r\nThanks in\
    \ advance"
  created_at: 2023-12-27 12:24:17+00:00
  edited: false
  hidden: false
  id: 658c17715a8f8a309e83fdfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-04T16:29:31.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9291098713874817
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>We may need more info then what you have supplied. My best guess
          is that is sounds like you may of supplied a path that doesn''t work to
          your llama-cpp-python script/tool. Double check that the file is where you
          expect it, and that the path is correct that is intended to be supplied
          to the loading part of your python script/tool. Note: I''m assuming that
          your possibly manually scripting in python to use the python to c/c++ bindings
          library functions from the python library llama-cpp-python. (<a rel="nofollow"
          href="https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama">https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama</a>)
          If you are not, then you likely need to consult the authors of what ever
          you are using to load and do inference with the model.</p>

          '
        raw: 'We may need more info then what you have supplied. My best guess is
          that is sounds like you may of supplied a path that doesn''t work to your
          llama-cpp-python script/tool. Double check that the file is where you expect
          it, and that the path is correct that is intended to be supplied to the
          loading part of your python script/tool. Note: I''m assuming that your possibly
          manually scripting in python to use the python to c/c++ bindings library
          functions from the python library llama-cpp-python. (https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama)
          If you are not, then you likely need to consult the authors of what ever
          you are using to load and do inference with the model.'
        updatedAt: '2024-01-04T16:29:31.492Z'
      numEdits: 0
      reactions: []
    id: 6596dceb70211b12ce0e28d6
    type: comment
  author: dyoung
  content: 'We may need more info then what you have supplied. My best guess is that
    is sounds like you may of supplied a path that doesn''t work to your llama-cpp-python
    script/tool. Double check that the file is where you expect it, and that the path
    is correct that is intended to be supplied to the loading part of your python
    script/tool. Note: I''m assuming that your possibly manually scripting in python
    to use the python to c/c++ bindings library functions from the python library
    llama-cpp-python. (https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama)
    If you are not, then you likely need to consult the authors of what ever you are
    using to load and do inference with the model.'
  created_at: 2024-01-04 16:29:31+00:00
  edited: false
  hidden: false
  id: 6596dceb70211b12ce0e28d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/dolphin-2_6-phi-2-GGUF
repo_type: model
status: open
target_branch: null
title: Model not found
