!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-12-27 11:43:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-27T11:43:16.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9665610790252686
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>One application of quantized models I''m interested in is long context
          inference. Will you make a 2 bit Yi-34b-200k?<br>I think that would be great
          for few shot learning over long contexts! </p>

          '
        raw: "One application of quantized models I'm interested in is long context\
          \ inference. Will you make a 2 bit Yi-34b-200k? \r\nI think that would be\
          \ great for few shot learning over long contexts! "
        updatedAt: '2023-12-27T11:43:16.780Z'
      numEdits: 0
      reactions: []
    id: 658c0dd4bc3644bd23918477
    type: comment
  author: KnutJaegersberg
  content: "One application of quantized models I'm interested in is long context\
    \ inference. Will you make a 2 bit Yi-34b-200k? \r\nI think that would be great\
    \ for few shot learning over long contexts! "
  created_at: 2023-12-27 11:43:16+00:00
  edited: false
  hidden: false
  id: 658c0dd4bc3644bd23918477
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e524712def4184fdd92384c00436deb3.svg
      fullname: Nianhui
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: NicoNico
      type: user
    createdAt: '2023-12-27T12:34:59.000Z'
    data:
      edited: true
      editors:
      - NicoNico
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532835483551025
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e524712def4184fdd92384c00436deb3.svg
          fullname: Nianhui
          isHf: false
          isPro: false
          name: NicoNico
          type: user
        html: '<p>Thanks for your continuous attention to this work. The long-context
          model is in the plan and by now we pay more attention to the lossless  (&lt;1%)
          quantization under lower bits (e.g., 2-bit, which is already quite close
          to our expectation).  Once we reach that point, we will release more compressed
          models to the community step by step.  </p>

          <p>If you can share some meaningful metrics about how can we measure the
          performance of "lossless" compressed long-context models, that would be
          ultra helpful for our current research, thanks.</p>

          '
        raw: "Thanks for your continuous attention to this work. The long-context\
          \ model is in the plan and by now we pay more attention to the lossless\
          \  (<1%) quantization under lower bits (e.g., 2-bit, which is already quite\
          \ close to our expectation).  Once we reach that point, we will release\
          \ more compressed models to the community step by step.  \n\nIf you can\
          \ share some meaningful metrics about how can we measure the performance\
          \ of \"lossless\" compressed long-context models, that would be ultra helpful\
          \ for our current research, thanks."
        updatedAt: '2023-12-27T12:49:17.856Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
    id: 658c19f37d3d5b1638d91cab
    type: comment
  author: NicoNico
  content: "Thanks for your continuous attention to this work. The long-context model\
    \ is in the plan and by now we pay more attention to the lossless  (<1%) quantization\
    \ under lower bits (e.g., 2-bit, which is already quite close to our expectation).\
    \  Once we reach that point, we will release more compressed models to the community\
    \ step by step.  \n\nIf you can share some meaningful metrics about how can we\
    \ measure the performance of \"lossless\" compressed long-context models, that\
    \ would be ultra helpful for our current research, thanks."
  created_at: 2023-12-27 12:34:59+00:00
  edited: true
  hidden: false
  id: 658c19f37d3d5b1638d91cab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-27T12:59:12.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8849812150001526
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>I can refer you to the works on long context window LLMs that are
          already around. </p>

          <p>Yarn uses perplexity and compares the perplexity of different models
          at different context lengths:<br><a href="https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k">https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k</a></p>

          <p>Giraffe makes use of several QA benchmarks:<br><a rel="nofollow" href="https://github.com/abacusai/long-context">https://github.com/abacusai/long-context</a></p>

          <p>longllama assesses improvement as passkey retrieval, QA over research
          papers and the improvement in few shot learning from using longer context
          few shot instances:<br><a href="https://huggingface.co/syzymon/long_llama_3b">https://huggingface.co/syzymon/long_llama_3b</a></p>

          <p>longlora additionally uses topic retrieval over long contexts as evaluation</p>

          <p><a rel="nofollow" href="https://arxiv.org/abs/2309.12307">https://arxiv.org/abs/2309.12307</a></p>

          '
        raw: "I can refer you to the works on long context window LLMs that are already\
          \ around. \n\nYarn uses perplexity and compares the perplexity of different\
          \ models at different context lengths: \nhttps://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k\n\
          \nGiraffe makes use of several QA benchmarks: \nhttps://github.com/abacusai/long-context\n\
          \nlongllama assesses improvement as passkey retrieval, QA over research\
          \ papers and the improvement in few shot learning from using longer context\
          \ few shot instances:\nhttps://huggingface.co/syzymon/long_llama_3b\n\n\
          longlora additionally uses topic retrieval over long contexts as evaluation\n\
          \nhttps://arxiv.org/abs/2309.12307"
        updatedAt: '2023-12-27T12:59:12.877Z'
      numEdits: 0
      reactions: []
      relatedEventId: 658c1fa0e7f846313cf6ad8c
    id: 658c1fa0e7f846313cf6ad89
    type: comment
  author: KnutJaegersberg
  content: "I can refer you to the works on long context window LLMs that are already\
    \ around. \n\nYarn uses perplexity and compares the perplexity of different models\
    \ at different context lengths: \nhttps://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k\n\
    \nGiraffe makes use of several QA benchmarks: \nhttps://github.com/abacusai/long-context\n\
    \nlongllama assesses improvement as passkey retrieval, QA over research papers\
    \ and the improvement in few shot learning from using longer context few shot\
    \ instances:\nhttps://huggingface.co/syzymon/long_llama_3b\n\nlonglora additionally\
    \ uses topic retrieval over long contexts as evaluation\n\nhttps://arxiv.org/abs/2309.12307"
  created_at: 2023-12-27 12:59:12+00:00
  edited: false
  hidden: false
  id: 658c1fa0e7f846313cf6ad89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-27T12:59:12.000Z'
    data:
      status: closed
    id: 658c1fa0e7f846313cf6ad8c
    type: status-change
  author: KnutJaegersberg
  created_at: 2023-12-27 12:59:12+00:00
  id: 658c1fa0e7f846313cf6ad8c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: GreenBitAI/yi-34b-w4a16g32
repo_type: model
status: closed
target_branch: null
title: 200k model
