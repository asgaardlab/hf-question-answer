!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-25 04:03:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-25T04:03:08.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8515531420707703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;LoneStriker&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/LoneStriker\">@<span class=\"underline\"\
          >LoneStriker</span></a></span>\n\n\t</span></span> </p>\n<p>Hi, checkout\
          \ out this quant method which have best performance to model size even compare\
          \ to  GPTQ <a rel=\"nofollow\" href=\"https://github.com/GreenBitAI/low_bit_llama\"\
          >https://github.com/GreenBitAI/low_bit_llama</a></p>\n"
        raw: "@TheBloke @LoneStriker \r\n\r\nHi, checkout out this quant method which\
          \ have best performance to model size even compare to  GPTQ https://github.com/GreenBitAI/low_bit_llama"
        updatedAt: '2023-12-25T04:03:08.720Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dillfrescott
    id: 6588fefc126b8d7eaed9f09e
    type: comment
  author: Yhyu13
  content: "@TheBloke @LoneStriker \r\n\r\nHi, checkout out this quant method which\
    \ have best performance to model size even compare to  GPTQ https://github.com/GreenBitAI/low_bit_llama"
  created_at: 2023-12-25 04:03:08+00:00
  edited: false
  hidden: false
  id: 6588fefc126b8d7eaed9f09e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-25T04:58:43.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8759679794311523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p>Thank you for the pointer to this new quant method. I had not heard\
          \ of it before.  Have you compared the perplexity to llama.cpp's GGUF and\
          \ Exllamav2's exl2 models at the same model sizes?  It would help adoption\
          \ of any new quantization method if there were equivalent measurements done\
          \ of both GreenBitAI's quantizations vs. llama.cpp and Exllamav2.</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> creates llama.cpp\
          \ models at sizes ranging from Q2 to Q8, Yi models here:<br><a href=\"https://huggingface.co/TheBloke/Yi-34B-GGUF/tree/main\"\
          >https://huggingface.co/TheBloke/Yi-34B-GGUF/tree/main</a></p>\n<p>I've\
          \ generated quants from 3.0 bpw to 8.0 bpw here:<br><a href=\"https://huggingface.co/models?p=2&amp;sort=created&amp;search=lonestriker%2Fyi-34b\"\
          >https://huggingface.co/models?p=2&amp;sort=created&amp;search=lonestriker%2Fyi-34b</a></p>\n\
          <p>exllamav2 is also capable of fractional bit-quantization because the\
          \ stated bitrate is an average over the layers.  A popular low-bit quant\
          \ size for Exllamav2 is 2.4 bpw (especially for 70B models.)  At 2.4 bpw,\
          \ a 70B model fits in a single 3090 or 4090 with full contxt.  The model\
          \ size is only 20 GB at 2.4 bpw and 22 GB at 2.65 bpw.  Example models here:<br><a\
          \ href=\"https://huggingface.co/models?sort=modified&amp;search=lonestriker+2.4bpw+70b\"\
          >https://huggingface.co/models?sort=modified&amp;search=lonestriker+2.4bpw+70b</a></p>\n\
          <p>And for Mixtral, at 3.0 bpw, the model will run entirely in a single\
          \ 3090 or 4090 at full 32K context:<br><a href=\"https://huggingface.co/models?sort=modified&amp;search=lonestriker+3.0bpw+8x7\"\
          >https://huggingface.co/models?sort=modified&amp;search=lonestriker+3.0bpw+8x7</a></p>\n"
        raw: 'Thank you for the pointer to this new quant method. I had not heard
          of it before.  Have you compared the perplexity to llama.cpp''s GGUF and
          Exllamav2''s exl2 models at the same model sizes?  It would help adoption
          of any new quantization method if there were equivalent measurements done
          of both GreenBitAI''s quantizations vs. llama.cpp and Exllamav2.


          @TheBloke creates llama.cpp models at sizes ranging from Q2 to Q8, Yi models
          here:

          https://huggingface.co/TheBloke/Yi-34B-GGUF/tree/main


          I''ve generated quants from 3.0 bpw to 8.0 bpw here:

          https://huggingface.co/models?p=2&sort=created&search=lonestriker%2Fyi-34b


          exllamav2 is also capable of fractional bit-quantization because the stated
          bitrate is an average over the layers.  A popular low-bit quant size for
          Exllamav2 is 2.4 bpw (especially for 70B models.)  At 2.4 bpw, a 70B model
          fits in a single 3090 or 4090 with full contxt.  The model size is only
          20 GB at 2.4 bpw and 22 GB at 2.65 bpw.  Example models here:

          https://huggingface.co/models?sort=modified&search=lonestriker+2.4bpw+70b


          And for Mixtral, at 3.0 bpw, the model will run entirely in a single 3090
          or 4090 at full 32K context:

          https://huggingface.co/models?sort=modified&search=lonestriker+3.0bpw+8x7

          '
        updatedAt: '2023-12-25T04:58:43.862Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 65890c0315b65eb9baf99dd2
    type: comment
  author: LoneStriker
  content: 'Thank you for the pointer to this new quant method. I had not heard of
    it before.  Have you compared the perplexity to llama.cpp''s GGUF and Exllamav2''s
    exl2 models at the same model sizes?  It would help adoption of any new quantization
    method if there were equivalent measurements done of both GreenBitAI''s quantizations
    vs. llama.cpp and Exllamav2.


    @TheBloke creates llama.cpp models at sizes ranging from Q2 to Q8, Yi models here:

    https://huggingface.co/TheBloke/Yi-34B-GGUF/tree/main


    I''ve generated quants from 3.0 bpw to 8.0 bpw here:

    https://huggingface.co/models?p=2&sort=created&search=lonestriker%2Fyi-34b


    exllamav2 is also capable of fractional bit-quantization because the stated bitrate
    is an average over the layers.  A popular low-bit quant size for Exllamav2 is
    2.4 bpw (especially for 70B models.)  At 2.4 bpw, a 70B model fits in a single
    3090 or 4090 with full contxt.  The model size is only 20 GB at 2.4 bpw and 22
    GB at 2.65 bpw.  Example models here:

    https://huggingface.co/models?sort=modified&search=lonestriker+2.4bpw+70b


    And for Mixtral, at 3.0 bpw, the model will run entirely in a single 3090 or 4090
    at full 32K context:

    https://huggingface.co/models?sort=modified&search=lonestriker+3.0bpw+8x7

    '
  created_at: 2023-12-25 04:58:43+00:00
  edited: false
  hidden: false
  id: 65890c0315b65eb9baf99dd2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: GreenBitAI/yi-34b-w4a16g32
repo_type: model
status: open
target_branch: null
title: New exciting quant method
