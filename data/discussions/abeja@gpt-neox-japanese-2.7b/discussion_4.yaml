!!python/object:huggingface_hub.community.DiscussionWithDetails
author: oshizo
conflicting_files: null
created_at: 2023-05-14 12:13:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5cf127b4837adccf445b636cf8b18c0.svg
      fullname: oshizo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oshizo
      type: user
    createdAt: '2023-05-14T13:13:46.000Z'
    data:
      edited: false
      editors:
      - oshizo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5cf127b4837adccf445b636cf8b18c0.svg
          fullname: oshizo
          isHf: false
          isPro: false
          name: oshizo
          type: user
        html: '<p>Thank you for publishing such a wonderful model.<br>I am experiencing
          an issue where setting gradient_checkpointing=True in the TrainingArguments
          does not seem to reduce the VRAM usage during training.</p>

          <p>Though my understanding may not be thorough, when I compare the source
          code of modeling_gpt_neox.py with modeling_gpt_neox_japanese.py, it appears
          that the latter does not include the conditional statement concerning self.gradient_checkpointing
          as seen here:<br><a rel="nofollow" href="https://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/models/gpt_neox/modeling_gpt_neox.py#L546">https://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/models/gpt_neox/modeling_gpt_neox.py#L546</a></p>

          <p>Is this an intentional modification or perhaps an oversight? I would
          appreciate any insights you might have regarding this.</p>

          <p>transformers v4.29.1</p>

          '
        raw: "Thank you for publishing such a wonderful model. \r\nI am experiencing\
          \ an issue where setting gradient_checkpointing=True in the TrainingArguments\
          \ does not seem to reduce the VRAM usage during training.\r\n\r\nThough\
          \ my understanding may not be thorough, when I compare the source code of\
          \ modeling_gpt_neox.py with modeling_gpt_neox_japanese.py, it appears that\
          \ the latter does not include the conditional statement concerning self.gradient_checkpointing\
          \ as seen here:\r\nhttps://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/models/gpt_neox/modeling_gpt_neox.py#L546\r\
          \n\r\nIs this an intentional modification or perhaps an oversight? I would\
          \ appreciate any insights you might have regarding this.\r\n\r\ntransformers\
          \ v4.29.1"
        updatedAt: '2023-05-14T13:13:46.261Z'
      numEdits: 0
      reactions: []
    id: 6460de8a96259bec21d11170
    type: comment
  author: oshizo
  content: "Thank you for publishing such a wonderful model. \r\nI am experiencing\
    \ an issue where setting gradient_checkpointing=True in the TrainingArguments\
    \ does not seem to reduce the VRAM usage during training.\r\n\r\nThough my understanding\
    \ may not be thorough, when I compare the source code of modeling_gpt_neox.py\
    \ with modeling_gpt_neox_japanese.py, it appears that the latter does not include\
    \ the conditional statement concerning self.gradient_checkpointing as seen here:\r\
    \nhttps://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/models/gpt_neox/modeling_gpt_neox.py#L546\r\
    \n\r\nIs this an intentional modification or perhaps an oversight? I would appreciate\
    \ any insights you might have regarding this.\r\n\r\ntransformers v4.29.1"
  created_at: 2023-05-14 12:13:46+00:00
  edited: false
  hidden: false
  id: 6460de8a96259bec21d11170
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657867898781-noauth.jpeg?w=200&h=200&f=face
      fullname: Shinya Otani
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SO0529
      type: user
    createdAt: '2023-05-18T00:18:48.000Z'
    data:
      edited: false
      editors:
      - SO0529
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657867898781-noauth.jpeg?w=200&h=200&f=face
          fullname: Shinya Otani
          isHf: false
          isPro: false
          name: SO0529
          type: user
        html: '<p>Thanks for looking at all the details and asking the question.<br>The
          difference regarding gradient_checkpointing is not intentional. At the time
          we submitted our pull request, GPT NeoX had the same configuration, but
          the gradient checkpointing has been corrected in the following commit, and
          the difference is now in place.<br><a rel="nofollow" href="https://github.com/huggingface/transformers/commit/225c36fbe5ae2bdb1880da52e093c7e53596a7d1">https://github.com/huggingface/transformers/commit/225c36fbe5ae2bdb1880da52e093c7e53596a7d1</a></p>

          '
        raw: 'Thanks for looking at all the details and asking the question.

          The difference regarding gradient_checkpointing is not intentional. At the
          time we submitted our pull request, GPT NeoX had the same configuration,
          but the gradient checkpointing has been corrected in the following commit,
          and the difference is now in place.

          https://github.com/huggingface/transformers/commit/225c36fbe5ae2bdb1880da52e093c7e53596a7d1'
        updatedAt: '2023-05-18T00:18:48.852Z'
      numEdits: 0
      reactions: []
    id: 64656ee8e8e31202cb547f05
    type: comment
  author: SO0529
  content: 'Thanks for looking at all the details and asking the question.

    The difference regarding gradient_checkpointing is not intentional. At the time
    we submitted our pull request, GPT NeoX had the same configuration, but the gradient
    checkpointing has been corrected in the following commit, and the difference is
    now in place.

    https://github.com/huggingface/transformers/commit/225c36fbe5ae2bdb1880da52e093c7e53596a7d1'
  created_at: 2023-05-17 23:18:48+00:00
  edited: false
  hidden: false
  id: 64656ee8e8e31202cb547f05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5cf127b4837adccf445b636cf8b18c0.svg
      fullname: oshizo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oshizo
      type: user
    createdAt: '2023-05-20T07:39:33.000Z'
    data:
      edited: false
      editors:
      - oshizo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5cf127b4837adccf445b636cf8b18c0.svg
          fullname: oshizo
          isHf: false
          isPro: false
          name: oshizo
          type: user
        html: '<p>Thank you for your response! I now understand the situation.<br>It
          might be helpful if you could incorporate the support for gradient_checkpointing
          or provide a warning when this flag is set to True.</p>

          '
        raw: 'Thank you for your response! I now understand the situation.

          It might be helpful if you could incorporate the support for gradient_checkpointing
          or provide a warning when this flag is set to True.'
        updatedAt: '2023-05-20T07:39:33.534Z'
      numEdits: 0
      reactions: []
    id: 6468793599182de17841d39e
    type: comment
  author: oshizo
  content: 'Thank you for your response! I now understand the situation.

    It might be helpful if you could incorporate the support for gradient_checkpointing
    or provide a warning when this flag is set to True.'
  created_at: 2023-05-20 06:39:33+00:00
  edited: false
  hidden: false
  id: 6468793599182de17841d39e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657867898781-noauth.jpeg?w=200&h=200&f=face
      fullname: Shinya Otani
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SO0529
      type: user
    createdAt: '2023-05-22T04:13:10.000Z'
    data:
      edited: false
      editors:
      - SO0529
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657867898781-noauth.jpeg?w=200&h=200&f=face
          fullname: Shinya Otani
          isHf: false
          isPro: false
          name: SO0529
          type: user
        html: '<p>We cannot promise a completion date, but we have started preparing
          for PR. Thank you for reminding up of the update opportunity!</p>

          '
        raw: We cannot promise a completion date, but we have started preparing for
          PR. Thank you for reminding up of the update opportunity!
        updatedAt: '2023-05-22T04:13:10.746Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - oshizo
    id: 646aebd63721aab2edffce05
    type: comment
  author: SO0529
  content: We cannot promise a completion date, but we have started preparing for
    PR. Thank you for reminding up of the update opportunity!
  created_at: 2023-05-22 03:13:10+00:00
  edited: false
  hidden: false
  id: 646aebd63721aab2edffce05
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: abeja/gpt-neox-japanese-2.7b
repo_type: model
status: open
target_branch: null
title: Does gradient checkpointing work with this model?
