!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kernel
conflicting_files: null
created_at: 2023-05-06 02:05:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/624ae13993d46cf4a0928032/kaIXTCGMbkuxuT59p0NAO.jpeg?w=200&h=200&f=face
      fullname: Panic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kernel
      type: user
    createdAt: '2023-05-06T03:05:59.000Z'
    data:
      edited: false
      editors:
      - Kernel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/624ae13993d46cf4a0928032/kaIXTCGMbkuxuT59p0NAO.jpeg?w=200&h=200&f=face
          fullname: Panic
          isHf: false
          isPro: false
          name: Kernel
          type: user
        html: '<p>Any plans for 8bit quantized model? I see that you don''t make such
          models, why so?  I think this is the best for GPU usage</p>

          '
        raw: Any plans for 8bit quantized model? I see that you don't make such models,
          why so?  I think this is the best for GPU usage
        updatedAt: '2023-05-06T03:05:59.659Z'
      numEdits: 0
      reactions: []
    id: 6455c41783fee1965c1865bc
    type: comment
  author: Kernel
  content: Any plans for 8bit quantized model? I see that you don't make such models,
    why so?  I think this is the best for GPU usage
  created_at: 2023-05-06 02:05:59+00:00
  edited: false
  hidden: false
  id: 6455c41783fee1965c1865bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-06T13:31:07.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>GGML models are CPU only, so GPU isn''t involved.</p>

          <p>I''ve never bothered with q8 because q5_1 is already so incredibly close
          to fp16, that there didn''t seem any point.</p>

          <p>Here''s the quantisation table from the README of llama.cpp:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tdLan0ePMKEarSIqnUurj.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tdLan0ePMKEarSIqnUurj.png"></a></p>

          <p>On a 13B model like this, fp16 scores 5.2455 and q5_1 is 5.2582.  That''s
          a difference of 0.24%.  Q8 scores 5.2458, which is 0.05% ''worse'' than
          FP16.  So it is better than q5_1, but is anyone ever really going to be
          able to spot the difference  between 0.24% higher perplexity vs 0.05% higher?</p>

          <p>So that''s why I never bothered.  Then again I guess I could upload them
          just for completeness!  It''s not like it uses any disk space for me once
          I''ve uploaded them - that''s on HF :)</p>

          <p>OK next time I do a model I''ll do q8 as well, and maybe I''ll add some
          q8''s for the last couple of models I did as well.</p>

          '
        raw: 'GGML models are CPU only, so GPU isn''t involved.


          I''ve never bothered with q8 because q5_1 is already so incredibly close
          to fp16, that there didn''t seem any point.


          Here''s the quantisation table from the README of llama.cpp:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tdLan0ePMKEarSIqnUurj.png)


          On a 13B model like this, fp16 scores 5.2455 and q5_1 is 5.2582.  That''s
          a difference of 0.24%.  Q8 scores 5.2458, which is 0.05% ''worse'' than
          FP16.  So it is better than q5_1, but is anyone ever really going to be
          able to spot the difference  between 0.24% higher perplexity vs 0.05% higher?


          So that''s why I never bothered.  Then again I guess I could upload them
          just for completeness!  It''s not like it uses any disk space for me once
          I''ve uploaded them - that''s on HF :)


          OK next time I do a model I''ll do q8 as well, and maybe I''ll add some
          q8''s for the last couple of models I did as well.'
        updatedAt: '2023-05-06T13:31:53.850Z'
      numEdits: 1
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - mirek190
        - eucdee
        - dzupin
        - nulled
        - Kwissbeats
        - mikeee
    id: 6456569bcd6567f52fb47248
    type: comment
  author: TheBloke
  content: 'GGML models are CPU only, so GPU isn''t involved.


    I''ve never bothered with q8 because q5_1 is already so incredibly close to fp16,
    that there didn''t seem any point.


    Here''s the quantisation table from the README of llama.cpp:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tdLan0ePMKEarSIqnUurj.png)


    On a 13B model like this, fp16 scores 5.2455 and q5_1 is 5.2582.  That''s a difference
    of 0.24%.  Q8 scores 5.2458, which is 0.05% ''worse'' than FP16.  So it is better
    than q5_1, but is anyone ever really going to be able to spot the difference  between
    0.24% higher perplexity vs 0.05% higher?


    So that''s why I never bothered.  Then again I guess I could upload them just
    for completeness!  It''s not like it uses any disk space for me once I''ve uploaded
    them - that''s on HF :)


    OK next time I do a model I''ll do q8 as well, and maybe I''ll add some q8''s
    for the last couple of models I did as well.'
  created_at: 2023-05-06 12:31:07+00:00
  edited: true
  hidden: false
  id: 6456569bcd6567f52fb47248
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/GPT4All-13B-snoozy-GGML
repo_type: model
status: open
target_branch: null
title: 8bit quantization
