!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RedXeol
conflicting_files: null
created_at: 2023-05-05 14:53:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-05T15:53:16.000Z'
    data:
      edited: true
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>Hello, I love your work, I would like to ask you if you use AutoGPTQ
          and what hardware should I have to be able to use it?<br>my goal is to create
          a GPTQ-4bit-128g of a GPT-J 6b model to be able to use it in oobabooga.</p>

          <p>Do you think that with my current PC configuration I can achieve something
          like this?<br>ram: 32GB<br>CPU: i7 10700<br>Gpu: nvidia rtx 3060 12GB</p>

          <p>If you know of any code that can make my life easier, I would appreciate
          it.</p>

          '
        raw: 'Hello, I love your work, I would like to ask you if you use AutoGPTQ
          and what hardware should I have to be able to use it?

          my goal is to create a GPTQ-4bit-128g of a GPT-J 6b model to be able to
          use it in oobabooga.


          Do you think that with my current PC configuration I can achieve something
          like this?

          ram: 32GB

          CPU: i7 10700

          Gpu: nvidia rtx 3060 12GB


          If you know of any code that can make my life easier, I would appreciate
          it.'
        updatedAt: '2023-05-05T15:54:04.975Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - RedXeol
    id: 6455266cc9c0dcc8c2489acc
    type: comment
  author: RedXeol
  content: 'Hello, I love your work, I would like to ask you if you use AutoGPTQ and
    what hardware should I have to be able to use it?

    my goal is to create a GPTQ-4bit-128g of a GPT-J 6b model to be able to use it
    in oobabooga.


    Do you think that with my current PC configuration I can achieve something like
    this?

    ram: 32GB

    CPU: i7 10700

    Gpu: nvidia rtx 3060 12GB


    If you know of any code that can make my life easier, I would appreciate it.'
  created_at: 2023-05-05 14:53:16+00:00
  edited: true
  hidden: false
  id: 6455266cc9c0dcc8c2489acc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T16:08:35.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I do not currently use AutoGPTQ to make these models, because before
          I do that I want to do an evaluation on the best dataset to use, and compare
          results with GPTQ-for-LLaMa.</p>

          <p>But yes I am using AutoGPTQ very regularly now for testing GPTQ inference.
          And I am trying to help make AutoGPTQ be the new standard for GPTQ, replacing
          GPTQ-for-LLaMa.  You will see I am posting quite a lot in <a rel="nofollow"
          href="https://github.com/PanQiWei/AutoGPTQ">https://github.com/PanQiWei/AutoGPTQ</a>
          at the moment.</p>

          <p>Regarding GPT-J: this is something I''ve not looked at yet. I''ve not
          quantised any GPT-J models because GPTQ-for-LLaMa doesn''t support them
          well.  AutoGPTQ should work for this, I''ve just not tested it yet.</p>

          <p>I think your system will be fine for both quantising and inference of
          a 6B model.</p>

          <p>I will warn you that AutoGPTQ is still in quite an early state and there
          are bugs and issues at the moment.  For example the example code in the
          README (which quantises an OPT model) currently produces bad output :)  So
          you might need to wait a few more days for it to be stable.</p>

          <p>But give it a go and see what happens and let me know if you encounter
          problems. And if you''ve found a bug, post it in  <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/issues">https://github.com/PanQiWei/AutoGPTQ/issues</a></p>

          '
        raw: 'I do not currently use AutoGPTQ to make these models, because before
          I do that I want to do an evaluation on the best dataset to use, and compare
          results with GPTQ-for-LLaMa.


          But yes I am using AutoGPTQ very regularly now for testing GPTQ inference.
          And I am trying to help make AutoGPTQ be the new standard for GPTQ, replacing
          GPTQ-for-LLaMa.  You will see I am posting quite a lot in https://github.com/PanQiWei/AutoGPTQ
          at the moment.


          Regarding GPT-J: this is something I''ve not looked at yet. I''ve not quantised
          any GPT-J models because GPTQ-for-LLaMa doesn''t support them well.  AutoGPTQ
          should work for this, I''ve just not tested it yet.


          I think your system will be fine for both quantising and inference of a
          6B model.


          I will warn you that AutoGPTQ is still in quite an early state and there
          are bugs and issues at the moment.  For example the example code in the
          README (which quantises an OPT model) currently produces bad output :)  So
          you might need to wait a few more days for it to be stable.


          But give it a go and see what happens and let me know if you encounter problems.
          And if you''ve found a bug, post it in  https://github.com/PanQiWei/AutoGPTQ/issues'
        updatedAt: '2023-05-05T16:09:00.036Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Sab1
    id: 64552a03f61f10d69dcad114
    type: comment
  author: TheBloke
  content: 'I do not currently use AutoGPTQ to make these models, because before I
    do that I want to do an evaluation on the best dataset to use, and compare results
    with GPTQ-for-LLaMa.


    But yes I am using AutoGPTQ very regularly now for testing GPTQ inference. And
    I am trying to help make AutoGPTQ be the new standard for GPTQ, replacing GPTQ-for-LLaMa.  You
    will see I am posting quite a lot in https://github.com/PanQiWei/AutoGPTQ at the
    moment.


    Regarding GPT-J: this is something I''ve not looked at yet. I''ve not quantised
    any GPT-J models because GPTQ-for-LLaMa doesn''t support them well.  AutoGPTQ
    should work for this, I''ve just not tested it yet.


    I think your system will be fine for both quantising and inference of a 6B model.


    I will warn you that AutoGPTQ is still in quite an early state and there are bugs
    and issues at the moment.  For example the example code in the README (which quantises
    an OPT model) currently produces bad output :)  So you might need to wait a few
    more days for it to be stable.


    But give it a go and see what happens and let me know if you encounter problems.
    And if you''ve found a bug, post it in  https://github.com/PanQiWei/AutoGPTQ/issues'
  created_at: 2023-05-05 15:08:35+00:00
  edited: true
  hidden: false
  id: 64552a03f61f10d69dcad114
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-05T16:15:37.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>Thank you very much, you have given me encouragement, I thought
          it was impossible with my current pc, I will use AutoGPTQ and I will try
          to do my best to achieve my goal, I will let you know if I succeed or if
          I run into some error, I hope it is not a bother for you . Thank you very
          much, I really admire you.</p>

          '
        raw: Thank you very much, you have given me encouragement, I thought it was
          impossible with my current pc, I will use AutoGPTQ and I will try to do
          my best to achieve my goal, I will let you know if I succeed or if I run
          into some error, I hope it is not a bother for you . Thank you very much,
          I really admire you.
        updatedAt: '2023-05-05T16:15:37.943Z'
      numEdits: 0
      reactions: []
    id: 64552ba9f61f10d69dcafc14
    type: comment
  author: RedXeol
  content: Thank you very much, you have given me encouragement, I thought it was
    impossible with my current pc, I will use AutoGPTQ and I will try to do my best
    to achieve my goal, I will let you know if I succeed or if I run into some error,
    I hope it is not a bother for you . Thank you very much, I really admire you.
  created_at: 2023-05-05 15:15:37+00:00
  edited: false
  hidden: false
  id: 64552ba9f61f10d69dcafc14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-05T16:59:07.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: "<p>Sorry for the inconvenience, do you know in the AutoGPTQ example\
          \ file how I indicate to the model that I need a version compatible with\
          \ oobabooga, that is, compat.no-act-order.safetensors<br>line:</p>\n<h1\
          \ id=\"save-quantized-model\">save quantized model</h1>\n<pre><code> model.save_quantized(quantized_model_dir)\n\
          \n # save quantized model using safetensors\n model.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)\n</code></pre>\n"
        raw: "Sorry for the inconvenience, do you know in the AutoGPTQ example file\
          \ how I indicate to the model that I need a version compatible with oobabooga,\
          \ that is, compat.no-act-order.safetensors\nline:\n  # save quantized model\n\
          \     model.save_quantized(quantized_model_dir)\n\n     # save quantized\
          \ model using safetensors\n     model.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)"
        updatedAt: '2023-05-05T16:59:07.638Z'
      numEdits: 0
      reactions: []
    id: 645535dbd55525a4feea7f12
    type: comment
  author: RedXeol
  content: "Sorry for the inconvenience, do you know in the AutoGPTQ example file\
    \ how I indicate to the model that I need a version compatible with oobabooga,\
    \ that is, compat.no-act-order.safetensors\nline:\n  # save quantized model\n\
    \     model.save_quantized(quantized_model_dir)\n\n     # save quantized model\
    \ using safetensors\n     model.save_quantized(quantized_model_dir, use_safetensors=True)"
  created_at: 2023-05-05 15:59:07+00:00
  edited: false
  hidden: false
  id: 645535dbd55525a4feea7f12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T17:57:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>So firstly <code>comat.no-act-order</code> is just my own naming\
          \ convention.  Compat to indicate it's most compatible, and <code>no-act-order</code>\
          \ to indicate it doesn't use the --act-order feature</p>\n<p>Act-order has\
          \ been renamed <code>desc_act</code> in AutoGPTQ.  So if you generate a\
          \ model without desc_act, it should in theory be compatible with older GPTQ-for-LLaMa\
          \ code.  But I've not yet tested this.</p>\n<p>The default is not to use\
          \ desc_act, so you should be fine anyway.</p>\n<p>Whether to use it or not\
          \ is specified in the quantization configuration:</p>\n<pre><code>quantize_config\
          \ = BaseQuantizeConfig(\n    bits=4,  # quantize model to 4-bit\n    group_size=128,\
          \  # it is recommended to set the value to 128\n   desc_act=False\n)\n\n\
          # load un-quantized model, by default, the model will always be loaded into\
          \ CPU memory\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\n# quantize model, the examples should be list of dict\
          \ whose keys can only be \"input_ids\" and \"attention_mask\"\nmodel.quantize(examples,\
          \ use_triton=False)\n\n model.save_quantized(quantized_model_dir, use_safetensors=True)\n\
          </code></pre>\n<p>We specified desc_act=False so it won't use it.  But desc_act=False\
          \ is the default, so it also won't be used if we didn't specifically add\
          \ that to the BaseQuantizeConfig()</p>\n<p>Note that there are currently\
          \ bugs in quantisation. I tested OPT quantisation earlier and the result\
          \ was unusable.  That's being tracked in this bug: <a rel=\"nofollow\" href=\"\
          https://github.com/PanQiWei/AutoGPTQ/issues/52\">https://github.com/PanQiWei/AutoGPTQ/issues/52</a></p>\n\
          <p>That may be specific to OPT and maybe it works on other models. But I'm\
          \ not sure yet.</p>\n<p>By the way, do check out the example scripts. There's\
          \ one called <code>quant_with_alpaca</code> that uses the Alpaca dataset\
          \ as the quantisation examples.  The dataset used may improve the quantisation\
          \ quality. When I quantise with GPTQ-for-LLaMa I currently use <code>c4</code>\
          \ as the quantisation dataset.</p>\n"
        raw: "So firstly `comat.no-act-order` is just my own naming convention.  Compat\
          \ to indicate it's most compatible, and `no-act-order` to indicate it doesn't\
          \ use the --act-order feature\n\nAct-order has been renamed `desc_act` in\
          \ AutoGPTQ.  So if you generate a model without desc_act, it should in theory\
          \ be compatible with older GPTQ-for-LLaMa code.  But I've not yet tested\
          \ this.\n\nThe default is not to use desc_act, so you should be fine anyway.\n\
          \nWhether to use it or not is specified in the quantization configuration:\n\
          ```\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # quantize model\
          \ to 4-bit\n    group_size=128,  # it is recommended to set the value to\
          \ 128\n   desc_act=False\n)\n\n# load un-quantized model, by default, the\
          \ model will always be loaded into CPU memory\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\n# quantize model, the examples should be list of dict\
          \ whose keys can only be \"input_ids\" and \"attention_mask\"\nmodel.quantize(examples,\
          \ use_triton=False)\n\n model.save_quantized(quantized_model_dir, use_safetensors=True)\n\
          ```\n\nWe specified desc_act=False so it won't use it.  But desc_act=False\
          \ is the default, so it also won't be used if we didn't specifically add\
          \ that to the BaseQuantizeConfig()\n\nNote that there are currently bugs\
          \ in quantisation. I tested OPT quantisation earlier and the result was\
          \ unusable.  That's being tracked in this bug: https://github.com/PanQiWei/AutoGPTQ/issues/52\n\
          \nThat may be specific to OPT and maybe it works on other models. But I'm\
          \ not sure yet.\n\nBy the way, do check out the example scripts. There's\
          \ one called `quant_with_alpaca` that uses the Alpaca dataset as the quantisation\
          \ examples.  The dataset used may improve the quantisation quality. When\
          \ I quantise with GPTQ-for-LLaMa I currently use `c4` as the quantisation\
          \ dataset."
        updatedAt: '2023-05-05T17:57:27.099Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - dzupin
        - RedXeol
        - mirek190
        - versae
    id: 64554387a473375be5756f87
    type: comment
  author: TheBloke
  content: "So firstly `comat.no-act-order` is just my own naming convention.  Compat\
    \ to indicate it's most compatible, and `no-act-order` to indicate it doesn't\
    \ use the --act-order feature\n\nAct-order has been renamed `desc_act` in AutoGPTQ.\
    \  So if you generate a model without desc_act, it should in theory be compatible\
    \ with older GPTQ-for-LLaMa code.  But I've not yet tested this.\n\nThe default\
    \ is not to use desc_act, so you should be fine anyway.\n\nWhether to use it or\
    \ not is specified in the quantization configuration:\n```\nquantize_config =\
    \ BaseQuantizeConfig(\n    bits=4,  # quantize model to 4-bit\n    group_size=128,\
    \  # it is recommended to set the value to 128\n   desc_act=False\n)\n\n# load\
    \ un-quantized model, by default, the model will always be loaded into CPU memory\n\
    model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n\
    \n# quantize model, the examples should be list of dict whose keys can only be\
    \ \"input_ids\" and \"attention_mask\"\nmodel.quantize(examples, use_triton=False)\n\
    \n model.save_quantized(quantized_model_dir, use_safetensors=True)\n```\n\nWe\
    \ specified desc_act=False so it won't use it.  But desc_act=False is the default,\
    \ so it also won't be used if we didn't specifically add that to the BaseQuantizeConfig()\n\
    \nNote that there are currently bugs in quantisation. I tested OPT quantisation\
    \ earlier and the result was unusable.  That's being tracked in this bug: https://github.com/PanQiWei/AutoGPTQ/issues/52\n\
    \nThat may be specific to OPT and maybe it works on other models. But I'm not\
    \ sure yet.\n\nBy the way, do check out the example scripts. There's one called\
    \ `quant_with_alpaca` that uses the Alpaca dataset as the quantisation examples.\
    \  The dataset used may improve the quantisation quality. When I quantise with\
    \ GPTQ-for-LLaMa I currently use `c4` as the quantisation dataset."
  created_at: 2023-05-05 16:57:27+00:00
  edited: false
  hidden: false
  id: 64554387a473375be5756f87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-05T21:58:42.000Z'
    data:
      edited: true
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: "<p>I did it, thank you very much for your guide... the tool is very\
          \ good, it did not fail me with a GPT-J 6b model<br>First I had to uninstall\
          \ cuda since it was not compatible, also I deleted all traces of torch on\
          \ my pc.<br>Then install CUDA 11.8<br>Then troch compatible with this version\
          \ pip install torch==2.0.0+cu118 torchvision -f <a rel=\"nofollow\" href=\"\
          https://download.pytorch.org/whl/cu118/torch_stable.html\">https://download.pytorch.org/whl/cu118/torch_stable.html</a><br>Then\
          \ download the model locally<br>And finally modify the basic_usage.py code\
          \ like this:<br>import operating system</p>\n<p>from transformers import\
          \ AutoTokenizer, TextGenerationPipeline<br>from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig</p>\n<p>pretrained_model_dir = \"A:/LLMs_LOCAL/caosgpt_j_6B_alpaca/\"\
          <br>quantized_model_dir = \"caosgpt-j-6B-alpaca-4bit-128g\"</p>\n<h1 id=\"\
          osmakedirsquantized_model_dir-exist_oktrue\">os.makedirs(quantized_model_dir,\
          \ exist_ok=True)</h1>\n<p>def main():<br>      tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir,\
          \ use_fast=True)<br>      examples = [<br>          tokenizer(<br>     \
          \         \"auto-gptq is an easy-to-use model quantification library with\
          \ user-friendly APIs, based on the GPTQ algorithm.\"<br>              ),<br>\
          \          tokenizer(<br>              \"Artificial intelligence has advanced\
          \ significantly in recent years.\"<br>              ),<br>          tokenizer(<br>\
          \              \"Model quantization can reduce model size and improve model\
          \ efficiency.\"<br>              ),<br>          tokenizer(<br>        \
          \      \"Quantization algorithms can reduce the amount of memory and power\
          \ required.\"<br>              ),<br>          tokenizer(<br>          \
          \    \"Deep learning is used in a variety of applications, from medicine\
          \ to marketing.\"<br>              ),<br>          tokenizer(<br>      \
          \        \"The GPT-4 architecture is the foundation of many next-generation\
          \ language models.\"<br>              ),<br>          tokenizer(<br>   \
          \           \"Natural language processing allows machines to understand\
          \ and communicate in human languages.\"<br>              ),<br>        \
          \  tokenizer(<br>              \"Convolutional neural networks are used\
          \ in computer vision.\"<br>              ),<br>          tokenizer(<br>\
          \              \"Optimization algorithms are fundamental for training deep\
          \ learning models.\"<br>              ),<br>          tokenizer(<br>   \
          \           \"Reinforcement learning is a machine learning technique in\
          \ which agents learn through interaction with their environment.\"<br> \
          \             )<br>      ]</p>\n<pre><code>  quantify_config = BaseQuantizeConfig(\n\
          \      bits=4, # quantize the model to 4 bits\n      group_size=128, # it\
          \ is recommended to set the value to 128\n      desc_act=False\n  )\n\n\
          \  # load the unquantified model, the model will always be force loaded\
          \ into the CPU\n  model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\n  # quantization model, examples must be a list of\
          \ dict whose keys contain \"input_ids\" and \"attention_mask\"\n  # with\
          \ low value type torch.LongTensor.\n  model.quantize(examples, use_triton=False)\n\
          \n  # save the quantized model\n  model.save_quantized(quantized_model_dir)\n\
          \n  # save the quantized model using security tensors\n  model.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)\n\n  # load quantized model, currently only supports\
          \ cpu or single gpu\n  model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ device=\"cuda:0\", use_triton=False)\n\n  # inference with model.generate\n\
          \  print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"\
          pt\").to(\"cuda:0\"))[0]))\n\n  # or you can also use pipeline\n  pipeline\
          \ = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=\"cuda:0\"\
          )\n  print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\n</code></pre>\n\
          <p>if <strong>name</strong> == \"<strong>main</strong>\":<br>      Import\
          \ registration</p>\n<pre><code>  record.basicConfig(\n      format=\"%(asctime)s\
          \ %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"\
          %Y-%m-%d %H:%M: %S\"\n  )\n\n  major()\n</code></pre>\n<p>finally i added\
          \ the tokenizer again and it works on oobabooga_windows</p>\n<p>Output generated\
          \ in 3.59 seconds (1.39 tokens/s, 5 tokens, context 25, seed 570147804)<br>Output\
          \ generated in 10.13 seconds (9.08 tokens/s, 92 tokens, context 59, seed\
          \ 771109588)<br>Output generated in 11.44 seconds (9.09 tokens/s, 104 tokens,\
          \ context 59, seed 1814752661)<br>Output generated in 23.26 seconds (8.55\
          \ tokens/s, 199 tokens, context 197, seed 873787634)<br>Output generated\
          \ in 8.74 seconds (3.89 tokens/s, 34 tokens, context 423, seed 1563385550)<br>Output\
          \ generated in 11.40 seconds (4.91 tokens/s, 56 tokens, context 497, seed\
          \ 390511124)<br>Output generated in 10.70 seconds (2.99 tokens/s, 32 tokens,\
          \ context 641, seed 1136593747)<br>Output generated in 9.73 seconds (1.44\
          \ tokens/s, 14 tokens, context 702, seed 1881813284)<br>So far it works\
          \ fine, I'll keep trying it... thank you very much</p>\n"
        raw: "I did it, thank you very much for your guide... the tool is very good,\
          \ it did not fail me with a GPT-J 6b model\nFirst I had to uninstall cuda\
          \ since it was not compatible, also I deleted all traces of torch on my\
          \ pc.\nThen install CUDA 11.8\nThen troch compatible with this version pip\
          \ install torch==2.0.0+cu118 torchvision -f https://download.pytorch.org/whl/cu118/torch_stable.html\n\
          Then download the model locally\nAnd finally modify the basic_usage.py code\
          \ like this:\nimport operating system\n\nfrom transformers import AutoTokenizer,\
          \ TextGenerationPipeline\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          \n\npretrained_model_dir = \"A:/LLMs_LOCAL/caosgpt_j_6B_alpaca/\"\nquantized_model_dir\
          \ = \"caosgpt-j-6B-alpaca-4bit-128g\"\n\n# os.makedirs(quantized_model_dir,\
          \ exist_ok=True)\n\n\ndef main():\n      tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir,\
          \ use_fast=True)\n      examples = [\n          tokenizer(\n           \
          \   \"auto-gptq is an easy-to-use model quantification library with user-friendly\
          \ APIs, based on the GPTQ algorithm.\"\n              ),\n          tokenizer(\n\
          \              \"Artificial intelligence has advanced significantly in recent\
          \ years.\"\n              ),\n          tokenizer(\n              \"Model\
          \ quantization can reduce model size and improve model efficiency.\"\n \
          \             ),\n          tokenizer(\n              \"Quantization algorithms\
          \ can reduce the amount of memory and power required.\"\n              ),\n\
          \          tokenizer(\n              \"Deep learning is used in a variety\
          \ of applications, from medicine to marketing.\"\n              ),\n   \
          \       tokenizer(\n              \"The GPT-4 architecture is the foundation\
          \ of many next-generation language models.\"\n              ),\n       \
          \   tokenizer(\n              \"Natural language processing allows machines\
          \ to understand and communicate in human languages.\"\n              ),\n\
          \          tokenizer(\n              \"Convolutional neural networks are\
          \ used in computer vision.\"\n              ),\n          tokenizer(\n \
          \             \"Optimization algorithms are fundamental for training deep\
          \ learning models.\"\n              ),\n          tokenizer(\n         \
          \     \"Reinforcement learning is a machine learning technique in which\
          \ agents learn through interaction with their environment.\"\n         \
          \     )\n      ]\n\n      quantify_config = BaseQuantizeConfig(\n      \
          \    bits=4, # quantize the model to 4 bits\n          group_size=128, #\
          \ it is recommended to set the value to 128\n          desc_act=False\n\
          \      )\n\n      # load the unquantified model, the model will always be\
          \ force loaded into the CPU\n      model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\n      # quantization model, examples must be a list\
          \ of dict whose keys contain \"input_ids\" and \"attention_mask\"\n    \
          \  # with low value type torch.LongTensor.\n      model.quantize(examples,\
          \ use_triton=False)\n\n      # save the quantized model\n      model.save_quantized(quantized_model_dir)\n\
          \n      # save the quantized model using security tensors\n      model.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)\n\n      # load quantized model, currently only\
          \ supports cpu or single gpu\n      model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ device=\"cuda:0\", use_triton=False)\n\n      # inference with model.generate\n\
          \      print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\"\
          , return_tensors=\"pt\").to(\"cuda:0\"))[0]))\n\n      # or you can also\
          \ use pipeline\n      pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer,\
          \ device=\"cuda:0\")\n      print(pipeline(\"auto-gptq is\")[0][\"generated_text\"\
          ])\n\n\nif __name__ == \"__main__\":\n      Import registration\n\n    \
          \  record.basicConfig(\n          format=\"%(asctime)s %(levelname)s [%(name)s]\
          \ %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M: %S\"\n  \
          \    )\n\n      major()\n\nfinally i added the tokenizer again and it works\
          \ on oobabooga_windows\n\nOutput generated in 3.59 seconds (1.39 tokens/s,\
          \ 5 tokens, context 25, seed 570147804)\nOutput generated in 10.13 seconds\
          \ (9.08 tokens/s, 92 tokens, context 59, seed 771109588)\nOutput generated\
          \ in 11.44 seconds (9.09 tokens/s, 104 tokens, context 59, seed 1814752661)\n\
          Output generated in 23.26 seconds (8.55 tokens/s, 199 tokens, context 197,\
          \ seed 873787634)\nOutput generated in 8.74 seconds (3.89 tokens/s, 34 tokens,\
          \ context 423, seed 1563385550)\nOutput generated in 11.40 seconds (4.91\
          \ tokens/s, 56 tokens, context 497, seed 390511124)\nOutput generated in\
          \ 10.70 seconds (2.99 tokens/s, 32 tokens, context 641, seed 1136593747)\n\
          Output generated in 9.73 seconds (1.44 tokens/s, 14 tokens, context 702,\
          \ seed 1881813284)\nSo far it works fine, I'll keep trying it... thank you\
          \ very much"
        updatedAt: '2023-05-05T22:53:06.085Z'
      numEdits: 1
      reactions: []
    id: 64557c12f61f10d69dd10e72
    type: comment
  author: RedXeol
  content: "I did it, thank you very much for your guide... the tool is very good,\
    \ it did not fail me with a GPT-J 6b model\nFirst I had to uninstall cuda since\
    \ it was not compatible, also I deleted all traces of torch on my pc.\nThen install\
    \ CUDA 11.8\nThen troch compatible with this version pip install torch==2.0.0+cu118\
    \ torchvision -f https://download.pytorch.org/whl/cu118/torch_stable.html\nThen\
    \ download the model locally\nAnd finally modify the basic_usage.py code like\
    \ this:\nimport operating system\n\nfrom transformers import AutoTokenizer, TextGenerationPipeline\n\
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\n\npretrained_model_dir\
    \ = \"A:/LLMs_LOCAL/caosgpt_j_6B_alpaca/\"\nquantized_model_dir = \"caosgpt-j-6B-alpaca-4bit-128g\"\
    \n\n# os.makedirs(quantized_model_dir, exist_ok=True)\n\n\ndef main():\n     \
    \ tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n\
    \      examples = [\n          tokenizer(\n              \"auto-gptq is an easy-to-use\
    \ model quantification library with user-friendly APIs, based on the GPTQ algorithm.\"\
    \n              ),\n          tokenizer(\n              \"Artificial intelligence\
    \ has advanced significantly in recent years.\"\n              ),\n          tokenizer(\n\
    \              \"Model quantization can reduce model size and improve model efficiency.\"\
    \n              ),\n          tokenizer(\n              \"Quantization algorithms\
    \ can reduce the amount of memory and power required.\"\n              ),\n  \
    \        tokenizer(\n              \"Deep learning is used in a variety of applications,\
    \ from medicine to marketing.\"\n              ),\n          tokenizer(\n    \
    \          \"The GPT-4 architecture is the foundation of many next-generation\
    \ language models.\"\n              ),\n          tokenizer(\n              \"\
    Natural language processing allows machines to understand and communicate in human\
    \ languages.\"\n              ),\n          tokenizer(\n              \"Convolutional\
    \ neural networks are used in computer vision.\"\n              ),\n         \
    \ tokenizer(\n              \"Optimization algorithms are fundamental for training\
    \ deep learning models.\"\n              ),\n          tokenizer(\n          \
    \    \"Reinforcement learning is a machine learning technique in which agents\
    \ learn through interaction with their environment.\"\n              )\n     \
    \ ]\n\n      quantify_config = BaseQuantizeConfig(\n          bits=4, # quantize\
    \ the model to 4 bits\n          group_size=128, # it is recommended to set the\
    \ value to 128\n          desc_act=False\n      )\n\n      # load the unquantified\
    \ model, the model will always be force loaded into the CPU\n      model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
    \ quantize_config)\n\n      # quantization model, examples must be a list of dict\
    \ whose keys contain \"input_ids\" and \"attention_mask\"\n      # with low value\
    \ type torch.LongTensor.\n      model.quantize(examples, use_triton=False)\n\n\
    \      # save the quantized model\n      model.save_quantized(quantized_model_dir)\n\
    \n      # save the quantized model using security tensors\n      model.save_quantized(quantized_model_dir,\
    \ use_safetensors=True)\n\n      # load quantized model, currently only supports\
    \ cpu or single gpu\n      model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
    \ device=\"cuda:0\", use_triton=False)\n\n      # inference with model.generate\n\
    \      print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"\
    pt\").to(\"cuda:0\"))[0]))\n\n      # or you can also use pipeline\n      pipeline\
    \ = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=\"cuda:0\"\
    )\n      print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\n\n\nif __name__\
    \ == \"__main__\":\n      Import registration\n\n      record.basicConfig(\n \
    \         format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO,\
    \ datefmt=\"%Y-%m-%d %H:%M: %S\"\n      )\n\n      major()\n\nfinally i added\
    \ the tokenizer again and it works on oobabooga_windows\n\nOutput generated in\
    \ 3.59 seconds (1.39 tokens/s, 5 tokens, context 25, seed 570147804)\nOutput generated\
    \ in 10.13 seconds (9.08 tokens/s, 92 tokens, context 59, seed 771109588)\nOutput\
    \ generated in 11.44 seconds (9.09 tokens/s, 104 tokens, context 59, seed 1814752661)\n\
    Output generated in 23.26 seconds (8.55 tokens/s, 199 tokens, context 197, seed\
    \ 873787634)\nOutput generated in 8.74 seconds (3.89 tokens/s, 34 tokens, context\
    \ 423, seed 1563385550)\nOutput generated in 11.40 seconds (4.91 tokens/s, 56\
    \ tokens, context 497, seed 390511124)\nOutput generated in 10.70 seconds (2.99\
    \ tokens/s, 32 tokens, context 641, seed 1136593747)\nOutput generated in 9.73\
    \ seconds (1.44 tokens/s, 14 tokens, context 702, seed 1881813284)\nSo far it\
    \ works fine, I'll keep trying it... thank you very much"
  created_at: 2023-05-05 20:58:42+00:00
  edited: true
  hidden: false
  id: 64557c12f61f10d69dd10e72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T22:05:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great to hear!</p>

          '
        raw: Great to hear!
        updatedAt: '2023-05-05T22:05:29.721Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RedXeol
    id: 64557da9d55525a4feef8498
    type: comment
  author: TheBloke
  content: Great to hear!
  created_at: 2023-05-05 21:05:29+00:00
  edited: false
  hidden: false
  id: 64557da9d55525a4feef8498
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/GPT4All-13B-snoozy-GGML
repo_type: model
status: open
target_branch: null
title: help me with a question
