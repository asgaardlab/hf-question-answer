!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rahul07
conflicting_files: null
created_at: 2023-08-23 06:39:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/828cbf568f3cd6ec48459a65a7fbfe20.svg
      fullname: rahul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rahul07
      type: user
    createdAt: '2023-08-23T07:39:55.000Z'
    data:
      edited: false
      editors:
      - rahul07
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5531278252601624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/828cbf568f3cd6ec48459a65a7fbfe20.svg
          fullname: rahul
          isHf: false
          isPro: false
          name: rahul07
          type: user
        html: '<p>getting this error while loading the model -<br> Could not load
          Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin.
          Received error fileno (type=value_error)</p>

          '
        raw: "getting this error while loading the model - \r\n Could not load Llama\
          \ model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin.\
          \ Received error fileno (type=value_error)"
        updatedAt: '2023-08-23T07:39:55.905Z'
      numEdits: 0
      reactions: []
    id: 64e5b7cb05abd2d87c90dcc1
    type: comment
  author: rahul07
  content: "getting this error while loading the model - \r\n Could not load Llama\
    \ model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin.\
    \ Received error fileno (type=value_error)"
  created_at: 2023-08-23 06:39:55+00:00
  edited: false
  hidden: false
  id: 64e5b7cb05abd2d87c90dcc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-23T08:13:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9919847846031189
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>How are you trying to load it? Using what client/library?</p>

          '
        raw: How are you trying to load it? Using what client/library?
        updatedAt: '2023-08-23T08:13:11.871Z'
      numEdits: 0
      reactions: []
    id: 64e5bf971ba234c1e5c56aed
    type: comment
  author: TheBloke
  content: How are you trying to load it? Using what client/library?
  created_at: 2023-08-23 07:13:11+00:00
  edited: false
  hidden: false
  id: 64e5bf971ba234c1e5c56aed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/828cbf568f3cd6ec48459a65a7fbfe20.svg
      fullname: rahul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rahul07
      type: user
    createdAt: '2023-08-23T09:28:15.000Z'
    data:
      edited: true
      editors:
      - rahul07
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5493143796920776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/828cbf568f3cd6ec48459a65a7fbfe20.svg
          fullname: rahul
          isHf: false
          isPro: false
          name: rahul07
          type: user
        html: "<p>I'm loading the model via this code -</p>\n<h1 id=\"loading-model\"\
          >Loading model,</h1>\n<pre><code>llm = LlamaCpp(\nmodel_path=model_path,\n\
          max_tokens=256,\nn_gpu_layers=n_gpu_layers,\nn_batch=n_batch,\ncallback_manager=callback_manager,\n\
          n_ctx=1024,\nverbose=False,\n        )\n</code></pre>\n<p>I'm trying to\
          \ pass a pdf and query it using this sheet via your model -  <a rel=\"nofollow\"\
          \ href=\"https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/QA%20Book%20PDF%20LangChain%20Llama%202/Final_Llama_CPP_Ask_Question_from_book_PDF_Llama.ipynb\"\
          >https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/QA%20Book%20PDF%20LangChain%20Llama%202/Final_Llama_CPP_Ask_Question_from_book_PDF_Llama.ipynb</a></p>\n"
        raw: "I'm loading the model via this code -\n   # Loading model,\n    llm\
          \ = LlamaCpp(\n    model_path=model_path,\n    max_tokens=256,\n    n_gpu_layers=n_gpu_layers,\n\
          \    n_batch=n_batch,\n    callback_manager=callback_manager,\n    n_ctx=1024,\n\
          \    verbose=False,\n            )\n\nI'm trying to pass a pdf and query\
          \ it using this sheet via your model -  https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/QA%20Book%20PDF%20LangChain%20Llama%202/Final_Llama_CPP_Ask_Question_from_book_PDF_Llama.ipynb"
        updatedAt: '2023-08-23T09:28:58.763Z'
      numEdits: 1
      reactions: []
    id: 64e5d12f9d76a3de753d8590
    type: comment
  author: rahul07
  content: "I'm loading the model via this code -\n   # Loading model,\n    llm =\
    \ LlamaCpp(\n    model_path=model_path,\n    max_tokens=256,\n    n_gpu_layers=n_gpu_layers,\n\
    \    n_batch=n_batch,\n    callback_manager=callback_manager,\n    n_ctx=1024,\n\
    \    verbose=False,\n            )\n\nI'm trying to pass a pdf and query it using\
    \ this sheet via your model -  https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/QA%20Book%20PDF%20LangChain%20Llama%202/Final_Llama_CPP_Ask_Question_from_book_PDF_Llama.ipynb"
  created_at: 2023-08-23 08:28:15+00:00
  edited: true
  hidden: false
  id: 64e5d12f9d76a3de753d8590
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af04278425412f50c65d9fe2162bfd0b.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hrrrrr
      type: user
    createdAt: '2023-08-24T03:36:20.000Z'
    data:
      edited: false
      editors:
      - Hrrrrr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7586694955825806
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af04278425412f50c65d9fe2162bfd0b.svg
          fullname: Harry
          isHf: false
          isPro: false
          name: Hrrrrr
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64e32b9bd5e75575cd5ddccf/JhboMy_5CrPn7RLpF2K3G.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64e32b9bd5e75575cd5ddccf/JhboMy_5CrPn7RLpF2K3G.png"></a></p>

          <p>I also got the same error, have you found the solution?</p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64e32b9bd5e75575cd5ddccf/JhboMy_5CrPn7RLpF2K3G.png)


          I also got the same error, have you found the solution?'
        updatedAt: '2023-08-24T03:36:20.115Z'
      numEdits: 0
      reactions: []
    id: 64e6d034a24f7172fa6aedae
    type: comment
  author: Hrrrrr
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64e32b9bd5e75575cd5ddccf/JhboMy_5CrPn7RLpF2K3G.png)


    I also got the same error, have you found the solution?'
  created_at: 2023-08-24 02:36:20+00:00
  edited: false
  hidden: false
  id: 64e6d034a24f7172fa6aedae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b06ee620c1e8ab83585452109c1549e.svg
      fullname: Kompala Lokesh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lokesh1200
      type: user
    createdAt: '2023-08-24T18:43:44.000Z'
    data:
      edited: false
      editors:
      - Lokesh1200
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6924422979354858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b06ee620c1e8ab83585452109c1549e.svg
          fullname: Kompala Lokesh
          isHf: false
          isPro: false
          name: Lokesh1200
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64be5679ef8c0e42bf45ccb1/O5Tc0cuisKtMw5gKQSAkc.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64be5679ef8c0e42bf45ccb1/O5Tc0cuisKtMw5gKQSAkc.png"></a><br>I
          also got the same error, have you found the solution?</p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64be5679ef8c0e42bf45ccb1/O5Tc0cuisKtMw5gKQSAkc.png)

          I also got the same error, have you found the solution?'
        updatedAt: '2023-08-24T18:43:44.785Z'
      numEdits: 0
      reactions: []
    id: 64e7a4e0c3b2443fb30fb861
    type: comment
  author: Lokesh1200
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64be5679ef8c0e42bf45ccb1/O5Tc0cuisKtMw5gKQSAkc.png)

    I also got the same error, have you found the solution?'
  created_at: 2023-08-24 17:43:44+00:00
  edited: false
  hidden: false
  id: 64e7a4e0c3b2443fb30fb861
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a977528834cb6ccbfc0cc4b3ac4ef4e.svg
      fullname: wasif
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kalam
      type: user
    createdAt: '2023-08-25T06:50:23.000Z'
    data:
      edited: false
      editors:
      - kalam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5992366075515747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a977528834cb6ccbfc0cc4b3ac4ef4e.svg
          fullname: wasif
          isHf: false
          isPro: false
          name: kalam
          type: user
        html: '<p>i am facing same error, how can resolve it please help me :-      (<br>
          Could not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin.
          Received error fileno (type=value_error)</p>

          '
        raw: "i am facing same error, how can resolve it please help me :-      (\n\
          \ Could not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin.\
          \ Received error fileno (type=value_error)"
        updatedAt: '2023-08-25T06:50:23.179Z'
      numEdits: 0
      reactions: []
    id: 64e84f2f7cf3df9db7533b4b
    type: comment
  author: kalam
  content: "i am facing same error, how can resolve it please help me :-      (\n\
    \ Could not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin.\
    \ Received error fileno (type=value_error)"
  created_at: 2023-08-25 05:50:23+00:00
  edited: false
  hidden: false
  id: 64e84f2f7cf3df9db7533b4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T07:27:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9745355248451233
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please report this to whatever client provides that code.  Nothing
          has changed with my models.</p>

          '
        raw: Please report this to whatever client provides that code.  Nothing has
          changed with my models.
        updatedAt: '2023-08-25T07:27:46.819Z'
      numEdits: 0
      reactions: []
    id: 64e857f2e6a52fff352cd345
    type: comment
  author: TheBloke
  content: Please report this to whatever client provides that code.  Nothing has
    changed with my models.
  created_at: 2023-08-25 06:27:46+00:00
  edited: false
  hidden: false
  id: 64e857f2e6a52fff352cd345
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2971389b9131eab71912ed1224538320.svg
      fullname: Amber Tiwari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shodhi
      type: user
    createdAt: '2023-08-26T13:01:04.000Z'
    data:
      edited: true
      editors:
      - shodhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9466895461082458
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2971389b9131eab71912ed1224538320.svg
          fullname: Amber Tiwari
          isHf: false
          isPro: false
          name: shodhi
          type: user
        html: '<p>Thanks a lot TheBloke for your immense work. I am working with llama2
          7b/13b q8 models both successfully in koboldcpp. But i can''t get both of
          them work with lLamaCpp. I am getting value errror, assertion error. Do
          you have any suggestions, i can try. Thanks.</p>

          '
        raw: Thanks a lot TheBloke for your immense work. I am working with llama2
          7b/13b q8 models both successfully in koboldcpp. But i can't get both of
          them work with lLamaCpp. I am getting value errror, assertion error. Do
          you have any suggestions, i can try. Thanks.
        updatedAt: '2023-08-26T13:01:34.665Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - teblanc
        - srisre111
    id: 64e9f79096f42afd626871d8
    type: comment
  author: shodhi
  content: Thanks a lot TheBloke for your immense work. I am working with llama2 7b/13b
    q8 models both successfully in koboldcpp. But i can't get both of them work with
    lLamaCpp. I am getting value errror, assertion error. Do you have any suggestions,
    i can try. Thanks.
  created_at: 2023-08-26 12:01:04+00:00
  edited: true
  hidden: false
  id: 64e9f79096f42afd626871d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-26T13:03:33.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8594776391983032
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;shodhi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/shodhi\">@<span class=\"\
          underline\">shodhi</span></a></span>\n\n\t</span></span> llama.cpp no longer\
          \ supports GGML models as of August 21st.  GGML has been replaced by a new\
          \ format called GGUF.</p>\n<p>I will soon be providing GGUF models for all\
          \ my existing GGML repos, but I'm waiting until they fix a bug with GGUF\
          \ models.   I will also soon update the READMEs on all my GGML models to\
          \ mention this.</p>\n<p>For now, please downgrade llama.cpp to commit <code>dadbed99e65252d79f81101a392d0d6497b86caa</code>\
          \ and rebuild it, and it will work fine with these and all other GGML files.\
          \  If you're using llama-cpp-python, please use version v0.1.78 or earlier.</p>\n"
        raw: '@shodhi llama.cpp no longer supports GGML models as of August 21st.  GGML
          has been replaced by a new format called GGUF.


          I will soon be providing GGUF models for all my existing GGML repos, but
          I''m waiting until they fix a bug with GGUF models.   I will also soon update
          the READMEs on all my GGML models to mention this.


          For now, please downgrade llama.cpp to commit `dadbed99e65252d79f81101a392d0d6497b86caa`
          and rebuild it, and it will work fine with these and all other GGML files.  If
          you''re using llama-cpp-python, please use version v0.1.78 or earlier.'
        updatedAt: '2023-08-26T13:04:09.042Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F92F"
        users:
        - adityaasish
        - teblanc
      - count: 2
        reaction: "\U0001F44D"
        users:
        - JimmyNYCU
        - sodingli
      - count: 1
        reaction: "\U0001F917"
        users:
        - shodhi
    id: 64e9f825646192530130bc4c
    type: comment
  author: TheBloke
  content: '@shodhi llama.cpp no longer supports GGML models as of August 21st.  GGML
    has been replaced by a new format called GGUF.


    I will soon be providing GGUF models for all my existing GGML repos, but I''m
    waiting until they fix a bug with GGUF models.   I will also soon update the READMEs
    on all my GGML models to mention this.


    For now, please downgrade llama.cpp to commit `dadbed99e65252d79f81101a392d0d6497b86caa`
    and rebuild it, and it will work fine with these and all other GGML files.  If
    you''re using llama-cpp-python, please use version v0.1.78 or earlier.'
  created_at: 2023-08-26 12:03:33+00:00
  edited: true
  hidden: false
  id: 64e9f825646192530130bc4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2971389b9131eab71912ed1224538320.svg
      fullname: Amber Tiwari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shodhi
      type: user
    createdAt: '2023-08-26T13:22:48.000Z'
    data:
      edited: false
      editors:
      - shodhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710633158683777
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2971389b9131eab71912ed1224538320.svg
          fullname: Amber Tiwari
          isHf: false
          isPro: false
          name: shodhi
          type: user
        html: '<p>Thank you so much for the prompt response. I will do as suggested
          and update it here. I was thinking of trying the model with Ctransformers
          inspite of llama also. Will update the results of that too here. Regards</p>

          '
        raw: Thank you so much for the prompt response. I will do as suggested and
          update it here. I was thinking of trying the model with Ctransformers inspite
          of llama also. Will update the results of that too here. Regards
        updatedAt: '2023-08-26T13:22:48.498Z'
      numEdits: 0
      reactions: []
    id: 64e9fca8233101ed99cf5031
    type: comment
  author: shodhi
  content: Thank you so much for the prompt response. I will do as suggested and update
    it here. I was thinking of trying the model with Ctransformers inspite of llama
    also. Will update the results of that too here. Regards
  created_at: 2023-08-26 12:22:48+00:00
  edited: false
  hidden: false
  id: 64e9fca8233101ed99cf5031
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2971389b9131eab71912ed1224538320.svg
      fullname: Amber Tiwari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shodhi
      type: user
    createdAt: '2023-08-26T14:38:31.000Z'
    data:
      edited: true
      editors:
      - shodhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8719519972801208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2971389b9131eab71912ed1224538320.svg
          fullname: Amber Tiwari
          isHf: false
          isPro: false
          name: shodhi
          type: user
        html: '<p>Unfortunately it doesnt works with llama-cpp-python v0.1.78/0.1.77/0.1.76.<br>Else,
          I want it to be worked with langchain''s LlamaCpp mostly?<br>No luck with
          cTransformers as well.<br>Any recommendations? Thanks</p>

          '
        raw: 'Unfortunately it doesnt works with llama-cpp-python v0.1.78/0.1.77/0.1.76.

          Else, I want it to be worked with langchain''s LlamaCpp mostly?

          No luck with cTransformers as well.

          Any recommendations? Thanks'
        updatedAt: '2023-08-26T14:39:45.711Z'
      numEdits: 1
      reactions: []
    id: 64ea0e679d939716d02e3a5a
    type: comment
  author: shodhi
  content: 'Unfortunately it doesnt works with llama-cpp-python v0.1.78/0.1.77/0.1.76.

    Else, I want it to be worked with langchain''s LlamaCpp mostly?

    No luck with cTransformers as well.

    Any recommendations? Thanks'
  created_at: 2023-08-26 13:38:31+00:00
  edited: true
  hidden: false
  id: 64ea0e679d939716d02e3a5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a977528834cb6ccbfc0cc4b3ac4ef4e.svg
      fullname: wasif
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kalam
      type: user
    createdAt: '2023-08-26T15:09:05.000Z'
    data:
      edited: false
      editors:
      - kalam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38303548097610474
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a977528834cb6ccbfc0cc4b3ac4ef4e.svg
          fullname: wasif
          isHf: false
          isPro: false
          name: kalam
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/kkyY08tpt801NTP5P_Dh-.png"><img
          alt="Screenshot (216).png" src="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/kkyY08tpt801NTP5P_Dh-.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/wkJRyw96ezOpxuFEpbzWj.png"><img
          alt="Screenshot (217).png" src="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/wkJRyw96ezOpxuFEpbzWj.png"></a><br>please
          take look , i am facing this error</p>

          '
        raw: '

          ![Screenshot (216).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/kkyY08tpt801NTP5P_Dh-.png)

          ![Screenshot (217).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/wkJRyw96ezOpxuFEpbzWj.png)

          please take look , i am facing this error'
        updatedAt: '2023-08-26T15:09:05.702Z'
      numEdits: 0
      reactions: []
    id: 64ea15912800cf7e9c8904a0
    type: comment
  author: kalam
  content: '

    ![Screenshot (216).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/kkyY08tpt801NTP5P_Dh-.png)

    ![Screenshot (217).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/wkJRyw96ezOpxuFEpbzWj.png)

    please take look , i am facing this error'
  created_at: 2023-08-26 14:09:05+00:00
  edited: false
  hidden: false
  id: 64ea15912800cf7e9c8904a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/950776037909f20547028242591f5879.svg
      fullname: David King
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: actionpace
      type: user
    createdAt: '2023-08-26T15:37:14.000Z'
    data:
      edited: true
      editors:
      - actionpace
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5587629675865173
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/950776037909f20547028242591f5879.svg
          fullname: David King
          isHf: false
          isPro: false
          name: actionpace
          type: user
        html: "<p>Use this colab code as a starting point<br>At this time I do not\
          \ know if some parameters in LlamaCpp() are ignored or if they need to be\
          \ in some sort of metafile as input to the conversion but at least the model\
          \ should work</p>\n<pre><code>!pip install -qq langchain wget \n!pip install\
          \ gguf  #https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\n!git\
          \ clone https://github.com/ggerganov/llama.cpp\n!pip -qq install git+https://github.com/huggingface/transformers\n\
          #Assuming you are using a GPU\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1\
          \ pip -qq install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n\
          \nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager\
          \ import CallbackManager\nfrom langchain.callbacks.streaming_stdout import\
          \ StreamingStdOutCallbackHandler\n\n# Callbacks support token-wise streaming\n\
          callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\
          # Verbose is required to pass to the callback manager\n\nfrom huggingface_hub\
          \ import hf_hub_download\nrepo_id=\"TheBloke/Llama-2-13B-GGML\"; filename=\"\
          llama-2-13b.ggmlv3.q5_1.bin\"\nhf_hub_download(\n    repo_id=repo_id, filename=filename,\n\
          \    local_dir=\"/content\"\n)\n\n!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py\
          \ --input `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
          \ | head -1`.gguf\n\nfilename=filename+\".gguf\"\n\n\nn_gpu_layers = 32\
          \  \nn_batch = 512  \nn_threads=4\nllm = LlamaCpp(\n    model_path=\"/content/\"\
          +filename,\n    n_threads=n_threads,\n    n_gpu_layers=n_gpu_layers,\n \
          \   n_batch=n_batch,\n    callback_manager=callback_manager,\n    n_ctx=2048,\n\
          \    temperature=0.8,\n    repeat_penalty=1.18,\n    top_p=1,\n    top_k=3,\n\
          \    max_tokens=256,\n    streaming=True,\n    #verbose=True,\n)\n</code></pre>\n"
        raw: "Use this colab code as a starting point\nAt this time I do not know\
          \ if some parameters in LlamaCpp() are ignored or if they need to be in\
          \ some sort of metafile as input to the conversion but at least the model\
          \ should work\n\n```\n!pip install -qq langchain wget \n!pip install gguf\
          \  #https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\n!git clone\
          \ https://github.com/ggerganov/llama.cpp\n!pip -qq install git+https://github.com/huggingface/transformers\n\
          #Assuming you are using a GPU\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1\
          \ pip -qq install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n\
          \nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager\
          \ import CallbackManager\nfrom langchain.callbacks.streaming_stdout import\
          \ StreamingStdOutCallbackHandler\n\n# Callbacks support token-wise streaming\n\
          callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\
          # Verbose is required to pass to the callback manager\n\nfrom huggingface_hub\
          \ import hf_hub_download\nrepo_id=\"TheBloke/Llama-2-13B-GGML\"; filename=\"\
          llama-2-13b.ggmlv3.q5_1.bin\"\nhf_hub_download(\n    repo_id=repo_id, filename=filename,\n\
          \    local_dir=\"/content\"\n)\n\n!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py\
          \ --input `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
          \ | head -1`.gguf\n\nfilename=filename+\".gguf\"\n\n\nn_gpu_layers = 32\
          \  \nn_batch = 512  \nn_threads=4\nllm = LlamaCpp(\n    model_path=\"/content/\"\
          +filename,\n    n_threads=n_threads,\n    n_gpu_layers=n_gpu_layers,\n \
          \   n_batch=n_batch,\n    callback_manager=callback_manager,\n    n_ctx=2048,\n\
          \    temperature=0.8,\n    repeat_penalty=1.18,\n    top_p=1,\n    top_k=3,\n\
          \    max_tokens=256,\n    streaming=True,\n    #verbose=True,\n)\n```"
        updatedAt: '2023-08-26T19:51:50.626Z'
      numEdits: 5
      reactions: []
    id: 64ea1c2af6d7c8bdfb8822da
    type: comment
  author: actionpace
  content: "Use this colab code as a starting point\nAt this time I do not know if\
    \ some parameters in LlamaCpp() are ignored or if they need to be in some sort\
    \ of metafile as input to the conversion but at least the model should work\n\n\
    ```\n!pip install -qq langchain wget \n!pip install gguf  #https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\n\
    !git clone https://github.com/ggerganov/llama.cpp\n!pip -qq install git+https://github.com/huggingface/transformers\n\
    #Assuming you are using a GPU\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1\
    \ pip -qq install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n\
    \nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import\
    \ CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\
    \n# Callbacks support token-wise streaming\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\
    # Verbose is required to pass to the callback manager\n\nfrom huggingface_hub\
    \ import hf_hub_download\nrepo_id=\"TheBloke/Llama-2-13B-GGML\"; filename=\"llama-2-13b.ggmlv3.q5_1.bin\"\
    \nhf_hub_download(\n    repo_id=repo_id, filename=filename,\n    local_dir=\"\
    /content\"\n)\n\n!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py --input\
    \ `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
    \ | head -1`.gguf\n\nfilename=filename+\".gguf\"\n\n\nn_gpu_layers = 32  \nn_batch\
    \ = 512  \nn_threads=4\nllm = LlamaCpp(\n    model_path=\"/content/\"+filename,\n\
    \    n_threads=n_threads,\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n\
    \    callback_manager=callback_manager,\n    n_ctx=2048,\n    temperature=0.8,\n\
    \    repeat_penalty=1.18,\n    top_p=1,\n    top_k=3,\n    max_tokens=256,\n \
    \   streaming=True,\n    #verbose=True,\n)\n```"
  created_at: 2023-08-26 14:37:14+00:00
  edited: true
  hidden: false
  id: 64ea1c2af6d7c8bdfb8822da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a977528834cb6ccbfc0cc4b3ac4ef4e.svg
      fullname: wasif
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kalam
      type: user
    createdAt: '2023-08-26T16:06:05.000Z'
    data:
      edited: false
      editors:
      - kalam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3930968642234802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a977528834cb6ccbfc0cc4b3ac4ef4e.svg
          fullname: wasif
          isHf: false
          isPro: false
          name: kalam
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/jgXEtTmEBFSs19YN6tF6S.png"><img
          alt="Screenshot (219).png" src="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/jgXEtTmEBFSs19YN6tF6S.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/_ldHkQdKzaFD055Jq6hdQ.png"><img
          alt="Screenshot (220).png" src="https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/_ldHkQdKzaFD055Jq6hdQ.png"></a><br>i
          try this ,but facing same error </p>

          '
        raw: '

          ![Screenshot (219).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/jgXEtTmEBFSs19YN6tF6S.png)

          ![Screenshot (220).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/_ldHkQdKzaFD055Jq6hdQ.png)

          i try this ,but facing same error '
        updatedAt: '2023-08-26T16:06:05.114Z'
      numEdits: 0
      reactions: []
    id: 64ea22ede0242abe3959cae0
    type: comment
  author: kalam
  content: '

    ![Screenshot (219).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/jgXEtTmEBFSs19YN6tF6S.png)

    ![Screenshot (220).png](https://cdn-uploads.huggingface.co/production/uploads/64d9d977cdee1e4d67718a77/_ldHkQdKzaFD055Jq6hdQ.png)

    i try this ,but facing same error '
  created_at: 2023-08-26 15:06:05+00:00
  edited: false
  hidden: false
  id: 64ea22ede0242abe3959cae0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/950776037909f20547028242591f5879.svg
      fullname: David King
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: actionpace
      type: user
    createdAt: '2023-08-26T16:08:56.000Z'
    data:
      edited: true
      editors:
      - actionpace
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.921002209186554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/950776037909f20547028242591f5879.svg
          fullname: David King
          isHf: false
          isPro: false
          name: actionpace
          type: user
        html: '<p>Make sure the single quotes on the conversion line are backtics
          `<br>In your code it looks like you removed them</p>

          '
        raw: 'Make sure the single quotes on the conversion line are backtics `

          In your code it looks like you removed them'
        updatedAt: '2023-08-26T16:11:50.636Z'
      numEdits: 1
      reactions: []
    id: 64ea2398c6511bd1920a465a
    type: comment
  author: actionpace
  content: 'Make sure the single quotes on the conversion line are backtics `

    In your code it looks like you removed them'
  created_at: 2023-08-26 15:08:56+00:00
  edited: true
  hidden: false
  id: 64ea2398c6511bd1920a465a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/950776037909f20547028242591f5879.svg
      fullname: David King
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: actionpace
      type: user
    createdAt: '2023-08-26T18:30:42.000Z'
    data:
      edited: true
      editors:
      - actionpace
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4484238624572754
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/950776037909f20547028242591f5879.svg
          fullname: David King
          isHf: false
          isPro: false
          name: actionpace
          type: user
        html: '<p>So far my work with this is showing dropped words. I''m not sure
          if it''s something due to the conversion or the Beta status of the new Llama.cpp<br>I
          will be going back to v0.1.78 but will keep an eye on the cutting edge to
          see how this works out</p>

          <pre><code>!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip -qq install
          --upgrade --force-reinstall llama-cpp-python==0.1.78 --no-cache-dir

          </code></pre>

          <p>More information for the conversion script<br>Looks like -c 4096 and
          --eps 1e-5 should be used for Llama2</p>

          <pre><code>!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py -c
          4096 --eps 1e-5 --input `ls -tr /content/*ggmlv3*.bin | head -1` --output
          `ls -tr /content/*ggmlv3*.bin | head -1`.gguf

          </code></pre>

          <p>Convert GGMLv3 models to GGUF</p>

          <p>--input, -i              (Input GGMLv3 filename)<br>--output, -o           (Output
          GGUF filename)<br>--name                     (Set model name)<br>--desc                       (Set
          model description)<br>--gqa                        default = 1      (grouped-query
          attention factor (use 8 for LLaMA2 70B))<br>--eps                         default
          = ''5.0e-06''       (RMS norm eps: Use 1e-6 for LLaMA1 and OpenLLaMA, use
          1e-5 for LLaMA2)<br>--context-length, -c      default = 2048       (Default
          max context length: LLaMA1 is typically 2048, LLaMA2 is typically 4096''))<br>--model-metadata-dir,
          -m        (Load HuggingFace/.pth vocab and metadata from the specified directory''))<br>--vocab-dir                 (directory
          containing tokenizer.model, if separate from model file - only meaningful
          with --model-metadata-dir)<br>--vocabtype              ["spm", "bpe"]               (vocab
          format - only meaningful with --model-metadata-dir and/or --vocab-dir (default:
          spm))</p>

          '
        raw: 'So far my work with this is showing dropped words. I''m not sure if
          it''s something due to the conversion or the Beta status of the new Llama.cpp

          I will be going back to v0.1.78 but will keep an eye on the cutting edge
          to see how this works out

          ```

          !CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip -qq install --upgrade
          --force-reinstall llama-cpp-python==0.1.78 --no-cache-dir

          ```


          More information for the conversion script

          Looks like -c 4096 and --eps 1e-5 should be used for Llama2


          ```

          !python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py -c 4096 --eps
          1e-5 --input `ls -tr /content/*ggmlv3*.bin | head -1` --output `ls -tr /content/*ggmlv3*.bin
          | head -1`.gguf

          ```

          Convert GGMLv3 models to GGUF


          --input, -i              (Input GGMLv3 filename)

          --output, -o           (Output GGUF filename)

          --name                     (Set model name)

          --desc                       (Set model description)

          --gqa                        default = 1      (grouped-query attention factor
          (use 8 for LLaMA2 70B))

          --eps                         default = ''5.0e-06''       (RMS norm eps:
          Use 1e-6 for LLaMA1 and OpenLLaMA, use 1e-5 for LLaMA2)

          --context-length, -c      default = 2048       (Default max context length:
          LLaMA1 is typically 2048, LLaMA2 is typically 4096''))

          --model-metadata-dir, -m        (Load HuggingFace/.pth vocab and metadata
          from the specified directory''))

          --vocab-dir                 (directory containing tokenizer.model, if separate
          from model file - only meaningful with --model-metadata-dir)

          --vocabtype              ["spm", "bpe"]               (vocab format - only
          meaningful with --model-metadata-dir and/or --vocab-dir (default: spm))

          '
        updatedAt: '2023-08-26T21:50:37.004Z'
      numEdits: 4
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - teblanc
        - MoAusaf
    id: 64ea44d28b59aa317e4609ee
    type: comment
  author: actionpace
  content: 'So far my work with this is showing dropped words. I''m not sure if it''s
    something due to the conversion or the Beta status of the new Llama.cpp

    I will be going back to v0.1.78 but will keep an eye on the cutting edge to see
    how this works out

    ```

    !CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip -qq install --upgrade --force-reinstall
    llama-cpp-python==0.1.78 --no-cache-dir

    ```


    More information for the conversion script

    Looks like -c 4096 and --eps 1e-5 should be used for Llama2


    ```

    !python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py -c 4096 --eps 1e-5
    --input `ls -tr /content/*ggmlv3*.bin | head -1` --output `ls -tr /content/*ggmlv3*.bin
    | head -1`.gguf

    ```

    Convert GGMLv3 models to GGUF


    --input, -i              (Input GGMLv3 filename)

    --output, -o           (Output GGUF filename)

    --name                     (Set model name)

    --desc                       (Set model description)

    --gqa                        default = 1      (grouped-query attention factor
    (use 8 for LLaMA2 70B))

    --eps                         default = ''5.0e-06''       (RMS norm eps: Use 1e-6
    for LLaMA1 and OpenLLaMA, use 1e-5 for LLaMA2)

    --context-length, -c      default = 2048       (Default max context length: LLaMA1
    is typically 2048, LLaMA2 is typically 4096''))

    --model-metadata-dir, -m        (Load HuggingFace/.pth vocab and metadata from
    the specified directory''))

    --vocab-dir                 (directory containing tokenizer.model, if separate
    from model file - only meaningful with --model-metadata-dir)

    --vocabtype              ["spm", "bpe"]               (vocab format - only meaningful
    with --model-metadata-dir and/or --vocab-dir (default: spm))

    '
  created_at: 2023-08-26 17:30:42+00:00
  edited: true
  hidden: false
  id: 64ea44d28b59aa317e4609ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/950776037909f20547028242591f5879.svg
      fullname: David King
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: actionpace
      type: user
    createdAt: '2023-08-27T09:49:22.000Z'
    data:
      edited: false
      editors:
      - actionpace
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.696666419506073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/950776037909f20547028242591f5879.svg
          fullname: David King
          isHf: false
          isPro: false
          name: actionpace
          type: user
        html: '<p>Info on successful conversions<br><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/2812#issuecomment-1694413605">https://github.com/ggerganov/llama.cpp/issues/2812#issuecomment-1694413605</a></p>

          <p><a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/15zvxta/comment/jxjj61s/?utm_source=share&amp;utm_medium=web2x&amp;context=3">https://www.reddit.com/r/LocalLLaMA/comments/15zvxta/comment/jxjj61s/?utm_source=share&amp;utm_medium=web2x&amp;context=3</a></p>

          '
        raw: 'Info on successful conversions

          https://github.com/ggerganov/llama.cpp/issues/2812#issuecomment-1694413605


          https://www.reddit.com/r/LocalLLaMA/comments/15zvxta/comment/jxjj61s/?utm_source=share&utm_medium=web2x&context=3'
        updatedAt: '2023-08-27T09:49:22.150Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MoAusaf
    id: 64eb1c22a4b2985194e8bb66
    type: comment
  author: actionpace
  content: 'Info on successful conversions

    https://github.com/ggerganov/llama.cpp/issues/2812#issuecomment-1694413605


    https://www.reddit.com/r/LocalLLaMA/comments/15zvxta/comment/jxjj61s/?utm_source=share&utm_medium=web2x&context=3'
  created_at: 2023-08-27 08:49:22+00:00
  edited: false
  hidden: false
  id: 64eb1c22a4b2985194e8bb66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe385d505e30ace0476e153e81cb188d.svg
      fullname: Abdelrahman Ahmed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdelrahmanAhmed
      type: user
    createdAt: '2023-09-01T11:35:50.000Z'
    data:
      edited: false
      editors:
      - AbdelrahmanAhmed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6041996479034424
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe385d505e30ace0476e153e81cb188d.svg
          fullname: Abdelrahman Ahmed
          isHf: false
          isPro: false
          name: AbdelrahmanAhmed
          type: user
        html: '<p>Fix for "Could not load Llama model from path":</p>

          <p>Download GGUF model from this link:<br><a href="https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF">https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF</a></p>

          <h1 id="code-example">Code Example:</h1>

          <p>model_name_or_path = "TheBloke/CodeLlama-13B-Python-GGUF"<br>model_basename
          = "codellama-13b-python.Q5_K_M.gguf"<br>model_path = hf_hub_download(repo_id=model_name_or_path,
          filename=model_basename)</p>

          <h1 id="then-change-verbosefalse-to-verbosetrue-like-the-following-code">Then
          Change "verbose=False" to "verbose=True" like the following code:</h1>

          <p>llm = LlamaCpp(<br>    model_path=model_path,<br>    max_tokens=256,<br>    n_gpu_layers=n_gpu_layers,<br>    n_batch=n_batch,<br>    callback_manager=callback_manager,<br>    n_ctx=1024,<br>    verbose=True,<br>)</p>

          '
        raw: "Fix for \"Could not load Llama model from path\":\n\nDownload GGUF model\
          \ from this link: \nhttps://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF\n\
          \n# Code Example:\nmodel_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\
          \nmodel_basename = \"codellama-13b-python.Q5_K_M.gguf\"\nmodel_path = hf_hub_download(repo_id=model_name_or_path,\
          \ filename=model_basename)\n\n# Then Change \"verbose=False\" to \"verbose=True\"\
          \ like the following code: \nllm = LlamaCpp(\n    model_path=model_path,\n\
          \    max_tokens=256,\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n\
          \    callback_manager=callback_manager,\n    n_ctx=1024,\n    verbose=True,\n\
          )"
        updatedAt: '2023-09-01T11:35:50.679Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - dorike
        - dduan123
        - shahreeza
        - lazyrook
        - JL73
        - johnsonhk88
        - Tzuyuan
    id: 64f1cc9699fe3b774d3889c1
    type: comment
  author: AbdelrahmanAhmed
  content: "Fix for \"Could not load Llama model from path\":\n\nDownload GGUF model\
    \ from this link: \nhttps://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF\n\
    \n# Code Example:\nmodel_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\
    \nmodel_basename = \"codellama-13b-python.Q5_K_M.gguf\"\nmodel_path = hf_hub_download(repo_id=model_name_or_path,\
    \ filename=model_basename)\n\n# Then Change \"verbose=False\" to \"verbose=True\"\
    \ like the following code: \nllm = LlamaCpp(\n    model_path=model_path,\n   \
    \ max_tokens=256,\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n    callback_manager=callback_manager,\n\
    \    n_ctx=1024,\n    verbose=True,\n)"
  created_at: 2023-09-01 10:35:50+00:00
  edited: false
  hidden: false
  id: 64f1cc9699fe3b774d3889c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/565c4b1239f07db01316ecfed303420b.svg
      fullname: suryaprakash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: advitiya
      type: user
    createdAt: '2023-09-03T03:28:32.000Z'
    data:
      edited: true
      editors:
      - advitiya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7705388069152832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/565c4b1239f07db01316ecfed303420b.svg
          fullname: suryaprakash
          isHf: false
          isPro: false
          name: advitiya
          type: user
        html: "<p>I used to get the same error then, I included these lines and it\
          \ worked!!</p>\n<p>!pip install gguf  #<a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\"\
          >https://github.com/ggerganov/llama.cpp/tree/master/gguf-py</a><br>!git\
          \ clone <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp\"\
          >https://github.com/ggerganov/llama.cpp</a></p>\n<p>model_name_or_path =\
          \ \"TheBloke/CodeLlama-13B-Python-GGUF\"<br>model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\
          </p>\n<p>thanks <span data-props=\"{&quot;user&quot;:&quot;AbdelrahmanAhmed&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AbdelrahmanAhmed\"\
          >@<span class=\"underline\">AbdelrahmanAhmed</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;actionpace&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/actionpace\">@<span class=\"\
          underline\">actionpace</span></a></span>\n\n\t</span></span></p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> thanks for the\
          \ great work:)</p>\n"
        raw: 'I used to get the same error then, I included these lines and it worked!!


          !pip install gguf  #https://github.com/ggerganov/llama.cpp/tree/master/gguf-py

          !git clone https://github.com/ggerganov/llama.cpp


          model_name_or_path = "TheBloke/CodeLlama-13B-Python-GGUF"

          model_basename = "codellama-13b-python.Q5_K_M.gguf"


          thanks @AbdelrahmanAhmed and @actionpace


          @TheBloke thanks for the great work:)

          '
        updatedAt: '2023-09-03T03:31:16.550Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - zuhashaik
    id: 64f3fd60e355a0e90fbd2090
    type: comment
  author: advitiya
  content: 'I used to get the same error then, I included these lines and it worked!!


    !pip install gguf  #https://github.com/ggerganov/llama.cpp/tree/master/gguf-py

    !git clone https://github.com/ggerganov/llama.cpp


    model_name_or_path = "TheBloke/CodeLlama-13B-Python-GGUF"

    model_basename = "codellama-13b-python.Q5_K_M.gguf"


    thanks @AbdelrahmanAhmed and @actionpace


    @TheBloke thanks for the great work:)

    '
  created_at: 2023-09-03 02:28:32+00:00
  edited: true
  hidden: false
  id: 64f3fd60e355a0e90fbd2090
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
      fullname: zuhair hasan shaik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuhashaik
      type: user
    createdAt: '2023-09-05T02:15:25.000Z'
    data:
      edited: true
      editors:
      - zuhashaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8507357239723206
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
          fullname: zuhair hasan shaik
          isHf: false
          isPro: false
          name: zuhashaik
          type: user
        html: '<p>any inputs on 70b ?<br>I initially loaded the GGML version by mistake
          instead of GGUF and discovered that LLama.cpp doesn''t support GGML. I then
          converted it to GGUF using the LLama.cpp repository, but I''m still encountering
          the same error.(70B versions)<br>still getting the same errors..</p>

          <p>modelq2gguf=''/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin''<br>llm
          = LlamaCpp(<br>model_path=modelq2gguf,<br>temperature=0.75,<br>max_tokens=2000,<br>top_p=1,<br>callback_manager=callback_manager,<br>verbose=True<br>)<br>ValidationError:
          1 validation error for LlamaCpp<br>root<br>[Could not load Llama model from
          path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.
          Received error (type=value_error)]<br>(ValidationError: 1 validation error
          for LlamaCpp<br><strong>root</strong><br>  Could not load Llama model from
          path: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin. Received
          error Model path does not exist: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin
          )</p>

          '
        raw: "any inputs on 70b ?\nI initially loaded the GGML version by mistake\
          \ instead of GGUF and discovered that LLama.cpp doesn't support GGML. I\
          \ then converted it to GGUF using the LLama.cpp repository, but I'm still\
          \ encountering the same error.(70B versions)\nstill getting the same errors..\n\
          \nmodelq2gguf='/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin'\n\
          llm = LlamaCpp(\nmodel_path=modelq2gguf,\ntemperature=0.75,\nmax_tokens=2000,\n\
          top_p=1,\ncallback_manager=callback_manager,\nverbose=True\n)\nValidationError:\
          \ 1 validation error for LlamaCpp\nroot\n[Could not load Llama model from\
          \ path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.\
          \ Received error (type=value_error)]\n(ValidationError: 1 validation error\
          \ for LlamaCpp\n__root__\n  Could not load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin.\
          \ Received error Model path does not exist: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin\
          \ )"
        updatedAt: '2023-09-05T02:21:28.694Z'
      numEdits: 1
      reactions: []
    id: 64f68f3d1caa48473b431fd1
    type: comment
  author: zuhashaik
  content: "any inputs on 70b ?\nI initially loaded the GGML version by mistake instead\
    \ of GGUF and discovered that LLama.cpp doesn't support GGML. I then converted\
    \ it to GGUF using the LLama.cpp repository, but I'm still encountering the same\
    \ error.(70B versions)\nstill getting the same errors..\n\nmodelq2gguf='/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin'\n\
    llm = LlamaCpp(\nmodel_path=modelq2gguf,\ntemperature=0.75,\nmax_tokens=2000,\n\
    top_p=1,\ncallback_manager=callback_manager,\nverbose=True\n)\nValidationError:\
    \ 1 validation error for LlamaCpp\nroot\n[Could not load Llama model from path:\
    \ /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin. Received\
    \ error (type=value_error)]\n(ValidationError: 1 validation error for LlamaCpp\n\
    __root__\n  Could not load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin.\
    \ Received error Model path does not exist: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin\
    \ )"
  created_at: 2023-09-05 01:15:25+00:00
  edited: true
  hidden: false
  id: 64f68f3d1caa48473b431fd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
      fullname: James Braza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamesbraza
      type: user
    createdAt: '2023-09-12T21:08:44.000Z'
    data:
      edited: true
      editors:
      - jamesbraza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5454468727111816
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
          fullname: James Braza
          isHf: false
          isPro: false
          name: jamesbraza
          type: user
        html: "<p>For those wanting a fully-baked way with macOS, that works as of\
          \ Sept 12 2023:</p>\n<pre><code class=\"language-txt\"># requirements.txt\n\
          huggingface-hub==0.17.1\nllama-cpp-python==0.1.85\n</code></pre>\n<p>Please\
          \ make sure your Python 3.11 supports amd64 per <a rel=\"nofollow\" href=\"\
          https://github.com/abetlen/llama-cpp-python#installation-from-pypi\">this</a>:</p>\n\
          <pre><code class=\"language-bash\">python -m venv venv\n<span class=\"hljs-built_in\"\
          >source</span> venv/bin/activate\nCMAKE_ARGS=<span class=\"hljs-string\"\
          >\"-DLLAMA_METAL=on\"</span> FORCE_CMAKE=1 python -m pip install -r requirements.txt\
          \ --no-cache-dir\n</code></pre>\n<p>Then this Python code:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">import</span> pathlib\n\
          \n<span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"\
          hljs-keyword\">import</span> hf_hub_download\n<span class=\"hljs-keyword\"\
          >from</span> llama_cpp <span class=\"hljs-keyword\">import</span> Llama\n\
          \nHF_REPO_NAME = <span class=\"hljs-string\">\"TheBloke/Llama-2-13B-chat-GGUF\"\
          </span>\nHF_MODEL_NAME = <span class=\"hljs-string\">\"llama-2-13b-chat.Q4_K_S.gguf\"\
          </span>\nREPO_MODELS_FOLDER = pathlib.Path(__file__).parent / <span class=\"\
          hljs-string\">\"models\"</span>\n\nREPO_MODELS_FOLDER.mkdir(exist_ok=<span\
          \ class=\"hljs-literal\">True</span>)\nmodel_path = hf_hub_download(\n \
          \   repo_id=HF_REPO_NAME, filename=HF_MODEL_NAME, local_dir=REPO_MODELS_FOLDER\n\
          )\n\nllm = Llama(model_path=model_path, n_gpu_layers=<span class=\"hljs-number\"\
          >1</span>)  <span class=\"hljs-comment\"># n_gpu_layers uses macOS Metal\
          \ GPU</span>\nllm(<span class=\"hljs-string\">\"What is the capital of China?\"\
          </span>)\n</code></pre>\n"
        raw: "For those wanting a fully-baked way with macOS, that works as of Sept\
          \ 12 2023:\n\n```txt\n# requirements.txt\nhuggingface-hub==0.17.1\nllama-cpp-python==0.1.85\n\
          ```\n\nPlease make sure your Python 3.11 supports amd64 per [this](https://github.com/abetlen/llama-cpp-python#installation-from-pypi):\n\
          \n```bash\npython -m venv venv\nsource venv/bin/activate\nCMAKE_ARGS=\"\
          -DLLAMA_METAL=on\" FORCE_CMAKE=1 python -m pip install -r requirements.txt\
          \ --no-cache-dir\n```\n\nThen this Python code:\n\n```python\nimport pathlib\n\
          \nfrom huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama\n\
          \nHF_REPO_NAME = \"TheBloke/Llama-2-13B-chat-GGUF\"\nHF_MODEL_NAME = \"\
          llama-2-13b-chat.Q4_K_S.gguf\"\nREPO_MODELS_FOLDER = pathlib.Path(__file__).parent\
          \ / \"models\"\n\nREPO_MODELS_FOLDER.mkdir(exist_ok=True)\nmodel_path =\
          \ hf_hub_download(\n    repo_id=HF_REPO_NAME, filename=HF_MODEL_NAME, local_dir=REPO_MODELS_FOLDER\n\
          )\n\nllm = Llama(model_path=model_path, n_gpu_layers=1)  # n_gpu_layers\
          \ uses macOS Metal GPU\nllm(\"What is the capital of China?\")\n```"
        updatedAt: '2023-09-15T19:54:58.699Z'
      numEdits: 6
      reactions: []
    id: 6500d35cb2078f22bac3b27c
    type: comment
  author: jamesbraza
  content: "For those wanting a fully-baked way with macOS, that works as of Sept\
    \ 12 2023:\n\n```txt\n# requirements.txt\nhuggingface-hub==0.17.1\nllama-cpp-python==0.1.85\n\
    ```\n\nPlease make sure your Python 3.11 supports amd64 per [this](https://github.com/abetlen/llama-cpp-python#installation-from-pypi):\n\
    \n```bash\npython -m venv venv\nsource venv/bin/activate\nCMAKE_ARGS=\"-DLLAMA_METAL=on\"\
    \ FORCE_CMAKE=1 python -m pip install -r requirements.txt --no-cache-dir\n```\n\
    \nThen this Python code:\n\n```python\nimport pathlib\n\nfrom huggingface_hub\
    \ import hf_hub_download\nfrom llama_cpp import Llama\n\nHF_REPO_NAME = \"TheBloke/Llama-2-13B-chat-GGUF\"\
    \nHF_MODEL_NAME = \"llama-2-13b-chat.Q4_K_S.gguf\"\nREPO_MODELS_FOLDER = pathlib.Path(__file__).parent\
    \ / \"models\"\n\nREPO_MODELS_FOLDER.mkdir(exist_ok=True)\nmodel_path = hf_hub_download(\n\
    \    repo_id=HF_REPO_NAME, filename=HF_MODEL_NAME, local_dir=REPO_MODELS_FOLDER\n\
    )\n\nllm = Llama(model_path=model_path, n_gpu_layers=1)  # n_gpu_layers uses macOS\
    \ Metal GPU\nllm(\"What is the capital of China?\")\n```"
  created_at: 2023-09-12 20:08:44+00:00
  edited: true
  hidden: false
  id: 6500d35cb2078f22bac3b27c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
      fullname: zuhair hasan shaik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuhashaik
      type: user
    createdAt: '2023-09-13T09:21:48.000Z'
    data:
      edited: false
      editors:
      - zuhashaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8660606741905212
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
          fullname: zuhair hasan shaik
          isHf: false
          isPro: false
          name: zuhashaik
          type: user
        html: '<p>can anyone explain me about gpu_layers..<br>I''ve 3 cards of nvidia
          v100 tesla 32 gb each, now how many gpu layers i have to pass as atribute:<br>llm
          = AutoModelForCausalLM.from_pretrained(''/media/iiit/Karvalo/zuhair/llama/llama70b_q2/genz-70b.Q2_K.gguf'',
          model_type=''llama'', gpu_layers=gpu_layers)<br>it is accepting from [0-infinite)
          as Ive checked it for 100000 as the value, its still accepting it.</p>

          '
        raw: 'can anyone explain me about gpu_layers..

          I''ve 3 cards of nvidia v100 tesla 32 gb each, now how many gpu layers i
          have to pass as atribute:

          llm = AutoModelForCausalLM.from_pretrained(''/media/iiit/Karvalo/zuhair/llama/llama70b_q2/genz-70b.Q2_K.gguf'',
          model_type=''llama'', gpu_layers=gpu_layers)

          it is accepting from [0-infinite) as Ive checked it for 100000 as the value,
          its still accepting it.'
        updatedAt: '2023-09-13T09:21:48.129Z'
      numEdits: 0
      reactions: []
    id: 65017f2caf460756d264908d
    type: comment
  author: zuhashaik
  content: 'can anyone explain me about gpu_layers..

    I''ve 3 cards of nvidia v100 tesla 32 gb each, now how many gpu layers i have
    to pass as atribute:

    llm = AutoModelForCausalLM.from_pretrained(''/media/iiit/Karvalo/zuhair/llama/llama70b_q2/genz-70b.Q2_K.gguf'',
    model_type=''llama'', gpu_layers=gpu_layers)

    it is accepting from [0-infinite) as Ive checked it for 100000 as the value, its
    still accepting it.'
  created_at: 2023-09-13 08:21:48+00:00
  edited: false
  hidden: false
  id: 65017f2caf460756d264908d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
      fullname: James Braza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamesbraza
      type: user
    createdAt: '2023-09-14T18:25:57.000Z'
    data:
      edited: false
      editors:
      - jamesbraza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8589030504226685
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
          fullname: James Braza
          isHf: false
          isPro: false
          name: jamesbraza
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;zuhashaik&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zuhashaik\">@<span class=\"\
          underline\">zuhashaik</span></a></span>\n\n\t</span></span> I think your\
          \ question is outside this discussion's scope, but check these links for\
          \ info:</p>\n<ul>\n<li><a rel=\"nofollow\" href=\"https://python.langchain.com/docs/integrations/llms/llamacpp#gpu\"\
          >https://python.langchain.com/docs/integrations/llms/llamacpp#gpu</a></li>\n\
          <li><a rel=\"nofollow\" href=\"https://github.com/h2oai/h2ogpt/blob/f2d71b3ec553c9da6b4753c6d873c8cb7b70be86/src/gen.py#L343\"\
          >https://github.com/h2oai/h2ogpt/blob/f2d71b3ec553c9da6b4753c6d873c8cb7b70be86/src/gen.py#L343</a></li>\n\
          </ul>\n<p>If you have further questions, I think it's worth a designated\
          \ discussion thread somewhere else</p>\n"
        raw: '@zuhashaik I think your question is outside this discussion''s scope,
          but check these links for info:


          - https://python.langchain.com/docs/integrations/llms/llamacpp#gpu

          - https://github.com/h2oai/h2ogpt/blob/f2d71b3ec553c9da6b4753c6d873c8cb7b70be86/src/gen.py#L343


          If you have further questions, I think it''s worth a designated discussion
          thread somewhere else'
        updatedAt: '2023-09-14T18:25:57.602Z'
      numEdits: 0
      reactions: []
    id: 6503503554b03500129128f5
    type: comment
  author: jamesbraza
  content: '@zuhashaik I think your question is outside this discussion''s scope,
    but check these links for info:


    - https://python.langchain.com/docs/integrations/llms/llamacpp#gpu

    - https://github.com/h2oai/h2ogpt/blob/f2d71b3ec553c9da6b4753c6d873c8cb7b70be86/src/gen.py#L343


    If you have further questions, I think it''s worth a designated discussion thread
    somewhere else'
  created_at: 2023-09-14 17:25:57+00:00
  edited: false
  hidden: false
  id: 6503503554b03500129128f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5d917e6b322c034e6ab7358195b4caf.svg
      fullname: Zainab salim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zainabsa99
      type: user
    createdAt: '2023-09-17T00:34:20.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/a5d917e6b322c034e6ab7358195b4caf.svg
          fullname: Zainab salim
          isHf: false
          isPro: false
          name: Zainabsa99
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-17T00:34:43.250Z'
      numEdits: 0
      reactions: []
    id: 6506498cd8d96e913b2d00c4
    type: comment
  author: Zainabsa99
  content: This comment has been hidden
  created_at: 2023-09-16 23:34:20+00:00
  edited: true
  hidden: true
  id: 6506498cd8d96e913b2d00c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73743366a937da9829b637576004bd9c.svg
      fullname: Vinayak Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dorike
      type: user
    createdAt: '2023-09-17T15:02:42.000Z'
    data:
      edited: false
      editors:
      - dorike
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6186326146125793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73743366a937da9829b637576004bd9c.svg
          fullname: Vinayak Sharma
          isHf: false
          isPro: false
          name: dorike
          type: user
        html: '<blockquote>

          <p>Fix for "Could not load Llama model from path":</p>

          <p>Download GGUF model from this link:<br><a href="https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF">https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF</a></p>

          <h1 id="code-example">Code Example:</h1>

          <p>model_name_or_path = "TheBloke/CodeLlama-13B-Python-GGUF"<br>model_basename
          = "codellama-13b-python.Q5_K_M.gguf"<br>model_path = hf_hub_download(repo_id=model_name_or_path,
          filename=model_basename)</p>

          <h1 id="then-change-verbosefalse-to-verbosetrue-like-the-following-code">Then
          Change "verbose=False" to "verbose=True" like the following code:</h1>

          <p>llm = LlamaCpp(<br>    model_path=model_path,<br>    max_tokens=256,<br>    n_gpu_layers=n_gpu_layers,<br>    n_batch=n_batch,<br>    callback_manager=callback_manager,<br>    n_ctx=1024,<br>    verbose=True,<br>)</p>

          </blockquote>

          <p>This "verbose=True" worked for me<br>Thanks</p>

          '
        raw: "> Fix for \"Could not load Llama model from path\":\n> \n> Download\
          \ GGUF model from this link: \n> https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF\n\
          > \n> # Code Example:\n> model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\
          \n> model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\n> model_path\
          \ = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n\
          > \n> # Then Change \"verbose=False\" to \"verbose=True\" like the following\
          \ code: \n> llm = LlamaCpp(\n>     model_path=model_path,\n>     max_tokens=256,\n\
          >     n_gpu_layers=n_gpu_layers,\n>     n_batch=n_batch,\n>     callback_manager=callback_manager,\n\
          >     n_ctx=1024,\n>     verbose=True,\n> )\n\nThis \"verbose=True\" worked\
          \ for me\nThanks"
        updatedAt: '2023-09-17T15:02:42.526Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - sid9993
        - SahilBhoite
        - samadghulam
    id: 65071512557a87f111d546ce
    type: comment
  author: dorike
  content: "> Fix for \"Could not load Llama model from path\":\n> \n> Download GGUF\
    \ model from this link: \n> https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF\n\
    > \n> # Code Example:\n> model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\
    \n> model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\n> model_path = hf_hub_download(repo_id=model_name_or_path,\
    \ filename=model_basename)\n> \n> # Then Change \"verbose=False\" to \"verbose=True\"\
    \ like the following code: \n> llm = LlamaCpp(\n>     model_path=model_path,\n\
    >     max_tokens=256,\n>     n_gpu_layers=n_gpu_layers,\n>     n_batch=n_batch,\n\
    >     callback_manager=callback_manager,\n>     n_ctx=1024,\n>     verbose=True,\n\
    > )\n\nThis \"verbose=True\" worked for me\nThanks"
  created_at: 2023-09-17 14:02:42+00:00
  edited: false
  hidden: false
  id: 65071512557a87f111d546ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
      fullname: zuhair hasan shaik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuhashaik
      type: user
    createdAt: '2023-09-18T19:18:31.000Z'
    data:
      edited: true
      editors:
      - zuhashaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.945639967918396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
          fullname: zuhair hasan shaik
          isHf: false
          isPro: false
          name: zuhashaik
          type: user
        html: '<p>Thank you all!<br>But I''m still confused about the value of gpu_layers,
          what does the value of gpu_layers say in llama.cpp<br>It works when you
          use gpu_layers=[0 / 10/ 100000000000].<br>what does the value indicates?
          percentage?</p>

          '
        raw: 'Thank you all!

          But I''m still confused about the value of gpu_layers, what does the value
          of gpu_layers say in llama.cpp

          It works when you use gpu_layers=[0 / 10/ 100000000000].

          what does the value indicates? percentage?'
        updatedAt: '2023-09-18T19:30:31.123Z'
      numEdits: 2
      reactions: []
    id: 6508a2872c804aed59b3c313
    type: comment
  author: zuhashaik
  content: 'Thank you all!

    But I''m still confused about the value of gpu_layers, what does the value of
    gpu_layers say in llama.cpp

    It works when you use gpu_layers=[0 / 10/ 100000000000].

    what does the value indicates? percentage?'
  created_at: 2023-09-18 18:18:31+00:00
  edited: true
  hidden: false
  id: 6508a2872c804aed59b3c313
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2331cec1fec3e5e46ab21f0ce76922b6.svg
      fullname: Ni Made Ganesh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: madeganesh228
      type: user
    createdAt: '2023-10-06T12:22:03.000Z'
    data:
      edited: false
      editors:
      - madeganesh228
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7410326600074768
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2331cec1fec3e5e46ab21f0ce76922b6.svg
          fullname: Ni Made Ganesh
          isHf: false
          isPro: false
          name: madeganesh228
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dorike&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dorike\">@<span class=\"\
          underline\">dorike</span></a></span>\n\n\t</span></span> hi, I've tried\
          \ your code, but I'm still facing the same error ( Could not load Llama\
          \ model from path: TheBloke/CodeLlama-13B-Python-GGUF/codellama-13b-python.Q5_K_M.gguf.\
          \ Received error Model path does not<br>exist: TheBloke/CodeLlama-13B-Python-GGUF/codellama-13b-python.Q5_K_M.gguf\
          \ (type=value_error))<br>do you have any solutions for this? thx</p>\n"
        raw: "@dorike hi, I've tried your code, but I'm still facing the same error\
          \ ( Could not load Llama model from path: TheBloke/CodeLlama-13B-Python-GGUF/codellama-13b-python.Q5_K_M.gguf.\
          \ Received error Model path does not      \nexist: TheBloke/CodeLlama-13B-Python-GGUF/codellama-13b-python.Q5_K_M.gguf\
          \ (type=value_error))\ndo you have any solutions for this? thx"
        updatedAt: '2023-10-06T12:22:03.763Z'
      numEdits: 0
      reactions: []
    id: 651ffbeb6f1df5466367c9f3
    type: comment
  author: madeganesh228
  content: "@dorike hi, I've tried your code, but I'm still facing the same error\
    \ ( Could not load Llama model from path: TheBloke/CodeLlama-13B-Python-GGUF/codellama-13b-python.Q5_K_M.gguf.\
    \ Received error Model path does not      \nexist: TheBloke/CodeLlama-13B-Python-GGUF/codellama-13b-python.Q5_K_M.gguf\
    \ (type=value_error))\ndo you have any solutions for this? thx"
  created_at: 2023-10-06 11:22:03+00:00
  edited: false
  hidden: false
  id: 651ffbeb6f1df5466367c9f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-06T13:00:04.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9568881392478943
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p>Gpu layers offload the model to gpu. I think around 50 to 70ish\
          \ should be enough?</p>\n<p>Also, the reason it\u2019s not working is because\
          \ you have to replace it with your model path? </p>\n"
        raw: "Gpu layers offload the model to gpu. I think around 50 to 70ish should\
          \ be enough?\n\nAlso, the reason it\u2019s not working is because you have\
          \ to replace it with your model path? "
        updatedAt: '2023-10-06T13:00:04.728Z'
      numEdits: 0
      reactions: []
    id: 652004d453181804b5ee726c
    type: comment
  author: YaTharThShaRma999
  content: "Gpu layers offload the model to gpu. I think around 50 to 70ish should\
    \ be enough?\n\nAlso, the reason it\u2019s not working is because you have to\
    \ replace it with your model path? "
  created_at: 2023-10-06 12:00:04+00:00
  edited: false
  hidden: false
  id: 652004d453181804b5ee726c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/652033eee3419abdcf336922/MW72bS7VEOsGdcSbG3uP4.jpeg?w=200&h=200&f=face
      fullname: Dario Cabezas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zethearc
      type: user
    createdAt: '2023-10-24T16:20:52.000Z'
    data:
      edited: false
      editors:
      - Zethearc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4708232879638672
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/652033eee3419abdcf336922/MW72bS7VEOsGdcSbG3uP4.jpeg?w=200&h=200&f=face
          fullname: Dario Cabezas
          isHf: false
          isPro: false
          name: Zethearc
          type: user
        html: '<p>!ls /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin<br>!cp
          /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin
          /content/<br>model_path = "/content/llama-2-13b-chat.ggmlv3.q5_1.bin"</p>

          <p>to solve</p>

          '
        raw: '!ls /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin

          !cp /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin
          /content/

          model_path = "/content/llama-2-13b-chat.ggmlv3.q5_1.bin"


          to solve'
        updatedAt: '2023-10-24T16:20:52.509Z'
      numEdits: 0
      reactions: []
    id: 6537eee47ec35db342f01021
    type: comment
  author: Zethearc
  content: '!ls /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin

    !cp /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin
    /content/

    model_path = "/content/llama-2-13b-chat.ggmlv3.q5_1.bin"


    to solve'
  created_at: 2023-10-24 15:20:52+00:00
  edited: false
  hidden: false
  id: 6537eee47ec35db342f01021
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a857ccaee95e769e3e7011847a981a2.svg
      fullname: Venkat Ganta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Venkat009
      type: user
    createdAt: '2023-10-27T10:09:48.000Z'
    data:
      edited: false
      editors:
      - Venkat009
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.86568683385849
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a857ccaee95e769e3e7011847a981a2.svg
          fullname: Venkat Ganta
          isHf: false
          isPro: false
          name: Venkat009
          type: user
        html: '<p>Can I do the same process for reading tables from a pdf?</p>

          '
        raw: 'Can I do the same process for reading tables from a pdf?

          '
        updatedAt: '2023-10-27T10:09:48.478Z'
      numEdits: 0
      reactions: []
    id: 653b8c6c0cb3d71c00e51c07
    type: comment
  author: Venkat009
  content: 'Can I do the same process for reading tables from a pdf?

    '
  created_at: 2023-10-27 09:09:48+00:00
  edited: false
  hidden: false
  id: 653b8c6c0cb3d71c00e51c07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642322142d5965e7229c3c93/T2owbwrLQTUd4zZzyBS9Q.png?w=200&h=200&f=face
      fullname: Mohammad Ausaf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoAusaf
      type: user
    createdAt: '2023-11-02T11:17:59.000Z'
    data:
      edited: false
      editors:
      - MoAusaf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6860154271125793
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642322142d5965e7229c3c93/T2owbwrLQTUd4zZzyBS9Q.png?w=200&h=200&f=face
          fullname: Mohammad Ausaf
          isHf: false
          isPro: false
          name: MoAusaf
          type: user
        html: "<blockquote>\n<p>any inputs on 70b ?<br>I initially loaded the GGML\
          \ version by mistake instead of GGUF and discovered that LLama.cpp doesn't\
          \ support GGML. I then converted it to GGUF using the LLama.cpp repository,\
          \ but I'm still encountering the same error.(70B versions)<br>still getting\
          \ the same errors..</p>\n<p>modelq2gguf='/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin'<br>llm\
          \ = LlamaCpp(<br>model_path=modelq2gguf,<br>temperature=0.75,<br>max_tokens=2000,<br>top_p=1,<br>callback_manager=callback_manager,<br>verbose=True<br>)<br>ValidationError:\
          \ 1 validation error for LlamaCpp<br>root<br>[Could not load Llama model\
          \ from path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.\
          \ Received error (type=value_error)]<br>(ValidationError: 1 validation error\
          \ for LlamaCpp<br><strong>root</strong><br>  Could not load Llama model\
          \ from path: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin.\
          \ Received error Model path does not exist: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin\
          \ )</p>\n</blockquote>\n<p>If someone is still trying the <span data-props=\"\
          {&quot;user&quot;:&quot;actionpace&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/actionpace\">@<span class=\"underline\"\
          >actionpace</span></a></span>\n\n\t</span></span> starter notebook given\
          \ above and getting the same error, try looking at the paths. For example,\
          \ I couldn't locate the conversion script at the path in the following cmd\
          \ or at least the name wasn't correct</p>\n<pre><code>!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py\
          \ --input `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
          \ | head -1`.gguf\n</code></pre>\n<p>go to the llama.cpp folder  (or in\
          \ the dir you passed as dir in the previous code line) and find the conversion\
          \ script manually, copy and paste the path into the above cmd, for me the\
          \ changed cmd was </p>\n<pre><code>!python /content/llama.cpp/convert-llama-ggml-to-gguf.py\
          \ --input `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
          \ | head -1`.gguf\n</code></pre>\n"
        raw: "> any inputs on 70b ?\n> I initially loaded the GGML version by mistake\
          \ instead of GGUF and discovered that LLama.cpp doesn't support GGML. I\
          \ then converted it to GGUF using the LLama.cpp repository, but I'm still\
          \ encountering the same error.(70B versions)\n> still getting the same errors..\n\
          > \n> modelq2gguf='/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin'\n\
          > llm = LlamaCpp(\n> model_path=modelq2gguf,\n> temperature=0.75,\n> max_tokens=2000,\n\
          > top_p=1,\n> callback_manager=callback_manager,\n> verbose=True\n> )\n\
          > ValidationError: 1 validation error for LlamaCpp\n> root\n> [Could not\
          \ load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.\
          \ Received error (type=value_error)]\n> (ValidationError: 1 validation error\
          \ for LlamaCpp\n> __root__\n>   Could not load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin.\
          \ Received error Model path does not exist: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin\
          \ )\n\nIf someone is still trying the @actionpace starter notebook given\
          \ above and getting the same error, try looking at the paths. For example,\
          \ I couldn't locate the conversion script at the path in the following cmd\
          \ or at least the name wasn't correct\n ```\n!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py\
          \ --input `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
          \ | head -1`.gguf\n```\ngo to the llama.cpp folder  (or in the dir you passed\
          \ as dir in the previous code line) and find the conversion script manually,\
          \ copy and paste the path into the above cmd, for me the changed cmd was\
          \ \n\n```\n!python /content/llama.cpp/convert-llama-ggml-to-gguf.py --input\
          \ `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
          \ | head -1`.gguf\n```"
        updatedAt: '2023-11-02T11:17:59.708Z'
      numEdits: 0
      reactions: []
    id: 65438567d0f52498c6bc3742
    type: comment
  author: MoAusaf
  content: "> any inputs on 70b ?\n> I initially loaded the GGML version by mistake\
    \ instead of GGUF and discovered that LLama.cpp doesn't support GGML. I then converted\
    \ it to GGUF using the LLama.cpp repository, but I'm still encountering the same\
    \ error.(70B versions)\n> still getting the same errors..\n> \n> modelq2gguf='/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin'\n\
    > llm = LlamaCpp(\n> model_path=modelq2gguf,\n> temperature=0.75,\n> max_tokens=2000,\n\
    > top_p=1,\n> callback_manager=callback_manager,\n> verbose=True\n> )\n> ValidationError:\
    \ 1 validation error for LlamaCpp\n> root\n> [Could not load Llama model from\
    \ path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.\
    \ Received error (type=value_error)]\n> (ValidationError: 1 validation error for\
    \ LlamaCpp\n> __root__\n>   Could not load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin.\
    \ Received error Model path does not exist: /media/iiit/Karvalo/zuhair/llama/llama-2-70b.ggmlv3.q4_1.bin\
    \ )\n\nIf someone is still trying the @actionpace starter notebook given above\
    \ and getting the same error, try looking at the paths. For example, I couldn't\
    \ locate the conversion script at the path in the following cmd or at least the\
    \ name wasn't correct\n ```\n!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py\
    \ --input `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin\
    \ | head -1`.gguf\n```\ngo to the llama.cpp folder  (or in the dir you passed\
    \ as dir in the previous code line) and find the conversion script manually, copy\
    \ and paste the path into the above cmd, for me the changed cmd was \n\n```\n\
    !python /content/llama.cpp/convert-llama-ggml-to-gguf.py --input `ls -t /content/*ggmlv3*.bin\
    \ | head -1` --output `ls -t /content/*ggmlv3*.bin | head -1`.gguf\n```"
  created_at: 2023-11-02 10:17:59+00:00
  edited: false
  hidden: false
  id: 65438567d0f52498c6bc3742
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f46e26f9a98c0938f1f1cbd66e76ba32.svg
      fullname: Sanjita Nepal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanjitaa
      type: user
    createdAt: '2023-11-29T11:39:43.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/f46e26f9a98c0938f1f1cbd66e76ba32.svg
          fullname: Sanjita Nepal
          isHf: false
          isPro: false
          name: sanjitaa
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-04T11:51:33.780Z'
      numEdits: 0
      reactions: []
    id: 656722ff52652936785c0ffb
    type: comment
  author: sanjitaa
  content: This comment has been hidden
  created_at: 2023-11-29 11:39:43+00:00
  edited: true
  hidden: true
  id: 656722ff52652936785c0ffb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kUijiECKc8bUVBg5N52z4.jpeg?w=200&h=200&f=face
      fullname: 'Bhoite '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SahilBhoite
      type: user
    createdAt: '2023-12-09T06:46:54.000Z'
    data:
      edited: false
      editors:
      - SahilBhoite
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5871061682701111
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kUijiECKc8bUVBg5N52z4.jpeg?w=200&h=200&f=face
          fullname: 'Bhoite '
          isHf: false
          isPro: false
          name: SahilBhoite
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Fix for "Could not load Llama model from path":</p>

          <p>Download GGUF model from this link:<br><a href="https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF">https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF</a></p>

          <h1 id="code-example">Code Example:</h1>

          <p>model_name_or_path = "TheBloke/CodeLlama-13B-Python-GGUF"<br>model_basename
          = "codellama-13b-python.Q5_K_M.gguf"<br>model_path = hf_hub_download(repo_id=model_name_or_path,
          filename=model_basename)</p>

          <h1 id="then-change-verbosefalse-to-verbosetrue-like-the-following-code">Then
          Change "verbose=False" to "verbose=True" like the following code:</h1>

          <p>llm = LlamaCpp(<br>    model_path=model_path,<br>    max_tokens=256,<br>    n_gpu_layers=n_gpu_layers,<br>    n_batch=n_batch,<br>    callback_manager=callback_manager,<br>    n_ctx=1024,<br>    verbose=True,<br>)</p>

          </blockquote>

          <p>This "verbose=True" worked for me<br>Thanks</p>

          </blockquote>

          '
        raw: "> > Fix for \"Could not load Llama model from path\":\n> > \n> > Download\
          \ GGUF model from this link: \n> > https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF\n\
          > > \n> > # Code Example:\n> > model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\
          \n> > model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\n> > model_path\
          \ = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n\
          > > \n> > # Then Change \"verbose=False\" to \"verbose=True\" like the following\
          \ code: \n> > llm = LlamaCpp(\n> >     model_path=model_path,\n> >     max_tokens=256,\n\
          > >     n_gpu_layers=n_gpu_layers,\n> >     n_batch=n_batch,\n> >     callback_manager=callback_manager,\n\
          > >     n_ctx=1024,\n> >     verbose=True,\n> > )\n> \n> This \"verbose=True\"\
          \ worked for me\n> Thanks\n\n"
        updatedAt: '2023-12-09T06:46:54.760Z'
      numEdits: 0
      reactions: []
    id: 65740d5e983403462a160592
    type: comment
  author: SahilBhoite
  content: "> > Fix for \"Could not load Llama model from path\":\n> > \n> > Download\
    \ GGUF model from this link: \n> > https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF\n\
    > > \n> > # Code Example:\n> > model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\
    \n> > model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\n> > model_path =\
    \ hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n> > \n\
    > > # Then Change \"verbose=False\" to \"verbose=True\" like the following code:\
    \ \n> > llm = LlamaCpp(\n> >     model_path=model_path,\n> >     max_tokens=256,\n\
    > >     n_gpu_layers=n_gpu_layers,\n> >     n_batch=n_batch,\n> >     callback_manager=callback_manager,\n\
    > >     n_ctx=1024,\n> >     verbose=True,\n> > )\n> \n> This \"verbose=True\"\
    \ worked for me\n> Thanks\n\n"
  created_at: 2023-12-09 06:46:54+00:00
  edited: false
  hidden: false
  id: 65740d5e983403462a160592
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Llama-2-13B-GGML
repo_type: model
status: open
target_branch: null
title: Could not load Llama model from path
