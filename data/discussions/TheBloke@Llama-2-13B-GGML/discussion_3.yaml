!!python/object:huggingface_hub.community.DiscussionWithDetails
author: david565
conflicting_files: null
created_at: 2023-07-23 12:40:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/687eca738336dbaa4896eafd9aaf5db5.svg
      fullname: David White
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: david565
      type: user
    createdAt: '2023-07-23T13:40:46.000Z'
    data:
      edited: false
      editors:
      - david565
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5893861055374146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/687eca738336dbaa4896eafd9aaf5db5.svg
          fullname: David White
          isHf: false
          isPro: false
          name: david565
          type: user
        html: '<p>Meta reports that the base models support 4096 context. Is it possible
          to make GGML models with 4096 context?</p>

          <p>llama.cpp:<br>$ ./main -c 4096 -m /media/data/llama-2-13b.ggmlv3.q6_K.bin<br>main:
          warning: base model only supports context sizes no greater than 2048 tokens
          (4096 specified)</p>

          '
        raw: "Meta reports that the base models support 4096 context. Is it possible\
          \ to make GGML models with 4096 context?\r\n\r\nllama.cpp:\r\n$ ./main -c\
          \ 4096 -m /media/data/llama-2-13b.ggmlv3.q6_K.bin\r\nmain: warning: base\
          \ model only supports context sizes no greater than 2048 tokens (4096 specified)"
        updatedAt: '2023-07-23T13:40:46.083Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - algorithm
    id: 64bd2dde12afb2f119365189
    type: comment
  author: david565
  content: "Meta reports that the base models support 4096 context. Is it possible\
    \ to make GGML models with 4096 context?\r\n\r\nllama.cpp:\r\n$ ./main -c 4096\
    \ -m /media/data/llama-2-13b.ggmlv3.q6_K.bin\r\nmain: warning: base model only\
    \ supports context sizes no greater than 2048 tokens (4096 specified)"
  created_at: 2023-07-23 12:40:46+00:00
  edited: false
  hidden: false
  id: 64bd2dde12afb2f119365189
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642d678777078db98b729188/UG42t8m7454PbDTSc7G5o.png?w=200&h=200&f=face
      fullname: algorithm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: algorithm
      type: user
    createdAt: '2023-07-24T19:02:43.000Z'
    data:
      edited: true
      editors:
      - algorithm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7726983428001404
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642d678777078db98b729188/UG42t8m7454PbDTSc7G5o.png?w=200&h=200&f=face
          fullname: algorithm
          isHf: false
          isPro: false
          name: algorithm
          type: user
        html: '<p>Getting the same error.</p>

          <p>Looking at this commit: <a href="https://huggingface.co/meta-llama/Llama-2-13b-hf/commit/f3b475aaed299d2389525d6ce4e542cc438833a4">https://huggingface.co/meta-llama/Llama-2-13b-hf/commit/f3b475aaed299d2389525d6ce4e542cc438833a4</a></p>

          <pre><code>"max_position_embeddings": 2048,

          </code></pre>

          <p>3 days ago this was changed to:</p>

          <pre><code>"max_position_embeddings": 4096

          </code></pre>

          <p>Edit: oops that''s the hf model, so I guess I''m not sure.</p>

          '
        raw: "Getting the same error.\n\nLooking at this commit: https://huggingface.co/meta-llama/Llama-2-13b-hf/commit/f3b475aaed299d2389525d6ce4e542cc438833a4\n\
          \n    \"max_position_embeddings\": 2048,\n\n3 days ago this was changed\
          \ to:\n\n    \"max_position_embeddings\": 4096\n\nEdit: oops that's the\
          \ hf model, so I guess I'm not sure."
        updatedAt: '2023-07-24T19:04:26.520Z'
      numEdits: 1
      reactions: []
    id: 64becad3329d247eddfa22d2
    type: comment
  author: algorithm
  content: "Getting the same error.\n\nLooking at this commit: https://huggingface.co/meta-llama/Llama-2-13b-hf/commit/f3b475aaed299d2389525d6ce4e542cc438833a4\n\
    \n    \"max_position_embeddings\": 2048,\n\n3 days ago this was changed to:\n\n\
    \    \"max_position_embeddings\": 4096\n\nEdit: oops that's the hf model, so I\
    \ guess I'm not sure."
  created_at: 2023-07-24 18:02:43+00:00
  edited: true
  hidden: false
  id: 64becad3329d247eddfa22d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T19:21:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8819808959960938
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I need to fix those config.json files and will do it now</p>

          <p>But it won''t change that warning message, which is currently hardcoded
          into llama.cpp and can be ignored on models you know have &gt;2048 context:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Q7CCgd_75Ri0HcifG_UcE.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Q7CCgd_75Ri0HcifG_UcE.png"></a></p>

          <p>So to be clear, yes my config.json files are wrong and will be updated,
          but that in no way affects the GGML models which work fine at 4096 context
          - or even greater using RoPE scaling.  And to be honest it doesn''t really
          affect the GPTQ models either, as the value in config.json is just a default/baseline
          and most clients let you specify the context value independently.</p>

          '
        raw: 'Yeah I need to fix those config.json files and will do it now


          But it won''t change that warning message, which is currently hardcoded
          into llama.cpp and can be ignored on models you know have >2048 context:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Q7CCgd_75Ri0HcifG_UcE.png)


          So to be clear, yes my config.json files are wrong and will be updated,
          but that in no way affects the GGML models which work fine at 4096 context
          - or even greater using RoPE scaling.  And to be honest it doesn''t really
          affect the GPTQ models either, as the value in config.json is just a default/baseline
          and most clients let you specify the context value independently.'
        updatedAt: '2023-07-24T19:21:43.284Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - algorithm
    id: 64becf471d40292dd31560e1
    type: comment
  author: TheBloke
  content: 'Yeah I need to fix those config.json files and will do it now


    But it won''t change that warning message, which is currently hardcoded into llama.cpp
    and can be ignored on models you know have >2048 context:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Q7CCgd_75Ri0HcifG_UcE.png)


    So to be clear, yes my config.json files are wrong and will be updated, but that
    in no way affects the GGML models which work fine at 4096 context - or even greater
    using RoPE scaling.  And to be honest it doesn''t really affect the GPTQ models
    either, as the value in config.json is just a default/baseline and most clients
    let you specify the context value independently.'
  created_at: 2023-07-24 18:21:43+00:00
  edited: false
  hidden: false
  id: 64becf471d40292dd31560e1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-13B-GGML
repo_type: model
status: open
target_branch: null
title: 4096 context?
