!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gopal7417
conflicting_files: null
created_at: 2023-09-26 15:21:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da0731cfb928701a02006ae000b74e35.svg
      fullname: Gopal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gopal7417
      type: user
    createdAt: '2023-09-26T16:21:15.000Z'
    data:
      edited: false
      editors:
      - gopal7417
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34429100155830383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da0731cfb928701a02006ae000b74e35.svg
          fullname: Gopal
          isHf: false
          isPro: false
          name: gopal7417
          type: user
        html: '<p>Im getting invalid response when i ask ''Calculate 2+2''. what changes
          i need to do here<br>from langchain.llms import CTransformers<br>from langchain.chains
          import LLMChain<br>from langchain import PromptTemplate<br>import os<br>import
          io<br>import time</p>

          <p>custom_prompt_template = """<br>{prompt}<br>"""</p>

          <p>def set_custom_prompt():<br>    prompt =   PromptTemplate(<br>        template
          = custom_prompt_template,<br>        input_variables = [''prompt'']<br>    )<br>    return
          prompt</p>

          <p>n_gpu_layers = 0<br>n_batch = 512<br>n_threads=4<br>def load_model():<br>    llm
          = CTransformers(<br>        model = ''llama-2-13b.ggmlv3.q4_0.bin'',<br>        model_type
          = ''llama'',<br>        n_threads=n_threads,<br>        n_gpu_layers=n_gpu_layers,<br>        n_batch=n_batch,<br>        n_ctx=2048,<br>        temperature=0.8,<br>        repeat_penalty=1.18,<br>        top_p=1,<br>        top_k=3,<br>        max_tokens=256,<br>        streaming=True,<br>        verbose=True,<br>    )<br>    return
          llm</p>

          <p>def chain_pipeline():<br>    llm = load_model()<br>    qa_prompt = set_custom_prompt()<br>    qa_chain
          = LLMChain(<br>        prompt = qa_prompt,<br>        llm = llm<br>    )<br>    return
          qa_chain</p>

          <p>llmchain = chain_pipeline()</p>

          <p>def bot(sentence):<br>    llm_response = llmchain.run({"prompt": sentence})<br>    print(llm_response)<br>    return
          llm_response</p>

          <p>if <strong>name</strong> == ''<strong>main</strong>'':<br>    bot(''Calculate
          2+2'')</p>

          <p>Console output:<br>PS C:\Projects\Test\RocketBuild\CodeLlama 2&gt; python
          app.py</p>

          <p>\end{pre}</p>

          <p>And i get this answer as : <code>10</code></p>

          <p>I don''t know why am getting that ans.</p>

          <p>Answer: The <code>expr</code> argument is a string, it looks like you
          are trying to pass a <code>Number</code> object into the function.</p>

          <p>This should work, unless I have missed something:</p>

          <p>\begin{code}<br>function expr() {<br>  return document.getElementById("result").value;<br>}<br>function
          show(x) {<br>  alert(''The result of'' + x + ''is '' + expr());<br>}<br>\end{code}</p>

          <p>\begin{code}<br><br><button>Show Result</button></p>


          \end{code}'
        raw: "\r\nIm getting invalid response when i ask 'Calculate 2+2'. what changes\
          \ i need to do here\r\nfrom langchain.llms import CTransformers\r\nfrom\
          \ langchain.chains import LLMChain\r\nfrom langchain import PromptTemplate\r\
          \nimport os   \r\nimport io\r\nimport time\r\n\r\ncustom_prompt_template\
          \ = \"\"\"\r\n{prompt}\r\n\"\"\"\r\n\r\ndef set_custom_prompt():\r\n   \
          \ prompt =   PromptTemplate(\r\n        template = custom_prompt_template,\r\
          \n        input_variables = ['prompt']\r\n    )\r\n    return prompt\r\n\
          \r\nn_gpu_layers = 0  \r\nn_batch = 512  \r\nn_threads=4\r\ndef load_model():\r\
          \n    llm = CTransformers(\r\n        model = 'llama-2-13b.ggmlv3.q4_0.bin',\r\
          \n        model_type = 'llama',\r\n        n_threads=n_threads,\r\n    \
          \    n_gpu_layers=n_gpu_layers,\r\n        n_batch=n_batch,\r\n        n_ctx=2048,\r\
          \n        temperature=0.8,\r\n        repeat_penalty=1.18,\r\n        top_p=1,\r\
          \n        top_k=3,\r\n        max_tokens=256,\r\n        streaming=True,\r\
          \n        verbose=True,\r\n    )     \r\n    return llm\r\n\r\ndef chain_pipeline():\r\
          \n    llm = load_model()\r\n    qa_prompt = set_custom_prompt()\r\n    qa_chain\
          \ = LLMChain(\r\n        prompt = qa_prompt,\r\n        llm = llm\r\n  \
          \  )\r\n    return qa_chain\r\n\r\nllmchain = chain_pipeline()\r\n\r\ndef\
          \ bot(sentence):\r\n    llm_response = llmchain.run({\"prompt\": sentence})\r\
          \n    print(llm_response)\r\n    return llm_response\r\n\r\n\r\nif __name__\
          \ == '__main__':\r\n    bot('Calculate 2+2')\r\n\r\nConsole output:\r\n\
          PS C:\\Projects\\Test\\RocketBuild\\CodeLlama 2> python app.py\r\n\r\n\\\
          end{pre}\r\n\r\nAnd i get this answer as : `10`\r\n\r\nI don't know why\
          \ am getting that ans.\r\n\r\nAnswer: The `expr` argument is a string, it\
          \ looks like you are trying to pass a `Number` object into the function.\r\
          \n\r\nThis should work, unless I have missed something:\r\n\r\n\\begin{code}\r\
          \nfunction expr() {\r\n  return document.getElementById(\"result\").value;\r\
          \n}\r\nfunction show(x) {\r\n  alert('The result of' + x + 'is ' + expr());\r\
          \n}\r\n\\end{code}\r\n\r\n\\begin{code}\r\n<input type=\"text\" id=\"expr\"\
          >\r\n<button onclick=\"show(document.getElementById('expr').value);\">Show\
          \ Result</button>\r\n\r\n<input type=\"text\" id=\"result\">\r\n\\end{code}"
        updatedAt: '2023-09-26T16:21:15.688Z'
      numEdits: 0
      reactions: []
    id: 651304fbbd21724fd43a767a
    type: comment
  author: gopal7417
  content: "\r\nIm getting invalid response when i ask 'Calculate 2+2'. what changes\
    \ i need to do here\r\nfrom langchain.llms import CTransformers\r\nfrom langchain.chains\
    \ import LLMChain\r\nfrom langchain import PromptTemplate\r\nimport os   \r\n\
    import io\r\nimport time\r\n\r\ncustom_prompt_template = \"\"\"\r\n{prompt}\r\n\
    \"\"\"\r\n\r\ndef set_custom_prompt():\r\n    prompt =   PromptTemplate(\r\n \
    \       template = custom_prompt_template,\r\n        input_variables = ['prompt']\r\
    \n    )\r\n    return prompt\r\n\r\nn_gpu_layers = 0  \r\nn_batch = 512  \r\n\
    n_threads=4\r\ndef load_model():\r\n    llm = CTransformers(\r\n        model\
    \ = 'llama-2-13b.ggmlv3.q4_0.bin',\r\n        model_type = 'llama',\r\n      \
    \  n_threads=n_threads,\r\n        n_gpu_layers=n_gpu_layers,\r\n        n_batch=n_batch,\r\
    \n        n_ctx=2048,\r\n        temperature=0.8,\r\n        repeat_penalty=1.18,\r\
    \n        top_p=1,\r\n        top_k=3,\r\n        max_tokens=256,\r\n        streaming=True,\r\
    \n        verbose=True,\r\n    )     \r\n    return llm\r\n\r\ndef chain_pipeline():\r\
    \n    llm = load_model()\r\n    qa_prompt = set_custom_prompt()\r\n    qa_chain\
    \ = LLMChain(\r\n        prompt = qa_prompt,\r\n        llm = llm\r\n    )\r\n\
    \    return qa_chain\r\n\r\nllmchain = chain_pipeline()\r\n\r\ndef bot(sentence):\r\
    \n    llm_response = llmchain.run({\"prompt\": sentence})\r\n    print(llm_response)\r\
    \n    return llm_response\r\n\r\n\r\nif __name__ == '__main__':\r\n    bot('Calculate\
    \ 2+2')\r\n\r\nConsole output:\r\nPS C:\\Projects\\Test\\RocketBuild\\CodeLlama\
    \ 2> python app.py\r\n\r\n\\end{pre}\r\n\r\nAnd i get this answer as : `10`\r\n\
    \r\nI don't know why am getting that ans.\r\n\r\nAnswer: The `expr` argument is\
    \ a string, it looks like you are trying to pass a `Number` object into the function.\r\
    \n\r\nThis should work, unless I have missed something:\r\n\r\n\\begin{code}\r\
    \nfunction expr() {\r\n  return document.getElementById(\"result\").value;\r\n\
    }\r\nfunction show(x) {\r\n  alert('The result of' + x + 'is ' + expr());\r\n\
    }\r\n\\end{code}\r\n\r\n\\begin{code}\r\n<input type=\"text\" id=\"expr\">\r\n\
    <button onclick=\"show(document.getElementById('expr').value);\">Show Result</button>\r\
    \n\r\n<input type=\"text\" id=\"result\">\r\n\\end{code}"
  created_at: 2023-09-26 15:21:15+00:00
  edited: false
  hidden: false
  id: 651304fbbd21724fd43a767a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Llama-2-13B-GGML
repo_type: model
status: open
target_branch: null
title: Why im getting invalid responses
