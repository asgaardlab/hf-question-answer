!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dm-mschubert
conflicting_files: null
created_at: 2023-07-22 12:33:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da3cfe2dff02af30de006012956c2194.svg
      fullname: Martin Schubert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dm-mschubert
      type: user
    createdAt: '2023-07-22T13:33:37.000Z'
    data:
      edited: false
      editors:
      - dm-mschubert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8753193616867065
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da3cfe2dff02af30de006012956c2194.svg
          fullname: Martin Schubert
          isHf: false
          isPro: false
          name: dm-mschubert
          type: user
        html: '<p>hi out there,<br>was wondering if anyone was able to deploy the
          model to the inference endpoint via the deploy button or the sagemaker script.<br>looks
          like having trouble with the following when looking at my cloudwatch logs:</p>

          <p><code>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate
          136.00 MiB (GPU 0; 22.20 GiB total capacity; 21.21 GiB already allocated;
          31.12 MiB free; 21.22 GiB reserved in total by PyTorch) If reserved memory
          is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>

          <p>Thank you for any help!</p>

          '
        raw: "hi out there,\r\nwas wondering if anyone was able to deploy the model\
          \ to the inference endpoint via the deploy button or the sagemaker script.\r\
          \nlooks like having trouble with the following when looking at my cloudwatch\
          \ logs:\r\n\r\n```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
          \ to allocate 136.00 MiB (GPU 0; 22.20 GiB total capacity; 21.21 GiB already\
          \ allocated; 31.12 MiB free; 21.22 GiB reserved in total by PyTorch) If\
          \ reserved memory is >> allocated memory try setting max_split_size_mb to\
          \ avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF```\r\
          \n\r\nThank you for any help!"
        updatedAt: '2023-07-22T13:33:37.049Z'
      numEdits: 0
      reactions: []
    id: 64bbdab1f671da974e07e78d
    type: comment
  author: dm-mschubert
  content: "hi out there,\r\nwas wondering if anyone was able to deploy the model\
    \ to the inference endpoint via the deploy button or the sagemaker script.\r\n\
    looks like having trouble with the following when looking at my cloudwatch logs:\r\
    \n\r\n```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00\
    \ MiB (GPU 0; 22.20 GiB total capacity; 21.21 GiB already allocated; 31.12 MiB\
    \ free; 21.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
    \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF```\r\n\r\nThank you for any\
    \ help!"
  created_at: 2023-07-22 12:33:37+00:00
  edited: false
  hidden: false
  id: 64bbdab1f671da974e07e78d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-07-22T13:34:16.000Z'
    data:
      edited: false
      editors:
      - flozi00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9834895133972168
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
          fullname: Florian Zimmermeister
          isHf: false
          isPro: false
          name: flozi00
          type: user
        html: '<p>That means that the gpu memory is too small</p>

          '
        raw: That means that the gpu memory is too small
        updatedAt: '2023-07-22T13:34:16.032Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dm-mschubert
    id: 64bbdad8da140e46192a4f0d
    type: comment
  author: flozi00
  content: That means that the gpu memory is too small
  created_at: 2023-07-22 12:34:16+00:00
  edited: false
  hidden: false
  id: 64bbdad8da140e46192a4f0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da3cfe2dff02af30de006012956c2194.svg
      fullname: Martin Schubert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dm-mschubert
      type: user
    createdAt: '2023-07-23T22:51:20.000Z'
    data:
      edited: true
      editors:
      - dm-mschubert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8382647037506104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da3cfe2dff02af30de006012956c2194.svg
          fullname: Martin Schubert
          isHf: false
          isPro: false
          name: dm-mschubert
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;flozi00&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/flozi00\">@<span class=\"\
          underline\">flozi00</span></a></span>\n\n\t</span></span>,<br>i kept struggling\
          \ to get the flozi00/Llama-2-13B-german-assistant-v2 deployed to aws sagemaker\
          \ including the endpoint for lambda access.<br>used the instructions in\
          \ the deploy dropdown here at huggingface.<br>upgraded the model to Notebook\
          \ instance type \"ml.g5.4xlarge\" but still seem to run out of memory:<br><code>torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 22.20 GiB total\
          \ capacity; 21.21 GiB already allocated; 31.12 MiB free; 21.22 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>\n<p>i also tried\
          \ to deploy via inference endpoint but always end up with the following\
          \ error:</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64a3e848db05de144aa09ec3/5mhZDT5-66pxdjMgDPO-u.png\"\
          ><img alt=\"Screenshot 2023-07-24 at 01.16.55.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64a3e848db05de144aa09ec3/5mhZDT5-66pxdjMgDPO-u.png\"\
          ></a><br>logs are like:</p>\n<pre><code>...File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\\\
          \", line 729, in warmup\\n raise RuntimeError(\\nRuntimeError: Not enough\
          \ memory to handle 16000 total tokens with 4096 prefill tokens. You need\
          \ to decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\\\
          n\"},\"target\":\"text_generation_launcher\"} 2023/07/24 01:05:17 ~ {\"\
          timestamp\":\"2023-07-23T23:05:17.751618Z\",\"level\":\"ERROR\",\"message\"\
          :\"Server error: Not enough memory to handle 16000 total tokens with 4096\
          \ prefill tokens. You need to decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\"\
          ,\"target\":\"text_generation_client\",\"filename\":\"router/client/src/lib.rs\"\
          ,\"line_number\":33,\"span\":{\"name\":\"warmup\"},\"spans\":[{\"max_input_length\"\
          :1024,\"max_prefill_tokens\":4096,\"max_total_tokens\":16000,\"name\":\"\
          warmup\"},{\"name\":\"warmup\"}]} 2023/07/24 01:05:17 ~ Error: Warmup(Generation(\"\
          Not enough memory to handle 16000 total tokens with 4096 prefill tokens.\
          \ You need to decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\"\
          )) 2023/07/24 01:05:17 ~ {\"timestamp\":\"2023-07-23T23:05:17.799769Z\"\
          ,\"level\":\"ERROR\",\"fields\":{\"message\":\"Webserver Crashed\"},\"target\"\
          :\"text_generation_launcher\"} 2023/07/24 01:05:17 ~ {\"timestamp\":\"2023-07-23T23:05:17.799803Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Shutting down shards\"},\"\
          target\":\"text_generation_launcher\"} 2023/07/24 01:05:17 ~ {\"timestamp\"\
          :\"2023-07-23T23:05:17.982467Z\",\"level\":\"INFO\",\"fields\":{\"message\"\
          :\"Shard 0 terminated\"},\"target\":\"text_generation_launcher\",\"span\"\
          :{\"rank\":0,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":0,\"name\"\
          :\"shard-manager\"}]} 2023/07/24 01:05:17 ~ Error: WebserverFailed 2023/07/24\
          \ 01:06:00 ~...\n</code></pre>\n<p>any kind of help, tutorial, video, or\
          \ aws sagemaker snippet would help. also happy to dm if possible.<br>thank\
          \ you!</p>\n"
        raw: 'Hi @flozi00,

          i kept struggling to get the flozi00/Llama-2-13B-german-assistant-v2 deployed
          to aws sagemaker including the endpoint for lambda access.

          used the instructions in the deploy dropdown here at huggingface.

          upgraded the model to Notebook instance type "ml.g5.4xlarge" but still seem
          to run out of memory:

          ```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00
          MiB (GPU 0; 22.20 GiB total capacity; 21.21 GiB already allocated; 31.12
          MiB free; 21.22 GiB reserved in total by PyTorch) If reserved memory is
          >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF```


          i also tried to deploy via inference endpoint but always end up with the
          following error:


          ![Screenshot 2023-07-24 at 01.16.55.png](https://cdn-uploads.huggingface.co/production/uploads/64a3e848db05de144aa09ec3/5mhZDT5-66pxdjMgDPO-u.png)

          logs are like:

          ```

          ...File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\",
          line 729, in warmup\n raise RuntimeError(\nRuntimeError: Not enough memory
          to handle 16000 total tokens with 4096 prefill tokens. You need to decrease
          `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\n"},"target":"text_generation_launcher"}
          2023/07/24 01:05:17 ~ {"timestamp":"2023-07-23T23:05:17.751618Z","level":"ERROR","message":"Server
          error: Not enough memory to handle 16000 total tokens with 4096 prefill
          tokens. You need to decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`","target":"text_generation_client","filename":"router/client/src/lib.rs","line_number":33,"span":{"name":"warmup"},"spans":[{"max_input_length":1024,"max_prefill_tokens":4096,"max_total_tokens":16000,"name":"warmup"},{"name":"warmup"}]}
          2023/07/24 01:05:17 ~ Error: Warmup(Generation("Not enough memory to handle
          16000 total tokens with 4096 prefill tokens. You need to decrease `--max-batch-total-tokens`
          or `--max-batch-prefill-tokens`")) 2023/07/24 01:05:17 ~ {"timestamp":"2023-07-23T23:05:17.799769Z","level":"ERROR","fields":{"message":"Webserver
          Crashed"},"target":"text_generation_launcher"} 2023/07/24 01:05:17 ~ {"timestamp":"2023-07-23T23:05:17.799803Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"} 2023/07/24 01:05:17 ~
          {"timestamp":"2023-07-23T23:05:17.982467Z","level":"INFO","fields":{"message":"Shard
          0 terminated"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}
          2023/07/24 01:05:17 ~ Error: WebserverFailed 2023/07/24 01:06:00 ~...

          ```


          any kind of help, tutorial, video, or aws sagemaker snippet would help.
          also happy to dm if possible.

          thank you!'
        updatedAt: '2023-07-23T23:19:39.144Z'
      numEdits: 2
      reactions: []
    id: 64bdaee8979949d2e220c9fc
    type: comment
  author: dm-mschubert
  content: 'Hi @flozi00,

    i kept struggling to get the flozi00/Llama-2-13B-german-assistant-v2 deployed
    to aws sagemaker including the endpoint for lambda access.

    used the instructions in the deploy dropdown here at huggingface.

    upgraded the model to Notebook instance type "ml.g5.4xlarge" but still seem to
    run out of memory:

    ```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB
    (GPU 0; 22.20 GiB total capacity; 21.21 GiB already allocated; 31.12 MiB free;
    21.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory
    try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory
    Management and PYTORCH_CUDA_ALLOC_CONF```


    i also tried to deploy via inference endpoint but always end up with the following
    error:


    ![Screenshot 2023-07-24 at 01.16.55.png](https://cdn-uploads.huggingface.co/production/uploads/64a3e848db05de144aa09ec3/5mhZDT5-66pxdjMgDPO-u.png)

    logs are like:

    ```

    ...File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\",
    line 729, in warmup\n raise RuntimeError(\nRuntimeError: Not enough memory to
    handle 16000 total tokens with 4096 prefill tokens. You need to decrease `--max-batch-total-tokens`
    or `--max-batch-prefill-tokens`\n"},"target":"text_generation_launcher"} 2023/07/24
    01:05:17 ~ {"timestamp":"2023-07-23T23:05:17.751618Z","level":"ERROR","message":"Server
    error: Not enough memory to handle 16000 total tokens with 4096 prefill tokens.
    You need to decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`","target":"text_generation_client","filename":"router/client/src/lib.rs","line_number":33,"span":{"name":"warmup"},"spans":[{"max_input_length":1024,"max_prefill_tokens":4096,"max_total_tokens":16000,"name":"warmup"},{"name":"warmup"}]}
    2023/07/24 01:05:17 ~ Error: Warmup(Generation("Not enough memory to handle 16000
    total tokens with 4096 prefill tokens. You need to decrease `--max-batch-total-tokens`
    or `--max-batch-prefill-tokens`")) 2023/07/24 01:05:17 ~ {"timestamp":"2023-07-23T23:05:17.799769Z","level":"ERROR","fields":{"message":"Webserver
    Crashed"},"target":"text_generation_launcher"} 2023/07/24 01:05:17 ~ {"timestamp":"2023-07-23T23:05:17.799803Z","level":"INFO","fields":{"message":"Shutting
    down shards"},"target":"text_generation_launcher"} 2023/07/24 01:05:17 ~ {"timestamp":"2023-07-23T23:05:17.982467Z","level":"INFO","fields":{"message":"Shard
    0 terminated"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}
    2023/07/24 01:05:17 ~ Error: WebserverFailed 2023/07/24 01:06:00 ~...

    ```


    any kind of help, tutorial, video, or aws sagemaker snippet would help. also happy
    to dm if possible.

    thank you!'
  created_at: 2023-07-23 21:51:20+00:00
  edited: true
  hidden: false
  id: 64bdaee8979949d2e220c9fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-07-24T09:52:20.000Z'
    data:
      edited: false
      editors:
      - flozi00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7989340424537659
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
          fullname: Florian Zimmermeister
          isHf: false
          isPro: false
          name: flozi00
          type: user
        html: '<p>Since i have not the time to debug the aws services, please contact
          <a rel="nofollow" href="https://www.primeline-systemhaus.de/">https://www.primeline-systemhaus.de/</a><br>primeline
          is the main sponsor of my research with own datacenters</p>

          '
        raw: 'Since i have not the time to debug the aws services, please contact
          https://www.primeline-systemhaus.de/

          primeline is the main sponsor of my research with own datacenters'
        updatedAt: '2023-07-24T09:52:20.426Z'
      numEdits: 0
      reactions: []
    id: 64be49d49f94ea25541688dc
    type: comment
  author: flozi00
  content: 'Since i have not the time to debug the aws services, please contact https://www.primeline-systemhaus.de/

    primeline is the main sponsor of my research with own datacenters'
  created_at: 2023-07-24 08:52:20+00:00
  edited: false
  hidden: false
  id: 64be49d49f94ea25541688dc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flozi00/Llama-2-13B-german-assistant-v2
repo_type: model
status: open
target_branch: null
title: ' Not able to deploy model successfully to inference endpoint due to error
  and also not sagemaker via script'
