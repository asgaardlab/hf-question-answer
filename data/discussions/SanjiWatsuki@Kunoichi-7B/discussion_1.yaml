!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cpt-Turtle
conflicting_files: null
created_at: 2024-01-05 19:05:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93e090f3fb6767dc787cd8fc468495ae.svg
      fullname: Cpt-Turtle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cpt-Turtle
      type: user
    createdAt: '2024-01-05T19:05:51.000Z'
    data:
      edited: false
      editors:
      - Cpt-Turtle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9582664966583252
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93e090f3fb6767dc787cd8fc468495ae.svg
          fullname: Cpt-Turtle
          isHf: false
          isPro: false
          name: Cpt-Turtle
          type: user
        html: '<p>I tried your models since loyal-macaroni-maid for RP which I found
          great.</p>

          <p>Then I tried silicon-maid, sonya and lelantos which did not compel me
          to switch from loyal-macaroni despite the higher benchmarks of newer models.</p>

          <p>Kunoichi though is really a level higher. It maintains the creativity
          level while being more clever (respecting new rules on-the-fly) and especially
          follows instructions more closely, which is tremendously useful.</p>

          <p>It does not feel like a 7B model. It feels way above.</p>

          <p>Awesome job. Keep up the good work.</p>

          <p>PS: It would be interesting to see an offspring at 13B and 20B. Or maybe
          an MoE... (even if the various MoE I have tried hardly convince me)</p>

          '
        raw: "I tried your models since loyal-macaroni-maid for RP which I found great.\r\
          \n\r\nThen I tried silicon-maid, sonya and lelantos which did not compel\
          \ me to switch from loyal-macaroni despite the higher benchmarks of newer\
          \ models.\r\n\r\nKunoichi though is really a level higher. It maintains\
          \ the creativity level while being more clever (respecting new rules on-the-fly)\
          \ and especially follows instructions more closely, which is tremendously\
          \ useful.\r\n\r\nIt does not feel like a 7B model. It feels way above.\r\
          \n\r\nAwesome job. Keep up the good work.\r\n\r\nPS: It would be interesting\
          \ to see an offspring at 13B and 20B. Or maybe an MoE... (even if the various\
          \ MoE I have tried hardly convince me)\r\n\r\n"
        updatedAt: '2024-01-05T19:05:51.116Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - SanjiWatsuki
        - airbentnomad
        - xpgx1
        - Susierune
    id: 6598530fe6df49a09d57217d
    type: comment
  author: Cpt-Turtle
  content: "I tried your models since loyal-macaroni-maid for RP which I found great.\r\
    \n\r\nThen I tried silicon-maid, sonya and lelantos which did not compel me to\
    \ switch from loyal-macaroni despite the higher benchmarks of newer models.\r\n\
    \r\nKunoichi though is really a level higher. It maintains the creativity level\
    \ while being more clever (respecting new rules on-the-fly) and especially follows\
    \ instructions more closely, which is tremendously useful.\r\n\r\nIt does not\
    \ feel like a 7B model. It feels way above.\r\n\r\nAwesome job. Keep up the good\
    \ work.\r\n\r\nPS: It would be interesting to see an offspring at 13B and 20B.\
    \ Or maybe an MoE... (even if the various MoE I have tried hardly convince me)\r\
    \n\r\n"
  created_at: 2024-01-05 19:05:51+00:00
  edited: false
  hidden: false
  id: 6598530fe6df49a09d57217d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dca01f72c2a2e59c52a90a8f1cdee924.svg
      fullname: Sanji Watsuki
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: SanjiWatsuki
      type: user
    createdAt: '2024-01-05T22:27:07.000Z'
    data:
      edited: false
      editors:
      - SanjiWatsuki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9842008352279663
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dca01f72c2a2e59c52a90a8f1cdee924.svg
          fullname: Sanji Watsuki
          isHf: false
          isPro: false
          name: SanjiWatsuki
          type: user
        html: '<p>Thanks for trying my models out! I agree - my hope is that this
          model is a noticeable step improvement from my previous ones. I''m looking
          forward to seeing more feedback from others who give it a shot!</p>

          <p>Most likely I''ll keep messing with 7Bs. I started making these 7B models
          because I had a hypothesis that 7Bs were more capable than 13/20Bs. Once
          Mixtral finetunes get figured out, I might try my hand at those, though
          :)</p>

          '
        raw: 'Thanks for trying my models out! I agree - my hope is that this model
          is a noticeable step improvement from my previous ones. I''m looking forward
          to seeing more feedback from others who give it a shot!


          Most likely I''ll keep messing with 7Bs. I started making these 7B models
          because I had a hypothesis that 7Bs were more capable than 13/20Bs. Once
          Mixtral finetunes get figured out, I might try my hand at those, though
          :)'
        updatedAt: '2024-01-05T22:27:07.319Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - airbentnomad
        - xpgx1
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wuweinero
    id: 6598823b58608c404432e6d8
    type: comment
  author: SanjiWatsuki
  content: 'Thanks for trying my models out! I agree - my hope is that this model
    is a noticeable step improvement from my previous ones. I''m looking forward to
    seeing more feedback from others who give it a shot!


    Most likely I''ll keep messing with 7Bs. I started making these 7B models because
    I had a hypothesis that 7Bs were more capable than 13/20Bs. Once Mixtral finetunes
    get figured out, I might try my hand at those, though :)'
  created_at: 2024-01-05 22:27:07+00:00
  edited: false
  hidden: false
  id: 6598823b58608c404432e6d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
      fullname: Santa Clause
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xpgx1
      type: user
    createdAt: '2024-01-10T14:06:34.000Z'
    data:
      edited: false
      editors:
      - xpgx1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9637280106544495
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
          fullname: Santa Clause
          isHf: false
          isPro: false
          name: xpgx1
          type: user
        html: '<p>Oh, it certainly is - I''ve never encountered such a smart cookie
          in a 7B package that is universally usable - It''s really nice ^-^'' - as
          it can roleplay nicely, but also answer rather quickly and be on point with
          the first response. It lacks a bit in regen capabilities (it needs more
          time to iterate on its own mistakes - but it''ll eventually catch them),
          but since the reply is so fast - that hardly matters to me.</p>

          <p>The safety bias is active, yesh yesh, but it''s such a step up from other
          models (I''ve personally tried, like Captn Turtle above - I''ve seen similar
          ones so I won''t list them) even compared to the 13B ones I have used -
          its really swell =) Especially since people with additional vram headroom
          can now increase context and activate the cfg-cache - for example, or ..yknow,
          USE the PC for other tasks as ST Extras and a smaller, lighter Stable Diffusion
          instance that also can use the vram budget. Such a smart 7B model is clearly
          a sweet spot, 8GB Vram users shouldn''t be left out =)</p>

          <p>But yes, any variety on this one seems a fantastic prospect! Thank you
          for trialing this one here - it absolutely enriched my experience with RP
          models - certainly! </p>

          '
        raw: 'Oh, it certainly is - I''ve never encountered such a smart cookie in
          a 7B package that is universally usable - It''s really nice ^-^'' - as it
          can roleplay nicely, but also answer rather quickly and be on point with
          the first response. It lacks a bit in regen capabilities (it needs more
          time to iterate on its own mistakes - but it''ll eventually catch them),
          but since the reply is so fast - that hardly matters to me.


          The safety bias is active, yesh yesh, but it''s such a step up from other
          models (I''ve personally tried, like Captn Turtle above - I''ve seen similar
          ones so I won''t list them) even compared to the 13B ones I have used -
          its really swell =) Especially since people with additional vram headroom
          can now increase context and activate the cfg-cache - for example, or ..yknow,
          USE the PC for other tasks as ST Extras and a smaller, lighter Stable Diffusion
          instance that also can use the vram budget. Such a smart 7B model is clearly
          a sweet spot, 8GB Vram users shouldn''t be left out =)


          But yes, any variety on this one seems a fantastic prospect! Thank you for
          trialing this one here - it absolutely enriched my experience with RP models
          - certainly! '
        updatedAt: '2024-01-10T14:06:34.997Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Susierune
    id: 659ea46a917b27e184fd47d8
    type: comment
  author: xpgx1
  content: 'Oh, it certainly is - I''ve never encountered such a smart cookie in a
    7B package that is universally usable - It''s really nice ^-^'' - as it can roleplay
    nicely, but also answer rather quickly and be on point with the first response.
    It lacks a bit in regen capabilities (it needs more time to iterate on its own
    mistakes - but it''ll eventually catch them), but since the reply is so fast -
    that hardly matters to me.


    The safety bias is active, yesh yesh, but it''s such a step up from other models
    (I''ve personally tried, like Captn Turtle above - I''ve seen similar ones so
    I won''t list them) even compared to the 13B ones I have used - its really swell
    =) Especially since people with additional vram headroom can now increase context
    and activate the cfg-cache - for example, or ..yknow, USE the PC for other tasks
    as ST Extras and a smaller, lighter Stable Diffusion instance that also can use
    the vram budget. Such a smart 7B model is clearly a sweet spot, 8GB Vram users
    shouldn''t be left out =)


    But yes, any variety on this one seems a fantastic prospect! Thank you for trialing
    this one here - it absolutely enriched my experience with RP models - certainly! '
  created_at: 2024-01-10 14:06:34+00:00
  edited: false
  hidden: false
  id: 659ea46a917b27e184fd47d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afaf17efe86a72e6c22e86f0d0471aa1.svg
      fullname: Kris Dreemurr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Susierune
      type: user
    createdAt: '2024-01-15T07:46:03.000Z'
    data:
      edited: true
      editors:
      - Susierune
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989202618598938
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afaf17efe86a72e6c22e86f0d0471aa1.svg
          fullname: Kris Dreemurr
          isHf: false
          isPro: false
          name: Susierune
          type: user
        html: '<p>I haven''t touched 7b models for a long time, not until I came across
          your Silicon-Maid model. I was surprised at how good that model is and was
          using it for RP for a week. </p>

          <p>Then I decided to try Kunoichi. I have to say that Kunoichi is one step
          above Silicon-Maid. </p>

          <p>I did see a noticeable difference in that the logic is slightly improved
          but it remains good at roleplaying and following the character card. All
          of which are important when it comes to a model for RP.  I forget how small
          this model is since it feels bigger. I''ve used both 13b and 20b models.
          Though I wouldn''t go as far as to say Kunoichi is as good as 20b models,
          (since I feel like 20b does a better job at storywriting) I''d say the quality
          meets somewhere in between 13b and 20b. Not bad for a little guy! Especially
          since there is only so much you can do with a small model.</p>

          <p> I am hoping that having these big improvements in 7b models will encourage
          13b models to make big improvements as well! </p>

          '
        raw: "I haven't touched 7b models for a long time, not until I came across\
          \ your Silicon-Maid model. I was surprised at how good that model is and\
          \ was using it for RP for a week. \n\nThen I decided to try Kunoichi. I\
          \ have to say that Kunoichi is one step above Silicon-Maid. \n\nI did see\
          \ a noticeable difference in that the logic is slightly improved but it\
          \ remains good at roleplaying and following the character card. All of which\
          \ are important when it comes to a model for RP.  I forget how small this\
          \ model is since it feels bigger. I've used both 13b and 20b models. Though\
          \ I wouldn't go as far as to say Kunoichi is as good as 20b models, (since\
          \ I feel like 20b does a better job at storywriting) I'd say the quality\
          \ meets somewhere in between 13b and 20b. Not bad for a little guy! Especially\
          \ since there is only so much you can do with a small model.\n\n I am hoping\
          \ that having these big improvements in 7b models will encourage 13b models\
          \ to make big improvements as well! "
        updatedAt: '2024-01-15T07:55:19.050Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xpgx1
    id: 65a4e2bb90b5e87bcd170a88
    type: comment
  author: Susierune
  content: "I haven't touched 7b models for a long time, not until I came across your\
    \ Silicon-Maid model. I was surprised at how good that model is and was using\
    \ it for RP for a week. \n\nThen I decided to try Kunoichi. I have to say that\
    \ Kunoichi is one step above Silicon-Maid. \n\nI did see a noticeable difference\
    \ in that the logic is slightly improved but it remains good at roleplaying and\
    \ following the character card. All of which are important when it comes to a\
    \ model for RP.  I forget how small this model is since it feels bigger. I've\
    \ used both 13b and 20b models. Though I wouldn't go as far as to say Kunoichi\
    \ is as good as 20b models, (since I feel like 20b does a better job at storywriting)\
    \ I'd say the quality meets somewhere in between 13b and 20b. Not bad for a little\
    \ guy! Especially since there is only so much you can do with a small model.\n\
    \n I am hoping that having these big improvements in 7b models will encourage\
    \ 13b models to make big improvements as well! "
  created_at: 2024-01-15 07:46:03+00:00
  edited: true
  hidden: false
  id: 65a4e2bb90b5e87bcd170a88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60656927408b2b5f6cd6b38161ec9ce.svg
      fullname: Tester
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tester100
      type: user
    createdAt: '2024-01-15T17:27:37.000Z'
    data:
      edited: false
      editors:
      - Tester100
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.985430121421814
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60656927408b2b5f6cd6b38161ec9ce.svg
          fullname: Tester
          isHf: false
          isPro: false
          name: Tester100
          type: user
        html: '<blockquote>

          <p>Most likely I''ll keep messing with 7Bs</p>

          </blockquote>

          <p>Some people out there respect this a lot for sure! Even when it not may
          be obvious. Because on Discord and even Reddit some of your models were
          recommended as an insider''s tip. It''s how I stumbled upon yours in the
          end.</p>

          '
        raw: '> Most likely I''ll keep messing with 7Bs


          Some people out there respect this a lot for sure! Even when it not may
          be obvious. Because on Discord and even Reddit some of your models were
          recommended as an insider''s tip. It''s how I stumbled upon yours in the
          end.'
        updatedAt: '2024-01-15T17:27:37.230Z'
      numEdits: 0
      reactions: []
    id: 65a56b09e82b0b84905718d4
    type: comment
  author: Tester100
  content: '> Most likely I''ll keep messing with 7Bs


    Some people out there respect this a lot for sure! Even when it not may be obvious.
    Because on Discord and even Reddit some of your models were recommended as an
    insider''s tip. It''s how I stumbled upon yours in the end.'
  created_at: 2024-01-15 17:27:37+00:00
  edited: false
  hidden: false
  id: 65a56b09e82b0b84905718d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3da845974b7dff88ceb6be4c44d83554.svg
      fullname: Fodas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zuzus
      type: user
    createdAt: '2024-01-19T21:03:38.000Z'
    data:
      edited: true
      editors:
      - Zuzus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8020597696304321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3da845974b7dff88ceb6be4c44d83554.svg
          fullname: Fodas
          isHf: false
          isPro: false
          name: Zuzus
          type: user
        html: "<p>Which Loader do I use for this model? Sorry for being such a newbie..\U0001F605\
          </p>\n"
        raw: "Which Loader do I use for this model? Sorry for being such a newbie..\U0001F605"
        updatedAt: '2024-01-19T21:03:49.649Z'
      numEdits: 1
      reactions: []
    id: 65aae3aa23a4f1b4ecd9e151
    type: comment
  author: Zuzus
  content: "Which Loader do I use for this model? Sorry for being such a newbie..\U0001F605"
  created_at: 2024-01-19 21:03:38+00:00
  edited: true
  hidden: false
  id: 65aae3aa23a4f1b4ecd9e151
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
      fullname: Santa Clause
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xpgx1
      type: user
    createdAt: '2024-01-21T16:33:50.000Z'
    data:
      edited: false
      editors:
      - xpgx1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8896085619926453
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
          fullname: Santa Clause
          isHf: false
          isPro: false
          name: xpgx1
          type: user
        html: "<blockquote>\n<p>Which Loader do I use for this model? Sorry for being\
          \ such a newbie..\U0001F605</p>\n</blockquote>\n<p>Ahh, nonsense. Everybody\
          \ starts somewhere! It would help if you tell us which software you wish\
          \ to use for this?<br>If you're using OobaBooga Web UI, for example, i'd\
          \ suggest using ExLlama2 or their _HF variant. They are actively maintained\
          \ and replaced the old exllama kernel. Perfect fit for GPU acceleration,\
          \ in my experience.</p>\n<p>Have a gr8 Sunday, Zuzus!</p>\n"
        raw: "> Which Loader do I use for this model? Sorry for being such a newbie..\U0001F605\
          \n\nAhh, nonsense. Everybody starts somewhere! It would help if you tell\
          \ us which software you wish to use for this? \nIf you're using OobaBooga\
          \ Web UI, for example, i'd suggest using ExLlama2 or their _HF variant.\
          \ They are actively maintained and replaced the old exllama kernel. Perfect\
          \ fit for GPU acceleration, in my experience.\n\nHave a gr8 Sunday, Zuzus!\n"
        updatedAt: '2024-01-21T16:33:50.753Z'
      numEdits: 0
      reactions: []
    id: 65ad476ec3fa44c7106b94ea
    type: comment
  author: xpgx1
  content: "> Which Loader do I use for this model? Sorry for being such a newbie..\U0001F605\
    \n\nAhh, nonsense. Everybody starts somewhere! It would help if you tell us which\
    \ software you wish to use for this? \nIf you're using OobaBooga Web UI, for example,\
    \ i'd suggest using ExLlama2 or their _HF variant. They are actively maintained\
    \ and replaced the old exllama kernel. Perfect fit for GPU acceleration, in my\
    \ experience.\n\nHave a gr8 Sunday, Zuzus!\n"
  created_at: 2024-01-21 16:33:50+00:00
  edited: false
  hidden: false
  id: 65ad476ec3fa44c7106b94ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3da845974b7dff88ceb6be4c44d83554.svg
      fullname: Fodas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zuzus
      type: user
    createdAt: '2024-01-21T19:41:56.000Z'
    data:
      edited: false
      editors:
      - Zuzus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9124188423156738
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3da845974b7dff88ceb6be4c44d83554.svg
          fullname: Fodas
          isHf: false
          isPro: false
          name: Zuzus
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;xpgx1&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xpgx1\">@<span class=\"\
          underline\">xpgx1</span></a></span>\n\n\t</span></span> I'm using OobaBooga,\
          \ I tried loading with ExLlama2/_HF but it gives me error, tested with llama.cpp\
          \ but still error. It only load with Transformers but it is so slow for\
          \ me.<br>What's the minimum VRAM Kunoichi-7B need?</p>\n"
        raw: '@xpgx1 I''m using OobaBooga, I tried loading with ExLlama2/_HF but it
          gives me error, tested with llama.cpp but still error. It only load with
          Transformers but it is so slow for me.

          What''s the minimum VRAM Kunoichi-7B need?'
        updatedAt: '2024-01-21T19:41:56.620Z'
      numEdits: 0
      reactions: []
    id: 65ad7384d63812c33e31f2b2
    type: comment
  author: Zuzus
  content: '@xpgx1 I''m using OobaBooga, I tried loading with ExLlama2/_HF but it
    gives me error, tested with llama.cpp but still error. It only load with Transformers
    but it is so slow for me.

    What''s the minimum VRAM Kunoichi-7B need?'
  created_at: 2024-01-21 19:41:56+00:00
  edited: false
  hidden: false
  id: 65ad7384d63812c33e31f2b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
      fullname: Santa Clause
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xpgx1
      type: user
    createdAt: '2024-01-22T17:11:36.000Z'
    data:
      edited: false
      editors:
      - xpgx1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8981977701187134
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
          fullname: Santa Clause
          isHf: false
          isPro: false
          name: xpgx1
          type: user
        html: '<p>A brief primer on all those fancy words:</p>

          <p>In general, we "end-users", who are just trying to make some inference
          for RP and ERP purposes =) -&gt; we use the so-called "quantized" variants
          of those LL models. Why? They simply save space (and have a lower accuracy
          - but that''s to be expected - You can''t have it all). </p>

          <p>THIS model here, "SanjiWatsuki/Kunoichi-7B", represents the "unquantized",
          floating point 16 variant. The "original" so to speak. Sanji fine-tuned
          this model and provided it here for us to use. When you select the "files
          and versions" tab at the top of the page, you can see how large these files
          are. In our case here, we''re dealing with 9.86 GB + 4.62 GB = 14.48 GB
          VRAM usage (roughly, it depends sometimes on the loader and what else is
          stored in the fastes RAM our GPUs can access, the VRAM.)</p>

          <p>-&gt; So, in order to use this model on a VRAM-constrained GPU, please
          download a quantized or "4-bit version" of this. I''d recommend "TheBloke/Kunoichi-7B-GPTQ"
          in the "gptq-4bit-32g-actorder_True" permutation or "branch". WTF is this
          branch you ask? It''s another fancy word indicating how exactly this model
          wants to be loaded. It has an impact on inference quality. The model files
          are, usually, the biggest files within a model card.</p>

          <p>To quickly download this quantized model from HF, just copy pasta this
          line into the ooba download field "TheBloke/Kunoichi-7B-GPTQ:gptq-4bit-32g-actorder_True".
          If this model finished loading, please repeat the loading process and report
          back here Zuzus. You will see it now fits into your 8GB VRAM budget. ^-^</p>

          <p>Lastly, whenever you encounter an error message - please include at least
          the last line of output so that others can understand WHICH error you got.
          </p>

          <p>I hope this helps!</p>

          '
        raw: "A brief primer on all those fancy words:\n\nIn general, we \"end-users\"\
          , who are just trying to make some inference for RP and ERP purposes =)\
          \ -> we use the so-called \"quantized\" variants of those LL models. Why?\
          \ They simply save space (and have a lower accuracy - but that's to be expected\
          \ - You can't have it all). \n\nTHIS model here, \"SanjiWatsuki/Kunoichi-7B\"\
          , represents the \"unquantized\", floating point 16 variant. The \"original\"\
          \ so to speak. Sanji fine-tuned this model and provided it here for us to\
          \ use. When you select the \"files and versions\" tab at the top of the\
          \ page, you can see how large these files are. In our case here, we're dealing\
          \ with 9.86 GB + 4.62 GB = 14.48 GB VRAM usage (roughly, it depends sometimes\
          \ on the loader and what else is stored in the fastes RAM our GPUs can access,\
          \ the VRAM.)\n\n-> So, in order to use this model on a VRAM-constrained\
          \ GPU, please download a quantized or \"4-bit version\" of this. I'd recommend\
          \ \"TheBloke/Kunoichi-7B-GPTQ\" in the \"gptq-4bit-32g-actorder_True\" permutation\
          \ or \"branch\". WTF is this branch you ask? It's another fancy word indicating\
          \ how exactly this model wants to be loaded. It has an impact on inference\
          \ quality. The model files are, usually, the biggest files within a model\
          \ card.\n\nTo quickly download this quantized model from HF, just copy pasta\
          \ this line into the ooba download field \"TheBloke/Kunoichi-7B-GPTQ:gptq-4bit-32g-actorder_True\"\
          . If this model finished loading, please repeat the loading process and\
          \ report back here Zuzus. You will see it now fits into your 8GB VRAM budget.\
          \ ^-^\n\nLastly, whenever you encounter an error message - please include\
          \ at least the last line of output so that others can understand WHICH error\
          \ you got. \n\nI hope this helps!\n"
        updatedAt: '2024-01-22T17:11:36.218Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Zuzus
        - Noire1
    id: 65aea1c8a134c07dde60d794
    type: comment
  author: xpgx1
  content: "A brief primer on all those fancy words:\n\nIn general, we \"end-users\"\
    , who are just trying to make some inference for RP and ERP purposes =) -> we\
    \ use the so-called \"quantized\" variants of those LL models. Why? They simply\
    \ save space (and have a lower accuracy - but that's to be expected - You can't\
    \ have it all). \n\nTHIS model here, \"SanjiWatsuki/Kunoichi-7B\", represents\
    \ the \"unquantized\", floating point 16 variant. The \"original\" so to speak.\
    \ Sanji fine-tuned this model and provided it here for us to use. When you select\
    \ the \"files and versions\" tab at the top of the page, you can see how large\
    \ these files are. In our case here, we're dealing with 9.86 GB + 4.62 GB = 14.48\
    \ GB VRAM usage (roughly, it depends sometimes on the loader and what else is\
    \ stored in the fastes RAM our GPUs can access, the VRAM.)\n\n-> So, in order\
    \ to use this model on a VRAM-constrained GPU, please download a quantized or\
    \ \"4-bit version\" of this. I'd recommend \"TheBloke/Kunoichi-7B-GPTQ\" in the\
    \ \"gptq-4bit-32g-actorder_True\" permutation or \"branch\". WTF is this branch\
    \ you ask? It's another fancy word indicating how exactly this model wants to\
    \ be loaded. It has an impact on inference quality. The model files are, usually,\
    \ the biggest files within a model card.\n\nTo quickly download this quantized\
    \ model from HF, just copy pasta this line into the ooba download field \"TheBloke/Kunoichi-7B-GPTQ:gptq-4bit-32g-actorder_True\"\
    . If this model finished loading, please repeat the loading process and report\
    \ back here Zuzus. You will see it now fits into your 8GB VRAM budget. ^-^\n\n\
    Lastly, whenever you encounter an error message - please include at least the\
    \ last line of output so that others can understand WHICH error you got. \n\n\
    I hope this helps!\n"
  created_at: 2024-01-22 17:11:36+00:00
  edited: false
  hidden: false
  id: 65aea1c8a134c07dde60d794
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3da845974b7dff88ceb6be4c44d83554.svg
      fullname: Fodas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zuzus
      type: user
    createdAt: '2024-01-22T17:40:01.000Z'
    data:
      edited: false
      editors:
      - Zuzus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9712912440299988
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3da845974b7dff88ceb6be4c44d83554.svg
          fullname: Fodas
          isHf: false
          isPro: false
          name: Zuzus
          type: user
        html: '<p>Working very well, thank you very much for explaining all this now
          I know what to look for correctly :D</p>

          '
        raw: Working very well, thank you very much for explaining all this now I
          know what to look for correctly :D
        updatedAt: '2024-01-22T17:40:01.292Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - xpgx1
    id: 65aea871308fc2952b0800d5
    type: comment
  author: Zuzus
  content: Working very well, thank you very much for explaining all this now I know
    what to look for correctly :D
  created_at: 2024-01-22 17:40:01+00:00
  edited: false
  hidden: false
  id: 65aea871308fc2952b0800d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lrKNyGiokuIPUTuRMV0Rx.jpeg?w=200&h=200&f=face
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Noire1
      type: user
    createdAt: '2024-01-22T17:54:01.000Z'
    data:
      edited: false
      editors:
      - Noire1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9402388334274292
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lrKNyGiokuIPUTuRMV0Rx.jpeg?w=200&h=200&f=face
          fullname: Victor
          isHf: false
          isPro: false
          name: Noire1
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;xpgx1&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xpgx1\">@<span class=\"\
          underline\">xpgx1</span></a></span>\n\n\t</span></span> I'm new to this\
          \ too.. thank you, you just explained what I needed. This is the first model\
          \ I loaded using Exllama and I'm surprised how fast it is!!!<br>I will pay\
          \ more attention to models that support Exllama loader.</p>\n<p>Would you\
          \ know how to tell if a model is prepared for Exllama if the developer does\
          \ not specify it on the page? and what's the context size? different from\
          \ .gguf loaded with llama.cpp it does not show how much context has been\
          \ trained on the cmd.</p>\n"
        raw: '@xpgx1 I''m new to this too.. thank you, you just explained what I needed.
          This is the first model I loaded using Exllama and I''m surprised how fast
          it is!!!

          I will pay more attention to models that support Exllama loader.


          Would you know how to tell if a model is prepared for Exllama if the developer
          does not specify it on the page? and what''s the context size? different
          from .gguf loaded with llama.cpp it does not show how much context has been
          trained on the cmd.'
        updatedAt: '2024-01-22T17:54:01.355Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xpgx1
    id: 65aeabb95f62b764449ecc00
    type: comment
  author: Noire1
  content: '@xpgx1 I''m new to this too.. thank you, you just explained what I needed.
    This is the first model I loaded using Exllama and I''m surprised how fast it
    is!!!

    I will pay more attention to models that support Exllama loader.


    Would you know how to tell if a model is prepared for Exllama if the developer
    does not specify it on the page? and what''s the context size? different from
    .gguf loaded with llama.cpp it does not show how much context has been trained
    on the cmd.'
  created_at: 2024-01-22 17:54:01+00:00
  edited: false
  hidden: false
  id: 65aeabb95f62b764449ecc00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
      fullname: Santa Clause
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xpgx1
      type: user
    createdAt: '2024-01-22T18:34:16.000Z'
    data:
      edited: true
      editors:
      - xpgx1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487529993057251
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6357e2e3a680a9d532c84146/gcW4PlIVy8jBBBb8aeh3K.png?w=200&h=200&f=face
          fullname: Santa Clause
          isHf: false
          isPro: false
          name: xpgx1
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Noire1&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Noire1\">@<span class=\"\
          underline\">Noire1</span></a></span>\n\n\t</span></span> - No problem. I\
          \ know how confusing (or at least complex) this can be. And it IS an emergent\
          \ tech field, so there is a huge wealth of info out there and not a lot\
          \ of easily reachable documentation. Its entirely normal to feel a bit lost.\
          \ Personally, I don't want people only flocking to cloud services, LLMs\
          \ should be in our hands, directly. So I think you bring up another valid\
          \ point, another fancy word and pitfall for starters, I forgot to mention\
          \ it above! </p>\n<p>The FORMATS - this is not a perfect list nor is it\
          \ comprehensive, but it will give you a quick reference:<br>When you quantize\
          \ the original, fp16 files, you can choose into which specific \"format\"\
          \ or \"container\" you want your LLM to fit. It's absolutely comparable\
          \ to mp4, mkv etc. Only in this case it also enables or disables certain\
          \ use cases. So it's a good idea to know what to expect, roughly.</p>\n\
          <p>GPTQ: My recommendation - as it's easy, fast, and \"simple\". This is\
          \ a pure GPU-accelerated \"loader\". Any model in this format will be loadable\
          \ by ExLlama and ExLlama2 (try using the _HF variants first - they often\
          \ tie in a bit better with SillyTavern - for example.)</p>\n<p>GGUF: It's\
          \ the successor to GGML and will be able to load models into RAM and VRAM\
          \ - splitting them effectively. This is a nice way to experiment with larger\
          \ models that would normally exceed your VRAM budget. Since LLMs are ...\
          \ \"structured\" in many different layers, like a lasagna (I'm not making\
          \ this up! =), you can offload a certain number of layers into the RAM.\
          \ Your CPU will take those and TRY to keep up with GPU-accelerated layers.\
          \ It will cost time per token, but it will be at least doable. This is more\
          \ or less a bit more involved, depending on the software you use. I love\
          \ Ooba, as it is very flexible. But ymmv.</p>\n<p>EXL2: New format, different\
          \ flavors, I'd skip it for now. If you guys need more info - just ask co-pilot.\
          \ Really, he will point you in the right direction (reddit =P).</p>\n<p>AWQ:\
          \ Another new format, looks rather promising and will be relevant going\
          \ forward. It's not necessary, in my mind, to expand on this here. Co-Pilot\
          \ knows! =)</p>\n<p>Others: Not that relevant, as these are probably BRAND\
          \ new, difficult to implement for starters and don't provide you any benefits,\
          \ currently (as of Jan24).</p>\n<p>=&gt; So - to enjoy these models I'd\
          \ suggest using the Huggingface search bar and typing in your model name\
          \ - plus GPTQ (or the format you like to use). That is also precisely why\
          \ \"TheBloke\" is essentially doing a REAL service - as he mercilessly converts\
          \ many popular models from fp16 into all kinds of formats and provides all\
          \ kinds of branches (and loader-specific variants).</p>\n<p>Phew, this was\
          \ a lot. Couldn't condense it more =) I hope this helps!</p>\n"
        raw: "@Noire1 - No problem. I know how confusing (or at least complex) this\
          \ can be. And it IS an emergent tech field, so there is a huge wealth of\
          \ info out there and not a lot of easily reachable documentation. Its entirely\
          \ normal to feel a bit lost. Personally, I don't want people only flocking\
          \ to cloud services, LLMs should be in our hands, directly. So I think you\
          \ bring up another valid point, another fancy word and pitfall for starters,\
          \ I forgot to mention it above! \n\nThe FORMATS - this is not a perfect\
          \ list nor is it comprehensive, but it will give you a quick reference:\n\
          When you quantize the original, fp16 files, you can choose into which specific\
          \ \"format\" or \"container\" you want your LLM to fit. It's absolutely\
          \ comparable to mp4, mkv etc. Only in this case it also enables or disables\
          \ certain use cases. So it's a good idea to know what to expect, roughly.\n\
          \nGPTQ: My recommendation - as it's easy, fast, and \"simple\". This is\
          \ a pure GPU-accelerated \"loader\". Any model in this format will be loadable\
          \ by ExLlama and ExLlama2 (try using the _HF variants first - they often\
          \ tie in a bit better with SillyTavern - for example.)\n\nGGUF: It's the\
          \ successor to GGML and will be able to load models into RAM and VRAM -\
          \ splitting them effectively. This is a nice way to experiment with larger\
          \ models that would normally exceed your VRAM budget. Since LLMs are ...\
          \ \"structured\" in many different layers, like a lasagna (I'm not making\
          \ this up! =), you can offload a certain number of layers into the RAM.\
          \ Your CPU will take those and TRY to keep up with GPU-accelerated layers.\
          \ It will cost time per token, but it will be at least doable. This is more\
          \ or less a bit more involved, depending on the software you use. I love\
          \ Ooba, as it is very flexible. But ymmv.\n\nEXL2: New format, different\
          \ flavors, I'd skip it for now. If you guys need more info - just ask co-pilot.\
          \ Really, he will point you in the right direction (reddit =P).\n\nAWQ:\
          \ Another new format, looks rather promising and will be relevant going\
          \ forward. It's not necessary, in my mind, to expand on this here. Co-Pilot\
          \ knows! =)\n\nOthers: Not that relevant, as these are probably BRAND new,\
          \ difficult to implement for starters and don't provide you any benefits,\
          \ currently (as of Jan24).\n\n=> So - to enjoy these models I'd suggest\
          \ using the Huggingface search bar and typing in your model name - plus\
          \ GPTQ (or the format you like to use). That is also precisely why \"TheBloke\"\
          \ is essentially doing a REAL service - as he mercilessly converts many\
          \ popular models from fp16 into all kinds of formats and provides all kinds\
          \ of branches (and loader-specific variants).\n\nPhew, this was a lot. Couldn't\
          \ condense it more =) I hope this helps!"
        updatedAt: '2024-01-22T18:35:02.681Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - 010O11
        - Noire1
    id: 65aeb5285033724f45710655
    type: comment
  author: xpgx1
  content: "@Noire1 - No problem. I know how confusing (or at least complex) this\
    \ can be. And it IS an emergent tech field, so there is a huge wealth of info\
    \ out there and not a lot of easily reachable documentation. Its entirely normal\
    \ to feel a bit lost. Personally, I don't want people only flocking to cloud services,\
    \ LLMs should be in our hands, directly. So I think you bring up another valid\
    \ point, another fancy word and pitfall for starters, I forgot to mention it above!\
    \ \n\nThe FORMATS - this is not a perfect list nor is it comprehensive, but it\
    \ will give you a quick reference:\nWhen you quantize the original, fp16 files,\
    \ you can choose into which specific \"format\" or \"container\" you want your\
    \ LLM to fit. It's absolutely comparable to mp4, mkv etc. Only in this case it\
    \ also enables or disables certain use cases. So it's a good idea to know what\
    \ to expect, roughly.\n\nGPTQ: My recommendation - as it's easy, fast, and \"\
    simple\". This is a pure GPU-accelerated \"loader\". Any model in this format\
    \ will be loadable by ExLlama and ExLlama2 (try using the _HF variants first -\
    \ they often tie in a bit better with SillyTavern - for example.)\n\nGGUF: It's\
    \ the successor to GGML and will be able to load models into RAM and VRAM - splitting\
    \ them effectively. This is a nice way to experiment with larger models that would\
    \ normally exceed your VRAM budget. Since LLMs are ... \"structured\" in many\
    \ different layers, like a lasagna (I'm not making this up! =), you can offload\
    \ a certain number of layers into the RAM. Your CPU will take those and TRY to\
    \ keep up with GPU-accelerated layers. It will cost time per token, but it will\
    \ be at least doable. This is more or less a bit more involved, depending on the\
    \ software you use. I love Ooba, as it is very flexible. But ymmv.\n\nEXL2: New\
    \ format, different flavors, I'd skip it for now. If you guys need more info -\
    \ just ask co-pilot. Really, he will point you in the right direction (reddit\
    \ =P).\n\nAWQ: Another new format, looks rather promising and will be relevant\
    \ going forward. It's not necessary, in my mind, to expand on this here. Co-Pilot\
    \ knows! =)\n\nOthers: Not that relevant, as these are probably BRAND new, difficult\
    \ to implement for starters and don't provide you any benefits, currently (as\
    \ of Jan24).\n\n=> So - to enjoy these models I'd suggest using the Huggingface\
    \ search bar and typing in your model name - plus GPTQ (or the format you like\
    \ to use). That is also precisely why \"TheBloke\" is essentially doing a REAL\
    \ service - as he mercilessly converts many popular models from fp16 into all\
    \ kinds of formats and provides all kinds of branches (and loader-specific variants).\n\
    \nPhew, this was a lot. Couldn't condense it more =) I hope this helps!"
  created_at: 2024-01-22 18:34:16+00:00
  edited: true
  hidden: false
  id: 65aeb5285033724f45710655
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lrKNyGiokuIPUTuRMV0Rx.jpeg?w=200&h=200&f=face
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Noire1
      type: user
    createdAt: '2024-01-22T20:41:28.000Z'
    data:
      edited: true
      editors:
      - Noire1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4339195787906647
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lrKNyGiokuIPUTuRMV0Rx.jpeg?w=200&h=200&f=face
          fullname: Victor
          isHf: false
          isPro: false
          name: Noire1
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;xpgx1&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xpgx1\">@<span class=\"\
          underline\">xpgx1</span></a></span>\n\n\t</span></span> Thank you soo sooo\
          \ much!!!</p>\n"
        raw: '@xpgx1 Thank you soo sooo much!!!'
        updatedAt: '2024-01-22T20:41:56.467Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - xpgx1
    id: 65aed2f8fcbe71d6d5b95508
    type: comment
  author: Noire1
  content: '@xpgx1 Thank you soo sooo much!!!'
  created_at: 2024-01-22 20:41:28+00:00
  edited: true
  hidden: false
  id: 65aed2f8fcbe71d6d5b95508
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: SanjiWatsuki/Kunoichi-7B
repo_type: model
status: open
target_branch: null
title: Great to... Awesome
