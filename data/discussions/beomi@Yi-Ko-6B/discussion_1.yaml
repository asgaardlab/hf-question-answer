!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ZzWater
conflicting_files: null
created_at: 2023-12-22 08:44:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43b37f44fc6168df570752dee77995ee.svg
      fullname: zho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZzWater
      type: user
    createdAt: '2023-12-22T08:44:24.000Z'
    data:
      edited: true
      editors:
      - ZzWater
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447266459465027
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43b37f44fc6168df570752dee77995ee.svg
          fullname: zho
          isHf: false
          isPro: false
          name: ZzWater
          type: user
        html: '<p>Hi beomi,<br>   I tried this model, and it performs very competitively
          on downstream tasks. Would you be willing to share some training details?
          For instance, </p>

          <ol>

          <li>How to expand the vocabulary, </li>

          <li>The scale and sources of continued pretraining data.</li>

          </ol>

          '
        raw: "Hi beomi,\n   I tried this model, and it performs very competitively\
          \ on downstream tasks. Would you be willing to share some training details?\
          \ For instance, \n  1. How to expand the vocabulary, \n  2. The scale and\
          \ sources of continued pretraining data."
        updatedAt: '2023-12-22T08:45:21.356Z'
      numEdits: 1
      reactions: []
    id: 65854c68c035ec8d5bb2e899
    type: comment
  author: ZzWater
  content: "Hi beomi,\n   I tried this model, and it performs very competitively on\
    \ downstream tasks. Would you be willing to share some training details? For instance,\
    \ \n  1. How to expand the vocabulary, \n  2. The scale and sources of continued\
    \ pretraining data."
  created_at: 2023-12-22 08:44:24+00:00
  edited: true
  hidden: false
  id: 65854c68c035ec8d5bb2e899
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: beomi
      type: user
    createdAt: '2024-01-05T13:42:05.000Z'
    data:
      edited: false
      editors:
      - beomi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9372191429138184
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
          fullname: Lee Junbum
          isHf: false
          isPro: false
          name: beomi
          type: user
        html: '<p>I trained new tokenizer with SPM on my Korean+English Corpus, and
          selected with my :eyes: and slected top ~16000 Korean tokens, and added
          them into Yi tokenizer.(and also added merges)</p>

          <p>This model is trained with more than 60B(~120GB) Korean tokens with new
          tokenizer -- which includes Korean Wikipedia and other online scrapped corpus.</p>

          '
        raw: 'I trained new tokenizer with SPM on my Korean+English Corpus, and selected
          with my :eyes: and slected top ~16000 Korean tokens, and added them into
          Yi tokenizer.(and also added merges)


          This model is trained with more than 60B(~120GB) Korean tokens with new
          tokenizer -- which includes Korean Wikipedia and other online scrapped corpus.'
        updatedAt: '2024-01-05T13:42:05.217Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6598072dc50abdfec41af80f
    id: 6598072dc50abdfec41af80a
    type: comment
  author: beomi
  content: 'I trained new tokenizer with SPM on my Korean+English Corpus, and selected
    with my :eyes: and slected top ~16000 Korean tokens, and added them into Yi tokenizer.(and
    also added merges)


    This model is trained with more than 60B(~120GB) Korean tokens with new tokenizer
    -- which includes Korean Wikipedia and other online scrapped corpus.'
  created_at: 2024-01-05 13:42:05+00:00
  edited: false
  hidden: false
  id: 6598072dc50abdfec41af80a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: beomi
      type: user
    createdAt: '2024-01-05T13:42:05.000Z'
    data:
      status: closed
    id: 6598072dc50abdfec41af80f
    type: status-change
  author: beomi
  created_at: 2024-01-05 13:42:05+00:00
  id: 6598072dc50abdfec41af80f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: beomi/Yi-Ko-6B
repo_type: model
status: closed
target_branch: null
title: Would you be willing to share some training details?
