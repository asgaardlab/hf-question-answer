!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomaarsen
conflicting_files: []
created_at: 2024-01-11 20:04:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-11T20:04:17.000Z'
    data:
      edited: true
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: "<p>Hello!</p>\n<h2 id=\"pull-request-overview\">Pull Request overview</h2>\n\
          <ul>\n<li>Allow loading a tokenizer via <code>AutoTokenizer(\"togethercomputer/m2-bert-80M-8k-retrieval\"\
          , trust_remote_code=True)</code></li>\n</ul>\n<h2 id=\"details\">Details</h2>\n\
          <p>I wanted to load the tokenizer for this model via <code>AutoTokenizer</code>:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          \ntesting_string = <span class=\"hljs-string\">\"Every morning, I make a\
          \ cup of coffee to start my day.\"</span>\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"togethercomputer/m2-bert-80M-8k-retrieval\"\
          </span>,\n    trust_remote_code=<span class=\"hljs-literal\">True</span>,\n\
          )\ninput_ids = tokenizer(\n    [testing_string],\n    return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>,\n    padding=<span class=\"hljs-string\"\
          >\"max_length\"</span>,\n    return_token_type_ids=<span class=\"hljs-literal\"\
          >False</span>,\n    truncation=<span class=\"hljs-literal\">True</span>,\n\
          )\n<span class=\"hljs-built_in\">print</span>(input_ids)\n</code></pre>\n\
          <p>But that isn't possible I'm afraid:</p>\n<pre><code>You are using a model\
          \ of type m2_bert to instantiate a model of type bert. This is not supported\
          \ for all configurations of models and can yield errors.\nTraceback (most\
          \ recent call last):\n  File \"c:\\code\\m2-bert-80M-8k-retrieval\\demo.py\"\
          , line 6, in &lt;module&gt;\n    tokenizer = AutoTokenizer.from_pretrained(\n\
          \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\tom\\\
          .conda\\envs\\sentence-transformers\\Lib\\site-packages\\transformers\\\
          models\\auto\\tokenization_auto.py\", line 710, in from_pretrained\n   \
          \ tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path,\
          \ **kwargs)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\tom\\.conda\\envs\\sentence-transformers\\Lib\\site-packages\\\
          transformers\\dynamic_module_utils.py\", line 480, in get_class_from_dynamic_module\n\
          \    module_file, class_name = class_reference.split(\".\")\n    ^^^^^^^^^^^^^^^^^^^^^^^\n\
          ValueError: not enough values to unpack (expected 2, got 1)\n</code></pre>\n\
          <p>The reasoning is that this line does not work: <a href=\"https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval/blob/main/config.json#L12\"\
          >https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval/blob/main/config.json#L12</a></p>\n\
          <p>This is two-fold:</p>\n<ol>\n<li>The AutoTokenizer in the auto_map expects\
          \ a tuple: the first parameter should be a \"slow\" tokenizer, and the second\
          \ a \"fast\" tokenizer.</li>\n<li>The values there should be classes, not\
          \ models.</li>\n</ol>\n<p>The normal approach is to fully include the correct\
          \ tokenizer files in this repository as well (cc: <span data-props=\"{&quot;user&quot;:&quot;osanseviero&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/osanseviero\"\
          >@<span class=\"underline\">osanseviero</span></a></span>\n\n\t</span></span>\
          \ to make sure). So in this PR I've included the <code>bert-base-cased</code>\
          \ tokenizer, but with <code>model_max_length</code> set to 8192 in the <code>tokenizer_config.json</code>.\
          \ This prevents users from having to specify it themselves.</p>\n<h2 id=\"\
          after-this-pr\">After this PR</h2>\n<p>The above script now returns:</p>\n\
          <pre><code>{'input_ids': tensor([[ 101, 4081, 2106,  ...,    0,    0,  \
          \  0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n</code></pre>\n\
          <p>\U0001F389</p>\n<ul>\n<li>Tom Aarsen</li>\n</ul>\n"
        raw: "Hello!\n\n## Pull Request overview\n* Allow loading a tokenizer via\
          \ `AutoTokenizer(\"togethercomputer/m2-bert-80M-8k-retrieval\", trust_remote_code=True)`\n\
          \n## Details\nI wanted to load the tokenizer for this model via `AutoTokenizer`:\n\
          ```python\nfrom transformers import AutoTokenizer\n\ntesting_string = \"\
          Every morning, I make a cup of coffee to start my day.\"\n\ntokenizer =\
          \ AutoTokenizer.from_pretrained(\n    \"togethercomputer/m2-bert-80M-8k-retrieval\"\
          ,\n    trust_remote_code=True,\n)\ninput_ids = tokenizer(\n    [testing_string],\n\
          \    return_tensors=\"pt\",\n    padding=\"max_length\",\n    return_token_type_ids=False,\n\
          \    truncation=True,\n)\nprint(input_ids)\n```\nBut that isn't possible\
          \ I'm afraid:\n```\nYou are using a model of type m2_bert to instantiate\
          \ a model of type bert. This is not supported for all configurations of\
          \ models and can yield errors.\nTraceback (most recent call last):\n  File\
          \ \"c:\\code\\m2-bert-80M-8k-retrieval\\demo.py\", line 6, in <module>\n\
          \    tokenizer = AutoTokenizer.from_pretrained(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\tom\\.conda\\envs\\sentence-transformers\\Lib\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py\", line 710, in from_pretrained\n\
          \    tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path,\
          \ **kwargs)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\tom\\.conda\\envs\\sentence-transformers\\Lib\\site-packages\\\
          transformers\\dynamic_module_utils.py\", line 480, in get_class_from_dynamic_module\n\
          \    module_file, class_name = class_reference.split(\".\")\n    ^^^^^^^^^^^^^^^^^^^^^^^\n\
          ValueError: not enough values to unpack (expected 2, got 1)\n```\n\nThe\
          \ reasoning is that this line does not work: https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval/blob/main/config.json#L12\n\
          \nThis is two-fold:\n1. The AutoTokenizer in the auto_map expects a tuple:\
          \ the first parameter should be a \"slow\" tokenizer, and the second a \"\
          fast\" tokenizer.\n2. The values there should be classes, not models.\n\n\
          The normal approach is to fully include the correct tokenizer files in this\
          \ repository as well (cc: @osanseviero to make sure). So in this PR I've\
          \ included the `bert-base-cased` tokenizer, but with `model_max_length`\
          \ set to 8192 in the `tokenizer_config.json`. This prevents users from having\
          \ to specify it themselves.\n\n## After this PR\nThe above script now returns:\n\
          ```\n{'input_ids': tensor([[ 101, 4081, 2106,  ...,    0,    0,    0]]),\
          \ 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n```\n\U0001F389\
          \n\n- Tom Aarsen"
        updatedAt: '2024-01-11T20:11:38.854Z'
      numEdits: 1
      reactions: []
    id: 65a049c1ff65c2d46d7f8638
    type: comment
  author: tomaarsen
  content: "Hello!\n\n## Pull Request overview\n* Allow loading a tokenizer via `AutoTokenizer(\"\
    togethercomputer/m2-bert-80M-8k-retrieval\", trust_remote_code=True)`\n\n## Details\n\
    I wanted to load the tokenizer for this model via `AutoTokenizer`:\n```python\n\
    from transformers import AutoTokenizer\n\ntesting_string = \"Every morning, I\
    \ make a cup of coffee to start my day.\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
    \    \"togethercomputer/m2-bert-80M-8k-retrieval\",\n    trust_remote_code=True,\n\
    )\ninput_ids = tokenizer(\n    [testing_string],\n    return_tensors=\"pt\",\n\
    \    padding=\"max_length\",\n    return_token_type_ids=False,\n    truncation=True,\n\
    )\nprint(input_ids)\n```\nBut that isn't possible I'm afraid:\n```\nYou are using\
    \ a model of type m2_bert to instantiate a model of type bert. This is not supported\
    \ for all configurations of models and can yield errors.\nTraceback (most recent\
    \ call last):\n  File \"c:\\code\\m2-bert-80M-8k-retrieval\\demo.py\", line 6,\
    \ in <module>\n    tokenizer = AutoTokenizer.from_pretrained(\n              \
    \  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\tom\\.conda\\envs\\sentence-transformers\\\
    Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 710,\
    \ in from_pretrained\n    tokenizer_class = get_class_from_dynamic_module(class_ref,\
    \ pretrained_model_name_or_path, **kwargs)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\tom\\.conda\\envs\\sentence-transformers\\Lib\\site-packages\\\
    transformers\\dynamic_module_utils.py\", line 480, in get_class_from_dynamic_module\n\
    \    module_file, class_name = class_reference.split(\".\")\n    ^^^^^^^^^^^^^^^^^^^^^^^\n\
    ValueError: not enough values to unpack (expected 2, got 1)\n```\n\nThe reasoning\
    \ is that this line does not work: https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval/blob/main/config.json#L12\n\
    \nThis is two-fold:\n1. The AutoTokenizer in the auto_map expects a tuple: the\
    \ first parameter should be a \"slow\" tokenizer, and the second a \"fast\" tokenizer.\n\
    2. The values there should be classes, not models.\n\nThe normal approach is to\
    \ fully include the correct tokenizer files in this repository as well (cc: @osanseviero\
    \ to make sure). So in this PR I've included the `bert-base-cased` tokenizer,\
    \ but with `model_max_length` set to 8192 in the `tokenizer_config.json`. This\
    \ prevents users from having to specify it themselves.\n\n## After this PR\nThe\
    \ above script now returns:\n```\n{'input_ids': tensor([[ 101, 4081, 2106,  ...,\
    \    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n\
    ```\n\U0001F389\n\n- Tom Aarsen"
  created_at: 2024-01-11 20:04:17+00:00
  edited: true
  hidden: false
  id: 65a049c1ff65c2d46d7f8638
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    createdAt: '2024-01-11T20:05:58.000Z'
    data:
      oid: fdd629a0b4b0d3543fd2cfe70db482ffdcfe95a7
      parents:
      - 66ea5d6b12ab7e3d332bba708d76f83ce2909b2e
      subject: 'Add bert-base-cased tokenizer with model_max_length: 8192'
    id: 65a04a260000000000000000
    type: commit
  author: deleted
  created_at: 2024-01-11 20:05:58+00:00
  id: 65a04a260000000000000000
  oid: fdd629a0b4b0d3543fd2cfe70db482ffdcfe95a7
  summary: 'Add bert-base-cased tokenizer with model_max_length: 8192'
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    createdAt: '2024-01-11T20:07:15.000Z'
    data:
      oid: 7d7932ee225d71aadc1ba6d16b2fcadba10db703
      parents:
      - fdd629a0b4b0d3543fd2cfe70db482ffdcfe95a7
      subject: Remove AutoTokenizer from the config
    id: 65a04a730000000000000000
    type: commit
  author: deleted
  created_at: 2024-01-11 20:07:15+00:00
  id: 65a04a730000000000000000
  oid: 7d7932ee225d71aadc1ba6d16b2fcadba10db703
  summary: Remove AutoTokenizer from the config
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-11T20:11:40.000Z'
    data:
      status: open
    id: 65a04b7cc2a026427af5c3ab
    type: status-change
  author: tomaarsen
  created_at: 2024-01-11 20:11:40+00:00
  id: 65a04b7cc2a026427af5c3ab
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
      fullname: Dan Fu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: danfu09
      type: user
    createdAt: '2024-01-12T00:44:52.000Z'
    data:
      edited: false
      editors:
      - danfu09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.952568769454956
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
          fullname: Dan Fu
          isHf: false
          isPro: false
          name: danfu09
          type: user
        html: "<p>Thank you for these PRs, they\u2019re really helpful! I saw you\
          \ referenced Slack messages in one of them, trying to find that message\
          \ to chat more!</p>\n"
        raw: "Thank you for these PRs, they\u2019re really helpful! I saw you referenced\
          \ Slack messages in one of them, trying to find that message to chat more!"
        updatedAt: '2024-01-12T00:44:52.477Z'
      numEdits: 0
      reactions: []
    id: 65a08b84300957620ba548ed
    type: comment
  author: danfu09
  content: "Thank you for these PRs, they\u2019re really helpful! I saw you referenced\
    \ Slack messages in one of them, trying to find that message to chat more!"
  created_at: 2024-01-12 00:44:52+00:00
  edited: false
  hidden: false
  id: 65a08b84300957620ba548ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
      fullname: Dan Fu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: danfu09
      type: user
    createdAt: '2024-01-12T21:34:48.000Z'
    data:
      status: merged
    id: 65a1b0781051c2b0dbf4ccdd
    type: status-change
  author: danfu09
  created_at: 2024-01-12 21:34:48+00:00
  id: 65a1b0781051c2b0dbf4ccdd
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 3fabfd0ef3a94da1d20e28df4034a4a031e775f1
num: 3
repo_id: togethercomputer/m2-bert-80M-8k-retrieval
repo_type: model
status: merged
target_branch: refs/heads/main
title: Allow loading via AutoTokenizer
