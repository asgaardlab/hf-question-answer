!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomaarsen
conflicting_files: []
created_at: 2024-01-11 19:48:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-11T19:48:41.000Z'
    data:
      edited: true
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: "<p>Hello!</p>\n<h2 id=\"pull-request-overview\">Pull Request overview</h2>\n\
          <ul>\n<li>Allow loading via <code>AutoModel</code></li>\n</ul>\n<h2 id=\"\
          details\">Details</h2>\n<p>I wanted to experiment with loading this model\
          \ with just <code>AutoModel</code>:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModel\n\nmax_seq_length = <span class=\"\
          hljs-number\">8192</span>\ntesting_string = <span class=\"hljs-string\"\
          >\"Every morning, I make a cup of coffee to start my day.\"</span>\nmodel\
          \ = AutoModel.from_pretrained(\n    <span class=\"hljs-string\">\"togethercomputer/m2-bert-80M-8k-retrieval\"\
          </span>,\n    trust_remote_code=<span class=\"hljs-literal\">True</span>\n\
          )\n\ntokenizer = AutoTokenizer.from_pretrained(\n    <span class=\"hljs-string\"\
          >\"bert-base-uncased\"</span>,\n    model_max_length=max_seq_length\n)\n\
          input_ids = tokenizer(\n    [testing_string],\n    return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>,\n    padding=<span class=\"hljs-string\"\
          >\"max_length\"</span>,\n    return_token_type_ids=<span class=\"hljs-literal\"\
          >False</span>,\n    truncation=<span class=\"hljs-literal\">True</span>,\n\
          \    max_length=max_seq_length\n)\n\nencoder_outputs, pooled_output = model(**input_ids)\n\
          <span class=\"hljs-built_in\">print</span>(encoder_outputs.shape, pooled_output.shape)\n\
          </code></pre>\n<p>But I ran into this issue:</p>\n<pre><code>You are using\
          \ a model of type m2_bert to instantiate a model of type bert. This is not\
          \ supported for all configurations of models and can yield errors.\nTraceback\
          \ (most recent call last):\n  File \"c:\\code\\m2-bert-80M-8k-retrieval\\\
          demo.py\", line 5, in &lt;module&gt;\n    model = AutoModel.from_pretrained(\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\tom\\.conda\\\
          envs\\sentence-transformers\\Lib\\site-packages\\transformers\\models\\\
          auto\\auto_factory.py\", line 519, in from_pretrained\n    raise ValueError(\n\
          ValueError: Unrecognized configuration class &lt;class 'transformers_modules.togethercomputer.m2-bert-80M-8k-retrieval.66ea5d6b12ab7e3d332bba708d76f83ce2909b2e.configuration_bert.BertConfig'&gt;\
          \ for this kind of AutoModel: AutoModel.\nModel type should be one of AlbertConfig,\
          \ AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig,\
          \ BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig,\
          \ BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig,\
          \ Blip2Config, BloomConfig, BridgeTowerConfig, CamembertConfig, CanineConfig,\
          \ ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, CodeGenConfig,\
          \ ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config,\
          \ CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig,\
          \ Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig,\
          \ DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig,\
          \ Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig,\
          \ EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig,\
          \ ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig,\
          \ FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig,\
          \ GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig,\
          \ GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig,\
          \ GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig,\
          \ InformerConfig, JukeboxConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config,\
          \ LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config,\
          \ LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig,\
          \ Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig,\
          \ MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MobileBertConfig,\
          \ MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config,\
          \ MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig,\
          \ NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig,\
          \ OpenAIGPTConfig, OPTConfig, OwlViTConfig, PegasusConfig, PegasusXConfig,\
          \ PerceiverConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig,\
          \ QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig,\
          \ RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig,\
          \ RoFormerConfig, RwkvConfig, SamConfig, SegformerConfig, SEWConfig, SEWDConfig,\
          \ Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig,\
          \ SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig,\
          \ T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig,\
          \ TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig,\
          \ TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig,\
          \ VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig,\
          \ ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VivitConfig, Wav2Vec2Config,\
          \ Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig,\
          \ XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig,\
          \ XLNetConfig, XmodConfig, YolosConfig, YosoConfig.\n</code></pre>\n<p>In\
          \ short, the custom BertConfig is not defined in AutoModel, so it doesn't\
          \ know what class to initialize. This PR fixes this by also setting a value\
          \ for <code>AutoModel</code> in the <code>config.json</code>.</p>\n<h2 id=\"\
          after-this-pr\">After this PR</h2>\n<p>The above script should now return:</p>\n\
          <pre><code>You are using a model of type m2_bert to instantiate a model\
          \ of type bert. This is not supported for all configurations of models and\
          \ can yield errors.\n-- Bidirectional: True\n-- Using Long Conv Residual:\
          \ True\n-- Hyena w: 10\n-- Hyena w mod: 1\n-- Hyena filter order: 128\n\
          -- Hyena filter dropout: 0.2\n-- Hyena filter wd: 0.1\n-- Hyena filter emb\
          \ dim: 5\n-- Hyena filter lr: 0.001\n-- Hyena filter lr pos emb: 1e-05\n\
          torch.Size([1, 8192, 768]) torch.Size([1, 768])\n</code></pre>\n<p>\U0001F389\
          </p>\n<ul>\n<li>Tom Aarsen</li>\n</ul>\n"
        raw: "Hello!\n\n## Pull Request overview\n* Allow loading via `AutoModel`\n\
          \n## Details\nI wanted to experiment with loading this model with just `AutoModel`:\n\
          ```python\nfrom transformers import AutoTokenizer, AutoModel\n\nmax_seq_length\
          \ = 8192\ntesting_string = \"Every morning, I make a cup of coffee to start\
          \ my day.\"\nmodel = AutoModel.from_pretrained(\n    \"togethercomputer/m2-bert-80M-8k-retrieval\"\
          ,\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
          \    \"bert-base-uncased\",\n    model_max_length=max_seq_length\n)\ninput_ids\
          \ = tokenizer(\n    [testing_string],\n    return_tensors=\"pt\",\n    padding=\"\
          max_length\",\n    return_token_type_ids=False,\n    truncation=True,\n\
          \    max_length=max_seq_length\n)\n\nencoder_outputs, pooled_output = model(**input_ids)\n\
          print(encoder_outputs.shape, pooled_output.shape)\n```\nBut I ran into this\
          \ issue:\n```\nYou are using a model of type m2_bert to instantiate a model\
          \ of type bert. This is not supported for all configurations of models and\
          \ can yield errors.\nTraceback (most recent call last):\n  File \"c:\\code\\\
          m2-bert-80M-8k-retrieval\\demo.py\", line 5, in <module>\n    model = AutoModel.from_pretrained(\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\tom\\.conda\\\
          envs\\sentence-transformers\\Lib\\site-packages\\transformers\\models\\\
          auto\\auto_factory.py\", line 519, in from_pretrained\n    raise ValueError(\n\
          ValueError: Unrecognized configuration class <class 'transformers_modules.togethercomputer.m2-bert-80M-8k-retrieval.66ea5d6b12ab7e3d332bba708d76f83ce2909b2e.configuration_bert.BertConfig'>\
          \ for this kind of AutoModel: AutoModel.\nModel type should be one of AlbertConfig,\
          \ AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig,\
          \ BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig,\
          \ BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig,\
          \ Blip2Config, BloomConfig, BridgeTowerConfig, CamembertConfig, CanineConfig,\
          \ ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, CodeGenConfig,\
          \ ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config,\
          \ CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig,\
          \ Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig,\
          \ DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig,\
          \ Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig,\
          \ EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig,\
          \ ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig,\
          \ FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig,\
          \ GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig,\
          \ GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig,\
          \ GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig,\
          \ InformerConfig, JukeboxConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config,\
          \ LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config,\
          \ LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig,\
          \ Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig,\
          \ MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MobileBertConfig,\
          \ MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config,\
          \ MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig,\
          \ NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig,\
          \ OpenAIGPTConfig, OPTConfig, OwlViTConfig, PegasusConfig, PegasusXConfig,\
          \ PerceiverConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig,\
          \ QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig,\
          \ RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig,\
          \ RoFormerConfig, RwkvConfig, SamConfig, SegformerConfig, SEWConfig, SEWDConfig,\
          \ Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig,\
          \ SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig,\
          \ T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig,\
          \ TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig,\
          \ TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig,\
          \ VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig,\
          \ ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VivitConfig, Wav2Vec2Config,\
          \ Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig,\
          \ XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig,\
          \ XLNetConfig, XmodConfig, YolosConfig, YosoConfig.\n```\n\nIn short, the\
          \ custom BertConfig is not defined in AutoModel, so it doesn't know what\
          \ class to initialize. This PR fixes this by also setting a value for `AutoModel`\
          \ in the `config.json`.\n\n## After this PR\nThe above script should now\
          \ return:\n```\nYou are using a model of type m2_bert to instantiate a model\
          \ of type bert. This is not supported for all configurations of models and\
          \ can yield errors.\n-- Bidirectional: True\n-- Using Long Conv Residual:\
          \ True\n-- Hyena w: 10\n-- Hyena w mod: 1\n-- Hyena filter order: 128\n\
          -- Hyena filter dropout: 0.2\n-- Hyena filter wd: 0.1\n-- Hyena filter emb\
          \ dim: 5\n-- Hyena filter lr: 0.001\n-- Hyena filter lr pos emb: 1e-05\n\
          torch.Size([1, 8192, 768]) torch.Size([1, 768])\n```\n\U0001F389\n\n- Tom\
          \ Aarsen"
        updatedAt: '2024-01-11T19:55:55.404Z'
      numEdits: 1
      reactions: []
    id: 65a046190c8d993b17f35696
    type: comment
  author: tomaarsen
  content: "Hello!\n\n## Pull Request overview\n* Allow loading via `AutoModel`\n\n\
    ## Details\nI wanted to experiment with loading this model with just `AutoModel`:\n\
    ```python\nfrom transformers import AutoTokenizer, AutoModel\n\nmax_seq_length\
    \ = 8192\ntesting_string = \"Every morning, I make a cup of coffee to start my\
    \ day.\"\nmodel = AutoModel.from_pretrained(\n    \"togethercomputer/m2-bert-80M-8k-retrieval\"\
    ,\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
    \    \"bert-base-uncased\",\n    model_max_length=max_seq_length\n)\ninput_ids\
    \ = tokenizer(\n    [testing_string],\n    return_tensors=\"pt\",\n    padding=\"\
    max_length\",\n    return_token_type_ids=False,\n    truncation=True,\n    max_length=max_seq_length\n\
    )\n\nencoder_outputs, pooled_output = model(**input_ids)\nprint(encoder_outputs.shape,\
    \ pooled_output.shape)\n```\nBut I ran into this issue:\n```\nYou are using a\
    \ model of type m2_bert to instantiate a model of type bert. This is not supported\
    \ for all configurations of models and can yield errors.\nTraceback (most recent\
    \ call last):\n  File \"c:\\code\\m2-bert-80M-8k-retrieval\\demo.py\", line 5,\
    \ in <module>\n    model = AutoModel.from_pretrained(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\tom\\.conda\\envs\\sentence-transformers\\Lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\", line 519, in from_pretrained\n\
    \    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers_modules.togethercomputer.m2-bert-80M-8k-retrieval.66ea5d6b12ab7e3d332bba708d76f83ce2909b2e.configuration_bert.BertConfig'>\
    \ for this kind of AutoModel: AutoModel.\nModel type should be one of AlbertConfig,\
    \ AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig,\
    \ BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig,\
    \ BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig,\
    \ Blip2Config, BloomConfig, BridgeTowerConfig, CamembertConfig, CanineConfig,\
    \ ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, CodeGenConfig, ConditionalDetrConfig,\
    \ ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig,\
    \ CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig,\
    \ DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig,\
    \ DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig,\
    \ DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig,\
    \ EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig,\
    \ FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig,\
    \ GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig,\
    \ GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig,\
    \ HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig,\
    \ LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig,\
    \ LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig,\
    \ M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig,\
    \ MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig,\
    \ MgpstrConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig,\
    \ MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig,\
    \ NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig,\
    \ OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, OwlViTConfig, PegasusConfig, PegasusXConfig,\
    \ PerceiverConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig,\
    \ QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig,\
    \ RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig,\
    \ SamConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config,\
    \ SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig,\
    \ Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig,\
    \ TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig,\
    \ TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig,\
    \ VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig,\
    \ ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VivitConfig, Wav2Vec2Config,\
    \ Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig,\
    \ XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig,\
    \ XmodConfig, YolosConfig, YosoConfig.\n```\n\nIn short, the custom BertConfig\
    \ is not defined in AutoModel, so it doesn't know what class to initialize. This\
    \ PR fixes this by also setting a value for `AutoModel` in the `config.json`.\n\
    \n## After this PR\nThe above script should now return:\n```\nYou are using a\
    \ model of type m2_bert to instantiate a model of type bert. This is not supported\
    \ for all configurations of models and can yield errors.\n-- Bidirectional: True\n\
    -- Using Long Conv Residual: True\n-- Hyena w: 10\n-- Hyena w mod: 1\n-- Hyena\
    \ filter order: 128\n-- Hyena filter dropout: 0.2\n-- Hyena filter wd: 0.1\n--\
    \ Hyena filter emb dim: 5\n-- Hyena filter lr: 0.001\n-- Hyena filter lr pos emb:\
    \ 1e-05\ntorch.Size([1, 8192, 768]) torch.Size([1, 768])\n```\n\U0001F389\n\n\
    - Tom Aarsen"
  created_at: 2024-01-11 19:48:41+00:00
  edited: true
  hidden: false
  id: 65a046190c8d993b17f35696
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    createdAt: '2024-01-11T19:53:09.000Z'
    data:
      oid: cbdd8c4cdda4f27a7d1980af3968434074b6d075
      parents:
      - 66ea5d6b12ab7e3d332bba708d76f83ce2909b2e
      subject: Allow loading via AutoModel
    id: 65a047250000000000000000
    type: commit
  author: deleted
  created_at: 2024-01-11 19:53:09+00:00
  id: 65a047250000000000000000
  oid: cbdd8c4cdda4f27a7d1980af3968434074b6d075
  summary: Allow loading via AutoModel
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-11T19:55:56.000Z'
    data:
      status: open
    id: 65a047cc2df78443661716ef
    type: status-change
  author: tomaarsen
  created_at: 2024-01-11 19:55:56+00:00
  id: 65a047cc2df78443661716ef
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
      fullname: Dan Fu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: danfu09
      type: user
    createdAt: '2024-01-12T21:36:00.000Z'
    data:
      edited: false
      editors:
      - danfu09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37232768535614014
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
          fullname: Dan Fu
          isHf: false
          isPro: false
          name: danfu09
          type: user
        html: '<p>Thank you!!</p>

          '
        raw: Thank you!!
        updatedAt: '2024-01-12T21:36:00.188Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a1b0c0ea9873876888296d
    id: 65a1b0c0ea98738768882968
    type: comment
  author: danfu09
  content: Thank you!!
  created_at: 2024-01-12 21:36:00+00:00
  edited: false
  hidden: false
  id: 65a1b0c0ea98738768882968
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
      fullname: Dan Fu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: danfu09
      type: user
    createdAt: '2024-01-12T21:36:00.000Z'
    data:
      status: merged
    id: 65a1b0c0ea9873876888296d
    type: status-change
  author: danfu09
  created_at: 2024-01-12 21:36:00+00:00
  id: 65a1b0c0ea9873876888296d
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 7d69cb93398727280a5d5c5eabfb16b24a62c5a6
num: 2
repo_id: togethercomputer/m2-bert-80M-8k-retrieval
repo_type: model
status: merged
target_branch: refs/heads/main
title: Allow loading via AutoModel
