!!python/object:huggingface_hub.community.DiscussionWithDetails
author: longquan
conflicting_files: null
created_at: 2023-08-21 23:57:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acc2b650f8780d55c50a5ea3c4ee6499.svg
      fullname: qiulongquan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: longquan
      type: user
    createdAt: '2023-08-22T00:57:57.000Z'
    data:
      edited: true
      editors:
      - longquan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9277306795120239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acc2b650f8780d55c50a5ea3c4ee6499.svg
          fullname: qiulongquan
          isHf: false
          isPro: true
          name: longquan
          type: user
        html: '<p>We merged the [izumi-lab/llm-japanese-dataset-vanilla] dataset with
          the 40,000 pieces of customized data, and then trained the LoRA model using
          the [meta-llama/Llama-2-7b-chat-hf] model, with the hyperparameters listed
          below.<br>{<br>  "auto_mapping": null,<br>  "base_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",<br>  "bias":
          "none",<br>  "fan_in_fan_out": false,<br>  "inference_mode": true,<br>  "init_lora_weights":
          true,<br>  "layers_pattern": null,<br>  "layers_to_transform": null,<br>  "lora_alpha":
          16,<br>  "lora_dropout": 0.05,<br>  "modules_to_save": null,<br>  "peft_type":
          "LORA",<br>  "r": 8,<br>  "revision": null,<br>  "target_modules": [<br>    "q_proj",<br>    "v_proj"<br>  ],<br>  "task_type":
          "CAUSAL_LM"<br>}<br>batch_size: int = 128,<br>micro_batch_size: int = 4,<br>num_epochs:
          int = 1,<br>learning_rate: float = 3e-4,<br>cutoff_len: int = 256,<br>val_set_size:
          int = 2000,</p>

          <p>Testing the new trained model revealed that it was completely unable
          to generate content related to the customized data, and it seems that the
          customized data was not trained into the LoRA model.<br>The validation loss
          for LoRA model training drops very smoothly to around 0.58 (training log
          summary table below)</p>

          <p>{<br>  "best_metric": 0.5886363387107849,<br>  "best_model_checkpoint":
          "./lora-alpaca_0821/checkpoint-1200",<br>  "epoch": 0.5269730612468951,<br>  "global_step":
          1200,<br>  "is_hyper_param_search": false,<br>  "is_local_process_zero":
          true,<br>  "is_world_process_zero": true,<br>   {<br>      "epoch": 0.53,<br>      "learning_rate":
          0.0001484152503445108,<br>      "loss": 0.3999,<br>      "step": 1200<br>    },<br>    {<br>      "epoch":
          0.53,<br>      "eval_loss": 0.5886363387107849,<br>      "eval_runtime":
          102.4819,<br>      "eval_samples_per_second": 19.516,<br>      "eval_steps_per_second":
          2.439,<br>      "step": 1200<br>    }<br>  ],<br>  "max_steps": 2277,<br>  "num_train_epochs":
          1,<br>  "total_flos": 1.1445327441611981e+18,<br>  "trial_name": null,<br>  "trial_params":
          null<br>}</p>

          <p>{''eval_loss'': 0.5886363387107849, ''eval_runtime'': 102.4819, ''eval_samples_per_second'':
          19.516, ''eval_steps_per_second'': 2.439, ''epoch'': 0.53}</p>

          <p>How can I fit the custom data into the LoRA model?<br>Any help you can
          give would be greatly appreciated!</p>

          '
        raw: "We merged the [izumi-lab/llm-japanese-dataset-vanilla] dataset with\
          \ the 40,000 pieces of customized data, and then trained the LoRA model\
          \ using the [meta-llama/Llama-2-7b-chat-hf] model, with the hyperparameters\
          \ listed below.\n{\n  \"auto_mapping\": null,\n  \"base_model_name_or_path\"\
          : \"meta-llama/Llama-2-7b-chat-hf\",\n  \"bias\": \"none\",\n  \"fan_in_fan_out\"\
          : false,\n  \"inference_mode\": true,\n  \"init_lora_weights\": true,\n\
          \  \"layers_pattern\": null,\n  \"layers_to_transform\": null,\n  \"lora_alpha\"\
          : 16,\n  \"lora_dropout\": 0.05,\n  \"modules_to_save\": null,\n  \"peft_type\"\
          : \"LORA\",\n  \"r\": 8,\n  \"revision\": null,\n  \"target_modules\": [\n\
          \    \"q_proj\",\n    \"v_proj\"\n  ],\n  \"task_type\": \"CAUSAL_LM\"\n\
          }\nbatch_size: int = 128,\nmicro_batch_size: int = 4,\nnum_epochs: int =\
          \ 1,\nlearning_rate: float = 3e-4,\ncutoff_len: int = 256,\nval_set_size:\
          \ int = 2000,\n\nTesting the new trained model revealed that it was completely\
          \ unable to generate content related to the customized data, and it seems\
          \ that the customized data was not trained into the LoRA model.\nThe validation\
          \ loss for LoRA model training drops very smoothly to around 0.58 (training\
          \ log summary table below)\n\n{\n  \"best_metric\": 0.5886363387107849,\n\
          \  \"best_model_checkpoint\": \"./lora-alpaca_0821/checkpoint-1200\",\n\
          \  \"epoch\": 0.5269730612468951,\n  \"global_step\": 1200,\n  \"is_hyper_param_search\"\
          : false,\n  \"is_local_process_zero\": true,\n  \"is_world_process_zero\"\
          : true,\n   {\n      \"epoch\": 0.53,\n      \"learning_rate\": 0.0001484152503445108,\n\
          \      \"loss\": 0.3999,\n      \"step\": 1200\n    },\n    {\n      \"\
          epoch\": 0.53,\n      \"eval_loss\": 0.5886363387107849,\n      \"eval_runtime\"\
          : 102.4819,\n      \"eval_samples_per_second\": 19.516,\n      \"eval_steps_per_second\"\
          : 2.439,\n      \"step\": 1200\n    }\n  ],\n  \"max_steps\": 2277,\n  \"\
          num_train_epochs\": 1,\n  \"total_flos\": 1.1445327441611981e+18,\n  \"\
          trial_name\": null,\n  \"trial_params\": null\n}\n\n{'eval_loss': 0.5886363387107849,\
          \ 'eval_runtime': 102.4819, 'eval_samples_per_second': 19.516, 'eval_steps_per_second':\
          \ 2.439, 'epoch': 0.53}\n\nHow can I fit the custom data into the LoRA model?\n\
          Any help you can give would be greatly appreciated!"
        updatedAt: '2023-08-22T01:05:47.967Z'
      numEdits: 2
      reactions: []
    id: 64e408154392e3565cec2498
    type: comment
  author: longquan
  content: "We merged the [izumi-lab/llm-japanese-dataset-vanilla] dataset with the\
    \ 40,000 pieces of customized data, and then trained the LoRA model using the\
    \ [meta-llama/Llama-2-7b-chat-hf] model, with the hyperparameters listed below.\n\
    {\n  \"auto_mapping\": null,\n  \"base_model_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\"\
    ,\n  \"bias\": \"none\",\n  \"fan_in_fan_out\": false,\n  \"inference_mode\":\
    \ true,\n  \"init_lora_weights\": true,\n  \"layers_pattern\": null,\n  \"layers_to_transform\"\
    : null,\n  \"lora_alpha\": 16,\n  \"lora_dropout\": 0.05,\n  \"modules_to_save\"\
    : null,\n  \"peft_type\": \"LORA\",\n  \"r\": 8,\n  \"revision\": null,\n  \"\
    target_modules\": [\n    \"q_proj\",\n    \"v_proj\"\n  ],\n  \"task_type\": \"\
    CAUSAL_LM\"\n}\nbatch_size: int = 128,\nmicro_batch_size: int = 4,\nnum_epochs:\
    \ int = 1,\nlearning_rate: float = 3e-4,\ncutoff_len: int = 256,\nval_set_size:\
    \ int = 2000,\n\nTesting the new trained model revealed that it was completely\
    \ unable to generate content related to the customized data, and it seems that\
    \ the customized data was not trained into the LoRA model.\nThe validation loss\
    \ for LoRA model training drops very smoothly to around 0.58 (training log summary\
    \ table below)\n\n{\n  \"best_metric\": 0.5886363387107849,\n  \"best_model_checkpoint\"\
    : \"./lora-alpaca_0821/checkpoint-1200\",\n  \"epoch\": 0.5269730612468951,\n\
    \  \"global_step\": 1200,\n  \"is_hyper_param_search\": false,\n  \"is_local_process_zero\"\
    : true,\n  \"is_world_process_zero\": true,\n   {\n      \"epoch\": 0.53,\n  \
    \    \"learning_rate\": 0.0001484152503445108,\n      \"loss\": 0.3999,\n    \
    \  \"step\": 1200\n    },\n    {\n      \"epoch\": 0.53,\n      \"eval_loss\"\
    : 0.5886363387107849,\n      \"eval_runtime\": 102.4819,\n      \"eval_samples_per_second\"\
    : 19.516,\n      \"eval_steps_per_second\": 2.439,\n      \"step\": 1200\n   \
    \ }\n  ],\n  \"max_steps\": 2277,\n  \"num_train_epochs\": 1,\n  \"total_flos\"\
    : 1.1445327441611981e+18,\n  \"trial_name\": null,\n  \"trial_params\": null\n\
    }\n\n{'eval_loss': 0.5886363387107849, 'eval_runtime': 102.4819, 'eval_samples_per_second':\
    \ 19.516, 'eval_steps_per_second': 2.439, 'epoch': 0.53}\n\nHow can I fit the\
    \ custom data into the LoRA model?\nAny help you can give would be greatly appreciated!"
  created_at: 2023-08-21 23:57:57+00:00
  edited: true
  hidden: false
  id: 64e408154392e3565cec2498
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/acc2b650f8780d55c50a5ea3c4ee6499.svg
      fullname: qiulongquan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: longquan
      type: user
    createdAt: '2023-08-22T00:58:15.000Z'
    data:
      from: use new trainedcompletely unable to generate content related to the customized
        data
      to: use new trained LoRA model that completely unable to generate content related
        to the customized data
    id: 64e40827030431c0c680e3e3
    type: title-change
  author: longquan
  created_at: 2023-08-21 23:58:15+00:00
  id: 64e40827030431c0c680e3e3
  new_title: use new trained LoRA model that completely unable to generate content
    related to the customized data
  old_title: use new trainedcompletely unable to generate content related to the customized
    data
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/acc2b650f8780d55c50a5ea3c4ee6499.svg
      fullname: qiulongquan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: longquan
      type: user
    createdAt: '2023-08-22T00:59:07.000Z'
    data:
      from: use new trained LoRA model that completely unable to generate content
        related to the customized data
      to: Use new trained of LoRA model that completely unable to generate content
        related to the customized data
    id: 64e4085b315c08ea11f12650
    type: title-change
  author: longquan
  created_at: 2023-08-21 23:59:07+00:00
  id: 64e4085b315c08ea11f12650
  new_title: Use new trained of LoRA model that completely unable to generate content
    related to the customized data
  old_title: use new trained LoRA model that completely unable to generate content
    related to the customized data
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Sparticle/llama-2-7b-chat-japanese-lora
repo_type: model
status: open
target_branch: null
title: Use new trained of LoRA model that completely unable to generate content related
  to the customized data
