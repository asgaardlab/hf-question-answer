!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jumerckx
conflicting_files: null
created_at: 2023-04-05 19:30:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4989b46f773f89cfb88f5d6bca25df7e.svg
      fullname: Jules Merckx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jumerckx
      type: user
    createdAt: '2023-04-05T20:30:43.000Z'
    data:
      edited: false
      editors:
      - jumerckx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4989b46f773f89cfb88f5d6bca25df7e.svg
          fullname: Jules Merckx
          isHf: false
          isPro: false
          name: jumerckx
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;yhavinga&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yhavinga\"\
          >@<span class=\"underline\">yhavinga</span></a></span>\n\n\t</span></span>,<br>I\
          \ was wondering whether you are planning on uploading the UL2 models that\
          \ have been fine-tuned for summarization (mentioned <a href=\"https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models#evaluation-setup\"\
          >here</a>)? I see the fine-tuned translation models are available, but don't\
          \ see the summarization models, or I might be missing it?<br>Either way,\
          \ your models are tremendously helpful already, we're currently using <a\
          \ href=\"https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cnn-test\">t5-v1.1-base-dutch-cnn-test</a>\
          \ in a school project!</p>\n"
        raw: "Hey @yhavinga,\r\nI was wondering whether you are planning on uploading\
          \ the UL2 models that have been fine-tuned for summarization (mentioned\
          \ [here](https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models#evaluation-setup))?\
          \ I see the fine-tuned translation models are available, but don't see the\
          \ summarization models, or I might be missing it?\r\nEither way, your models\
          \ are tremendously helpful already, we're currently using [t5-v1.1-base-dutch-cnn-test](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cnn-test)\
          \ in a school project!"
        updatedAt: '2023-04-05T20:30:43.570Z'
      numEdits: 0
      reactions: []
    id: 642dda7347833318f0e9a286
    type: comment
  author: jumerckx
  content: "Hey @yhavinga,\r\nI was wondering whether you are planning on uploading\
    \ the UL2 models that have been fine-tuned for summarization (mentioned [here](https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models#evaluation-setup))?\
    \ I see the fine-tuned translation models are available, but don't see the summarization\
    \ models, or I might be missing it?\r\nEither way, your models are tremendously\
    \ helpful already, we're currently using [t5-v1.1-base-dutch-cnn-test](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cnn-test)\
    \ in a school project!"
  created_at: 2023-04-05 19:30:43+00:00
  edited: false
  hidden: false
  id: 642dda7347833318f0e9a286
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
      fullname: Yeb Havinga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yhavinga
      type: user
    createdAt: '2023-04-06T12:44:58.000Z'
    data:
      edited: false
      editors:
      - yhavinga
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
          fullname: Yeb Havinga
          isHf: false
          isPro: false
          name: yhavinga
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;jumerckx&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jumerckx\"\
          >@<span class=\"underline\">jumerckx</span></a></span>\n\n\t</span></span>\
          \ - none of the evaluation-fine-tuned models are released, these are not\
          \ optimal and not intended for inference. (for evaluation of the pre-trained\
          \ models I ran a limited fixed number of fine-tuning steps) At some time\
          \ in the past I became a bit obsessed with trying to improve en-&gt;nl dutch\
          \ translation, the best or most interesting models I've released on the\
          \ hub. (e.g. ul2-large-en-nl which I am quite happy with.. still things\
          \ could be improved but I think its better than google translate and on\
          \ par/better than deepl). In the past I've also done that for summarization,\
          \ but didn't do a recent summarization fine-tune of e.g. ul2-base. I guess\
          \ there are some things to do there for Dutch, e.g. compare bart with t5/ul2\
          \ models, and also trying to deal with longer (input) context lengths, especially\
          \ seen in e.g. the cnn dataset. Maybe the long-t5 models (which didn't do\
          \ too well when I compared them to the other dutch models) shine in the\
          \ long context arena? OTOH the ul2-dutch models have a new tokenizer that\
          \ is about 30% more efficient than the old tokenizer I used, so I still\
          \ guess ul2 will be a good competitor there are well).<br>Thanks for the\
          \ kind words about my models, nice to get feedback that they're useful!<br>--\
          \ Yeb</p>\n"
        raw: 'Hi @jumerckx - none of the evaluation-fine-tuned models are released,
          these are not optimal and not intended for inference. (for evaluation of
          the pre-trained models I ran a limited fixed number of fine-tuning steps)
          At some time in the past I became a bit obsessed with trying to improve
          en->nl dutch translation, the best or most interesting models I''ve released
          on the hub. (e.g. ul2-large-en-nl which I am quite happy with.. still things
          could be improved but I think its better than google translate and on par/better
          than deepl). In the past I''ve also done that for summarization, but didn''t
          do a recent summarization fine-tune of e.g. ul2-base. I guess there are
          some things to do there for Dutch, e.g. compare bart with t5/ul2 models,
          and also trying to deal with longer (input) context lengths, especially
          seen in e.g. the cnn dataset. Maybe the long-t5 models (which didn''t do
          too well when I compared them to the other dutch models) shine in the long
          context arena? OTOH the ul2-dutch models have a new tokenizer that is about
          30% more efficient than the old tokenizer I used, so I still guess ul2 will
          be a good competitor there are well).

          Thanks for the kind words about my models, nice to get feedback that they''re
          useful!

          -- Yeb'
        updatedAt: '2023-04-06T12:44:58.419Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - jumerckx
    id: 642ebeca6bf69c0011953e61
    type: comment
  author: yhavinga
  content: 'Hi @jumerckx - none of the evaluation-fine-tuned models are released,
    these are not optimal and not intended for inference. (for evaluation of the pre-trained
    models I ran a limited fixed number of fine-tuning steps) At some time in the
    past I became a bit obsessed with trying to improve en->nl dutch translation,
    the best or most interesting models I''ve released on the hub. (e.g. ul2-large-en-nl
    which I am quite happy with.. still things could be improved but I think its better
    than google translate and on par/better than deepl). In the past I''ve also done
    that for summarization, but didn''t do a recent summarization fine-tune of e.g.
    ul2-base. I guess there are some things to do there for Dutch, e.g. compare bart
    with t5/ul2 models, and also trying to deal with longer (input) context lengths,
    especially seen in e.g. the cnn dataset. Maybe the long-t5 models (which didn''t
    do too well when I compared them to the other dutch models) shine in the long
    context arena? OTOH the ul2-dutch models have a new tokenizer that is about 30%
    more efficient than the old tokenizer I used, so I still guess ul2 will be a good
    competitor there are well).

    Thanks for the kind words about my models, nice to get feedback that they''re
    useful!

    -- Yeb'
  created_at: 2023-04-06 11:44:58+00:00
  edited: false
  hidden: false
  id: 642ebeca6bf69c0011953e61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
      fullname: Yeb Havinga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yhavinga
      type: user
    createdAt: '2023-05-17T09:36:17.000Z'
    data:
      status: closed
    id: 6464a01106cd98685a9d97e0
    type: status-change
  author: yhavinga
  created_at: 2023-05-17 08:36:17+00:00
  id: 6464a01106cd98685a9d97e0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yhavinga/ul2-base-dutch
repo_type: model
status: closed
target_branch: null
title: UL2 Fine-tuned summarization model
