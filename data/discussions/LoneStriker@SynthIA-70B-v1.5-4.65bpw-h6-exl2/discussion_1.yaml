!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dnhkng
conflicting_files: null
created_at: 2023-10-26 12:52:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnhkng
      type: user
    createdAt: '2023-10-26T13:52:38.000Z'
    data:
      edited: false
      editors:
      - dnhkng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9434323906898499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
          fullname: David
          isHf: false
          isPro: false
          name: dnhkng
          type: user
        html: '<p>I see a bunch of variants of various compression ratios. I like
          to know which is the best size to use. That basically means using up all
          the available GPU ram on 2x 4090s, and having enough for aa decent sized
          context.</p>

          '
        raw: I see a bunch of variants of various compression ratios. I like to know
          which is the best size to use. That basically means using up all the available
          GPU ram on 2x 4090s, and having enough for aa decent sized context.
        updatedAt: '2023-10-26T13:52:38.735Z'
      numEdits: 0
      reactions: []
    id: 653a6f261e1140b0c7dad1f0
    type: comment
  author: dnhkng
  content: I see a bunch of variants of various compression ratios. I like to know
    which is the best size to use. That basically means using up all the available
    GPU ram on 2x 4090s, and having enough for aa decent sized context.
  created_at: 2023-10-26 12:52:38+00:00
  edited: false
  hidden: false
  id: 653a6f261e1140b0c7dad1f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-10-26T20:20:45.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769015908241272
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>This size is a good option and one I run often myself with 2x 4090s.  Some
          folks have had good luck with the 5.0bpw models as well. It really depends
          on what else you''re running on your GPU (desktop vs. headless Linux server).  Anything
          from 4.0 and up should not lose too much in perplexity measurements from
          the base fp16 model.</p>

          '
        raw: This size is a good option and one I run often myself with 2x 4090s.  Some
          folks have had good luck with the 5.0bpw models as well. It really depends
          on what else you're running on your GPU (desktop vs. headless Linux server).  Anything
          from 4.0 and up should not lose too much in perplexity measurements from
          the base fp16 model.
        updatedAt: '2023-10-26T20:20:45.701Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dnhkng
    id: 653aca1df1017cf05b9525ba
    type: comment
  author: LoneStriker
  content: This size is a good option and one I run often myself with 2x 4090s.  Some
    folks have had good luck with the 5.0bpw models as well. It really depends on
    what else you're running on your GPU (desktop vs. headless Linux server).  Anything
    from 4.0 and up should not lose too much in perplexity measurements from the base
    fp16 model.
  created_at: 2023-10-26 19:20:45+00:00
  edited: false
  hidden: false
  id: 653aca1df1017cf05b9525ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
      fullname: Richard Meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hisma
      type: user
    createdAt: '2023-10-27T22:40:06.000Z'
    data:
      edited: false
      editors:
      - Hisma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9613580703735352
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
          fullname: Richard Meyer
          isHf: false
          isPro: false
          name: Hisma
          type: user
        html: '<p>hey I have issues w/ this model not remembering previous context
          while in instruction mode.  For instance, I ask it a question, I get a response,
          and I want to follow up w/ a question related to the previous response,
          and it comes up with a completely random response that had nothing to do
          w/ the previous context.  ie, I ask it to generate code.  I ask it to modify
          the code, and it will spit out code with mistakes.  So I ask it to correct
          the mistakes, and it will pull up some other random code that is completely
          unrelated to what I originally asked.<br>I have a 4.85bpw gguf version of
          this same model and it doesn''t do that.   Any ideas what could be going
          on?  FWIW I''m using n_ctx 8192.  </p>

          '
        raw: "hey I have issues w/ this model not remembering previous context while\
          \ in instruction mode.  For instance, I ask it a question, I get a response,\
          \ and I want to follow up w/ a question related to the previous response,\
          \ and it comes up with a completely random response that had nothing to\
          \ do w/ the previous context.  ie, I ask it to generate code.  I ask it\
          \ to modify the code, and it will spit out code with mistakes.  So I ask\
          \ it to correct the mistakes, and it will pull up some other random code\
          \ that is completely unrelated to what I originally asked.\nI have a 4.85bpw\
          \ gguf version of this same model and it doesn't do that.   Any ideas what\
          \ could be going on?  FWIW I'm using n_ctx 8192.  \n"
        updatedAt: '2023-10-27T22:40:06.208Z'
      numEdits: 0
      reactions: []
    id: 653c3c46ffd60206c876a07b
    type: comment
  author: Hisma
  content: "hey I have issues w/ this model not remembering previous context while\
    \ in instruction mode.  For instance, I ask it a question, I get a response, and\
    \ I want to follow up w/ a question related to the previous response, and it comes\
    \ up with a completely random response that had nothing to do w/ the previous\
    \ context.  ie, I ask it to generate code.  I ask it to modify the code, and it\
    \ will spit out code with mistakes.  So I ask it to correct the mistakes, and\
    \ it will pull up some other random code that is completely unrelated to what\
    \ I originally asked.\nI have a 4.85bpw gguf version of this same model and it\
    \ doesn't do that.   Any ideas what could be going on?  FWIW I'm using n_ctx 8192.\
    \  \n"
  created_at: 2023-10-27 21:40:06+00:00
  edited: false
  hidden: false
  id: 653c3c46ffd60206c876a07b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-10-28T04:39:16.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9083893299102783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>To close this out:<br>As discussed in Discord, the issue here is
          somewhere inside of ooba itself.  The textgen-webui is simply not passing
          the context back to the model for processing, so the model cannot act on
          that information.</p>

          '
        raw: 'To close this out:

          As discussed in Discord, the issue here is somewhere inside of ooba itself.  The
          textgen-webui is simply not passing the context back to the model for processing,
          so the model cannot act on that information.'
        updatedAt: '2023-10-28T04:39:16.937Z'
      numEdits: 0
      reactions: []
    id: 653c90746426f79e29445627
    type: comment
  author: LoneStriker
  content: 'To close this out:

    As discussed in Discord, the issue here is somewhere inside of ooba itself.  The
    textgen-webui is simply not passing the context back to the model for processing,
    so the model cannot act on that information.'
  created_at: 2023-10-28 03:39:16+00:00
  edited: false
  hidden: false
  id: 653c90746426f79e29445627
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
      fullname: Richard Meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hisma
      type: user
    createdAt: '2023-10-28T04:51:30.000Z'
    data:
      edited: false
      editors:
      - Hisma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9920187592506409
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
          fullname: Richard Meyer
          isHf: false
          isPro: false
          name: Hisma
          type: user
        html: '<p>Yeah you can close this one.  The problem was that my prompt needed
          to be more specific.  It''s not an ooba issue specifically, though that
          was part of it.  It was also a synthia issue.  When I was more specific
          in my prompt, ie "can you re-write the previous program"? it then picked
          up the context correctly and modified the original program.<br>I haven''t
          had to be that specific w/ any other models, but that''s no biggie.  It''s
          something I''ll make note of in the future.<br>You can see the chat history
          in discord where we tested it.  Others had the same problem I did until
          they were very deliberate w/ their prompting.</p>

          '
        raw: "Yeah you can close this one.  The problem was that my prompt needed\
          \ to be more specific.  It's not an ooba issue specifically, though that\
          \ was part of it.  It was also a synthia issue.  When I was more specific\
          \ in my prompt, ie \"can you re-write the previous program\"? it then picked\
          \ up the context correctly and modified the original program.\nI haven't\
          \ had to be that specific w/ any other models, but that's no biggie.  It's\
          \ something I'll make note of in the future.  \nYou can see the chat history\
          \ in discord where we tested it.  Others had the same problem I did until\
          \ they were very deliberate w/ their prompting."
        updatedAt: '2023-10-28T04:51:30.106Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 653c9352c2307cc4485aab1b
    type: comment
  author: Hisma
  content: "Yeah you can close this one.  The problem was that my prompt needed to\
    \ be more specific.  It's not an ooba issue specifically, though that was part\
    \ of it.  It was also a synthia issue.  When I was more specific in my prompt,\
    \ ie \"can you re-write the previous program\"? it then picked up the context\
    \ correctly and modified the original program.\nI haven't had to be that specific\
    \ w/ any other models, but that's no biggie.  It's something I'll make note of\
    \ in the future.  \nYou can see the chat history in discord where we tested it.\
    \  Others had the same problem I did until they were very deliberate w/ their\
    \ prompting."
  created_at: 2023-10-28 03:51:30+00:00
  edited: false
  hidden: false
  id: 653c9352c2307cc4485aab1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-10-28T07:33:18.000Z'
    data:
      status: closed
    id: 653cb93e48e79d7e63b2d154
    type: status-change
  author: LoneStriker
  created_at: 2023-10-28 06:33:18+00:00
  id: 653cb93e48e79d7e63b2d154
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/SynthIA-70B-v1.5-4.65bpw-h6-exl2
repo_type: model
status: closed
target_branch: null
title: Is this the best size for 2x 4090?
