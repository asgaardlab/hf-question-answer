!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jgranie
conflicting_files: null
created_at: 2023-05-28 14:15:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67d735259924192b4d425e5e7e650aa8.svg
      fullname: "Jean Grani\xE9"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jgranie
      type: user
    createdAt: '2023-05-28T15:15:30.000Z'
    data:
      edited: false
      editors:
      - jgranie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67d735259924192b4d425e5e7e650aa8.svg
          fullname: "Jean Grani\xE9"
          isHf: false
          isPro: false
          name: jgranie
          type: user
        html: '<p>Hi,<br>Each time I get the same reply for the same question, it
          seems a bit confusing while I''m using a random seed.<br>For example : "can
          you write a poem about a cyberpunk city in a dystopic world" (yes, very
          original ^^) writes me the exact same poetry with this model or for example
          the wizardly-13 uncensored.<br>I''m using oobabooga on mac m1 and I followed
          the parameters given in the model card (and I have upgraded llama.cpp to
          the right version)<br>I don''t know if the models are intended to react
          like this or if the problem is on the oobabooga side (or my installation...)
          ?<br>Any clue ?</p>

          '
        raw: "Hi,\r\nEach time I get the same reply for the same question, it seems\
          \ a bit confusing while I'm using a random seed.\r\nFor example : \"can\
          \ you write a poem about a cyberpunk city in a dystopic world\" (yes, very\
          \ original ^^) writes me the exact same poetry with this model or for example\
          \ the wizardly-13 uncensored.\r\nI'm using oobabooga on mac m1 and I followed\
          \ the parameters given in the model card (and I have upgraded llama.cpp\
          \ to the right version)\r\nI don't know if the models are intended to react\
          \ like this or if the problem is on the oobabooga side (or my installation...)\
          \ ?\r\nAny clue ?"
        updatedAt: '2023-05-28T15:15:30.690Z'
      numEdits: 0
      reactions: []
    id: 6473701271f07ae738d1e432
    type: comment
  author: jgranie
  content: "Hi,\r\nEach time I get the same reply for the same question, it seems\
    \ a bit confusing while I'm using a random seed.\r\nFor example : \"can you write\
    \ a poem about a cyberpunk city in a dystopic world\" (yes, very original ^^)\
    \ writes me the exact same poetry with this model or for example the wizardly-13\
    \ uncensored.\r\nI'm using oobabooga on mac m1 and I followed the parameters given\
    \ in the model card (and I have upgraded llama.cpp to the right version)\r\nI\
    \ don't know if the models are intended to react like this or if the problem is\
    \ on the oobabooga side (or my installation...) ?\r\nAny clue ?"
  created_at: 2023-05-28 14:15:30+00:00
  edited: false
  hidden: false
  id: 6473701271f07ae738d1e432
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-05-29T13:45:46.000Z'
    data:
      edited: true
      editors:
      - Mikael110
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: '<p>If you go the ''parameter'' tab of Oobabooga''s UI what are your
          current settings? Based on how the samplers are configured you can get near-deterministic
          output ,meaning basically the same result every time. </p>

          <p>In particular I''d focus on the <code>top_p</code> parameter. If that
          is set to a low value then you are severely restricting the randomness of
          the model. So my first suggestion is to set it to a value like 0.95 if it
          is not already set to that and see if that fixes your issue.</p>

          <p>You could also try out the Mirostat sampler which is a newer sampler
          that was added recently by setting the <code>mirostat_mode</code>to 1 or
          2 (2 being recommended). That sampler overrides the traditional samplers
          (top_p, top_k, etc) and does a pretty good job at randomizing the output.</p>

          '
        raw: "If you go the 'parameter' tab of Oobabooga's UI what are your current\
          \ settings? Based on how the samplers are configured you can get near-deterministic\
          \ output ,meaning basically the same result every time. \n\nIn particular\
          \ I'd focus on the `top_p` parameter. If that is set to a low value then\
          \ you are severely restricting the randomness of the model. So my first\
          \ suggestion is to set it to a value like 0.95 if it is not already set\
          \ to that and see if that fixes your issue.\n\nYou could also try out the\
          \ Mirostat sampler which is a newer sampler that was added recently by setting\
          \ the `mirostat_mode`to 1 or 2 (2 being recommended). That sampler overrides\
          \ the traditional samplers (top_p, top_k, etc) and does a pretty good job\
          \ at randomizing the output."
        updatedAt: '2023-05-29T13:46:01.443Z'
      numEdits: 1
      reactions: []
    id: 6474ac8a82907acdddedd3d1
    type: comment
  author: Mikael110
  content: "If you go the 'parameter' tab of Oobabooga's UI what are your current\
    \ settings? Based on how the samplers are configured you can get near-deterministic\
    \ output ,meaning basically the same result every time. \n\nIn particular I'd\
    \ focus on the `top_p` parameter. If that is set to a low value then you are severely\
    \ restricting the randomness of the model. So my first suggestion is to set it\
    \ to a value like 0.95 if it is not already set to that and see if that fixes\
    \ your issue.\n\nYou could also try out the Mirostat sampler which is a newer\
    \ sampler that was added recently by setting the `mirostat_mode`to 1 or 2 (2 being\
    \ recommended). That sampler overrides the traditional samplers (top_p, top_k,\
    \ etc) and does a pretty good job at randomizing the output."
  created_at: 2023-05-29 12:45:46+00:00
  edited: true
  hidden: false
  id: 6474ac8a82907acdddedd3d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67d735259924192b4d425e5e7e650aa8.svg
      fullname: "Jean Grani\xE9"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jgranie
      type: user
    createdAt: '2023-05-29T20:43:12.000Z'
    data:
      edited: false
      editors:
      - jgranie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67d735259924192b4d425e5e7e650aa8.svg
          fullname: "Jean Grani\xE9"
          isHf: false
          isPro: false
          name: jgranie
          type: user
        html: '<p>Thanks for your reply, it works fine with the top_p parameter set
          to 0.95 (initially it was 0.5), I will read more attentively the llama readme
          next time :)</p>

          '
        raw: Thanks for your reply, it works fine with the top_p parameter set to
          0.95 (initially it was 0.5), I will read more attentively the llama readme
          next time :)
        updatedAt: '2023-05-29T20:43:12.220Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64750e605ada8510bc47b3b6
    id: 64750e605ada8510bc47b3b5
    type: comment
  author: jgranie
  content: Thanks for your reply, it works fine with the top_p parameter set to 0.95
    (initially it was 0.5), I will read more attentively the llama readme next time
    :)
  created_at: 2023-05-29 19:43:12+00:00
  edited: false
  hidden: false
  id: 64750e605ada8510bc47b3b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/67d735259924192b4d425e5e7e650aa8.svg
      fullname: "Jean Grani\xE9"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jgranie
      type: user
    createdAt: '2023-05-29T20:43:12.000Z'
    data:
      status: closed
    id: 64750e605ada8510bc47b3b6
    type: status-change
  author: jgranie
  created_at: 2023-05-29 19:43:12+00:00
  id: 64750e605ada8510bc47b3b6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/guanaco-13B-GGML
repo_type: model
status: closed
target_branch: null
title: Same generation each time
