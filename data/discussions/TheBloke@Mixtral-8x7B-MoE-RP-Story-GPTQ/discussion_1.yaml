!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MrHillsss
conflicting_files: null
created_at: 2023-12-15 00:24:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b35c6d28384d90f6a8f770eddaf6cd30.svg
      fullname: Meme
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrHillsss
      type: user
    createdAt: '2023-12-15T00:24:32.000Z'
    data:
      edited: false
      editors:
      - MrHillsss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5523045659065247
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b35c6d28384d90f6a8f770eddaf6cd30.svg
          fullname: Meme
          isHf: false
          isPro: false
          name: MrHillsss
          type: user
        html: '<p>Getting an error while loading</p>

          '
        raw: Getting an error while loading
        updatedAt: '2023-12-15T00:24:32.127Z'
      numEdits: 0
      reactions: []
    id: 657b9cc08c3aee70546abae8
    type: comment
  author: MrHillsss
  content: Getting an error while loading
  created_at: 2023-12-15 00:24:32+00:00
  edited: false
  hidden: false
  id: 657b9cc08c3aee70546abae8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640d1967c63516d41ed1d05c/ZuRb18FYGAlz1TnYU5cSf.jpeg?w=200&h=200&f=face
      fullname: Amelia Dolinska
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raaniel
      type: user
    createdAt: '2023-12-15T22:01:48.000Z'
    data:
      edited: false
      editors:
      - Raaniel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4901180863380432
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640d1967c63516d41ed1d05c/ZuRb18FYGAlz1TnYU5cSf.jpeg?w=200&h=200&f=face
          fullname: Amelia Dolinska
          isHf: false
          isPro: false
          name: Raaniel
          type: user
        html: '<p>Hi, I am experiencing similiar error while loading a model. ValueError:
          Unrecognized layer: model.layers.0.block_sparse_moe.experts.0.w1.bias</p>

          '
        raw: 'Hi, I am experiencing similiar error while loading a model. ValueError:
          Unrecognized layer: model.layers.0.block_sparse_moe.experts.0.w1.bias'
        updatedAt: '2023-12-15T22:01:48.715Z'
      numEdits: 0
      reactions: []
    id: 657ccccc9a9e347d4b7d4045
    type: comment
  author: Raaniel
  content: 'Hi, I am experiencing similiar error while loading a model. ValueError:
    Unrecognized layer: model.layers.0.block_sparse_moe.experts.0.w1.bias'
  created_at: 2023-12-15 22:01:48+00:00
  edited: false
  hidden: false
  id: 657ccccc9a9e347d4b7d4045
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4ff13d65ed6d82041e35517242365ba.svg
      fullname: Novel Stories
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Novelo
      type: user
    createdAt: '2023-12-16T01:02:38.000Z'
    data:
      edited: false
      editors:
      - Novelo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6604130268096924
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4ff13d65ed6d82041e35517242365ba.svg
          fullname: Novel Stories
          isHf: false
          isPro: false
          name: Novelo
          type: user
        html: '<p>Same here, tried 32, 128 etc, same error.</p>

          '
        raw: Same here, tried 32, 128 etc, same error.
        updatedAt: '2023-12-16T01:02:38.765Z'
      numEdits: 0
      reactions: []
    id: 657cf72e365456e362ec35cd
    type: comment
  author: Novelo
  content: Same here, tried 32, 128 etc, same error.
  created_at: 2023-12-16 01:02:38+00:00
  edited: false
  hidden: false
  id: 657cf72e365456e362ec35cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e706fe7bb21a046f80895901704b204a.svg
      fullname: Jeffrey Gilbert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Renegadesoffun
      type: user
    createdAt: '2023-12-16T11:15:39.000Z'
    data:
      edited: false
      editors:
      - Renegadesoffun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3498168885707855
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e706fe7bb21a046f80895901704b204a.svg
          fullname: Jeffrey Gilbert
          isHf: false
          isPro: false
          name: Renegadesoffun
          type: user
        html: '<p>hmm i played with loading it with autogptq and it said it loaded
          on my 24gb vram but for first response got this error<br>RuntimeError: cannot
          reshape tensor of 0 elements into shape [-1, 1, 0] because the unspecified
          dimension size -1 can be any value and is ambiguous</p>

          <p>dunno what im doin wrong</p>

          <p>Traceback (most recent call last):<br>  File "D:\booga\text-generation-webui\modules\callbacks.py",
          line 57, in gentask<br>    ret = self.mfunc(callback=_callback, *args, **self.kwargs)<br>  File
          "D:\booga\text-generation-webui\modules\text_generation.py", line 351, in
          generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1764, in generate<br>    return self.sample(<br>  File "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2861, in sample<br>    outputs = self(<br>  File "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\transformers\models\mixtral\modeling_mixtral.py",
          line 1213, in forward<br>    outputs = self.model(<br>  File "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\transformers\models\mixtral\modeling_mixtral.py",
          line 1081, in forward<br>    layer_outputs = decoder_layer(<br>  File "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\transformers\models\mixtral\modeling_mixtral.py",
          line 810, in forward<br>    hidden_states, router_logits = self.block_sparse_moe(hidden_states)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\transformers\models\mixtral\modeling_mixtral.py",
          line 708, in forward<br>    router_logits = self.gate(hidden_states)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\booga\text-generation-webui\installer_files\env\lib\site-packages\auto_gptq\nn_modules\qlinear\qlinear_cuda_old.py",
          line 239, in forward<br>    zeros = zeros.reshape(-1, 1, zeros.shape[1]
          * zeros.shape[2])<br>RuntimeError: cannot reshape tensor of 0 elements into
          shape [-1, 1, 0] because the unspecified dimension size -1 can be any value
          and is ambiguous</p>

          '
        raw: "hmm i played with loading it with autogptq and it said it loaded on\
          \ my 24gb vram but for first response got this error\nRuntimeError: cannot\
          \ reshape tensor of 0 elements into shape [-1, 1, 0] because the unspecified\
          \ dimension size -1 can be any value and is ambiguous\n\ndunno what im doin\
          \ wrong\n\nTraceback (most recent call last):\n  File \"D:\\booga\\text-generation-webui\\\
          modules\\callbacks.py\", line 57, in gentask\n    ret = self.mfunc(callback=_callback,\
          \ *args, **self.kwargs)\n  File \"D:\\booga\\text-generation-webui\\modules\\\
          text_generation.py\", line 351, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\\
          site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n\
          \    return func(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 1764, in generate\n    return self.sample(\n  File \"D:\\booga\\\
          text-generation-webui\\installer_files\\env\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 2861, in sample\n    outputs = self(\n  File\
          \ \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return\
          \ self._call_impl(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\mixtral\\modeling_mixtral.py\", line 1213, in forward\n\
          \    outputs = self.model(\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\mixtral\\\
          modeling_mixtral.py\", line 1081, in forward\n    layer_outputs = decoder_layer(\n\
          \  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n\
          \    return self._call_impl(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\mixtral\\modeling_mixtral.py\", line 810, in forward\n\
          \    hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n\
          \  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n\
          \    return self._call_impl(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\mixtral\\modeling_mixtral.py\", line 708, in forward\n\
          \    router_logits = self.gate(hidden_states)\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\nn_modules\\qlinear\\\
          qlinear_cuda_old.py\", line 239, in forward\n    zeros = zeros.reshape(-1,\
          \ 1, zeros.shape[1] * zeros.shape[2])\nRuntimeError: cannot reshape tensor\
          \ of 0 elements into shape [-1, 1, 0] because the unspecified dimension\
          \ size -1 can be any value and is ambiguous"
        updatedAt: '2023-12-16T11:15:39.557Z'
      numEdits: 0
      reactions: []
    id: 657d86dbd70b7308f3560a27
    type: comment
  author: Renegadesoffun
  content: "hmm i played with loading it with autogptq and it said it loaded on my\
    \ 24gb vram but for first response got this error\nRuntimeError: cannot reshape\
    \ tensor of 0 elements into shape [-1, 1, 0] because the unspecified dimension\
    \ size -1 can be any value and is ambiguous\n\ndunno what im doin wrong\n\nTraceback\
    \ (most recent call last):\n  File \"D:\\booga\\text-generation-webui\\modules\\\
    callbacks.py\", line 57, in gentask\n    ret = self.mfunc(callback=_callback,\
    \ *args, **self.kwargs)\n  File \"D:\\booga\\text-generation-webui\\modules\\\
    text_generation.py\", line 351, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
    \  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
    torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
    \ **kwargs)\n  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\\
    lib\\site-packages\\transformers\\generation\\utils.py\", line 1764, in generate\n\
    \    return self.sample(\n  File \"D:\\booga\\text-generation-webui\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2861, in sample\n\
    \    outputs = self(\n  File \"D:\\booga\\text-generation-webui\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n\
    \    return self._call_impl(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"D:\\\
    booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\mixtral\\modeling_mixtral.py\", line 1213, in forward\n    outputs = self.model(\n\
    \  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return\
    \ self._call_impl(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"D:\\\
    booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\mixtral\\modeling_mixtral.py\", line 1081, in forward\n    layer_outputs\
    \ = decoder_layer(\n  File \"D:\\booga\\text-generation-webui\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n\
    \    return self._call_impl(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"D:\\\
    booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\mixtral\\modeling_mixtral.py\", line 810, in forward\n    hidden_states,\
    \ router_logits = self.block_sparse_moe(hidden_states)\n  File \"D:\\booga\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n \
    \ File \"D:\\booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\mixtral\\modeling_mixtral.py\", line\
    \ 708, in forward\n    router_logits = self.gate(hidden_states)\n  File \"D:\\\
    booga\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args,\
    \ **kwargs)\n  File \"D:\\booga\\text-generation-webui\\installer_files\\env\\\
    lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n\
    \    return forward_call(*args, **kwargs)\n  File \"D:\\booga\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\auto_gptq\\nn_modules\\qlinear\\qlinear_cuda_old.py\"\
    , line 239, in forward\n    zeros = zeros.reshape(-1, 1, zeros.shape[1] * zeros.shape[2])\n\
    RuntimeError: cannot reshape tensor of 0 elements into shape [-1, 1, 0] because\
    \ the unspecified dimension size -1 can be any value and is ambiguous"
  created_at: 2023-12-16 11:15:39+00:00
  edited: false
  hidden: false
  id: 657d86dbd70b7308f3560a27
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Mixtral-8x7B-MoE-RP-Story-GPTQ
repo_type: model
status: open
target_branch: null
title: 'ValueError: ## Could not find model.layers.0.mlp.down_proj.* in model'
