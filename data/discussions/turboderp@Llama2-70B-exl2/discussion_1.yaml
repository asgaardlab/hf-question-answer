!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2024-01-01 03:43:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-01T03:43:49.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8283133506774902
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Hey!<br>Could you make a 3.75bpw exl2-2 quant of this Llama2 70b
          please?<br>I run a 3090+3060 config, and I''d be curious to test the best
          quant of the base Llama 70b I can run at a decent context (Alpha 2, 6912
          ctx) with your new optimized quantization method.<br>Happy new year 2024,
          and thanks for providing fast inference at the best quantization quality
          available for the greatest number of people!</p>

          '
        raw: 'Hey!

          Could you make a 3.75bpw exl2-2 quant of this Llama2 70b please?

          I run a 3090+3060 config, and I''d be curious to test the best quant of
          the base Llama 70b I can run at a decent context (Alpha 2, 6912 ctx) with
          your new optimized quantization method.

          Happy new year 2024, and thanks for providing fast inference at the best
          quantization quality available for the greatest number of people!'
        updatedAt: '2024-01-01T03:44:06.109Z'
      numEdits: 1
      reactions: []
    id: 659234f54a24a3877831ce55
    type: comment
  author: Nexesenex
  content: 'Hey!

    Could you make a 3.75bpw exl2-2 quant of this Llama2 70b please?

    I run a 3090+3060 config, and I''d be curious to test the best quant of the base
    Llama 70b I can run at a decent context (Alpha 2, 6912 ctx) with your new optimized
    quantization method.

    Happy new year 2024, and thanks for providing fast inference at the best quantization
    quality available for the greatest number of people!'
  created_at: 2024-01-01 03:43:49+00:00
  edited: true
  hidden: false
  id: 659234f54a24a3877831ce55
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: turboderp/Llama2-70B-exl2
repo_type: model
status: open
target_branch: null
title: 3.75bpw quant.
