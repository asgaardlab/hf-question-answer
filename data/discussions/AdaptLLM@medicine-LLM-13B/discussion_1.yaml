!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ttkciar
conflicting_files: null
created_at: 2024-01-02 04:39:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661973791156-noauth.jpeg?w=200&h=200&f=face
      fullname: TTK Ciar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ttkciar
      type: user
    createdAt: '2024-01-02T04:39:58.000Z'
    data:
      edited: true
      editors:
      - ttkciar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9635507464408875
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661973791156-noauth.jpeg?w=200&h=200&f=face
          fullname: TTK Ciar
          isHf: false
          isPro: false
          name: ttkciar
          type: user
        html: '<p>Thank you very much for making your hard work available for general
          use!  I have been using Medalpaca-13B as a medical research assistant for
          about a year, and have tried other models to see if they do a better job
          (Asclepius-13B, Meditron-7B).  Today I tried medicine-LLM-13B.</p>

          <p>This model is very good at explaining anatomy and biochemistry, but is
          not very knowledgeable about diseases (in particular diabetes and ulcerative
          colitis), and tends to get distracted by minor wording differences in its
          prompt.  For example, it dealt much better with "dedifferentiation theory"
          than with "genetic dedifferentiation theory", even though they are exactly
          the same thing -- with the latter, it went off on unrelated tangents about
          genetic factors.</p>

          <p>It also has some funny inference glitches which makes me think its training
          format might not have been strictly consistent.  It will sometimes infer
          additional prompt questions followed by "[/SYS]" or "[/INST]" before answering
          the question (and its own question).</p>

          <p>That having been said, this is a <em>much</em> better medical inference
          model than Asclepius-13B or Meditron-7B, hands-down.  I think it might be
          a better model than Medalpaca-13B for some applications, but for my particular
          use-case I am not yet convinced it can replace Medalpaca-13B.  I will continue
          to use it alongside Medalpaca-13B for a while to figure out its strengths.</p>

          '
        raw: 'Thank you very much for making your hard work available for general
          use!  I have been using Medalpaca-13B as a medical research assistant for
          about a year, and have tried other models to see if they do a better job
          (Asclepius-13B, Meditron-7B).  Today I tried medicine-LLM-13B.


          This model is very good at explaining anatomy and biochemistry, but is not
          very knowledgeable about diseases (in particular diabetes and ulcerative
          colitis), and tends to get distracted by minor wording differences in its
          prompt.  For example, it dealt much better with "dedifferentiation theory"
          than with "genetic dedifferentiation theory", even though they are exactly
          the same thing -- with the latter, it went off on unrelated tangents about
          genetic factors.


          It also has some funny inference glitches which makes me think its training
          format might not have been strictly consistent.  It will sometimes infer
          additional prompt questions followed by "[/SYS]" or "[/INST]" before answering
          the question (and its own question).


          That having been said, this is a *much* better medical inference model than
          Asclepius-13B or Meditron-7B, hands-down.  I think it might be a better
          model than Medalpaca-13B for some applications, but for my particular use-case
          I am not yet convinced it can replace Medalpaca-13B.  I will continue to
          use it alongside Medalpaca-13B for a while to figure out its strengths.'
        updatedAt: '2024-01-02T04:43:00.592Z'
      numEdits: 1
      reactions: []
    id: 6593939ec0b1372b2e3f369f
    type: comment
  author: ttkciar
  content: 'Thank you very much for making your hard work available for general use!  I
    have been using Medalpaca-13B as a medical research assistant for about a year,
    and have tried other models to see if they do a better job (Asclepius-13B, Meditron-7B).  Today
    I tried medicine-LLM-13B.


    This model is very good at explaining anatomy and biochemistry, but is not very
    knowledgeable about diseases (in particular diabetes and ulcerative colitis),
    and tends to get distracted by minor wording differences in its prompt.  For example,
    it dealt much better with "dedifferentiation theory" than with "genetic dedifferentiation
    theory", even though they are exactly the same thing -- with the latter, it went
    off on unrelated tangents about genetic factors.


    It also has some funny inference glitches which makes me think its training format
    might not have been strictly consistent.  It will sometimes infer additional prompt
    questions followed by "[/SYS]" or "[/INST]" before answering the question (and
    its own question).


    That having been said, this is a *much* better medical inference model than Asclepius-13B
    or Meditron-7B, hands-down.  I think it might be a better model than Medalpaca-13B
    for some applications, but for my particular use-case I am not yet convinced it
    can replace Medalpaca-13B.  I will continue to use it alongside Medalpaca-13B
    for a while to figure out its strengths.'
  created_at: 2024-01-02 04:39:58+00:00
  edited: true
  hidden: false
  id: 6593939ec0b1372b2e3f369f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650801ced5578ef7e20b33d4/oLptSnKMecbu62EgglmO6.png?w=200&h=200&f=face
      fullname: AdaptLLM
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: AdaptLLM
      type: user
    createdAt: '2024-01-02T05:43:03.000Z'
    data:
      edited: true
      editors:
      - AdaptLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6347131133079529
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650801ced5578ef7e20b33d4/oLptSnKMecbu62EgglmO6.png?w=200&h=200&f=face
          fullname: AdaptLLM
          isHf: false
          isPro: false
          name: AdaptLLM
          type: user
        html: "<p>Hi, thank you for sharing your detailed feedback!\U0001F497 </p>\n\
          <p>Regarding the inference glitches you've noticed, it seems like there\
          \ might be a misunderstanding about the prompt template. For the <a href=\"\
          https://huggingface.co/AdaptLLM/medicine-chat\">chat models</a>, the \"\
          [/SYS]\" or \"[/INST]\" tags are indeed part of the training template.</p>\n\
          <p>However, <strong>for the <a href=\"https://huggingface.co/AdaptLLM/medicine-LLM-13B\"\
          >13B base model</a>,  you don't need to adhere to the same prompt template</strong>.\
          \ Instead, you can just simply put your request as the input, as illustrated\
          \ in the example below:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"AdaptLLM/medicine-LLM-13B\"</span>)\ntokenizer\
          \ = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"AdaptLLM/medicine-LLM-13B\"\
          </span>, use_fast=<span class=\"hljs-literal\">False</span>)\n\n<span class=\"\
          hljs-comment\"># Put your input here:</span>\nuser_input = <span class=\"\
          hljs-string\">'''Question: Which of the following is an example of monosomy?</span>\n\
          <span class=\"hljs-string\">Options:</span>\n<span class=\"hljs-string\"\
          >- 46,XX</span>\n<span class=\"hljs-string\">- 47,XXX</span>\n<span class=\"\
          hljs-string\">- 69,XYY</span>\n<span class=\"hljs-string\">- 45,X</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">Please\
          \ provide your choice first and then provide explanations if possible.'''</span>\n\
          \n<span class=\"hljs-comment\"># <span class=\"hljs-doctag\">NOTE:</span>\
          \ you do NOT need to follow the prompt template for chat models here</span>\n\
          prompt = user_input\n\ninputs = tokenizer(prompt, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>, add_special_tokens=<span class=\"hljs-literal\"\
          >False</span>).input_ids.to(model.device)\n\noutputs = model.generate(input_ids=inputs,\
          \ max_length=<span class=\"hljs-number\">2048</span>)[<span class=\"hljs-number\"\
          >0</span>]\n\nanswer_start = <span class=\"hljs-built_in\">int</span>(inputs.shape[-<span\
          \ class=\"hljs-number\">1</span>])\npred = tokenizer.decode(outputs[answer_start:],\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'###\
          \ User Input:\\n<span class=\"hljs-subst\">{user_input}</span>\\n\\n###\
          \ Assistant Output:\\n<span class=\"hljs-subst\">{pred}</span>'</span>)\n\
          </code></pre>\n<p>We hope this clarifies things! Looking forward to your\
          \ continued feedback!</p>\n"
        raw: "Hi, thank you for sharing your detailed feedback!\U0001F497 \n\nRegarding\
          \ the inference glitches you've noticed, it seems like there might be a\
          \ misunderstanding about the prompt template. For the [chat models](https://huggingface.co/AdaptLLM/medicine-chat),\
          \ the \"[/SYS]\" or \"[/INST]\" tags are indeed part of the training template.\n\
          \nHowever, **for the [13B base model](https://huggingface.co/AdaptLLM/medicine-LLM-13B),\
          \  you don't need to adhere to the same prompt template**. Instead, you\
          \ can just simply put your request as the input, as illustrated in the example\
          \ below:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"AdaptLLM/medicine-LLM-13B\"\
          )\ntokenizer = AutoTokenizer.from_pretrained(\"AdaptLLM/medicine-LLM-13B\"\
          , use_fast=False)\n\n# Put your input here:\nuser_input = '''Question: Which\
          \ of the following is an example of monosomy?\nOptions:\n- 46,XX\n- 47,XXX\n\
          - 69,XYY\n- 45,X\n\nPlease provide your choice first and then provide explanations\
          \ if possible.'''\n\n# NOTE: you do NOT need to follow the prompt template\
          \ for chat models here\nprompt = user_input\n\ninputs = tokenizer(prompt,\
          \ return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n\
          \noutputs = model.generate(input_ids=inputs, max_length=2048)[0]\n\nanswer_start\
          \ = int(inputs.shape[-1])\npred = tokenizer.decode(outputs[answer_start:],\
          \ skip_special_tokens=True)\n\nprint(f'### User Input:\\n{user_input}\\\
          n\\n### Assistant Output:\\n{pred}')\n```\n\nWe hope this clarifies things!\
          \ Looking forward to your continued feedback!"
        updatedAt: '2024-01-02T05:51:35.510Z'
      numEdits: 1
      reactions: []
    id: 6593a26787944e494eb24949
    type: comment
  author: AdaptLLM
  content: "Hi, thank you for sharing your detailed feedback!\U0001F497 \n\nRegarding\
    \ the inference glitches you've noticed, it seems like there might be a misunderstanding\
    \ about the prompt template. For the [chat models](https://huggingface.co/AdaptLLM/medicine-chat),\
    \ the \"[/SYS]\" or \"[/INST]\" tags are indeed part of the training template.\n\
    \nHowever, **for the [13B base model](https://huggingface.co/AdaptLLM/medicine-LLM-13B),\
    \  you don't need to adhere to the same prompt template**. Instead, you can just\
    \ simply put your request as the input, as illustrated in the example below:\n\
    \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\
    model = AutoModelForCausalLM.from_pretrained(\"AdaptLLM/medicine-LLM-13B\")\n\
    tokenizer = AutoTokenizer.from_pretrained(\"AdaptLLM/medicine-LLM-13B\", use_fast=False)\n\
    \n# Put your input here:\nuser_input = '''Question: Which of the following is\
    \ an example of monosomy?\nOptions:\n- 46,XX\n- 47,XXX\n- 69,XYY\n- 45,X\n\nPlease\
    \ provide your choice first and then provide explanations if possible.'''\n\n\
    # NOTE: you do NOT need to follow the prompt template for chat models here\nprompt\
    \ = user_input\n\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n\
    \noutputs = model.generate(input_ids=inputs, max_length=2048)[0]\n\nanswer_start\
    \ = int(inputs.shape[-1])\npred = tokenizer.decode(outputs[answer_start:], skip_special_tokens=True)\n\
    \nprint(f'### User Input:\\n{user_input}\\n\\n### Assistant Output:\\n{pred}')\n\
    ```\n\nWe hope this clarifies things! Looking forward to your continued feedback!"
  created_at: 2024-01-02 05:43:03+00:00
  edited: true
  hidden: false
  id: 6593a26787944e494eb24949
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650801ced5578ef7e20b33d4/oLptSnKMecbu62EgglmO6.png?w=200&h=200&f=face
      fullname: AdaptLLM
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: AdaptLLM
      type: user
    createdAt: '2024-01-02T14:36:44.000Z'
    data:
      edited: false
      editors:
      - AdaptLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8029050827026367
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650801ced5578ef7e20b33d4/oLptSnKMecbu62EgglmO6.png?w=200&h=200&f=face
          fullname: AdaptLLM
          isHf: false
          isPro: false
          name: AdaptLLM
          type: user
        html: "<p>BTW, \U0001F917we highly recommend switching to the <a href=\"https://huggingface.co/AdaptLLM/medicine-chat\"\
          >chat model</a> developed from llama-2-chat-7b for better response quality!\
          \ </p>\n"
        raw: "BTW, \U0001F917we highly recommend switching to the [chat model](https://huggingface.co/AdaptLLM/medicine-chat)\
          \ developed from llama-2-chat-7b for better response quality! "
        updatedAt: '2024-01-02T14:36:44.807Z'
      numEdits: 0
      reactions: []
    id: 65941f7cac02633c0d2b8099
    type: comment
  author: AdaptLLM
  content: "BTW, \U0001F917we highly recommend switching to the [chat model](https://huggingface.co/AdaptLLM/medicine-chat)\
    \ developed from llama-2-chat-7b for better response quality! "
  created_at: 2024-01-02 14:36:44+00:00
  edited: false
  hidden: false
  id: 65941f7cac02633c0d2b8099
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: AdaptLLM/medicine-LLM-13B
repo_type: model
status: open
target_branch: null
title: Quick critique
