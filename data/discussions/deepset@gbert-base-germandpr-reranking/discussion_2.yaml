!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andreP
conflicting_files: null
created_at: 2023-06-26 10:54:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2023-06-26T11:54:19.000Z'
    data:
      edited: false
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9004351496696472
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>Hi,</p>\n<p>question - many Cross-Encoders take pairs of Question\
          \ and Candidate Documents as input and deliver a single relevance score\
          \ value from -10 ... +10 for each such pair (not related ... very relevant).<br>There\
          \ is also a model wrapper CrossEncoder in the Sentence Transformers package,\
          \ that I'd like to use.<br>I can use this CrossEncoder together with this\
          \ model, but instead of single numbers i get pairs of numbers (Label0/Label1).\
          \ Is Label1 the positive score (match) and i can use this as relevance score?\
          \ Seems to work quite well...<br>Or should I somehow combine this two values?\
          \ A bit more documentation would be great, because not everyone want to\
          \ use the full Haystack/FARM-stack and just want to use the model itself,\
          \ e.g. via a seperate Inference Server etc.<br>How is it trained, what does\
          \ each of these 2 classification labels indicate?</p>\n<p>Another question:\
          \ How well does it work on mixed German / English?</p>\n<p>Thx and best\
          \ regards,<br>Andr\xE9</p>\n"
        raw: "Hi,\r\n\r\nquestion - many Cross-Encoders take pairs of Question and\
          \ Candidate Documents as input and deliver a single relevance score value\
          \ from -10 ... +10 for each such pair (not related ... very relevant).\r\
          \nThere is also a model wrapper CrossEncoder in the Sentence Transformers\
          \ package, that I'd like to use.\r\nI can use this CrossEncoder together\
          \ with this model, but instead of single numbers i get pairs of numbers\
          \ (Label0/Label1). Is Label1 the positive score (match) and i can use this\
          \ as relevance score? Seems to work quite well...\r\nOr should I somehow\
          \ combine this two values? A bit more documentation would be great, because\
          \ not everyone want to use the full Haystack/FARM-stack and just want to\
          \ use the model itself, e.g. via a seperate Inference Server etc.\r\nHow\
          \ is it trained, what does each of these 2 classification labels indicate?\r\
          \n\r\nAnother question: How well does it work on mixed German / English?\r\
          \n\r\nThx and best regards,\r\nAndr\xE9"
        updatedAt: '2023-06-26T11:54:19.930Z'
      numEdits: 0
      reactions: []
    id: 64997c6b906b89ee66f3f964
    type: comment
  author: andreP
  content: "Hi,\r\n\r\nquestion - many Cross-Encoders take pairs of Question and Candidate\
    \ Documents as input and deliver a single relevance score value from -10 ... +10\
    \ for each such pair (not related ... very relevant).\r\nThere is also a model\
    \ wrapper CrossEncoder in the Sentence Transformers package, that I'd like to\
    \ use.\r\nI can use this CrossEncoder together with this model, but instead of\
    \ single numbers i get pairs of numbers (Label0/Label1). Is Label1 the positive\
    \ score (match) and i can use this as relevance score? Seems to work quite well...\r\
    \nOr should I somehow combine this two values? A bit more documentation would\
    \ be great, because not everyone want to use the full Haystack/FARM-stack and\
    \ just want to use the model itself, e.g. via a seperate Inference Server etc.\r\
    \nHow is it trained, what does each of these 2 classification labels indicate?\r\
    \n\r\nAnother question: How well does it work on mixed German / English?\r\n\r\
    \nThx and best regards,\r\nAndr\xE9"
  created_at: 2023-06-26 10:54:19+00:00
  edited: false
  hidden: false
  id: 64997c6b906b89ee66f3f964
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: deepset/gbert-base-germandpr-reranking
repo_type: model
status: open
target_branch: null
title: Model with Sentence Transforrmers
