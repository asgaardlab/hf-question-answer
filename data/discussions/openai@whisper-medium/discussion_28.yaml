!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ait-paca
conflicting_files: null
created_at: 2023-11-22 09:50:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c47b09195e6010a5ed591fd134a69709.svg
      fullname: Ait Paca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ait-paca
      type: user
    createdAt: '2023-11-22T09:50:01.000Z'
    data:
      edited: false
      editors:
      - ait-paca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8736869096755981
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c47b09195e6010a5ed591fd134a69709.svg
          fullname: Ait Paca
          isHf: false
          isPro: false
          name: ait-paca
          type: user
        html: '<p>Hi,<br>Thanks for sharing this model and related work.</p>

          <p>I downloaded the Whisper-Medium model using HF_hub snapshot download,
          by ignoring the patterns for msgpack, h5, and safetensors. Thereby, only
          pytorch_model.bin as the model file, and all the rest of the repo files
          are downloaded.</p>

          <p>In config.json, the torch_dtype is given as float32. If I use default
          settings for pipeline or AutoConfig..., AutoModel..., it works fine. However,
          if I try using it with torch_dtype = torch.float16, it gives error of different
          sizes tensor.</p>

          <p>Is it even possible to use (any) Whisper pytorch model with different
          dtype than for the entry in config.json or I''m making any basic mistake?
          If it is possible to use with different dtype, what adaptation I need to
          make?</p>

          '
        raw: "Hi,\r\nThanks for sharing this model and related work.\r\n\r\nI downloaded\
          \ the Whisper-Medium model using HF_hub snapshot download, by ignoring the\
          \ patterns for msgpack, h5, and safetensors. Thereby, only pytorch_model.bin\
          \ as the model file, and all the rest of the repo files are downloaded.\r\
          \n\r\nIn config.json, the torch_dtype is given as float32. If I use default\
          \ settings for pipeline or AutoConfig..., AutoModel..., it works fine. However,\
          \ if I try using it with torch_dtype = torch.float16, it gives error of\
          \ different sizes tensor.\r\n\r\nIs it even possible to use (any) Whisper\
          \ pytorch model with different dtype than for the entry in config.json or\
          \ I'm making any basic mistake? If it is possible to use with different\
          \ dtype, what adaptation I need to make?"
        updatedAt: '2023-11-22T09:50:01.619Z'
      numEdits: 0
      reactions: []
    id: 655dcec959fa2e3099757557
    type: comment
  author: ait-paca
  content: "Hi,\r\nThanks for sharing this model and related work.\r\n\r\nI downloaded\
    \ the Whisper-Medium model using HF_hub snapshot download, by ignoring the patterns\
    \ for msgpack, h5, and safetensors. Thereby, only pytorch_model.bin as the model\
    \ file, and all the rest of the repo files are downloaded.\r\n\r\nIn config.json,\
    \ the torch_dtype is given as float32. If I use default settings for pipeline\
    \ or AutoConfig..., AutoModel..., it works fine. However, if I try using it with\
    \ torch_dtype = torch.float16, it gives error of different sizes tensor.\r\n\r\
    \nIs it even possible to use (any) Whisper pytorch model with different dtype\
    \ than for the entry in config.json or I'm making any basic mistake? If it is\
    \ possible to use with different dtype, what adaptation I need to make?"
  created_at: 2023-11-22 09:50:01+00:00
  edited: false
  hidden: false
  id: 655dcec959fa2e3099757557
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: openai/whisper-medium
repo_type: model
status: open
target_branch: null
title: Is to possible to use Whisper pytorch model with different dtype than the entry
  in config.json?
