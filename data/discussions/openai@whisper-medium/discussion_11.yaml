!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hannan
conflicting_files: null
created_at: 2023-02-28 08:47:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dbe3535f60382f71be9b7011de2dbd3c.svg
      fullname: Komari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hannan
      type: user
    createdAt: '2023-02-28T08:47:41.000Z'
    data:
      edited: false
      editors:
      - Hannan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dbe3535f60382f71be9b7011de2dbd3c.svg
          fullname: Komari
          isHf: false
          isPro: false
          name: Hannan
          type: user
        html: '<p>I have observed that Whisper model is deployed faster when its output
          is going to be in English language (even when the input audio is non-English).
          It''s inference time is about 30% lower when setting translation task or
          language is set to be English or un-force decoder-ids.<br>These are some
          tests on a 10s input audio in German language on my 1070 GPU:</p>

          <p>whisper-small:</p>

          <ul>

          <li>By setting <code>model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
          language="en", task="translate")</code>: Inference time = ~370ms</li>

          <li>By setting <code>model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
          language="en", task="transcribe")</code>: Inference time = ~370ms</li>

          <li>By setting <code>model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
          language="de", task="transcribe")</code>: Inference time = ~480ms</li>

          <li>By setting <code>model.config.forced_decoder_ids = None</code>: Inference
          time = ~480ms</li>

          </ul>

          <p>whisper-medium:</p>

          <ul>

          <li>By setting <code>model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
          language="en", task="translate")</code>: Inference time = ~850ms</li>

          <li>By setting <code>model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
          language="en", task="transcribe")</code>: Inference time = ~850ms</li>

          <li>By setting <code>model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
          language="de", task="transcribe")</code>: Inference time = ~1060ms</li>

          <li>By setting <code>model.config.forced_decoder_ids = None</code>: Inference
          time = ~1060ms</li>

          </ul>

          <p>This results shows that the decoder of Whisper model is faster when generating
          English text.<br>Do you have any intuition of the reason of such a difference
          in inference time between English and non-English output text?<br>And do
          you have any idea for making whisper as fast as English translation for
          non-English transcription?</p>

          '
        raw: "I have observed that Whisper model is deployed faster when its output\
          \ is going to be in English language (even when the input audio is non-English).\
          \ It's inference time is about 30% lower when setting translation task or\
          \ language is set to be English or un-force decoder-ids.\r\nThese are some\
          \ tests on a 10s input audio in German language on my 1070 GPU:\r\n\r\n\
          whisper-small:\r\n* By setting `model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\
          \ language=\"en\", task=\"translate\")`: Inference time = ~370ms\r\n* By\
          \ setting `model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\
          \ language=\"en\", task=\"transcribe\")`: Inference time = ~370ms\r\n* By\
          \ setting `model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\
          \ language=\"de\", task=\"transcribe\")`: Inference time = ~480ms\r\n* By\
          \ setting `model.config.forced_decoder_ids = None`: Inference time = ~480ms\r\
          \n\r\nwhisper-medium:\r\n* By setting `model.config.forced_decoder_ids =\
          \ processor.get_decoder_prompt_ids( language=\"en\", task=\"translate\"\
          )`: Inference time = ~850ms\r\n* By setting `model.config.forced_decoder_ids\
          \ = processor.get_decoder_prompt_ids( language=\"en\", task=\"transcribe\"\
          )`: Inference time = ~850ms\r\n* By setting `model.config.forced_decoder_ids\
          \ = processor.get_decoder_prompt_ids( language=\"de\", task=\"transcribe\"\
          )`: Inference time = ~1060ms\r\n* By setting `model.config.forced_decoder_ids\
          \ = None`: Inference time = ~1060ms\r\n\r\nThis results shows that the decoder\
          \ of Whisper model is faster when generating English text. \r\nDo you have\
          \ any intuition of the reason of such a difference in inference time between\
          \ English and non-English output text?\r\nAnd do you have any idea for making\
          \ whisper as fast as English translation for non-English transcription?"
        updatedAt: '2023-02-28T08:47:41.762Z'
      numEdits: 0
      reactions: []
    id: 63fdbfad6315a264aba4b724
    type: comment
  author: Hannan
  content: "I have observed that Whisper model is deployed faster when its output\
    \ is going to be in English language (even when the input audio is non-English).\
    \ It's inference time is about 30% lower when setting translation task or language\
    \ is set to be English or un-force decoder-ids.\r\nThese are some tests on a 10s\
    \ input audio in German language on my 1070 GPU:\r\n\r\nwhisper-small:\r\n* By\
    \ setting `model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\
    \ language=\"en\", task=\"translate\")`: Inference time = ~370ms\r\n* By setting\
    \ `model.config.forced_decoder_ids = processor.get_decoder_prompt_ids( language=\"\
    en\", task=\"transcribe\")`: Inference time = ~370ms\r\n* By setting `model.config.forced_decoder_ids\
    \ = processor.get_decoder_prompt_ids( language=\"de\", task=\"transcribe\")`:\
    \ Inference time = ~480ms\r\n* By setting `model.config.forced_decoder_ids = None`:\
    \ Inference time = ~480ms\r\n\r\nwhisper-medium:\r\n* By setting `model.config.forced_decoder_ids\
    \ = processor.get_decoder_prompt_ids( language=\"en\", task=\"translate\")`: Inference\
    \ time = ~850ms\r\n* By setting `model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\
    \ language=\"en\", task=\"transcribe\")`: Inference time = ~850ms\r\n* By setting\
    \ `model.config.forced_decoder_ids = processor.get_decoder_prompt_ids( language=\"\
    de\", task=\"transcribe\")`: Inference time = ~1060ms\r\n* By setting `model.config.forced_decoder_ids\
    \ = None`: Inference time = ~1060ms\r\n\r\nThis results shows that the decoder\
    \ of Whisper model is faster when generating English text. \r\nDo you have any\
    \ intuition of the reason of such a difference in inference time between English\
    \ and non-English output text?\r\nAnd do you have any idea for making whisper\
    \ as fast as English translation for non-English transcription?"
  created_at: 2023-02-28 08:47:41+00:00
  edited: false
  hidden: false
  id: 63fdbfad6315a264aba4b724
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-03-03T16:22:32.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Hannan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Hannan\">@<span class=\"\
          underline\">Hannan</span></a></span>\n\n\t</span></span>! Wow that's a super\
          \ interesting observation. The only thing that I can think of is that the\
          \ equivalent German text has more tokens that the English one, thus requiring\
          \ more calls of the decoder and so longer decoding time. </p>\n<p>Could\
          \ you generate the same number of tokens in both cases for a fair comparison?\
          \ You can set <code>max_new_tokens=128, min_new_tokens=128</code> to generate\
          \ 128 tokens in each case:</p>\n<pre><code class=\"language-python\">pred_ids\
          \ = model.generate(**inputs, max_new_tokens=<span class=\"hljs-number\"\
          >128</span>, min_new_tokens=<span class=\"hljs-number\">128</span>)\n</code></pre>\n"
        raw: "Hey @Hannan! Wow that's a super interesting observation. The only thing\
          \ that I can think of is that the equivalent German text has more tokens\
          \ that the English one, thus requiring more calls of the decoder and so\
          \ longer decoding time. \n\nCould you generate the same number of tokens\
          \ in both cases for a fair comparison? You can set `max_new_tokens=128,\
          \ min_new_tokens=128` to generate 128 tokens in each case:\n\n```python\n\
          pred_ids = model.generate(**inputs, max_new_tokens=128, min_new_tokens=128)\n\
          ```"
        updatedAt: '2023-03-03T16:22:32.630Z'
      numEdits: 0
      reactions: []
    id: 64021ec8fc948f5b169d9cd2
    type: comment
  author: sanchit-gandhi
  content: "Hey @Hannan! Wow that's a super interesting observation. The only thing\
    \ that I can think of is that the equivalent German text has more tokens that\
    \ the English one, thus requiring more calls of the decoder and so longer decoding\
    \ time. \n\nCould you generate the same number of tokens in both cases for a fair\
    \ comparison? You can set `max_new_tokens=128, min_new_tokens=128` to generate\
    \ 128 tokens in each case:\n\n```python\npred_ids = model.generate(**inputs, max_new_tokens=128,\
    \ min_new_tokens=128)\n```"
  created_at: 2023-03-03 16:22:32+00:00
  edited: false
  hidden: false
  id: 64021ec8fc948f5b169d9cd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dbe3535f60382f71be9b7011de2dbd3c.svg
      fullname: Komari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hannan
      type: user
    createdAt: '2023-03-04T07:00:58.000Z'
    data:
      edited: false
      editors:
      - Hannan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dbe3535f60382f71be9b7011de2dbd3c.svg
          fullname: Komari
          isHf: false
          isPro: false
          name: Hannan
          type: user
        html: "<p>Thank you a lot <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>!<br>It\
          \ was exactly because of longer transcriptions of German rather than English.<br>My\
          \ current using transformers (v 4.24.0) <code>min_new_tokens</code> was\
          \ not defined as a parameter for model.generate() but by setting <code>max_new_tokens\
          \ </code>to a fixed value of 64, the inference time gets exactly the same\
          \ in both tasks.<br>Your intuition was accurate!</p>\n"
        raw: 'Thank you a lot @sanchit-gandhi!

          It was exactly because of longer transcriptions of German rather than English.

          My current using transformers (v 4.24.0) `min_new_tokens` was not defined
          as a parameter for model.generate() but by setting `max_new_tokens `to a
          fixed value of 64, the inference time gets exactly the same in both tasks.

          Your intuition was accurate!'
        updatedAt: '2023-03-04T07:00:58.441Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sanchit-gandhi
      - count: 1
        reaction: "\U0001F917"
        users:
        - sanchit-gandhi
      relatedEventId: 6402ecaa4682c8c27709da67
    id: 6402ecaa4682c8c27709da66
    type: comment
  author: Hannan
  content: 'Thank you a lot @sanchit-gandhi!

    It was exactly because of longer transcriptions of German rather than English.

    My current using transformers (v 4.24.0) `min_new_tokens` was not defined as a
    parameter for model.generate() but by setting `max_new_tokens `to a fixed value
    of 64, the inference time gets exactly the same in both tasks.

    Your intuition was accurate!'
  created_at: 2023-03-04 07:00:58+00:00
  edited: false
  hidden: false
  id: 6402ecaa4682c8c27709da66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/dbe3535f60382f71be9b7011de2dbd3c.svg
      fullname: Komari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hannan
      type: user
    createdAt: '2023-03-04T07:00:58.000Z'
    data:
      status: closed
    id: 6402ecaa4682c8c27709da67
    type: status-change
  author: Hannan
  created_at: 2023-03-04 07:00:58+00:00
  id: 6402ecaa4682c8c27709da67
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: openai/whisper-medium
repo_type: model
status: closed
target_branch: null
title: Whisper model deployment is faster when generating English text at output
