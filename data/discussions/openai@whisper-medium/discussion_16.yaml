!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KKotaki
conflicting_files: null
created_at: 2023-04-10 10:47:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90a18e6ee270b793491daf7d2f81f936.svg
      fullname: Kentaro Kotaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KKotaki
      type: user
    createdAt: '2023-04-10T11:47:21.000Z'
    data:
      edited: false
      editors:
      - KKotaki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90a18e6ee270b793491daf7d2f81f936.svg
          fullname: Kentaro Kotaki
          isHf: false
          isPro: false
          name: KKotaki
          type: user
        html: "<p>An error occurs in the following chord in rare cases.<br>Please\
          \ let me know the solution.</p>\n<p>There was no difference in the shape\
          \ of the input from other voice data.</p>\n<p>code: </p>\n<pre><code>device\
          \ = 'cuda'\nmodel_path = 'openai/whisper-medium'\n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_path)\n\
          processor = WhisperProcessor.from_pretrained(model_path, language=\"Japanese\"\
          , task=\"transcribe\")\n\nmodel.config.forced_decoder_ids = self._processor.get_decoder_prompt_ids(\
          \ language = \"ja\", task = \"transcribe\")\nmodel.config.suppress_tokens\
          \ = []\nmodel.to(device)\n\ninputs = processor.feature_extractor(\n    \
          \        audio_data,\n            return_tensors=\"pt\",\n            sampling_rate=16_000\n\
          ).input_features.to(device)\nprint(inputs.shape)\n# torch.Size([1, 80, 3000])\n\
          \npredicted_ids = self._model.generate(\n            inputs,\n         \
          \   max_length=sample_rate * 30,\n           forced_decoder_ids=model.config.forced_decoder_ids\n\
          )\n# error occured!!\n# I don't think \"forced_decoder_ids=...\" needs to\
          \ be done, but for some reason the language is \"fi\", so I specify it explicitly.\n\
          </code></pre>\n<p>error:</p>\n<pre><code>Traceback (most recent call last):\n\
          \  File \"/xxx/infer.py\", line 68, in transcribe\n    predicted_ids = self._model.generate(\n\
          \  File \"/xxx/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/xxx/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 1391, in generate\n    return self.greedy_search(\n  File \"/xxx/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 2189, in greedy_search\n    next_token_logits = outputs.logits[:,\
          \ -1, :]\nIndexError: index -1 is out of bounds for dimension 1 with size\
          \ 0\n</code></pre>\n"
        raw: "An error occurs in the following chord in rare cases.\r\nPlease let\
          \ me know the solution.\r\n\r\nThere was no difference in the shape of the\
          \ input from other voice data.\r\n\r\ncode: \r\n```\r\ndevice = 'cuda'\r\
          \nmodel_path = 'openai/whisper-medium'\r\n\r\nmodel = WhisperForConditionalGeneration.from_pretrained(model_path)\r\
          \nprocessor = WhisperProcessor.from_pretrained(model_path, language=\"Japanese\"\
          , task=\"transcribe\")\r\n\r\nmodel.config.forced_decoder_ids = self._processor.get_decoder_prompt_ids(\
          \ language = \"ja\", task = \"transcribe\")\r\nmodel.config.suppress_tokens\
          \ = []\r\nmodel.to(device)\r\n\r\ninputs = processor.feature_extractor(\r\
          \n            audio_data,\r\n            return_tensors=\"pt\",\r\n    \
          \        sampling_rate=16_000\r\n).input_features.to(device)\r\nprint(inputs.shape)\r\
          \n# torch.Size([1, 80, 3000])\r\n\r\npredicted_ids = self._model.generate(\r\
          \n            inputs,\r\n            max_length=sample_rate * 30,\r\n  \
          \         forced_decoder_ids=model.config.forced_decoder_ids\r\n)\r\n# error\
          \ occured!!\r\n# I don't think \"forced_decoder_ids=...\" needs to be done,\
          \ but for some reason the language is \"fi\", so I specify it explicitly.\r\
          \n```\r\n\r\nerror:\r\n```\r\nTraceback (most recent call last):\r\n  File\
          \ \"/xxx/infer.py\", line 68, in transcribe\r\n    predicted_ids = self._model.generate(\r\
          \n  File \"/xxx/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n \
          \ File \"/xxx/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 1391, in generate\r\n    return self.greedy_search(\r\n  File \"\
          /xxx/lib/python3.9/site-packages/transformers/generation/utils.py\", line\
          \ 2189, in greedy_search\r\n    next_token_logits = outputs.logits[:, -1,\
          \ :]\r\nIndexError: index -1 is out of bounds for dimension 1 with size\
          \ 0\r\n```"
        updatedAt: '2023-04-10T11:47:21.037Z'
      numEdits: 0
      reactions: []
    id: 6433f749a5aed21dd112beff
    type: comment
  author: KKotaki
  content: "An error occurs in the following chord in rare cases.\r\nPlease let me\
    \ know the solution.\r\n\r\nThere was no difference in the shape of the input\
    \ from other voice data.\r\n\r\ncode: \r\n```\r\ndevice = 'cuda'\r\nmodel_path\
    \ = 'openai/whisper-medium'\r\n\r\nmodel = WhisperForConditionalGeneration.from_pretrained(model_path)\r\
    \nprocessor = WhisperProcessor.from_pretrained(model_path, language=\"Japanese\"\
    , task=\"transcribe\")\r\n\r\nmodel.config.forced_decoder_ids = self._processor.get_decoder_prompt_ids(\
    \ language = \"ja\", task = \"transcribe\")\r\nmodel.config.suppress_tokens =\
    \ []\r\nmodel.to(device)\r\n\r\ninputs = processor.feature_extractor(\r\n    \
    \        audio_data,\r\n            return_tensors=\"pt\",\r\n            sampling_rate=16_000\r\
    \n).input_features.to(device)\r\nprint(inputs.shape)\r\n# torch.Size([1, 80, 3000])\r\
    \n\r\npredicted_ids = self._model.generate(\r\n            inputs,\r\n       \
    \     max_length=sample_rate * 30,\r\n           forced_decoder_ids=model.config.forced_decoder_ids\r\
    \n)\r\n# error occured!!\r\n# I don't think \"forced_decoder_ids=...\" needs to\
    \ be done, but for some reason the language is \"fi\", so I specify it explicitly.\r\
    \n```\r\n\r\nerror:\r\n```\r\nTraceback (most recent call last):\r\n  File \"\
    /xxx/infer.py\", line 68, in transcribe\r\n    predicted_ids = self._model.generate(\r\
    \n  File \"/xxx/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line\
    \ 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/xxx/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 1391, in generate\r\n    return self.greedy_search(\r\n  File \"/xxx/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 2189, in greedy_search\r\n    next_token_logits = outputs.logits[:, -1,\
    \ :]\r\nIndexError: index -1 is out of bounds for dimension 1 with size 0\r\n\
    ```"
  created_at: 2023-04-10 10:47:21+00:00
  edited: false
  hidden: false
  id: 6433f749a5aed21dd112beff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-04-21T12:03:42.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;KKotaki&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KKotaki\"\
          >@<span class=\"underline\">KKotaki</span></a></span>\n\n\t</span></span>!\
          \ Thanks for reporting this! It looks like there's an issue with the generation\
          \ code. Could you open an issue in HF Transformers, sharing the code, audio\
          \ file and full error trace? We'll then be able to discuss the issue with\
          \ you and propose a fix \U0001F917 Thank you! Issue link here: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/issues/new?assignees=&amp;labels=&amp;template=bug-report.yml\"\
          >https://github.com/huggingface/transformers/issues/new?assignees=&amp;labels=&amp;template=bug-report.yml</a></p>\n"
        raw: "Hey @KKotaki! Thanks for reporting this! It looks like there's an issue\
          \ with the generation code. Could you open an issue in HF Transformers,\
          \ sharing the code, audio file and full error trace? We'll then be able\
          \ to discuss the issue with you and propose a fix \U0001F917 Thank you!\
          \ Issue link here: https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.yml"
        updatedAt: '2023-04-21T12:03:42.601Z'
      numEdits: 0
      reactions: []
    id: 64427b9e72b4ca7eeee9f00e
    type: comment
  author: sanchit-gandhi
  content: "Hey @KKotaki! Thanks for reporting this! It looks like there's an issue\
    \ with the generation code. Could you open an issue in HF Transformers, sharing\
    \ the code, audio file and full error trace? We'll then be able to discuss the\
    \ issue with you and propose a fix \U0001F917 Thank you! Issue link here: https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.yml"
  created_at: 2023-04-21 11:03:42+00:00
  edited: false
  hidden: false
  id: 64427b9e72b4ca7eeee9f00e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-04-21T12:05:10.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Taking a closer look at your code, I notice that you have <code>max_length</code>
          set to 30 * 16000 - it''s worth noting that max length corresponds to the
          maximum length of the output text tokens (rather than the audio inputs),
          so you can set this to ~256.</p>

          <p>See <a href="https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.max_length">https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.max_length</a></p>

          '
        raw: 'Taking a closer look at your code, I notice that you have `max_length`
          set to 30 * 16000 - it''s worth noting that max length corresponds to the
          maximum length of the output text tokens (rather than the audio inputs),
          so you can set this to ~256.


          See https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.max_length'
        updatedAt: '2023-04-21T12:06:48.131Z'
      numEdits: 2
      reactions: []
    id: 64427bf672b4ca7eeee9fa2c
    type: comment
  author: sanchit-gandhi
  content: 'Taking a closer look at your code, I notice that you have `max_length`
    set to 30 * 16000 - it''s worth noting that max length corresponds to the maximum
    length of the output text tokens (rather than the audio inputs), so you can set
    this to ~256.


    See https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.max_length'
  created_at: 2023-04-21 11:05:10+00:00
  edited: true
  hidden: false
  id: 64427bf672b4ca7eeee9fa2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-04-21T12:06:05.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>With the latest version of transformers (4.29), you can omit <code>forced_decoder_ids</code>
          and pass <code>language="japanese"</code> directly to <code>generate</code></p>

          '
        raw: With the latest version of transformers (4.29), you can omit `forced_decoder_ids`
          and pass `language="japanese"` directly to `generate`
        updatedAt: '2023-04-27T16:47:38.606Z'
      numEdits: 1
      reactions: []
    id: 64427c2d3e00705a4b8cfd9d
    type: comment
  author: sanchit-gandhi
  content: With the latest version of transformers (4.29), you can omit `forced_decoder_ids`
    and pass `language="japanese"` directly to `generate`
  created_at: 2023-04-21 11:06:05+00:00
  edited: true
  hidden: false
  id: 64427c2d3e00705a4b8cfd9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-04-21T12:07:14.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>If these two suggestions do not work, I''d recommend opening an
          issue as described above!</p>

          '
        raw: If these two suggestions do not work, I'd recommend opening an issue
          as described above!
        updatedAt: '2023-04-21T12:07:14.907Z'
      numEdits: 0
      reactions: []
    id: 64427c723e00705a4b8d067c
    type: comment
  author: sanchit-gandhi
  content: If these two suggestions do not work, I'd recommend opening an issue as
    described above!
  created_at: 2023-04-21 11:07:14+00:00
  edited: false
  hidden: false
  id: 64427c723e00705a4b8d067c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90a18e6ee270b793491daf7d2f81f936.svg
      fullname: Kentaro Kotaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KKotaki
      type: user
    createdAt: '2023-04-25T01:12:38.000Z'
    data:
      edited: false
      editors:
      - KKotaki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90a18e6ee270b793491daf7d2f81f936.svg
          fullname: Kentaro Kotaki
          isHf: false
          isPro: false
          name: KKotaki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span><br>Thank\
          \ you for your response!<br>I will try and reply to you regarding the points\
          \ you raised.</p>\n"
        raw: "@sanchit-gandhi \nThank you for your response!\nI will try and reply\
          \ to you regarding the points you raised."
        updatedAt: '2023-04-25T01:12:38.046Z'
      numEdits: 0
      reactions: []
    id: 64472906177a44e335e96b24
    type: comment
  author: KKotaki
  content: "@sanchit-gandhi \nThank you for your response!\nI will try and reply to\
    \ you regarding the points you raised."
  created_at: 2023-04-25 00:12:38+00:00
  edited: false
  hidden: false
  id: 64472906177a44e335e96b24
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: openai/whisper-medium
repo_type: model
status: open
target_branch: null
title: IndexError while using with language "japanese" (transcribe)
