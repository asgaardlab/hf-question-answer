!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philip30
conflicting_files: null
created_at: 2023-04-05 05:39:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e78d36f995800def9fcf5cb5bd3a6e39.svg
      fullname: Philip Arthur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philip30
      type: user
    createdAt: '2023-04-05T06:39:45.000Z'
    data:
      edited: false
      editors:
      - philip30
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e78d36f995800def9fcf5cb5bd3a6e39.svg
          fullname: Philip Arthur
          isHf: false
          isPro: false
          name: philip30
          type: user
        html: '<p>Hi there, </p>

          <p>Thank you for an amazing contribution for the whisper models. I observed
          some different (worse) results when using the model wrapped inside transformers
          package compared to the original openai whisper code. I am just wondering
          if:</p>

          <ol>

          <li>There''s a different decoding algorithm</li>

          <li>There''s some configuration I forgot to configure</li>

          </ol>

          <p>The apparent problems are the overlapping speech problems. While I realized
          there are some similar discussion in the openai github community here: <a
          rel="nofollow" href="https://github.com/openai/whisper/discussions/434">https://github.com/openai/whisper/discussions/434</a>,
          but during my internal testing I saw that the model from openai is far better
          at multiple speakers multi turns environment. </p>

          <p>Thank you.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6363096e2c0bb59ecc1cee56/rMZK4rz9qQ8sJu9Il8k6z.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6363096e2c0bb59ecc1cee56/rMZK4rz9qQ8sJu9Il8k6z.png"></a></p>

          '
        raw: "Hi there, \r\n\r\nThank you for an amazing contribution for the whisper\
          \ models. I observed some different (worse) results when using the model\
          \ wrapped inside transformers package compared to the original openai whisper\
          \ code. I am just wondering if:\r\n1. There's a different decoding algorithm\r\
          \n2. There's some configuration I forgot to configure\r\n\r\nThe apparent\
          \ problems are the overlapping speech problems. While I realized there are\
          \ some similar discussion in the openai github community here: https://github.com/openai/whisper/discussions/434,\
          \ but during my internal testing I saw that the model from openai is far\
          \ better at multiple speakers multi turns environment. \r\n\r\nThank you.\r\
          \n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6363096e2c0bb59ecc1cee56/rMZK4rz9qQ8sJu9Il8k6z.png)\r\
          \n\r\n"
        updatedAt: '2023-04-05T06:39:45.795Z'
      numEdits: 0
      reactions: []
    id: 642d17b11793288453175c60
    type: comment
  author: philip30
  content: "Hi there, \r\n\r\nThank you for an amazing contribution for the whisper\
    \ models. I observed some different (worse) results when using the model wrapped\
    \ inside transformers package compared to the original openai whisper code. I\
    \ am just wondering if:\r\n1. There's a different decoding algorithm\r\n2. There's\
    \ some configuration I forgot to configure\r\n\r\nThe apparent problems are the\
    \ overlapping speech problems. While I realized there are some similar discussion\
    \ in the openai github community here: https://github.com/openai/whisper/discussions/434,\
    \ but during my internal testing I saw that the model from openai is far better\
    \ at multiple speakers multi turns environment. \r\n\r\nThank you.\r\n\r\n\r\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6363096e2c0bb59ecc1cee56/rMZK4rz9qQ8sJu9Il8k6z.png)\r\
    \n\r\n"
  created_at: 2023-04-05 05:39:45+00:00
  edited: false
  hidden: false
  id: 642d17b11793288453175c60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/457bbbc3b95c16ac84580bdff49eff9b.svg
      fullname: ankit sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ankitsharma12345
      type: user
    createdAt: '2023-04-06T03:56:11.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      - ankitsharma12345
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>I think you can try using pipeline for audios larger than 30 secs\
          \ in length.</p>\n<p>Long-Form Transcription<br>The Whisper model is intrinsically\
          \ designed to work on audio samples of up to 30s in duration. However, by\
          \ using a chunking algorithm, it can be used to transcribe audio samples\
          \ of up to arbitrary length. This is possible through Transformers pipeline\
          \ method. Chunking is enabled by setting chunk_length_s=30 when instantiating\
          \ the pipeline. It can also be extended to predict utterance level timestamps\
          \ by passing return_timestamps=True:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-meta\">&gt;&gt;&gt; </span><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> pipeline\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span><span\
          \ class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\"\
          >import</span> load_dataset\n\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>device\
          \ = <span class=\"hljs-string\">\"cuda:0\"</span> <span class=\"hljs-keyword\"\
          >if</span> torch.cuda.is_available() <span class=\"hljs-keyword\">else</span>\
          \ <span class=\"hljs-string\">\"cpu\"</span>\n\n<span class=\"hljs-meta\"\
          >&gt;&gt;&gt; </span>pipe = pipeline(\n  <span class=\"hljs-string\">\"\
          automatic-speech-recognition\"</span>,\n  model=<span class=\"hljs-string\"\
          >\"openai/whisper-small.en\"</span>,\n  chunk_length_s=<span class=\"hljs-number\"\
          >30</span>,\n  device=device,\n)\n\n<span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span>ds = load_dataset(<span class=\"hljs-string\">\"hf-internal-testing/librispeech_asr_dummy\"\
          </span>, <span class=\"hljs-string\">\"clean\"</span>, split=<span class=\"\
          hljs-string\">\"validation\"</span>)\n<span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span>sample = ds[<span class=\"hljs-number\">0</span>][<span class=\"\
          hljs-string\">\"audio\"</span>]\n\n<span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span>prediction = pipe(sample.copy())[<span class=\"hljs-string\">\"\
          text\"</span>]\n<span class=\"hljs-string\">\" Mr. Quilter is the apostle\
          \ of the middle classes, and we are glad to welcome his gospel.\"</span>\n\
          \n<span class=\"hljs-meta\">&gt;&gt;&gt; </span><span class=\"hljs-comment\"\
          ># we can also return timestamps for the predictions</span>\n<span class=\"\
          hljs-meta\">&gt;&gt;&gt; </span>prediction = pipe(sample, return_timestamps=<span\
          \ class=\"hljs-literal\">True</span>)[<span class=\"hljs-string\">\"chunks\"\
          </span>]\n[{<span class=\"hljs-string\">'text'</span>: <span class=\"hljs-string\"\
          >' Mr. Quilter is the apostle of the middle classes and we are glad to welcome\
          \ his gospel.'</span>,\n  <span class=\"hljs-string\">'timestamp'</span>:\
          \ (<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">5.44</span>)}]\n\
          </code></pre>\n"
        raw: "I think you can try using pipeline for audios larger than 30 secs in\
          \ length.\n\nLong-Form Transcription\nThe Whisper model is intrinsically\
          \ designed to work on audio samples of up to 30s in duration. However, by\
          \ using a chunking algorithm, it can be used to transcribe audio samples\
          \ of up to arbitrary length. This is possible through Transformers pipeline\
          \ method. Chunking is enabled by setting chunk_length_s=30 when instantiating\
          \ the pipeline. It can also be extended to predict utterance level timestamps\
          \ by passing return_timestamps=True:\n\n```python\n>>> import torch\n>>>\
          \ from transformers import pipeline\n>>> from datasets import load_dataset\n\
          \n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n\
          >>> pipe = pipeline(\n  \"automatic-speech-recognition\",\n  model=\"openai/whisper-small.en\"\
          ,\n  chunk_length_s=30,\n  device=device,\n)\n\n>>> ds = load_dataset(\"\
          hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\
          )\n>>> sample = ds[0][\"audio\"]\n\n>>> prediction = pipe(sample.copy())[\"\
          text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are\
          \ glad to welcome his gospel.\"\n\n>>> # we can also return timestamps for\
          \ the predictions\n>>> prediction = pipe(sample, return_timestamps=True)[\"\
          chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes\
          \ and we are glad to welcome his gospel.',\n  'timestamp': (0.0, 5.44)}]\n\
          ```"
        updatedAt: '2023-04-18T16:38:17.967Z'
      numEdits: 1
      reactions: []
    id: 642e42db32bdf5af73f0d3b2
    type: comment
  author: ankitsharma12345
  content: "I think you can try using pipeline for audios larger than 30 secs in length.\n\
    \nLong-Form Transcription\nThe Whisper model is intrinsically designed to work\
    \ on audio samples of up to 30s in duration. However, by using a chunking algorithm,\
    \ it can be used to transcribe audio samples of up to arbitrary length. This is\
    \ possible through Transformers pipeline method. Chunking is enabled by setting\
    \ chunk_length_s=30 when instantiating the pipeline. It can also be extended to\
    \ predict utterance level timestamps by passing return_timestamps=True:\n\n```python\n\
    >>> import torch\n>>> from transformers import pipeline\n>>> from datasets import\
    \ load_dataset\n\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"\
    cpu\"\n\n>>> pipe = pipeline(\n  \"automatic-speech-recognition\",\n  model=\"\
    openai/whisper-small.en\",\n  chunk_length_s=30,\n  device=device,\n)\n\n>>> ds\
    \ = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"\
    validation\")\n>>> sample = ds[0][\"audio\"]\n\n>>> prediction = pipe(sample.copy())[\"\
    text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad\
    \ to welcome his gospel.\"\n\n>>> # we can also return timestamps for the predictions\n\
    >>> prediction = pipe(sample, return_timestamps=True)[\"chunks\"]\n[{'text': '\
    \ Mr. Quilter is the apostle of the middle classes and we are glad to welcome\
    \ his gospel.',\n  'timestamp': (0.0, 5.44)}]\n```"
  created_at: 2023-04-06 02:56:11+00:00
  edited: true
  hidden: false
  id: 642e42db32bdf5af73f0d3b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e78d36f995800def9fcf5cb5bd3a6e39.svg
      fullname: Philip Arthur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philip30
      type: user
    createdAt: '2023-04-15T00:14:45.000Z'
    data:
      edited: false
      editors:
      - philip30
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e78d36f995800def9fcf5cb5bd3a6e39.svg
          fullname: Philip Arthur
          isHf: false
          isPro: false
          name: philip30
          type: user
        html: '<p>I confirmed that the solution is working, thanks!</p>

          '
        raw: I confirmed that the solution is working, thanks!
        updatedAt: '2023-04-15T00:14:45.315Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6439ec75623c970188051b08
    id: 6439ec75623c970188051b07
    type: comment
  author: philip30
  content: I confirmed that the solution is working, thanks!
  created_at: 2023-04-14 23:14:45+00:00
  edited: false
  hidden: false
  id: 6439ec75623c970188051b07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e78d36f995800def9fcf5cb5bd3a6e39.svg
      fullname: Philip Arthur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philip30
      type: user
    createdAt: '2023-04-15T00:14:45.000Z'
    data:
      status: closed
    id: 6439ec75623c970188051b08
    type: status-change
  author: philip30
  created_at: 2023-04-14 23:14:45+00:00
  id: 6439ec75623c970188051b08
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: openai/whisper-medium
repo_type: model
status: closed
target_branch: null
title: Different outputs when using the original openai whisper model and the model
  wrapped by huggingface
