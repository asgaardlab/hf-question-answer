!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hiiamsid
conflicting_files: null
created_at: 2023-10-16 16:07:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632562296929-614c3bace0ee7bff0f3bdc57.jpeg?w=200&h=200&f=face
      fullname: Siddhartha Shrestha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiiamsid
      type: user
    createdAt: '2023-10-16T17:07:40.000Z'
    data:
      edited: false
      editors:
      - hiiamsid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9020292162895203
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632562296929-614c3bace0ee7bff0f3bdc57.jpeg?w=200&h=200&f=face
          fullname: Siddhartha Shrestha
          isHf: false
          isPro: false
          name: hiiamsid
          type: user
        html: '<p>I am trying to finetune longorca 13b-16K model using lora on multi-gpu
          i.e. 3 A100 gpu with 80GB memory each. But it is continuously throwing CudaOutOfMemory.
          So, can you say if this is normal and it requires more memory or maybe my
          code base has some memory leakage? If it requires more memory can you please
          give me estimated requirements for finetuning 13b-16k models.</p>

          '
        raw: I am trying to finetune longorca 13b-16K model using lora on multi-gpu
          i.e. 3 A100 gpu with 80GB memory each. But it is continuously throwing CudaOutOfMemory.
          So, can you say if this is normal and it requires more memory or maybe my
          code base has some memory leakage? If it requires more memory can you please
          give me estimated requirements for finetuning 13b-16k models.
        updatedAt: '2023-10-16T17:07:40.075Z'
      numEdits: 0
      reactions: []
    id: 652d6ddc84595aeba18d37a0
    type: comment
  author: hiiamsid
  content: I am trying to finetune longorca 13b-16K model using lora on multi-gpu
    i.e. 3 A100 gpu with 80GB memory each. But it is continuously throwing CudaOutOfMemory.
    So, can you say if this is normal and it requires more memory or maybe my code
    base has some memory leakage? If it requires more memory can you please give me
    estimated requirements for finetuning 13b-16k models.
  created_at: 2023-10-16 16:07:40+00:00
  edited: false
  hidden: false
  id: 652d6ddc84595aeba18d37a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-17T12:51:35.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9817371368408203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Hmm it not take more than 80gb of memory? I think first put gguf
          training is not ideal. It mainly uses cpu which will make it extremely slow.
          I think using something like peft is much more efficient.</p>

          '
        raw: Hmm it not take more than 80gb of memory? I think first put gguf training
          is not ideal. It mainly uses cpu which will make it extremely slow. I think
          using something like peft is much more efficient.
        updatedAt: '2023-10-17T12:51:35.148Z'
      numEdits: 0
      reactions: []
    id: 652e83579b4774d3a4c3ea62
    type: comment
  author: YaTharThShaRma999
  content: Hmm it not take more than 80gb of memory? I think first put gguf training
    is not ideal. It mainly uses cpu which will make it extremely slow. I think using
    something like peft is much more efficient.
  created_at: 2023-10-17 11:51:35+00:00
  edited: false
  hidden: false
  id: 652e83579b4774d3a4c3ea62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632562296929-614c3bace0ee7bff0f3bdc57.jpeg?w=200&h=200&f=face
      fullname: Siddhartha Shrestha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiiamsid
      type: user
    createdAt: '2023-10-17T13:00:22.000Z'
    data:
      edited: false
      editors:
      - hiiamsid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9495524168014526
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632562296929-614c3bace0ee7bff0f3bdc57.jpeg?w=200&h=200&f=face
          fullname: Siddhartha Shrestha
          isHf: false
          isPro: false
          name: hiiamsid
          type: user
        html: '<p>@johnwick123forevr  I am not doing any optimization like gguf, I
          am just taking fp32 and only adding the Lora layer.  Then I tried to finetune
          but even 3 GPU *A100 (80GB) is not being sufficient. My main problem is
          my max_length which should be around 8192, I think this is impacting gpu
          consumption. How did you tell if 80 GB is sufficient?</p>

          '
        raw: '@johnwick123forevr  I am not doing any optimization like gguf, I am
          just taking fp32 and only adding the Lora layer.  Then I tried to finetune
          but even 3 GPU *A100 (80GB) is not being sufficient. My main problem is
          my max_length which should be around 8192, I think this is impacting gpu
          consumption. How did you tell if 80 GB is sufficient?'
        updatedAt: '2023-10-17T13:00:22.632Z'
      numEdits: 0
      reactions: []
    id: 652e85668efb5d6242e668dc
    type: comment
  author: hiiamsid
  content: '@johnwick123forevr  I am not doing any optimization like gguf, I am just
    taking fp32 and only adding the Lora layer.  Then I tried to finetune but even
    3 GPU *A100 (80GB) is not being sufficient. My main problem is my max_length which
    should be around 8192, I think this is impacting gpu consumption. How did you
    tell if 80 GB is sufficient?'
  created_at: 2023-10-17 12:00:22+00:00
  edited: false
  hidden: false
  id: 652e85668efb5d6242e668dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-17T14:22:21.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9401367902755737
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I don''t know how much extra VRAM is required for training at 8192
          as I''ve not done it personally.  But if I was going to try, I would definitely
          want Flash Attention 2 included, as this reduces VRAM consumption</p>

          <p>I would strongly recommend to try the Axolotl training framework: <a
          rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl">https://github.com/OpenAccess-AI-Collective/axolotl</a></p>

          <p>It supports:</p>

          <ul>

          <li>Full fine tuning (no LoRA)</li>

          <li>LoRA</li>

          <li>qLoRA - quantised, so even less VRAM is needed. Slightly lower quality</li>

          <li>Flash Attention 2, to reduce VRAM usage from extended context</li>

          <li>Deepspeed or FSDP offload, which uses RAM instead of VRAM; this can
          be another way to use less VRAM.  Eg use DeepSpeed Zero2 or Zero3</li>

          </ul>

          <p>Try a LoRA + Flash Attention 2, or LoRA + Flash Attention 2 + DeepSpeed,
          and I am sure you will do better.</p>

          <p>And if that still fails, try qLoRA instead, and then 80GB will be more
          than enough VRAM.  Even 48GB is enough for 70B qLoRA (at least at 4096 context
          - maybe you''d need the full 80 for 8192, not sure.)</p>

          '
        raw: 'I don''t know how much extra VRAM is required for training at 8192 as
          I''ve not done it personally.  But if I was going to try, I would definitely
          want Flash Attention 2 included, as this reduces VRAM consumption


          I would strongly recommend to try the Axolotl training framework: https://github.com/OpenAccess-AI-Collective/axolotl


          It supports:

          - Full fine tuning (no LoRA)

          - LoRA

          - qLoRA - quantised, so even less VRAM is needed. Slightly lower quality

          - Flash Attention 2, to reduce VRAM usage from extended context

          - Deepspeed or FSDP offload, which uses RAM instead of VRAM; this can be
          another way to use less VRAM.  Eg use DeepSpeed Zero2 or Zero3


          Try a LoRA + Flash Attention 2, or LoRA + Flash Attention 2 + DeepSpeed,
          and I am sure you will do better.


          And if that still fails, try qLoRA instead, and then 80GB will be more than
          enough VRAM.  Even 48GB is enough for 70B qLoRA (at least at 4096 context
          - maybe you''d need the full 80 for 8192, not sure.)'
        updatedAt: '2023-10-17T14:22:32.756Z'
      numEdits: 1
      reactions: []
    id: 652e989d902fe76a6d06cc22
    type: comment
  author: TheBloke
  content: 'I don''t know how much extra VRAM is required for training at 8192 as
    I''ve not done it personally.  But if I was going to try, I would definitely want
    Flash Attention 2 included, as this reduces VRAM consumption


    I would strongly recommend to try the Axolotl training framework: https://github.com/OpenAccess-AI-Collective/axolotl


    It supports:

    - Full fine tuning (no LoRA)

    - LoRA

    - qLoRA - quantised, so even less VRAM is needed. Slightly lower quality

    - Flash Attention 2, to reduce VRAM usage from extended context

    - Deepspeed or FSDP offload, which uses RAM instead of VRAM; this can be another
    way to use less VRAM.  Eg use DeepSpeed Zero2 or Zero3


    Try a LoRA + Flash Attention 2, or LoRA + Flash Attention 2 + DeepSpeed, and I
    am sure you will do better.


    And if that still fails, try qLoRA instead, and then 80GB will be more than enough
    VRAM.  Even 48GB is enough for 70B qLoRA (at least at 4096 context - maybe you''d
    need the full 80 for 8192, not sure.)'
  created_at: 2023-10-17 13:22:21+00:00
  edited: true
  hidden: false
  id: 652e989d902fe76a6d06cc22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-17T23:40:37.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9625886678695679
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Yep as Thebloke said, axolotl is great. Also, with Lora training
          it should easily fit in 80gb vram. </p>

          <p>Axolotl should do the same exact same thing and you can train with any
          dataset as well. You just have to edit a yaml file.</p>

          '
        raw: "Yep as Thebloke said, axolotl is great. Also, with Lora training it\
          \ should easily fit in 80gb vram. \n\nAxolotl should do the same exact same\
          \ thing and you can train with any dataset as well. You just have to edit\
          \ a yaml file."
        updatedAt: '2023-10-17T23:40:37.305Z'
      numEdits: 0
      reactions: []
    id: 652f1b75b355cf468cdce3ba
    type: comment
  author: YaTharThShaRma999
  content: "Yep as Thebloke said, axolotl is great. Also, with Lora training it should\
    \ easily fit in 80gb vram. \n\nAxolotl should do the same exact same thing and\
    \ you can train with any dataset as well. You just have to edit a yaml file."
  created_at: 2023-10-17 22:40:37+00:00
  edited: false
  hidden: false
  id: 652f1b75b355cf468cdce3ba
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/LlongOrca-13B-16K-GGUF
repo_type: model
status: open
target_branch: null
title: Resource Required to train
