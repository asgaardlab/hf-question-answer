!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArthurCamara
conflicting_files: null
created_at: 2022-10-24 10:58:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63fe371369ae3c5911b9d4b31fdb3a5f.svg
      fullname: "Arthur Barbosa C\xE2mara"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurCamara
      type: user
    createdAt: '2022-10-24T11:58:35.000Z'
    data:
      edited: true
      editors:
      - ArthurCamara
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63fe371369ae3c5911b9d4b31fdb3a5f.svg
          fullname: "Arthur Barbosa C\xE2mara"
          isHf: false
          isPro: false
          name: ArthurCamara
          type: user
        html: "<p>Basically what the title says.<br>If I initiate a T5 model and place\
          \ it on CPU or CUDA, it works as intended:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, LongT5EncoderModel\nmodel = LongT5EncoderModel.from_pretrained(<span\
          \ class=\"hljs-string\">\"google/long-t5-tglobal-base\"</span>)\ntokenizer\
          \ = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"google/long-t5-tglobal-base\"\
          </span>)\ninputs = tokenizer(<span class=\"hljs-string\">\"&lt;extra_id_0&gt;\
          \  Hello, my dog is cute\"</span>, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>)\nmodel(**inputs)\nOut[<span class=\"hljs-number\">19</span>]:\n\
          BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-<span\
          \ class=\"hljs-number\">0.1624</span>, -<span class=\"hljs-number\">0.1439</span>,\
          \  <span class=\"hljs-number\">0.0011</span>,  ..., -<span class=\"hljs-number\"\
          >0.2340</span>,  <span class=\"hljs-number\">0.2113</span>,  <span class=\"\
          hljs-number\">0.1893</span>],\n         [ <span class=\"hljs-number\">0.0917</span>,\
          \  <span class=\"hljs-number\">0.0566</span>,  <span class=\"hljs-number\"\
          >0.0013</span>,  ...,  <span class=\"hljs-number\">0.0890</span>,  <span\
          \ class=\"hljs-number\">0.2563</span>, -<span class=\"hljs-number\">0.1880</span>],\n\
          \         [ <span class=\"hljs-number\">0.0172</span>, -<span class=\"hljs-number\"\
          >0.0204</span>,  <span class=\"hljs-number\">0.0013</span>,  ...,  <span\
          \ class=\"hljs-number\">0.0048</span>, -<span class=\"hljs-number\">0.0575</span>,\
          \ -<span class=\"hljs-number\">0.0638</span>],\n         ...,\n        \
          \ [-<span class=\"hljs-number\">0.0219</span>, -<span class=\"hljs-number\"\
          >0.0702</span>,  <span class=\"hljs-number\">0.0009</span>,  ..., -<span\
          \ class=\"hljs-number\">0.0568</span>, -<span class=\"hljs-number\">0.0474</span>,\
          \  <span class=\"hljs-number\">0.0188</span>],\n         [-<span class=\"\
          hljs-number\">0.0876</span>,  <span class=\"hljs-number\">0.0266</span>,\
          \  <span class=\"hljs-number\">0.0008</span>,  ...,  <span class=\"hljs-number\"\
          >0.0385</span>,  <span class=\"hljs-number\">0.0675</span>,  <span class=\"\
          hljs-number\">0.2390</span>],\n         [-<span class=\"hljs-number\">0.0128</span>,\
          \ -<span class=\"hljs-number\">0.0052</span>, -<span class=\"hljs-number\"\
          >0.0009</span>,  ..., -<span class=\"hljs-number\">0.0212</span>,  <span\
          \ class=\"hljs-number\">0.0151</span>, -<span class=\"hljs-number\">0.0093</span>]]],\n\
          \       grad_fn=&lt;MulBackward0&gt;), past_key_values=<span class=\"hljs-literal\"\
          >None</span>, hidden_states=<span class=\"hljs-literal\">None</span>, attentions=<span\
          \ class=\"hljs-literal\">None</span>, cross_attentions=<span class=\"hljs-literal\"\
          >None</span>)\n</code></pre>\n<p>That's as expected. The same behaviour\
          \ happens on a GPU:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-comment\">## From GPU</span>\nmodel = model.to(<span class=\"hljs-string\"\
          >\"cuda\"</span>)\n<span class=\"hljs-keyword\">for</span> k, v <span class=\"\
          hljs-keyword\">in</span> inputs.items():\n        inputs[k] = v.to(<span\
          \ class=\"hljs-string\">\"cuda\"</span>)\nmodel(**inputs)\nOut[<span class=\"\
          hljs-number\">22</span>]:\nBaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-<span\
          \ class=\"hljs-number\">0.1624</span>, -<span class=\"hljs-number\">0.1439</span>,\
          \  <span class=\"hljs-number\">0.0011</span>,  ..., -<span class=\"hljs-number\"\
          >0.2340</span>,  <span class=\"hljs-number\">0.2113</span>,  <span class=\"\
          hljs-number\">0.1893</span>],\n         [ <span class=\"hljs-number\">0.0917</span>,\
          \  <span class=\"hljs-number\">0.0566</span>,  <span class=\"hljs-number\"\
          >0.0013</span>,  ...,  <span class=\"hljs-number\">0.0890</span>,  <span\
          \ class=\"hljs-number\">0.2563</span>, -<span class=\"hljs-number\">0.1880</span>],\n\
          \         [ <span class=\"hljs-number\">0.0172</span>, -<span class=\"hljs-number\"\
          >0.0204</span>,  <span class=\"hljs-number\">0.0013</span>,  ...,  <span\
          \ class=\"hljs-number\">0.0048</span>, -<span class=\"hljs-number\">0.0575</span>,\
          \ -<span class=\"hljs-number\">0.0638</span>],\n         ...,\n        \
          \ [-<span class=\"hljs-number\">0.0219</span>, -<span class=\"hljs-number\"\
          >0.0702</span>,  <span class=\"hljs-number\">0.0009</span>,  ..., -<span\
          \ class=\"hljs-number\">0.0568</span>, -<span class=\"hljs-number\">0.0474</span>,\
          \  <span class=\"hljs-number\">0.0188</span>],\n         [-<span class=\"\
          hljs-number\">0.0876</span>,  <span class=\"hljs-number\">0.0266</span>,\
          \  <span class=\"hljs-number\">0.0008</span>,  ...,  <span class=\"hljs-number\"\
          >0.0385</span>,  <span class=\"hljs-number\">0.0675</span>,  <span class=\"\
          hljs-number\">0.2390</span>],\n         [-<span class=\"hljs-number\">0.0128</span>,\
          \ -<span class=\"hljs-number\">0.0052</span>, -<span class=\"hljs-number\"\
          >0.0009</span>,  ..., -<span class=\"hljs-number\">0.0212</span>,  <span\
          \ class=\"hljs-number\">0.0151</span>, -<span class=\"hljs-number\">0.0093</span>]]],\n\
          \       device=<span class=\"hljs-string\">'cuda:0'</span>, grad_fn=&lt;MulBackward0&gt;),\
          \ past_key_values=<span class=\"hljs-literal\">None</span>, hidden_states=<span\
          \ class=\"hljs-literal\">None</span>, attentions=<span class=\"hljs-literal\"\
          >None</span>, cross_attentions=<span class=\"hljs-literal\">None</span>)\n\
          </code></pre>\n<p>But if call <code>half()</code> on the model, it only\
          \ returns <code>nan</code>s:</p>\n<pre><code class=\"language-python\">model\
          \ = model.half()\nmodel(**inputs)\nOut[<span class=\"hljs-number\">24</span>]:\n\
          BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[nan,\
          \ nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan,\
          \ nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n\
          \         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,\
          \  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]]],\
          \ device=<span class=\"hljs-string\">'cuda:0'</span>,\n       dtype=torch.float16,\
          \ grad_fn=&lt;MulBackward0&gt;), past_key_values=<span class=\"hljs-literal\"\
          >None</span>, hidden_states=<span class=\"hljs-literal\">None</span>, attentions=<span\
          \ class=\"hljs-literal\">None</span>, cross_attentions=<span class=\"hljs-literal\"\
          >None</span>)\n</code></pre>\n<p>Removing the <code>&lt;extra_id_0&gt;</code>\
          \ doesn't really help either.<br>Any ideas on what is causing this?</p>\n"
        raw: "Basically what the title says. \nIf I initiate a T5 model and place\
          \ it on CPU or CUDA, it works as intended:\n\n```python\nfrom transformers\
          \ import AutoTokenizer, LongT5EncoderModel\nmodel = LongT5EncoderModel.from_pretrained(\"\
          google/long-t5-tglobal-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
          google/long-t5-tglobal-base\")\ninputs = tokenizer(\"<extra_id_0>  Hello,\
          \ my dog is cute\", return_tensors=\"pt\")\nmodel(**inputs)\nOut[19]:\n\
          BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.1624,\
          \ -0.1439,  0.0011,  ..., -0.2340,  0.2113,  0.1893],\n         [ 0.0917,\
          \  0.0566,  0.0013,  ...,  0.0890,  0.2563, -0.1880],\n         [ 0.0172,\
          \ -0.0204,  0.0013,  ...,  0.0048, -0.0575, -0.0638],\n         ...,\n \
          \        [-0.0219, -0.0702,  0.0009,  ..., -0.0568, -0.0474,  0.0188],\n\
          \         [-0.0876,  0.0266,  0.0008,  ...,  0.0385,  0.0675,  0.2390],\n\
          \         [-0.0128, -0.0052, -0.0009,  ..., -0.0212,  0.0151, -0.0093]]],\n\
          \       grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None,\
          \ attentions=None, cross_attentions=None)\n```\nThat's as expected. The\
          \ same behaviour happens on a GPU:\n```python\n## From GPU\nmodel = model.to(\"\
          cuda\")\nfor k, v in inputs.items():\n        inputs[k] = v.to(\"cuda\"\
          )\nmodel(**inputs)\nOut[22]:\nBaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.1624,\
          \ -0.1439,  0.0011,  ..., -0.2340,  0.2113,  0.1893],\n         [ 0.0917,\
          \  0.0566,  0.0013,  ...,  0.0890,  0.2563, -0.1880],\n         [ 0.0172,\
          \ -0.0204,  0.0013,  ...,  0.0048, -0.0575, -0.0638],\n         ...,\n \
          \        [-0.0219, -0.0702,  0.0009,  ..., -0.0568, -0.0474,  0.0188],\n\
          \         [-0.0876,  0.0266,  0.0008,  ...,  0.0385,  0.0675,  0.2390],\n\
          \         [-0.0128, -0.0052, -0.0009,  ..., -0.0212,  0.0151, -0.0093]]],\n\
          \       device='cuda:0', grad_fn=<MulBackward0>), past_key_values=None,\
          \ hidden_states=None, attentions=None, cross_attentions=None)\n```\n\nBut\
          \ if call `half()` on the model, it only returns `nan`s:\n\n```python\n\
          model = model.half()\nmodel(**inputs)\nOut[24]:\nBaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[nan,\
          \ nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan,\
          \ nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n\
          \         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,\
          \  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]]],\
          \ device='cuda:0',\n       dtype=torch.float16, grad_fn=<MulBackward0>),\
          \ past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n\
          \n```\n\nRemoving the `<extra_id_0>` doesn't really help either.\nAny ideas\
          \ on what is causing this?"
        updatedAt: '2022-10-24T12:20:09.231Z'
      numEdits: 1
      reactions: []
    id: 63567debc469f5c6f345ce1a
    type: comment
  author: ArthurCamara
  content: "Basically what the title says. \nIf I initiate a T5 model and place it\
    \ on CPU or CUDA, it works as intended:\n\n```python\nfrom transformers import\
    \ AutoTokenizer, LongT5EncoderModel\nmodel = LongT5EncoderModel.from_pretrained(\"\
    google/long-t5-tglobal-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-tglobal-base\"\
    )\ninputs = tokenizer(\"<extra_id_0>  Hello, my dog is cute\", return_tensors=\"\
    pt\")\nmodel(**inputs)\nOut[19]:\nBaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.1624,\
    \ -0.1439,  0.0011,  ..., -0.2340,  0.2113,  0.1893],\n         [ 0.0917,  0.0566,\
    \  0.0013,  ...,  0.0890,  0.2563, -0.1880],\n         [ 0.0172, -0.0204,  0.0013,\
    \  ...,  0.0048, -0.0575, -0.0638],\n         ...,\n         [-0.0219, -0.0702,\
    \  0.0009,  ..., -0.0568, -0.0474,  0.0188],\n         [-0.0876,  0.0266,  0.0008,\
    \  ...,  0.0385,  0.0675,  0.2390],\n         [-0.0128, -0.0052, -0.0009,  ...,\
    \ -0.0212,  0.0151, -0.0093]]],\n       grad_fn=<MulBackward0>), past_key_values=None,\
    \ hidden_states=None, attentions=None, cross_attentions=None)\n```\nThat's as\
    \ expected. The same behaviour happens on a GPU:\n```python\n## From GPU\nmodel\
    \ = model.to(\"cuda\")\nfor k, v in inputs.items():\n        inputs[k] = v.to(\"\
    cuda\")\nmodel(**inputs)\nOut[22]:\nBaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.1624,\
    \ -0.1439,  0.0011,  ..., -0.2340,  0.2113,  0.1893],\n         [ 0.0917,  0.0566,\
    \  0.0013,  ...,  0.0890,  0.2563, -0.1880],\n         [ 0.0172, -0.0204,  0.0013,\
    \  ...,  0.0048, -0.0575, -0.0638],\n         ...,\n         [-0.0219, -0.0702,\
    \  0.0009,  ..., -0.0568, -0.0474,  0.0188],\n         [-0.0876,  0.0266,  0.0008,\
    \  ...,  0.0385,  0.0675,  0.2390],\n         [-0.0128, -0.0052, -0.0009,  ...,\
    \ -0.0212,  0.0151, -0.0093]]],\n       device='cuda:0', grad_fn=<MulBackward0>),\
    \ past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n\
    ```\n\nBut if call `half()` on the model, it only returns `nan`s:\n\n```python\n\
    model = model.half()\nmodel(**inputs)\nOut[24]:\nBaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[nan,\
    \ nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n\
    \         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan,\
    \ nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n\
    \         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n       dtype=torch.float16,\
    \ grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=None,\
    \ cross_attentions=None)\n\n```\n\nRemoving the `<extra_id_0>` doesn't really\
    \ help either.\nAny ideas on what is causing this?"
  created_at: 2022-10-24 10:58:35+00:00
  edited: true
  hidden: false
  id: 63567debc469f5c6f345ce1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63fe371369ae3c5911b9d4b31fdb3a5f.svg
      fullname: "Arthur Barbosa C\xE2mara"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurCamara
      type: user
    createdAt: '2022-10-24T12:26:48.000Z'
    data:
      edited: true
      editors:
      - ArthurCamara
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63fe371369ae3c5911b9d4b31fdb3a5f.svg
          fullname: "Arthur Barbosa C\xE2mara"
          isHf: false
          isPro: false
          name: ArthurCamara
          type: user
        html: '<p>Nevermind. Switching to main instead of release solves this.<br>EDIT:
          No it didn''t =(<br>Using BF16 solved</p>

          '
        raw: "Nevermind. Switching to main instead of release solves this.\nEDIT:\
          \ No it didn't =( \nUsing BF16 solved"
        updatedAt: '2022-10-24T14:41:38.917Z'
      numEdits: 1
      reactions: []
      relatedEventId: 63568488c469f5c6f3460b63
    id: 63568488c469f5c6f3460b62
    type: comment
  author: ArthurCamara
  content: "Nevermind. Switching to main instead of release solves this.\nEDIT: No\
    \ it didn't =( \nUsing BF16 solved"
  created_at: 2022-10-24 11:26:48+00:00
  edited: true
  hidden: false
  id: 63568488c469f5c6f3460b62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/63fe371369ae3c5911b9d4b31fdb3a5f.svg
      fullname: "Arthur Barbosa C\xE2mara"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurCamara
      type: user
    createdAt: '2022-10-24T12:26:48.000Z'
    data:
      status: closed
    id: 63568488c469f5c6f3460b63
    type: status-change
  author: ArthurCamara
  created_at: 2022-10-24 11:26:48+00:00
  id: 63568488c469f5c6f3460b63
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg?w=200&h=200&f=face
      fullname: Nicholas Broad
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: true
      name: nbroad
      type: user
    createdAt: '2022-10-24T13:08:39.000Z'
    data:
      edited: false
      editors:
      - nbroad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg?w=200&h=200&f=face
          fullname: Nicholas Broad
          isHf: true
          isPro: true
          name: nbroad
          type: user
        html: '<p>Maybe this<br><a rel="nofollow" href="https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315">https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315</a></p>

          '
        raw: 'Maybe this

          https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315'
        updatedAt: '2022-10-24T13:08:39.643Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - ybelkada
        - ArthurCamara
    id: 63568e57ad327bec58a09526
    type: comment
  author: nbroad
  content: 'Maybe this

    https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315'
  created_at: 2022-10-24 12:08:39+00:00
  edited: false
  hidden: false
  id: 63568e57ad327bec58a09526
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63fe371369ae3c5911b9d4b31fdb3a5f.svg
      fullname: "Arthur Barbosa C\xE2mara"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurCamara
      type: user
    createdAt: '2022-10-24T14:41:10.000Z'
    data:
      edited: false
      editors:
      - ArthurCamara
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63fe371369ae3c5911b9d4b31fdb3a5f.svg
          fullname: "Arthur Barbosa C\xE2mara"
          isHf: false
          isPro: false
          name: ArthurCamara
          type: user
        html: '<p>Yup. BF16 solved.</p>

          '
        raw: Yup. BF16 solved.
        updatedAt: '2022-10-24T14:41:10.826Z'
      numEdits: 0
      reactions: []
    id: 6356a406a027ec5e56a5b672
    type: comment
  author: ArthurCamara
  content: Yup. BF16 solved.
  created_at: 2022-10-24 13:41:10+00:00
  edited: false
  hidden: false
  id: 6356a406a027ec5e56a5b672
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: google/long-t5-tglobal-base
repo_type: model
status: closed
target_branch: null
title: LongT5 fails in FP16 mode
