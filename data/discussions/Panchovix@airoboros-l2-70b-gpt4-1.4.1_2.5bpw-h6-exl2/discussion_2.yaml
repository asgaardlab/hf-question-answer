!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MasonJ
conflicting_files: null
created_at: 2023-09-17 02:04:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
      fullname: Mason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MasonJ
      type: user
    createdAt: '2023-09-17T03:04:18.000Z'
    data:
      edited: true
      editors:
      - MasonJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43401533365249634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
          fullname: Mason
          isHf: false
          isPro: false
          name: MasonJ
          type: user
        html: '<p>I''d love to run this model but I get the below error in Ooba on
          an RTX 3090ti. I get the error with both the ExLlamav2 and ExLlamav2_HF
          model loaders. Task manger shows 22.7/24.0 GB on the GPU when the loading
          fails so I don''t believe I ran out of VRAM. I can run turboderp_LLama2-70B-2.5bpw-h6-exl2
          and turboderp_LLama2-70B-chat-2.55bpw-h6-exl2 just fine. Any insight would
          be appreciated.</p>

          <p>2023-09-16 22:53:25 INFO:Loading airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2...<br>2023-09-16
          22:53:38 ERROR:Failed to load the model.<br>Traceback (most recent call
          last):<br>  File "S:\LLMs\Interfaces\oobabooga_windows\text-generation-webui\modules\ui_model_menu.py",
          line 194, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(shared.model_name, loader)<br>  File "S:\LLMs\Interfaces\oobabooga_windows\text-generation-webui\modules\models.py",
          line 85, in load_model<br>    tokenizer = load_tokenizer(model_name, model)<br>  File
          "S:\LLMs\Interfaces\oobabooga_windows\text-generation-webui\modules\models.py",
          line 102, in load_tokenizer<br>    tokenizer = AutoTokenizer.from_pretrained(<br>  File
          "S:\LLMs\Interfaces\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\auto\tokenization_auto.py",
          line 736, in from_pretrained<br>    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>  File "S:\LLMs\Interfaces\oobabooga_windows\installer_files\env\lib\site-packages\transformers\tokenization_utils_base.py",
          line 1854, in from_pretrained<br>    return cls._from_pretrained(<br>  File
          "S:\LLMs\Interfaces\oobabooga_windows\installer_files\env\lib\site-packages\transformers\tokenization_utils_base.py",
          line 2017, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>  File
          "S:\LLMs\Interfaces\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\tokenization_llama.py",
          line 156, in __init__<br>    self.sp_model = self.get_spm_processor()<br>  File
          "S:\LLMs\Interfaces\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\tokenization_llama.py",
          line 172, in get_spm_processor<br>    model = model_pb2.ModelProto.FromString(sp_model)<br>google.protobuf.message.DecodeError:
          Error parsing message</p>

          '
        raw: "I'd love to run this model but I get the below error in Ooba on an RTX\
          \ 3090ti. I get the error with both the ExLlamav2 and ExLlamav2_HF model\
          \ loaders. Task manger shows 22.7/24.0 GB on the GPU when the loading fails\
          \ so I don't believe I ran out of VRAM. I can run turboderp_LLama2-70B-2.5bpw-h6-exl2\
          \ and turboderp_LLama2-70B-chat-2.55bpw-h6-exl2 just fine. Any insight would\
          \ be appreciated.\n\n2023-09-16 22:53:25 INFO:Loading airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2...\n\
          2023-09-16 22:53:38 ERROR:Failed to load the model.\nTraceback (most recent\
          \ call last):\n  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\text-generation-webui\\\
          modules\\ui_model_menu.py\", line 194, in load_model_wrapper\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)\n  File \"S:\\\
          LLMs\\Interfaces\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
          , line 85, in load_model\n    tokenizer = load_tokenizer(model_name, model)\n\
          \  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\", line 102, in load_tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n\
          \  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\",\
          \ line 736, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
          , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\tokenization_utils_base.py\", line 2017, in\
          \ _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n \
          \ File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py\"\
          , line 156, in \\_\\_init\\_\\_\n    self.sp_model = self.get_spm_processor()\n\
          \  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py\"\
          , line 172, in get_spm_processor\n    model = model_pb2.ModelProto.FromString(sp_model)\n\
          google.protobuf.message.DecodeError: Error parsing message"
        updatedAt: '2023-09-17T03:10:13.170Z'
      numEdits: 4
      reactions: []
    id: 65066cb2dacc94cd6cff5520
    type: comment
  author: MasonJ
  content: "I'd love to run this model but I get the below error in Ooba on an RTX\
    \ 3090ti. I get the error with both the ExLlamav2 and ExLlamav2_HF model loaders.\
    \ Task manger shows 22.7/24.0 GB on the GPU when the loading fails so I don't\
    \ believe I ran out of VRAM. I can run turboderp_LLama2-70B-2.5bpw-h6-exl2 and\
    \ turboderp_LLama2-70B-chat-2.55bpw-h6-exl2 just fine. Any insight would be appreciated.\n\
    \n2023-09-16 22:53:25 INFO:Loading airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2...\n\
    2023-09-16 22:53:38 ERROR:Failed to load the model.\nTraceback (most recent call\
    \ last):\n  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\text-generation-webui\\\
    modules\\ui_model_menu.py\", line 194, in load_model_wrapper\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\n  File \"S:\\LLMs\\\
    Interfaces\\oobabooga_windows\\text-generation-webui\\modules\\models.py\", line\
    \ 85, in load_model\n    tokenizer = load_tokenizer(model_name, model)\n  File\
    \ \"S:\\LLMs\\Interfaces\\oobabooga_windows\\text-generation-webui\\modules\\\
    models.py\", line 102, in load_tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n\
    \  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 736, in\
    \ from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1854,\
    \ in from_pretrained\n    return cls._from_pretrained(\n  File \"S:\\LLMs\\Interfaces\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
    , line 2017, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
    \  File \"S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\transformers\\models\\llama\\tokenization_llama.py\", line 156,\
    \ in \\_\\_init\\_\\_\n    self.sp_model = self.get_spm_processor()\n  File \"\
    S:\\LLMs\\Interfaces\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\tokenization_llama.py\", line 172, in get_spm_processor\n\
    \    model = model_pb2.ModelProto.FromString(sp_model)\ngoogle.protobuf.message.DecodeError:\
    \ Error parsing message"
  created_at: 2023-09-17 02:04:18+00:00
  edited: true
  hidden: false
  id: 65066cb2dacc94cd6cff5520
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-09-17T04:47:39.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9820078611373901
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>Honestly I have never seen that issue. Which version of transformers/exllamav2
          are you using? Even searching for that error I can''t find it something
          clear about it.</p>

          '
        raw: Honestly I have never seen that issue. Which version of transformers/exllamav2
          are you using? Even searching for that error I can't find it something clear
          about it.
        updatedAt: '2023-09-17T04:47:39.002Z'
      numEdits: 0
      reactions: []
    id: 650684eb773ceaa8d529b251
    type: comment
  author: Panchovix
  content: Honestly I have never seen that issue. Which version of transformers/exllamav2
    are you using? Even searching for that error I can't find it something clear about
    it.
  created_at: 2023-09-17 03:47:39+00:00
  edited: false
  hidden: false
  id: 650684eb773ceaa8d529b251
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
      fullname: Mason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MasonJ
      type: user
    createdAt: '2023-09-17T08:07:54.000Z'
    data:
      edited: false
      editors:
      - MasonJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.221018984913826
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
          fullname: Mason
          isHf: false
          isPro: false
          name: MasonJ
          type: user
        html: '<p>I''m using a fresh install of the latest Ooba Web UI (commit 0668f4e67fe158a385eff87ace9d0676d657df20).
          I included the output of ''pip list'' from the conda env below. If something
          is amiss in my setup, I have no idea what it might be.</p>

          <p></p><pre><code><br>Package                   Version<br>-------------------------
          ------------<br>absl-py                   1.4.0<br>accelerate                0.22.0<br>aiofiles                  23.1.0<br>aiohttp                   3.8.5<br>aiosignal                 1.3.1<br>altair                    5.1.1<br>antlr4-python3-runtime    4.9.3<br>anyio                     4.0.0<br>appdirs                   1.4.4<br>asttokens                 2.4.0<br>async-timeout             4.0.3<br>attrs                     23.1.0<br>auto-gptq                 0.4.2+cu117<br>backcall                  0.2.0<br>beautifulsoup4            4.12.2<br>bitsandbytes              0.41.1<br>blinker                   1.6.2<br>cachetools                5.3.1<br>certifi                   2022.12.7<br>cffi                      1.15.1<br>charset-normalizer        2.1.1<br>click                     8.1.7<br>colorama                  0.4.6<br>coloredlogs               15.0.1<br>contourpy                 1.1.0<br>cramjam                   2.7.0<br>ctransformers             0.2.27+cu117<br>cycler                    0.11.0<br>datasets                  2.14.5<br>decorator                 5.1.1<br>deep-translator           1.9.2<br>dill                      0.3.7<br>diskcache                 5.6.3<br>docker-pycreds            0.4.0<br>docopt                    0.6.2<br>einops                    0.6.1<br>elevenlabs                0.2.24<br>exceptiongroup            1.1.3<br>executing                 1.2.0<br>exllama                   0.0.17+cu117<br>exllamav2                 0.0.1<br>fastapi                   0.95.2<br>fastparquet               2023.8.0<br>ffmpeg                    1.4<br>ffmpeg-python             0.2.0<br>ffmpy                     0.3.1<br>filelock                  3.9.0<br>Flask                     2.3.3<br>flask-cloudflared         0.0.12<br>fonttools                 4.42.1<br>frozenlist                1.4.0<br>fsspec                    2023.6.0<br>future                    0.18.3<br>gitdb                     4.0.10<br>GitPython                 3.1.36<br>google-auth               2.23.0<br>google-auth-oauthlib      1.0.0<br>gptq-for-llama            0.1.0+cu117<br>gradio                    3.33.1<br>gradio_client             0.2.5<br>grpcio                    1.58.0<br>h11                       0.14.0<br>httpcore                  0.18.0<br>httpx                     0.25.0<br>huggingface-hub           0.17.1<br>humanfriendly             10.0<br>idna                      3.4<br>ipython                   8.15.0<br>itsdangerous              2.1.2<br>jedi                      0.19.0<br>Jinja2                    3.1.2<br>joblib                    1.3.2<br>jsonschema                4.19.0<br>jsonschema-specifications
          2023.7.1<br>kiwisolver                1.4.5<br>linkify-it-py             2.0.2<br>llama-cpp-python          0.1.85<br>llama-cpp-python-cuda     0.1.85+cu117<br>llvmlite                  0.40.1<br>Markdown                  3.4.4<br>markdown-it-py            2.2.0<br>MarkupSafe                2.1.2<br>matplotlib                3.8.0<br>matplotlib-inline         0.1.6<br>mdit-py-plugins           0.3.3<br>mdurl                     0.1.2<br>more-itertools            10.1.0<br>mpmath                    1.2.1<br>multidict                 6.0.4<br>multiprocess              0.70.15<br>networkx                  3.0<br>ngrok                     0.9.0<br>ninja                     1.11.1<br>nltk                      3.8.1<br>num2words                 0.5.12<br>numba                     0.57.1<br>numpy                     1.24.0<br>oauthlib                  3.2.2<br>omegaconf                 2.3.0<br>openai-whisper            20230314<br>optimum                   1.13.1<br>orjson                    3.9.7<br>packaging                 23.1<br>pandas                    2.1.0<br>parso                     0.8.3<br>pathtools                 0.1.2<br>peft                      0.5.0<br>pickleshare               0.7.5<br>Pillow                    10.0.1<br>pip                       23.2.1<br>prompt-toolkit            3.0.39<br>protobuf                  4.24.3<br>psutil                    5.9.5<br>pure-eval                 0.2.2<br>py-cpuinfo                9.0.0<br>pyarrow                   13.0.0<br>pyasn1                    0.5.0<br>pyasn1-modules            0.3.0<br>pycparser                 2.21<br>pydantic                  1.10.12<br>pydub                     0.25.1<br>Pygments                  2.16.1<br>pyparsing                 3.1.1<br>pyreadline3               3.4.1<br>python-dateutil           2.8.2<br>python-multipart          0.0.6<br>pytz                      2023.3.post1<br>PyYAML                    6.0.1<br>referencing               0.30.2<br>regex                     2023.8.8<br>requests                  2.31.0<br>requests-oauthlib         1.3.1<br>rouge                     1.0.1<br>rpds-py                   0.10.3<br>rsa                       4.9<br>safetensors               0.3.2<br>scikit-learn              1.3.0<br>scipy                     1.11.2<br>semantic-version          2.10.0<br>sentence-transformers     2.2.2<br>sentencepiece             0.1.99<br>sentry-sdk                1.31.0<br>setproctitle              1.3.2<br>setuptools                68.0.0<br>six                       1.16.0<br>smmap                     5.0.0<br>sniffio                   1.3.0<br>soundfile                 0.12.1<br>soupsieve                 2.5<br>SpeechRecognition         3.10.0<br>stack-data                0.6.2<br>starlette                 0.27.0<br>sympy                     1.11.1<br>tensorboard               2.14.0<br>tensorboard-data-server   0.7.1<br>threadpoolctl             3.2.0<br>tiktoken                  0.3.1<br>tokenizers                0.13.3<br>toolz                     0.12.0<br>torch                     2.0.1+cu117<br>torchaudio                2.0.2+cu117<br>torchvision               0.15.2+cu117<br>tqdm                      4.66.1<br>traitlets                 5.10.0<br>transformers              4.33.2<br>typing_extensions         4.7.1<br>tzdata                    2023.3<br>uc-micro-py               1.0.2<br>urllib3                   1.26.13<br>uvicorn                   0.23.2<br>wandb                     0.15.10<br>wcwidth                   0.2.6<br>websockets                11.0.2<br>Werkzeug                  2.3.7<br>wheel                     0.38.4<br>xxhash                    3.3.0<br>yarl                      1.9.2</code></pre><p></p>

          '
        raw: 'I''m using a fresh install of the latest Ooba Web UI (commit 0668f4e67fe158a385eff87ace9d0676d657df20).
          I included the output of ''pip list'' from the conda env below. If something
          is amiss in my setup, I have no idea what it might be.


          <ttt><pre><code>

          Package                   Version

          \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\- \-\-\-\-\-\-\-\-\-\-\-\-

          absl-py                   1.4.0

          accelerate                0.22.0

          aiofiles                  23.1.0

          aiohttp                   3.8.5

          aiosignal                 1.3.1

          altair                    5.1.1

          antlr4-python3-runtime    4.9.3

          anyio                     4.0.0

          appdirs                   1.4.4

          asttokens                 2.4.0

          async-timeout             4.0.3

          attrs                     23.1.0

          auto-gptq                 0.4.2+cu117

          backcall                  0.2.0

          beautifulsoup4            4.12.2

          bitsandbytes              0.41.1

          blinker                   1.6.2

          cachetools                5.3.1

          certifi                   2022.12.7

          cffi                      1.15.1

          charset-normalizer        2.1.1

          click                     8.1.7

          colorama                  0.4.6

          coloredlogs               15.0.1

          contourpy                 1.1.0

          cramjam                   2.7.0

          ctransformers             0.2.27+cu117

          cycler                    0.11.0

          datasets                  2.14.5

          decorator                 5.1.1

          deep-translator           1.9.2

          dill                      0.3.7

          diskcache                 5.6.3

          docker-pycreds            0.4.0

          docopt                    0.6.2

          einops                    0.6.1

          elevenlabs                0.2.24

          exceptiongroup            1.1.3

          executing                 1.2.0

          exllama                   0.0.17+cu117

          exllamav2                 0.0.1

          fastapi                   0.95.2

          fastparquet               2023.8.0

          ffmpeg                    1.4

          ffmpeg-python             0.2.0

          ffmpy                     0.3.1

          filelock                  3.9.0

          Flask                     2.3.3

          flask-cloudflared         0.0.12

          fonttools                 4.42.1

          frozenlist                1.4.0

          fsspec                    2023.6.0

          future                    0.18.3

          gitdb                     4.0.10

          GitPython                 3.1.36

          google-auth               2.23.0

          google-auth-oauthlib      1.0.0

          gptq-for-llama            0.1.0+cu117

          gradio                    3.33.1

          gradio_client             0.2.5

          grpcio                    1.58.0

          h11                       0.14.0

          httpcore                  0.18.0

          httpx                     0.25.0

          huggingface-hub           0.17.1

          humanfriendly             10.0

          idna                      3.4

          ipython                   8.15.0

          itsdangerous              2.1.2

          jedi                      0.19.0

          Jinja2                    3.1.2

          joblib                    1.3.2

          jsonschema                4.19.0

          jsonschema-specifications 2023.7.1

          kiwisolver                1.4.5

          linkify-it-py             2.0.2

          llama-cpp-python          0.1.85

          llama-cpp-python-cuda     0.1.85+cu117

          llvmlite                  0.40.1

          Markdown                  3.4.4

          markdown-it-py            2.2.0

          MarkupSafe                2.1.2

          matplotlib                3.8.0

          matplotlib-inline         0.1.6

          mdit-py-plugins           0.3.3

          mdurl                     0.1.2

          more-itertools            10.1.0

          mpmath                    1.2.1

          multidict                 6.0.4

          multiprocess              0.70.15

          networkx                  3.0

          ngrok                     0.9.0

          ninja                     1.11.1

          nltk                      3.8.1

          num2words                 0.5.12

          numba                     0.57.1

          numpy                     1.24.0

          oauthlib                  3.2.2

          omegaconf                 2.3.0

          openai-whisper            20230314

          optimum                   1.13.1

          orjson                    3.9.7

          packaging                 23.1

          pandas                    2.1.0

          parso                     0.8.3

          pathtools                 0.1.2

          peft                      0.5.0

          pickleshare               0.7.5

          Pillow                    10.0.1

          pip                       23.2.1

          prompt-toolkit            3.0.39

          protobuf                  4.24.3

          psutil                    5.9.5

          pure-eval                 0.2.2

          py-cpuinfo                9.0.0

          pyarrow                   13.0.0

          pyasn1                    0.5.0

          pyasn1-modules            0.3.0

          pycparser                 2.21

          pydantic                  1.10.12

          pydub                     0.25.1

          Pygments                  2.16.1

          pyparsing                 3.1.1

          pyreadline3               3.4.1

          python-dateutil           2.8.2

          python-multipart          0.0.6

          pytz                      2023.3.post1

          PyYAML                    6.0.1

          referencing               0.30.2

          regex                     2023.8.8

          requests                  2.31.0

          requests-oauthlib         1.3.1

          rouge                     1.0.1

          rpds-py                   0.10.3

          rsa                       4.9

          safetensors               0.3.2

          scikit-learn              1.3.0

          scipy                     1.11.2

          semantic-version          2.10.0

          sentence-transformers     2.2.2

          sentencepiece             0.1.99

          sentry-sdk                1.31.0

          setproctitle              1.3.2

          setuptools                68.0.0

          six                       1.16.0

          smmap                     5.0.0

          sniffio                   1.3.0

          soundfile                 0.12.1

          soupsieve                 2.5

          SpeechRecognition         3.10.0

          stack-data                0.6.2

          starlette                 0.27.0

          sympy                     1.11.1

          tensorboard               2.14.0

          tensorboard-data-server   0.7.1

          threadpoolctl             3.2.0

          tiktoken                  0.3.1

          tokenizers                0.13.3

          toolz                     0.12.0

          torch                     2.0.1+cu117

          torchaudio                2.0.2+cu117

          torchvision               0.15.2+cu117

          tqdm                      4.66.1

          traitlets                 5.10.0

          transformers              4.33.2

          typing_extensions         4.7.1

          tzdata                    2023.3

          uc-micro-py               1.0.2

          urllib3                   1.26.13

          uvicorn                   0.23.2

          wandb                     0.15.10

          wcwidth                   0.2.6

          websockets                11.0.2

          Werkzeug                  2.3.7

          wheel                     0.38.4

          xxhash                    3.3.0

          yarl                      1.9.2</code></pre></ttt>'
        updatedAt: '2023-09-17T08:07:54.674Z'
      numEdits: 0
      reactions: []
    id: 6506b3da338e4879e44924fa
    type: comment
  author: MasonJ
  content: 'I''m using a fresh install of the latest Ooba Web UI (commit 0668f4e67fe158a385eff87ace9d0676d657df20).
    I included the output of ''pip list'' from the conda env below. If something is
    amiss in my setup, I have no idea what it might be.


    <ttt><pre><code>

    Package                   Version

    \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\- \-\-\-\-\-\-\-\-\-\-\-\-

    absl-py                   1.4.0

    accelerate                0.22.0

    aiofiles                  23.1.0

    aiohttp                   3.8.5

    aiosignal                 1.3.1

    altair                    5.1.1

    antlr4-python3-runtime    4.9.3

    anyio                     4.0.0

    appdirs                   1.4.4

    asttokens                 2.4.0

    async-timeout             4.0.3

    attrs                     23.1.0

    auto-gptq                 0.4.2+cu117

    backcall                  0.2.0

    beautifulsoup4            4.12.2

    bitsandbytes              0.41.1

    blinker                   1.6.2

    cachetools                5.3.1

    certifi                   2022.12.7

    cffi                      1.15.1

    charset-normalizer        2.1.1

    click                     8.1.7

    colorama                  0.4.6

    coloredlogs               15.0.1

    contourpy                 1.1.0

    cramjam                   2.7.0

    ctransformers             0.2.27+cu117

    cycler                    0.11.0

    datasets                  2.14.5

    decorator                 5.1.1

    deep-translator           1.9.2

    dill                      0.3.7

    diskcache                 5.6.3

    docker-pycreds            0.4.0

    docopt                    0.6.2

    einops                    0.6.1

    elevenlabs                0.2.24

    exceptiongroup            1.1.3

    executing                 1.2.0

    exllama                   0.0.17+cu117

    exllamav2                 0.0.1

    fastapi                   0.95.2

    fastparquet               2023.8.0

    ffmpeg                    1.4

    ffmpeg-python             0.2.0

    ffmpy                     0.3.1

    filelock                  3.9.0

    Flask                     2.3.3

    flask-cloudflared         0.0.12

    fonttools                 4.42.1

    frozenlist                1.4.0

    fsspec                    2023.6.0

    future                    0.18.3

    gitdb                     4.0.10

    GitPython                 3.1.36

    google-auth               2.23.0

    google-auth-oauthlib      1.0.0

    gptq-for-llama            0.1.0+cu117

    gradio                    3.33.1

    gradio_client             0.2.5

    grpcio                    1.58.0

    h11                       0.14.0

    httpcore                  0.18.0

    httpx                     0.25.0

    huggingface-hub           0.17.1

    humanfriendly             10.0

    idna                      3.4

    ipython                   8.15.0

    itsdangerous              2.1.2

    jedi                      0.19.0

    Jinja2                    3.1.2

    joblib                    1.3.2

    jsonschema                4.19.0

    jsonschema-specifications 2023.7.1

    kiwisolver                1.4.5

    linkify-it-py             2.0.2

    llama-cpp-python          0.1.85

    llama-cpp-python-cuda     0.1.85+cu117

    llvmlite                  0.40.1

    Markdown                  3.4.4

    markdown-it-py            2.2.0

    MarkupSafe                2.1.2

    matplotlib                3.8.0

    matplotlib-inline         0.1.6

    mdit-py-plugins           0.3.3

    mdurl                     0.1.2

    more-itertools            10.1.0

    mpmath                    1.2.1

    multidict                 6.0.4

    multiprocess              0.70.15

    networkx                  3.0

    ngrok                     0.9.0

    ninja                     1.11.1

    nltk                      3.8.1

    num2words                 0.5.12

    numba                     0.57.1

    numpy                     1.24.0

    oauthlib                  3.2.2

    omegaconf                 2.3.0

    openai-whisper            20230314

    optimum                   1.13.1

    orjson                    3.9.7

    packaging                 23.1

    pandas                    2.1.0

    parso                     0.8.3

    pathtools                 0.1.2

    peft                      0.5.0

    pickleshare               0.7.5

    Pillow                    10.0.1

    pip                       23.2.1

    prompt-toolkit            3.0.39

    protobuf                  4.24.3

    psutil                    5.9.5

    pure-eval                 0.2.2

    py-cpuinfo                9.0.0

    pyarrow                   13.0.0

    pyasn1                    0.5.0

    pyasn1-modules            0.3.0

    pycparser                 2.21

    pydantic                  1.10.12

    pydub                     0.25.1

    Pygments                  2.16.1

    pyparsing                 3.1.1

    pyreadline3               3.4.1

    python-dateutil           2.8.2

    python-multipart          0.0.6

    pytz                      2023.3.post1

    PyYAML                    6.0.1

    referencing               0.30.2

    regex                     2023.8.8

    requests                  2.31.0

    requests-oauthlib         1.3.1

    rouge                     1.0.1

    rpds-py                   0.10.3

    rsa                       4.9

    safetensors               0.3.2

    scikit-learn              1.3.0

    scipy                     1.11.2

    semantic-version          2.10.0

    sentence-transformers     2.2.2

    sentencepiece             0.1.99

    sentry-sdk                1.31.0

    setproctitle              1.3.2

    setuptools                68.0.0

    six                       1.16.0

    smmap                     5.0.0

    sniffio                   1.3.0

    soundfile                 0.12.1

    soupsieve                 2.5

    SpeechRecognition         3.10.0

    stack-data                0.6.2

    starlette                 0.27.0

    sympy                     1.11.1

    tensorboard               2.14.0

    tensorboard-data-server   0.7.1

    threadpoolctl             3.2.0

    tiktoken                  0.3.1

    tokenizers                0.13.3

    toolz                     0.12.0

    torch                     2.0.1+cu117

    torchaudio                2.0.2+cu117

    torchvision               0.15.2+cu117

    tqdm                      4.66.1

    traitlets                 5.10.0

    transformers              4.33.2

    typing_extensions         4.7.1

    tzdata                    2023.3

    uc-micro-py               1.0.2

    urllib3                   1.26.13

    uvicorn                   0.23.2

    wandb                     0.15.10

    wcwidth                   0.2.6

    websockets                11.0.2

    Werkzeug                  2.3.7

    wheel                     0.38.4

    xxhash                    3.3.0

    yarl                      1.9.2</code></pre></ttt>'
  created_at: 2023-09-17 07:07:54+00:00
  edited: false
  hidden: false
  id: 6506b3da338e4879e44924fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-09-17T17:52:04.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9900962114334106
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>All seems good, so I''m not exactly sure why is that happening to
          you.</p>

          <p>I will re-do the quant in some hours with latest exllamav2, so stay tuned
          to that to see if it works for you.</p>

          '
        raw: 'All seems good, so I''m not exactly sure why is that happening to you.


          I will re-do the quant in some hours with latest exllamav2, so stay tuned
          to that to see if it works for you.'
        updatedAt: '2023-09-17T17:52:04.859Z'
      numEdits: 0
      reactions: []
    id: 65073cc40761d34d28f6ad96
    type: comment
  author: Panchovix
  content: 'All seems good, so I''m not exactly sure why is that happening to you.


    I will re-do the quant in some hours with latest exllamav2, so stay tuned to that
    to see if it works for you.'
  created_at: 2023-09-17 16:52:04+00:00
  edited: false
  hidden: false
  id: 65073cc40761d34d28f6ad96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-09-18T06:54:24.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7735793590545654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>I''ve uploaded a new version, with sharded files.</p>

          <p>Can you test if it loads fine for you?</p>

          <p>It is loading fine on my case, while using ooba webui.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/ZtnbgHGBX2N8zU2Cxruge.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/ZtnbgHGBX2N8zU2Cxruge.png"></a></p>

          '
        raw: 'I''ve uploaded a new version, with sharded files.


          Can you test if it loads fine for you?


          It is loading fine on my case, while using ooba webui.



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/ZtnbgHGBX2N8zU2Cxruge.png)

          '
        updatedAt: '2023-09-18T06:54:24.973Z'
      numEdits: 0
      reactions: []
    id: 6507f420d3de67a54626df67
    type: comment
  author: Panchovix
  content: 'I''ve uploaded a new version, with sharded files.


    Can you test if it loads fine for you?


    It is loading fine on my case, while using ooba webui.



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/ZtnbgHGBX2N8zU2Cxruge.png)

    '
  created_at: 2023-09-18 05:54:24+00:00
  edited: false
  hidden: false
  id: 6507f420d3de67a54626df67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
      fullname: Mason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MasonJ
      type: user
    createdAt: '2023-09-19T07:45:29.000Z'
    data:
      edited: true
      editors:
      - MasonJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9443243741989136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
          fullname: Mason
          isHf: false
          isPro: false
          name: MasonJ
          type: user
        html: '<p>I updated ooba and downloaded the new sharded version but I still
          get the exact same error, word for word.</p>

          <p>I did notice a warning about flash-attention this time, which I can see
          from your screenshot, you don''t have.<br>[WARNING:You are running ExLlamaV2
          without flash-attention.]<br>I don''t know if flash-attention is needed
          or not.</p>

          <p>I will probably try to run the model with a different interface next
          to see if that changes anything. Unless you have another idea to troubleshoot
          further.<br>I appreciate the model, and the help.</p>

          '
        raw: 'I updated ooba and downloaded the new sharded version but I still get
          the exact same error, word for word.


          I did notice a warning about flash-attention this time, which I can see
          from your screenshot, you don''t have.

          [WARNING:You are running ExLlamaV2 without flash-attention.]

          I don''t know if flash-attention is needed or not.


          I will probably try to run the model with a different interface next to
          see if that changes anything. Unless you have another idea to troubleshoot
          further.

          I appreciate the model, and the help.'
        updatedAt: '2023-09-19T07:49:56.835Z'
      numEdits: 3
      reactions: []
    id: 65095199e0850b3ff02f8196
    type: comment
  author: MasonJ
  content: 'I updated ooba and downloaded the new sharded version but I still get
    the exact same error, word for word.


    I did notice a warning about flash-attention this time, which I can see from your
    screenshot, you don''t have.

    [WARNING:You are running ExLlamaV2 without flash-attention.]

    I don''t know if flash-attention is needed or not.


    I will probably try to run the model with a different interface next to see if
    that changes anything. Unless you have another idea to troubleshoot further.

    I appreciate the model, and the help.'
  created_at: 2023-09-19 06:45:29+00:00
  edited: true
  hidden: false
  id: 65095199e0850b3ff02f8196
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
      fullname: Mason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MasonJ
      type: user
    createdAt: '2023-09-19T08:09:04.000Z'
    data:
      edited: true
      editors:
      - MasonJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9596881866455078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/562db5c304785c4518ce8a5c7e82fab2.svg
          fullname: Mason
          isHf: false
          isPro: false
          name: MasonJ
          type: user
        html: '<p>I finally found the problem! The model loads correctly now and has
          been working great so far.</p>

          <p>The problem was due to git LFS + my own stupidity. Ooba was trying to
          use a pointer version of tokenizer.model instead of the actual file.</p>

          <p>Sorry for any unnecessary work on your part. I truly do appreciate the
          assistance. Perhaps if someone else makes my same mistake, they can find
          this thread and realize the issue.</p>

          '
        raw: 'I finally found the problem! The model loads correctly now and has been
          working great so far.


          The problem was due to git LFS + my own stupidity. Ooba was trying to use
          a pointer version of tokenizer.model instead of the actual file.


          Sorry for any unnecessary work on your part. I truly do appreciate the assistance.
          Perhaps if someone else makes my same mistake, they can find this thread
          and realize the issue.'
        updatedAt: '2023-09-19T08:16:49.945Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Panchovix
    id: 650957202c804aed59cd2640
    type: comment
  author: MasonJ
  content: 'I finally found the problem! The model loads correctly now and has been
    working great so far.


    The problem was due to git LFS + my own stupidity. Ooba was trying to use a pointer
    version of tokenizer.model instead of the actual file.


    Sorry for any unnecessary work on your part. I truly do appreciate the assistance.
    Perhaps if someone else makes my same mistake, they can find this thread and realize
    the issue.'
  created_at: 2023-09-19 07:09:04+00:00
  edited: true
  hidden: false
  id: 650957202c804aed59cd2640
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Panchovix/airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Error parsing message when loading model
