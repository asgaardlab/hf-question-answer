!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-09-16 03:31:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-09-16T04:31:26.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4100636839866638
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: "<p>I tried this model that you kindly shared, and that's what I get\
          \ on Ooba, with a RTX 3090 :</p>\n<p>127.0.0.1 - - [16/Sep/2023 06:27:45]\
          \ \"GET /api/v1/model HTTP/1.1\" 200 -<br>2023-09-16 06:28:16 INFO:Loading\
          \ Panchovix_airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2...<br>2023-09-16\
          \ 06:28:31 ERROR:Failed to load the model.</p>\n<p>Traceback (most recent\
          \ call last):<br>File \u201CU:\\oobabooga_windows\\text-generation-webui\\\
          modules\\ui_model_menu.py\u201D, line 194, in load_model_wrapper</p>\n<p>shared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)<br>File \u201C\
          U:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D,\
          \ line 77, in load_model</p>\n<p>output = load_func_map<a rel=\"nofollow\"\
          \ href=\"model_name\">loader</a><br>File \u201CU:\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 338, in ExLlamav2_loader</p>\n<p>model, tokenizer\
          \ = Exllamav2Model.from_pretrained(model_name)<br>File \u201CU:\\oobabooga_windows\\\
          text-generation-webui\\modules\\exllamav2.py\u201D, line 40, in from_pretrained</p>\n\
          <p>model.load(split)<br>File \u201CU:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\exllamav2\\model.py\u201D, line 233, in load</p>\n\
          <p>for module in self.modules: module.load()<br>File \u201CU:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\exllamav2\\mlp.py\u201D, line\
          \ 44, in load</p>\n<p>self.up_proj.load()<br>File \u201CU:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\exllamav2\\linear.py\u201D, line\
          \ 37, in load</p>\n<p>if w is None: w = self.load_weight()<br>File \u201C\
          U:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllamav2\\\
          module.py\u201D, line 79, in load_weight</p>\n<p>qtensors = self.load_multi([\"\
          q_weight\", \"q_invperm\", \"q_scale\", \"q_scale_max\", \"q_groups\", \"\
          q_perm\"])<br>File \u201CU:\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\exllamav2\\module.py\u201D, line 69, in load_multi</p>\n\
          <p>tensors[k] = st.get_tensor(self.key + \".\" + k).to(self.device())<br>RuntimeError:\
          \ [enforce fail at \u2026\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator:\
          \ not enough memory: you tried to allocate 88080384 bytes.</p>\n<p>This,\
          \ while :<br>2023-09-16 06:34:32 INFO:Loading turboderp_LLama2-70B-chat-2.55bpw-h6-exl2...<br>2023-09-16\
          \ 06:35:01 INFO:Loaded the model in 28.95 seconds.</p>\n"
        raw: "I tried this model that you kindly shared, and that's what I get on\
          \ Ooba, with a RTX 3090 :\n\n127.0.0.1 - - [16/Sep/2023 06:27:45] \"GET\
          \ /api/v1/model HTTP/1.1\" 200 -\n2023-09-16 06:28:16 INFO:Loading Panchovix_airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2...\n\
          2023-09-16 06:28:31 ERROR:Failed to load the model.\n\nTraceback (most recent\
          \ call last):\nFile \u201CU:\\oobabooga_windows\\text-generation-webui\\\
          modules\\ui_model_menu.py\u201D, line 194, in load_model_wrapper\n\nshared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)\nFile \u201C\
          U:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D,\
          \ line 77, in load_model\n\noutput = load_func_map[loader](model_name)\n\
          File \u201CU:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 338, in ExLlamav2_loader\n\nmodel, tokenizer = Exllamav2Model.from_pretrained(model_name)\n\
          File \u201CU:\\oobabooga_windows\\text-generation-webui\\modules\\exllamav2.py\u201D\
          , line 40, in from_pretrained\n\nmodel.load(split)\nFile \u201CU:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\exllamav2\\model.py\u201D, line\
          \ 233, in load\n\nfor module in self.modules: module.load()\nFile \u201C\
          U:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllamav2\\\
          mlp.py\u201D, line 44, in load\n\nself.up_proj.load()\nFile \u201CU:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\exllamav2\\linear.py\u201D, line\
          \ 37, in load\n\nif w is None: w = self.load_weight()\nFile \u201CU:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\exllamav2\\module.py\u201D, line\
          \ 79, in load_weight\n\nqtensors = self.load_multi([\"q_weight\", \"q_invperm\"\
          , \"q_scale\", \"q_scale_max\", \"q_groups\", \"q_perm\"])\nFile \u201C\
          U:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllamav2\\\
          module.py\u201D, line 69, in load_multi\n\ntensors[k] = st.get_tensor(self.key\
          \ + \".\" + k).to(self.device())\nRuntimeError: [enforce fail at \u2026\\\
          c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough\
          \ memory: you tried to allocate 88080384 bytes.\n\nThis, while :\n2023-09-16\
          \ 06:34:32 INFO:Loading turboderp_LLama2-70B-chat-2.55bpw-h6-exl2...\n2023-09-16\
          \ 06:35:01 INFO:Loaded the model in 28.95 seconds."
        updatedAt: '2023-09-16T04:35:58.816Z'
      numEdits: 2
      reactions: []
    id: 65052f9e3740c884002e9312
    type: comment
  author: Nexesenex
  content: "I tried this model that you kindly shared, and that's what I get on Ooba,\
    \ with a RTX 3090 :\n\n127.0.0.1 - - [16/Sep/2023 06:27:45] \"GET /api/v1/model\
    \ HTTP/1.1\" 200 -\n2023-09-16 06:28:16 INFO:Loading Panchovix_airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2...\n\
    2023-09-16 06:28:31 ERROR:Failed to load the model.\n\nTraceback (most recent\
    \ call last):\nFile \u201CU:\\oobabooga_windows\\text-generation-webui\\modules\\\
    ui_model_menu.py\u201D, line 194, in load_model_wrapper\n\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name, loader)\nFile \u201CU:\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 77, in load_model\n\noutput\
    \ = load_func_map[loader](model_name)\nFile \u201CU:\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 338, in ExLlamav2_loader\n\nmodel, tokenizer =\
    \ Exllamav2Model.from_pretrained(model_name)\nFile \u201CU:\\oobabooga_windows\\\
    text-generation-webui\\modules\\exllamav2.py\u201D, line 40, in from_pretrained\n\
    \nmodel.load(split)\nFile \u201CU:\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\exllamav2\\model.py\u201D, line 233, in load\n\nfor module\
    \ in self.modules: module.load()\nFile \u201CU:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\exllamav2\\mlp.py\u201D, line 44, in load\n\nself.up_proj.load()\n\
    File \u201CU:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllamav2\\\
    linear.py\u201D, line 37, in load\n\nif w is None: w = self.load_weight()\nFile\
    \ \u201CU:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllamav2\\\
    module.py\u201D, line 79, in load_weight\n\nqtensors = self.load_multi([\"q_weight\"\
    , \"q_invperm\", \"q_scale\", \"q_scale_max\", \"q_groups\", \"q_perm\"])\nFile\
    \ \u201CU:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllamav2\\\
    module.py\u201D, line 69, in load_multi\n\ntensors[k] = st.get_tensor(self.key\
    \ + \".\" + k).to(self.device())\nRuntimeError: [enforce fail at \u2026\\c10\\\
    core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you\
    \ tried to allocate 88080384 bytes.\n\nThis, while :\n2023-09-16 06:34:32 INFO:Loading\
    \ turboderp_LLama2-70B-chat-2.55bpw-h6-exl2...\n2023-09-16 06:35:01 INFO:Loaded\
    \ the model in 28.95 seconds."
  created_at: 2023-09-16 03:31:26+00:00
  edited: true
  hidden: false
  id: 65052f9e3740c884002e9312
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-09-16T06:01:11.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9203539490699768
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>The error  <code>DefaultCPUAllocator: not enough memory</code> refers
          as not having enough RAM when loading the model.</p>

          <p>You could try increasing swap file and see again.</p>

          <p>I can load the model without any warning on 64GB RAM (and 200GB Swap),
          but the RAM itself should be enough for a model of this size.</p>

          '
        raw: 'The error  `DefaultCPUAllocator: not enough memory` refers as not having
          enough RAM when loading the model.


          You could try increasing swap file and see again.


          I can load the model without any warning on 64GB RAM (and 200GB Swap), but
          the RAM itself should be enough for a model of this size.'
        updatedAt: '2023-09-16T06:01:11.845Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 650544a7d55dd4e15c83c8d3
    type: comment
  author: Panchovix
  content: 'The error  `DefaultCPUAllocator: not enough memory` refers as not having
    enough RAM when loading the model.


    You could try increasing swap file and see again.


    I can load the model without any warning on 64GB RAM (and 200GB Swap), but the
    RAM itself should be enough for a model of this size.'
  created_at: 2023-09-16 05:01:11+00:00
  edited: false
  hidden: false
  id: 650544a7d55dd4e15c83c8d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-09-16T06:24:21.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9652628898620605
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Thanks Panchovix, it works now. I''m surprised though that this
          quant needs a swap, while the 2.55bpw of Turboderp (in 3 safetensor files)
          doesn''t. But we''re still in Alpha stage, so it''s super already.<br>I''m
          hasty to be able to quantize myself, I''m using 3072 context right now.<br>Once
          again, thank you very much !</p>

          <p>Edit : I was a bit optimistic. I can remain around 10 tokens/s with 1792
          ctx only. But it works.<br>Edit2 : I guess I''ll have to buy a second 3090
          to have a decent output ! :D</p>

          '
        raw: 'Thanks Panchovix, it works now. I''m surprised though that this quant
          needs a swap, while the 2.55bpw of Turboderp (in 3 safetensor files) doesn''t.
          But we''re still in Alpha stage, so it''s super already.

          I''m hasty to be able to quantize myself, I''m using 3072 context right
          now.

          Once again, thank you very much !


          Edit : I was a bit optimistic. I can remain around 10 tokens/s with 1792
          ctx only. But it works.

          Edit2 : I guess I''ll have to buy a second 3090 to have a decent output
          ! :D'
        updatedAt: '2023-09-16T06:49:07.781Z'
      numEdits: 2
      reactions: []
    id: 65054a15dacc94cd6cdc94ac
    type: comment
  author: Nexesenex
  content: 'Thanks Panchovix, it works now. I''m surprised though that this quant
    needs a swap, while the 2.55bpw of Turboderp (in 3 safetensor files) doesn''t.
    But we''re still in Alpha stage, so it''s super already.

    I''m hasty to be able to quantize myself, I''m using 3072 context right now.

    Once again, thank you very much !


    Edit : I was a bit optimistic. I can remain around 10 tokens/s with 1792 ctx only.
    But it works.

    Edit2 : I guess I''ll have to buy a second 3090 to have a decent output ! :D'
  created_at: 2023-09-16 05:24:21+00:00
  edited: true
  hidden: false
  id: 65054a15dacc94cd6cdc94ac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Panchovix/airoboros-l2-70b-gpt4-1.4.1_2.5bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Error while loading
