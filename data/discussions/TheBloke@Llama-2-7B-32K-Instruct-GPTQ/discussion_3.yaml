!!python/object:huggingface_hub.community.DiscussionWithDetails
author: harshilp
conflicting_files: null
created_at: 2023-11-03 09:37:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475bfe0541c5d3726f716cab0a782f26.svg
      fullname: Harshil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harshilp
      type: user
    createdAt: '2023-11-03T10:37:54.000Z'
    data:
      edited: false
      editors:
      - harshilp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9343746900558472
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475bfe0541c5d3726f716cab0a782f26.svg
          fullname: Harshil
          isHf: false
          isPro: false
          name: harshilp
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ !</p>\n<p>I was wondering how you quantized the model using 32k sequence\
          \ length. I have a Llama 7B model that I extended to 32k context length\
          \ using rope scaling and then trained it on private data. When I tried to\
          \ quantize it to 4 bits on calibration dataset of 32k sequence length, I\
          \ ran into OOM errors on 80GB A100. I have more GPUs available but am not\
          \ sure how to use them as quantization seems to only use one GPU. I am using\
          \ the auto-gptq integration with huggingface transformers for quantization.\
          \ </p>\n"
        raw: "Hey @TheBloke !\r\n\r\nI was wondering how you quantized the model using\
          \ 32k sequence length. I have a Llama 7B model that I extended to 32k context\
          \ length using rope scaling and then trained it on private data. When I\
          \ tried to quantize it to 4 bits on calibration dataset of 32k sequence\
          \ length, I ran into OOM errors on 80GB A100. I have more GPUs available\
          \ but am not sure how to use them as quantization seems to only use one\
          \ GPU. I am using the auto-gptq integration with huggingface transformers\
          \ for quantization. "
        updatedAt: '2023-11-03T10:37:54.562Z'
      numEdits: 0
      reactions: []
    id: 6544cd82fcb96b8b487c5ff7
    type: comment
  author: harshilp
  content: "Hey @TheBloke !\r\n\r\nI was wondering how you quantized the model using\
    \ 32k sequence length. I have a Llama 7B model that I extended to 32k context\
    \ length using rope scaling and then trained it on private data. When I tried\
    \ to quantize it to 4 bits on calibration dataset of 32k sequence length, I ran\
    \ into OOM errors on 80GB A100. I have more GPUs available but am not sure how\
    \ to use them as quantization seems to only use one GPU. I am using the auto-gptq\
    \ integration with huggingface transformers for quantization. "
  created_at: 2023-11-03 09:37:54+00:00
  edited: false
  hidden: false
  id: 6544cd82fcb96b8b487c5ff7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-7B-32K-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: OOM when quantizing for 32k context length
