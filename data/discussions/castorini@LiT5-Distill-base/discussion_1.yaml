!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lucasmccabe-lmi
conflicting_files: null
created_at: 2024-01-19 22:48:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6442f477f99d6629a71bd343/FbG6Tn1bTBrLCz11Lzg9q.png?w=200&h=200&f=face
      fullname: Lucas H. McCabe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucasmccabe-lmi
      type: user
    createdAt: '2024-01-19T22:48:57.000Z'
    data:
      edited: false
      editors:
      - lucasmccabe-lmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6625164747238159
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6442f477f99d6629a71bd343/FbG6Tn1bTBrLCz11Lzg9q.png?w=200&h=200&f=face
          fullname: Lucas H. McCabe
          isHf: false
          isPro: false
          name: lucasmccabe-lmi
          type: user
        html: '<p>Hi! I''m a bit confused about usage here.  From the paper, my impression
          is that the prompt template looks like this:</p>

          <p>"""<br>query = "Test query"<br>passages = ["This is Passage 1", "This
          is Passage 2", "This is Passage 3"]</p>

          <p>input_text = ""<br>for i in range(len(passages)):<br>    input_text +=
          "Search Query: %s " %query<br>    input_text += "Passage: [%d]\n%s "%(i+1,
          passages[i])<br>input_text += "Relevance Ranking:"<br>"""</p>

          <p>And usage with the transformers library looks like this:</p>

          <p>"""<br>input_tokens = tokenizer.encode(input_text, return_tensors="pt")<br>output_tokens
          = model.generate(input_tokens)<br>output_text = tokenizer.decode(output_tokens[0],
          skip_special_tokens=True)<br>"""</p>

          <p>Does this look right to you? When I try this, I get an output of "[1]
          &gt; [2] &gt; [3] &gt; [4] &gt; [5]" regardless of how many or what passages
          I provide.</p>

          '
        raw: "Hi! I'm a bit confused about usage here.  From the paper, my impression\
          \ is that the prompt template looks like this:\r\n\r\n\"\"\"\r\nquery =\
          \ \"Test query\"\r\npassages = [\"This is Passage 1\", \"This is Passage\
          \ 2\", \"This is Passage 3\"]\r\n\r\ninput_text = \"\"\r\nfor i in range(len(passages)):\r\
          \n    input_text += \"Search Query: %s \" %query\r\n    input_text += \"\
          Passage: [%d]\\n%s \"%(i+1, passages[i])\r\ninput_text += \"Relevance Ranking:\"\
          \r\n\"\"\"\r\n\r\nAnd usage with the transformers library looks like this:\r\
          \n\r\n\"\"\"\r\ninput_tokens = tokenizer.encode(input_text, return_tensors=\"\
          pt\")\r\noutput_tokens = model.generate(input_tokens)\r\noutput_text = tokenizer.decode(output_tokens[0],\
          \ skip_special_tokens=True)\r\n\"\"\"\r\n\r\nDoes this look right to you?\
          \ When I try this, I get an output of \"[1] > [2] > [3] > [4] > [5]\" regardless\
          \ of how many or what passages I provide."
        updatedAt: '2024-01-19T22:48:57.872Z'
      numEdits: 0
      reactions: []
    id: 65aafc59b68db4f26e0fb991
    type: comment
  author: lucasmccabe-lmi
  content: "Hi! I'm a bit confused about usage here.  From the paper, my impression\
    \ is that the prompt template looks like this:\r\n\r\n\"\"\"\r\nquery = \"Test\
    \ query\"\r\npassages = [\"This is Passage 1\", \"This is Passage 2\", \"This\
    \ is Passage 3\"]\r\n\r\ninput_text = \"\"\r\nfor i in range(len(passages)):\r\
    \n    input_text += \"Search Query: %s \" %query\r\n    input_text += \"Passage:\
    \ [%d]\\n%s \"%(i+1, passages[i])\r\ninput_text += \"Relevance Ranking:\"\r\n\"\
    \"\"\r\n\r\nAnd usage with the transformers library looks like this:\r\n\r\n\"\
    \"\"\r\ninput_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\r\n\
    output_tokens = model.generate(input_tokens)\r\noutput_text = tokenizer.decode(output_tokens[0],\
    \ skip_special_tokens=True)\r\n\"\"\"\r\n\r\nDoes this look right to you? When\
    \ I try this, I get an output of \"[1] > [2] > [3] > [4] > [5]\" regardless of\
    \ how many or what passages I provide."
  created_at: 2024-01-19 22:48:57+00:00
  edited: false
  hidden: false
  id: 65aafc59b68db4f26e0fb991
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bfd45343351bc64f71e856c450add1e.svg
      fullname: Manveer Singh Tamber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: manveertamber
      type: user
    createdAt: '2024-01-19T23:04:23.000Z'
    data:
      edited: false
      editors:
      - manveertamber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9029167890548706
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bfd45343351bc64f71e856c450add1e.svg
          fullname: Manveer Singh Tamber
          isHf: false
          isPro: false
          name: manveertamber
          type: user
        html: '<p>The input template is a bit more nuanced than that because every
          passage is encoded separately by the model''s encoder. Additionally, you
          would need to load the model as a FiD model and not just a T5 model. I recommend
          starting with our codebase: <a rel="nofollow" href="https://github.com/castorini/LiT5">https://github.com/castorini/LiT5</a>
          and in particular our script to perform reranking with LiT5-Distill: <a
          rel="nofollow" href="https://github.com/castorini/LiT5/blob/main/LiT5-Distill.sh">https://github.com/castorini/LiT5/blob/main/LiT5-Distill.sh</a>.
          Then, to look at the generated ordering you would probe the generated_permutations:
          <a rel="nofollow" href="https://github.com/castorini/LiT5/blob/main/FiD/LiT5-Distill.py#L37">https://github.com/castorini/LiT5/blob/main/FiD/LiT5-Distill.py#L37</a>.
          Happy to help further.</p>

          '
        raw: 'The input template is a bit more nuanced than that because every passage
          is encoded separately by the model''s encoder. Additionally, you would need
          to load the model as a FiD model and not just a T5 model. I recommend starting
          with our codebase: https://github.com/castorini/LiT5 and in particular our
          script to perform reranking with LiT5-Distill: https://github.com/castorini/LiT5/blob/main/LiT5-Distill.sh.
          Then, to look at the generated ordering you would probe the generated_permutations:
          https://github.com/castorini/LiT5/blob/main/FiD/LiT5-Distill.py#L37. Happy
          to help further.'
        updatedAt: '2024-01-19T23:04:23.369Z'
      numEdits: 0
      reactions: []
    id: 65aafff76a55aac02adb0963
    type: comment
  author: manveertamber
  content: 'The input template is a bit more nuanced than that because every passage
    is encoded separately by the model''s encoder. Additionally, you would need to
    load the model as a FiD model and not just a T5 model. I recommend starting with
    our codebase: https://github.com/castorini/LiT5 and in particular our script to
    perform reranking with LiT5-Distill: https://github.com/castorini/LiT5/blob/main/LiT5-Distill.sh.
    Then, to look at the generated ordering you would probe the generated_permutations:
    https://github.com/castorini/LiT5/blob/main/FiD/LiT5-Distill.py#L37. Happy to
    help further.'
  created_at: 2024-01-19 23:04:23+00:00
  edited: false
  hidden: false
  id: 65aafff76a55aac02adb0963
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: castorini/LiT5-Distill-base
repo_type: model
status: open
target_branch: null
title: Usage
