!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AMKimia
conflicting_files: null
created_at: 2023-10-10 10:03:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2164be3dd5d5c677f2b43a4f11a9ec85.svg
      fullname: AMKimia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AMKimia
      type: user
    createdAt: '2023-10-10T11:03:24.000Z'
    data:
      edited: false
      editors:
      - AMKimia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9045711755752563
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2164be3dd5d5c677f2b43a4f11a9ec85.svg
          fullname: AMKimia
          isHf: false
          isPro: false
          name: AMKimia
          type: user
        html: '<p>Im trying to find out all possible languges for a mixed language
          text (just as in your example) but im not being able to show more than only
          on of them, the most probable</p>

          '
        raw: Im trying to find out all possible languges for a mixed language text
          (just as in your example) but im not being able to show more than only on
          of them, the most probable
        updatedAt: '2023-10-10T11:03:24.162Z'
      numEdits: 0
      reactions: []
    id: 65252f7ce3dee5692266e20c
    type: comment
  author: AMKimia
  content: Im trying to find out all possible languges for a mixed language text (just
    as in your example) but im not being able to show more than only on of them, the
    most probable
  created_at: 2023-10-10 10:03:24+00:00
  edited: false
  hidden: false
  id: 65252f7ce3dee5692266e20c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668085749659-6362576480c1a705a6e8ecaa.png?w=200&h=200&f=face
      fullname: ERC DiDIP
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ERCDiDip
      type: user
    createdAt: '2023-10-21T19:14:02.000Z'
    data:
      edited: true
      editors:
      - ERCDiDip
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2670786380767822
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668085749659-6362576480c1a705a6e8ecaa.png?w=200&h=200&f=face
          fullname: ERC DiDIP
          isHf: false
          isPro: false
          name: ERCDiDip
          type: user
        html: '<p> If you''d like to see the probabilities for all possible languages,
          you might need to manipulate the raw output logits from the model.</p>

          <p>#Install packages<br>!pip install transformers --quiet</p>

          <p>#Import libraries<br>import torch<br>from transformers import pipeline,
          AutoTokenizer, AutoModelForSequenceClassification<br>import torch.nn.functional
          as F</p>

          <p>#Load tokenizer and model<br>tokenizer = AutoTokenizer.from_pretrained("ERCDiDip/langdetect")<br>model
          = AutoModelForSequenceClassification.from_pretrained("ERCDiDip/langdetect")</p>

          <p>def classify_with_probabilities(text):<br>    #Tokenize and get output
          logits<br>    inputs = tokenizer(text, return_tensors=''pt'', truncation=True,
          max_length=512)<br>    outputs = model(**inputs)<br>    logits = outputs.logits<br>    #Convert
          logits to probabilities<br>    probs = F.softmax(logits, dim=1).squeeze().tolist()<br>    #Pair
          with labels and sort<br>    label_prob_pairs = list(zip(model.config.id2label.values(),
          probs))<br>    label_prob_pairs.sort(key=lambda x: x[1], reverse=True)<br>    return
          label_prob_pairs</p>

          <p>#Use function<br>result = classify_with_probabilities("clemens etc dilecto
          filio scolastico ecclesie wetflari ensi treveren dioc salutem etc significarunt
          nobis dilecti filii commendator et fratres hospitalis beate marie theotonicorum")<br>print(result)<br>Out:
          [(''la'', 0.9999949932098389), (''mhd'', 2.3282084384845803e-06), (''fnhd'',
          4.401515241170273e-07), (''mt'', 1.912285796379365e-07), (''it'', 1.8400885437586112e-07),
          (''gml'', 1.6747328857036337e-07), (''sq'', 9.396003264328101e-08), (''sl'',
          9.104244469426703e-08), (''ar'', 8.333452683473297e-08), (''es'', 7.961761383512567e-08),
          (''tr'', 7.74629711486341e-08), (''fr'', 7.255131606598297e-08), (''bg'',
          7.096467413703067e-08), (''de'', 6.920742379179501e-08), (''pt'', 6.821716880267559e-08),
          (''sk'', 6.656565432194839e-08), (''he'', 6.295641696851817e-08), (''ru'',
          5.417512127792179e-08), (''da'', 5.0310834609490485e-08), (''fro'', 5.0152941355463554e-08),
          (''sv'', 4.9287699255273765e-08), (''se'', 4.7595925423138397e-08), (''en'',
          4.286414423404494e-08), (''lv'', 3.890071198497935e-08), (''uk'', 3.8869192309221035e-08),
          (''lt'', 3.5666676723167257e-08), (''ro'', 3.513213542305493e-08), (''hr'',
          3.3360986861907804e-08), (''ca'', 3.2240908609537655e-08), (''no'', 3.004357651548162e-08),
          (''et'', 2.9618950847520864e-08), (''pl'', 2.9189688888209275e-08), (''grc'',
          2.3447864094805482e-08), (''fi'', 2.2075809624766407e-08), (''el'', 2.155294964722998e-08),
          (''hu'', 1.8249098232558936e-08), (''cs'', 1.665422466601285e-08), (''chu'',
          1.46433540848534e-08), (''nl'', 1.0680254902695197e-08), (''eu'', 7.244770117154076e-09),
          (''zh'', 5.538413283545651e-09)]</p>

          '
        raw: " If you'd like to see the probabilities for all possible languages,\
          \ you might need to manipulate the raw output logits from the model.\n\n\
          #Install packages\n!pip install transformers --quiet\n\n#Import libraries\n\
          import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n\
          import torch.nn.functional as F\n\n#Load tokenizer and model\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"ERCDiDip/langdetect\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
          ERCDiDip/langdetect\")\n\ndef classify_with_probabilities(text):\n    #Tokenize\
          \ and get output logits\n    inputs = tokenizer(text, return_tensors='pt',\
          \ truncation=True, max_length=512)\n    outputs = model(**inputs)\n    logits\
          \ = outputs.logits\n    #Convert logits to probabilities\n    probs = F.softmax(logits,\
          \ dim=1).squeeze().tolist()\n    #Pair with labels and sort\n    label_prob_pairs\
          \ = list(zip(model.config.id2label.values(), probs))\n    label_prob_pairs.sort(key=lambda\
          \ x: x[1], reverse=True)\n    return label_prob_pairs\n\n#Use function\n\
          result = classify_with_probabilities(\"clemens etc dilecto filio scolastico\
          \ ecclesie wetflari ensi treveren dioc salutem etc significarunt nobis dilecti\
          \ filii commendator et fratres hospitalis beate marie theotonicorum\")\n\
          print(result)\nOut: [('la', 0.9999949932098389), ('mhd', 2.3282084384845803e-06),\
          \ ('fnhd', 4.401515241170273e-07), ('mt', 1.912285796379365e-07), ('it',\
          \ 1.8400885437586112e-07), ('gml', 1.6747328857036337e-07), ('sq', 9.396003264328101e-08),\
          \ ('sl', 9.104244469426703e-08), ('ar', 8.333452683473297e-08), ('es', 7.961761383512567e-08),\
          \ ('tr', 7.74629711486341e-08), ('fr', 7.255131606598297e-08), ('bg', 7.096467413703067e-08),\
          \ ('de', 6.920742379179501e-08), ('pt', 6.821716880267559e-08), ('sk', 6.656565432194839e-08),\
          \ ('he', 6.295641696851817e-08), ('ru', 5.417512127792179e-08), ('da', 5.0310834609490485e-08),\
          \ ('fro', 5.0152941355463554e-08), ('sv', 4.9287699255273765e-08), ('se',\
          \ 4.7595925423138397e-08), ('en', 4.286414423404494e-08), ('lv', 3.890071198497935e-08),\
          \ ('uk', 3.8869192309221035e-08), ('lt', 3.5666676723167257e-08), ('ro',\
          \ 3.513213542305493e-08), ('hr', 3.3360986861907804e-08), ('ca', 3.2240908609537655e-08),\
          \ ('no', 3.004357651548162e-08), ('et', 2.9618950847520864e-08), ('pl',\
          \ 2.9189688888209275e-08), ('grc', 2.3447864094805482e-08), ('fi', 2.2075809624766407e-08),\
          \ ('el', 2.155294964722998e-08), ('hu', 1.8249098232558936e-08), ('cs',\
          \ 1.665422466601285e-08), ('chu', 1.46433540848534e-08), ('nl', 1.0680254902695197e-08),\
          \ ('eu', 7.244770117154076e-09), ('zh', 5.538413283545651e-09)]"
        updatedAt: '2023-10-21T19:17:11.283Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AMKimia
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - AMKimia
      relatedEventId: 653422fa8bde2fae1982976c
    id: 653422fa8bde2fae1982976b
    type: comment
  author: ERCDiDip
  content: " If you'd like to see the probabilities for all possible languages, you\
    \ might need to manipulate the raw output logits from the model.\n\n#Install packages\n\
    !pip install transformers --quiet\n\n#Import libraries\nimport torch\nfrom transformers\
    \ import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nimport torch.nn.functional\
    \ as F\n\n#Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"\
    ERCDiDip/langdetect\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
    ERCDiDip/langdetect\")\n\ndef classify_with_probabilities(text):\n    #Tokenize\
    \ and get output logits\n    inputs = tokenizer(text, return_tensors='pt', truncation=True,\
    \ max_length=512)\n    outputs = model(**inputs)\n    logits = outputs.logits\n\
    \    #Convert logits to probabilities\n    probs = F.softmax(logits, dim=1).squeeze().tolist()\n\
    \    #Pair with labels and sort\n    label_prob_pairs = list(zip(model.config.id2label.values(),\
    \ probs))\n    label_prob_pairs.sort(key=lambda x: x[1], reverse=True)\n    return\
    \ label_prob_pairs\n\n#Use function\nresult = classify_with_probabilities(\"clemens\
    \ etc dilecto filio scolastico ecclesie wetflari ensi treveren dioc salutem etc\
    \ significarunt nobis dilecti filii commendator et fratres hospitalis beate marie\
    \ theotonicorum\")\nprint(result)\nOut: [('la', 0.9999949932098389), ('mhd', 2.3282084384845803e-06),\
    \ ('fnhd', 4.401515241170273e-07), ('mt', 1.912285796379365e-07), ('it', 1.8400885437586112e-07),\
    \ ('gml', 1.6747328857036337e-07), ('sq', 9.396003264328101e-08), ('sl', 9.104244469426703e-08),\
    \ ('ar', 8.333452683473297e-08), ('es', 7.961761383512567e-08), ('tr', 7.74629711486341e-08),\
    \ ('fr', 7.255131606598297e-08), ('bg', 7.096467413703067e-08), ('de', 6.920742379179501e-08),\
    \ ('pt', 6.821716880267559e-08), ('sk', 6.656565432194839e-08), ('he', 6.295641696851817e-08),\
    \ ('ru', 5.417512127792179e-08), ('da', 5.0310834609490485e-08), ('fro', 5.0152941355463554e-08),\
    \ ('sv', 4.9287699255273765e-08), ('se', 4.7595925423138397e-08), ('en', 4.286414423404494e-08),\
    \ ('lv', 3.890071198497935e-08), ('uk', 3.8869192309221035e-08), ('lt', 3.5666676723167257e-08),\
    \ ('ro', 3.513213542305493e-08), ('hr', 3.3360986861907804e-08), ('ca', 3.2240908609537655e-08),\
    \ ('no', 3.004357651548162e-08), ('et', 2.9618950847520864e-08), ('pl', 2.9189688888209275e-08),\
    \ ('grc', 2.3447864094805482e-08), ('fi', 2.2075809624766407e-08), ('el', 2.155294964722998e-08),\
    \ ('hu', 1.8249098232558936e-08), ('cs', 1.665422466601285e-08), ('chu', 1.46433540848534e-08),\
    \ ('nl', 1.0680254902695197e-08), ('eu', 7.244770117154076e-09), ('zh', 5.538413283545651e-09)]"
  created_at: 2023-10-21 18:14:02+00:00
  edited: true
  hidden: false
  id: 653422fa8bde2fae1982976b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668085749659-6362576480c1a705a6e8ecaa.png?w=200&h=200&f=face
      fullname: ERC DiDIP
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ERCDiDip
      type: user
    createdAt: '2023-10-21T19:14:02.000Z'
    data:
      status: closed
    id: 653422fa8bde2fae1982976c
    type: status-change
  author: ERCDiDip
  created_at: 2023-10-21 18:14:02+00:00
  id: 653422fa8bde2fae1982976c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2164be3dd5d5c677f2b43a4f11a9ec85.svg
      fullname: AMKimia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AMKimia
      type: user
    createdAt: '2023-10-23T13:26:38.000Z'
    data:
      edited: false
      editors:
      - AMKimia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.870770275592804
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2164be3dd5d5c677f2b43a4f11a9ec85.svg
          fullname: AMKimia
          isHf: false
          isPro: false
          name: AMKimia
          type: user
        html: '<p>Thanks so so so much, i will do some testings and let you know!</p>

          '
        raw: 'Thanks so so so much, i will do some testings and let you know!

          '
        updatedAt: '2023-10-23T13:26:38.851Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - ERCDiDip
    id: 6536748ec33220a87280108f
    type: comment
  author: AMKimia
  content: 'Thanks so so so much, i will do some testings and let you know!

    '
  created_at: 2023-10-23 12:26:38+00:00
  edited: false
  hidden: false
  id: 6536748ec33220a87280108f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668085749659-6362576480c1a705a6e8ecaa.png?w=200&h=200&f=face
      fullname: ERC DiDIP
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ERCDiDip
      type: user
    createdAt: '2023-10-24T18:26:14.000Z'
    data:
      edited: true
      editors:
      - ERCDiDip
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40357348322868347
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668085749659-6362576480c1a705a6e8ecaa.png?w=200&h=200&f=face
          fullname: ERC DiDIP
          isHf: false
          isPro: false
          name: ERCDiDip
          type: user
        html: "<p>Very welcome :) But probably the sliding window would be a better\
          \ choice.  The following code break the input text into overlapping segments\
          \ (sliding windows) and uses our model to detect the language of each segment.\
          \ It then counts and returns the N (n_most_common=2) most frequently detected\
          \ languages from these segments.</p>\n<p>!pip install transformers --quiet</p>\n\
          <p>import torch<br>from transformers import pipeline<br>from collections\
          \ import Counter</p>\n<p>classificator = pipeline(\"text-classification\"\
          , model=\"ERCDiDip/langdetect\")</p>\n<p>def sliding_window(text, window_size=30,\
          \ stride=15):<br>    return [text[i:i+window_size] for i in range(0, len(text)\
          \ - window_size + 1, stride)]</p>\n<p>def detect_languages(text, n_most_common=2):<br>\
          \    windows = sliding_window(text)<br>    detected_languages = []<br> \
          \   for window in windows:<br>        result = classificator(window)<br>\
          \        detected_languages.append(result[0]['label'])<br>    # Count languages<br>\
          \    language_counts = Counter(detected_languages)<br>    # Get the most\
          \ common N languages<br>    common_languages = language_counts.most_common(n_most_common)<br>\
          \    return common_languages</p>\n<p>result = detect_languages(\"clemens\
          \ etc dilecto filio scolastico ecclesie wetflari ensi treveren dioc salutem\
          \ etc significarunt nobis dilecti filii commendator et fratres hospitalis\
          \ beate marie theotonicorum. Anerkennung des F\xFCrstenstandes der Gr\xE4\
          fin Gertrude von Schaumburg und ihrer 9 Kinder aus der Ehe mit dem Kurf\xFC\
          rsten und Landgrafen Friedrich Wilhelm von Hessen unter dem Titel F\xFC\
          rstinnen und F\xFCrsten von Hanau im Kaiserreich \xD6sterreich unter Franz\
          \ Joseph I.  \")<br>print(result)<br>[('la', 9), ('de', 9)]</p>\n<p>All\
          \ the best!</p>\n"
        raw: "Very welcome :) But probably the sliding window would be a better choice.\
          \  The following code break the input text into overlapping segments (sliding\
          \ windows) and uses our model to detect the language of each segment. It\
          \ then counts and returns the N (n_most_common=2) most frequently detected\
          \ languages from these segments.\n\n!pip install transformers --quiet\n\n\
          import torch\nfrom transformers import pipeline\nfrom collections import\
          \ Counter\n\nclassificator = pipeline(\"text-classification\", model=\"\
          ERCDiDip/langdetect\")\n\ndef sliding_window(text, window_size=30, stride=15):\n\
          \    return [text[i:i+window_size] for i in range(0, len(text) - window_size\
          \ + 1, stride)]\n\ndef detect_languages(text, n_most_common=2):\n    windows\
          \ = sliding_window(text)\n    detected_languages = []\n    for window in\
          \ windows:\n        result = classificator(window)\n        detected_languages.append(result[0]['label'])\n\
          \    # Count languages\n    language_counts = Counter(detected_languages)\n\
          \    # Get the most common N languages\n    common_languages = language_counts.most_common(n_most_common)\n\
          \    return common_languages\n\nresult = detect_languages(\"clemens etc\
          \ dilecto filio scolastico ecclesie wetflari ensi treveren dioc salutem\
          \ etc significarunt nobis dilecti filii commendator et fratres hospitalis\
          \ beate marie theotonicorum. Anerkennung des F\xFCrstenstandes der Gr\xE4\
          fin Gertrude von Schaumburg und ihrer 9 Kinder aus der Ehe mit dem Kurf\xFC\
          rsten und Landgrafen Friedrich Wilhelm von Hessen unter dem Titel F\xFC\
          rstinnen und F\xFCrsten von Hanau im Kaiserreich \xD6sterreich unter Franz\
          \ Joseph I.  \")\nprint(result)\n[('la', 9), ('de', 9)]\n\nAll the best!\n"
        updatedAt: '2023-10-24T18:29:29.777Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - AMKimia
    id: 65380c46b1ac7a3f2c3a3490
    type: comment
  author: ERCDiDip
  content: "Very welcome :) But probably the sliding window would be a better choice.\
    \  The following code break the input text into overlapping segments (sliding\
    \ windows) and uses our model to detect the language of each segment. It then\
    \ counts and returns the N (n_most_common=2) most frequently detected languages\
    \ from these segments.\n\n!pip install transformers --quiet\n\nimport torch\n\
    from transformers import pipeline\nfrom collections import Counter\n\nclassificator\
    \ = pipeline(\"text-classification\", model=\"ERCDiDip/langdetect\")\n\ndef sliding_window(text,\
    \ window_size=30, stride=15):\n    return [text[i:i+window_size] for i in range(0,\
    \ len(text) - window_size + 1, stride)]\n\ndef detect_languages(text, n_most_common=2):\n\
    \    windows = sliding_window(text)\n    detected_languages = []\n    for window\
    \ in windows:\n        result = classificator(window)\n        detected_languages.append(result[0]['label'])\n\
    \    # Count languages\n    language_counts = Counter(detected_languages)\n  \
    \  # Get the most common N languages\n    common_languages = language_counts.most_common(n_most_common)\n\
    \    return common_languages\n\nresult = detect_languages(\"clemens etc dilecto\
    \ filio scolastico ecclesie wetflari ensi treveren dioc salutem etc significarunt\
    \ nobis dilecti filii commendator et fratres hospitalis beate marie theotonicorum.\
    \ Anerkennung des F\xFCrstenstandes der Gr\xE4fin Gertrude von Schaumburg und\
    \ ihrer 9 Kinder aus der Ehe mit dem Kurf\xFCrsten und Landgrafen Friedrich Wilhelm\
    \ von Hessen unter dem Titel F\xFCrstinnen und F\xFCrsten von Hanau im Kaiserreich\
    \ \xD6sterreich unter Franz Joseph I.  \")\nprint(result)\n[('la', 9), ('de',\
    \ 9)]\n\nAll the best!\n"
  created_at: 2023-10-24 17:26:14+00:00
  edited: true
  hidden: false
  id: 65380c46b1ac7a3f2c3a3490
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668085749659-6362576480c1a705a6e8ecaa.png?w=200&h=200&f=face
      fullname: ERC DiDIP
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ERCDiDip
      type: user
    createdAt: '2023-10-24T18:30:22.000Z'
    data:
      pinned: true
    id: 65380d3e4f746350b335865d
    type: pinning-change
  author: ERCDiDip
  created_at: 2023-10-24 17:30:22+00:00
  id: 65380d3e4f746350b335865d
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2164be3dd5d5c677f2b43a4f11a9ec85.svg
      fullname: AMKimia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AMKimia
      type: user
    createdAt: '2023-10-25T06:49:34.000Z'
    data:
      edited: false
      editors:
      - AMKimia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659651517868042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2164be3dd5d5c677f2b43a4f11a9ec85.svg
          fullname: AMKimia
          isHf: false
          isPro: false
          name: AMKimia
          type: user
        html: '<p>let us give it a try and will let you know</p>

          '
        raw: let us give it a try and will let you know
        updatedAt: '2023-10-25T06:49:34.269Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ERCDiDip
    id: 6538ba7e09aa85bf9566f945
    type: comment
  author: AMKimia
  content: let us give it a try and will let you know
  created_at: 2023-10-25 05:49:34+00:00
  edited: false
  hidden: false
  id: 6538ba7e09aa85bf9566f945
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ERCDiDip/langdetect
repo_type: model
status: closed
target_branch: null
title: What pipeline are you using to show up all probabilities for 1 text?
