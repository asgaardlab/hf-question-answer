!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DanielCL
conflicting_files: null
created_at: 2023-08-14 07:51:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b7914713d052c19832931bd7608a80b.svg
      fullname: Liao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielCL
      type: user
    createdAt: '2023-08-14T08:51:16.000Z'
    data:
      edited: true
      editors:
      - DanielCL
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9328429698944092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b7914713d052c19832931bd7608a80b.svg
          fullname: Liao
          isHf: false
          isPro: false
          name: DanielCL
          type: user
        html: '<p>According to the demo video at <a href="https://huggingface.co/blog/swift-coreml-llm">https://huggingface.co/blog/swift-coreml-llm</a>,
          the speed of this model was 600token/second (pretty cool). </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/MrGYiobeDjKZlGGEcfJDf.png"><img
          alt="Screenshot 2023-08-14 at 16.52.09.png" src="https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/MrGYiobeDjKZlGGEcfJDf.png"></a></p>

          <p>So, I have cloned <a rel="nofollow" href="https://github.com/huggingface/swift-chat">https://github.com/huggingface/swift-chat</a>,
          and downloaded this model successfully at coreml-projects/Llama-2-7b-chat-coreml,
          and it can run without error.</p>

          <p>Although I don''t know which chip M2 or M1 or something even more powerful
          is used in the demo, but my M1 8G RAM macbook air can only run at 0.01token/s
          seems very wrong. </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/gHBmL2zlgtVk5g9m2m4kG.png"><img
          alt="Screenshot 2023-08-14 at 16.33.39.png" src="https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/gHBmL2zlgtVk5g9m2m4kG.png"></a></p>

          <p>What am I doing wrong? </p>

          <p>Thank you so much!</p>

          '
        raw: "According to the demo video at https://huggingface.co/blog/swift-coreml-llm,\
          \ the speed of this model was 600token/second (pretty cool). \n\n![Screenshot\
          \ 2023-08-14 at 16.52.09.png](https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/MrGYiobeDjKZlGGEcfJDf.png)\n\
          \n\nSo, I have cloned https://github.com/huggingface/swift-chat, and downloaded\
          \ this model successfully at coreml-projects/Llama-2-7b-chat-coreml, and\
          \ it can run without error.\n\nAlthough I don't know which chip M2 or M1\
          \ or something even more powerful is used in the demo, but my M1 8G RAM\
          \ macbook air can only run at 0.01token/s seems very wrong. \n\n\n![Screenshot\
          \ 2023-08-14 at 16.33.39.png](https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/gHBmL2zlgtVk5g9m2m4kG.png)\n\
          \n\n\nWhat am I doing wrong? \n\nThank you so much!"
        updatedAt: '2023-08-14T08:53:06.785Z'
      numEdits: 1
      reactions: []
    id: 64d9eb04c79ca7ce7707bc91
    type: comment
  author: DanielCL
  content: "According to the demo video at https://huggingface.co/blog/swift-coreml-llm,\
    \ the speed of this model was 600token/second (pretty cool). \n\n![Screenshot\
    \ 2023-08-14 at 16.52.09.png](https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/MrGYiobeDjKZlGGEcfJDf.png)\n\
    \n\nSo, I have cloned https://github.com/huggingface/swift-chat, and downloaded\
    \ this model successfully at coreml-projects/Llama-2-7b-chat-coreml, and it can\
    \ run without error.\n\nAlthough I don't know which chip M2 or M1 or something\
    \ even more powerful is used in the demo, but my M1 8G RAM macbook air can only\
    \ run at 0.01token/s seems very wrong. \n\n\n![Screenshot 2023-08-14 at 16.33.39.png](https://cdn-uploads.huggingface.co/production/uploads/6278f86b3ac0e9f9e3de2a61/gHBmL2zlgtVk5g9m2m4kG.png)\n\
    \n\n\nWhat am I doing wrong? \n\nThank you so much!"
  created_at: 2023-08-14 07:51:16+00:00
  edited: true
  hidden: false
  id: 64d9eb04c79ca7ce7707bc91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-08-14T11:31:47.000Z'
    data:
      edited: true
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9846264123916626
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<p>The demo video you refer to indicates a generation speed of 6.42
          tokens per second (as opposed to 600), but 0.01 tokens per second definitely
          seems wrong.</p>

          <p><video controls="" width="600" type="video/mp4" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/swift-transformers/llama-2-7b-chat.mp4"></video></p>

          <p>It might simply be an issue with timing, as if this was indeed the case,
          it would take 100 seconds per token (which would mean the text shown would
          take ~30 minutes (which I doubt is the case). Can you take a recording,
          or maybe just answer whether it is actually that slow?</p>

          '
        raw: 'The demo video you refer to indicates a generation speed of 6.42 tokens
          per second (as opposed to 600), but 0.01 tokens per second definitely seems
          wrong.


          <video src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/swift-transformers/llama-2-7b-chat.mp4"
          type="video/mp4" width=600 controls ></video>


          It might simply be an issue with timing, as if this was indeed the case,
          it would take 100 seconds per token (which would mean the text shown would
          take ~30 minutes (which I doubt is the case). Can you take a recording,
          or maybe just answer whether it is actually that slow?'
        updatedAt: '2023-08-14T11:33:21.472Z'
      numEdits: 6
      reactions: []
    id: 64da10a3311afacb538e3692
    type: comment
  author: Xenova
  content: 'The demo video you refer to indicates a generation speed of 6.42 tokens
    per second (as opposed to 600), but 0.01 tokens per second definitely seems wrong.


    <video src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/swift-transformers/llama-2-7b-chat.mp4"
    type="video/mp4" width=600 controls ></video>


    It might simply be an issue with timing, as if this was indeed the case, it would
    take 100 seconds per token (which would mean the text shown would take ~30 minutes
    (which I doubt is the case). Can you take a recording, or maybe just answer whether
    it is actually that slow?'
  created_at: 2023-08-14 10:31:47+00:00
  edited: true
  hidden: false
  id: 64da10a3311afacb538e3692
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-08-14T11:33:33.000Z'
    data:
      status: closed
    id: 64da110dfcf8221c914ae9f8
    type: status-change
  author: Xenova
  created_at: 2023-08-14 10:33:33+00:00
  id: 64da110dfcf8221c914ae9f8
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-08-14T11:33:40.000Z'
    data:
      status: open
    id: 64da111495da0aa95cddb240
    type: status-change
  author: Xenova
  created_at: 2023-08-14 10:33:40+00:00
  id: 64da111495da0aa95cddb240
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b7914713d052c19832931bd7608a80b.svg
      fullname: Liao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielCL
      type: user
    createdAt: '2023-08-16T13:18:01.000Z'
    data:
      edited: false
      editors:
      - DanielCL
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761911034584045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b7914713d052c19832931bd7608a80b.svg
          fullname: Liao
          isHf: false
          isPro: false
          name: DanielCL
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;Xenova&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Xenova\"\
          >@<span class=\"underline\">Xenova</span></a></span>\n\n\t</span></span>\
          \ for your reply. It is very very slow in deed about 20 minutes or more\
          \ per answer, every time I run it and every question on each run. </p>\n"
        raw: 'Thanks @Xenova for your reply. It is very very slow in deed about 20
          minutes or more per answer, every time I run it and every question on each
          run. '
        updatedAt: '2023-08-16T13:18:01.879Z'
      numEdits: 0
      reactions: []
    id: 64dccc8930bd3fef69611a07
    type: comment
  author: DanielCL
  content: 'Thanks @Xenova for your reply. It is very very slow in deed about 20 minutes
    or more per answer, every time I run it and every question on each run. '
  created_at: 2023-08-16 12:18:01+00:00
  edited: false
  hidden: false
  id: 64dccc8930bd3fef69611a07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3dedf6873c99c096727337c8888a269.svg
      fullname: Michael Keith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nifta
      type: user
    createdAt: '2023-08-19T02:04:58.000Z'
    data:
      edited: false
      editors:
      - nifta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9711857438087463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3dedf6873c99c096727337c8888a269.svg
          fullname: Michael Keith
          isHf: false
          isPro: false
          name: nifta
          type: user
        html: '<p>Not sure if a "me too" is useful, but it''s the same on my 2022
          m2 16gb air. i get ~0.02 tokens/s. I''ve tried release builds as well as
          debug. Happy to provide more details/test stuff.</p>

          '
        raw: Not sure if a "me too" is useful, but it's the same on my 2022 m2 16gb
          air. i get ~0.02 tokens/s. I've tried release builds as well as debug. Happy
          to provide more details/test stuff.
        updatedAt: '2023-08-19T02:04:58.646Z'
      numEdits: 0
      reactions: []
    id: 64e0234a6dab6a1d8251de97
    type: comment
  author: nifta
  content: Not sure if a "me too" is useful, but it's the same on my 2022 m2 16gb
    air. i get ~0.02 tokens/s. I've tried release builds as well as debug. Happy to
    provide more details/test stuff.
  created_at: 2023-08-19 01:04:58+00:00
  edited: false
  hidden: false
  id: 64e0234a6dab6a1d8251de97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd62edec23ab347bef4489a83dc2d455.svg
      fullname: Christopher Pietsch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chrispie
      type: user
    createdAt: '2023-08-21T10:59:36.000Z'
    data:
      edited: false
      editors:
      - chrispie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.887039303779602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd62edec23ab347bef4489a83dc2d455.svg
          fullname: Christopher Pietsch
          isHf: false
          isPro: false
          name: chrispie
          type: user
        html: '<p>I made an issue @ <a rel="nofollow" href="https://github.com/huggingface/swift-chat/issues/3">https://github.com/huggingface/swift-chat/issues/3</a>
          and get 0.02 tokens/s too.</p>

          '
        raw: I made an issue @ https://github.com/huggingface/swift-chat/issues/3
          and get 0.02 tokens/s too.
        updatedAt: '2023-08-21T10:59:36.456Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - DanielCL
    id: 64e34398e5bd41dd056edbab
    type: comment
  author: chrispie
  content: I made an issue @ https://github.com/huggingface/swift-chat/issues/3 and
    get 0.02 tokens/s too.
  created_at: 2023-08-21 09:59:36+00:00
  edited: false
  hidden: false
  id: 64e34398e5bd41dd056edbab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-08-23T10:43:48.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.947837233543396
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: '<p>My computer is an M1 Max with 64 GB. I believe the main factor that
          affects performance in this case is the amount of RAM installed. Because
          the model is so large, the system will swap if there''s not enough memory
          to hold everything in RAM, and that will kill performance.</p>

          <p>I''ll run some tests and answer in the github issue!</p>

          '
        raw: 'My computer is an M1 Max with 64 GB. I believe the main factor that
          affects performance in this case is the amount of RAM installed. Because
          the model is so large, the system will swap if there''s not enough memory
          to hold everything in RAM, and that will kill performance.


          I''ll run some tests and answer in the github issue!'
        updatedAt: '2023-08-23T10:43:48.952Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - DanielCL
        - Tasaloris13
    id: 64e5e2e4fbc88c920c0738c7
    type: comment
  author: pcuenq
  content: 'My computer is an M1 Max with 64 GB. I believe the main factor that affects
    performance in this case is the amount of RAM installed. Because the model is
    so large, the system will swap if there''s not enough memory to hold everything
    in RAM, and that will kill performance.


    I''ll run some tests and answer in the github issue!'
  created_at: 2023-08-23 09:43:48+00:00
  edited: false
  hidden: false
  id: 64e5e2e4fbc88c920c0738c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: coreml-projects/Llama-2-7b-chat-coreml
repo_type: model
status: open
target_branch: null
title: M1 8G RAM macbook air run at 0.01token/second, something is wrong right?
