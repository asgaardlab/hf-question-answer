!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VlaTal
conflicting_files: null
created_at: 2023-07-27 13:49:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/51c69d56e15d7f5610ea41c620414884.svg
      fullname: Vladyslav Talakh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VlaTal
      type: user
    createdAt: '2023-07-27T14:49:05.000Z'
    data:
      edited: true
      editors:
      - VlaTal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6190259456634521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/51c69d56e15d7f5610ea41c620414884.svg
          fullname: Vladyslav Talakh
          isHf: false
          isPro: false
          name: VlaTal
          type: user
        html: "<p>I load model in 8bit and it fit at all in my GPU, but GPU don't\
          \ work at all. GPU doesn't even heats a lot, like with other models. But\
          \ the CPU works on 100% percent. And the inference speed is about 2-3 tokens/s.<br>Here`s\
          \  a code which I use for loading and inferece:</p>\n<pre><code>model =\
          \ 'WizardLM/WizardCoder-15B-V1.0'\n\ndef load_model(model = model):\n  \
          \  tokenizer = AutoTokenizer.from_pretrained(model)\n    model = AutoModelForCausalLM.from_pretrained(model,\
          \ device_map=device_map, load_in_8bit = True)\n    return tokenizer, model\n\
          \ntokenizer, model = load_model(model)\n\ngeneration_config = GenerationConfig(\n\
          \    temperature=0.0,\n    top_p=0.95,\n    top_k=50,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    pad_token_id=tokenizer.pad_token_id,\n)\n\nprompt_template = f'''\n\
          Below is an instruction that describes a task. Write a response that appropriately\
          \ completes the request\n\n### Instruction: {prompt}\n\n### Response:'''\n\
          \ninputs = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda\"\
          )\ngenerated_ids = model.generate(**inputs, generation_config=generation_config,\
          \ max_new_tokens=3000)\noutputs = tokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=True)\n\nprint(outputs[0])\n</code></pre>\n"
        raw: "I load model in 8bit and it fit at all in my GPU, but GPU don't work\
          \ at all. GPU doesn't even heats a lot, like with other models. But the\
          \ CPU works on 100% percent. And the inference speed is about 2-3 tokens/s.\n\
          Here`s  a code which I use for loading and inferece:\n```\nmodel = 'WizardLM/WizardCoder-15B-V1.0'\n\
          \ndef load_model(model = model):\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\
          \    model = AutoModelForCausalLM.from_pretrained(model, device_map=device_map,\
          \ load_in_8bit = True)\n    return tokenizer, model\n\ntokenizer, model\
          \ = load_model(model)\n\ngeneration_config = GenerationConfig(\n    temperature=0.0,\n\
          \    top_p=0.95,\n    top_k=50,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    pad_token_id=tokenizer.pad_token_id,\n)\n\nprompt_template = f'''\n\
          Below is an instruction that describes a task. Write a response that appropriately\
          \ completes the request\n\n### Instruction: {prompt}\n\n### Response:'''\n\
          \ninputs = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda\"\
          )\ngenerated_ids = model.generate(**inputs, generation_config=generation_config,\
          \ max_new_tokens=3000)\noutputs = tokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=True)\n\nprint(outputs[0])\n```"
        updatedAt: '2023-07-27T14:49:22.397Z'
      numEdits: 1
      reactions: []
    id: 64c283e12b1ffc2eecc28818
    type: comment
  author: VlaTal
  content: "I load model in 8bit and it fit at all in my GPU, but GPU don't work at\
    \ all. GPU doesn't even heats a lot, like with other models. But the CPU works\
    \ on 100% percent. And the inference speed is about 2-3 tokens/s.\nHere`s  a code\
    \ which I use for loading and inferece:\n```\nmodel = 'WizardLM/WizardCoder-15B-V1.0'\n\
    \ndef load_model(model = model):\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\
    \    model = AutoModelForCausalLM.from_pretrained(model, device_map=device_map,\
    \ load_in_8bit = True)\n    return tokenizer, model\n\ntokenizer, model = load_model(model)\n\
    \ngeneration_config = GenerationConfig(\n    temperature=0.0,\n    top_p=0.95,\n\
    \    top_k=50,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.pad_token_id,\n\
    )\n\nprompt_template = f'''\nBelow is an instruction that describes a task. Write\
    \ a response that appropriately completes the request\n\n### Instruction: {prompt}\n\
    \n### Response:'''\n\ninputs = tokenizer(prompt_template, return_tensors=\"pt\"\
    ).to(\"cuda\")\ngenerated_ids = model.generate(**inputs, generation_config=generation_config,\
    \ max_new_tokens=3000)\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\
    \nprint(outputs[0])\n```"
  created_at: 2023-07-27 13:49:05+00:00
  edited: true
  hidden: false
  id: 64c283e12b1ffc2eecc28818
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: WizardLM/WizardCoder-15B-V1.0
repo_type: model
status: open
target_branch: null
title: Inference's speed
