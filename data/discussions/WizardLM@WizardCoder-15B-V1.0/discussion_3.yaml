!!python/object:huggingface_hub.community.DiscussionWithDetails
author: innate
conflicting_files: null
created_at: 2023-06-16 17:51:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
      fullname: nate
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: innate
      type: user
    createdAt: '2023-06-16T18:51:43.000Z'
    data:
      edited: false
      editors:
      - innate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9252774715423584
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
          fullname: nate
          isHf: false
          isPro: false
          name: innate
          type: user
        html: '<p>I was wondering what the reccomended amount of VRAM neccesary would
          be to run this model using KoboldAI or oobabooga web-ui.<br>I''ve been using
          the below image as reference but understand it''s not exact:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/j2a0i5BzZc8zfUmC29tqs.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/j2a0i5BzZc8zfUmC29tqs.png"></a></p>

          <p>I apologize if this is a question generated by ignorance of the inner
          working of LLMs and AI in general but appreciate any help or info you could
          share to enlighten me.</p>

          <p>I''ve been looking for a model to use for coding assistance and want
          to know if I''ll be able to run this with my current local setup:<br>Ryzen
          7 5800X3D<br>32GB RAM<br>2x 3070 8GB</p>

          '
        raw: "I was wondering what the reccomended amount of VRAM neccesary would\
          \ be to run this model using KoboldAI or oobabooga web-ui.\r\nI've been\
          \ using the below image as reference but understand it's not exact:\r\n\r\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/j2a0i5BzZc8zfUmC29tqs.png)\r\
          \n\r\nI apologize if this is a question generated by ignorance of the inner\
          \ working of LLMs and AI in general but appreciate any help or info you\
          \ could share to enlighten me.\r\n\r\nI've been looking for a model to use\
          \ for coding assistance and want to know if I'll be able to run this with\
          \ my current local setup:\r\nRyzen 7 5800X3D\r\n32GB RAM\r\n2x 3070 8GB"
        updatedAt: '2023-06-16T18:51:43.548Z'
      numEdits: 0
      reactions: []
    id: 648caf3fb750349d4a6c3689
    type: comment
  author: innate
  content: "I was wondering what the reccomended amount of VRAM neccesary would be\
    \ to run this model using KoboldAI or oobabooga web-ui.\r\nI've been using the\
    \ below image as reference but understand it's not exact:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/j2a0i5BzZc8zfUmC29tqs.png)\r\
    \n\r\nI apologize if this is a question generated by ignorance of the inner working\
    \ of LLMs and AI in general but appreciate any help or info you could share to\
    \ enlighten me.\r\n\r\nI've been looking for a model to use for coding assistance\
    \ and want to know if I'll be able to run this with my current local setup:\r\n\
    Ryzen 7 5800X3D\r\n32GB RAM\r\n2x 3070 8GB"
  created_at: 2023-06-16 17:51:43+00:00
  edited: false
  hidden: false
  id: 648caf3fb750349d4a6c3689
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd09710a75fdaf376d134f234ad1ed12.svg
      fullname: Rajesh Akkineni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkinenirajesh
      type: user
    createdAt: '2023-06-17T08:32:19.000Z'
    data:
      edited: false
      editors:
      - akkinenirajesh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.948252260684967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd09710a75fdaf376d134f234ad1ed12.svg
          fullname: Rajesh Akkineni
          isHf: false
          isPro: false
          name: akkinenirajesh
          type: user
        html: '<p>I have a 64BM RAM and  RTX 4090 with 24GB GPU. Not sure what is
          going on, (I am new to all this). Trying to load init the model itself (with
          the given config) is causing overflow of my system RAM. It did not start
          loading the weights yet. </p>

          <p>BTW I think your table is wrong. If you have 30B params that means bfloat16
          takes 2 for each. So it will be 60GB? There is no way it will fit in 24GB
          GPU Memory, or 32GM RAM. May be you are talking about 4bit or 8bit quantization?
          </p>

          <p>I am looking for someone to talk to on these, Are there any other venues?
          Is there any live community? </p>

          '
        raw: "I have a 64BM RAM and  RTX 4090 with 24GB GPU. Not sure what is going\
          \ on, (I am new to all this). Trying to load init the model itself (with\
          \ the given config) is causing overflow of my system RAM. It did not start\
          \ loading the weights yet. \n\nBTW I think your table is wrong. If you have\
          \ 30B params that means bfloat16 takes 2 for each. So it will be 60GB? There\
          \ is no way it will fit in 24GB GPU Memory, or 32GM RAM. May be you are\
          \ talking about 4bit or 8bit quantization? \n\nI am looking for someone\
          \ to talk to on these, Are there any other venues? Is there any live community? "
        updatedAt: '2023-06-17T08:32:19.550Z'
      numEdits: 0
      reactions: []
    id: 648d6f93dee03837c8344010
    type: comment
  author: akkinenirajesh
  content: "I have a 64BM RAM and  RTX 4090 with 24GB GPU. Not sure what is going\
    \ on, (I am new to all this). Trying to load init the model itself (with the given\
    \ config) is causing overflow of my system RAM. It did not start loading the weights\
    \ yet. \n\nBTW I think your table is wrong. If you have 30B params that means\
    \ bfloat16 takes 2 for each. So it will be 60GB? There is no way it will fit in\
    \ 24GB GPU Memory, or 32GM RAM. May be you are talking about 4bit or 8bit quantization?\
    \ \n\nI am looking for someone to talk to on these, Are there any other venues?\
    \ Is there any live community? "
  created_at: 2023-06-17 07:32:19+00:00
  edited: false
  hidden: false
  id: 648d6f93dee03837c8344010
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe62f9fefe5d2e80dc37839ae44ac2be.svg
      fullname: R M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Wildstar50
      type: user
    createdAt: '2023-06-17T15:32:58.000Z'
    data:
      edited: false
      editors:
      - Wildstar50
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8732324242591858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe62f9fefe5d2e80dc37839ae44ac2be.svg
          fullname: R M
          isHf: false
          isPro: false
          name: Wildstar50
          type: user
        html: '<p>Someone will correct me if I''m wrong, but if you look at the Files
          list pytorch_model.bin is 31GB.  This must be loaded into VRAM.  So even
          a 4090 can''t run this as-is.</p>

          <p>However, TheBloke quantizes models to 4-bit, which allow them to be loaded
          by commercial cards.  His version of this model is ~9GB.<br><a href="https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ">https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ</a></p>

          <p>You then load this model into the text-generation UI found here:<br><a
          rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p>

          <p>Aitrepeneur just did a review of TheBloke''s quantized version:<br><a
          rel="nofollow" href="https://youtu.be/XjsyHrmd3Xo">https://youtu.be/XjsyHrmd3Xo</a></p>

          '
        raw: 'Someone will correct me if I''m wrong, but if you look at the Files
          list pytorch_model.bin is 31GB.  This must be loaded into VRAM.  So even
          a 4090 can''t run this as-is.


          However, TheBloke quantizes models to 4-bit, which allow them to be loaded
          by commercial cards.  His version of this model is ~9GB.

          https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ


          You then load this model into the text-generation UI found here:

          https://github.com/oobabooga/text-generation-webui


          Aitrepeneur just did a review of TheBloke''s quantized version:

          https://youtu.be/XjsyHrmd3Xo'
        updatedAt: '2023-06-17T15:32:58.522Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - innate
    id: 648dd22a8a66ab9239a906b6
    type: comment
  author: Wildstar50
  content: 'Someone will correct me if I''m wrong, but if you look at the Files list
    pytorch_model.bin is 31GB.  This must be loaded into VRAM.  So even a 4090 can''t
    run this as-is.


    However, TheBloke quantizes models to 4-bit, which allow them to be loaded by
    commercial cards.  His version of this model is ~9GB.

    https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ


    You then load this model into the text-generation UI found here:

    https://github.com/oobabooga/text-generation-webui


    Aitrepeneur just did a review of TheBloke''s quantized version:

    https://youtu.be/XjsyHrmd3Xo'
  created_at: 2023-06-17 14:32:58+00:00
  edited: false
  hidden: false
  id: 648dd22a8a66ab9239a906b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
      fullname: nate
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: innate
      type: user
    createdAt: '2023-06-17T21:05:29.000Z'
    data:
      edited: false
      editors:
      - innate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9779101014137268
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
          fullname: nate
          isHf: false
          isPro: false
          name: innate
          type: user
        html: '<p>Thank you for your reply. The chart I posted does refer to 4bit
          quantization models, I apologize for the confusion as I myself did not know
          until you mentioned it and I still need a better understanding of this stuff
          overall. I''m going to try TheBloke''s 4bit model and see how it performs,
          from Aitrepeneur''s review it looks like it''ll be good for my purposes
          right now. Also going to have to read up on some papers that discuss or
          explain quantization. Here''s a good link if anyone else wants it - <a rel="nofollow"
          href="https://rentry.org/LocalModelsPapers">https://rentry.org/LocalModelsPapers</a>.</p>

          <p>And I too overflowed my system trying to run this model as is lol - I
          watched as my screen color fade from full VRAM uage and 97% of my RAM used
          before the system crashed.</p>

          '
        raw: 'Thank you for your reply. The chart I posted does refer to 4bit quantization
          models, I apologize for the confusion as I myself did not know until you
          mentioned it and I still need a better understanding of this stuff overall.
          I''m going to try TheBloke''s 4bit model and see how it performs, from Aitrepeneur''s
          review it looks like it''ll be good for my purposes right now. Also going
          to have to read up on some papers that discuss or explain quantization.
          Here''s a good link if anyone else wants it - https://rentry.org/LocalModelsPapers.


          And I too overflowed my system trying to run this model as is lol - I watched
          as my screen color fade from full VRAM uage and 97% of my RAM used before
          the system crashed.'
        updatedAt: '2023-06-17T21:05:29.534Z'
      numEdits: 0
      reactions: []
    id: 648e201941aa8acdac83fe5e
    type: comment
  author: innate
  content: 'Thank you for your reply. The chart I posted does refer to 4bit quantization
    models, I apologize for the confusion as I myself did not know until you mentioned
    it and I still need a better understanding of this stuff overall. I''m going to
    try TheBloke''s 4bit model and see how it performs, from Aitrepeneur''s review
    it looks like it''ll be good for my purposes right now. Also going to have to
    read up on some papers that discuss or explain quantization. Here''s a good link
    if anyone else wants it - https://rentry.org/LocalModelsPapers.


    And I too overflowed my system trying to run this model as is lol - I watched
    as my screen color fade from full VRAM uage and 97% of my RAM used before the
    system crashed.'
  created_at: 2023-06-17 20:05:29+00:00
  edited: false
  hidden: false
  id: 648e201941aa8acdac83fe5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe62f9fefe5d2e80dc37839ae44ac2be.svg
      fullname: R M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Wildstar50
      type: user
    createdAt: '2023-06-17T21:09:08.000Z'
    data:
      edited: false
      editors:
      - Wildstar50
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9868353605270386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe62f9fefe5d2e80dc37839ae44ac2be.svg
          fullname: R M
          isHf: false
          isPro: false
          name: Wildstar50
          type: user
        html: '<p>Innate, you said you had 8GB of VRAM.  TheBloke''s version is 9GB.  So,
          yes, unfortunately it won''t fit. :)</p>

          '
        raw: Innate, you said you had 8GB of VRAM.  TheBloke's version is 9GB.  So,
          yes, unfortunately it won't fit. :)
        updatedAt: '2023-06-17T21:09:08.341Z'
      numEdits: 0
      reactions: []
    id: 648e20f49237602e0d52e37b
    type: comment
  author: Wildstar50
  content: Innate, you said you had 8GB of VRAM.  TheBloke's version is 9GB.  So,
    yes, unfortunately it won't fit. :)
  created_at: 2023-06-17 20:09:08+00:00
  edited: false
  hidden: false
  id: 648e20f49237602e0d52e37b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
      fullname: nate
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: innate
      type: user
    createdAt: '2023-06-17T22:09:09.000Z'
    data:
      edited: false
      editors:
      - innate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8167502880096436
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
          fullname: nate
          isHf: false
          isPro: false
          name: innate
          type: user
        html: "<p>I was able to load TheBloke's version by sharing the memory over\
          \ my 2 GPUs using the text-generation-webui, since I have 2 8GB 3070's in\
          \ my PC.<br>\u2022\u1D17\u2022<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/QC0C_65K9H_-RnIgPl8xF.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/QC0C_65K9H_-RnIgPl8xF.png\"\
          ></a></p>\n"
        raw: "I was able to load TheBloke's version by sharing the memory over my\
          \ 2 GPUs using the text-generation-webui, since I have 2 8GB 3070's in my\
          \ PC.\n\u2022\u1D17\u2022\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/QC0C_65K9H_-RnIgPl8xF.png)\n"
        updatedAt: '2023-06-17T22:09:09.686Z'
      numEdits: 0
      reactions: []
    id: 648e2f05b7dab2d0ac52c45d
    type: comment
  author: innate
  content: "I was able to load TheBloke's version by sharing the memory over my 2\
    \ GPUs using the text-generation-webui, since I have 2 8GB 3070's in my PC.\n\u2022\
    \u1D17\u2022\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/636fe99c9633c0fb6ff01a0b/QC0C_65K9H_-RnIgPl8xF.png)\n"
  created_at: 2023-06-17 21:09:09+00:00
  edited: false
  hidden: false
  id: 648e2f05b7dab2d0ac52c45d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/b5X7LnHkwvtKUnMweIazj.png?w=200&h=200&f=face
      fullname: valdanito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: valdanito
      type: user
    createdAt: '2023-06-19T03:50:04.000Z'
    data:
      edited: false
      editors:
      - valdanito
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5928588509559631
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/b5X7LnHkwvtKUnMweIazj.png?w=200&h=200&f=face
          fullname: valdanito
          isHf: false
          isPro: false
          name: valdanito
          type: user
        html: "<p>My 80G VRAM is full, but I still can't get a response<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641d4342043963b1c0a58cd3/Cltn7TDmvnkWL_vM3iCEY.png\"\
          ><img alt=\"vram.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641d4342043963b1c0a58cd3/Cltn7TDmvnkWL_vM3iCEY.png\"\
          ></a></p>\n<p>My code:</p>\n<pre><code>model_path = \"/data/cache_models/offline_cache/WizardCoder-15B-V1.0\"\
          \ndef init():\n    global tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ revision=\"main\", local_files_only=True)\n    global llm_model\n    llm_model\
          \ = AutoModelForCausalLM.from_pretrained(\n        model_path,\n       \
          \ revision=\"main\",\n        device_map=\"sequential\",\n        torch_dtype=torch.bfloat16,\n\
          \    )\n</code></pre>\n"
        raw: "My 80G VRAM is full, but I still can't get a response\n![vram.png](https://cdn-uploads.huggingface.co/production/uploads/641d4342043963b1c0a58cd3/Cltn7TDmvnkWL_vM3iCEY.png)\n\
          \nMy code:\n```\nmodel_path = \"/data/cache_models/offline_cache/WizardCoder-15B-V1.0\"\
          \ndef init():\n    global tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ revision=\"main\", local_files_only=True)\n    global llm_model\n    llm_model\
          \ = AutoModelForCausalLM.from_pretrained(\n        model_path,\n       \
          \ revision=\"main\",\n        device_map=\"sequential\",\n        torch_dtype=torch.bfloat16,\n\
          \    )\n```"
        updatedAt: '2023-06-19T03:50:04.104Z'
      numEdits: 0
      reactions: []
    id: 648fd06c0e933a59d0b00710
    type: comment
  author: valdanito
  content: "My 80G VRAM is full, but I still can't get a response\n![vram.png](https://cdn-uploads.huggingface.co/production/uploads/641d4342043963b1c0a58cd3/Cltn7TDmvnkWL_vM3iCEY.png)\n\
    \nMy code:\n```\nmodel_path = \"/data/cache_models/offline_cache/WizardCoder-15B-V1.0\"\
    \ndef init():\n    global tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ revision=\"main\", local_files_only=True)\n    global llm_model\n    llm_model\
    \ = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        revision=\"\
    main\",\n        device_map=\"sequential\",\n        torch_dtype=torch.bfloat16,\n\
    \    )\n```"
  created_at: 2023-06-19 02:50:04+00:00
  edited: false
  hidden: false
  id: 648fd06c0e933a59d0b00710
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd09710a75fdaf376d134f234ad1ed12.svg
      fullname: Rajesh Akkineni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkinenirajesh
      type: user
    createdAt: '2023-06-19T05:49:33.000Z'
    data:
      edited: false
      editors:
      - akkinenirajesh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9385605454444885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd09710a75fdaf376d134f234ad1ed12.svg
          fullname: Rajesh Akkineni
          isHf: false
          isPro: false
          name: akkinenirajesh
          type: user
        html: '<p>Do any one really understand what is going on there?<br>I have 100''s
          of questions, and no idea whom to ask..</p>

          <p>Why does it take more than 30GB (15x2)?<br>What are the exact computation
          requirements to generate one token?<br>What are the computation requirements
          to fine-tune this model? </p>

          <p>I am trying to understand all these in detail by writing my own program
          to run any Hugging Face model(Without using any of those python modules
          provided). May be then I will understand. </p>

          '
        raw: "Do any one really understand what is going on there? \nI have 100's\
          \ of questions, and no idea whom to ask..\n\nWhy does it take more than\
          \ 30GB (15x2)?\nWhat are the exact computation requirements to generate\
          \ one token? \nWhat are the computation requirements to fine-tune this model?\
          \ \n\nI am trying to understand all these in detail by writing my own program\
          \ to run any Hugging Face model(Without using any of those python modules\
          \ provided). May be then I will understand. "
        updatedAt: '2023-06-19T05:49:33.655Z'
      numEdits: 0
      reactions: []
    id: 648fec6da3f3b05681c46b57
    type: comment
  author: akkinenirajesh
  content: "Do any one really understand what is going on there? \nI have 100's of\
    \ questions, and no idea whom to ask..\n\nWhy does it take more than 30GB (15x2)?\n\
    What are the exact computation requirements to generate one token? \nWhat are\
    \ the computation requirements to fine-tune this model? \n\nI am trying to understand\
    \ all these in detail by writing my own program to run any Hugging Face model(Without\
    \ using any of those python modules provided). May be then I will understand. "
  created_at: 2023-06-19 04:49:33+00:00
  edited: false
  hidden: false
  id: 648fec6da3f3b05681c46b57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
      fullname: nate
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: innate
      type: user
    createdAt: '2023-06-20T03:35:50.000Z'
    data:
      edited: false
      editors:
      - innate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9272779822349548
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
          fullname: nate
          isHf: false
          isPro: false
          name: innate
          type: user
        html: '<p>I do not know the requirements to run this specific model by WizardLM.
          If it wasn''t clear in my above post, I am using TheBloke''s 4bit quantization
          version to reduce the VRAM required ---&gt; <a href="https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ">https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ</a><br>Model
          quantization is a method of reducing the size of a trained model while maintaining
          accuracy. It works by reducing the precision of the weights and activations
          used by a model without affecting (significantly) the overall accuracy.<br>That''s
          not to say the quantized model ran perfect out of the box either, I had
          to read all of the text-generation-webui documentation to get it to load
          &amp; work properly. I also read a few OpenAI &amp; Meta papers to gain
          a better understanding of what goes on "under the hood", I''ll post the
          few I''ve read down below. Not trying to tell anyone what to do but: learning
          is a journey, frustration is natural, discipline is what will put you over
          the finish line guide you towards better understanding. &lt;3<br><a rel="nofollow"
          href="https://arxiv.org/abs/2304.12210">https://arxiv.org/abs/2304.12210</a>
          - A Cookbook of Self-Supervised Learning<br><a rel="nofollow" href="https://arxiv.org/abs/2205.01068">https://arxiv.org/abs/2205.01068</a>
          - OPT: Open Pre-trained Transformer Language Models<br><a rel="nofollow"
          href="https://arxiv.org/abs/2305.20050">https://arxiv.org/abs/2305.20050</a>
          - Let''s Verify Step by Step<br>If stuff in the papers are going over your
          head, don''t worry. Google is your friend.</p>

          '
        raw: 'I do not know the requirements to run this specific model by WizardLM.
          If it wasn''t clear in my above post, I am using TheBloke''s 4bit quantization
          version to reduce the VRAM required ---> https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ

          Model quantization is a method of reducing the size of a trained model while
          maintaining accuracy. It works by reducing the precision of the weights
          and activations used by a model without affecting (significantly) the overall
          accuracy.

          That''s not to say the quantized model ran perfect out of the box either,
          I had to read all of the text-generation-webui documentation to get it to
          load & work properly. I also read a few OpenAI & Meta papers to gain a better
          understanding of what goes on "under the hood", I''ll post the few I''ve
          read down below. Not trying to tell anyone what to do but: learning is a
          journey, frustration is natural, discipline is what will put you over the
          finish line guide you towards better understanding. <3

          https://arxiv.org/abs/2304.12210 - A Cookbook of Self-Supervised Learning

          https://arxiv.org/abs/2205.01068 - OPT: Open Pre-trained Transformer Language
          Models

          https://arxiv.org/abs/2305.20050 - Let''s Verify Step by Step

          If stuff in the papers are going over your head, don''t worry. Google is
          your friend.'
        updatedAt: '2023-06-20T03:35:50.747Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AroOmega
      relatedEventId: 64911e965e0c930da17ef6f4
    id: 64911e965e0c930da17ef6f3
    type: comment
  author: innate
  content: 'I do not know the requirements to run this specific model by WizardLM.
    If it wasn''t clear in my above post, I am using TheBloke''s 4bit quantization
    version to reduce the VRAM required ---> https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ

    Model quantization is a method of reducing the size of a trained model while maintaining
    accuracy. It works by reducing the precision of the weights and activations used
    by a model without affecting (significantly) the overall accuracy.

    That''s not to say the quantized model ran perfect out of the box either, I had
    to read all of the text-generation-webui documentation to get it to load & work
    properly. I also read a few OpenAI & Meta papers to gain a better understanding
    of what goes on "under the hood", I''ll post the few I''ve read down below. Not
    trying to tell anyone what to do but: learning is a journey, frustration is natural,
    discipline is what will put you over the finish line guide you towards better
    understanding. <3

    https://arxiv.org/abs/2304.12210 - A Cookbook of Self-Supervised Learning

    https://arxiv.org/abs/2205.01068 - OPT: Open Pre-trained Transformer Language
    Models

    https://arxiv.org/abs/2305.20050 - Let''s Verify Step by Step

    If stuff in the papers are going over your head, don''t worry. Google is your
    friend.'
  created_at: 2023-06-20 02:35:50+00:00
  edited: false
  hidden: false
  id: 64911e965e0c930da17ef6f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668278670023-noauth.png?w=200&h=200&f=face
      fullname: nate
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: innate
      type: user
    createdAt: '2023-06-20T03:35:50.000Z'
    data:
      status: closed
    id: 64911e965e0c930da17ef6f4
    type: status-change
  author: innate
  created_at: 2023-06-20 02:35:50+00:00
  id: 64911e965e0c930da17ef6f4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: WizardLM/WizardCoder-15B-V1.0
repo_type: model
status: closed
target_branch: null
title: VRAM Requirements
