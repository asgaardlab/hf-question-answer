!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RedXeol
conflicting_files: null
created_at: 2023-05-09 20:16:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-09T21:16:30.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>I have a pc with an rtx 3060 (12GB), i7 10700 and 32GB ram running
          a model of "gptj 6b GPTQ-4bit-128g" in text-generation-webui.</p>

          <p>This model reaches a maximum of 8GB in memory, and even though the speed
          is very good, I would like to know if the number of tokens per second can
          be further accelerated. so that the waiting time when using the api in python
          or tavernIA is faster in seconds. (since they both send more than one request
          to the text-generation-webui api before delivering a single chat response.)
          do you know of any settings that can speed up this process?</p>

          '
        raw: "I have a pc with an rtx 3060 (12GB), i7 10700 and 32GB ram running a\
          \ model of \"gptj 6b GPTQ-4bit-128g\" in text-generation-webui.\r\n\r\n\
          This model reaches a maximum of 8GB in memory, and even though the speed\
          \ is very good, I would like to know if the number of tokens per second\
          \ can be further accelerated. so that the waiting time when using the api\
          \ in python or tavernIA is faster in seconds. (since they both send more\
          \ than one request to the text-generation-webui api before delivering a\
          \ single chat response.) do you know of any settings that can speed up this\
          \ process?"
        updatedAt: '2023-05-09T21:16:30.783Z'
      numEdits: 0
      reactions: []
    id: 645ab82ec266796265bb134b
    type: comment
  author: RedXeol
  content: "I have a pc with an rtx 3060 (12GB), i7 10700 and 32GB ram running a model\
    \ of \"gptj 6b GPTQ-4bit-128g\" in text-generation-webui.\r\n\r\nThis model reaches\
    \ a maximum of 8GB in memory, and even though the speed is very good, I would\
    \ like to know if the number of tokens per second can be further accelerated.\
    \ so that the waiting time when using the api in python or tavernIA is faster\
    \ in seconds. (since they both send more than one request to the text-generation-webui\
    \ api before delivering a single chat response.) do you know of any settings that\
    \ can speed up this process?"
  created_at: 2023-05-09 20:16:30+00:00
  edited: false
  hidden: false
  id: 645ab82ec266796265bb134b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T10:00:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>In text-gen-ui you can try the <code>xformers</code> parameter,
          apparently that can improve performance a bit</p>

          <p>I recently learned that single prompt performance is bottlenecked on
          CPU for most users.  If you run a prompt and look at your GPU usage %, you
          will likely see it is &lt;100%, maybe a lot lower like 50% or 25%.  This
          is because Python is using 100% of one core, and that is the limit to performance.</p>

          <p>If you''re using the model from Python code and want to send many prompts,
          then for maximum performance you shouldn''t use text-gen-ui and instead
          write your own code using Hugging Face <code>pipeline</code>(<a href="https://huggingface.co/docs/transformers/main_classes/pipelines">https://huggingface.co/docs/transformers/main_classes/pipelines</a>)
          with batched data.  You can get massive speed improvements by batching data
          with pipelines, because then CPU is no longer the bottleneck and it can
          use the full GPU power.</p>

          <p>I am working on Python code that enables easily loading GPTQ models and
          running inference using AutoGPTQ. It will be ready to release in the next
          24 hours.  I''ll link the code to you when it''s ready and you can try using
          it as a base for your task. I think you will get better performance with
          that.</p>

          '
        raw: 'In text-gen-ui you can try the `xformers` parameter, apparently that
          can improve performance a bit


          I recently learned that single prompt performance is bottlenecked on CPU
          for most users.  If you run a prompt and look at your GPU usage %, you will
          likely see it is <100%, maybe a lot lower like 50% or 25%.  This is because
          Python is using 100% of one core, and that is the limit to performance.


          If you''re using the model from Python code and want to send many prompts,
          then for maximum performance you shouldn''t use text-gen-ui and instead
          write your own code using Hugging Face `pipeline`(https://huggingface.co/docs/transformers/main_classes/pipelines)
          with batched data.  You can get massive speed improvements by batching data
          with pipelines, because then CPU is no longer the bottleneck and it can
          use the full GPU power.


          I am working on Python code that enables easily loading GPTQ models and
          running inference using AutoGPTQ. It will be ready to release in the next
          24 hours.  I''ll link the code to you when it''s ready and you can try using
          it as a base for your task. I think you will get better performance with
          that.'
        updatedAt: '2023-05-10T10:00:06.787Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - RedXeol
    id: 645b6b26337b2ccf07f7cb18
    type: comment
  author: TheBloke
  content: 'In text-gen-ui you can try the `xformers` parameter, apparently that can
    improve performance a bit


    I recently learned that single prompt performance is bottlenecked on CPU for most
    users.  If you run a prompt and look at your GPU usage %, you will likely see
    it is <100%, maybe a lot lower like 50% or 25%.  This is because Python is using
    100% of one core, and that is the limit to performance.


    If you''re using the model from Python code and want to send many prompts, then
    for maximum performance you shouldn''t use text-gen-ui and instead write your
    own code using Hugging Face `pipeline`(https://huggingface.co/docs/transformers/main_classes/pipelines)
    with batched data.  You can get massive speed improvements by batching data with
    pipelines, because then CPU is no longer the bottleneck and it can use the full
    GPU power.


    I am working on Python code that enables easily loading GPTQ models and running
    inference using AutoGPTQ. It will be ready to release in the next 24 hours.  I''ll
    link the code to you when it''s ready and you can try using it as a base for your
    task. I think you will get better performance with that.'
  created_at: 2023-05-10 09:00:06+00:00
  edited: false
  hidden: false
  id: 645b6b26337b2ccf07f7cb18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-10T12:52:12.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>god you are amazing please let me know when you have the codes.  I''m
          too excited!!  thank you so much.</p>

          '
        raw: god you are amazing please let me know when you have the codes.  I'm
          too excited!!  thank you so much.
        updatedAt: '2023-05-10T12:52:12.482Z'
      numEdits: 0
      reactions: []
    id: 645b937c2c76efd4c66512da
    type: comment
  author: RedXeol
  content: god you are amazing please let me know when you have the codes.  I'm too
    excited!!  thank you so much.
  created_at: 2023-05-10 11:52:12+00:00
  edited: false
  hidden: false
  id: 645b937c2c76efd4c66512da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-13T15:41:22.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>Hello, sorry for the inconvenience, I am very interested in the
          Python code that you are working on, I think that you know too much and
          I am not at that level. don''t forget to tell me if there is any new update.
          thank you so much.</p>

          '
        raw: Hello, sorry for the inconvenience, I am very interested in the Python
          code that you are working on, I think that you know too much and I am not
          at that level. don't forget to tell me if there is any new update. thank
          you so much.
        updatedAt: '2023-05-13T15:41:22.430Z'
      numEdits: 0
      reactions: []
    id: 645fafa2446a4fa46957de57
    type: comment
  author: RedXeol
  content: Hello, sorry for the inconvenience, I am very interested in the Python
    code that you are working on, I think that you know too much and I am not at that
    level. don't forget to tell me if there is any new update. thank you so much.
  created_at: 2023-05-13 14:41:22+00:00
  edited: false
  hidden: false
  id: 645fafa2446a4fa46957de57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T23:40:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry for the delay. I still haven''t released the code because
          I''ve not had time to clean it up.</p>

          <p>But what I said about batched data for pipeline may not be true.. or
          at least, it has problems.  I tested it again yesterday and got corrupt
          data back.  It seems that pipeline with batches works fine if the prompts
          are all the same or quite similar. But when using a batch of very different
          prompts, something strange happens.  Some prompts return OK, others come
          back with gibberish.  And I''ve just not had time to investigate why that
          is, yet.  I don''t know if it''s a bug or something I''m doing wrong, or
          just an expected result.</p>

          <p>Ping me again tomorrow and I''ll send you the current code</p>

          '
        raw: 'Sorry for the delay. I still haven''t released the code because I''ve
          not had time to clean it up.


          But what I said about batched data for pipeline may not be true.. or at
          least, it has problems.  I tested it again yesterday and got corrupt data
          back.  It seems that pipeline with batches works fine if the prompts are
          all the same or quite similar. But when using a batch of very different
          prompts, something strange happens.  Some prompts return OK, others come
          back with gibberish.  And I''ve just not had time to investigate why that
          is, yet.  I don''t know if it''s a bug or something I''m doing wrong, or
          just an expected result.


          Ping me again tomorrow and I''ll send you the current code'
        updatedAt: '2023-05-13T23:40:54.445Z'
      numEdits: 0
      reactions: []
    id: 6460200625a4075bcf9ed517
    type: comment
  author: TheBloke
  content: 'Sorry for the delay. I still haven''t released the code because I''ve
    not had time to clean it up.


    But what I said about batched data for pipeline may not be true.. or at least,
    it has problems.  I tested it again yesterday and got corrupt data back.  It seems
    that pipeline with batches works fine if the prompts are all the same or quite
    similar. But when using a batch of very different prompts, something strange happens.  Some
    prompts return OK, others come back with gibberish.  And I''ve just not had time
    to investigate why that is, yet.  I don''t know if it''s a bug or something I''m
    doing wrong, or just an expected result.


    Ping me again tomorrow and I''ll send you the current code'
  created_at: 2023-05-13 22:40:54+00:00
  edited: false
  hidden: false
  id: 6460200625a4075bcf9ed517
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-16T20:22:10.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>Will\
          \ you be posting your code to Github, or some other place accessible to\
          \ everyone?</p>\n"
        raw: "@TheBloke \n\nWill you be posting your code to Github, or some other\
          \ place accessible to everyone?"
        updatedAt: '2023-05-16T20:22:10.958Z'
      numEdits: 0
      reactions: []
    id: 6463e5f206c6a952bd03bf96
    type: comment
  author: mancub
  content: "@TheBloke \n\nWill you be posting your code to Github, or some other place\
    \ accessible to everyone?"
  created_at: 2023-05-16 19:22:10+00:00
  edited: false
  hidden: false
  id: 6463e5f206c6a952bd03bf96
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/h2ogpt-oasst1-512-30B-GGML
repo_type: model
status: open
target_branch: null
title: I have a question with the tokens
