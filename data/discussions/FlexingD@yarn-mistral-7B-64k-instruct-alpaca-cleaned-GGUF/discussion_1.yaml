!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hoioi
conflicting_files: null
created_at: 2024-01-01 12:14:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2024-01-01T12:14:05.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6714905500411987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>What''s the prompt format for using this model? </p>

          '
        raw: 'What''s the prompt format for using this model? '
        updatedAt: '2024-01-01T12:14:05.844Z'
      numEdits: 0
      reactions: []
    id: 6592ac8df5a209eeac6cc371
    type: comment
  author: Hoioi
  content: 'What''s the prompt format for using this model? '
  created_at: 2024-01-01 12:14:05+00:00
  edited: false
  hidden: false
  id: 6592ac8df5a209eeac6cc371
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uyf7eFLM_--gV6hKTPY_z.png?w=200&h=200&f=face
      fullname: FlexingD
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: FlexingD
      type: user
    createdAt: '2024-01-01T12:30:28.000Z'
    data:
      edited: false
      editors:
      - FlexingD
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5198357105255127
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uyf7eFLM_--gV6hKTPY_z.png?w=200&h=200&f=face
          fullname: FlexingD
          isHf: false
          isPro: false
          name: FlexingD
          type: user
        html: '<p>def make_inference(instruction, context = None):<br>  if context:<br>    prompt
          = f"Below is an instruction that describes a task, paired with an input
          that provides further context.\n\n### Instruction: \n{instruction}\n\n###
          Input: \n{context}\n\n### Response: \n"<br>  else:<br>    prompt = f"Below
          is an instruction that describes a task. Write a response that appropriately
          completes the request.\n\n### Instruction: \n{instruction}\n\n### Response:
          \n"<br>  inputs = tokenizer(prompt, return_tensors="pt", return_token_type_ids=False).to("cuda:0")<br>  outputs
          = quant_model.generate(**inputs, max_new_tokens=500)<br>  display(Markdown((tokenizer.decode(outputs[0],
          skip_special_tokens=True))))</p>

          <p>make_inference("Summarize the context.", "Context... ...")</p>

          '
        raw: "def make_inference(instruction, context = None):\n  if context:\n  \
          \  prompt = f\"Below is an instruction that describes a task, paired with\
          \ an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\\
          n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n  else:\n    prompt\
          \ = f\"Below is an instruction that describes a task. Write a response that\
          \ appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\\
          n\\n### Response: \\n\"\n  inputs = tokenizer(prompt, return_tensors=\"\
          pt\", return_token_type_ids=False).to(\"cuda:0\")\n  outputs = quant_model.generate(**inputs,\
          \ max_new_tokens=500)\n  display(Markdown((tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))))\n\nmake_inference(\"Summarize the context.\"\
          , \"Context... ...\")"
        updatedAt: '2024-01-01T12:30:28.210Z'
      numEdits: 0
      reactions: []
    id: 6592b064a2607099282d83e3
    type: comment
  author: FlexingD
  content: "def make_inference(instruction, context = None):\n  if context:\n    prompt\
    \ = f\"Below is an instruction that describes a task, paired with an input that\
    \ provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input:\
    \ \\n{context}\\n\\n### Response: \\n\"\n  else:\n    prompt = f\"Below is an\
    \ instruction that describes a task. Write a response that appropriately completes\
    \ the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\
    \n  inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"\
    cuda:0\")\n  outputs = quant_model.generate(**inputs, max_new_tokens=500)\n  display(Markdown((tokenizer.decode(outputs[0],\
    \ skip_special_tokens=True))))\n\nmake_inference(\"Summarize the context.\", \"\
    Context... ...\")"
  created_at: 2024-01-01 12:30:28+00:00
  edited: false
  hidden: false
  id: 6592b064a2607099282d83e3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: FlexingD/yarn-mistral-7B-64k-instruct-alpaca-cleaned-GGUF
repo_type: model
status: open
target_branch: null
title: Prompt format?
