!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vt404v2
conflicting_files: null
created_at: 2023-06-15 15:49:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64615a13cf638aa8f8554a57/0-B4Bjh7RhA1Kr9Hwq2RX.jpeg?w=200&h=200&f=face
      fullname: VT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vt404v2
      type: user
    createdAt: '2023-06-15T16:49:22.000Z'
    data:
      edited: false
      editors:
      - vt404v2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7706727981567383
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64615a13cf638aa8f8554a57/0-B4Bjh7RhA1Kr9Hwq2RX.jpeg?w=200&h=200&f=face
          fullname: VT
          isHf: false
          isPro: false
          name: vt404v2
          type: user
        html: '<p>Chat with h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2 here <a rel="nofollow"
          href="https://gpt-gm.h2o.ai/">https://gpt-gm.h2o.ai/</a> looks very fast.
          Can you please tell me what GPU you are using for inference? I get about
          6.5 tokens/s with 500 tokens prompt and 32 new tokens on A100 80Gb.</p>

          '
        raw: Chat with h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2 here https://gpt-gm.h2o.ai/
          looks very fast. Can you please tell me what GPU you are using for inference?
          I get about 6.5 tokens/s with 500 tokens prompt and 32 new tokens on A100
          80Gb.
        updatedAt: '2023-06-15T16:49:22.026Z'
      numEdits: 0
      reactions: []
    id: 648b411222147f735472aa4c
    type: comment
  author: vt404v2
  content: Chat with h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2 here https://gpt-gm.h2o.ai/
    looks very fast. Can you please tell me what GPU you are using for inference?
    I get about 6.5 tokens/s with 500 tokens prompt and 32 new tokens on A100 80Gb.
  created_at: 2023-06-15 15:49:22+00:00
  edited: false
  hidden: false
  id: 648b411222147f735472aa4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
      fullname: Pascal Pfeiffer
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ilu000
      type: user
    createdAt: '2023-06-15T16:55:24.000Z'
    data:
      edited: false
      editors:
      - ilu000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8706979751586914
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
          fullname: Pascal Pfeiffer
          isHf: false
          isPro: false
          name: ilu000
          type: user
        html: '<p>We are hosting the model on a A100 80GB using the awesome inference
          repository from Hugging Face <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a>.<br>Actually,
          the GPU is even shared with the other 7B model.</p>

          '
        raw: 'We are hosting the model on a A100 80GB using the awesome inference
          repository from Hugging Face https://github.com/huggingface/text-generation-inference.

          Actually, the GPU is even shared with the other 7B model.'
        updatedAt: '2023-06-15T16:55:24.202Z'
      numEdits: 0
      reactions: []
    id: 648b427cdf4710674c5c6b02
    type: comment
  author: ilu000
  content: 'We are hosting the model on a A100 80GB using the awesome inference repository
    from Hugging Face https://github.com/huggingface/text-generation-inference.

    Actually, the GPU is even shared with the other 7B model.'
  created_at: 2023-06-15 15:55:24+00:00
  edited: false
  hidden: false
  id: 648b427cdf4710674c5c6b02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64615a13cf638aa8f8554a57/0-B4Bjh7RhA1Kr9Hwq2RX.jpeg?w=200&h=200&f=face
      fullname: VT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vt404v2
      type: user
    createdAt: '2023-06-15T18:21:01.000Z'
    data:
      edited: false
      editors:
      - vt404v2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9407398104667664
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64615a13cf638aa8f8554a57/0-B4Bjh7RhA1Kr9Hwq2RX.jpeg?w=200&h=200&f=face
          fullname: VT
          isHf: false
          isPro: false
          name: vt404v2
          type: user
        html: '<p>Thanks, it works for me</p>

          '
        raw: Thanks, it works for me
        updatedAt: '2023-06-15T18:21:01.665Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - ilu000
      relatedEventId: 648b568d31c8c03f5c73ab99
    id: 648b568d31c8c03f5c73ab96
    type: comment
  author: vt404v2
  content: Thanks, it works for me
  created_at: 2023-06-15 17:21:01+00:00
  edited: false
  hidden: false
  id: 648b568d31c8c03f5c73ab96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64615a13cf638aa8f8554a57/0-B4Bjh7RhA1Kr9Hwq2RX.jpeg?w=200&h=200&f=face
      fullname: VT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vt404v2
      type: user
    createdAt: '2023-06-15T18:21:01.000Z'
    data:
      status: closed
    id: 648b568d31c8c03f5c73ab99
    type: status-change
  author: vt404v2
  created_at: 2023-06-15 17:21:01+00:00
  id: 648b568d31c8c03f5c73ab99
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2
repo_type: model
status: closed
target_branch: null
title: GPU for inference
