!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cmp-nct
conflicting_files: null
created_at: 2023-07-13 21:35:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-13T22:35:42.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9678763151168823
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>I noticed that &lt;|prompt|&gt; is not added as a special token,
          was that an error in training or should it be like this? </p>

          <p>What are the advantages of this model in comparison to the official OpenAssistant
          7B Falcon variant ?<br>I''m not familiar with the differences between the
          datasets, this one appears to use one of the early variants ?</p>

          '
        raw: "I noticed that <|prompt|> is not added as a special token, was that\
          \ an error in training or should it be like this? \r\n\r\nWhat are the advantages\
          \ of this model in comparison to the official OpenAssistant 7B Falcon variant\
          \ ?\r\nI'm not familiar with the differences between the datasets, this\
          \ one appears to use one of the early variants ?"
        updatedAt: '2023-07-13T22:35:42.766Z'
      numEdits: 0
      reactions: []
    id: 64b07c3e14b3c5479ee26dc4
    type: comment
  author: cmp-nct
  content: "I noticed that <|prompt|> is not added as a special token, was that an\
    \ error in training or should it be like this? \r\n\r\nWhat are the advantages\
    \ of this model in comparison to the official OpenAssistant 7B Falcon variant\
    \ ?\r\nI'm not familiar with the differences between the datasets, this one appears\
    \ to use one of the early variants ?"
  created_at: 2023-07-13 21:35:42+00:00
  edited: false
  hidden: false
  id: 64b07c3e14b3c5479ee26dc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
      fullname: Pascal Pfeiffer
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ilu000
      type: user
    createdAt: '2023-07-17T14:46:09.000Z'
    data:
      edited: false
      editors:
      - ilu000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8764665126800537
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
          fullname: Pascal Pfeiffer
          isHf: false
          isPro: false
          name: ilu000
          type: user
        html: '<p>Yes, these are not trained as additional tokens, but rather as text
          that get''s tokenized into multiple tokens.<br>With a larger training corpus,
          it may be better to actually train new tokens and we do support this option
          in <a rel="nofollow" href="https://github.com/h2oai/h2o-llmstudio">H2O LLM
          Studio</a>. For the OASST dataset, just using words was usually superior.
          </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6316fc44c92fd6fee3161e9a/_UlM-rwFftp2W_dnB7U1r.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6316fc44c92fd6fee3161e9a/_UlM-rwFftp2W_dnB7U1r.png"></a></p>

          '
        raw: "Yes, these are not trained as additional tokens, but rather as text\
          \ that get's tokenized into multiple tokens. \nWith a larger training corpus,\
          \ it may be better to actually train new tokens and we do support this option\
          \ in [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio). For the OASST\
          \ dataset, just using words was usually superior. \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6316fc44c92fd6fee3161e9a/_UlM-rwFftp2W_dnB7U1r.png)\n"
        updatedAt: '2023-07-17T14:46:09.349Z'
      numEdits: 0
      reactions: []
    id: 64b5543109a62ee7b08371ed
    type: comment
  author: ilu000
  content: "Yes, these are not trained as additional tokens, but rather as text that\
    \ get's tokenized into multiple tokens. \nWith a larger training corpus, it may\
    \ be better to actually train new tokens and we do support this option in [H2O\
    \ LLM Studio](https://github.com/h2oai/h2o-llmstudio). For the OASST dataset,\
    \ just using words was usually superior. \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6316fc44c92fd6fee3161e9a/_UlM-rwFftp2W_dnB7U1r.png)\n"
  created_at: 2023-07-17 13:46:09+00:00
  edited: false
  hidden: false
  id: 64b5543109a62ee7b08371ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-19T16:34:37.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8365857601165771
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>The way regex-bpe works creates a ton of tokens out of the "special
          token" if it''s not pre-tokenized:<br>    39 -&gt; ''&lt;''<br>   103 -&gt;
          ''|''<br> 18269 -&gt; ''prom''<br>   444 -&gt; ''pt''<br> 54146 -&gt; ''|&gt;''<br>    11
          -&gt; ''&lt;|endoftext|&gt;''<br>    39 -&gt; ''&lt;''<br>   103 -&gt; ''|''<br>
          46617 -&gt; ''answer''<br> 54146 -&gt; ''|&gt;''</p>

          <p>Using "&lt;|XXX|&gt;" could be harmful to the model, it has never seen
          such tokens following each other in any of the regular training.<br>It makes
          sense to use that syntax when creating a special token because it''s the
          style TII chose but when not tokenizing them I''d recommend using a normal
          word instead.</p>

          '
        raw: "The way regex-bpe works creates a ton of tokens out of the \"special\
          \ token\" if it's not pre-tokenized:\n    39 -> '<'\n   103 -> '|'\n 18269\
          \ -> 'prom'\n   444 -> 'pt'\n 54146 -> '|>'\n    11 -> '<|endoftext|>'\n\
          \    39 -> '<'\n   103 -> '|'\n 46617 -> 'answer'\n 54146 -> '|>'\n\nUsing\
          \ \"<|XXX|>\" could be harmful to the model, it has never seen such tokens\
          \ following each other in any of the regular training.\nIt makes sense to\
          \ use that syntax when creating a special token because it's the style TII\
          \ chose but when not tokenizing them I'd recommend using a normal word instead."
        updatedAt: '2023-07-19T16:34:37.410Z'
      numEdits: 0
      reactions: []
    id: 64b8109d0de7289de3593e4b
    type: comment
  author: cmp-nct
  content: "The way regex-bpe works creates a ton of tokens out of the \"special token\"\
    \ if it's not pre-tokenized:\n    39 -> '<'\n   103 -> '|'\n 18269 -> 'prom'\n\
    \   444 -> 'pt'\n 54146 -> '|>'\n    11 -> '<|endoftext|>'\n    39 -> '<'\n  \
    \ 103 -> '|'\n 46617 -> 'answer'\n 54146 -> '|>'\n\nUsing \"<|XXX|>\" could be\
    \ harmful to the model, it has never seen such tokens following each other in\
    \ any of the regular training.\nIt makes sense to use that syntax when creating\
    \ a special token because it's the style TII chose but when not tokenizing them\
    \ I'd recommend using a normal word instead."
  created_at: 2023-07-19 15:34:37+00:00
  edited: false
  hidden: false
  id: 64b8109d0de7289de3593e4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
      fullname: Pascal Pfeiffer
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ilu000
      type: user
    createdAt: '2023-07-19T16:54:35.000Z'
    data:
      edited: false
      editors:
      - ilu000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9578917026519775
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
          fullname: Pascal Pfeiffer
          isHf: false
          isPro: false
          name: ilu000
          type: user
        html: '<p>You are more than welcome to test out normal words instead. The
          full training config is public in this repository. We saw that using special
          "words" are slightly slightly better in our evals. </p>

          '
        raw: 'You are more than welcome to test out normal words instead. The full
          training config is public in this repository. We saw that using special
          "words" are slightly slightly better in our evals. '
        updatedAt: '2023-07-19T16:54:35.479Z'
      numEdits: 0
      reactions: []
    id: 64b8154b2fccad9f5fef652c
    type: comment
  author: ilu000
  content: 'You are more than welcome to test out normal words instead. The full training
    config is public in this repository. We saw that using special "words" are slightly
    slightly better in our evals. '
  created_at: 2023-07-19 15:54:35+00:00
  edited: false
  hidden: false
  id: 64b8154b2fccad9f5fef652c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-19T17:05:06.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8816104531288147
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>I guess the model learns to react different if the word is never
          seen before.<br>Though &lt;|prompt|&gt; is 5 tokens, &lt;|answer|&gt; is
          4</p>

          '
        raw: 'I guess the model learns to react different if the word is never seen
          before.

          Though <|prompt|> is 5 tokens, <|answer|> is 4'
        updatedAt: '2023-07-19T17:05:06.885Z'
      numEdits: 0
      reactions: []
    id: 64b817c2126cfeb8fdbc7282
    type: comment
  author: cmp-nct
  content: 'I guess the model learns to react different if the word is never seen
    before.

    Though <|prompt|> is 5 tokens, <|answer|> is 4'
  created_at: 2023-07-19 16:05:06+00:00
  edited: false
  hidden: false
  id: 64b817c2126cfeb8fdbc7282
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2
repo_type: model
status: open
target_branch: null
title: No special tokens ?
