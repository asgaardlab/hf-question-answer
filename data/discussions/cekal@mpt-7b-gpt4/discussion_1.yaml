!!python/object:huggingface_hub.community.DiscussionWithDetails
author: g30rv17ys
conflicting_files: null
created_at: 2023-05-17 04:28:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
      fullname: g30rv1ty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g30rv17ys
      type: user
    createdAt: '2023-05-17T05:28:23.000Z'
    data:
      edited: false
      editors:
      - g30rv17ys
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
          fullname: g30rv1ty
          isHf: false
          isPro: false
          name: g30rv17ys
          type: user
        html: '<p>could we get a notebook to fine-tune the MPT model?</p>

          '
        raw: could we get a notebook to fine-tune the MPT model?
        updatedAt: '2023-05-17T05:28:23.842Z'
      numEdits: 0
      reactions: []
    id: 646465f7eca41ed5029d9f27
    type: comment
  author: g30rv17ys
  content: could we get a notebook to fine-tune the MPT model?
  created_at: 2023-05-17 04:28:23+00:00
  edited: false
  hidden: false
  id: 646465f7eca41ed5029d9f27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-05-17T05:57:15.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: "<p>When I get home I\u2019ll send you a script that you can use. You\
          \ will have to use my modified base model (same model just has support for\
          \ peft) in order to make it work.</p>\n<p>You can refer to these URLs in\
          \ the meantime:<br>patched model - peft compatible: <a href=\"https://huggingface.co/cekal/mpt-7b-peft-compatible\"\
          >cekal/mpt-7b-peft-compatible</a></p>\n<p>How I patched it: <a rel=\"nofollow\"\
          \ href=\"https://github.com/iwalton3/mpt-lora-patch\">Github</a></p>\n<p>Will\
          \ send you the script in ~ 3 hours</p>\n"
        raw: "When I get home I\u2019ll send you a script that you can use. You will\
          \ have to use my modified base model (same model just has support for peft)\
          \ in order to make it work.\n\nYou can refer to these URLs in the meantime:\n\
          patched model - peft compatible: [cekal/mpt-7b-peft-compatible](https://huggingface.co/cekal/mpt-7b-peft-compatible)\n\
          \nHow I patched it: [Github](https://github.com/iwalton3/mpt-lora-patch)\n\
          \nWill send you the script in ~ 3 hours"
        updatedAt: '2023-05-17T05:57:15.326Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - g30rv17ys
    id: 64646cbb6dad99445df08b2a
    type: comment
  author: cekal
  content: "When I get home I\u2019ll send you a script that you can use. You will\
    \ have to use my modified base model (same model just has support for peft) in\
    \ order to make it work.\n\nYou can refer to these URLs in the meantime:\npatched\
    \ model - peft compatible: [cekal/mpt-7b-peft-compatible](https://huggingface.co/cekal/mpt-7b-peft-compatible)\n\
    \nHow I patched it: [Github](https://github.com/iwalton3/mpt-lora-patch)\n\nWill\
    \ send you the script in ~ 3 hours"
  created_at: 2023-05-17 04:57:15+00:00
  edited: false
  hidden: false
  id: 64646cbb6dad99445df08b2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-05-17T06:01:55.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: '<p>It was finetuned on the classic alpaca gpt4 dataset, you can find
          that here: <a href="https://huggingface.co/datasets/vicgalle/alpaca-gpt4">https://huggingface.co/datasets/vicgalle/alpaca-gpt4</a></p>

          '
        raw: 'It was finetuned on the classic alpaca gpt4 dataset, you can find that
          here: https://huggingface.co/datasets/vicgalle/alpaca-gpt4'
        updatedAt: '2023-05-17T06:01:55.453Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - g30rv17ys
    id: 64646dd36dad99445df094ab
    type: comment
  author: cekal
  content: 'It was finetuned on the classic alpaca gpt4 dataset, you can find that
    here: https://huggingface.co/datasets/vicgalle/alpaca-gpt4'
  created_at: 2023-05-17 05:01:55+00:00
  edited: false
  hidden: false
  id: 64646dd36dad99445df094ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
      fullname: g30rv1ty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g30rv17ys
      type: user
    createdAt: '2023-05-17T06:06:02.000Z'
    data:
      edited: true
      editors:
      - g30rv17ys
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
          fullname: g30rv1ty
          isHf: false
          isPro: false
          name: g30rv17ys
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cekal&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cekal\">@<span class=\"\
          underline\">cekal</span></a></span>\n\n\t</span></span> thank you for your\
          \ reply and links.<br>looking forward to the fine-tuning script.</p>\n"
        raw: '@cekal thank you for your reply and links.

          looking forward to the fine-tuning script.'
        updatedAt: '2023-05-17T06:06:20.560Z'
      numEdits: 2
      reactions: []
    id: 64646ecabe81e08b43b03b80
    type: comment
  author: g30rv17ys
  content: '@cekal thank you for your reply and links.

    looking forward to the fine-tuning script.'
  created_at: 2023-05-17 05:06:02+00:00
  edited: true
  hidden: false
  id: 64646ecabe81e08b43b03b80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-05-17T10:25:48.000Z'
    data:
      edited: true
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: "<p>Here is the script I used to finetune the model. It is the same\
          \ structure as for LLaMA-LoRA, but I had to adjust a few things to make\
          \ it run. You can adjust other parameters like cutoff_len etc. based on\
          \ your needs. If you need anything else, just let me know.</p>\n<pre><code>import\
          \ os\nimport sys\nfrom typing import List\n\nimport fire\nimport torch\n\
          import transformers\nfrom datasets import load_dataset\n\n\"\"\"\nUnused\
          \ imports:\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\"\"\"\n\n\
          from peft import (\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n\
          \    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n\
          )\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom\
          \ utils.prompter import Prompter\n\n\ndef train(\n    # model/data params\n\
          \    base_model: str = \"cekal/mpt-7b-peft-compatible\",  # the only required\
          \ argument\n    data_path: str = \"alpaca_data_gpt4.json\",\n    output_dir:\
          \ str = \"./lora-alpaca\",\n    # training hyperparams\n    batch_size:\
          \ int = 128,\n    micro_batch_size: int = 4,\n    num_epochs: int = 1,\n\
          \    learning_rate: float = 3e-4,\n    cutoff_len: int = 2048,\n    val_set_size:\
          \ int = 2000,\n    # lora hyperparams\n    lora_r: int = 8,\n    lora_alpha:\
          \ int = 16,\n    lora_dropout: float = 0.05,\n    lora_target_modules: List[str]\
          \ = [\n        \"Wqkv\",\n    ],\n    # llm hyperparams\n    train_on_inputs:\
          \ bool = True,  # if False, masks out inputs in loss\n    add_eos_token:\
          \ bool = False,\n    group_by_length: bool = False,  # faster, but produces\
          \ an odd training loss curve\n    # wandb params\n    wandb_project: str\
          \ = \"\",\n    wandb_run_name: str = \"\",\n    wandb_watch: str = \"\"\
          ,  # options: false | gradients | all\n    wandb_log_model: str = \"\",\
          \  # options: false | true\n    resume_from_checkpoint: str = None,  # either\
          \ training checkpoint or final adapter\n):\n    if int(os.environ.get(\"\
          LOCAL_RANK\", 0)) == 0:\n        print(\n            f\"Training Alpaca-LoRA\
          \ model with params:\\n\"\n            f\"base_model: {base_model}\\n\"\n\
          \            f\"data_path: {data_path}\\n\"\n            f\"output_dir:\
          \ {output_dir}\\n\"\n            f\"batch_size: {batch_size}\\n\"\n    \
          \        f\"micro_batch_size: {micro_batch_size}\\n\"\n            f\"num_epochs:\
          \ {num_epochs}\\n\"\n            f\"learning_rate: {learning_rate}\\n\"\n\
          \            f\"cutoff_len: {cutoff_len}\\n\"\n            f\"val_set_size:\
          \ {val_set_size}\\n\"\n            f\"lora_r: {lora_r}\\n\"\n          \
          \  f\"lora_alpha: {lora_alpha}\\n\"\n            f\"lora_dropout: {lora_dropout}\\\
          n\"\n            f\"lora_target_modules: {lora_target_modules}\\n\"\n  \
          \          f\"train_on_inputs: {train_on_inputs}\\n\"\n            f\"add_eos_token:\
          \ {add_eos_token}\\n\"\n            f\"group_by_length: {group_by_length}\\\
          n\"\n            f\"wandb_project: {wandb_project}\\n\"\n            f\"\
          wandb_run_name: {wandb_run_name}\\n\"\n            f\"wandb_watch: {wandb_watch}\\\
          n\"\n            f\"wandb_log_model: {wandb_log_model}\\n\"\n          \
          \  f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n \
          \       )\n    assert (\n        base_model\n    ), \"Please specify a --base_model,\
          \ e.g. --base_model='huggyllama/llama-7b'\"\n    gradient_accumulation_steps\
          \ = batch_size // micro_batch_size\n\n    prompter = Prompter\n\n    device_map\
          \ = \"auto\"\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\
          \    ddp = world_size != 1\n    if ddp:\n        device_map = {\"\": int(os.environ.get(\"\
          LOCAL_RANK\") or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps\
          \ // world_size\n\n    # Check if parameter passed or if set within environ\n\
          \    use_wandb = len(wandb_project) &gt; 0 or (\n        \"WANDB_PROJECT\"\
          \ in os.environ and len(os.environ[\"WANDB_PROJECT\"]) &gt; 0\n    )\n \
          \   # Only overwrite environ if wandb param passed\n    if len(wandb_project)\
          \ &gt; 0:\n        os.environ[\"WANDB_PROJECT\"] = wandb_project\n    if\
          \ len(wandb_watch) &gt; 0:\n        os.environ[\"WANDB_WATCH\"] = wandb_watch\n\
          \    if len(wandb_log_model) &gt; 0:\n        os.environ[\"WANDB_LOG_MODEL\"\
          ] = wandb_log_model\n\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        base_model,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n\
          \        device_map=device_map,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\
          \n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different\
          \ from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow\
          \ batched inference\n\n    def tokenize(prompt, add_eos_token=True):\n \
          \       result = tokenizer(\n            prompt,\n            truncation=True,\n\
          \            max_length=cutoff_len,\n            padding=False,\n      \
          \      return_tensors=None,\n        )\n        if result[\"input_ids\"\
          ]:\n            if (\n                result[\"input_ids\"][-1] != tokenizer.eos_token_id\n\
          \                and len(result[\"input_ids\"]) &lt; cutoff_len\n      \
          \          and add_eos_token\n            ):\n                result[\"\
          input_ids\"].append(tokenizer.eos_token_id)\n                result[\"attention_mask\"\
          ].append(1)\n    \n        result[\"labels\"] = result[\"input_ids\"].copy()\n\
          \n        return result\n\n\n    def generate_and_tokenize_prompt(data_point):\n\
          \        conversations = data_point[\"conversations\"]\n        prompts\
          \ = []\n        responses = []\n    \n        for conversation in conversations:\n\
          \            from_role = conversation[\"from\"]\n            value = conversation[\"\
          value\"]\n    \n            if from_role == \"human\":\n               \
          \ prompts.append(\"[user]: \" + value)\n            elif from_role == \"\
          gpt\":\n                responses.append(\"[AI]: \" + value)\n    \n   \
          \     full_prompt = \"\\n\".join(prompts + responses)\n        tokenized_full_prompt\
          \ = tokenize(full_prompt)\n    \n        return tokenized_full_prompt\n\n\
          \n\n\n    model = prepare_model_for_int8_training(model)\n\n    config =\
          \ LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n     \
          \   target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n\
          \        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model\
          \ = get_peft_model(model, config)\n\n    if data_path.endswith(\".json\"\
          ) or data_path.endswith(\".jsonl\"):\n        data = load_dataset(\"json\"\
          , data_files=data_path)\n    else:\n        data = load_dataset(data_path)\n\
          \n    if resume_from_checkpoint:\n        # Check the available weights\
          \ and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint,\
          \ \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n\
          \            checkpoint_name = os.path.join(\n                resume_from_checkpoint,\
          \ \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config\
          \ above has to fit\n            resume_from_checkpoint = (\n           \
          \     False  # So the trainer won't try loading its state\n            )\n\
          \        # The two files above have a different name depending on how they\
          \ were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n\
          \            print(f\"Restarting from {checkpoint_name}\")\n           \
          \ adapters_weights = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model,\
          \ adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name}\
          \ not found\")\n\n    model.print_trainable_parameters()  # Be more transparent\
          \ about the % of trainable params.\n\n    if val_set_size &gt; 0:\n    \
          \    train_val = data[\"train\"].train_test_split(\n            test_size=val_set_size,\
          \ shuffle=True, seed=42\n        )\n        train_data = (\n           \
          \ train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n   \
          \     )\n        val_data = (\n            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        val_data = None\n\n    if not ddp and torch.cuda.device_count()\
          \ &gt; 1:\n        # keeps Trainer from trying its own DataParallelism when\
          \ more than 1 gpu is available\n        model.is_parallelizable = True\n\
          \        model.model_parallel = True\n\n    trainer = transformers.Trainer(\n\
          \        model=model,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n\
          \        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n\
          \            gradient_accumulation_steps=gradient_accumulation_steps,\n\
          \            warmup_steps=100,\n            num_train_epochs=num_epochs,\n\
          \            learning_rate=learning_rate,\n            fp16=True,\n    \
          \        logging_steps=10,\n            optim=\"adamw_torch\",\n       \
          \     evaluation_strategy=\"steps\" if val_set_size &gt; 0 else \"no\",\n\
          \            save_strategy=\"steps\",\n            eval_steps=200 if val_set_size\
          \ &gt; 0 else None,\n            save_steps=200,\n            output_dir=output_dir,\n\
          \            save_total_limit=3,\n            load_best_model_at_end=True\
          \ if val_set_size &gt; 0 else False,\n            ddp_find_unused_parameters=False\
          \ if ddp else None,\n            group_by_length=group_by_length,\n    \
          \        report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name\
          \ if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n\
          \            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n\
          \        ),\n    )\n    model.config.use_cache = False\n\n    old_state_dict\
          \ = model.state_dict\n    model.state_dict = (\n        lambda self, *_,\
          \ **__: get_peft_model_state_dict(\n            self, old_state_dict()\n\
          \        )\n    ).__get__(model, type(model))\n\n    if torch.__version__\
          \ &gt;= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\
          \n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n \
          \   model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's\
          \ a warning about missing keys above, please disregard :)\"\n    )\n\n\n\
          if __name__ == \"__main__\":\n    fire.Fire(train)\n</code></pre>\n"
        raw: "Here is the script I used to finetune the model. It is the same structure\
          \ as for LLaMA-LoRA, but I had to adjust a few things to make it run. You\
          \ can adjust other parameters like cutoff_len etc. based on your needs.\
          \ If you need anything else, just let me know.\n\n```\nimport os\nimport\
          \ sys\nfrom typing import List\n\nimport fire\nimport torch\nimport transformers\n\
          from datasets import load_dataset\n\n\"\"\"\nUnused imports:\nimport torch.nn\
          \ as nn\nimport bitsandbytes as bnb\n\"\"\"\n\nfrom peft import (\n    LoraConfig,\n\
          \    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n\
          \    set_peft_model_state_dict,\n)\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nfrom utils.prompter import Prompter\n\n\ndef train(\n\
          \    # model/data params\n    base_model: str = \"cekal/mpt-7b-peft-compatible\"\
          ,  # the only required argument\n    data_path: str = \"alpaca_data_gpt4.json\"\
          ,\n    output_dir: str = \"./lora-alpaca\",\n    # training hyperparams\n\
          \    batch_size: int = 128,\n    micro_batch_size: int = 4,\n    num_epochs:\
          \ int = 1,\n    learning_rate: float = 3e-4,\n    cutoff_len: int = 2048,\n\
          \    val_set_size: int = 2000,\n    # lora hyperparams\n    lora_r: int\
          \ = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n   \
          \ lora_target_modules: List[str] = [\n        \"Wqkv\",\n    ],\n    # llm\
          \ hyperparams\n    train_on_inputs: bool = True,  # if False, masks out\
          \ inputs in loss\n    add_eos_token: bool = False,\n    group_by_length:\
          \ bool = False,  # faster, but produces an odd training loss curve\n   \
          \ # wandb params\n    wandb_project: str = \"\",\n    wandb_run_name: str\
          \ = \"\",\n    wandb_watch: str = \"\",  # options: false | gradients |\
          \ all\n    wandb_log_model: str = \"\",  # options: false | true\n    resume_from_checkpoint:\
          \ str = None,  # either training checkpoint or final adapter\n):\n    if\
          \ int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n        print(\n       \
          \     f\"Training Alpaca-LoRA model with params:\\n\"\n            f\"base_model:\
          \ {base_model}\\n\"\n            f\"data_path: {data_path}\\n\"\n      \
          \      f\"output_dir: {output_dir}\\n\"\n            f\"batch_size: {batch_size}\\\
          n\"\n            f\"micro_batch_size: {micro_batch_size}\\n\"\n        \
          \    f\"num_epochs: {num_epochs}\\n\"\n            f\"learning_rate: {learning_rate}\\\
          n\"\n            f\"cutoff_len: {cutoff_len}\\n\"\n            f\"val_set_size:\
          \ {val_set_size}\\n\"\n            f\"lora_r: {lora_r}\\n\"\n          \
          \  f\"lora_alpha: {lora_alpha}\\n\"\n            f\"lora_dropout: {lora_dropout}\\\
          n\"\n            f\"lora_target_modules: {lora_target_modules}\\n\"\n  \
          \          f\"train_on_inputs: {train_on_inputs}\\n\"\n            f\"add_eos_token:\
          \ {add_eos_token}\\n\"\n            f\"group_by_length: {group_by_length}\\\
          n\"\n            f\"wandb_project: {wandb_project}\\n\"\n            f\"\
          wandb_run_name: {wandb_run_name}\\n\"\n            f\"wandb_watch: {wandb_watch}\\\
          n\"\n            f\"wandb_log_model: {wandb_log_model}\\n\"\n          \
          \  f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n \
          \       )\n    assert (\n        base_model\n    ), \"Please specify a --base_model,\
          \ e.g. --base_model='huggyllama/llama-7b'\"\n    gradient_accumulation_steps\
          \ = batch_size // micro_batch_size\n\n    prompter = Prompter\n\n    device_map\
          \ = \"auto\"\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\
          \    ddp = world_size != 1\n    if ddp:\n        device_map = {\"\": int(os.environ.get(\"\
          LOCAL_RANK\") or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps\
          \ // world_size\n\n    # Check if parameter passed or if set within environ\n\
          \    use_wandb = len(wandb_project) > 0 or (\n        \"WANDB_PROJECT\"\
          \ in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n    )\n    #\
          \ Only overwrite environ if wandb param passed\n    if len(wandb_project)\
          \ > 0:\n        os.environ[\"WANDB_PROJECT\"] = wandb_project\n    if len(wandb_watch)\
          \ > 0:\n        os.environ[\"WANDB_WATCH\"] = wandb_watch\n    if len(wandb_log_model)\
          \ > 0:\n        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n\n  \
          \  model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n\
          \        load_in_8bit=True,\n        torch_dtype=torch.float16,\n      \
          \  device_map=device_map,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\
          \n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different\
          \ from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow\
          \ batched inference\n\n    def tokenize(prompt, add_eos_token=True):\n \
          \       result = tokenizer(\n            prompt,\n            truncation=True,\n\
          \            max_length=cutoff_len,\n            padding=False,\n      \
          \      return_tensors=None,\n        )\n        if result[\"input_ids\"\
          ]:\n            if (\n                result[\"input_ids\"][-1] != tokenizer.eos_token_id\n\
          \                and len(result[\"input_ids\"]) < cutoff_len\n         \
          \       and add_eos_token\n            ):\n                result[\"input_ids\"\
          ].append(tokenizer.eos_token_id)\n                result[\"attention_mask\"\
          ].append(1)\n    \n        result[\"labels\"] = result[\"input_ids\"].copy()\n\
          \n        return result\n\n\n    def generate_and_tokenize_prompt(data_point):\n\
          \        conversations = data_point[\"conversations\"]\n        prompts\
          \ = []\n        responses = []\n    \n        for conversation in conversations:\n\
          \            from_role = conversation[\"from\"]\n            value = conversation[\"\
          value\"]\n    \n            if from_role == \"human\":\n               \
          \ prompts.append(\"[user]: \" + value)\n            elif from_role == \"\
          gpt\":\n                responses.append(\"[AI]: \" + value)\n    \n   \
          \     full_prompt = \"\\n\".join(prompts + responses)\n        tokenized_full_prompt\
          \ = tokenize(full_prompt)\n    \n        return tokenized_full_prompt\n\n\
          \n\n\n    model = prepare_model_for_int8_training(model)\n\n    config =\
          \ LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n     \
          \   target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n\
          \        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model\
          \ = get_peft_model(model, config)\n\n    if data_path.endswith(\".json\"\
          ) or data_path.endswith(\".jsonl\"):\n        data = load_dataset(\"json\"\
          , data_files=data_path)\n    else:\n        data = load_dataset(data_path)\n\
          \n    if resume_from_checkpoint:\n        # Check the available weights\
          \ and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint,\
          \ \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n\
          \            checkpoint_name = os.path.join(\n                resume_from_checkpoint,\
          \ \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config\
          \ above has to fit\n            resume_from_checkpoint = (\n           \
          \     False  # So the trainer won't try loading its state\n            )\n\
          \        # The two files above have a different name depending on how they\
          \ were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n\
          \            print(f\"Restarting from {checkpoint_name}\")\n           \
          \ adapters_weights = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model,\
          \ adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name}\
          \ not found\")\n\n    model.print_trainable_parameters()  # Be more transparent\
          \ about the % of trainable params.\n\n    if val_set_size > 0:\n       \
          \ train_val = data[\"train\"].train_test_split(\n            test_size=val_set_size,\
          \ shuffle=True, seed=42\n        )\n        train_data = (\n           \
          \ train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n   \
          \     )\n        val_data = (\n            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        val_data = None\n\n    if not ddp and torch.cuda.device_count()\
          \ > 1:\n        # keeps Trainer from trying its own DataParallelism when\
          \ more than 1 gpu is available\n        model.is_parallelizable = True\n\
          \        model.model_parallel = True\n\n    trainer = transformers.Trainer(\n\
          \        model=model,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n\
          \        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n\
          \            gradient_accumulation_steps=gradient_accumulation_steps,\n\
          \            warmup_steps=100,\n            num_train_epochs=num_epochs,\n\
          \            learning_rate=learning_rate,\n            fp16=True,\n    \
          \        logging_steps=10,\n            optim=\"adamw_torch\",\n       \
          \     evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n \
          \           save_strategy=\"steps\",\n            eval_steps=200 if val_set_size\
          \ > 0 else None,\n            save_steps=200,\n            output_dir=output_dir,\n\
          \            save_total_limit=3,\n            load_best_model_at_end=True\
          \ if val_set_size > 0 else False,\n            ddp_find_unused_parameters=False\
          \ if ddp else None,\n            group_by_length=group_by_length,\n    \
          \        report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name\
          \ if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n\
          \            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n\
          \        ),\n    )\n    model.config.use_cache = False\n\n    old_state_dict\
          \ = model.state_dict\n    model.state_dict = (\n        lambda self, *_,\
          \ **__: get_peft_model_state_dict(\n            self, old_state_dict()\n\
          \        )\n    ).__get__(model, type(model))\n\n    if torch.__version__\
          \ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\
          \n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n \
          \   model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's\
          \ a warning about missing keys above, please disregard :)\"\n    )\n\n\n\
          if __name__ == \"__main__\":\n    fire.Fire(train)\n```"
        updatedAt: '2023-05-17T13:38:31.606Z'
      numEdits: 1
      reactions: []
    id: 6464abac27d27c1fe7440e7e
    type: comment
  author: cekal
  content: "Here is the script I used to finetune the model. It is the same structure\
    \ as for LLaMA-LoRA, but I had to adjust a few things to make it run. You can\
    \ adjust other parameters like cutoff_len etc. based on your needs. If you need\
    \ anything else, just let me know.\n\n```\nimport os\nimport sys\nfrom typing\
    \ import List\n\nimport fire\nimport torch\nimport transformers\nfrom datasets\
    \ import load_dataset\n\n\"\"\"\nUnused imports:\nimport torch.nn as nn\nimport\
    \ bitsandbytes as bnb\n\"\"\"\n\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n\
    \    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n\
    )\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter\
    \ import Prompter\n\n\ndef train(\n    # model/data params\n    base_model: str\
    \ = \"cekal/mpt-7b-peft-compatible\",  # the only required argument\n    data_path:\
    \ str = \"alpaca_data_gpt4.json\",\n    output_dir: str = \"./lora-alpaca\",\n\
    \    # training hyperparams\n    batch_size: int = 128,\n    micro_batch_size:\
    \ int = 4,\n    num_epochs: int = 1,\n    learning_rate: float = 3e-4,\n    cutoff_len:\
    \ int = 2048,\n    val_set_size: int = 2000,\n    # lora hyperparams\n    lora_r:\
    \ int = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n    lora_target_modules:\
    \ List[str] = [\n        \"Wqkv\",\n    ],\n    # llm hyperparams\n    train_on_inputs:\
    \ bool = True,  # if False, masks out inputs in loss\n    add_eos_token: bool\
    \ = False,\n    group_by_length: bool = False,  # faster, but produces an odd\
    \ training loss curve\n    # wandb params\n    wandb_project: str = \"\",\n  \
    \  wandb_run_name: str = \"\",\n    wandb_watch: str = \"\",  # options: false\
    \ | gradients | all\n    wandb_log_model: str = \"\",  # options: false | true\n\
    \    resume_from_checkpoint: str = None,  # either training checkpoint or final\
    \ adapter\n):\n    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n        print(\n\
    \            f\"Training Alpaca-LoRA model with params:\\n\"\n            f\"\
    base_model: {base_model}\\n\"\n            f\"data_path: {data_path}\\n\"\n  \
    \          f\"output_dir: {output_dir}\\n\"\n            f\"batch_size: {batch_size}\\\
    n\"\n            f\"micro_batch_size: {micro_batch_size}\\n\"\n            f\"\
    num_epochs: {num_epochs}\\n\"\n            f\"learning_rate: {learning_rate}\\\
    n\"\n            f\"cutoff_len: {cutoff_len}\\n\"\n            f\"val_set_size:\
    \ {val_set_size}\\n\"\n            f\"lora_r: {lora_r}\\n\"\n            f\"lora_alpha:\
    \ {lora_alpha}\\n\"\n            f\"lora_dropout: {lora_dropout}\\n\"\n      \
    \      f\"lora_target_modules: {lora_target_modules}\\n\"\n            f\"train_on_inputs:\
    \ {train_on_inputs}\\n\"\n            f\"add_eos_token: {add_eos_token}\\n\"\n\
    \            f\"group_by_length: {group_by_length}\\n\"\n            f\"wandb_project:\
    \ {wandb_project}\\n\"\n            f\"wandb_run_name: {wandb_run_name}\\n\"\n\
    \            f\"wandb_watch: {wandb_watch}\\n\"\n            f\"wandb_log_model:\
    \ {wandb_log_model}\\n\"\n            f\"resume_from_checkpoint: {resume_from_checkpoint\
    \ or False}\\n\"\n        )\n    assert (\n        base_model\n    ), \"Please\
    \ specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n    gradient_accumulation_steps\
    \ = batch_size // micro_batch_size\n\n    prompter = Prompter\n\n    device_map\
    \ = \"auto\"\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp\
    \ = world_size != 1\n    if ddp:\n        device_map = {\"\": int(os.environ.get(\"\
    LOCAL_RANK\") or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps\
    \ // world_size\n\n    # Check if parameter passed or if set within environ\n\
    \    use_wandb = len(wandb_project) > 0 or (\n        \"WANDB_PROJECT\" in os.environ\
    \ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n    )\n    # Only overwrite environ\
    \ if wandb param passed\n    if len(wandb_project) > 0:\n        os.environ[\"\
    WANDB_PROJECT\"] = wandb_project\n    if len(wandb_watch) > 0:\n        os.environ[\"\
    WANDB_WATCH\"] = wandb_watch\n    if len(wandb_log_model) > 0:\n        os.environ[\"\
    WANDB_LOG_MODEL\"] = wandb_log_model\n\n    model = AutoModelForCausalLM.from_pretrained(\n\
    \        base_model,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n\
    \        device_map=device_map,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\
    \n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different\
    \ from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow batched\
    \ inference\n\n    def tokenize(prompt, add_eos_token=True):\n        result =\
    \ tokenizer(\n            prompt,\n            truncation=True,\n            max_length=cutoff_len,\n\
    \            padding=False,\n            return_tensors=None,\n        )\n   \
    \     if result[\"input_ids\"]:\n            if (\n                result[\"input_ids\"\
    ][-1] != tokenizer.eos_token_id\n                and len(result[\"input_ids\"\
    ]) < cutoff_len\n                and add_eos_token\n            ):\n         \
    \       result[\"input_ids\"].append(tokenizer.eos_token_id)\n               \
    \ result[\"attention_mask\"].append(1)\n    \n        result[\"labels\"] = result[\"\
    input_ids\"].copy()\n\n        return result\n\n\n    def generate_and_tokenize_prompt(data_point):\n\
    \        conversations = data_point[\"conversations\"]\n        prompts = []\n\
    \        responses = []\n    \n        for conversation in conversations:\n  \
    \          from_role = conversation[\"from\"]\n            value = conversation[\"\
    value\"]\n    \n            if from_role == \"human\":\n                prompts.append(\"\
    [user]: \" + value)\n            elif from_role == \"gpt\":\n                responses.append(\"\
    [AI]: \" + value)\n    \n        full_prompt = \"\\n\".join(prompts + responses)\n\
    \        tokenized_full_prompt = tokenize(full_prompt)\n    \n        return tokenized_full_prompt\n\
    \n\n\n\n    model = prepare_model_for_int8_training(model)\n\n    config = LoraConfig(\n\
    \        r=lora_r,\n        lora_alpha=lora_alpha,\n        target_modules=lora_target_modules,\n\
    \        lora_dropout=lora_dropout,\n        bias=\"none\",\n        task_type=\"\
    CAUSAL_LM\",\n    )\n    model = get_peft_model(model, config)\n\n    if data_path.endswith(\"\
    .json\") or data_path.endswith(\".jsonl\"):\n        data = load_dataset(\"json\"\
    , data_files=data_path)\n    else:\n        data = load_dataset(data_path)\n\n\
    \    if resume_from_checkpoint:\n        # Check the available weights and load\
    \ them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint,\
    \ \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n\
    \            checkpoint_name = os.path.join(\n                resume_from_checkpoint,\
    \ \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config above\
    \ has to fit\n            resume_from_checkpoint = (\n                False  #\
    \ So the trainer won't try loading its state\n            )\n        # The two\
    \ files above have a different name depending on how they were saved, but are\
    \ actually the same.\n        if os.path.exists(checkpoint_name):\n          \
    \  print(f\"Restarting from {checkpoint_name}\")\n            adapters_weights\
    \ = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model,\
    \ adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name}\
    \ not found\")\n\n    model.print_trainable_parameters()  # Be more transparent\
    \ about the % of trainable params.\n\n    if val_set_size > 0:\n        train_val\
    \ = data[\"train\"].train_test_split(\n            test_size=val_set_size, shuffle=True,\
    \ seed=42\n        )\n        train_data = (\n            train_val[\"train\"\
    ].shuffle().map(generate_and_tokenize_prompt)\n        )\n        val_data = (\n\
    \            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\
    \        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\
    \        val_data = None\n\n    if not ddp and torch.cuda.device_count() > 1:\n\
    \        # keeps Trainer from trying its own DataParallelism when more than 1\
    \ gpu is available\n        model.is_parallelizable = True\n        model.model_parallel\
    \ = True\n\n    trainer = transformers.Trainer(\n        model=model,\n      \
    \  train_dataset=train_data,\n        eval_dataset=val_data,\n        args=transformers.TrainingArguments(\n\
    \            per_device_train_batch_size=micro_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n\
    \            warmup_steps=100,\n            num_train_epochs=num_epochs,\n   \
    \         learning_rate=learning_rate,\n            fp16=True,\n            logging_steps=10,\n\
    \            optim=\"adamw_torch\",\n            evaluation_strategy=\"steps\"\
    \ if val_set_size > 0 else \"no\",\n            save_strategy=\"steps\",\n   \
    \         eval_steps=200 if val_set_size > 0 else None,\n            save_steps=200,\n\
    \            output_dir=output_dir,\n            save_total_limit=3,\n       \
    \     load_best_model_at_end=True if val_set_size > 0 else False,\n          \
    \  ddp_find_unused_parameters=False if ddp else None,\n            group_by_length=group_by_length,\n\
    \            report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name\
    \ if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n\
    \            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n\
    \        ),\n    )\n    model.config.use_cache = False\n\n    old_state_dict =\
    \ model.state_dict\n    model.state_dict = (\n        lambda self, *_, **__: get_peft_model_state_dict(\n\
    \            self, old_state_dict()\n        )\n    ).__get__(model, type(model))\n\
    \n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model\
    \ = torch.compile(model)\n\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\
    \n    model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's\
    \ a warning about missing keys above, please disregard :)\"\n    )\n\n\nif __name__\
    \ == \"__main__\":\n    fire.Fire(train)\n```"
  created_at: 2023-05-17 09:25:48+00:00
  edited: true
  hidden: false
  id: 6464abac27d27c1fe7440e7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-05-17T13:39:53.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: "<p>I just realized I modified the 'def generate_and_tokenize_prompt'\
          \ to fit my current dataset structure. You may want to use the following\
          \ script instead to finetune the model on the default alpaca dataset:</p>\n\
          <pre><code>import os\nimport sys\nfrom typing import List\n\nimport fire\n\
          import torch\nimport transformers\nfrom datasets import load_dataset\n\n\
          \"\"\"\nUnused imports:\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\
          \"\"\"\n\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n   \
          \ get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n   \
          \ set_peft_model_state_dict,\n)\nfrom transformers import LlamaForCausalLM,\
          \ LlamaTokenizer\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \nfrom utils.prompter import Prompter\n\n\ndef train(\n    # model/data\
          \ params\n    base_model: str = \"cekal/mpt-7b-peft-compatible\",  # the\
          \ only required argument\n    data_path: str = \"alpaca_data_gpt4.json\"\
          ,\n    output_dir: str = \"./lora-alpaca\",\n    # training hyperparams\n\
          \    batch_size: int = 128,\n    micro_batch_size: int = 4,\n    num_epochs:\
          \ int = 3,\n    learning_rate: float = 3e-4,\n    cutoff_len: int = 256,\n\
          \    val_set_size: int = 2000,\n    # lora hyperparams\n    lora_r: int\
          \ = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n   \
          \ lora_target_modules: List[str] = [\"Wqkv\"],\n    # lora_target_modules:\
          \ List[str] = [\n    #     \"q_proj\",\n    #     \"v_proj\",\n    # ],\n\
          \    # llm hyperparams\n    train_on_inputs: bool = True,  # if False, masks\
          \ out inputs in loss\n    add_eos_token: bool = False,\n    group_by_length:\
          \ bool = False,  # faster, but produces an odd training loss curve\n   \
          \ # wandb params\n    wandb_project: str = \"\",\n    wandb_run_name: str\
          \ = \"\",\n    wandb_watch: str = \"\",  # options: false | gradients |\
          \ all\n    wandb_log_model: str = \"\",  # options: false | true\n    resume_from_checkpoint:\
          \ str = None,  # either training checkpoint or final adapter\n    prompt_template_name:\
          \ str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n\
          ):\n    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n        print(\n\
          \            f\"Training Alpaca-LoRA model with params:\\n\"\n         \
          \   f\"base_model: {base_model}\\n\"\n            f\"data_path: {data_path}\\\
          n\"\n            f\"output_dir: {output_dir}\\n\"\n            f\"batch_size:\
          \ {batch_size}\\n\"\n            f\"micro_batch_size: {micro_batch_size}\\\
          n\"\n            f\"num_epochs: {num_epochs}\\n\"\n            f\"learning_rate:\
          \ {learning_rate}\\n\"\n            f\"cutoff_len: {cutoff_len}\\n\"\n \
          \           f\"val_set_size: {val_set_size}\\n\"\n            f\"lora_r:\
          \ {lora_r}\\n\"\n            f\"lora_alpha: {lora_alpha}\\n\"\n        \
          \    f\"lora_dropout: {lora_dropout}\\n\"\n            f\"lora_target_modules:\
          \ {lora_target_modules}\\n\"\n            f\"train_on_inputs: {train_on_inputs}\\\
          n\"\n            f\"add_eos_token: {add_eos_token}\\n\"\n            f\"\
          group_by_length: {group_by_length}\\n\"\n            f\"wandb_project: {wandb_project}\\\
          n\"\n            f\"wandb_run_name: {wandb_run_name}\\n\"\n            f\"\
          wandb_watch: {wandb_watch}\\n\"\n            f\"wandb_log_model: {wandb_log_model}\\\
          n\"\n            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\\
          n\"\n            f\"prompt template: {prompt_template_name}\\n\"\n     \
          \   )\n    assert (\n        base_model\n    ), \"Please specify a --base_model,\
          \ e.g. --base_model='huggyllama/llama-7b'\"\n    gradient_accumulation_steps\
          \ = batch_size // micro_batch_size\n\n    prompter = Prompter(prompt_template_name)\n\
          \n    device_map = {\"\": 0}    #\"auto\" # {\"\": 0}\n    world_size =\
          \ int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n  \
          \  if ddp:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\"\
          ) or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps\
          \ // world_size\n    print(f\"device map: {device_map}\")\n    # Check if\
          \ parameter passed or if set within environ\n    use_wandb = len(wandb_project)\
          \ &gt; 0 or (\n        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"\
          WANDB_PROJECT\"]) &gt; 0\n    )\n    # Only overwrite environ if wandb param\
          \ passed\n    if len(wandb_project) &gt; 0:\n        os.environ[\"WANDB_PROJECT\"\
          ] = wandb_project\n    if len(wandb_watch) &gt; 0:\n        os.environ[\"\
          WANDB_WATCH\"] = wandb_watch\n    if len(wandb_log_model) &gt; 0:\n    \
          \    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n\n    # quantization_config\
          \ = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n    # config\
          \ = transformers.AutoConfig.from_pretrained(\n    #     'mosaicml/mpt-7b',\n\
          \    #     trust_remote_code=True\n    # )\n    # config.attn_config['attn_impl']\
          \ = 'triton'\n\n    # model = LlamaForCausalLM.from_pretrained(\n    model\
          \ = AutoModelForCausalLM.from_pretrained(\n        # 'mosaicml/mpt-7b',\n\
          \        base_model,\n        trust_remote_code=True,\n        # base_model,\n\
          \        load_in_8bit=True,\n        torch_dtype=torch.float16,\n      \
          \  device_map=device_map,\n        # quantization_config=quantization_config,\n\
          \        # load_in_8bit_fp32_cpu_offload=True\n    )\n\n    # tokenizer\
          \ = LlamaTokenizer.from_pretrained(base_model)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\
          \n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different\
          \ from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow\
          \ batched inference\n\n    def tokenize(prompt, add_eos_token=True):\n \
          \       # there's probably a way to do this with the tokenizer settings\n\
          \        # but again, gotta move fast\n        result = tokenizer(\n   \
          \         prompt,\n            truncation=True,\n            max_length=cutoff_len,\n\
          \            padding=False,\n            return_tensors=None,\n        )\n\
          \        if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n\
          \            and len(result[\"input_ids\"]) &lt; cutoff_len\n          \
          \  and add_eos_token\n        ):\n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n\
          \            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"\
          ] = result[\"input_ids\"].copy()\n\n        return result\n\n    def generate_and_tokenize_prompt(data_point):\n\
          \        full_prompt = prompter.generate_prompt(\n            data_point[\"\
          instruction\"],\n            data_point[\"input\"],\n            data_point[\"\
          output\"],\n        )\n        tokenized_full_prompt = tokenize(full_prompt)\n\
          \        if not train_on_inputs:\n            user_prompt = prompter.generate_prompt(\n\
          \                data_point[\"instruction\"], data_point[\"input\"]\n  \
          \          )\n            tokenized_user_prompt = tokenize(\n          \
          \      user_prompt, add_eos_token=add_eos_token\n            )\n       \
          \     user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\n  \
          \          if add_eos_token:\n                user_prompt_len -= 1\n\n \
          \           tokenized_full_prompt[\"labels\"] = [\n                -100\n\
          \            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n\
          \                user_prompt_len:\n            ]  # could be sped up, probably\n\
          \        return tokenized_full_prompt\n\n    # model = prepare_model_for_int8_training(model)\n\
          \n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n\
          \        target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n\
          \        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model\
          \ = get_peft_model(model, config)\n\n    if data_path.endswith(\".json\"\
          ) or data_path.endswith(\".jsonl\"):\n        data = load_dataset(\"json\"\
          , data_files=data_path)\n    else:\n        data = load_dataset(data_path)\n\
          \n    if resume_from_checkpoint:\n        # Check the available weights\
          \ and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint,\
          \ \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n\
          \            checkpoint_name = os.path.join(\n                resume_from_checkpoint,\
          \ \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config\
          \ above has to fit\n            resume_from_checkpoint = (\n           \
          \     False  # So the trainer won't try loading its state\n            )\n\
          \        # The two files above have a different name depending on how they\
          \ were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n\
          \            print(f\"Restarting from {checkpoint_name}\")\n           \
          \ adapters_weights = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model,\
          \ adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name}\
          \ not found\")\n\n    model.print_trainable_parameters()  # Be more transparent\
          \ about the % of trainable params.\n\n    if val_set_size &gt; 0:\n    \
          \    train_val = data[\"train\"].train_test_split(\n            test_size=val_set_size,\
          \ shuffle=True, seed=42\n        )\n        train_data = (\n           \
          \ train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n   \
          \     )\n        val_data = (\n            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        val_data = None\n\n    if not ddp and torch.cuda.device_count()\
          \ &gt; 1:\n        # keeps Trainer from trying its own DataParallelism when\
          \ more than 1 gpu is available\n        model.is_parallelizable = True\n\
          \        model.model_parallel = True\n\n    trainer = transformers.Trainer(\n\
          \        model=model,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n\
          \        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n\
          \            gradient_accumulation_steps=gradient_accumulation_steps,\n\
          \            warmup_steps=100,\n            num_train_epochs=num_epochs,\n\
          \            learning_rate=learning_rate,\n            fp16=True,\n    \
          \        logging_steps=10,\n            optim=\"adamw_torch\",\n       \
          \     evaluation_strategy=\"steps\" if val_set_size &gt; 0 else \"no\",\n\
          \            save_strategy=\"steps\",\n            eval_steps=200 if val_set_size\
          \ &gt; 0 else None,\n            save_steps=200,\n            output_dir=output_dir,\n\
          \            save_total_limit=3,\n            load_best_model_at_end=True\
          \ if val_set_size &gt; 0 else False,\n            ddp_find_unused_parameters=False\
          \ if ddp else None,\n            group_by_length=group_by_length,\n    \
          \        report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name\
          \ if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n\
          \            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n\
          \        ),\n    )\n    model.config.use_cache = False\n#     config = transformers.AutoConfig.from_pretrained(\n\
          #   'mosaicml/mpt-7b',\n#   trust_remote_code=True\n# )\n    # model.config.trust_remote_code\
          \ = True\n    # model.config.attn_config['attn_impl'] = 'triton'\n\n   \
          \ old_state_dict = model.state_dict\n    model.state_dict = (\n        lambda\
          \ self, *_, **__: get_peft_model_state_dict(\n            self, old_state_dict()\n\
          \        )\n    ).__get__(model, type(model))\n\n    if torch.__version__\
          \ &gt;= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\
          \n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n \
          \   model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's\
          \ a warning about missing keys above, please disregard :)\"\n    )\n\n\n\
          if __name__ == \"__main__\":\n    fire.Fire(train)\n</code></pre>\n<p>I\
          \ also forgot to modify the query_key_value parameter in the previous response.\
          \ This code should now run smoothly.</p>\n"
        raw: "I just realized I modified the 'def generate_and_tokenize_prompt' to\
          \ fit my current dataset structure. You may want to use the following script\
          \ instead to finetune the model on the default alpaca dataset:\n```\nimport\
          \ os\nimport sys\nfrom typing import List\n\nimport fire\nimport torch\n\
          import transformers\nfrom datasets import load_dataset\n\n\"\"\"\nUnused\
          \ imports:\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\"\"\"\n\n\
          from peft import (\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n\
          \    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n\
          )\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import\
          \ Prompter\n\n\ndef train(\n    # model/data params\n    base_model: str\
          \ = \"cekal/mpt-7b-peft-compatible\",  # the only required argument\n  \
          \  data_path: str = \"alpaca_data_gpt4.json\",\n    output_dir: str = \"\
          ./lora-alpaca\",\n    # training hyperparams\n    batch_size: int = 128,\n\
          \    micro_batch_size: int = 4,\n    num_epochs: int = 3,\n    learning_rate:\
          \ float = 3e-4,\n    cutoff_len: int = 256,\n    val_set_size: int = 2000,\n\
          \    # lora hyperparams\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n\
          \    lora_dropout: float = 0.05,\n    lora_target_modules: List[str] = [\"\
          Wqkv\"],\n    # lora_target_modules: List[str] = [\n    #     \"q_proj\"\
          ,\n    #     \"v_proj\",\n    # ],\n    # llm hyperparams\n    train_on_inputs:\
          \ bool = True,  # if False, masks out inputs in loss\n    add_eos_token:\
          \ bool = False,\n    group_by_length: bool = False,  # faster, but produces\
          \ an odd training loss curve\n    # wandb params\n    wandb_project: str\
          \ = \"\",\n    wandb_run_name: str = \"\",\n    wandb_watch: str = \"\"\
          ,  # options: false | gradients | all\n    wandb_log_model: str = \"\",\
          \  # options: false | true\n    resume_from_checkpoint: str = None,  # either\
          \ training checkpoint or final adapter\n    prompt_template_name: str =\
          \ \"alpaca\",  # The prompt template to use, will default to alpaca.\n):\n\
          \    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n        print(\n \
          \           f\"Training Alpaca-LoRA model with params:\\n\"\n          \
          \  f\"base_model: {base_model}\\n\"\n            f\"data_path: {data_path}\\\
          n\"\n            f\"output_dir: {output_dir}\\n\"\n            f\"batch_size:\
          \ {batch_size}\\n\"\n            f\"micro_batch_size: {micro_batch_size}\\\
          n\"\n            f\"num_epochs: {num_epochs}\\n\"\n            f\"learning_rate:\
          \ {learning_rate}\\n\"\n            f\"cutoff_len: {cutoff_len}\\n\"\n \
          \           f\"val_set_size: {val_set_size}\\n\"\n            f\"lora_r:\
          \ {lora_r}\\n\"\n            f\"lora_alpha: {lora_alpha}\\n\"\n        \
          \    f\"lora_dropout: {lora_dropout}\\n\"\n            f\"lora_target_modules:\
          \ {lora_target_modules}\\n\"\n            f\"train_on_inputs: {train_on_inputs}\\\
          n\"\n            f\"add_eos_token: {add_eos_token}\\n\"\n            f\"\
          group_by_length: {group_by_length}\\n\"\n            f\"wandb_project: {wandb_project}\\\
          n\"\n            f\"wandb_run_name: {wandb_run_name}\\n\"\n            f\"\
          wandb_watch: {wandb_watch}\\n\"\n            f\"wandb_log_model: {wandb_log_model}\\\
          n\"\n            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\\
          n\"\n            f\"prompt template: {prompt_template_name}\\n\"\n     \
          \   )\n    assert (\n        base_model\n    ), \"Please specify a --base_model,\
          \ e.g. --base_model='huggyllama/llama-7b'\"\n    gradient_accumulation_steps\
          \ = batch_size // micro_batch_size\n\n    prompter = Prompter(prompt_template_name)\n\
          \n    device_map = {\"\": 0}    #\"auto\" # {\"\": 0}\n    world_size =\
          \ int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n  \
          \  if ddp:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\"\
          ) or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps\
          \ // world_size\n    print(f\"device map: {device_map}\")\n    # Check if\
          \ parameter passed or if set within environ\n    use_wandb = len(wandb_project)\
          \ > 0 or (\n        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"\
          WANDB_PROJECT\"]) > 0\n    )\n    # Only overwrite environ if wandb param\
          \ passed\n    if len(wandb_project) > 0:\n        os.environ[\"WANDB_PROJECT\"\
          ] = wandb_project\n    if len(wandb_watch) > 0:\n        os.environ[\"WANDB_WATCH\"\
          ] = wandb_watch\n    if len(wandb_log_model) > 0:\n        os.environ[\"\
          WANDB_LOG_MODEL\"] = wandb_log_model\n\n    # quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n\
          \    # config = transformers.AutoConfig.from_pretrained(\n    #     'mosaicml/mpt-7b',\n\
          \    #     trust_remote_code=True\n    # )\n    # config.attn_config['attn_impl']\
          \ = 'triton'\n\n    # model = LlamaForCausalLM.from_pretrained(\n    model\
          \ = AutoModelForCausalLM.from_pretrained(\n        # 'mosaicml/mpt-7b',\n\
          \        base_model,\n        trust_remote_code=True,\n        # base_model,\n\
          \        load_in_8bit=True,\n        torch_dtype=torch.float16,\n      \
          \  device_map=device_map,\n        # quantization_config=quantization_config,\n\
          \        # load_in_8bit_fp32_cpu_offload=True\n    )\n\n    # tokenizer\
          \ = LlamaTokenizer.from_pretrained(base_model)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\
          \n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different\
          \ from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow\
          \ batched inference\n\n    def tokenize(prompt, add_eos_token=True):\n \
          \       # there's probably a way to do this with the tokenizer settings\n\
          \        # but again, gotta move fast\n        result = tokenizer(\n   \
          \         prompt,\n            truncation=True,\n            max_length=cutoff_len,\n\
          \            padding=False,\n            return_tensors=None,\n        )\n\
          \        if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n\
          \            and len(result[\"input_ids\"]) < cutoff_len\n            and\
          \ add_eos_token\n        ):\n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n\
          \            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"\
          ] = result[\"input_ids\"].copy()\n\n        return result\n\n    def generate_and_tokenize_prompt(data_point):\n\
          \        full_prompt = prompter.generate_prompt(\n            data_point[\"\
          instruction\"],\n            data_point[\"input\"],\n            data_point[\"\
          output\"],\n        )\n        tokenized_full_prompt = tokenize(full_prompt)\n\
          \        if not train_on_inputs:\n            user_prompt = prompter.generate_prompt(\n\
          \                data_point[\"instruction\"], data_point[\"input\"]\n  \
          \          )\n            tokenized_user_prompt = tokenize(\n          \
          \      user_prompt, add_eos_token=add_eos_token\n            )\n       \
          \     user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\n  \
          \          if add_eos_token:\n                user_prompt_len -= 1\n\n \
          \           tokenized_full_prompt[\"labels\"] = [\n                -100\n\
          \            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n\
          \                user_prompt_len:\n            ]  # could be sped up, probably\n\
          \        return tokenized_full_prompt\n\n    # model = prepare_model_for_int8_training(model)\n\
          \n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n\
          \        target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n\
          \        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model\
          \ = get_peft_model(model, config)\n\n    if data_path.endswith(\".json\"\
          ) or data_path.endswith(\".jsonl\"):\n        data = load_dataset(\"json\"\
          , data_files=data_path)\n    else:\n        data = load_dataset(data_path)\n\
          \n    if resume_from_checkpoint:\n        # Check the available weights\
          \ and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint,\
          \ \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n\
          \            checkpoint_name = os.path.join(\n                resume_from_checkpoint,\
          \ \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config\
          \ above has to fit\n            resume_from_checkpoint = (\n           \
          \     False  # So the trainer won't try loading its state\n            )\n\
          \        # The two files above have a different name depending on how they\
          \ were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n\
          \            print(f\"Restarting from {checkpoint_name}\")\n           \
          \ adapters_weights = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model,\
          \ adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name}\
          \ not found\")\n\n    model.print_trainable_parameters()  # Be more transparent\
          \ about the % of trainable params.\n\n    if val_set_size > 0:\n       \
          \ train_val = data[\"train\"].train_test_split(\n            test_size=val_set_size,\
          \ shuffle=True, seed=42\n        )\n        train_data = (\n           \
          \ train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n   \
          \     )\n        val_data = (\n            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\
          \        val_data = None\n\n    if not ddp and torch.cuda.device_count()\
          \ > 1:\n        # keeps Trainer from trying its own DataParallelism when\
          \ more than 1 gpu is available\n        model.is_parallelizable = True\n\
          \        model.model_parallel = True\n\n    trainer = transformers.Trainer(\n\
          \        model=model,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n\
          \        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n\
          \            gradient_accumulation_steps=gradient_accumulation_steps,\n\
          \            warmup_steps=100,\n            num_train_epochs=num_epochs,\n\
          \            learning_rate=learning_rate,\n            fp16=True,\n    \
          \        logging_steps=10,\n            optim=\"adamw_torch\",\n       \
          \     evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n \
          \           save_strategy=\"steps\",\n            eval_steps=200 if val_set_size\
          \ > 0 else None,\n            save_steps=200,\n            output_dir=output_dir,\n\
          \            save_total_limit=3,\n            load_best_model_at_end=True\
          \ if val_set_size > 0 else False,\n            ddp_find_unused_parameters=False\
          \ if ddp else None,\n            group_by_length=group_by_length,\n    \
          \        report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name\
          \ if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n\
          \            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n\
          \        ),\n    )\n    model.config.use_cache = False\n#     config = transformers.AutoConfig.from_pretrained(\n\
          #   'mosaicml/mpt-7b',\n#   trust_remote_code=True\n# )\n    # model.config.trust_remote_code\
          \ = True\n    # model.config.attn_config['attn_impl'] = 'triton'\n\n   \
          \ old_state_dict = model.state_dict\n    model.state_dict = (\n        lambda\
          \ self, *_, **__: get_peft_model_state_dict(\n            self, old_state_dict()\n\
          \        )\n    ).__get__(model, type(model))\n\n    if torch.__version__\
          \ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\
          \n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n \
          \   model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's\
          \ a warning about missing keys above, please disregard :)\"\n    )\n\n\n\
          if __name__ == \"__main__\":\n    fire.Fire(train)\n```\n\nI also forgot\
          \ to modify the query_key_value parameter in the previous response. This\
          \ code should now run smoothly."
        updatedAt: '2023-05-17T13:39:53.780Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - g30rv17ys
    id: 6464d9294855e06b95064804
    type: comment
  author: cekal
  content: "I just realized I modified the 'def generate_and_tokenize_prompt' to fit\
    \ my current dataset structure. You may want to use the following script instead\
    \ to finetune the model on the default alpaca dataset:\n```\nimport os\nimport\
    \ sys\nfrom typing import List\n\nimport fire\nimport torch\nimport transformers\n\
    from datasets import load_dataset\n\n\"\"\"\nUnused imports:\nimport torch.nn\
    \ as nn\nimport bitsandbytes as bnb\n\"\"\"\n\nfrom peft import (\n    LoraConfig,\n\
    \    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n\
    \    set_peft_model_state_dict,\n)\nfrom transformers import LlamaForCausalLM,\
    \ LlamaTokenizer\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
    \nfrom utils.prompter import Prompter\n\n\ndef train(\n    # model/data params\n\
    \    base_model: str = \"cekal/mpt-7b-peft-compatible\",  # the only required\
    \ argument\n    data_path: str = \"alpaca_data_gpt4.json\",\n    output_dir: str\
    \ = \"./lora-alpaca\",\n    # training hyperparams\n    batch_size: int = 128,\n\
    \    micro_batch_size: int = 4,\n    num_epochs: int = 3,\n    learning_rate:\
    \ float = 3e-4,\n    cutoff_len: int = 256,\n    val_set_size: int = 2000,\n \
    \   # lora hyperparams\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n    lora_dropout:\
    \ float = 0.05,\n    lora_target_modules: List[str] = [\"Wqkv\"],\n    # lora_target_modules:\
    \ List[str] = [\n    #     \"q_proj\",\n    #     \"v_proj\",\n    # ],\n    #\
    \ llm hyperparams\n    train_on_inputs: bool = True,  # if False, masks out inputs\
    \ in loss\n    add_eos_token: bool = False,\n    group_by_length: bool = False,\
    \  # faster, but produces an odd training loss curve\n    # wandb params\n   \
    \ wandb_project: str = \"\",\n    wandb_run_name: str = \"\",\n    wandb_watch:\
    \ str = \"\",  # options: false | gradients | all\n    wandb_log_model: str =\
    \ \"\",  # options: false | true\n    resume_from_checkpoint: str = None,  # either\
    \ training checkpoint or final adapter\n    prompt_template_name: str = \"alpaca\"\
    ,  # The prompt template to use, will default to alpaca.\n):\n    if int(os.environ.get(\"\
    LOCAL_RANK\", 0)) == 0:\n        print(\n            f\"Training Alpaca-LoRA model\
    \ with params:\\n\"\n            f\"base_model: {base_model}\\n\"\n          \
    \  f\"data_path: {data_path}\\n\"\n            f\"output_dir: {output_dir}\\n\"\
    \n            f\"batch_size: {batch_size}\\n\"\n            f\"micro_batch_size:\
    \ {micro_batch_size}\\n\"\n            f\"num_epochs: {num_epochs}\\n\"\n    \
    \        f\"learning_rate: {learning_rate}\\n\"\n            f\"cutoff_len: {cutoff_len}\\\
    n\"\n            f\"val_set_size: {val_set_size}\\n\"\n            f\"lora_r:\
    \ {lora_r}\\n\"\n            f\"lora_alpha: {lora_alpha}\\n\"\n            f\"\
    lora_dropout: {lora_dropout}\\n\"\n            f\"lora_target_modules: {lora_target_modules}\\\
    n\"\n            f\"train_on_inputs: {train_on_inputs}\\n\"\n            f\"add_eos_token:\
    \ {add_eos_token}\\n\"\n            f\"group_by_length: {group_by_length}\\n\"\
    \n            f\"wandb_project: {wandb_project}\\n\"\n            f\"wandb_run_name:\
    \ {wandb_run_name}\\n\"\n            f\"wandb_watch: {wandb_watch}\\n\"\n    \
    \        f\"wandb_log_model: {wandb_log_model}\\n\"\n            f\"resume_from_checkpoint:\
    \ {resume_from_checkpoint or False}\\n\"\n            f\"prompt template: {prompt_template_name}\\\
    n\"\n        )\n    assert (\n        base_model\n    ), \"Please specify a --base_model,\
    \ e.g. --base_model='huggyllama/llama-7b'\"\n    gradient_accumulation_steps =\
    \ batch_size // micro_batch_size\n\n    prompter = Prompter(prompt_template_name)\n\
    \n    device_map = {\"\": 0}    #\"auto\" # {\"\": 0}\n    world_size = int(os.environ.get(\"\
    WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if ddp:\n        device_map\
    \ = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n        gradient_accumulation_steps\
    \ = gradient_accumulation_steps // world_size\n    print(f\"device map: {device_map}\"\
    )\n    # Check if parameter passed or if set within environ\n    use_wandb = len(wandb_project)\
    \ > 0 or (\n        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"\
    ]) > 0\n    )\n    # Only overwrite environ if wandb param passed\n    if len(wandb_project)\
    \ > 0:\n        os.environ[\"WANDB_PROJECT\"] = wandb_project\n    if len(wandb_watch)\
    \ > 0:\n        os.environ[\"WANDB_WATCH\"] = wandb_watch\n    if len(wandb_log_model)\
    \ > 0:\n        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n\n    # quantization_config\
    \ = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n    # config =\
    \ transformers.AutoConfig.from_pretrained(\n    #     'mosaicml/mpt-7b',\n   \
    \ #     trust_remote_code=True\n    # )\n    # config.attn_config['attn_impl']\
    \ = 'triton'\n\n    # model = LlamaForCausalLM.from_pretrained(\n    model = AutoModelForCausalLM.from_pretrained(\n\
    \        # 'mosaicml/mpt-7b',\n        base_model,\n        trust_remote_code=True,\n\
    \        # base_model,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n\
    \        device_map=device_map,\n        # quantization_config=quantization_config,\n\
    \        # load_in_8bit_fp32_cpu_offload=True\n    )\n\n    # tokenizer = LlamaTokenizer.from_pretrained(base_model)\n\
    \    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\n    tokenizer.pad_token_id\
    \ = (\n        0  # unk. we want this to be different from the eos token\n   \
    \ )\n    tokenizer.padding_side = \"left\"  # Allow batched inference\n\n    def\
    \ tokenize(prompt, add_eos_token=True):\n        # there's probably a way to do\
    \ this with the tokenizer settings\n        # but again, gotta move fast\n   \
    \     result = tokenizer(\n            prompt,\n            truncation=True,\n\
    \            max_length=cutoff_len,\n            padding=False,\n            return_tensors=None,\n\
    \        )\n        if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n\
    \            and len(result[\"input_ids\"]) < cutoff_len\n            and add_eos_token\n\
    \        ):\n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n\
    \            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"\
    ] = result[\"input_ids\"].copy()\n\n        return result\n\n    def generate_and_tokenize_prompt(data_point):\n\
    \        full_prompt = prompter.generate_prompt(\n            data_point[\"instruction\"\
    ],\n            data_point[\"input\"],\n            data_point[\"output\"],\n\
    \        )\n        tokenized_full_prompt = tokenize(full_prompt)\n        if\
    \ not train_on_inputs:\n            user_prompt = prompter.generate_prompt(\n\
    \                data_point[\"instruction\"], data_point[\"input\"]\n        \
    \    )\n            tokenized_user_prompt = tokenize(\n                user_prompt,\
    \ add_eos_token=add_eos_token\n            )\n            user_prompt_len = len(tokenized_user_prompt[\"\
    input_ids\"])\n\n            if add_eos_token:\n                user_prompt_len\
    \ -= 1\n\n            tokenized_full_prompt[\"labels\"] = [\n                -100\n\
    \            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n      \
    \          user_prompt_len:\n            ]  # could be sped up, probably\n   \
    \     return tokenized_full_prompt\n\n    # model = prepare_model_for_int8_training(model)\n\
    \n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n\
    \        target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n\
    \        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model =\
    \ get_peft_model(model, config)\n\n    if data_path.endswith(\".json\") or data_path.endswith(\"\
    .jsonl\"):\n        data = load_dataset(\"json\", data_files=data_path)\n    else:\n\
    \        data = load_dataset(data_path)\n\n    if resume_from_checkpoint:\n  \
    \      # Check the available weights and load them\n        checkpoint_name =\
    \ os.path.join(\n            resume_from_checkpoint, \"pytorch_model.bin\"\n \
    \       )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n\
    \            checkpoint_name = os.path.join(\n                resume_from_checkpoint,\
    \ \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config above\
    \ has to fit\n            resume_from_checkpoint = (\n                False  #\
    \ So the trainer won't try loading its state\n            )\n        # The two\
    \ files above have a different name depending on how they were saved, but are\
    \ actually the same.\n        if os.path.exists(checkpoint_name):\n          \
    \  print(f\"Restarting from {checkpoint_name}\")\n            adapters_weights\
    \ = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model,\
    \ adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name}\
    \ not found\")\n\n    model.print_trainable_parameters()  # Be more transparent\
    \ about the % of trainable params.\n\n    if val_set_size > 0:\n        train_val\
    \ = data[\"train\"].train_test_split(\n            test_size=val_set_size, shuffle=True,\
    \ seed=42\n        )\n        train_data = (\n            train_val[\"train\"\
    ].shuffle().map(generate_and_tokenize_prompt)\n        )\n        val_data = (\n\
    \            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\
    \        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\
    \        val_data = None\n\n    if not ddp and torch.cuda.device_count() > 1:\n\
    \        # keeps Trainer from trying its own DataParallelism when more than 1\
    \ gpu is available\n        model.is_parallelizable = True\n        model.model_parallel\
    \ = True\n\n    trainer = transformers.Trainer(\n        model=model,\n      \
    \  train_dataset=train_data,\n        eval_dataset=val_data,\n        args=transformers.TrainingArguments(\n\
    \            per_device_train_batch_size=micro_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n\
    \            warmup_steps=100,\n            num_train_epochs=num_epochs,\n   \
    \         learning_rate=learning_rate,\n            fp16=True,\n            logging_steps=10,\n\
    \            optim=\"adamw_torch\",\n            evaluation_strategy=\"steps\"\
    \ if val_set_size > 0 else \"no\",\n            save_strategy=\"steps\",\n   \
    \         eval_steps=200 if val_set_size > 0 else None,\n            save_steps=200,\n\
    \            output_dir=output_dir,\n            save_total_limit=3,\n       \
    \     load_best_model_at_end=True if val_set_size > 0 else False,\n          \
    \  ddp_find_unused_parameters=False if ddp else None,\n            group_by_length=group_by_length,\n\
    \            report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name\
    \ if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n\
    \            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n\
    \        ),\n    )\n    model.config.use_cache = False\n#     config = transformers.AutoConfig.from_pretrained(\n\
    #   'mosaicml/mpt-7b',\n#   trust_remote_code=True\n# )\n    # model.config.trust_remote_code\
    \ = True\n    # model.config.attn_config['attn_impl'] = 'triton'\n\n    old_state_dict\
    \ = model.state_dict\n    model.state_dict = (\n        lambda self, *_, **__:\
    \ get_peft_model_state_dict(\n            self, old_state_dict()\n        )\n\
    \    ).__get__(model, type(model))\n\n    if torch.__version__ >= \"2\" and sys.platform\
    \ != \"win32\":\n        model = torch.compile(model)\n\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\
    \n    model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's\
    \ a warning about missing keys above, please disregard :)\"\n    )\n\n\nif __name__\
    \ == \"__main__\":\n    fire.Fire(train)\n```\n\nI also forgot to modify the query_key_value\
    \ parameter in the previous response. This code should now run smoothly."
  created_at: 2023-05-17 12:39:53+00:00
  edited: false
  hidden: false
  id: 6464d9294855e06b95064804
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-05-17T14:41:18.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: '<p>I''ve made a full training notebook, check it here: <a rel="nofollow"
          href="https://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing">https://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing</a></p>

          '
        raw: 'I''ve made a full training notebook, check it here: https://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing'
        updatedAt: '2023-05-17T14:41:18.738Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - g30rv17ys
    id: 6464e78e6ceebdc7fd893490
    type: comment
  author: cekal
  content: 'I''ve made a full training notebook, check it here: https://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing'
  created_at: 2023-05-17 13:41:18+00:00
  edited: false
  hidden: false
  id: 6464e78e6ceebdc7fd893490
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
      fullname: g30rv1ty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g30rv17ys
      type: user
    createdAt: '2023-05-17T17:29:18.000Z'
    data:
      edited: false
      editors:
      - g30rv17ys
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
          fullname: g30rv1ty
          isHf: false
          isPro: false
          name: g30rv17ys
          type: user
        html: "<p>thank you <span data-props=\"{&quot;user&quot;:&quot;cekal&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cekal\"\
          >@<span class=\"underline\">cekal</span></a></span>\n\n\t</span></span>\
          \ for the notebook. I will be testing it out soon.</p>\n"
        raw: thank you @cekal for the notebook. I will be testing it out soon.
        updatedAt: '2023-05-17T17:29:18.368Z'
      numEdits: 0
      reactions: []
    id: 64650eeee8e31202cb4f2ecc
    type: comment
  author: g30rv17ys
  content: thank you @cekal for the notebook. I will be testing it out soon.
  created_at: 2023-05-17 16:29:18+00:00
  edited: false
  hidden: false
  id: 64650eeee8e31202cb4f2ecc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
      fullname: g30rv1ty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g30rv17ys
      type: user
    createdAt: '2023-05-17T17:31:49.000Z'
    data:
      edited: false
      editors:
      - g30rv17ys
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665920159204-618ccf9219d7d2a706004b98.png?w=200&h=200&f=face
          fullname: g30rv1ty
          isHf: false
          isPro: false
          name: g30rv17ys
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cekal&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cekal\">@<span class=\"\
          underline\">cekal</span></a></span>\n\n\t</span></span> any ideas on how\
          \ you would go about fine-tuning the model on this dataset : <a href=\"\
          https://huggingface.co/datasets/ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\"\
          >https://huggingface.co/datasets/ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered</a><br>can\
          \ you do it directly, or do you have to convert the dataset into any specific\
          \ format?<br>would be great to know how the notebook could be edited to\
          \ maybe train on this dataset. (which line of code)</p>\n"
        raw: '@cekal any ideas on how you would go about fine-tuning the model on
          this dataset : https://huggingface.co/datasets/ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered

          can you do it directly, or do you have to convert the dataset into any specific
          format?

          would be great to know how the notebook could be edited to maybe train on
          this dataset. (which line of code)'
        updatedAt: '2023-05-17T17:31:49.241Z'
      numEdits: 0
      reactions: []
    id: 64650f85e8e31202cb4f38f5
    type: comment
  author: g30rv17ys
  content: '@cekal any ideas on how you would go about fine-tuning the model on this
    dataset : https://huggingface.co/datasets/ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered

    can you do it directly, or do you have to convert the dataset into any specific
    format?

    would be great to know how the notebook could be edited to maybe train on this
    dataset. (which line of code)'
  created_at: 2023-05-17 16:31:49+00:00
  edited: false
  hidden: false
  id: 64650f85e8e31202cb4f38f5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: cekal/mpt-7b-gpt4
repo_type: model
status: open
target_branch: null
title: can you share the dataset used to instruction
