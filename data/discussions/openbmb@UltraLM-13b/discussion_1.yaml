!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-06-29 20:17:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-29T21:17:13.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532092213630676
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hi there</p>

          <p>Thanks for the model, looks good.</p>

          <p>I''m doing quantisations which will be uploaded at:</p>

          <ul>

          <li><a href="https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML">https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML</a></li>

          <li><a href="https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ">https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ</a></li>

          </ul>

          <p>I just wanted to let you know that because you''ve increased the vocab
          size to 32001, this breaks compatibility with the latest GGML quantisation
          methods, called k-quant.</p>

          <p>This may be resolved some time in the future, but for now it means I
          can only release the older formats.</p>

          <p>My understanding is that that extra 32001th token, the PAD, was added
          as something of a hack very early in the history of Llama open source models.
          It was a hack used by one particular model creator I think only because
          they forgot to set up special_tokens_map.json correctly :)  Since then it''s
          stuck around, being copied from model to model, despite not being needed.  Unfortunately
          WizardLM inherited it for example, and a number of models have used their
          code since.</p>

          <p>I''m starting a campaign to try and get it phased out, because it causes
          tons of problems for developers outside the sphere of Python inference.</p>

          <p>Just thought I''d let you know for your next model - and also so I can
          point people to this post when they inevitably ask me why I''ve not released
          the latest quantisation GGML formats for your model :)</p>

          <p>Thanks</p>

          <p>PS. You can read about the issue with k-quants here: <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/issues/1919">https://github.com/ggerganov/llama.cpp/issues/1919</a></p>

          '
        raw: 'Hi there


          Thanks for the model, looks good.


          I''m doing quantisations which will be uploaded at:

          - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML

          - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ


          I just wanted to let you know that because you''ve increased the vocab size
          to 32001, this breaks compatibility with the latest GGML quantisation methods,
          called k-quant.


          This may be resolved some time in the future, but for now it means I can
          only release the older formats.


          My understanding is that that extra 32001th token, the PAD, was added as
          something of a hack very early in the history of Llama open source models.
          It was a hack used by one particular model creator I think only because
          they forgot to set up special_tokens_map.json correctly :)  Since then it''s
          stuck around, being copied from model to model, despite not being needed.  Unfortunately
          WizardLM inherited it for example, and a number of models have used their
          code since.


          I''m starting a campaign to try and get it phased out, because it causes
          tons of problems for developers outside the sphere of Python inference.


          Just thought I''d let you know for your next model - and also so I can point
          people to this post when they inevitably ask me why I''ve not released the
          latest quantisation GGML formats for your model :)


          Thanks


          PS. You can read about the issue with k-quants here: https://github.com/ggerganov/llama.cpp/issues/1919'
        updatedAt: '2023-06-30T07:50:39.938Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - nacs
        - YearZero
        - stingning
        - ignmilton
        - nixjoe
    id: 649df4d913e6f65bf6c2e2ec
    type: comment
  author: TheBloke
  content: 'Hi there


    Thanks for the model, looks good.


    I''m doing quantisations which will be uploaded at:

    - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML

    - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ


    I just wanted to let you know that because you''ve increased the vocab size to
    32001, this breaks compatibility with the latest GGML quantisation methods, called
    k-quant.


    This may be resolved some time in the future, but for now it means I can only
    release the older formats.


    My understanding is that that extra 32001th token, the PAD, was added as something
    of a hack very early in the history of Llama open source models. It was a hack
    used by one particular model creator I think only because they forgot to set up
    special_tokens_map.json correctly :)  Since then it''s stuck around, being copied
    from model to model, despite not being needed.  Unfortunately WizardLM inherited
    it for example, and a number of models have used their code since.


    I''m starting a campaign to try and get it phased out, because it causes tons
    of problems for developers outside the sphere of Python inference.


    Just thought I''d let you know for your next model - and also so I can point people
    to this post when they inevitably ask me why I''ve not released the latest quantisation
    GGML formats for your model :)


    Thanks


    PS. You can read about the issue with k-quants here: https://github.com/ggerganov/llama.cpp/issues/1919'
  created_at: 2023-06-29 20:17:13+00:00
  edited: true
  hidden: false
  id: 649df4d913e6f65bf6c2e2ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12bcd18d215abf91f297f93007733148.svg
      fullname: Ning Ding
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stingning
      type: user
    createdAt: '2023-06-30T03:04:46.000Z'
    data:
      edited: false
      editors:
      - stingning
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9453011155128479
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12bcd18d215abf91f297f93007733148.svg
          fullname: Ning Ding
          isHf: false
          isPro: false
          name: stingning
          type: user
        html: '<blockquote>

          <p>Hi there</p>

          <p>Thanks for the model, looks good.</p>

          <p>I''m doing quantisations which will be uploaded at:</p>

          <ul>

          <li><a href="https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML">https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML</a></li>

          <li><a href="https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ">https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ</a></li>

          </ul>

          <p>I just wanted to let you know that because you''ve increased the vocab
          size to 32001, this breaks compatibility with the latest GGML quantisation
          methods, called k-quant.</p>

          <p>This may be resolved some time in the future, but for now it means I
          can only release the older formats.</p>

          <p>My understanding is that that extra 32001th token, the PAD, was added
          as something of a hack very early in the history of Llama open source models.
          It was a hack used by one particular model creator I think only because
          they forgot to set up special_tokens_map.json correctly :)  Since then it''s
          stuck around, being copied from model to model, despite not being needed.  Unfortunately
          WizardLM inherited it for example, and a number of models have used their
          code since.</p>

          <p>I''m starting a campaign to try and get it phased out, because it causes
          tons of problems for developers outside the sphere of Python inference.</p>

          <p>Just thought I''d let you know for your next model - and also so I can
          point people to this post when they inevitably ask me why I''ve not released
          the latest quantisation GGML formats for your model :)</p>

          <p>Thanks</p>

          </blockquote>

          <p>Hi there, </p>

          <p>Thanks for the message!    We will eliminate the token in the next models.
          Thanks again!</p>

          '
        raw: "> Hi there\n> \n> Thanks for the model, looks good.\n> \n> I'm doing\
          \ quantisations which will be uploaded at:\n> - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML\n\
          > - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ\n> \n> I just\
          \ wanted to let you know that because you've increased the vocab size to\
          \ 32001, this breaks compatibility with the latest GGML quantisation methods,\
          \ called k-quant.\n> \n> This may be resolved some time in the future, but\
          \ for now it means I can only release the older formats.\n> \n> My understanding\
          \ is that that extra 32001th token, the PAD, was added as something of a\
          \ hack very early in the history of Llama open source models. It was a hack\
          \ used by one particular model creator I think only because they forgot\
          \ to set up special_tokens_map.json correctly :)  Since then it's stuck\
          \ around, being copied from model to model, despite not being needed.  Unfortunately\
          \ WizardLM inherited it for example, and a number of models have used their\
          \ code since.\n> \n> I'm starting a campaign to try and get it phased out,\
          \ because it causes tons of problems for developers outside the sphere of\
          \ Python inference.\n> \n> Just thought I'd let you know for your next model\
          \ - and also so I can point people to this post when they inevitably ask\
          \ me why I've not released the latest quantisation GGML formats for your\
          \ model :)\n> \n> Thanks\n\nHi there, \n\nThanks for the message!    We\
          \ will eliminate the token in the next models. Thanks again!\n\n"
        updatedAt: '2023-06-30T03:04:46.403Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 649e464e5379dc792b812317
    type: comment
  author: stingning
  content: "> Hi there\n> \n> Thanks for the model, looks good.\n> \n> I'm doing quantisations\
    \ which will be uploaded at:\n> - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GGML\n\
    > - https://huggingface.co/openbmb/TheBloke/UltraLM-13B-GPTQ\n> \n> I just wanted\
    \ to let you know that because you've increased the vocab size to 32001, this\
    \ breaks compatibility with the latest GGML quantisation methods, called k-quant.\n\
    > \n> This may be resolved some time in the future, but for now it means I can\
    \ only release the older formats.\n> \n> My understanding is that that extra 32001th\
    \ token, the PAD, was added as something of a hack very early in the history of\
    \ Llama open source models. It was a hack used by one particular model creator\
    \ I think only because they forgot to set up special_tokens_map.json correctly\
    \ :)  Since then it's stuck around, being copied from model to model, despite\
    \ not being needed.  Unfortunately WizardLM inherited it for example, and a number\
    \ of models have used their code since.\n> \n> I'm starting a campaign to try\
    \ and get it phased out, because it causes tons of problems for developers outside\
    \ the sphere of Python inference.\n> \n> Just thought I'd let you know for your\
    \ next model - and also so I can point people to this post when they inevitably\
    \ ask me why I've not released the latest quantisation GGML formats for your model\
    \ :)\n> \n> Thanks\n\nHi there, \n\nThanks for the message!    We will eliminate\
    \ the token in the next models. Thanks again!\n\n"
  created_at: 2023-06-30 02:04:46+00:00
  edited: false
  hidden: false
  id: 649e464e5379dc792b812317
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2d5b9f0bd91e9a9c5d8d056229b10b9.svg
      fullname: TianZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kirayz
      type: user
    createdAt: '2023-07-03T05:19:21.000Z'
    data:
      edited: false
      editors:
      - kirayz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9019426703453064
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2d5b9f0bd91e9a9c5d8d056229b10b9.svg
          fullname: TianZ
          isHf: false
          isPro: false
          name: kirayz
          type: user
        html: '<p>Dumb question: Is there any possibility that we can manually eliminate
          the extra token (PAD)? If possible at all, what''d be some pointers that
          we can chase? Thank you! If not possible, what''d be the reasoning? Curious
          to know. Thanks for the great model! </p>

          '
        raw: 'Dumb question: Is there any possibility that we can manually eliminate
          the extra token (PAD)? If possible at all, what''d be some pointers that
          we can chase? Thank you! If not possible, what''d be the reasoning? Curious
          to know. Thanks for the great model! '
        updatedAt: '2023-07-03T05:19:21.207Z'
      numEdits: 0
      reactions: []
    id: 64a25a59660cce8b86d9e10b
    type: comment
  author: kirayz
  content: 'Dumb question: Is there any possibility that we can manually eliminate
    the extra token (PAD)? If possible at all, what''d be some pointers that we can
    chase? Thank you! If not possible, what''d be the reasoning? Curious to know.
    Thanks for the great model! '
  created_at: 2023-07-03 04:19:21+00:00
  edited: false
  hidden: false
  id: 64a25a59660cce8b86d9e10b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: openbmb/UltraLM-13b
repo_type: model
status: open
target_branch: null
title: Vocab size 32001 causes problems for quantisation
