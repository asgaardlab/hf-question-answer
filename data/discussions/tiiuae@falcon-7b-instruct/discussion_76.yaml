!!python/object:huggingface_hub.community.DiscussionWithDetails
author: archonlith
conflicting_files: null
created_at: 2023-08-23 14:23:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/adLUysfQCBOaoYk8LVGBD.jpeg?w=200&h=200&f=face
      fullname: JEREMY D GAMET
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: archonlith
      type: user
    createdAt: '2023-08-23T15:23:39.000Z'
    data:
      edited: false
      editors:
      - archonlith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5666244626045227
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/adLUysfQCBOaoYk8LVGBD.jpeg?w=200&h=200&f=face
          fullname: JEREMY D GAMET
          isHf: false
          isPro: false
          name: archonlith
          type: user
        html: "<p>I'm just getting started running some generation scenarios but I'd\
          \ prefer not to spend all day finding the best parameters for speed/accuracy\
          \ to be optimized. What works for you configuration wise?</p>\n<h3 id=\"\
          i-have-been-testing-with\">I have been testing with:</h3>\n<pre><code>model\
          \ = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          model = AutoModelForCausalLM.from_pretrained (model, trust_remote_code=True)\n\
          \nmodel.config.max_new_tokens = 2000\ngen_cfg = GenerationConfig.from_model_config(model.config)\n\
          gen_cfg.max_new_tokens = 2000\ngen_cfg.max_time = 90.0\n\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n    generation_config=gen_cfg\n)\n</code></pre>\n"
        raw: "I'm just getting started running some generation scenarios but I'd prefer\
          \ not to spend all day finding the best parameters for speed/accuracy to\
          \ be optimized. What works for you configuration wise?\r\n\r\n### I have\
          \ been testing with:\r\n```\r\nmodel = \"tiiuae/falcon-7b-instruct\"\r\n\
          \r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\nmodel = AutoModelForCausalLM.from_pretrained\
          \ (model, trust_remote_code=True)\r\n\r\nmodel.config.max_new_tokens = 2000\r\
          \ngen_cfg = GenerationConfig.from_model_config(model.config)\r\ngen_cfg.max_new_tokens\
          \ = 2000\r\ngen_cfg.max_time = 90.0\r\n\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\",\r\n    generation_config=gen_cfg\r\n)\r\n```"
        updatedAt: '2023-08-23T15:23:39.977Z'
      numEdits: 0
      reactions: []
    id: 64e6247bcb5c8fbcd291db20
    type: comment
  author: archonlith
  content: "I'm just getting started running some generation scenarios but I'd prefer\
    \ not to spend all day finding the best parameters for speed/accuracy to be optimized.\
    \ What works for you configuration wise?\r\n\r\n### I have been testing with:\r\
    \n```\r\nmodel = \"tiiuae/falcon-7b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained (model, trust_remote_code=True)\r\
    \n\r\nmodel.config.max_new_tokens = 2000\r\ngen_cfg = GenerationConfig.from_model_config(model.config)\r\
    \ngen_cfg.max_new_tokens = 2000\r\ngen_cfg.max_time = 90.0\r\n\r\npipeline = transformers.pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n    device_map=\"\
    auto\",\r\n    generation_config=gen_cfg\r\n)\r\n```"
  created_at: 2023-08-23 14:23:39+00:00
  edited: false
  hidden: false
  id: 64e6247bcb5c8fbcd291db20
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 76
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: 'Share your recommended configurations for speed. '
