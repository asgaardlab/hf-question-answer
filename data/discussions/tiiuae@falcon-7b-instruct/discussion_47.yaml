!!python/object:huggingface_hub.community.DiscussionWithDetails
author: subhashhf
conflicting_files: null
created_at: 2023-06-23 11:21:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f1b5798f52cb06614726b1a0462ea1e8.svg
      fullname: Subhash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: subhashhf
      type: user
    createdAt: '2023-06-23T12:21:30.000Z'
    data:
      edited: false
      editors:
      - subhashhf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.558057963848114
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f1b5798f52cb06614726b1a0462ea1e8.svg
          fullname: Subhash
          isHf: false
          isPro: false
          name: subhashhf
          type: user
        html: '<p>Hi , </p>

          <p>I have cloned the repo using git lfs clone </p>

          <p>I am using the following code </p>

          <pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering


          # specify the local path where your model is

          model_path = "/this/is/local/path/falcon-7b-instruct"


          # Load the model and the tokenizer

          # Load the model and the tokenizer with trust_remote_code=True

          tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

          model = AutoModelForQuestionAnswering.from_pretrained(model_path, trust_remote_code=True)


          # Now, you can use the model to answer questions

          question = "What is the capital of France?"

          context = "Paris is the capital and most populous city of France."


          # you need to encode the input

          inputs = tokenizer.encode_plus(question, context, return_tensors=''pt'')


          # get model output

          answer_start_scores, answer_end_scores = model(**inputs)


          # Get the most likely beginning and end of answer with the argmax of the
          scores

          answer_start = torch.argmax(answer_start_scores)

          answer_end = torch.argmax(answer_end_scores) + 1


          # Get the answer. Convert the tokens to strings and join them

          answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end]))


          print(answer)

          </code></pre>

          <p>I get the following error </p>

          <pre><code>ValueError: Unrecognized configuration class &lt;class ''transformers_modules.falcon-7b-instruct.configuration_RW.RWConfig''&gt;
          for this kind of AutoModel: AutoModelForQuestionAnswering.

          Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig,
          BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig,
          Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig,
          ErnieConfig, ErnieMConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config,
          GPTNeoConfig, GPTNeoXConfig, GPTJConfig, IBertConfig, LayoutLMv2Config,
          LayoutLMv3Config, LEDConfig, LiltConfig, LongformerConfig, LukeConfig, LxmertConfig,
          MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig,
          MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, OPTConfig, QDQBertConfig,
          ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig,
          RoCBertConfig, RoFormerConfig, SplinterConfig, SqueezeBertConfig, XLMConfig,
          XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.

          </code></pre>

          <p>Please help me with this.  </p>

          '
        raw: "Hi , \r\n\r\nI have cloned the repo using git lfs clone \r\n\r\nI am\
          \ using the following code \r\n\r\n```\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForQuestionAnswering\r\n\r\n# specify the local path where your\
          \ model is\r\nmodel_path = \"/this/is/local/path/falcon-7b-instruct\"\r\n\
          \r\n# Load the model and the tokenizer\r\n# Load the model and the tokenizer\
          \ with trust_remote_code=True\r\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)\r\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path,\
          \ trust_remote_code=True)\r\n\r\n# Now, you can use the model to answer\
          \ questions\r\nquestion = \"What is the capital of France?\"\r\ncontext\
          \ = \"Paris is the capital and most populous city of France.\"\r\n\r\n#\
          \ you need to encode the input\r\ninputs = tokenizer.encode_plus(question,\
          \ context, return_tensors='pt')\r\n\r\n# get model output\r\nanswer_start_scores,\
          \ answer_end_scores = model(**inputs)\r\n\r\n# Get the most likely beginning\
          \ and end of answer with the argmax of the scores\r\nanswer_start = torch.argmax(answer_start_scores)\r\
          \nanswer_end = torch.argmax(answer_end_scores) + 1\r\n\r\n# Get the answer.\
          \ Convert the tokens to strings and join them\r\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"\
          input_ids\"][0][answer_start:answer_end]))\r\n\r\nprint(answer)\r\n```\r\
          \n\r\nI get the following error \r\n\r\n\r\n```\r\nValueError: Unrecognized\
          \ configuration class <class 'transformers_modules.falcon-7b-instruct.configuration_RW.RWConfig'>\
          \ for this kind of AutoModel: AutoModelForQuestionAnswering.\r\nModel type\
          \ should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig,\
          \ BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig,\
          \ Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig,\
          \ ElectraConfig, ErnieConfig, ErnieMConfig, FlaubertConfig, FNetConfig,\
          \ FunnelConfig, GPT2Config, GPTNeoConfig, GPTNeoXConfig, GPTJConfig, IBertConfig,\
          \ LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LongformerConfig,\
          \ LukeConfig, LxmertConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig,\
          \ MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig,\
          \ OPTConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SplinterConfig,\
          \ SqueezeBertConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig,\
          \ XmodConfig, YosoConfig.\r\n```\r\n\r\nPlease help me with this.  "
        updatedAt: '2023-06-23T12:21:30.262Z'
      numEdits: 0
      reactions: []
    id: 64958e4a2cfd44d23620814b
    type: comment
  author: subhashhf
  content: "Hi , \r\n\r\nI have cloned the repo using git lfs clone \r\n\r\nI am using\
    \ the following code \r\n\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\r\
    \n\r\n# specify the local path where your model is\r\nmodel_path = \"/this/is/local/path/falcon-7b-instruct\"\
    \r\n\r\n# Load the model and the tokenizer\r\n# Load the model and the tokenizer\
    \ with trust_remote_code=True\r\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ trust_remote_code=True)\r\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path,\
    \ trust_remote_code=True)\r\n\r\n# Now, you can use the model to answer questions\r\
    \nquestion = \"What is the capital of France?\"\r\ncontext = \"Paris is the capital\
    \ and most populous city of France.\"\r\n\r\n# you need to encode the input\r\n\
    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\r\n\r\n\
    # get model output\r\nanswer_start_scores, answer_end_scores = model(**inputs)\r\
    \n\r\n# Get the most likely beginning and end of answer with the argmax of the\
    \ scores\r\nanswer_start = torch.argmax(answer_start_scores)\r\nanswer_end = torch.argmax(answer_end_scores)\
    \ + 1\r\n\r\n# Get the answer. Convert the tokens to strings and join them\r\n\
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"\
    input_ids\"][0][answer_start:answer_end]))\r\n\r\nprint(answer)\r\n```\r\n\r\n\
    I get the following error \r\n\r\n\r\n```\r\nValueError: Unrecognized configuration\
    \ class <class 'transformers_modules.falcon-7b-instruct.configuration_RW.RWConfig'>\
    \ for this kind of AutoModel: AutoModelForQuestionAnswering.\r\nModel type should\
    \ be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig,\
    \ BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, Data2VecTextConfig,\
    \ DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig,\
    \ ErnieMConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPTNeoConfig,\
    \ GPTNeoXConfig, GPTJConfig, IBertConfig, LayoutLMv2Config, LayoutLMv3Config,\
    \ LEDConfig, LiltConfig, LongformerConfig, LukeConfig, LxmertConfig, MarkupLMConfig,\
    \ MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig,\
    \ MvpConfig, NezhaConfig, NystromformerConfig, OPTConfig, QDQBertConfig, ReformerConfig,\
    \ RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig,\
    \ SplinterConfig, SqueezeBertConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig,\
    \ XLNetConfig, XmodConfig, YosoConfig.\r\n```\r\n\r\nPlease help me with this.\
    \  "
  created_at: 2023-06-23 11:21:30+00:00
  edited: false
  hidden: false
  id: 64958e4a2cfd44d23620814b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
      fullname: Bruce D'Ambrosio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bdambrosio
      type: user
    createdAt: '2023-06-24T01:55:30.000Z'
    data:
      edited: false
      editors:
      - bdambrosio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9748815894126892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
          fullname: Bruce D'Ambrosio
          isHf: false
          isPro: false
          name: bdambrosio
          type: user
        html: '<p>Pipeline works out of the box.<br>you can find that file, though,
          in the model folder (model_path), along with a couple of others you may
          need. just import them</p>

          '
        raw: 'Pipeline works out of the box.

          you can find that file, though, in the model folder (model_path), along
          with a couple of others you may need. just import them'
        updatedAt: '2023-06-24T01:55:30.634Z'
      numEdits: 0
      reactions: []
    id: 64964d122282a5257b428831
    type: comment
  author: bdambrosio
  content: 'Pipeline works out of the box.

    you can find that file, though, in the model folder (model_path), along with a
    couple of others you may need. just import them'
  created_at: 2023-06-24 00:55:30+00:00
  edited: false
  hidden: false
  id: 64964d122282a5257b428831
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/o8Ulf7Tan062ifN8nXZIN.png?w=200&h=200&f=face
      fullname: jimbo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cr00
      type: user
    createdAt: '2023-06-30T04:31:41.000Z'
    data:
      edited: false
      editors:
      - cr00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.99016273021698
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/o8Ulf7Tan062ifN8nXZIN.png?w=200&h=200&f=face
          fullname: jimbo
          isHf: false
          isPro: false
          name: cr00
          type: user
        html: '<p>I may be wrong but I believe they mentioned that this is due to
          some update needed in huggingface? but that it could be ignored</p>

          '
        raw: I may be wrong but I believe they mentioned that this is due to some
          update needed in huggingface? but that it could be ignored
        updatedAt: '2023-06-30T04:31:41.750Z'
      numEdits: 0
      reactions: []
    id: 649e5aad1b8a453e899970fd
    type: comment
  author: cr00
  content: I may be wrong but I believe they mentioned that this is due to some update
    needed in huggingface? but that it could be ignored
  created_at: 2023-06-30 03:31:41+00:00
  edited: false
  hidden: false
  id: 649e5aad1b8a453e899970fd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 47
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: unable to use falcon-7b-instruct using transformers
