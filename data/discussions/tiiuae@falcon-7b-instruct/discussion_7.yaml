!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aviadatlas
conflicting_files: null
created_at: 2023-05-28 12:29:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11fa20f8a1563534b14348def4d0c8bc.svg
      fullname: Aviad Atlas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aviadatlas
      type: user
    createdAt: '2023-05-28T13:29:28.000Z'
    data:
      edited: false
      editors:
      - aviadatlas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11fa20f8a1563534b14348def4d0c8bc.svg
          fullname: Aviad Atlas
          isHf: false
          isPro: false
          name: aviadatlas
          type: user
        html: "<p>Hi,</p>\n<p>When I simply run the example code, I'm able to download\
          \ the model, but then get an error:</p>\n<p>\u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502       \
          \                                                               \u2502<br>\u2502\
          \                                                                      \
          \        \u2502<br>\u2502    5 model = \"tiiuae/falcon-7b-instruct\"   \
          \                                  \u2502<br>\u2502    6               \
          \                                                          \u2502<br>\u2502\
          \    7 tokenizer = AutoTokenizer.from_pretrained(model)                \
          \        \u2502<br>\u2502 \u2771  8 pipeline = transformers.pipeline(  \
          \                                     \u2502<br>\u2502    9 \u2502   \"\
          text-generation\",                                                  \u2502\
          <br>\u2502   10 \u2502   model=model,                                  \
          \                      \u2502<br>\u2502   11 \u2502   tokenizer=tokenizer,\
          \                                                \u2502<br>\u2502      \
          \                                                                      \
          \  \u2502<br>\u2502 transformers/pipelines/<strong>init</strong>.py:788\
          \ in pipeline                           \u2502<br>\u2502               \
          \                                                               \u2502<br>\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None       \u2502<br>\u2502   786 \u2502   # Will load the correct model\
          \ if possible                          \u2502<br>\u2502   787 \u2502   model_classes\
          \ = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"p \u2502<br>\u2502\
          \ \u2771 788 \u2502   framework, model = infer_framework_load_model(   \
          \                  \u2502<br>\u2502   789 \u2502   \u2502   model,     \
          \                                                    \u2502<br>\u2502  \
          \ 790 \u2502   \u2502   model_classes=model_classes,                   \
          \                \u2502<br>\u2502   791 \u2502   \u2502   config=config,\
          \                                                 \u2502<br>\u2502     \
          \                                                                      \
          \   \u2502<br>\u2502 transformers/pipelines/base.py:279 in infer_framework_load_model\
          \             \u2502<br>\u2502                                         \
          \                                     \u2502<br>\u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \     \u2502<br>\u2502    277 \u2502   \u2502                          \
          \                                       \u2502<br>\u2502    278 \u2502 \
          \  \u2502   if isinstance(model, str):                                 \
          \   \u2502<br>\u2502 \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"\
          Could not load model {model} with any  \u2502<br>\u2502    280 \u2502  \
          \                                                                   \u2502\
          <br>\u2502    281 \u2502   framework = \"tf\" if \"keras.engine.training.Model\"\
          \ in str(inspect. \u2502<br>\u2502    282 \u2502   return framework, model\
          \                                           \u2502<br>\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256F<br>ValueError: Could not load model tiiuae/falcon-7b-instruct\
          \ with any of the<br>following classes: (&lt;class<br>'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class<br>'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).</p>\n\
          <p>Can someone please assist? I upgraded transformers and torch to the most\
          \ up to date versions. I'm running this on macOS 13.3.1, M1 Pro</p>\n"
        raw: "Hi,\r\n\r\nWhen I simply run the example code, I'm able to download\
          \ the model, but then get an error:\r\n\r\n\u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 <module>\
          \                                                                     \u2502\
          \r\n\u2502                                                             \
          \                 \u2502\r\n\u2502    5 model = \"tiiuae/falcon-7b-instruct\"\
          \                                     \u2502\r\n\u2502    6            \
          \                                                             \u2502\r\n\
          \u2502    7 tokenizer = AutoTokenizer.from_pretrained(model)           \
          \             \u2502\r\n\u2502 \u2771  8 pipeline = transformers.pipeline(\
          \                                       \u2502\r\n\u2502    9 \u2502   \"\
          text-generation\",                                                  \u2502\
          \r\n\u2502   10 \u2502   model=model,                                  \
          \                      \u2502\r\n\u2502   11 \u2502   tokenizer=tokenizer,\
          \                                                \u2502\r\n\u2502      \
          \                                                                      \
          \  \u2502\r\n\u2502 transformers/pipelines/__init__.py:788 in pipeline \
          \                          \u2502\r\n\u2502                            \
          \                                                  \u2502\r\n\u2502   785\
          \ \u2502   # Forced if framework already defined, inferred if it's None\
          \       \u2502\r\n\u2502   786 \u2502   # Will load the correct model if\
          \ possible                          \u2502\r\n\u2502   787 \u2502   model_classes\
          \ = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"p \u2502\r\n\
          \u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
          \                     \u2502\r\n\u2502   789 \u2502   \u2502   model,  \
          \                                                       \u2502\r\n\u2502\
          \   790 \u2502   \u2502   model_classes=model_classes,                 \
          \                  \u2502\r\n\u2502   791 \u2502   \u2502   config=config,\
          \                                                 \u2502\r\n\u2502     \
          \                                                                      \
          \   \u2502\r\n\u2502 transformers/pipelines/base.py:279 in infer_framework_load_model\
          \             \u2502\r\n\u2502                                         \
          \                                     \u2502\r\n\u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \     \u2502\r\n\u2502    277 \u2502   \u2502                          \
          \                                       \u2502\r\n\u2502    278 \u2502 \
          \  \u2502   if isinstance(model, str):                                 \
          \   \u2502\r\n\u2502 \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"\
          Could not load model {model} with any  \u2502\r\n\u2502    280 \u2502  \
          \                                                                   \u2502\
          \r\n\u2502    281 \u2502   framework = \"tf\" if \"keras.engine.training.Model\"\
          \ in str(inspect. \u2502\r\n\u2502    282 \u2502   return framework, model\
          \                                           \u2502\r\n\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256F\r\nValueError: Could not load model tiiuae/falcon-7b-instruct\
          \ with any of the \r\nfollowing classes: (<class \r\n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \r\n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\r\
          \n\r\nCan someone please assist? I upgraded transformers and torch to the\
          \ most up to date versions. I'm running this on macOS 13.3.1, M1 Pro"
        updatedAt: '2023-05-28T13:29:28.274Z'
      numEdits: 0
      reactions: []
    id: 6473573816ef9c21dba69242
    type: comment
  author: aviadatlas
  content: "Hi,\r\n\r\nWhen I simply run the example code, I'm able to download the\
    \ model, but then get an error:\r\n\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u256E\r\n\u2502 <module>                                  \
    \                                   \u2502\r\n\u2502                         \
    \                                                     \u2502\r\n\u2502    5 model\
    \ = \"tiiuae/falcon-7b-instruct\"                                     \u2502\r\
    \n\u2502    6                                                                \
    \         \u2502\r\n\u2502    7 tokenizer = AutoTokenizer.from_pretrained(model)\
    \                        \u2502\r\n\u2502 \u2771  8 pipeline = transformers.pipeline(\
    \                                       \u2502\r\n\u2502    9 \u2502   \"text-generation\"\
    ,                                                  \u2502\r\n\u2502   10 \u2502\
    \   model=model,                                                        \u2502\
    \r\n\u2502   11 \u2502   tokenizer=tokenizer,                                \
    \                \u2502\r\n\u2502                                            \
    \                                  \u2502\r\n\u2502 transformers/pipelines/__init__.py:788\
    \ in pipeline                           \u2502\r\n\u2502                     \
    \                                                         \u2502\r\n\u2502   785\
    \ \u2502   # Forced if framework already defined, inferred if it's None      \
    \ \u2502\r\n\u2502   786 \u2502   # Will load the correct model if possible  \
    \                        \u2502\r\n\u2502   787 \u2502   model_classes = {\"tf\"\
    : targeted_task[\"tf\"], \"pt\": targeted_task[\"p \u2502\r\n\u2502 \u2771 788\
    \ \u2502   framework, model = infer_framework_load_model(                    \
    \ \u2502\r\n\u2502   789 \u2502   \u2502   model,                            \
    \                             \u2502\r\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
    \                                   \u2502\r\n\u2502   791 \u2502   \u2502   config=config,\
    \                                                 \u2502\r\n\u2502           \
    \                                                                   \u2502\r\n\
    \u2502 transformers/pipelines/base.py:279 in infer_framework_load_model      \
    \       \u2502\r\n\u2502                                                     \
    \                         \u2502\r\n\u2502    276 \u2502   \u2502   \u2502   \u2502\
    \   continue                                              \u2502\r\n\u2502   \
    \ 277 \u2502   \u2502                                                        \
    \         \u2502\r\n\u2502    278 \u2502   \u2502   if isinstance(model, str):\
    \                                    \u2502\r\n\u2502 \u2771  279 \u2502   \u2502\
    \   \u2502   raise ValueError(f\"Could not load model {model} with any  \u2502\
    \r\n\u2502    280 \u2502                                                     \
    \                \u2502\r\n\u2502    281 \u2502   framework = \"tf\" if \"keras.engine.training.Model\"\
    \ in str(inspect. \u2502\r\n\u2502    282 \u2502   return framework, model   \
    \                                        \u2502\r\n\u2570\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nValueError: Could\
    \ not load model tiiuae/falcon-7b-instruct with any of the \r\nfollowing classes:\
    \ (<class \r\n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class \r\n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\r\
    \n\r\nCan someone please assist? I upgraded transformers and torch to the most\
    \ up to date versions. I'm running this on macOS 13.3.1, M1 Pro"
  created_at: 2023-05-28 12:29:28+00:00
  edited: false
  hidden: false
  id: 6473573816ef9c21dba69242
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
      fullname: 'Sasan '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sasanou
      type: user
    createdAt: '2023-05-28T16:22:33.000Z'
    data:
      edited: true
      editors:
      - Sasanou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
          fullname: 'Sasan '
          isHf: false
          isPro: false
          name: Sasanou
          type: user
        html: '<p>Same here - also on a MB M1 Pro</p>

          '
        raw: Same here - also on a MB M1 Pro
        updatedAt: '2023-05-28T16:23:16.675Z'
      numEdits: 1
      reactions: []
    id: 64737fc92a74fb43ccdf9cec
    type: comment
  author: Sasanou
  content: Same here - also on a MB M1 Pro
  created_at: 2023-05-28 15:22:33+00:00
  edited: true
  hidden: false
  id: 64737fc92a74fb43ccdf9cec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e644992d17fe0a6f97d8930a6aa64aa5.svg
      fullname: Vladimir P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CleverShovel
      type: user
    createdAt: '2023-05-28T17:55:56.000Z'
    data:
      edited: false
      editors:
      - CleverShovel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e644992d17fe0a6f97d8930a6aa64aa5.svg
          fullname: Vladimir P
          isHf: false
          isPro: false
          name: CleverShovel
          type: user
        html: "<p>Perhaps you need to install the latest versions of libraries from\
          \ github. There is an example how to install them. Probably you need not\
          \ all of them, but maybe useful</p>\n<pre><code>pip install -q -U bitsandbytes\n\
          pip install -q -U git+https://github.com/huggingface/transformers.git \n\
          pip install -q -U git+https://github.com/huggingface/peft.git\npip install\
          \ -q -U git+https://github.com/huggingface/accelerate.git\npip install -q\
          \ einops\n</code></pre>\n"
        raw: "Perhaps you need to install the latest versions of libraries from github.\
          \ There is an example how to install them. Probably you need not all of\
          \ them, but maybe useful\n```\npip install -q -U bitsandbytes\npip install\
          \ -q -U git+https://github.com/huggingface/transformers.git \npip install\
          \ -q -U git+https://github.com/huggingface/peft.git\npip install -q -U git+https://github.com/huggingface/accelerate.git\n\
          pip install -q einops\n```"
        updatedAt: '2023-05-28T17:55:56.651Z'
      numEdits: 0
      reactions: []
    id: 647395ac63001a0002ce2e10
    type: comment
  author: CleverShovel
  content: "Perhaps you need to install the latest versions of libraries from github.\
    \ There is an example how to install them. Probably you need not all of them,\
    \ but maybe useful\n```\npip install -q -U bitsandbytes\npip install -q -U git+https://github.com/huggingface/transformers.git\
    \ \npip install -q -U git+https://github.com/huggingface/peft.git\npip install\
    \ -q -U git+https://github.com/huggingface/accelerate.git\npip install -q einops\n\
    ```"
  created_at: 2023-05-28 16:55:56+00:00
  edited: false
  hidden: false
  id: 647395ac63001a0002ce2e10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11fa20f8a1563534b14348def4d0c8bc.svg
      fullname: Aviad Atlas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aviadatlas
      type: user
    createdAt: '2023-05-28T18:51:07.000Z'
    data:
      edited: true
      editors:
      - aviadatlas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11fa20f8a1563534b14348def4d0c8bc.svg
          fullname: Aviad Atlas
          isHf: false
          isPro: false
          name: aviadatlas
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CleverShovel&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CleverShovel\"\
          >@<span class=\"underline\">CleverShovel</span></a></span>\n\n\t</span></span>\
          \ Thanks, I did this, but it didn't solve it. Oddly enough though the error\
          \ just changed a bit:</p>\n<p>\u2502 transformers/pipelines/base.py:278\
          \ in infer_framework_load_model             \u2502<br>\u2502           \
          \                                                                   \u2502\
          <br>\u2502    275 \u2502   \u2502   \u2502   \u2502   continue         \
          \                                     \u2502<br>\u2502    276 \u2502   \u2502\
          \                                                                 \u2502\
          <br>\u2502    277 \u2502   \u2502   if isinstance(model, str):         \
          \                           \u2502<br>\u2502 \u2771  278 \u2502   \u2502\
          \   \u2502   raise ValueError(f\"Could not load model {model} with any \
          \ \u2502<br>\u2502    279 \u2502                                       \
          \                              \u2502<br>\u2502    280 \u2502   framework\
          \ = infer_framework(model.<strong>class</strong>)                      \u2502\
          <br>\u2502    281 \u2502   return framework, model                     \
          \                      \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F<br>ValueError: Could not load model tiiuae/falcon-7b-instruct with\
          \ any of the<br>following classes: (&lt;class<br>'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,).</p>\n\
          <p>I also tried running it on Colab and get the same issue:</p>\n<pre><code>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 in &lt;cell\
          \ line: 8&gt;:8                                                        \
          \                      \u2502\n\u2502                                  \
          \                                                                \u2502\n\
          \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
          \ in pipeline       \u2502\n\u2502                                     \
          \                                                             \u2502\n\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None                           \u2502\n\u2502   786 \u2502   # Will load\
          \ the correct model if possible                                        \
          \      \u2502\n\u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}                 \u2502\n\u2502 \u2771\
          \ 788 \u2502   framework, model = infer_framework_load_model(          \
          \                               \u2502\n\u2502   789 \u2502   \u2502   model,\
          \                                                                      \
          \       \u2502\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n\u2502 \
          \  791 \u2502   \u2502   config=config,                                \
          \                                     \u2502\n\u2502                   \
          \                                                                      \
          \         \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:278\
          \ in                    \u2502\n\u2502 infer_framework_load_model      \
          \                                                                 \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502    275 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502\n\u2502    276 \u2502   \u2502        \
          \                                                                      \
          \       \u2502\n\u2502    277 \u2502   \u2502   if isinstance(model, str):\
          \                                                        \u2502\n\u2502\
          \ \u2771  278 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load\
          \ model {model} with any of the following cl  \u2502\n\u2502    279 \u2502\
          \                                                                      \
          \                   \u2502\n\u2502    280 \u2502   framework = infer_framework(model.__class__)\
          \                                          \u2502\n\u2502    281 \u2502\
          \   return framework, model                                            \
          \                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError: Could not\
          \ load model tiiuae/falcon-7b-instruct with any of the following classes:\
          \ (&lt;class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).\n\
          </code></pre>\n"
        raw: "@CleverShovel Thanks, I did this, but it didn't solve it. Oddly enough\
          \ though the error just changed a bit:\n\n\u2502 transformers/pipelines/base.py:278\
          \ in infer_framework_load_model             \u2502\n\u2502             \
          \                                                                 \u2502\
          \n\u2502    275 \u2502   \u2502   \u2502   \u2502   continue           \
          \                                   \u2502\n\u2502    276 \u2502   \u2502\
          \                                                                 \u2502\
          \n\u2502    277 \u2502   \u2502   if isinstance(model, str):           \
          \                         \u2502\n\u2502 \u2771  278 \u2502   \u2502   \u2502\
          \   raise ValueError(f\"Could not load model {model} with any  \u2502\n\u2502\
          \    279 \u2502                                                        \
          \             \u2502\n\u2502    280 \u2502   framework = infer_framework(model.__class__)\
          \                      \u2502\n\u2502    281 \u2502   return framework,\
          \ model                                           \u2502\n\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256F\nValueError: Could not load model tiiuae/falcon-7b-instruct\
          \ with any of the \nfollowing classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
          \n\n\nI also tried running it on Colab and get the same issue:\n```\n\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 in <cell\
          \ line: 8>:8                                                           \
          \                   \u2502\n\u2502                                     \
          \                                                             \u2502\n\u2502\
          \ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
          \ in pipeline       \u2502\n\u2502                                     \
          \                                                             \u2502\n\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None                           \u2502\n\u2502   786 \u2502   # Will load\
          \ the correct model if possible                                        \
          \      \u2502\n\u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}                 \u2502\n\u2502 \u2771\
          \ 788 \u2502   framework, model = infer_framework_load_model(          \
          \                               \u2502\n\u2502   789 \u2502   \u2502   model,\
          \                                                                      \
          \       \u2502\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n\u2502 \
          \  791 \u2502   \u2502   config=config,                                \
          \                                     \u2502\n\u2502                   \
          \                                                                      \
          \         \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:278\
          \ in                    \u2502\n\u2502 infer_framework_load_model      \
          \                                                                 \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502    275 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502\n\u2502    276 \u2502   \u2502        \
          \                                                                      \
          \       \u2502\n\u2502    277 \u2502   \u2502   if isinstance(model, str):\
          \                                                        \u2502\n\u2502\
          \ \u2771  278 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load\
          \ model {model} with any of the following cl  \u2502\n\u2502    279 \u2502\
          \                                                                      \
          \                   \u2502\n\u2502    280 \u2502   framework = infer_framework(model.__class__)\
          \                                          \u2502\n\u2502    281 \u2502\
          \   return framework, model                                            \
          \                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError: Could not\
          \ load model tiiuae/falcon-7b-instruct with any of the following classes:\
          \ (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
          ```"
        updatedAt: '2023-05-28T19:04:52.327Z'
      numEdits: 6
      reactions: []
    id: 6473a29b2a74fb43cce1dea8
    type: comment
  author: aviadatlas
  content: "@CleverShovel Thanks, I did this, but it didn't solve it. Oddly enough\
    \ though the error just changed a bit:\n\n\u2502 transformers/pipelines/base.py:278\
    \ in infer_framework_load_model             \u2502\n\u2502                   \
    \                                                           \u2502\n\u2502   \
    \ 275 \u2502   \u2502   \u2502   \u2502   continue                           \
    \                   \u2502\n\u2502    276 \u2502   \u2502                    \
    \                                             \u2502\n\u2502    277 \u2502   \u2502\
    \   if isinstance(model, str):                                    \u2502\n\u2502\
    \ \u2771  278 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load model\
    \ {model} with any  \u2502\n\u2502    279 \u2502                             \
    \                                        \u2502\n\u2502    280 \u2502   framework\
    \ = infer_framework(model.__class__)                      \u2502\n\u2502    281\
    \ \u2502   return framework, model                                           \u2502\
    \n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u256F\nValueError: Could not load model tiiuae/falcon-7b-instruct with\
    \ any of the \nfollowing classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
    \n\n\nI also tried running it on Colab and get the same issue:\n```\n\u256D\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u256E\n\u2502 in <cell line: 8>:8                               \
    \                                               \u2502\n\u2502               \
    \                                                                            \
    \       \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
    \ in pipeline       \u2502\n\u2502                                           \
    \                                                       \u2502\n\u2502   785 \u2502\
    \   # Forced if framework already defined, inferred if it's None             \
    \              \u2502\n\u2502   786 \u2502   # Will load the correct model if\
    \ possible                                              \u2502\n\u2502   787 \u2502\
    \   model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"\
    ]}                 \u2502\n\u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
    \                                         \u2502\n\u2502   789 \u2502   \u2502\
    \   model,                                                                   \
    \          \u2502\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
    \                                                       \u2502\n\u2502   791 \u2502\
    \   \u2502   config=config,                                                  \
    \                   \u2502\n\u2502                                           \
    \                                                       \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:278\
    \ in                    \u2502\n\u2502 infer_framework_load_model            \
    \                                                           \u2502\n\u2502   \
    \                                                                            \
    \                   \u2502\n\u2502    275 \u2502   \u2502   \u2502   \u2502  \
    \ continue                                                                  \u2502\
    \n\u2502    276 \u2502   \u2502                                              \
    \                                       \u2502\n\u2502    277 \u2502   \u2502\
    \   if isinstance(model, str):                                               \
    \         \u2502\n\u2502 \u2771  278 \u2502   \u2502   \u2502   raise ValueError(f\"\
    Could not load model {model} with any of the following cl  \u2502\n\u2502    279\
    \ \u2502                                                                     \
    \                    \u2502\n\u2502    280 \u2502   framework = infer_framework(model.__class__)\
    \                                          \u2502\n\u2502    281 \u2502   return\
    \ framework, model                                                           \
    \    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError:\
    \ Could not load model tiiuae/falcon-7b-instruct with any of the following classes:\
    \ (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class\
    \ \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n```"
  created_at: 2023-05-28 17:51:07+00:00
  edited: true
  hidden: false
  id: 6473a29b2a74fb43cce1dea8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e644992d17fe0a6f97d8930a6aa64aa5.svg
      fullname: Vladimir P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CleverShovel
      type: user
    createdAt: '2023-05-28T19:47:10.000Z'
    data:
      edited: false
      editors:
      - CleverShovel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e644992d17fe0a6f97d8930a6aa64aa5.svg
          fullname: Vladimir P
          isHf: false
          isPro: false
          name: CleverShovel
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aviadatlas&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aviadatlas\">@<span class=\"\
          underline\">aviadatlas</span></a></span>\n\n\t</span></span> Yeah, it is\
          \ strange, maybe some problem with transformers.pipeline.<br>I managed to\
          \ launch it on Colab, here is <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1cUmmXgIfTz61F1sCotjDV6nsyfGk1ScA?usp=sharing\"\
          >notebook</a>. I used sharded version of this model otherwise it can't be\
          \ loaded in free tier Colab.</p>\n"
        raw: "@aviadatlas Yeah, it is strange, maybe some problem with transformers.pipeline.\
          \ \nI managed to launch it on Colab, here is [notebook](https://colab.research.google.com/drive/1cUmmXgIfTz61F1sCotjDV6nsyfGk1ScA?usp=sharing).\
          \ I used sharded version of this model otherwise it can't be loaded in free\
          \ tier Colab."
        updatedAt: '2023-05-28T19:47:10.960Z'
      numEdits: 0
      reactions: []
    id: 6473afbe6cff2f8672095893
    type: comment
  author: CleverShovel
  content: "@aviadatlas Yeah, it is strange, maybe some problem with transformers.pipeline.\
    \ \nI managed to launch it on Colab, here is [notebook](https://colab.research.google.com/drive/1cUmmXgIfTz61F1sCotjDV6nsyfGk1ScA?usp=sharing).\
    \ I used sharded version of this model otherwise it can't be loaded in free tier\
    \ Colab."
  created_at: 2023-05-28 18:47:10+00:00
  edited: false
  hidden: false
  id: 6473afbe6cff2f8672095893
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d54dd22a152d269323c933dfe5dc68fd.svg
      fullname: Chile Saltlime
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chilesaltlime
      type: user
    createdAt: '2023-05-28T21:06:27.000Z'
    data:
      edited: true
      editors:
      - chilesaltlime
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d54dd22a152d269323c933dfe5dc68fd.svg
          fullname: Chile Saltlime
          isHf: false
          isPro: false
          name: chilesaltlime
          type: user
        html: '<p>I''m having the same exact issue as OP.</p>

          <p>It would help if there were a requirements.txt or environment.yaml or
          dockerfile so we are not just guessing when it comes to library versioning,
          especially in such a fast-moving space as LLMs. Without version numbers,
          the dependencies are almost guaranteed to not be in sync with pip / conda
          (even if you can figure out what  dependencies are needed by getting pip
          errors over and over).</p>

          '
        raw: 'I''m having the same exact issue as OP.


          It would help if there were a requirements.txt or environment.yaml or dockerfile
          so we are not just guessing when it comes to library versioning, especially
          in such a fast-moving space as LLMs. Without version numbers, the dependencies
          are almost guaranteed to not be in sync with pip / conda (even if you can
          figure out what  dependencies are needed by getting pip errors over and
          over).'
        updatedAt: '2023-05-28T21:11:29.663Z'
      numEdits: 4
      reactions: []
    id: 6473c253352c94a20ddb16d5
    type: comment
  author: chilesaltlime
  content: 'I''m having the same exact issue as OP.


    It would help if there were a requirements.txt or environment.yaml or dockerfile
    so we are not just guessing when it comes to library versioning, especially in
    such a fast-moving space as LLMs. Without version numbers, the dependencies are
    almost guaranteed to not be in sync with pip / conda (even if you can figure out
    what  dependencies are needed by getting pip errors over and over).'
  created_at: 2023-05-28 20:06:27+00:00
  edited: true
  hidden: false
  id: 6473c253352c94a20ddb16d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d54dd22a152d269323c933dfe5dc68fd.svg
      fullname: Chile Saltlime
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chilesaltlime
      type: user
    createdAt: '2023-05-28T21:08:50.000Z'
    data:
      edited: false
      editors:
      - chilesaltlime
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d54dd22a152d269323c933dfe5dc68fd.svg
          fullname: Chile Saltlime
          isHf: false
          isPro: false
          name: chilesaltlime
          type: user
        html: '<p>I got this to work by running <code>pip install -U transformers</code>.
          I''d used conda to install transformers originally.</p>

          '
        raw: I got this to work by running `pip install -U transformers`. I'd used
          conda to install transformers originally.
        updatedAt: '2023-05-28T21:08:50.735Z'
      numEdits: 0
      reactions: []
    id: 6473c2e26cff2f86720a7984
    type: comment
  author: chilesaltlime
  content: I got this to work by running `pip install -U transformers`. I'd used conda
    to install transformers originally.
  created_at: 2023-05-28 20:08:50+00:00
  edited: false
  hidden: false
  id: 6473c2e26cff2f86720a7984
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd4fda2d91f59b329fdc36203b42ee63.svg
      fullname: shrinivas kamath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shri1510
      type: user
    createdAt: '2023-05-30T07:08:30.000Z'
    data:
      edited: false
      editors:
      - shri1510
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd4fda2d91f59b329fdc36203b42ee63.svg
          fullname: shrinivas kamath
          isHf: false
          isPro: false
          name: shri1510
          type: user
        html: '<p>I''m still getting the same error. Does anybody have a solution
          for this?</p>

          '
        raw: I'm still getting the same error. Does anybody have a solution for this?
        updatedAt: '2023-05-30T07:08:30.624Z'
      numEdits: 0
      reactions: []
    id: 6475a0ee09e773226332b667
    type: comment
  author: shri1510
  content: I'm still getting the same error. Does anybody have a solution for this?
  created_at: 2023-05-30 06:08:30+00:00
  edited: false
  hidden: false
  id: 6475a0ee09e773226332b667
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/308e98b9720521aa4e9c52a78dc95e56.svg
      fullname: Vishaal Udandarao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vishaal27
      type: user
    createdAt: '2023-05-31T21:30:52.000Z'
    data:
      edited: false
      editors:
      - vishaal27
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/308e98b9720521aa4e9c52a78dc95e56.svg
          fullname: Vishaal Udandarao
          isHf: false
          isPro: false
          name: vishaal27
          type: user
        html: '<p>Me too!</p>

          '
        raw: Me too!
        updatedAt: '2023-05-31T21:30:52.704Z'
      numEdits: 0
      reactions: []
    id: 6477bc8cf911e9e76c6a8077
    type: comment
  author: vishaal27
  content: Me too!
  created_at: 2023-05-31 20:30:52+00:00
  edited: false
  hidden: false
  id: 6477bc8cf911e9e76c6a8077
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f6299cccad6b1cc7f535e6eef9fa456.svg
      fullname: Francesco Cabras
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: celcov
      type: user
    createdAt: '2023-06-01T13:42:10.000Z'
    data:
      edited: false
      editors:
      - celcov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f6299cccad6b1cc7f535e6eef9fa456.svg
          fullname: Francesco Cabras
          isHf: false
          isPro: false
          name: celcov
          type: user
        html: '<p>For me it was a memory error, the way I solved this was by exporting
          TRANSFORMERS_CACHE to a different location</p>

          '
        raw: For me it was a memory error, the way I solved this was by exporting
          TRANSFORMERS_CACHE to a different location
        updatedAt: '2023-06-01T13:42:10.335Z'
      numEdits: 0
      reactions: []
    id: 6478a0321f9756aa89d30609
    type: comment
  author: celcov
  content: For me it was a memory error, the way I solved this was by exporting TRANSFORMERS_CACHE
    to a different location
  created_at: 2023-06-01 12:42:10+00:00
  edited: false
  hidden: false
  id: 6478a0321f9756aa89d30609
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/308e98b9720521aa4e9c52a78dc95e56.svg
      fullname: Vishaal Udandarao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vishaal27
      type: user
    createdAt: '2023-06-01T13:51:04.000Z'
    data:
      edited: false
      editors:
      - vishaal27
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/308e98b9720521aa4e9c52a78dc95e56.svg
          fullname: Vishaal Udandarao
          isHf: false
          isPro: false
          name: vishaal27
          type: user
        html: '<p>Can someone who it is working for please share an conda env.yaml
          or requirements.txt file reproducing their exact config so that we can try
          to use that setup to load the model and run it? This would be really helpful!
          :)</p>

          '
        raw: Can someone who it is working for please share an conda env.yaml or requirements.txt
          file reproducing their exact config so that we can try to use that setup
          to load the model and run it? This would be really helpful! :)
        updatedAt: '2023-06-01T13:51:04.000Z'
      numEdits: 0
      reactions: []
    id: 6478a2481f9756aa89d339e6
    type: comment
  author: vishaal27
  content: Can someone who it is working for please share an conda env.yaml or requirements.txt
    file reproducing their exact config so that we can try to use that setup to load
    the model and run it? This would be really helpful! :)
  created_at: 2023-06-01 12:51:04+00:00
  edited: false
  hidden: false
  id: 6478a2481f9756aa89d339e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dbf6bfbad259ebeae1ea9b44897b90c1.svg
      fullname: Chet Patel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chpatel2
      type: user
    createdAt: '2023-06-01T20:47:49.000Z'
    data:
      edited: false
      editors:
      - chpatel2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dbf6bfbad259ebeae1ea9b44897b90c1.svg
          fullname: Chet Patel
          isHf: false
          isPro: false
          name: chpatel2
          type: user
        html: '<p>Getting the same error. Hard to work with this stuff without versions
          of dependencies.</p>

          '
        raw: Getting the same error. Hard to work with this stuff without versions
          of dependencies.
        updatedAt: '2023-06-01T20:47:49.962Z'
      numEdits: 0
      reactions: []
    id: 647903f5dbf97e0b5cc8488f
    type: comment
  author: chpatel2
  content: Getting the same error. Hard to work with this stuff without versions of
    dependencies.
  created_at: 2023-06-01 19:47:49+00:00
  edited: false
  hidden: false
  id: 647903f5dbf97e0b5cc8488f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
      fullname: 'Sasan '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sasanou
      type: user
    createdAt: '2023-06-04T14:19:21.000Z'
    data:
      edited: false
      editors:
      - Sasanou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4198915660381317
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
          fullname: 'Sasan '
          isHf: false
          isPro: false
          name: Sasanou
          type: user
        html: "<p>Hello Thanks for your help all, I Get the below error now, can anyone\
          \ please help ?</p>\n<p>Loading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 2/2 [01:01&lt;00:00, 30.80s/it]<br>Traceback (most recent call last):<br>\
          \  File \"/Users/clustered/Python Application/02. Falcon_Model/Falcon-Application.py\"\
          , line 10, in <br>    tokenizer = AutoTokenizer.from_pretrained(model)<br>\
          \  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 643, in from_pretrained<br>    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
          \ **kwargs)<br>  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 487, in get_tokenizer_config<br>    resolved_config_file = cached_file(<br>\
          \  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 417, in cached_file<br>    resolved_file = hf_hub_download(<br> \
          \ File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py\"\
          , line 112, in _inner_fn<br>  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py\"\
          , line 166, in validate_repo_id<br>huggingface_hub.utils.<em>validators.HFValidationError:\
          \ Repo id must use alphanumeric chars or '-', '</em>', '.', '--' and '..'\
          \ are forbidden, '-' and '.' cannot start or end the name, max length is\
          \ 96: 'RWForCausalLM(<br>  (transformer): RWModel(<br>    (word_embeddings):\
          \ Embedding(65024, 4544)<br>    (h): ModuleList(<br>      (0-31): 32 x DecoderLayer(<br>\
          \        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)<br>\
          \        (self_attention): Attention(<br>          (maybe_rotary): RotaryEmbedding()<br>\
          \          (query_key_value): Linear(in_features=4544, out_features=4672,\
          \ bias=False)<br>          (dense): Linear(in_features=4544, out_features=4544,\
          \ bias=False)<br>          (attention_dropout): Dropout(p=0.0, inplace=False)<br>\
          \        )<br>        (mlp): MLP(<br>          (dense_h_to_4h): Linear(in_features=4544,\
          \ out_features=18176, bias=False)<br>          (act): GELU(approximate='none')<br>\
          \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544,\
          \ bias=False)<br>        )<br>      )<br>    )<br>    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)<br>  )<br>  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)</p>\n"
        raw: "Hello Thanks for your help all, I Get the below error now, can anyone\
          \ please help ?\n\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
          \ [01:01<00:00, 30.80s/it]\nTraceback (most recent call last):\n  File \"\
          /Users/clustered/Python Application/02. Falcon_Model/Falcon-Application.py\"\
          , line 10, in <module>\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\
          \  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 643, in from_pretrained\n    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
          \ **kwargs)\n  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 487, in get_tokenizer_config\n    resolved_config_file = cached_file(\n\
          \  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 417, in cached_file\n    resolved_file = hf_hub_download(\n  File\
          \ \"/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py\"\
          , line 112, in _inner_fn\n  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py\"\
          , line 166, in validate_repo_id\nhuggingface_hub.utils._validators.HFValidationError:\
          \ Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\
          \ forbidden, '-' and '.' cannot start or end the name, max length is 96:\
          \ 'RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings): Embedding(65024,\
          \ 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n      \
          \  (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
          \        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n\
          \          (query_key_value): Linear(in_features=4544, out_features=4672,\
          \ bias=False)\n          (dense): Linear(in_features=4544, out_features=4544,\
          \ bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n\
          \        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544,\
          \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
          \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544,\
          \ bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)"
        updatedAt: '2023-06-04T14:19:21.076Z'
      numEdits: 0
      reactions: []
    id: 647c9d69c788767ab5cf3c32
    type: comment
  author: Sasanou
  content: "Hello Thanks for your help all, I Get the below error now, can anyone\
    \ please help ?\n\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 2/2 [01:01<00:00, 30.80s/it]\nTraceback (most recent\
    \ call last):\n  File \"/Users/clustered/Python Application/02. Falcon_Model/Falcon-Application.py\"\
    , line 10, in <module>\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\
    \  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 643, in from_pretrained\n    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
    \ **kwargs)\n  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 487, in get_tokenizer_config\n    resolved_config_file = cached_file(\n\
    \  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py\"\
    , line 417, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py\"\
    , line 112, in _inner_fn\n  File \"/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py\"\
    , line 166, in validate_repo_id\nhuggingface_hub.utils._validators.HFValidationError:\
    \ Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden,\
    \ '-' and '.' cannot start or end the name, max length is 96: 'RWForCausalLM(\n\
    \  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n  \
    \  (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n        (input_layernorm):\
    \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n        (self_attention):\
    \ Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value):\
    \ Linear(in_features=4544, out_features=4672, bias=False)\n          (dense):\
    \ Linear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout):\
    \ Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n          (dense_h_to_4h):\
    \ Linear(in_features=4544, out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
    \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n\
    \        )\n      )\n    )\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
    \  )\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)"
  created_at: 2023-06-04 13:19:21+00:00
  edited: false
  hidden: false
  id: 647c9d69c788767ab5cf3c32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
      fullname: 'Sasan '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sasanou
      type: user
    createdAt: '2023-06-05T13:37:10.000Z'
    data:
      edited: false
      editors:
      - Sasanou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8811805248260498
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
          fullname: 'Sasan '
          isHf: false
          isPro: false
          name: Sasanou
          type: user
        html: '<p>Can anyone please help 9n this ? :)</p>

          '
        raw: Can anyone please help 9n this ? :)
        updatedAt: '2023-06-05T13:37:10.807Z'
      numEdits: 0
      reactions: []
    id: 647de5065214d172cbb8a3cf
    type: comment
  author: Sasanou
  content: Can anyone please help 9n this ? :)
  created_at: 2023-06-05 12:37:10+00:00
  edited: false
  hidden: false
  id: 647de5065214d172cbb8a3cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
      fullname: 'Sasan '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sasanou
      type: user
    createdAt: '2023-06-07T09:11:41.000Z'
    data:
      edited: false
      editors:
      - Sasanou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9396926760673523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c662dbe7b4e12ed19349e946e0edd19.svg
          fullname: 'Sasan '
          isHf: false
          isPro: false
          name: Sasanou
          type: user
        html: '<p>Anyone?</p>

          '
        raw: Anyone?
        updatedAt: '2023-06-07T09:11:41.666Z'
      numEdits: 0
      reactions: []
    id: 648049cde1421e205fd39d18
    type: comment
  author: Sasanou
  content: Anyone?
  created_at: 2023-06-07 08:11:41+00:00
  edited: false
  hidden: false
  id: 648049cde1421e205fd39d18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11fa20f8a1563534b14348def4d0c8bc.svg
      fullname: Aviad Atlas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aviadatlas
      type: user
    createdAt: '2023-06-07T11:18:35.000Z'
    data:
      edited: false
      editors:
      - aviadatlas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9370332956314087
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11fa20f8a1563534b14348def4d0c8bc.svg
          fullname: Aviad Atlas
          isHf: false
          isPro: false
          name: aviadatlas
          type: user
        html: '<p>So I managed to make the sample code work on Colab at least. I just
          needed to have the GPU enabled via "Runtime -&gt; Change runtime type"</p>

          '
        raw: So I managed to make the sample code work on Colab at least. I just needed
          to have the GPU enabled via "Runtime -> Change runtime type"
        updatedAt: '2023-06-07T11:18:35.769Z'
      numEdits: 0
      reactions: []
    id: 6480678bbb25a636c9d8dad4
    type: comment
  author: aviadatlas
  content: So I managed to make the sample code work on Colab at least. I just needed
    to have the GPU enabled via "Runtime -> Change runtime type"
  created_at: 2023-06-07 10:18:35+00:00
  edited: false
  hidden: false
  id: 6480678bbb25a636c9d8dad4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:11:06.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9364085793495178
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>It seems like the issue here has been solved -- but feel free to
          also check-out this great <a href="https://huggingface.co/blog/falcon">blogpost</a>
          from HuggingFace on using Falcon. </p>

          '
        raw: 'It seems like the issue here has been solved -- but feel free to also
          check-out this great [blogpost](https://huggingface.co/blog/falcon) from
          HuggingFace on using Falcon. '
        updatedAt: '2023-06-09T14:11:06.057Z'
      numEdits: 0
      reactions: []
      relatedEventId: 648332fa6e7ccc304d297c5c
    id: 648332fa6e7ccc304d297c54
    type: comment
  author: FalconLLM
  content: 'It seems like the issue here has been solved -- but feel free to also
    check-out this great [blogpost](https://huggingface.co/blog/falcon) from HuggingFace
    on using Falcon. '
  created_at: 2023-06-09 13:11:06+00:00
  edited: false
  hidden: false
  id: 648332fa6e7ccc304d297c54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:11:06.000Z'
    data:
      status: closed
    id: 648332fa6e7ccc304d297c5c
    type: status-change
  author: FalconLLM
  created_at: 2023-06-09 13:11:06+00:00
  id: 648332fa6e7ccc304d297c5c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Getting an error with the example code
