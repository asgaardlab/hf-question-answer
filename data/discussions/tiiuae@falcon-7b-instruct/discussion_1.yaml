!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Supreeth
conflicting_files: null
created_at: 2023-05-25 14:41:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1627701114915-noauth.png?w=200&h=200&f=face
      fullname: Supreeth Rao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Supreeth
      type: user
    createdAt: '2023-05-25T15:41:20.000Z'
    data:
      edited: false
      editors:
      - Supreeth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1627701114915-noauth.png?w=200&h=200&f=face
          fullname: Supreeth Rao
          isHf: false
          isPro: false
          name: Supreeth
          type: user
        html: '<p>Hey,<br>it would really help If the docs contained some example
          inference code and prompt formats.</p>

          <p>Thanks !</p>

          '
        raw: "Hey,\r\nit would really help If the docs contained some example inference\
          \ code and prompt formats.\r\n\r\nThanks !"
        updatedAt: '2023-05-25T15:41:20.480Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - shiv007
        - zhoupf
        - hongyuxiao
        - zidark
        - KingZack
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - arshad615
        - hongyuxiao
    id: 646f81a05b8225e31ae4ec27
    type: comment
  author: Supreeth
  content: "Hey,\r\nit would really help If the docs contained some example inference\
    \ code and prompt formats.\r\n\r\nThanks !"
  created_at: 2023-05-25 14:41:20+00:00
  edited: false
  hidden: false
  id: 646f81a05b8225e31ae4ec27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620282175694-noauth.png?w=200&h=200&f=face
      fullname: Julien Launay
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: slippylolo
      type: user
    createdAt: '2023-05-26T10:33:46.000Z'
    data:
      edited: false
      editors:
      - slippylolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620282175694-noauth.png?w=200&h=200&f=face
          fullname: Julien Launay
          isHf: false
          isPro: false
          name: slippylolo
          type: user
        html: "<p>\U0001F44B Hello Supreeth, </p>\n<p>Our release has been on the\
          \ low-key side for now, and we are preparing for a more public release soon.\
          \ </p>\n<p>We have added to the model card:</p>\n<ul>\n<li>An inference\
          \ code sample;</li>\n<li>Details about the instruct/chat dataset used.</li>\n\
          </ul>\n<p>I hope this is helpful for you!</p>\n<p>We have not used any very\
          \ specific prompt format. For chat, you should be able to get something\
          \ nice with simply:</p>\n<pre><code>User: Can you tell me what is brutalism?\n\
          Assistant:\n</code></pre>\n<p>(the most important part here being not having\
          \ a stray space at the start of the assistant message, as this will mess\
          \ with tokenisation!)</p>\n"
        raw: "\U0001F44B Hello Supreeth, \n\nOur release has been on the low-key side\
          \ for now, and we are preparing for a more public release soon. \n\nWe have\
          \ added to the model card:\n* An inference code sample;\n* Details about\
          \ the instruct/chat dataset used. \n\nI hope this is helpful for you!\n\n\
          We have not used any very specific prompt format. For chat, you should be\
          \ able to get something nice with simply:\n```\nUser: Can you tell me what\
          \ is brutalism?\nAssistant:\n```\n\n(the most important part here being\
          \ not having a stray space at the start of the assistant message, as this\
          \ will mess with tokenisation!)"
        updatedAt: '2023-05-26T10:33:46.477Z'
      numEdits: 0
      reactions:
      - count: 12
        reaction: "\U0001F44D"
        users:
        - MaziyarPanahi
        - Supreeth
        - y12studio
        - cyberspyde
        - mkozak
        - deepakdhiman69
        - nischalhp
        - Yiannis128
        - edmond
        - moeinh77
        - hongyuxiao
        - Nicolas-BZRD
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hongyuxiao
    id: 64708b0a3df93fddece002a4
    type: comment
  author: slippylolo
  content: "\U0001F44B Hello Supreeth, \n\nOur release has been on the low-key side\
    \ for now, and we are preparing for a more public release soon. \n\nWe have added\
    \ to the model card:\n* An inference code sample;\n* Details about the instruct/chat\
    \ dataset used. \n\nI hope this is helpful for you!\n\nWe have not used any very\
    \ specific prompt format. For chat, you should be able to get something nice with\
    \ simply:\n```\nUser: Can you tell me what is brutalism?\nAssistant:\n```\n\n\
    (the most important part here being not having a stray space at the start of the\
    \ assistant message, as this will mess with tokenisation!)"
  created_at: 2023-05-26 09:33:46+00:00
  edited: false
  hidden: false
  id: 64708b0a3df93fddece002a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-05-26T16:53:37.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;slippylolo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/slippylolo\"\
          >@<span class=\"underline\">slippylolo</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Is there any end of the text tokens like <code>&lt;/s&gt;</code>\
          \ or <code>&lt;|endoftext|&gt;</code>? I was wondering if there is another\
          \ token formatting that can be used:</p>\n<pre><code>Below is an instruction\
          \ that describes a task, paired with an input that provides further context.\
          \ Write a response that appropriately completes the request.\n\n### Instruction:\n\
          {instruction}\n\n### Input:\n{input}\n\n### Response:\n</code></pre>\n"
        raw: "Hi @slippylolo \n\nIs there any end of the text tokens like `</s>` or\
          \ `<|endoftext|>`? I was wondering if there is another token formatting\
          \ that can be used:\n\n```\nBelow is an instruction that describes a task,\
          \ paired with an input that provides further context. Write a response that\
          \ appropriately completes the request.\n\n### Instruction:\n{instruction}\n\
          \n### Input:\n{input}\n\n### Response:\n```"
        updatedAt: '2023-05-26T16:53:37.009Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - rustamg
        - Annorita
    id: 6470e4111f0e7ee7fb1e9640
    type: comment
  author: MaziyarPanahi
  content: "Hi @slippylolo \n\nIs there any end of the text tokens like `</s>` or\
    \ `<|endoftext|>`? I was wondering if there is another token formatting that can\
    \ be used:\n\n```\nBelow is an instruction that describes a task, paired with\
    \ an input that provides further context. Write a response that appropriately\
    \ completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\
    \n### Response:\n```"
  created_at: 2023-05-26 15:53:37+00:00
  edited: false
  hidden: false
  id: 6470e4111f0e7ee7fb1e9640
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:10:12.000Z'
    data:
      pinned: true
    id: 6475a154c894b5c9cf6e92ad
    type: pinning-change
  author: FalconLLM
  created_at: 2023-05-30 06:10:12+00:00
  id: 6475a154c894b5c9cf6e92ad
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-06-01T08:02:22.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>I am going to share any prompt that worked for me here, starting\
          \ with this classic template:</p>\n<pre><code>\"\"\"Answer the question\
          \ as truthfully as possible using the provided text, and if the answer is\
          \ not contained within the text below, say \"I don't know\"\n\nContext:\n\
          The men's high jump event at the 2020 Summer Olympics took place between\
          \ 30 July and 1 August 2021 at the Olympic Stadium.\n33 athletes from 24\
          \ nations competed; the total possible number depended on how many nations\
          \ would use universality places \nto enter athletes in addition to the 32\
          \ qualifying through mark or ranking (no universality places were used in\
          \ 2021).\nItalian athlete Gianmarco Tamberi along with Qatari athlete Mutaz\
          \ Essa Barshim emerged as joint winners of the event following\na tie between\
          \ both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to\
          \ share the gold medal in a rare instance\nwhere the athletes of different\
          \ nations had agreed to share the same medal in the history of Olympics.\
          \ \nBarshim in particular was heard to ask a competition official \"Can\
          \ we have two golds?\" in response to being offered a \n'jump off'. Maksim\
          \ Nedasekau of Belarus took bronze. The medals were the first ever in the\
          \ men's high jump for Italy and \nBelarus, the first gold in the men's high\
          \ jump for Italy and Qatar, and the third consecutive medal in the men's\
          \ high jump\nfor Qatar (all by Barshim). Barshim became only the second\
          \ man to earn three medals in high jump, joining Patrik Sj\xF6berg\nof Sweden\
          \ (1984 to 1992).\n\nWho won the 2020 Summer Olympics men's high jump?\"\
          \"\"\n</code></pre>\n<p>Which it answered <code>Gianmarco Tamberi (Italy)\
          \ and Mutaz Essa Barshim (Qatar)</code></p>\n<p>To be sure it does respect\
          \ the instruction at the beginning:</p>\n<pre><code>\"\"\"Answer the question\
          \ as truthfully as possible using the provided text, and if the answer is\
          \ not contained within the text below, say \"I don't know my lord!\"\n\n\
          Context:\nThe men's high jump event at the 2020 Summer Olympics took place\
          \ between 30 July and 1 August 2021 at the Olympic Stadium.\n33 athletes\
          \ from 24 nations competed; the total possible number depended on how many\
          \ nations would use universality places \nto enter athletes in addition\
          \ to the 32 qualifying through mark or ranking (no universality places were\
          \ used in 2021).\nItalian athlete Gianmarco Tamberi along with Qatari athlete\
          \ Mutaz Essa Barshim emerged as joint winners of the event following\na\
          \ tie between both of them as they cleared 2.37m. Both Tamberi and Barshim\
          \ agreed to share the gold medal in a rare instance\nwhere the athletes\
          \ of different nations had agreed to share the same medal in the history\
          \ of Olympics. \nBarshim in particular was heard to ask a competition official\
          \ \"Can we have two golds?\" in response to being offered a \n'jump off'.\
          \ Maksim Nedasekau of Belarus took bronze. The medals were the first ever\
          \ in the men's high jump for Italy and \nBelarus, the first gold in the\
          \ men's high jump for Italy and Qatar, and the third consecutive medal in\
          \ the men's high jump\nfor Qatar (all by Barshim). Barshim became only the\
          \ second man to earn three medals in high jump, joining Patrik Sj\xF6berg\n\
          of Sweden (1984 to 1992).\n\nWho is the president of France?\"\"\"\n</code></pre>\n\
          <p>I wanted to be sure this <code>I don't know</code> part comes from the\
          \ instruction, so I added <code>my lord!</code>. Which it answered: <code>I\
          \ don't know my lord!</code></p>\n<h2 id=\"langchain\">LangChain</h2>\n\
          <p>If you want to use the same prompt template in LangChain:</p>\n<pre><code\
          \ class=\"language-python\">template = <span class=\"hljs-string\">\"\"\"\
          Answer the question as truthfully as possible using the provided text, and\
          \ if the answer is not contained within the text below, say \"I don't know\"\
          </span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"\
          >Context:</span>\n<span class=\"hljs-string\">{context}</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">{query}\"\"\"</span>.strip()\n\
          \n<span class=\"hljs-keyword\">from</span> langchain.prompts <span class=\"\
          hljs-keyword\">import</span> PromptTemplate\n\nprompt_template = PromptTemplate(\n\
          \    input_variables=[<span class=\"hljs-string\">\"query\"</span>, <span\
          \ class=\"hljs-string\">\"context\"</span>],\n    template=template\n)\n\
          \nprompt_template\n</code></pre>\n<p>Which can be used:</p>\n<pre><code\
          \ class=\"language-python\">context = <span class=\"hljs-string\">\"\"\"\
          </span>\n<span class=\"hljs-string\">The men's high jump event at the 2020\
          \ Summer Olympics took place between 30 July and 1 August 2021 at the Olympic\
          \ Stadium.</span>\n<span class=\"hljs-string\">33 athletes from 24 nations\
          \ competed; the total possible number depended on how many nations would\
          \ use universality places </span>\n<span class=\"hljs-string\">to enter\
          \ athletes in addition to the 32 qualifying through mark or ranking (no\
          \ universality places were used in 2021).</span>\n<span class=\"hljs-string\"\
          >Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa\
          \ Barshim emerged as joint winners of the event following</span>\n<span\
          \ class=\"hljs-string\">a tie between both of them as they cleared 2.37m.\
          \ Both Tamberi and Barshim agreed to share the gold medal in a rare instance</span>\n\
          <span class=\"hljs-string\">where the athletes of different nations had\
          \ agreed to share the same medal in the history of Olympics. </span>\n<span\
          \ class=\"hljs-string\">Barshim in particular was heard to ask a competition\
          \ official \"Can we have two golds?\" in response to being offered a </span>\n\
          <span class=\"hljs-string\">'jump off'. Maksim Nedasekau of Belarus took\
          \ bronze. The medals were the first ever in the men's high jump for Italy\
          \ and </span>\n<span class=\"hljs-string\">Belarus, the first gold in the\
          \ men's high jump for Italy and Qatar, and the third consecutive medal in\
          \ the men's high jump</span>\n<span class=\"hljs-string\">for Qatar (all\
          \ by Barshim). Barshim became only the second man to earn three medals in\
          \ high jump, joining Patrik Sj\xF6berg</span>\n<span class=\"hljs-string\"\
          >of Sweden (1984 to 1992).\"\"\"</span>\n\nquery = <span class=\"hljs-string\"\
          >\"Who won the 2020 Summer Olympics men's high jump?\"</span>\n\nprompt\
          \ = prompt_template.<span class=\"hljs-built_in\">format</span>(query=query,\
          \ context=context)\n<span class=\"hljs-built_in\">print</span>(prompt)\n\
          </code></pre>\n"
        raw: "I am going to share any prompt that worked for me here, starting with\
          \ this classic template:\n\n\n```\n\"\"\"Answer the question as truthfully\
          \ as possible using the provided text, and if the answer is not contained\
          \ within the text below, say \"I don't know\"\n\nContext:\nThe men's high\
          \ jump event at the 2020 Summer Olympics took place between 30 July and\
          \ 1 August 2021 at the Olympic Stadium.\n33 athletes from 24 nations competed;\
          \ the total possible number depended on how many nations would use universality\
          \ places \nto enter athletes in addition to the 32 qualifying through mark\
          \ or ranking (no universality places were used in 2021).\nItalian athlete\
          \ Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged\
          \ as joint winners of the event following\na tie between both of them as\
          \ they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold\
          \ medal in a rare instance\nwhere the athletes of different nations had\
          \ agreed to share the same medal in the history of Olympics. \nBarshim in\
          \ particular was heard to ask a competition official \"Can we have two golds?\"\
          \ in response to being offered a \n'jump off'. Maksim Nedasekau of Belarus\
          \ took bronze. The medals were the first ever in the men's high jump for\
          \ Italy and \nBelarus, the first gold in the men's high jump for Italy and\
          \ Qatar, and the third consecutive medal in the men's high jump\nfor Qatar\
          \ (all by Barshim). Barshim became only the second man to earn three medals\
          \ in high jump, joining Patrik Sj\xF6berg\nof Sweden (1984 to 1992).\n\n\
          Who won the 2020 Summer Olympics men's high jump?\"\"\"\n```\nWhich it answered\
          \ `Gianmarco Tamberi (Italy) and Mutaz Essa Barshim (Qatar)`\n\nTo be sure\
          \ it does respect the instruction at the beginning:\n\n```\n\"\"\"Answer\
          \ the question as truthfully as possible using the provided text, and if\
          \ the answer is not contained within the text below, say \"I don't know\
          \ my lord!\"\n\nContext:\nThe men's high jump event at the 2020 Summer Olympics\
          \ took place between 30 July and 1 August 2021 at the Olympic Stadium.\n\
          33 athletes from 24 nations competed; the total possible number depended\
          \ on how many nations would use universality places \nto enter athletes\
          \ in addition to the 32 qualifying through mark or ranking (no universality\
          \ places were used in 2021).\nItalian athlete Gianmarco Tamberi along with\
          \ Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event\
          \ following\na tie between both of them as they cleared 2.37m. Both Tamberi\
          \ and Barshim agreed to share the gold medal in a rare instance\nwhere the\
          \ athletes of different nations had agreed to share the same medal in the\
          \ history of Olympics. \nBarshim in particular was heard to ask a competition\
          \ official \"Can we have two golds?\" in response to being offered a \n\
          'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the\
          \ first ever in the men's high jump for Italy and \nBelarus, the first gold\
          \ in the men's high jump for Italy and Qatar, and the third consecutive\
          \ medal in the men's high jump\nfor Qatar (all by Barshim). Barshim became\
          \ only the second man to earn three medals in high jump, joining Patrik\
          \ Sj\xF6berg\nof Sweden (1984 to 1992).\n\nWho is the president of France?\"\
          \"\"\n```\nI wanted to be sure this `I don't know` part comes from the instruction,\
          \ so I added `my lord!`. Which it answered: `I don't know my lord!`\n\n\
          ## LangChain\nIf you want to use the same prompt template in LangChain:\n\
          \n```python\ntemplate = \"\"\"Answer the question as truthfully as possible\
          \ using the provided text, and if the answer is not contained within the\
          \ text below, say \"I don't know\"\n\nContext:\n{context}\n\n{query}\"\"\
          \".strip()\n\nfrom langchain.prompts import PromptTemplate\n\nprompt_template\
          \ = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n   \
          \ template=template\n)\n\nprompt_template\n```\n\nWhich can be used:\n\n\
          ```python\ncontext = \"\"\"\nThe men's high jump event at the 2020 Summer\
          \ Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.\n\
          33 athletes from 24 nations competed; the total possible number depended\
          \ on how many nations would use universality places \nto enter athletes\
          \ in addition to the 32 qualifying through mark or ranking (no universality\
          \ places were used in 2021).\nItalian athlete Gianmarco Tamberi along with\
          \ Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event\
          \ following\na tie between both of them as they cleared 2.37m. Both Tamberi\
          \ and Barshim agreed to share the gold medal in a rare instance\nwhere the\
          \ athletes of different nations had agreed to share the same medal in the\
          \ history of Olympics. \nBarshim in particular was heard to ask a competition\
          \ official \"Can we have two golds?\" in response to being offered a \n\
          'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the\
          \ first ever in the men's high jump for Italy and \nBelarus, the first gold\
          \ in the men's high jump for Italy and Qatar, and the third consecutive\
          \ medal in the men's high jump\nfor Qatar (all by Barshim). Barshim became\
          \ only the second man to earn three medals in high jump, joining Patrik\
          \ Sj\xF6berg\nof Sweden (1984 to 1992).\"\"\"\n\nquery = \"Who won the 2020\
          \ Summer Olympics men's high jump?\"\n\nprompt = prompt_template.format(query=query,\
          \ context=context)\nprint(prompt)\n```"
        updatedAt: '2023-06-01T08:02:22.518Z'
      numEdits: 0
      reactions:
      - count: 12
        reaction: "\U0001F44D"
        users:
        - AliSab
        - y12studio
        - Neroro64
        - joeedwardscfp
        - deniscoady
        - upasana-mittal
        - Dawn007
        - nischalhp
        - rustamg
        - ybm11
        - GianniCatBug
        - MaheswaraSai
    id: 6478508e9c1f42c1f4d8b0bf
    type: comment
  author: MaziyarPanahi
  content: "I am going to share any prompt that worked for me here, starting with\
    \ this classic template:\n\n\n```\n\"\"\"Answer the question as truthfully as\
    \ possible using the provided text, and if the answer is not contained within\
    \ the text below, say \"I don't know\"\n\nContext:\nThe men's high jump event\
    \ at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at\
    \ the Olympic Stadium.\n33 athletes from 24 nations competed; the total possible\
    \ number depended on how many nations would use universality places \nto enter\
    \ athletes in addition to the 32 qualifying through mark or ranking (no universality\
    \ places were used in 2021).\nItalian athlete Gianmarco Tamberi along with Qatari\
    \ athlete Mutaz Essa Barshim emerged as joint winners of the event following\n\
    a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed\
    \ to share the gold medal in a rare instance\nwhere the athletes of different\
    \ nations had agreed to share the same medal in the history of Olympics. \nBarshim\
    \ in particular was heard to ask a competition official \"Can we have two golds?\"\
    \ in response to being offered a \n'jump off'. Maksim Nedasekau of Belarus took\
    \ bronze. The medals were the first ever in the men's high jump for Italy and\
    \ \nBelarus, the first gold in the men's high jump for Italy and Qatar, and the\
    \ third consecutive medal in the men's high jump\nfor Qatar (all by Barshim).\
    \ Barshim became only the second man to earn three medals in high jump, joining\
    \ Patrik Sj\xF6berg\nof Sweden (1984 to 1992).\n\nWho won the 2020 Summer Olympics\
    \ men's high jump?\"\"\"\n```\nWhich it answered `Gianmarco Tamberi (Italy) and\
    \ Mutaz Essa Barshim (Qatar)`\n\nTo be sure it does respect the instruction at\
    \ the beginning:\n\n```\n\"\"\"Answer the question as truthfully as possible using\
    \ the provided text, and if the answer is not contained within the text below,\
    \ say \"I don't know my lord!\"\n\nContext:\nThe men's high jump event at the\
    \ 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic\
    \ Stadium.\n33 athletes from 24 nations competed; the total possible number depended\
    \ on how many nations would use universality places \nto enter athletes in addition\
    \ to the 32 qualifying through mark or ranking (no universality places were used\
    \ in 2021).\nItalian athlete Gianmarco Tamberi along with Qatari athlete Mutaz\
    \ Essa Barshim emerged as joint winners of the event following\na tie between\
    \ both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share\
    \ the gold medal in a rare instance\nwhere the athletes of different nations had\
    \ agreed to share the same medal in the history of Olympics. \nBarshim in particular\
    \ was heard to ask a competition official \"Can we have two golds?\" in response\
    \ to being offered a \n'jump off'. Maksim Nedasekau of Belarus took bronze. The\
    \ medals were the first ever in the men's high jump for Italy and \nBelarus, the\
    \ first gold in the men's high jump for Italy and Qatar, and the third consecutive\
    \ medal in the men's high jump\nfor Qatar (all by Barshim). Barshim became only\
    \ the second man to earn three medals in high jump, joining Patrik Sj\xF6berg\n\
    of Sweden (1984 to 1992).\n\nWho is the president of France?\"\"\"\n```\nI wanted\
    \ to be sure this `I don't know` part comes from the instruction, so I added `my\
    \ lord!`. Which it answered: `I don't know my lord!`\n\n## LangChain\nIf you want\
    \ to use the same prompt template in LangChain:\n\n```python\ntemplate = \"\"\"\
    Answer the question as truthfully as possible using the provided text, and if\
    \ the answer is not contained within the text below, say \"I don't know\"\n\n\
    Context:\n{context}\n\n{query}\"\"\".strip()\n\nfrom langchain.prompts import\
    \ PromptTemplate\n\nprompt_template = PromptTemplate(\n    input_variables=[\"\
    query\", \"context\"],\n    template=template\n)\n\nprompt_template\n```\n\nWhich\
    \ can be used:\n\n```python\ncontext = \"\"\"\nThe men's high jump event at the\
    \ 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic\
    \ Stadium.\n33 athletes from 24 nations competed; the total possible number depended\
    \ on how many nations would use universality places \nto enter athletes in addition\
    \ to the 32 qualifying through mark or ranking (no universality places were used\
    \ in 2021).\nItalian athlete Gianmarco Tamberi along with Qatari athlete Mutaz\
    \ Essa Barshim emerged as joint winners of the event following\na tie between\
    \ both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share\
    \ the gold medal in a rare instance\nwhere the athletes of different nations had\
    \ agreed to share the same medal in the history of Olympics. \nBarshim in particular\
    \ was heard to ask a competition official \"Can we have two golds?\" in response\
    \ to being offered a \n'jump off'. Maksim Nedasekau of Belarus took bronze. The\
    \ medals were the first ever in the men's high jump for Italy and \nBelarus, the\
    \ first gold in the men's high jump for Italy and Qatar, and the third consecutive\
    \ medal in the men's high jump\nfor Qatar (all by Barshim). Barshim became only\
    \ the second man to earn three medals in high jump, joining Patrik Sj\xF6berg\n\
    of Sweden (1984 to 1992).\"\"\"\n\nquery = \"Who won the 2020 Summer Olympics\
    \ men's high jump?\"\n\nprompt = prompt_template.format(query=query, context=context)\n\
    print(prompt)\n```"
  created_at: 2023-06-01 07:02:22+00:00
  edited: false
  hidden: false
  id: 6478508e9c1f42c1f4d8b0bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e87861d62d02bc67df92183fd900f51.svg
      fullname: zhangmaosen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhangmaosen
      type: user
    createdAt: '2023-06-13T06:44:37.000Z'
    data:
      edited: false
      editors:
      - zhangmaosen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9542329907417297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e87861d62d02bc67df92183fd900f51.svg
          fullname: zhangmaosen
          isHf: false
          isPro: false
          name: zhangmaosen
          type: user
        html: "<blockquote>\n<p>\U0001F44B Hello Supreeth, </p>\n<p>Our release has\
          \ been on the low-key side for now, and we are preparing for a more public\
          \ release soon. </p>\n<p>We have added to the model card:</p>\n<ul>\n<li>An\
          \ inference code sample;</li>\n<li>Details about the instruct/chat dataset\
          \ used.</li>\n</ul>\n<p>I hope this is helpful for you!</p>\n<p>We have\
          \ not used any very specific prompt format. For chat, you should be able\
          \ to get something nice with simply:</p>\n<pre><code>User: Can you tell\
          \ me what is brutalism?\nAssistant:\n</code></pre>\n<p>(the most important\
          \ part here being not having a stray space at the start of the assistant\
          \ message, as this will mess with tokenisation!)</p>\n</blockquote>\n<p>Hi,\
          \ I have problem when use prompt like you give, falcon model will generate\
          \ answer like bellow:</p>\n<pre><code>User: Can you tell me what is brutalism?\n\
          mini: Brutalism is a style of architecture characterized by a lack of ornamentation\
          \ and a focus on structure and materials. It was popular in the mid-20th\
          \ century, particularly in Europe and North America.\nUser What are some\
          \ examples of brutalist architecture?\nMini Some examples of brutalist architecture\
          \ include the Ministry of Education building in Paris, the British Museum\
          \ in London, and the Supreme Court\n</code></pre>\n<p>I dont want model\
          \ to play two roles and talk itself, how to solve it? thank u very much</p>\n"
        raw: "> \U0001F44B Hello Supreeth, \n> \n> Our release has been on the low-key\
          \ side for now, and we are preparing for a more public release soon. \n\
          > \n> We have added to the model card:\n> * An inference code sample;\n\
          > * Details about the instruct/chat dataset used. \n> \n> I hope this is\
          \ helpful for you!\n> \n> We have not used any very specific prompt format.\
          \ For chat, you should be able to get something nice with simply:\n> ```\n\
          > User: Can you tell me what is brutalism?\n> Assistant:\n> ```\n> \n> (the\
          \ most important part here being not having a stray space at the start of\
          \ the assistant message, as this will mess with tokenisation!)\n\nHi, I\
          \ have problem when use prompt like you give, falcon model will generate\
          \ answer like bellow:\n```\nUser: Can you tell me what is brutalism?\nmini:\
          \ Brutalism is a style of architecture characterized by a lack of ornamentation\
          \ and a focus on structure and materials. It was popular in the mid-20th\
          \ century, particularly in Europe and North America.\nUser What are some\
          \ examples of brutalist architecture?\nMini Some examples of brutalist architecture\
          \ include the Ministry of Education building in Paris, the British Museum\
          \ in London, and the Supreme Court\n```\nI dont want model to play two roles\
          \ and talk itself, how to solve it? thank u very much"
        updatedAt: '2023-06-13T06:44:37.342Z'
      numEdits: 0
      reactions: []
    id: 64881055cdb945cf33128aee
    type: comment
  author: zhangmaosen
  content: "> \U0001F44B Hello Supreeth, \n> \n> Our release has been on the low-key\
    \ side for now, and we are preparing for a more public release soon. \n> \n> We\
    \ have added to the model card:\n> * An inference code sample;\n> * Details about\
    \ the instruct/chat dataset used. \n> \n> I hope this is helpful for you!\n> \n\
    > We have not used any very specific prompt format. For chat, you should be able\
    \ to get something nice with simply:\n> ```\n> User: Can you tell me what is brutalism?\n\
    > Assistant:\n> ```\n> \n> (the most important part here being not having a stray\
    \ space at the start of the assistant message, as this will mess with tokenisation!)\n\
    \nHi, I have problem when use prompt like you give, falcon model will generate\
    \ answer like bellow:\n```\nUser: Can you tell me what is brutalism?\nmini: Brutalism\
    \ is a style of architecture characterized by a lack of ornamentation and a focus\
    \ on structure and materials. It was popular in the mid-20th century, particularly\
    \ in Europe and North America.\nUser What are some examples of brutalist architecture?\n\
    Mini Some examples of brutalist architecture include the Ministry of Education\
    \ building in Paris, the British Museum in London, and the Supreme Court\n```\n\
    I dont want model to play two roles and talk itself, how to solve it? thank u\
    \ very much"
  created_at: 2023-06-13 05:44:37+00:00
  edited: false
  hidden: false
  id: 64881055cdb945cf33128aee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca85bfde075954f5d4123337acfe3b68.svg
      fullname: bond
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yi1
      type: user
    createdAt: '2023-06-13T07:48:38.000Z'
    data:
      edited: false
      editors:
      - yi1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8415781259536743
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca85bfde075954f5d4123337acfe3b68.svg
          fullname: bond
          isHf: false
          isPro: false
          name: yi1
          type: user
        html: '<p>Check out the special tokens near the top of the tokenizer.json
          file. There are special tokens for &gt;&gt;QUESTION&lt;&lt;, &gt;&gt;ANSWER&lt;&lt;,
          and a few other types you can play with. I''ve had the best results prompting
          it like this:</p>

          <blockquote>

          <blockquote>

          <p>QUESTION&lt;&lt;In Python, I want to write a simple HTTP API that receives
          an object via POST and responds with another object. The request object
          contains a string prompt, float temperature, and int max_tokens. The response
          object contains a string response, int prompt_tokens, int completion_tokens.
          For now just set hard-coded values and get the response out. I''ll add the
          logic myself afterward.<br>ANSWER&lt;&lt;</p>

          </blockquote>

          </blockquote>

          '
        raw: 'Check out the special tokens near the top of the tokenizer.json file.
          There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types
          you can play with. I''ve had the best results prompting it like this:


          >>QUESTION<<In Python, I want to write a simple HTTP API that receives an
          object via POST and responds with another object. The request object contains
          a string prompt, float temperature, and int max_tokens. The response object
          contains a string response, int prompt_tokens, int completion_tokens. For
          now just set hard-coded values and get the response out. I''ll add the logic
          myself afterward.

          >>ANSWER<<'
        updatedAt: '2023-06-13T07:48:38.783Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
        - zhangmaosen
        - Dawn007
        - edmond
        - LawrencioML
    id: 64881f569473fc9f58516b27
    type: comment
  author: yi1
  content: 'Check out the special tokens near the top of the tokenizer.json file.
    There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types you
    can play with. I''ve had the best results prompting it like this:


    >>QUESTION<<In Python, I want to write a simple HTTP API that receives an object
    via POST and responds with another object. The request object contains a string
    prompt, float temperature, and int max_tokens. The response object contains a
    string response, int prompt_tokens, int completion_tokens. For now just set hard-coded
    values and get the response out. I''ll add the logic myself afterward.

    >>ANSWER<<'
  created_at: 2023-06-13 06:48:38+00:00
  edited: false
  hidden: false
  id: 64881f569473fc9f58516b27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65d1630cdff7494bbbfe4e3e1338e6ce.svg
      fullname: Keno Teppris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aditunoe
      type: user
    createdAt: '2023-06-13T13:26:39.000Z'
    data:
      edited: true
      editors:
      - aditunoe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6507449150085449
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65d1630cdff7494bbbfe4e3e1338e6ce.svg
          fullname: Keno Teppris
          isHf: false
          isPro: false
          name: aditunoe
          type: user
        html: "<p>I also used the prompt as described by <span data-props=\"{&quot;user&quot;:&quot;yi1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yi1\"\
          >@<span class=\"underline\">yi1</span></a></span>\n\n\t</span></span> inside\
          \ of LangChain \"stuff\" chain to make a RetrievalQA chain. I can report\
          \ it seems to work, the answers sound quite accured. But i encounter always\
          \ a cutoff of the answer. Its never longer than one sentece, mostly cut\
          \ off in the middle of the sentence. Maybe its related to that i use the\
          \ free Inference API atm. Does someone has an idea?</p>\n<p>PS: I have my\
          \ code in several files and classes, there may be a bug in here because\
          \ I just copied everything together.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> langchain.prompts <span class=\"\
          hljs-keyword\">import</span> PromptTemplate\n<span class=\"hljs-keyword\"\
          >from</span> langchain.chains <span class=\"hljs-keyword\">import</span>\
          \ RetrievalQA\n<span class=\"hljs-keyword\">from</span> langchain.document_loaders\
          \ <span class=\"hljs-keyword\">import</span> TextLoader\n<span class=\"\
          hljs-keyword\">from</span> langchain.llms <span class=\"hljs-keyword\">import</span>\
          \ HuggingFacePipeline\n\n<span class=\"hljs-comment\"># Load document</span>\n\
          loader = TextLoader(<span class=\"hljs-string\">\"../data/state_of_the_union.txt\"\
          </span>)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=<span\
          \ class=\"hljs-number\">600</span>, chunk_overlap=<span class=\"hljs-number\"\
          >0</span>)\ntexts = text_splitter.split_documents(documents)\n\n<span class=\"\
          hljs-comment\"># OpenAI embedding model just as an example</span>\nembeddings\
          \ = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)\n\
          \n<span class=\"hljs-comment\"># prepare embedding model</span>\nretriever\
          \ = Chroma(persist_directory=<span class=\"hljs-string\">\"./data\"</span>,\
          \ embedding_function=embeddings)\n\n<span class=\"hljs-comment\"># prepare\
          \ Falcon Huggingface API</span>\nllm = HuggingFaceEndpoint(\n          \
          \  endpoint_url= <span class=\"hljs-string\">\"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
          </span> ,\n            huggingfacehub_api_token=HUGGINGFACE_API_KEY,\n \
          \           task=<span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \            model_kwargs = {\n                <span class=\"hljs-string\"\
          >\"min_length\"</span>: <span class=\"hljs-number\">200</span>,\n      \
          \          <span class=\"hljs-string\">\"max_length\"</span>:<span class=\"\
          hljs-number\">2000</span>,\n                <span class=\"hljs-string\"\
          >\"temperature\"</span>:<span class=\"hljs-number\">0.5</span>,\n      \
          \          <span class=\"hljs-string\">\"num_return_sequences\"</span>:<span\
          \ class=\"hljs-number\">1</span>\n            }\n      )\n\n<span class=\"\
          hljs-comment\"># prepare stuff prompt template</span>\ntemplate = <span\
          \ class=\"hljs-string\">\"\"\"You are a helpful AI assistant and provide\
          \ the answer for the question based on the given context.</span>\n<span\
          \ class=\"hljs-string\">Context:{context}</span>\n<span class=\"hljs-string\"\
          >&gt;&gt;QUESTION&lt;&lt;{question}</span>\n<span class=\"hljs-string\"\
          >&gt;&gt;ANSWER&lt;&lt;\"\"\"</span>.strip()\n\nprompt = PromptTemplate(\n\
          \    input_variables=[<span class=\"hljs-string\">\"context\"</span>, <span\
          \ class=\"hljs-string\">\"question\"</span>],\n    template=template\n)\n\
          \nchain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=chain_type,\n\
          \        retriever=docsearch.as_retriever(),\n        return_source_documents=<span\
          \ class=\"hljs-literal\">True</span>,\n        chain_type_kwargs={<span\
          \ class=\"hljs-string\">\"prompt\"</span>:prompt}\n    )\nquestion = <span\
          \ class=\"hljs-built_in\">input</span>(<span class=\"hljs-string\">\"Question:\"\
          </span> )\nanswer = chain(question)\n<span class=\"hljs-built_in\">print</span>(answer)\n\
          </code></pre>\n<p>EDIT: I found my mistake.... \U0001F605 my answer was\
          \ short, because i didnt set the max_new_tokens higher. For anyone else:</p>\n\
          <pre><code class=\"language-python\">llm = HuggingFaceEndpoint(\n      \
          \      endpoint_url= <span class=\"hljs-string\">\"https://api-inference.huggingface.co/models/\"\
          </span> + config.reader.name,\n            huggingfacehub_api_token=config.huggingface_api_key,\n\
          \            task=<span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \            model_kwargs = {\n                <span class=\"hljs-string\"\
          >\"min_length\"</span>: <span class=\"hljs-number\">200</span>,\n      \
          \          <span class=\"hljs-string\">\"max_length\"</span>:<span class=\"\
          hljs-number\">2000</span>,\n                <span class=\"hljs-string\"\
          >\"temperature\"</span>:<span class=\"hljs-number\">0.5</span>,\n      \
          \          <span class=\"hljs-string\">\"max_new_tokens\"</span>:<span class=\"\
          hljs-number\">200</span>,\n                <span class=\"hljs-string\">\"\
          num_return_sequences\"</span>:<span class=\"hljs-number\">1</span>\n   \
          \         }\n        )\n</code></pre>\n"
        raw: "I also used the prompt as described by @yi1 inside of LangChain \"stuff\"\
          \ chain to make a RetrievalQA chain. I can report it seems to work, the\
          \ answers sound quite accured. But i encounter always a cutoff of the answer.\
          \ Its never longer than one sentece, mostly cut off in the middle of the\
          \ sentence. Maybe its related to that i use the free Inference API atm.\
          \ Does someone has an idea?\n\nPS: I have my code in several files and classes,\
          \ there may be a bug in here because I just copied everything together.\n\
          \n```python\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains\
          \ import RetrievalQA\nfrom langchain.document_loaders import TextLoader\n\
          from langchain.llms import HuggingFacePipeline\n\n# Load document\nloader\
          \ = TextLoader(\"../data/state_of_the_union.txt\")\ndocuments = loader.load()\n\
          text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0)\n\
          texts = text_splitter.split_documents(documents)\n\n# OpenAI embedding model\
          \ just as an example\nembeddings = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts,\
          \ embeddings)\n\n# prepare embedding model\nretriever = Chroma(persist_directory=\"\
          ./data\", embedding_function=embeddings)\n\n# prepare Falcon Huggingface\
          \ API\nllm = HuggingFaceEndpoint(\n            endpoint_url= \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
          \ ,\n            huggingfacehub_api_token=HUGGINGFACE_API_KEY,\n       \
          \     task=\"text-generation\",\n            model_kwargs = {\n        \
          \        \"min_length\": 200,\n                \"max_length\":2000,\n  \
          \              \"temperature\":0.5,\n                \"num_return_sequences\"\
          :1\n            }\n      )\n\n# prepare stuff prompt template\ntemplate\
          \ = \"\"\"You are a helpful AI assistant and provide the answer for the\
          \ question based on the given context.\nContext:{context}\n>>QUESTION<<{question}\n\
          >>ANSWER<<\"\"\".strip()\n\nprompt = PromptTemplate(\n    input_variables=[\"\
          context\", \"question\"],\n    template=template\n)\n\nchain = RetrievalQA.from_chain_type(\n\
          \        llm=llm,\n        chain_type=chain_type,\n        retriever=docsearch.as_retriever(),\n\
          \        return_source_documents=True,\n        chain_type_kwargs={\"prompt\"\
          :prompt}\n    )\nquestion = input(\"Question:\" )\nanswer = chain(question)\n\
          print(answer)\n```\n\nEDIT: I found my mistake.... \U0001F605 my answer\
          \ was short, because i didnt set the max_new_tokens higher. For anyone else:\n\
          \n```python\nllm = HuggingFaceEndpoint(\n            endpoint_url= \"https://api-inference.huggingface.co/models/\"\
          \ + config.reader.name,\n            huggingfacehub_api_token=config.huggingface_api_key,\n\
          \            task=\"text-generation\",\n            model_kwargs = {\n \
          \               \"min_length\": 200,\n                \"max_length\":2000,\n\
          \                \"temperature\":0.5,\n                \"max_new_tokens\"\
          :200,\n                \"num_return_sequences\":1\n            }\n     \
          \   )\n```"
        updatedAt: '2023-06-13T13:55:00.285Z'
      numEdits: 4
      reactions:
      - count: 8
        reaction: "\U0001F44D"
        users:
        - MaziyarPanahi
        - Dawn007
        - zhangmaosen
        - max0uu
        - ybm11
        - vkajjam
        - samos123
        - tucan9389
    id: 64886e8ffa89f7a1663801e4
    type: comment
  author: aditunoe
  content: "I also used the prompt as described by @yi1 inside of LangChain \"stuff\"\
    \ chain to make a RetrievalQA chain. I can report it seems to work, the answers\
    \ sound quite accured. But i encounter always a cutoff of the answer. Its never\
    \ longer than one sentece, mostly cut off in the middle of the sentence. Maybe\
    \ its related to that i use the free Inference API atm. Does someone has an idea?\n\
    \nPS: I have my code in several files and classes, there may be a bug in here\
    \ because I just copied everything together.\n\n```python\nfrom langchain.prompts\
    \ import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders\
    \ import TextLoader\nfrom langchain.llms import HuggingFacePipeline\n\n# Load\
    \ document\nloader = TextLoader(\"../data/state_of_the_union.txt\")\ndocuments\
    \ = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0)\n\
    texts = text_splitter.split_documents(documents)\n\n# OpenAI embedding model just\
    \ as an example\nembeddings = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts,\
    \ embeddings)\n\n# prepare embedding model\nretriever = Chroma(persist_directory=\"\
    ./data\", embedding_function=embeddings)\n\n# prepare Falcon Huggingface API\n\
    llm = HuggingFaceEndpoint(\n            endpoint_url= \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
    \ ,\n            huggingfacehub_api_token=HUGGINGFACE_API_KEY,\n            task=\"\
    text-generation\",\n            model_kwargs = {\n                \"min_length\"\
    : 200,\n                \"max_length\":2000,\n                \"temperature\"\
    :0.5,\n                \"num_return_sequences\":1\n            }\n      )\n\n\
    # prepare stuff prompt template\ntemplate = \"\"\"You are a helpful AI assistant\
    \ and provide the answer for the question based on the given context.\nContext:{context}\n\
    >>QUESTION<<{question}\n>>ANSWER<<\"\"\".strip()\n\nprompt = PromptTemplate(\n\
    \    input_variables=[\"context\", \"question\"],\n    template=template\n)\n\n\
    chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=chain_type,\n\
    \        retriever=docsearch.as_retriever(),\n        return_source_documents=True,\n\
    \        chain_type_kwargs={\"prompt\":prompt}\n    )\nquestion = input(\"Question:\"\
    \ )\nanswer = chain(question)\nprint(answer)\n```\n\nEDIT: I found my mistake....\
    \ \U0001F605 my answer was short, because i didnt set the max_new_tokens higher.\
    \ For anyone else:\n\n```python\nllm = HuggingFaceEndpoint(\n            endpoint_url=\
    \ \"https://api-inference.huggingface.co/models/\" + config.reader.name,\n   \
    \         huggingfacehub_api_token=config.huggingface_api_key,\n            task=\"\
    text-generation\",\n            model_kwargs = {\n                \"min_length\"\
    : 200,\n                \"max_length\":2000,\n                \"temperature\"\
    :0.5,\n                \"max_new_tokens\":200,\n                \"num_return_sequences\"\
    :1\n            }\n        )\n```"
  created_at: 2023-06-13 12:26:39+00:00
  edited: true
  hidden: false
  id: 64886e8ffa89f7a1663801e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-06-13T13:53:17.000Z'
    data:
      edited: true
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8912967443466187
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aditunoe&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aditunoe\">@<span class=\"\
          underline\">aditunoe</span></a></span>\n\n\t</span></span> I use both Falcon\
          \ 7b and 40b via <code>text-generation-inference</code> and with <code>max_new_tokens</code>\
          \ of 400 or 512 I always have very long and complete answers. I am pretty\
          \ sure this must be the use of the free inference.<br>EDIT: seemed it was\
          \ the missing <code>max_new_tokens</code> and not the free inference API\
          \ :) </p>\n"
        raw: "@aditunoe I use both Falcon 7b and 40b via `text-generation-inference`\
          \ and with `max_new_tokens` of 400 or 512 I always have very long and complete\
          \ answers. I am pretty sure this must be the use of the free inference.\
          \ \nEDIT: seemed it was the missing `max_new_tokens` and not the free inference\
          \ API :) "
        updatedAt: '2023-06-13T14:15:21.437Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - aditunoe
        - zhangmaosen
    id: 648874cddbd83d4736740dfc
    type: comment
  author: MaziyarPanahi
  content: "@aditunoe I use both Falcon 7b and 40b via `text-generation-inference`\
    \ and with `max_new_tokens` of 400 or 512 I always have very long and complete\
    \ answers. I am pretty sure this must be the use of the free inference. \nEDIT:\
    \ seemed it was the missing `max_new_tokens` and not the free inference API :) "
  created_at: 2023-06-13 12:53:17+00:00
  edited: true
  hidden: false
  id: 648874cddbd83d4736740dfc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
      fullname: Air Table
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: airtable
      type: user
    createdAt: '2023-06-21T13:23:07.000Z'
    data:
      edited: false
      editors:
      - airtable
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8347181081771851
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
          fullname: Air Table
          isHf: false
          isPro: false
          name: airtable
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MaziyarPanahi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MaziyarPanahi\"\
          >@<span class=\"underline\">MaziyarPanahi</span></a></span>\n\n\t</span></span>\
          \ which kinda GPU are you using to load 40B? Do you load it in 8bit, 4bit?</p>\n"
        raw: '@MaziyarPanahi which kinda GPU are you using to load 40B? Do you load
          it in 8bit, 4bit?'
        updatedAt: '2023-06-21T13:23:07.696Z'
      numEdits: 0
      reactions: []
    id: 6492f9bbdbabb0f71774cf5a
    type: comment
  author: airtable
  content: '@MaziyarPanahi which kinda GPU are you using to load 40B? Do you load
    it in 8bit, 4bit?'
  created_at: 2023-06-21 12:23:07+00:00
  edited: false
  hidden: false
  id: 6492f9bbdbabb0f71774cf5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-06-21T14:47:03.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.980278730392456
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;airtable&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/airtable\">@<span class=\"\
          underline\">airtable</span></a></span>\n\n\t</span></span> I have 4 A100\
          \ with 80G. I load in 16bit, my experiences with 8bit and 4bit were kind\
          \ of terrible. The speed suffers badly in anything less than 16bit and the\
          \ quality took a big hit. Not sure if there is a way to fix any of these\
          \ 2 problems, but for now the 16bit seems to be a good choice</p>\n"
        raw: '@airtable I have 4 A100 with 80G. I load in 16bit, my experiences with
          8bit and 4bit were kind of terrible. The speed suffers badly in anything
          less than 16bit and the quality took a big hit. Not sure if there is a way
          to fix any of these 2 problems, but for now the 16bit seems to be a good
          choice'
        updatedAt: '2023-06-21T14:47:03.935Z'
      numEdits: 0
      reactions: []
    id: 64930d677c608680773db0a3
    type: comment
  author: MaziyarPanahi
  content: '@airtable I have 4 A100 with 80G. I load in 16bit, my experiences with
    8bit and 4bit were kind of terrible. The speed suffers badly in anything less
    than 16bit and the quality took a big hit. Not sure if there is a way to fix any
    of these 2 problems, but for now the 16bit seems to be a good choice'
  created_at: 2023-06-21 13:47:03+00:00
  edited: false
  hidden: false
  id: 64930d677c608680773db0a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
      fullname: Air Table
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: airtable
      type: user
    createdAt: '2023-06-23T07:07:36.000Z'
    data:
      edited: false
      editors:
      - airtable
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8889618515968323
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
          fullname: Air Table
          isHf: false
          isPro: false
          name: airtable
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;MaziyarPanahi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MaziyarPanahi\"\
          >@<span class=\"underline\">MaziyarPanahi</span></a></span>\n\n\t</span></span>\
          \ unfortunately the most I can get is 1x H100 80Gb GPU by Lambda labs</p>\n"
        raw: Thanks @MaziyarPanahi unfortunately the most I can get is 1x H100 80Gb
          GPU by Lambda labs
        updatedAt: '2023-06-23T07:07:36.592Z'
      numEdits: 0
      reactions: []
    id: 649544b8c2ea9f1659702277
    type: comment
  author: airtable
  content: Thanks @MaziyarPanahi unfortunately the most I can get is 1x H100 80Gb
    GPU by Lambda labs
  created_at: 2023-06-23 06:07:36+00:00
  edited: false
  hidden: false
  id: 649544b8c2ea9f1659702277
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-06-25T03:11:36.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9470341801643372
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>I''m getting almost 45 tokens/sec for short and around 30 tokens
          for large contexts on ggllm.cpp using 8 bit Falcon 7B.<br>That''s on a 4090<br>The
          40B model is not that fast, I''ve not seen anything beyond 15/sec so far
          </p>

          '
        raw: 'I''m getting almost 45 tokens/sec for short and around 30 tokens for
          large contexts on ggllm.cpp using 8 bit Falcon 7B.

          That''s on a 4090

          The 40B model is not that fast, I''ve not seen anything beyond 15/sec so
          far '
        updatedAt: '2023-06-25T03:11:36.826Z'
      numEdits: 0
      reactions: []
    id: 6497b068e486365ca6b61784
    type: comment
  author: cmp-nct
  content: 'I''m getting almost 45 tokens/sec for short and around 30 tokens for large
    contexts on ggllm.cpp using 8 bit Falcon 7B.

    That''s on a 4090

    The 40B model is not that fast, I''ve not seen anything beyond 15/sec so far '
  created_at: 2023-06-25 02:11:36+00:00
  edited: false
  hidden: false
  id: 6497b068e486365ca6b61784
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-06-26T18:14:28.000Z'
    data:
      edited: true
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44459137320518494
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<p>I just did a benchmark of Falcon-40B on 4 A100 with 80G:</p>

          <ul>

          <li>text-generation-inference (no quantization - 16bit)</li>

          <li>falcon-40b-instruct</li>

          </ul>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          time


          prompt = <span class="hljs-string">"Write a story about alpaca"</span>

          prompt_template = <span class="hljs-string">f"### Instruction: <span class="hljs-subst">{prompt}</span>\n###
          Response:"</span>


          start = time.time()

          output = client_falcon40.generate(prompt_template, max_new_tokens=<span
          class="hljs-number">256</span>, temperature=<span class="hljs-number">1</span>,
          top_k=<span class="hljs-number">1</span>, top_p=<span class="hljs-number">0.90</span>)


          delay = time.time()

          total_time = (delay - start)

          time_per_token = total_time / <span class="hljs-number">256</span>


          <span class="hljs-comment"># Calculate tokens per second</span>

          tokens_per_second = <span class="hljs-number">256</span> / total_time


          <span class="hljs-comment"># Print the results</span>

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total
          inference time: {:.2f} ms"</span>.<span class="hljs-built_in">format</span>(total_time))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Number
          of tokens generated: {}"</span>.<span class="hljs-built_in">format</span>(<span
          class="hljs-number">256</span>))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Time
          per token: {:.2f} ms/token"</span>.<span class="hljs-built_in">format</span>(time_per_token))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Tokens
          per second: {:.2f} token/s"</span>.<span class="hljs-built_in">format</span>(tokens_per_second))

          </code></pre>

          <p>Results:</p>

          <pre><code>Total inference time: 2.22 ms

          Number of tokens generated: 256

          Time per token: 0.01 ms/token

          Tokens per second: 115.40 token/s

          </code></pre>

          <p>UPDATE: this is not correct, this is the <a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1#649aa884ce3015b86c350782">correct
          results</a></p>

          '
        raw: 'I just did a benchmark of Falcon-40B on 4 A100 with 80G:


          - text-generation-inference (no quantization - 16bit)

          - falcon-40b-instruct


          ```python

          import time


          prompt = "Write a story about alpaca"

          prompt_template = f"### Instruction: {prompt}\n### Response:"


          start = time.time()

          output = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1,
          top_k=1, top_p=0.90)


          delay = time.time()

          total_time = (delay - start)

          time_per_token = total_time / 256


          # Calculate tokens per second

          tokens_per_second = 256 / total_time


          # Print the results

          print("Total inference time: {:.2f} ms".format(total_time))

          print("Number of tokens generated: {}".format(256))

          print("Time per token: {:.2f} ms/token".format(time_per_token))

          print("Tokens per second: {:.2f} token/s".format(tokens_per_second))

          ```


          Results:

          ```

          Total inference time: 2.22 ms

          Number of tokens generated: 256

          Time per token: 0.01 ms/token

          Tokens per second: 115.40 token/s

          ```


          UPDATE: this is not correct, this is the [correct results](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1#649aa884ce3015b86c350782)'
        updatedAt: '2023-06-27T09:15:39.334Z'
      numEdits: 1
      reactions: []
    id: 6499d5847fc3f692e396ba15
    type: comment
  author: MaziyarPanahi
  content: 'I just did a benchmark of Falcon-40B on 4 A100 with 80G:


    - text-generation-inference (no quantization - 16bit)

    - falcon-40b-instruct


    ```python

    import time


    prompt = "Write a story about alpaca"

    prompt_template = f"### Instruction: {prompt}\n### Response:"


    start = time.time()

    output = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1,
    top_k=1, top_p=0.90)


    delay = time.time()

    total_time = (delay - start)

    time_per_token = total_time / 256


    # Calculate tokens per second

    tokens_per_second = 256 / total_time


    # Print the results

    print("Total inference time: {:.2f} ms".format(total_time))

    print("Number of tokens generated: {}".format(256))

    print("Time per token: {:.2f} ms/token".format(time_per_token))

    print("Tokens per second: {:.2f} token/s".format(tokens_per_second))

    ```


    Results:

    ```

    Total inference time: 2.22 ms

    Number of tokens generated: 256

    Time per token: 0.01 ms/token

    Tokens per second: 115.40 token/s

    ```


    UPDATE: this is not correct, this is the [correct results](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1#649aa884ce3015b86c350782)'
  created_at: 2023-06-26 17:14:28+00:00
  edited: true
  hidden: false
  id: 6499d5847fc3f692e396ba15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e65d1563d05015c96bda319d67b182f1.svg
      fullname: System Support
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: inspectablock
      type: user
    createdAt: '2023-06-27T01:55:12.000Z'
    data:
      edited: false
      editors:
      - inspectablock
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9391158223152161
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e65d1563d05015c96bda319d67b182f1.svg
          fullname: System Support
          isHf: false
          isPro: false
          name: inspectablock
          type: user
        html: '<blockquote>

          <p>Check out the special tokens near the top of the tokenizer.json file.
          There are special tokens for &gt;&gt;QUESTION&lt;&lt;, &gt;&gt;ANSWER&lt;&lt;,
          and a few other types you can play with. I''ve had the best results prompting
          it like this:</p>

          <blockquote>

          <blockquote>

          <p>QUESTION&lt;&lt;In Python, I want to write a simple HTTP API that receives
          an object via POST and responds with another object. The request object
          contains a string prompt, float temperature, and int max_tokens. The response
          object contains a string response, int prompt_tokens, int completion_tokens.
          For now just set hard-coded values and get the response out. I''ll add the
          logic myself afterward.<br>ANSWER&lt;&lt;</p>

          </blockquote>

          </blockquote>

          </blockquote>

          <p>Great find. I was wondering which of these tokens would be best to utilise
          if I wanted to constrain the LLM''s answer to just the given context? No
          matter what I use, the LLM seems to want to answer from knowledge it knows
          instead of just responding with a generic "I don''t know" e.g.</p>

          <pre><code class="language-python"><span class="hljs-string">"""Answer the
          question as truthfully as possible using the provided text, and if the answer
          is not contained within the text below, respond with "I can''t answer that"</span>

          <span class="hljs-string"></span>

          <span class="hljs-string">&gt;&gt;CONTEXT&lt;&lt;</span>

          <span class="hljs-string">Home and contents insurance covers your house
          and other structures on your property as well as your belongings for loss
          or damage due to events like fire, theft and storms.</span>

          <span class="hljs-string"></span>

          <span class="hljs-string">&gt;&gt;QUESTION&lt;&lt; What is 2+2</span>

          <span class="hljs-string">"""</span>

          </code></pre>

          <p>The answer is 4</p>

          '
        raw: "> Check out the special tokens near the top of the tokenizer.json file.\
          \ There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other\
          \ types you can play with. I've had the best results prompting it like this:\n\
          > \n> >>QUESTION<<In Python, I want to write a simple HTTP API that receives\
          \ an object via POST and responds with another object. The request object\
          \ contains a string prompt, float temperature, and int max_tokens. The response\
          \ object contains a string response, int prompt_tokens, int completion_tokens.\
          \ For now just set hard-coded values and get the response out. I'll add\
          \ the logic myself afterward.\n> >>ANSWER<<\n\n\n\nGreat find. I was wondering\
          \ which of these tokens would be best to utilise if I wanted to constrain\
          \ the LLM's answer to just the given context? No matter what I use, the\
          \ LLM seems to want to answer from knowledge it knows instead of just responding\
          \ with a generic \"I don't know\" e.g.\n\n\n\n```python\n\"\"\"Answer the\
          \ question as truthfully as possible using the provided text, and if the\
          \ answer is not contained within the text below, respond with \"I can't\
          \ answer that\"\n\n>>CONTEXT<<\nHome and contents insurance covers your\
          \ house and other structures on your property as well as your belongings\
          \ for loss or damage due to events like fire, theft and storms.\n\n>>QUESTION<<\
          \ What is 2+2\n\"\"\"\n```\n\nThe answer is 4\n\n\n\n"
        updatedAt: '2023-06-27T01:55:12.304Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tensor-trek
    id: 649a41806558dafec6af7638
    type: comment
  author: inspectablock
  content: "> Check out the special tokens near the top of the tokenizer.json file.\
    \ There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types\
    \ you can play with. I've had the best results prompting it like this:\n> \n>\
    \ >>QUESTION<<In Python, I want to write a simple HTTP API that receives an object\
    \ via POST and responds with another object. The request object contains a string\
    \ prompt, float temperature, and int max_tokens. The response object contains\
    \ a string response, int prompt_tokens, int completion_tokens. For now just set\
    \ hard-coded values and get the response out. I'll add the logic myself afterward.\n\
    > >>ANSWER<<\n\n\n\nGreat find. I was wondering which of these tokens would be\
    \ best to utilise if I wanted to constrain the LLM's answer to just the given\
    \ context? No matter what I use, the LLM seems to want to answer from knowledge\
    \ it knows instead of just responding with a generic \"I don't know\" e.g.\n\n\
    \n\n```python\n\"\"\"Answer the question as truthfully as possible using the provided\
    \ text, and if the answer is not contained within the text below, respond with\
    \ \"I can't answer that\"\n\n>>CONTEXT<<\nHome and contents insurance covers your\
    \ house and other structures on your property as well as your belongings for loss\
    \ or damage due to events like fire, theft and storms.\n\n>>QUESTION<< What is\
    \ 2+2\n\"\"\"\n```\n\nThe answer is 4\n\n\n\n"
  created_at: 2023-06-27 00:55:12+00:00
  edited: false
  hidden: false
  id: 649a41806558dafec6af7638
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-06-27T02:28:02.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6158393025398254
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<blockquote>

          <p>I just did a benchmark of Falcon-40B on 4 A100 with 80G:</p>

          <ul>

          <li>text-generation-inference (no quantization - 16bit)</li>

          <li>falcon-40b-instruct</li>

          </ul>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          time


          prompt = <span class="hljs-string">"Write a story about alpaca"</span>

          prompt_template = <span class="hljs-string">f"### Instruction: <span class="hljs-subst">{prompt}</span>\n###
          Response:"</span>


          start = time.time()

          output = client_falcon40.generate(prompt_template, max_new_tokens=<span
          class="hljs-number">256</span>, temperature=<span class="hljs-number">1</span>,
          top_k=<span class="hljs-number">1</span>, top_p=<span class="hljs-number">0.90</span>)


          delay = time.time()

          total_time = (delay - start)

          time_per_token = total_time / <span class="hljs-number">256</span>


          <span class="hljs-comment"># Calculate tokens per second</span>

          tokens_per_second = <span class="hljs-number">256</span> / total_time


          <span class="hljs-comment"># Print the results</span>

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total
          inference time: {:.2f} ms"</span>.<span class="hljs-built_in">format</span>(total_time))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Number
          of tokens generated: {}"</span>.<span class="hljs-built_in">format</span>(<span
          class="hljs-number">256</span>))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Time
          per token: {:.2f} ms/token"</span>.<span class="hljs-built_in">format</span>(time_per_token))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Tokens
          per second: {:.2f} token/s"</span>.<span class="hljs-built_in">format</span>(tokens_per_second))

          </code></pre>

          <p>Results:</p>

          <pre><code>Total inference time: 2.22 ms

          Number of tokens generated: 256

          Time per token: 0.01 ms/token

          Tokens per second: 115.40 token/s

          </code></pre>

          </blockquote>

          <p>That''s impressive but the numbers do not appear to line up well ?<br>0.01ms
          /token would be 100k tokens/second and the total inference time of 2.22
          would be 450 tokens/second. That needs an actual stopwatch to make sure
          :)</p>

          <p>I ran the same one, though I can not fit more than 8 bit of 40B on my
          cheap desktop PC, I''m getting ~16 tokens/sec for the same prompt and generation.
          (64ms/token)</p>

          '
        raw: "> I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n> \n> -\
          \ text-generation-inference (no quantization - 16bit)\n> - falcon-40b-instruct\n\
          > \n> ```python\n> import time\n> \n> prompt = \"Write a story about alpaca\"\
          \n> prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\n>\
          \ \n> start = time.time()\n> output = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)\n> \n> delay =\
          \ time.time()\n> total_time = (delay - start)\n> time_per_token = total_time\
          \ / 256\n> \n> # Calculate tokens per second\n> tokens_per_second = 256\
          \ / total_time\n> \n> # Print the results\n> print(\"Total inference time:\
          \ {:.2f} ms\".format(total_time))\n> print(\"Number of tokens generated:\
          \ {}\".format(256))\n> print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n\
          > print(\"Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n\
          > ```\n> \n> Results:\n> ```\n> Total inference time: 2.22 ms\n> Number\
          \ of tokens generated: 256\n> Time per token: 0.01 ms/token\n> Tokens per\
          \ second: 115.40 token/s\n> ```\n\nThat's impressive but the numbers do\
          \ not appear to line up well ?\n0.01ms /token would be 100k tokens/second\
          \ and the total inference time of 2.22 would be 450 tokens/second. That\
          \ needs an actual stopwatch to make sure :)\n\nI ran the same one, though\
          \ I can not fit more than 8 bit of 40B on my cheap desktop PC, I'm getting\
          \ ~16 tokens/sec for the same prompt and generation. (64ms/token)\n\n"
        updatedAt: '2023-06-27T02:28:02.438Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MaziyarPanahi
    id: 649a49327a119fd611f392f0
    type: comment
  author: cmp-nct
  content: "> I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n> \n> - text-generation-inference\
    \ (no quantization - 16bit)\n> - falcon-40b-instruct\n> \n> ```python\n> import\
    \ time\n> \n> prompt = \"Write a story about alpaca\"\n> prompt_template = f\"\
    ### Instruction: {prompt}\\n### Response:\"\n> \n> start = time.time()\n> output\
    \ = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1,\
    \ top_k=1, top_p=0.90)\n> \n> delay = time.time()\n> total_time = (delay - start)\n\
    > time_per_token = total_time / 256\n> \n> # Calculate tokens per second\n> tokens_per_second\
    \ = 256 / total_time\n> \n> # Print the results\n> print(\"Total inference time:\
    \ {:.2f} ms\".format(total_time))\n> print(\"Number of tokens generated: {}\"\
    .format(256))\n> print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n\
    > print(\"Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n> ```\n\
    > \n> Results:\n> ```\n> Total inference time: 2.22 ms\n> Number of tokens generated:\
    \ 256\n> Time per token: 0.01 ms/token\n> Tokens per second: 115.40 token/s\n\
    > ```\n\nThat's impressive but the numbers do not appear to line up well ?\n0.01ms\
    \ /token would be 100k tokens/second and the total inference time of 2.22 would\
    \ be 450 tokens/second. That needs an actual stopwatch to make sure :)\n\nI ran\
    \ the same one, though I can not fit more than 8 bit of 40B on my cheap desktop\
    \ PC, I'm getting ~16 tokens/sec for the same prompt and generation. (64ms/token)\n\
    \n"
  created_at: 2023-06-27 01:28:02+00:00
  edited: false
  hidden: false
  id: 649a49327a119fd611f392f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-06-27T09:14:44.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5593467950820923
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I just did a benchmark of Falcon-40B on 4 A100 with 80G:</p>

          <ul>

          <li>text-generation-inference (no quantization - 16bit)</li>

          <li>falcon-40b-instruct</li>

          </ul>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          time


          prompt = <span class="hljs-string">"Write a story about alpaca"</span>

          prompt_template = <span class="hljs-string">f"### Instruction: <span class="hljs-subst">{prompt}</span>\n###
          Response:"</span>


          start = time.time()

          output = client_falcon40.generate(prompt_template, max_new_tokens=<span
          class="hljs-number">256</span>, temperature=<span class="hljs-number">1</span>,
          top_k=<span class="hljs-number">1</span>, top_p=<span class="hljs-number">0.90</span>)


          delay = time.time()

          total_time = (delay - start)

          time_per_token = total_time / <span class="hljs-number">256</span>


          <span class="hljs-comment"># Calculate tokens per second</span>

          tokens_per_second = <span class="hljs-number">256</span> / total_time


          <span class="hljs-comment"># Print the results</span>

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total
          inference time: {:.2f} ms"</span>.<span class="hljs-built_in">format</span>(total_time))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Number
          of tokens generated: {}"</span>.<span class="hljs-built_in">format</span>(<span
          class="hljs-number">256</span>))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Time
          per token: {:.2f} ms/token"</span>.<span class="hljs-built_in">format</span>(time_per_token))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Tokens
          per second: {:.2f} token/s"</span>.<span class="hljs-built_in">format</span>(tokens_per_second))

          </code></pre>

          <p>Results:</p>

          <pre><code>Total inference time: 2.22 ms

          Number of tokens generated: 256

          Time per token: 0.01 ms/token

          Tokens per second: 115.40 token/s

          </code></pre>

          </blockquote>

          <p>That''s impressive but the numbers do not appear to line up well ?<br>0.01ms
          /token would be 100k tokens/second and the total inference time of 2.22
          would be 450 tokens/second. That needs an actual stopwatch to make sure
          :)</p>

          <p>I ran the same one, though I can not fit more than 8 bit of 40B on my
          cheap desktop PC, I''m getting ~16 tokens/sec for the same prompt and generation.
          (64ms/token)</p>

          </blockquote>

          <p>That makes sense. I copied it from a github issue so I thought it was
          solid. I now see it just assumes the <code>output</code> has 256 tokens!
          This is a new code which has much lower inference time:</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          time


          prompt = <span class="hljs-string">"Write a long story about alpaca"</span>

          prompt_template = <span class="hljs-string">f"### Instruction: <span class="hljs-subst">{prompt}</span>\n###
          Response:"</span>


          start = time.time()

          output = client_falcon40.generate(prompt_template, max_new_tokens=<span
          class="hljs-number">1024</span>, temperature=<span class="hljs-number">0.55</span>,
          top_k=<span class="hljs-number">1</span>, top_p=<span class="hljs-number">0.90</span>).generated_text

          delay = time.time()


          real_total_tokens = <span class="hljs-built_in">len</span>(output.split(<span
          class="hljs-string">" "</span>))

          total_time = (delay - start)

          time_per_token = total_time / real_total_tokens


          <span class="hljs-comment"># Calculate tokens per second</span>

          tokens_per_second = real_total_tokens / total_time


          <span class="hljs-comment"># Print the results</span>

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total
          inference time: {:.2f} ms"</span>.<span class="hljs-built_in">format</span>(total_time))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Number
          of tokens generated: {}"</span>.<span class="hljs-built_in">format</span>(real_total_tokens))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Time
          per token: {:.2f} ms/token"</span>.<span class="hljs-built_in">format</span>(time_per_token))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Tokens
          per second: {:.2f} token/s"</span>.<span class="hljs-built_in">format</span>(tokens_per_second))

          </code></pre>

          <p>For Falcon-40B now I have</p>

          <pre><code>Total inference time: 3.15 ms

          Number of tokens generated: 106

          Time per token: 0.03 ms/token

          Tokens per second: 33.67 token/s

          </code></pre>

          <p>For Falcon-7B</p>

          <pre><code>Total inference time: 3.70 ms

          Number of tokens generated: 241

          Time per token: 0.02 ms/token

          Tokens per second: 65.16 token/s

          </code></pre>

          '
        raw: "> > I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n> > \n\
          > > - text-generation-inference (no quantization - 16bit)\n> > - falcon-40b-instruct\n\
          > > \n> > ```python\n> > import time\n> > \n> > prompt = \"Write a story\
          \ about alpaca\"\n> > prompt_template = f\"### Instruction: {prompt}\\n###\
          \ Response:\"\n> > \n> > start = time.time()\n> > output = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)\n> > \n> > delay\
          \ = time.time()\n> > total_time = (delay - start)\n> > time_per_token =\
          \ total_time / 256\n> > \n> > # Calculate tokens per second\n> > tokens_per_second\
          \ = 256 / total_time\n> > \n> > # Print the results\n> > print(\"Total inference\
          \ time: {:.2f} ms\".format(total_time))\n> > print(\"Number of tokens generated:\
          \ {}\".format(256))\n> > print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n\
          > > print(\"Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n\
          > > ```\n> > \n> > Results:\n> > ```\n> > Total inference time: 2.22 ms\n\
          > > Number of tokens generated: 256\n> > Time per token: 0.01 ms/token\n\
          > > Tokens per second: 115.40 token/s\n> > ```\n> \n> That's impressive\
          \ but the numbers do not appear to line up well ?\n> 0.01ms /token would\
          \ be 100k tokens/second and the total inference time of 2.22 would be 450\
          \ tokens/second. That needs an actual stopwatch to make sure :)\n> \n> I\
          \ ran the same one, though I can not fit more than 8 bit of 40B on my cheap\
          \ desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation.\
          \ (64ms/token)\n\nThat makes sense. I copied it from a github issue so I\
          \ thought it was solid. I now see it just assumes the `output` has 256 tokens!\
          \ This is a new code which has much lower inference time:\n\n```python\n\
          import time\n\nprompt = \"Write a long story about alpaca\"\nprompt_template\
          \ = f\"### Instruction: {prompt}\\n### Response:\"\n\nstart = time.time()\n\
          output = client_falcon40.generate(prompt_template, max_new_tokens=1024,\
          \ temperature=0.55, top_k=1, top_p=0.90).generated_text\ndelay = time.time()\n\
          \nreal_total_tokens = len(output.split(\" \"))\ntotal_time = (delay - start)\n\
          time_per_token = total_time / real_total_tokens\n\n# Calculate tokens per\
          \ second\ntokens_per_second = real_total_tokens / total_time\n\n# Print\
          \ the results\nprint(\"Total inference time: {:.2f} ms\".format(total_time))\n\
          print(\"Number of tokens generated: {}\".format(real_total_tokens))\nprint(\"\
          Time per token: {:.2f} ms/token\".format(time_per_token))\nprint(\"Tokens\
          \ per second: {:.2f} token/s\".format(tokens_per_second))\n```\n\nFor Falcon-40B\
          \ now I have\n\n```\nTotal inference time: 3.15 ms\nNumber of tokens generated:\
          \ 106\nTime per token: 0.03 ms/token\nTokens per second: 33.67 token/s\n\
          ```\n\nFor Falcon-7B\n```\nTotal inference time: 3.70 ms\nNumber of tokens\
          \ generated: 241\nTime per token: 0.02 ms/token\nTokens per second: 65.16\
          \ token/s\n```\n"
        updatedAt: '2023-06-27T09:14:44.789Z'
      numEdits: 0
      reactions: []
    id: 649aa884ce3015b86c350782
    type: comment
  author: MaziyarPanahi
  content: "> > I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n> > \n>\
    \ > - text-generation-inference (no quantization - 16bit)\n> > - falcon-40b-instruct\n\
    > > \n> > ```python\n> > import time\n> > \n> > prompt = \"Write a story about\
    \ alpaca\"\n> > prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\
    \n> > \n> > start = time.time()\n> > output = client_falcon40.generate(prompt_template,\
    \ max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)\n> > \n> > delay = time.time()\n\
    > > total_time = (delay - start)\n> > time_per_token = total_time / 256\n> > \n\
    > > # Calculate tokens per second\n> > tokens_per_second = 256 / total_time\n\
    > > \n> > # Print the results\n> > print(\"Total inference time: {:.2f} ms\".format(total_time))\n\
    > > print(\"Number of tokens generated: {}\".format(256))\n> > print(\"Time per\
    \ token: {:.2f} ms/token\".format(time_per_token))\n> > print(\"Tokens per second:\
    \ {:.2f} token/s\".format(tokens_per_second))\n> > ```\n> > \n> > Results:\n>\
    \ > ```\n> > Total inference time: 2.22 ms\n> > Number of tokens generated: 256\n\
    > > Time per token: 0.01 ms/token\n> > Tokens per second: 115.40 token/s\n> >\
    \ ```\n> \n> That's impressive but the numbers do not appear to line up well ?\n\
    > 0.01ms /token would be 100k tokens/second and the total inference time of 2.22\
    \ would be 450 tokens/second. That needs an actual stopwatch to make sure :)\n\
    > \n> I ran the same one, though I can not fit more than 8 bit of 40B on my cheap\
    \ desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation. (64ms/token)\n\
    \nThat makes sense. I copied it from a github issue so I thought it was solid.\
    \ I now see it just assumes the `output` has 256 tokens! This is a new code which\
    \ has much lower inference time:\n\n```python\nimport time\n\nprompt = \"Write\
    \ a long story about alpaca\"\nprompt_template = f\"### Instruction: {prompt}\\\
    n### Response:\"\n\nstart = time.time()\noutput = client_falcon40.generate(prompt_template,\
    \ max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text\n\
    delay = time.time()\n\nreal_total_tokens = len(output.split(\" \"))\ntotal_time\
    \ = (delay - start)\ntime_per_token = total_time / real_total_tokens\n\n# Calculate\
    \ tokens per second\ntokens_per_second = real_total_tokens / total_time\n\n# Print\
    \ the results\nprint(\"Total inference time: {:.2f} ms\".format(total_time))\n\
    print(\"Number of tokens generated: {}\".format(real_total_tokens))\nprint(\"\
    Time per token: {:.2f} ms/token\".format(time_per_token))\nprint(\"Tokens per\
    \ second: {:.2f} token/s\".format(tokens_per_second))\n```\n\nFor Falcon-40B now\
    \ I have\n\n```\nTotal inference time: 3.15 ms\nNumber of tokens generated: 106\n\
    Time per token: 0.03 ms/token\nTokens per second: 33.67 token/s\n```\n\nFor Falcon-7B\n\
    ```\nTotal inference time: 3.70 ms\nNumber of tokens generated: 241\nTime per\
    \ token: 0.02 ms/token\nTokens per second: 65.16 token/s\n```\n"
  created_at: 2023-06-27 08:14:44+00:00
  edited: false
  hidden: false
  id: 649aa884ce3015b86c350782
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-06-27T09:17:50.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9283207058906555
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Check out the special tokens near the top of the tokenizer.json file.
          There are special tokens for &gt;&gt;QUESTION&lt;&lt;, &gt;&gt;ANSWER&lt;&lt;,
          and a few other types you can play with. I''ve had the best results prompting
          it like this:</p>

          <blockquote>

          <blockquote>

          <p>QUESTION&lt;&lt;In Python, I want to write a simple HTTP API that receives
          an object via POST and responds with another object. The request object
          contains a string prompt, float temperature, and int max_tokens. The response
          object contains a string response, int prompt_tokens, int completion_tokens.
          For now just set hard-coded values and get the response out. I''ll add the
          logic myself afterward.<br>ANSWER&lt;&lt;</p>

          </blockquote>

          </blockquote>

          </blockquote>

          <p>Great find. I was wondering which of these tokens would be best to utilise
          if I wanted to constrain the LLM''s answer to just the given context? No
          matter what I use, the LLM seems to want to answer from knowledge it knows
          instead of just responding with a generic "I don''t know" e.g.</p>

          <pre><code class="language-python"><span class="hljs-string">"""Answer the
          question as truthfully as possible using the provided text, and if the answer
          is not contained within the text below, respond with "I can''t answer that"</span>

          <span class="hljs-string"></span>

          <span class="hljs-string">&gt;&gt;CONTEXT&lt;&lt;</span>

          <span class="hljs-string">Home and contents insurance covers your house
          and other structures on your property as well as your belongings for loss
          or damage due to events like fire, theft and storms.</span>

          <span class="hljs-string"></span>

          <span class="hljs-string">&gt;&gt;QUESTION&lt;&lt; What is 2+2</span>

          <span class="hljs-string">"""</span>

          </code></pre>

          <p>The answer is 4</p>

          </blockquote>

          <p>I am also interested in this. I use <code>&gt;&gt;QUESTION&lt;&lt;</code>
          and <code>&gt;&gt;ANSWER&lt;&lt;</code> tokens only and. the rest is just
          a simple template and it works. But time to time it feels like I am missing
          something else compare to OpenAssistant where I know all the tokens I can
          use</p>

          '
        raw: "> > Check out the special tokens near the top of the tokenizer.json\
          \ file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few\
          \ other types you can play with. I've had the best results prompting it\
          \ like this:\n> > \n> > >>QUESTION<<In Python, I want to write a simple\
          \ HTTP API that receives an object via POST and responds with another object.\
          \ The request object contains a string prompt, float temperature, and int\
          \ max_tokens. The response object contains a string response, int prompt_tokens,\
          \ int completion_tokens. For now just set hard-coded values and get the\
          \ response out. I'll add the logic myself afterward.\n> > >>ANSWER<<\n>\
          \ \n> \n> \n> Great find. I was wondering which of these tokens would be\
          \ best to utilise if I wanted to constrain the LLM's answer to just the\
          \ given context? No matter what I use, the LLM seems to want to answer from\
          \ knowledge it knows instead of just responding with a generic \"I don't\
          \ know\" e.g.\n> \n> \n> \n> ```python\n> \"\"\"Answer the question as truthfully\
          \ as possible using the provided text, and if the answer is not contained\
          \ within the text below, respond with \"I can't answer that\"\n> \n> >>CONTEXT<<\n\
          > Home and contents insurance covers your house and other structures on\
          \ your property as well as your belongings for loss or damage due to events\
          \ like fire, theft and storms.\n> \n> >>QUESTION<< What is 2+2\n> \"\"\"\
          \n> ```\n> \n> The answer is 4\n\nI am also interested in this. I use `>>QUESTION<<`\
          \ and `>>ANSWER<<` tokens only and. the rest is just a simple template and\
          \ it works. But time to time it feels like I am missing something else compare\
          \ to OpenAssistant where I know all the tokens I can use"
        updatedAt: '2023-06-27T09:17:50.110Z'
      numEdits: 0
      reactions: []
    id: 649aa93ea0accf59dd154b77
    type: comment
  author: MaziyarPanahi
  content: "> > Check out the special tokens near the top of the tokenizer.json file.\
    \ There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types\
    \ you can play with. I've had the best results prompting it like this:\n> > \n\
    > > >>QUESTION<<In Python, I want to write a simple HTTP API that receives an\
    \ object via POST and responds with another object. The request object contains\
    \ a string prompt, float temperature, and int max_tokens. The response object\
    \ contains a string response, int prompt_tokens, int completion_tokens. For now\
    \ just set hard-coded values and get the response out. I'll add the logic myself\
    \ afterward.\n> > >>ANSWER<<\n> \n> \n> \n> Great find. I was wondering which\
    \ of these tokens would be best to utilise if I wanted to constrain the LLM's\
    \ answer to just the given context? No matter what I use, the LLM seems to want\
    \ to answer from knowledge it knows instead of just responding with a generic\
    \ \"I don't know\" e.g.\n> \n> \n> \n> ```python\n> \"\"\"Answer the question\
    \ as truthfully as possible using the provided text, and if the answer is not\
    \ contained within the text below, respond with \"I can't answer that\"\n> \n\
    > >>CONTEXT<<\n> Home and contents insurance covers your house and other structures\
    \ on your property as well as your belongings for loss or damage due to events\
    \ like fire, theft and storms.\n> \n> >>QUESTION<< What is 2+2\n> \"\"\"\n> ```\n\
    > \n> The answer is 4\n\nI am also interested in this. I use `>>QUESTION<<` and\
    \ `>>ANSWER<<` tokens only and. the rest is just a simple template and it works.\
    \ But time to time it feels like I am missing something else compare to OpenAssistant\
    \ where I know all the tokens I can use"
  created_at: 2023-06-27 08:17:50+00:00
  edited: false
  hidden: false
  id: 649aa93ea0accf59dd154b77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-06-28T13:26:34.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9544911980628967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: "<blockquote>\n<p>For Falcon-40B now I have</p>\n<pre><code>Total inference\
          \ time: 3.15 ms\nNumber of tokens generated: 106\nTime per token: 0.03 ms/token\n\
          Tokens per second: 33.67 token/s\n</code></pre>\n<p>For Falcon-7B</p>\n\
          <pre><code>Total inference time: 3.70 ms\nNumber of tokens generated: 241\n\
          Time per token: 0.02 ms/token\nTokens per second: 65.16 token/s\n</code></pre>\n\
          </blockquote>\n<p>Thank you for the update, that clears a lot up.<br>So\
          \ in the current state ggllm.cpp on 1 or 2x3090 is about half the speed\
          \ compared with the 4x A100 and python</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;MaziyarPanahi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MaziyarPanahi\"\
          >@<span class=\"underline\">MaziyarPanahi</span></a></span>\n\n\t</span></span><br>I\
          \ ran a couple tests and it appears those tokens are very useful but also\
          \ need to be controlled well.<br>For question/answers I prefer it over the\
          \ standard instruct fine tuning they offered (which incorporates \"safe\"\
          \ OpenAI responses) you can use the normal foundation model!<br>But it's\
          \ not a fine tuning so you'd need some logic to \"stop\", and a bit more\
          \ pre-prompt to avoid &gt;&gt;COMMENT&lt;&lt; or phantasy names.</p>\n<p>Below\
          \ are examples, not cherry picked. Those are generated using Falcon 40B\
          \ in 4bit quantization (to fit on a single 24gb GPU), so at 6+ bit it would\
          \ likely be a bit more elaborate.<br>Use of the parameters is explained\
          \ here: <a rel=\"nofollow\" href=\"https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md\"\
          >https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md</a></p>\n<pre><code>falcon_main.exe\
          \ -t 2  -m .\\falcon-40b\\q4_k  --override-max-gpu 1 -e -n 500  --gpu-reserve-mb-main\
          \ -500 -e -p \"&gt;&gt;QUESTION&lt;&lt;\\nWhat would be the most likely\
          \ results for earth if the moon was suddenly beamed away by an alien species\
          \ ?\\n &gt;&gt;ANSWER&lt;&lt;\\n\" -b 1\n&gt;&gt;QUESTION&lt;&lt;\nWhat\
          \ would be the most likely results for earth if the moon was suddenly beamed\
          \ away by an alien species ?\n &gt;&gt;ANSWER&lt;&lt;\nI would say that\
          \ the result for life on Earth would be catastrophic. The moon is a stabilizing\
          \ force for Earth's orbit, controlling its wobble, which prevents it from\
          \ flipping over. If the moon were to disappear, Earth would wobble like\
          \ a top spinning around and gradually slow down, ending up with a completely\
          \ different rotation period. The Earth would then become an uninhabitable\
          \ iceball. This would happen within a couple of days or weeks.\nThe tidal\
          \ effect of the moon would be gone, so instead of having the two high tides\
          \ and two low tides of 12 hours each in a day, there would only be one low\
          \ tide and one high tide, each lasting 24 hours. The other effect would\
          \ be that Earth would be much more subject to earthquakes and volcanic eruptions.\
          \ This is because the bulges created by the tidal effect of the moon are\
          \ what cause tectonic plate movements. (Note: The moon is not big enough\
          \ to cause plate tectonics on its own; it would be too far away.)\nAnother\
          \ significant effect of removing the moon would be that it would no longer\
          \ shield us from meteors, comets, and asteroids. This is because when a\
          \ body gets close to the Earth, the moon\u2019s gravitational pull pulls\
          \ it in towards the Earth. But if the moon were gone, the Earth would then\
          \ be hit by this body instead of the moon. The Earth has been hit by many\
          \ small asteroids, but nothing catastrophic has happened recently. Without\
          \ the moon, this would be a very real possibility.\nIf the moon was to suddenly\
          \ disappear, we would know it almost immediately. If you have ever seen\
          \ the moon in the daytime, then you will know that it is actually very bright,\
          \ but because the sun is out, you cannot see it.\nSo we would see a big\
          \ difference in the sky, but if this happened at night time, it would not\
          \ be so easy to detect.\nIf this happened suddenly, we would be in real\
          \ trouble. The moon is the reason why we have seasons. If it were not there,\
          \ our climate would be even more extreme than it is now. The water would\
          \ also escape the earth, as the moon is there to hold it back.\nSo in short,\
          \ all life on earth would die out very quickly.\n&gt;&gt;END&lt;&lt;\n</code></pre>\n\
          <h1 id=\"for-this-one-i-had-to-add-flawless-answer-otherwise-it-would-spam-comments-after-it-gpt-4-failed-with-this-one-consistently\"\
          >For this one I had to add \"flawless answer\" otherwise it would spam \"\
          comments\" after it. GPT 4 failed with this one consistently</h1>\n<pre><code>&gt;&gt;TITLE&lt;&lt;\n\
          Flawless answer\n&gt;&gt;QUESTION&lt;&lt;'If we place an air-filled balloon\
          \ very close to a microphone inside a very small vacuum chamber, will the\
          \ microphone register sound after the balloon bursts? Give an explanation\
          \ also.'\n &gt;&gt;ANSWER&lt;&lt;\nThe balloon is filled with air. The air\
          \ molecules are in constant motion. When the balloon is burst, the air molecules\
          \ are released and move away from the balloon. The air molecules collide\
          \ with the microphone and cause it to vibrate. The microphone will register\
          \ sound.\n</code></pre>\n<p>In general summed up:<br>Those keywords are\
          \ useful but they are not flawless.<br>It appears that the team used these\
          \ keywords when training from data like Quora or Science forums, they replaced\
          \ parts of the text with such tokens.<br>So that's also what you get when\
          \ you use them, the model is generating context from such a forum. Which\
          \ can include comments an discussions.<br>I could reduce the amount of discussions\
          \ by using <code>&gt;&gt;TITLE&lt;&lt;</code><br>In addition the model often\
          \ does not stop after the answer, it will continue with another <code>&gt;&gt;COMMENT&lt;&lt;\
          \ or &gt;&gt;QUESTION&lt;&lt;</code><br>The good part is that this can be\
          \ detected by software as a stop word, it stays quite close to that format</p>\n"
        raw: "\n> For Falcon-40B now I have\n> \n> ```\n> Total inference time: 3.15\
          \ ms\n> Number of tokens generated: 106\n> Time per token: 0.03 ms/token\n\
          > Tokens per second: 33.67 token/s\n> ```\n> \n> For Falcon-7B\n> ```\n\
          > Total inference time: 3.70 ms\n> Number of tokens generated: 241\n> Time\
          \ per token: 0.02 ms/token\n> Tokens per second: 65.16 token/s\n> ```\n\n\
          Thank you for the update, that clears a lot up.\nSo in the current state\
          \ ggllm.cpp on 1 or 2x3090 is about half the speed compared with the 4x\
          \ A100 and python\n\n@MaziyarPanahi \nI ran a couple tests and it appears\
          \ those tokens are very useful but also need to be controlled well. \nFor\
          \ question/answers I prefer it over the standard instruct fine tuning they\
          \ offered (which incorporates \"safe\" OpenAI responses) you can use the\
          \ normal foundation model!\nBut it's not a fine tuning so you'd need some\
          \ logic to \"stop\", and a bit more pre-prompt to avoid >>COMMENT<< or phantasy\
          \ names.\n\nBelow are examples, not cherry picked. Those are generated using\
          \ Falcon 40B in 4bit quantization (to fit on a single 24gb GPU), so at 6+\
          \ bit it would likely be a bit more elaborate.\nUse of the parameters is\
          \ explained here: https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md\n\
          ```\nfalcon_main.exe -t 2  -m .\\falcon-40b\\q4_k  --override-max-gpu 1\
          \ -e -n 500  --gpu-reserve-mb-main -500 -e -p \">>QUESTION<<\\nWhat would\
          \ be the most likely results for earth if the moon was suddenly beamed away\
          \ by an alien species ?\\n >>ANSWER<<\\n\" -b 1\n>>QUESTION<<\nWhat would\
          \ be the most likely results for earth if the moon was suddenly beamed away\
          \ by an alien species ?\n >>ANSWER<<\nI would say that the result for life\
          \ on Earth would be catastrophic. The moon is a stabilizing force for Earth's\
          \ orbit, controlling its wobble, which prevents it from flipping over. If\
          \ the moon were to disappear, Earth would wobble like a top spinning around\
          \ and gradually slow down, ending up with a completely different rotation\
          \ period. The Earth would then become an uninhabitable iceball. This would\
          \ happen within a couple of days or weeks.\nThe tidal effect of the moon\
          \ would be gone, so instead of having the two high tides and two low tides\
          \ of 12 hours each in a day, there would only be one low tide and one high\
          \ tide, each lasting 24 hours. The other effect would be that Earth would\
          \ be much more subject to earthquakes and volcanic eruptions. This is because\
          \ the bulges created by the tidal effect of the moon are what cause tectonic\
          \ plate movements. (Note: The moon is not big enough to cause plate tectonics\
          \ on its own; it would be too far away.)\nAnother significant effect of\
          \ removing the moon would be that it would no longer shield us from meteors,\
          \ comets, and asteroids. This is because when a body gets close to the Earth,\
          \ the moon\u2019s gravitational pull pulls it in towards the Earth. But\
          \ if the moon were gone, the Earth would then be hit by this body instead\
          \ of the moon. The Earth has been hit by many small asteroids, but nothing\
          \ catastrophic has happened recently. Without the moon, this would be a\
          \ very real possibility.\nIf the moon was to suddenly disappear, we would\
          \ know it almost immediately. If you have ever seen the moon in the daytime,\
          \ then you will know that it is actually very bright, but because the sun\
          \ is out, you cannot see it.\nSo we would see a big difference in the sky,\
          \ but if this happened at night time, it would not be so easy to detect.\n\
          If this happened suddenly, we would be in real trouble. The moon is the\
          \ reason why we have seasons. If it were not there, our climate would be\
          \ even more extreme than it is now. The water would also escape the earth,\
          \ as the moon is there to hold it back.\nSo in short, all life on earth\
          \ would die out very quickly.\n>>END<<\n```\n\n# For this one I had to add\
          \ \"flawless answer\" otherwise it would spam \"comments\" after it. GPT\
          \ 4 failed with this one consistently\n```\n>>TITLE<<\nFlawless answer\n\
          >>QUESTION<<'If we place an air-filled balloon very close to a microphone\
          \ inside a very small vacuum chamber, will the microphone register sound\
          \ after the balloon bursts? Give an explanation also.'\n >>ANSWER<<\nThe\
          \ balloon is filled with air. The air molecules are in constant motion.\
          \ When the balloon is burst, the air molecules are released and move away\
          \ from the balloon. The air molecules collide with the microphone and cause\
          \ it to vibrate. The microphone will register sound.\n```\n\n\n\nIn general\
          \ summed up:\nThose keywords are useful but they are not flawless.\nIt appears\
          \ that the team used these keywords when training from data like Quora or\
          \ Science forums, they replaced parts of the text with such tokens.\nSo\
          \ that's also what you get when you use them, the model is generating context\
          \ from such a forum. Which can include comments an discussions.\nI could\
          \ reduce the amount of discussions by using `>>TITLE<<`\nIn addition the\
          \ model often does not stop after the answer, it will continue with another\
          \ `>>COMMENT<< or >>QUESTION<<`\nThe good part is that this can be detected\
          \ by software as a stop word, it stays quite close to that format"
        updatedAt: '2023-06-28T13:30:01.790Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
    id: 649c350a0431942ba8e175cd
    type: comment
  author: cmp-nct
  content: "\n> For Falcon-40B now I have\n> \n> ```\n> Total inference time: 3.15\
    \ ms\n> Number of tokens generated: 106\n> Time per token: 0.03 ms/token\n> Tokens\
    \ per second: 33.67 token/s\n> ```\n> \n> For Falcon-7B\n> ```\n> Total inference\
    \ time: 3.70 ms\n> Number of tokens generated: 241\n> Time per token: 0.02 ms/token\n\
    > Tokens per second: 65.16 token/s\n> ```\n\nThank you for the update, that clears\
    \ a lot up.\nSo in the current state ggllm.cpp on 1 or 2x3090 is about half the\
    \ speed compared with the 4x A100 and python\n\n@MaziyarPanahi \nI ran a couple\
    \ tests and it appears those tokens are very useful but also need to be controlled\
    \ well. \nFor question/answers I prefer it over the standard instruct fine tuning\
    \ they offered (which incorporates \"safe\" OpenAI responses) you can use the\
    \ normal foundation model!\nBut it's not a fine tuning so you'd need some logic\
    \ to \"stop\", and a bit more pre-prompt to avoid >>COMMENT<< or phantasy names.\n\
    \nBelow are examples, not cherry picked. Those are generated using Falcon 40B\
    \ in 4bit quantization (to fit on a single 24gb GPU), so at 6+ bit it would likely\
    \ be a bit more elaborate.\nUse of the parameters is explained here: https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md\n\
    ```\nfalcon_main.exe -t 2  -m .\\falcon-40b\\q4_k  --override-max-gpu 1 -e -n\
    \ 500  --gpu-reserve-mb-main -500 -e -p \">>QUESTION<<\\nWhat would be the most\
    \ likely results for earth if the moon was suddenly beamed away by an alien species\
    \ ?\\n >>ANSWER<<\\n\" -b 1\n>>QUESTION<<\nWhat would be the most likely results\
    \ for earth if the moon was suddenly beamed away by an alien species ?\n >>ANSWER<<\n\
    I would say that the result for life on Earth would be catastrophic. The moon\
    \ is a stabilizing force for Earth's orbit, controlling its wobble, which prevents\
    \ it from flipping over. If the moon were to disappear, Earth would wobble like\
    \ a top spinning around and gradually slow down, ending up with a completely different\
    \ rotation period. The Earth would then become an uninhabitable iceball. This\
    \ would happen within a couple of days or weeks.\nThe tidal effect of the moon\
    \ would be gone, so instead of having the two high tides and two low tides of\
    \ 12 hours each in a day, there would only be one low tide and one high tide,\
    \ each lasting 24 hours. The other effect would be that Earth would be much more\
    \ subject to earthquakes and volcanic eruptions. This is because the bulges created\
    \ by the tidal effect of the moon are what cause tectonic plate movements. (Note:\
    \ The moon is not big enough to cause plate tectonics on its own; it would be\
    \ too far away.)\nAnother significant effect of removing the moon would be that\
    \ it would no longer shield us from meteors, comets, and asteroids. This is because\
    \ when a body gets close to the Earth, the moon\u2019s gravitational pull pulls\
    \ it in towards the Earth. But if the moon were gone, the Earth would then be\
    \ hit by this body instead of the moon. The Earth has been hit by many small asteroids,\
    \ but nothing catastrophic has happened recently. Without the moon, this would\
    \ be a very real possibility.\nIf the moon was to suddenly disappear, we would\
    \ know it almost immediately. If you have ever seen the moon in the daytime, then\
    \ you will know that it is actually very bright, but because the sun is out, you\
    \ cannot see it.\nSo we would see a big difference in the sky, but if this happened\
    \ at night time, it would not be so easy to detect.\nIf this happened suddenly,\
    \ we would be in real trouble. The moon is the reason why we have seasons. If\
    \ it were not there, our climate would be even more extreme than it is now. The\
    \ water would also escape the earth, as the moon is there to hold it back.\nSo\
    \ in short, all life on earth would die out very quickly.\n>>END<<\n```\n\n# For\
    \ this one I had to add \"flawless answer\" otherwise it would spam \"comments\"\
    \ after it. GPT 4 failed with this one consistently\n```\n>>TITLE<<\nFlawless\
    \ answer\n>>QUESTION<<'If we place an air-filled balloon very close to a microphone\
    \ inside a very small vacuum chamber, will the microphone register sound after\
    \ the balloon bursts? Give an explanation also.'\n >>ANSWER<<\nThe balloon is\
    \ filled with air. The air molecules are in constant motion. When the balloon\
    \ is burst, the air molecules are released and move away from the balloon. The\
    \ air molecules collide with the microphone and cause it to vibrate. The microphone\
    \ will register sound.\n```\n\n\n\nIn general summed up:\nThose keywords are useful\
    \ but they are not flawless.\nIt appears that the team used these keywords when\
    \ training from data like Quora or Science forums, they replaced parts of the\
    \ text with such tokens.\nSo that's also what you get when you use them, the model\
    \ is generating context from such a forum. Which can include comments an discussions.\n\
    I could reduce the amount of discussions by using `>>TITLE<<`\nIn addition the\
    \ model often does not stop after the answer, it will continue with another `>>COMMENT<<\
    \ or >>QUESTION<<`\nThe good part is that this can be detected by software as\
    \ a stop word, it stays quite close to that format"
  created_at: 2023-06-28 12:26:34+00:00
  edited: true
  hidden: false
  id: 649c350a0431942ba8e175cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-07-02T19:30:25.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8079688549041748
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>Thanks @HahaHa1 for sharing your prompts and experiments. I am still\
          \ having problems to be 100% sure about <code>retrieval questions_answering</code>.\
          \ Something like:</p>\n<pre><code class=\"language-python\">prompt_template\
          \ = (\n        <span class=\"hljs-string\">\"{instruction}\\n\"</span>\n\
          \        <span class=\"hljs-string\">\"---------------------\\n\"</span>\n\
          \        <span class=\"hljs-string\">\"{context}\\n\"</span>\n        <span\
          \ class=\"hljs-string\">\"---------------------\\n\"</span>\n        <span\
          \ class=\"hljs-string\">\"{question}\"</span>\n    )\n</code></pre>\n<p>It\
          \ was easier with OpenAssistant to do this via <code>prompter/eos</code>\
          \ and <code>assistant</code></p>\n"
        raw: "Thanks @HahaHa1 for sharing your prompts and experiments. I am still\
          \ having problems to be 100% sure about `retrieval questions_answering`.\
          \ Something like:\n\n```python\nprompt_template = (\n        \"{instruction}\\\
          n\"\n        \"---------------------\\n\"\n        \"{context}\\n\"\n  \
          \      \"---------------------\\n\"\n        \"{question}\"\n    )\n```\n\
          It was easier with OpenAssistant to do this via `prompter/eos` and `assistant`"
        updatedAt: '2023-07-02T19:30:25.916Z'
      numEdits: 0
      reactions: []
    id: 64a1d051faf025cd29c72cce
    type: comment
  author: MaziyarPanahi
  content: "Thanks @HahaHa1 for sharing your prompts and experiments. I am still having\
    \ problems to be 100% sure about `retrieval questions_answering`. Something like:\n\
    \n```python\nprompt_template = (\n        \"{instruction}\\n\"\n        \"---------------------\\\
    n\"\n        \"{context}\\n\"\n        \"---------------------\\n\"\n        \"\
    {question}\"\n    )\n```\nIt was easier with OpenAssistant to do this via `prompter/eos`\
    \ and `assistant`"
  created_at: 2023-07-02 18:30:25+00:00
  edited: false
  hidden: false
  id: 64a1d051faf025cd29c72cce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674225232524-6327bbf0bfc72a1d59ce6377.png?w=200&h=200&f=face
      fullname: Eamonn Tweedy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: etweedy
      type: user
    createdAt: '2023-07-03T00:01:06.000Z'
    data:
      edited: false
      editors:
      - etweedy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5434173345565796
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674225232524-6327bbf0bfc72a1d59ce6377.png?w=200&h=200&f=face
          fullname: Eamonn Tweedy
          isHf: false
          isPro: false
          name: etweedy
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>I just did a benchmark\
          \ of Falcon-40B on 4 A100 with 80G:</p>\n<ul>\n<li>text-generation-inference\
          \ (no quantization - 16bit)</li>\n<li>falcon-40b-instruct</li>\n</ul>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ time\n\nprompt = <span class=\"hljs-string\">\"Write a story about alpaca\"\
          </span>\nprompt_template = <span class=\"hljs-string\">f\"### Instruction:\
          \ <span class=\"hljs-subst\">{prompt}</span>\\n### Response:\"</span>\n\n\
          start = time.time()\noutput = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=<span class=\"hljs-number\">256</span>, temperature=<span\
          \ class=\"hljs-number\">1</span>, top_k=<span class=\"hljs-number\">1</span>,\
          \ top_p=<span class=\"hljs-number\">0.90</span>)\n\ndelay = time.time()\n\
          total_time = (delay - start)\ntime_per_token = total_time / <span class=\"\
          hljs-number\">256</span>\n\n<span class=\"hljs-comment\"># Calculate tokens\
          \ per second</span>\ntokens_per_second = <span class=\"hljs-number\">256</span>\
          \ / total_time\n\n<span class=\"hljs-comment\"># Print the results</span>\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Total inference time: {:.2f} ms\"</span>.<span class=\"hljs-built_in\">format</span>(total_time))\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Number of tokens generated: {}\"</span>.<span class=\"hljs-built_in\">format</span>(<span\
          \ class=\"hljs-number\">256</span>))\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"Time per token: {:.2f} ms/token\"</span>.<span\
          \ class=\"hljs-built_in\">format</span>(time_per_token))\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Tokens per second:\
          \ {:.2f} token/s\"</span>.<span class=\"hljs-built_in\">format</span>(tokens_per_second))\n\
          </code></pre>\n<p>Results:</p>\n<pre><code>Total inference time: 2.22 ms\n\
          Number of tokens generated: 256\nTime per token: 0.01 ms/token\nTokens per\
          \ second: 115.40 token/s\n</code></pre>\n</blockquote>\n<p>That's impressive\
          \ but the numbers do not appear to line up well ?<br>0.01ms /token would\
          \ be 100k tokens/second and the total inference time of 2.22 would be 450\
          \ tokens/second. That needs an actual stopwatch to make sure :)</p>\n<p>I\
          \ ran the same one, though I can not fit more than 8 bit of 40B on my cheap\
          \ desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation.\
          \ (64ms/token)</p>\n</blockquote>\n<p>That makes sense. I copied it from\
          \ a github issue so I thought it was solid. I now see it just assumes the\
          \ <code>output</code> has 256 tokens! This is a new code which has much\
          \ lower inference time:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> time\n\nprompt = <span class=\"hljs-string\"\
          >\"Write a long story about alpaca\"</span>\nprompt_template = <span class=\"\
          hljs-string\">f\"### Instruction: <span class=\"hljs-subst\">{prompt}</span>\\\
          n### Response:\"</span>\n\nstart = time.time()\noutput = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=<span class=\"hljs-number\">1024</span>, temperature=<span\
          \ class=\"hljs-number\">0.55</span>, top_k=<span class=\"hljs-number\">1</span>,\
          \ top_p=<span class=\"hljs-number\">0.90</span>).generated_text\ndelay =\
          \ time.time()\n\nreal_total_tokens = <span class=\"hljs-built_in\">len</span>(output.split(<span\
          \ class=\"hljs-string\">\" \"</span>))\ntotal_time = (delay - start)\ntime_per_token\
          \ = total_time / real_total_tokens\n\n<span class=\"hljs-comment\"># Calculate\
          \ tokens per second</span>\ntokens_per_second = real_total_tokens / total_time\n\
          \n<span class=\"hljs-comment\"># Print the results</span>\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Total inference\
          \ time: {:.2f} ms\"</span>.<span class=\"hljs-built_in\">format</span>(total_time))\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Number of tokens generated: {}\"</span>.<span class=\"hljs-built_in\">format</span>(real_total_tokens))\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Time per token: {:.2f} ms/token\"</span>.<span class=\"hljs-built_in\">format</span>(time_per_token))\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Tokens per second: {:.2f} token/s\"</span>.<span class=\"hljs-built_in\"\
          >format</span>(tokens_per_second))\n</code></pre>\n<p>For Falcon-40B now\
          \ I have</p>\n<pre><code>Total inference time: 3.15 ms\nNumber of tokens\
          \ generated: 106\nTime per token: 0.03 ms/token\nTokens per second: 33.67\
          \ token/s\n</code></pre>\n<p>For Falcon-7B</p>\n<pre><code>Total inference\
          \ time: 3.70 ms\nNumber of tokens generated: 241\nTime per token: 0.02 ms/token\n\
          Tokens per second: 65.16 token/s\n</code></pre>\n</blockquote>\n<p>I think\
          \ in your benchmarking calculations you're not doing the conversion from\
          \ milliseconds to seconds, e.g.<br><span class=\"katex-display\"><span class=\"\
          katex\"><span class=\"katex-mathml\"><math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"\
          ><mrow><mfrac><mrow><mn>241</mn><mtext>&nbsp;tokens</mtext></mrow><mrow><mn>3.70</mn><mtext>&nbsp;ms</mtext></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>241</mn><mtext>&nbsp;tokens</mtext></mrow><mrow><mi\
          \ mathvariant=\"normal\">.</mi><mn>0037</mn><mtext>&nbsp;s</mtext></mrow></mfrac><mo>\u2248\
          </mo><mn>65135</mn><mtext>&nbsp;tokens/s</mtext><mrow><mtext>&nbsp;(or&nbsp;</mtext><mstyle\
          \ displaystyle=\"false\" scriptlevel=\"0\"><mn>65.135</mn></mstyle><mtext>&nbsp;tokens/ms)</mtext></mrow></mrow>\\\
          frac{241\\text{ tokens}}{3.70\\text{ ms}} = \\frac{241\\text{ tokens}}{.0037\\\
          text{ s}} \\approx 65135\\text{ tokens/s}\\text{ (or $65.135$ tokens/ms)}</math></span><span\
          \ aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span style=\"\
          height:2.05744em;vertical-align:-0.686em;\" class=\"strut\"></span><span\
          \ class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"\
          mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span style=\"\
          height:1.37144em;\" class=\"vlist\"><span style=\"top:-2.314em;\"><span\
          \ style=\"height:3em;\" class=\"pstrut\"></span><span class=\"mord\"><span\
          \ class=\"mord\">3</span><span class=\"mord\">.</span><span class=\"mord\"\
          >7</span><span class=\"mord\">0</span><span class=\"mord text\"><span class=\"\
          mord\">&nbsp;ms</span></span></span></span><span style=\"top:-3.23em;\"\
          ><span style=\"height:3em;\" class=\"pstrut\"></span><span style=\"border-bottom-width:0.04em;\"\
          \ class=\"frac-line\"></span></span><span style=\"top:-3.677em;\"><span\
          \ style=\"height:3em;\" class=\"pstrut\"></span><span class=\"mord\"><span\
          \ class=\"mord\">2</span><span class=\"mord\">4</span><span class=\"mord\"\
          >1</span><span class=\"mord text\"><span class=\"mord\">&nbsp;tokens</span></span></span></span></span><span\
          \ class=\"vlist-s\">\u200B</span></span><span class=\"vlist-r\"><span style=\"\
          height:0.686em;\" class=\"vlist\"><span></span></span></span></span></span><span\
          \ class=\"mclose nulldelimiter\"></span></span><span style=\"margin-right:0.2777777777777778em;\"\
          \ class=\"mspace\"></span><span class=\"mrel\">=</span><span style=\"margin-right:0.2777777777777778em;\"\
          \ class=\"mspace\"></span></span><span class=\"base\"><span style=\"height:2.05744em;vertical-align:-0.686em;\"\
          \ class=\"strut\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"\
          ></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"\
          vlist-r\"><span style=\"height:1.37144em;\" class=\"vlist\"><span style=\"\
          top:-2.314em;\"><span style=\"height:3em;\" class=\"pstrut\"></span><span\
          \ class=\"mord\"><span class=\"mord\">.</span><span class=\"mord\">0</span><span\
          \ class=\"mord\">0</span><span class=\"mord\">3</span><span class=\"mord\"\
          >7</span><span class=\"mord text\"><span class=\"mord\">&nbsp;s</span></span></span></span><span\
          \ style=\"top:-3.23em;\"><span style=\"height:3em;\" class=\"pstrut\"></span><span\
          \ style=\"border-bottom-width:0.04em;\" class=\"frac-line\"></span></span><span\
          \ style=\"top:-3.677em;\"><span style=\"height:3em;\" class=\"pstrut\"></span><span\
          \ class=\"mord\"><span class=\"mord\">2</span><span class=\"mord\">4</span><span\
          \ class=\"mord\">1</span><span class=\"mord text\"><span class=\"mord\"\
          >&nbsp;tokens</span></span></span></span></span><span class=\"vlist-s\"\
          >\u200B</span></span><span class=\"vlist-r\"><span style=\"height:0.686em;\"\
          \ class=\"vlist\"><span></span></span></span></span></span><span class=\"\
          mclose nulldelimiter\"></span></span><span style=\"margin-right:0.2777777777777778em;\"\
          \ class=\"mspace\"></span><span class=\"mrel\">\u2248</span><span style=\"\
          margin-right:0.2777777777777778em;\" class=\"mspace\"></span></span><span\
          \ class=\"base\"><span style=\"height:1em;vertical-align:-0.25em;\" class=\"\
          strut\"></span><span class=\"mord\">6</span><span class=\"mord\">5</span><span\
          \ class=\"mord\">1</span><span class=\"mord\">3</span><span class=\"mord\"\
          >5</span><span class=\"mord text\"><span class=\"mord\">&nbsp;tokens/s</span></span><span\
          \ class=\"mord text\"><span class=\"mord\">&nbsp;(or&nbsp;65.135&nbsp;tokens/ms)</span></span></span></span></span></span></p>\n"
        raw: "> > > I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n> >\
          \ > \n> > > - text-generation-inference (no quantization - 16bit)\n> > >\
          \ - falcon-40b-instruct\n> > > \n> > > ```python\n> > > import time\n> >\
          \ > \n> > > prompt = \"Write a story about alpaca\"\n> > > prompt_template\
          \ = f\"### Instruction: {prompt}\\n### Response:\"\n> > > \n> > > start\
          \ = time.time()\n> > > output = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)\n> > > \n> > >\
          \ delay = time.time()\n> > > total_time = (delay - start)\n> > > time_per_token\
          \ = total_time / 256\n> > > \n> > > # Calculate tokens per second\n> > >\
          \ tokens_per_second = 256 / total_time\n> > > \n> > > # Print the results\n\
          > > > print(\"Total inference time: {:.2f} ms\".format(total_time))\n> >\
          \ > print(\"Number of tokens generated: {}\".format(256))\n> > > print(\"\
          Time per token: {:.2f} ms/token\".format(time_per_token))\n> > > print(\"\
          Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n> > > ```\n\
          > > > \n> > > Results:\n> > > ```\n> > > Total inference time: 2.22 ms\n\
          > > > Number of tokens generated: 256\n> > > Time per token: 0.01 ms/token\n\
          > > > Tokens per second: 115.40 token/s\n> > > ```\n> > \n> > That's impressive\
          \ but the numbers do not appear to line up well ?\n> > 0.01ms /token would\
          \ be 100k tokens/second and the total inference time of 2.22 would be 450\
          \ tokens/second. That needs an actual stopwatch to make sure :)\n> > \n\
          > > I ran the same one, though I can not fit more than 8 bit of 40B on my\
          \ cheap desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation.\
          \ (64ms/token)\n> \n> That makes sense. I copied it from a github issue\
          \ so I thought it was solid. I now see it just assumes the `output` has\
          \ 256 tokens! This is a new code which has much lower inference time:\n\
          > \n> ```python\n> import time\n> \n> prompt = \"Write a long story about\
          \ alpaca\"\n> prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\
          \n> \n> start = time.time()\n> output = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text\n\
          > delay = time.time()\n> \n> real_total_tokens = len(output.split(\" \"\
          ))\n> total_time = (delay - start)\n> time_per_token = total_time / real_total_tokens\n\
          > \n> # Calculate tokens per second\n> tokens_per_second = real_total_tokens\
          \ / total_time\n> \n> # Print the results\n> print(\"Total inference time:\
          \ {:.2f} ms\".format(total_time))\n> print(\"Number of tokens generated:\
          \ {}\".format(real_total_tokens))\n> print(\"Time per token: {:.2f} ms/token\"\
          .format(time_per_token))\n> print(\"Tokens per second: {:.2f} token/s\"\
          .format(tokens_per_second))\n> ```\n> \n> For Falcon-40B now I have\n> \n\
          > ```\n> Total inference time: 3.15 ms\n> Number of tokens generated: 106\n\
          > Time per token: 0.03 ms/token\n> Tokens per second: 33.67 token/s\n> ```\n\
          > \n> For Falcon-7B\n> ```\n> Total inference time: 3.70 ms\n> Number of\
          \ tokens generated: 241\n> Time per token: 0.02 ms/token\n> Tokens per second:\
          \ 65.16 token/s\n> ```\n\nI think in your benchmarking calculations you're\
          \ not doing the conversion from milliseconds to seconds, e.g.\n$$\\frac{241\\\
          text{ tokens}}{3.70\\text{ ms}} = \\frac{241\\text{ tokens}}{.0037\\text{\
          \ s}} \\approx 65135\\text{ tokens/s}\\text{ (or $65.135$ tokens/ms)}$$"
        updatedAt: '2023-07-03T00:01:06.551Z'
      numEdits: 0
      reactions: []
    id: 64a20fc27d4f8315cdba3c70
    type: comment
  author: etweedy
  content: "> > > I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n> > >\
    \ \n> > > - text-generation-inference (no quantization - 16bit)\n> > > - falcon-40b-instruct\n\
    > > > \n> > > ```python\n> > > import time\n> > > \n> > > prompt = \"Write a story\
    \ about alpaca\"\n> > > prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\
    \n> > > \n> > > start = time.time()\n> > > output = client_falcon40.generate(prompt_template,\
    \ max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)\n> > > \n> > > delay\
    \ = time.time()\n> > > total_time = (delay - start)\n> > > time_per_token = total_time\
    \ / 256\n> > > \n> > > # Calculate tokens per second\n> > > tokens_per_second\
    \ = 256 / total_time\n> > > \n> > > # Print the results\n> > > print(\"Total inference\
    \ time: {:.2f} ms\".format(total_time))\n> > > print(\"Number of tokens generated:\
    \ {}\".format(256))\n> > > print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n\
    > > > print(\"Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n\
    > > > ```\n> > > \n> > > Results:\n> > > ```\n> > > Total inference time: 2.22\
    \ ms\n> > > Number of tokens generated: 256\n> > > Time per token: 0.01 ms/token\n\
    > > > Tokens per second: 115.40 token/s\n> > > ```\n> > \n> > That's impressive\
    \ but the numbers do not appear to line up well ?\n> > 0.01ms /token would be\
    \ 100k tokens/second and the total inference time of 2.22 would be 450 tokens/second.\
    \ That needs an actual stopwatch to make sure :)\n> > \n> > I ran the same one,\
    \ though I can not fit more than 8 bit of 40B on my cheap desktop PC, I'm getting\
    \ ~16 tokens/sec for the same prompt and generation. (64ms/token)\n> \n> That\
    \ makes sense. I copied it from a github issue so I thought it was solid. I now\
    \ see it just assumes the `output` has 256 tokens! This is a new code which has\
    \ much lower inference time:\n> \n> ```python\n> import time\n> \n> prompt = \"\
    Write a long story about alpaca\"\n> prompt_template = f\"### Instruction: {prompt}\\\
    n### Response:\"\n> \n> start = time.time()\n> output = client_falcon40.generate(prompt_template,\
    \ max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text\n\
    > delay = time.time()\n> \n> real_total_tokens = len(output.split(\" \"))\n> total_time\
    \ = (delay - start)\n> time_per_token = total_time / real_total_tokens\n> \n>\
    \ # Calculate tokens per second\n> tokens_per_second = real_total_tokens / total_time\n\
    > \n> # Print the results\n> print(\"Total inference time: {:.2f} ms\".format(total_time))\n\
    > print(\"Number of tokens generated: {}\".format(real_total_tokens))\n> print(\"\
    Time per token: {:.2f} ms/token\".format(time_per_token))\n> print(\"Tokens per\
    \ second: {:.2f} token/s\".format(tokens_per_second))\n> ```\n> \n> For Falcon-40B\
    \ now I have\n> \n> ```\n> Total inference time: 3.15 ms\n> Number of tokens generated:\
    \ 106\n> Time per token: 0.03 ms/token\n> Tokens per second: 33.67 token/s\n>\
    \ ```\n> \n> For Falcon-7B\n> ```\n> Total inference time: 3.70 ms\n> Number of\
    \ tokens generated: 241\n> Time per token: 0.02 ms/token\n> Tokens per second:\
    \ 65.16 token/s\n> ```\n\nI think in your benchmarking calculations you're not\
    \ doing the conversion from milliseconds to seconds, e.g.\n$$\\frac{241\\text{\
    \ tokens}}{3.70\\text{ ms}} = \\frac{241\\text{ tokens}}{.0037\\text{ s}} \\approx\
    \ 65135\\text{ tokens/s}\\text{ (or $65.135$ tokens/ms)}$$"
  created_at: 2023-07-02 23:01:06+00:00
  edited: false
  hidden: false
  id: 64a20fc27d4f8315cdba3c70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-07-04T17:46:13.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5477388501167297
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>I just did a benchmark of Falcon-40B on 4 A100 with 80G:</p>

          <ul>

          <li>text-generation-inference (no quantization - 16bit)</li>

          <li>falcon-40b-instruct</li>

          </ul>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          time


          prompt = <span class="hljs-string">"Write a story about alpaca"</span>

          prompt_template = <span class="hljs-string">f"### Instruction: <span class="hljs-subst">{prompt}</span>\n###
          Response:"</span>


          start = time.time()

          output = client_falcon40.generate(prompt_template, max_new_tokens=<span
          class="hljs-number">256</span>, temperature=<span class="hljs-number">1</span>,
          top_k=<span class="hljs-number">1</span>, top_p=<span class="hljs-number">0.90</span>)


          delay = time.time()

          total_time = (delay - start)

          time_per_token = total_time / <span class="hljs-number">256</span>


          <span class="hljs-comment"># Calculate tokens per second</span>

          tokens_per_second = <span class="hljs-number">256</span> / total_time


          <span class="hljs-comment"># Print the results</span>

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total
          inference time: {:.2f} ms"</span>.<span class="hljs-built_in">format</span>(total_time))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Number
          of tokens generated: {}"</span>.<span class="hljs-built_in">format</span>(<span
          class="hljs-number">256</span>))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Time
          per token: {:.2f} ms/token"</span>.<span class="hljs-built_in">format</span>(time_per_token))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Tokens
          per second: {:.2f} token/s"</span>.<span class="hljs-built_in">format</span>(tokens_per_second))

          </code></pre>

          <p>Results:</p>

          <pre><code>Total inference time: 2.22 ms

          Number of tokens generated: 256

          Time per token: 0.01 ms/token

          Tokens per second: 115.40 token/s

          </code></pre>

          </blockquote>

          <p>That''s impressive but the numbers do not appear to line up well ?<br>0.01ms
          /token would be 100k tokens/second and the total inference time of 2.22
          would be 450 tokens/second. That needs an actual stopwatch to make sure
          :)</p>

          <p>I ran the same one, though I can not fit more than 8 bit of 40B on my
          cheap desktop PC, I''m getting ~16 tokens/sec for the same prompt and generation.
          (64ms/token)</p>

          </blockquote>

          <p>That makes sense. I copied it from a github issue so I thought it was
          solid. I now see it just assumes the <code>output</code> has 256 tokens!
          This is a new code which has much lower inference time:</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          time


          prompt = <span class="hljs-string">"Write a long story about alpaca"</span>

          prompt_template = <span class="hljs-string">f"### Instruction: <span class="hljs-subst">{prompt}</span>\n###
          Response:"</span>


          start = time.time()

          output = client_falcon40.generate(prompt_template, max_new_tokens=<span
          class="hljs-number">1024</span>, temperature=<span class="hljs-number">0.55</span>,
          top_k=<span class="hljs-number">1</span>, top_p=<span class="hljs-number">0.90</span>).generated_text

          delay = time.time()


          real_total_tokens = <span class="hljs-built_in">len</span>(output.split(<span
          class="hljs-string">" "</span>))

          total_time = (delay - start)

          time_per_token = total_time / real_total_tokens


          <span class="hljs-comment"># Calculate tokens per second</span>

          tokens_per_second = real_total_tokens / total_time


          <span class="hljs-comment"># Print the results</span>

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total
          inference time: {:.2f} ms"</span>.<span class="hljs-built_in">format</span>(total_time))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Number
          of tokens generated: {}"</span>.<span class="hljs-built_in">format</span>(real_total_tokens))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Time
          per token: {:.2f} ms/token"</span>.<span class="hljs-built_in">format</span>(time_per_token))

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Tokens
          per second: {:.2f} token/s"</span>.<span class="hljs-built_in">format</span>(tokens_per_second))

          </code></pre>

          <p>For Falcon-40B now I have</p>

          <pre><code>Total inference time: 3.15 ms

          Number of tokens generated: 106

          Time per token: 0.03 ms/token

          Tokens per second: 33.67 token/s

          </code></pre>

          <p>For Falcon-7B</p>

          <pre><code>Total inference time: 3.70 ms

          Number of tokens generated: 241

          Time per token: 0.02 ms/token

          Tokens per second: 65.16 token/s

          </code></pre>

          </blockquote>

          <p>I think in your benchmarking calculations you''re not doing the conversion
          from milliseconds to seconds, e.g.<br>$$\frac{241\text{ tokens}}{3.70\text{
          ms}} = \frac{241\text{ tokens}}{.0037\text{ s}} \approx 65135\text{ tokens/s}\text{
          (or $65.135$ tokens/ms)}$$</p>

          </blockquote>

          <p>There is an issue with the <code>Total inference time: 3.70 ms</code>.
          This should be <code>3.70s</code>. So the <code>60 tokens per second</code>
          check out considering the LLM model and the hardware limitations. (I am
          not even sure I can tokenize the text with 65k tokens/s :D)</p>

          '
        raw: "> > > > I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n>\
          \ > > > \n> > > > - text-generation-inference (no quantization - 16bit)\n\
          > > > > - falcon-40b-instruct\n> > > > \n> > > > ```python\n> > > > import\
          \ time\n> > > > \n> > > > prompt = \"Write a story about alpaca\"\n> > >\
          \ > prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\n>\
          \ > > > \n> > > > start = time.time()\n> > > > output = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)\n> > > > \n> >\
          \ > > delay = time.time()\n> > > > total_time = (delay - start)\n> > > >\
          \ time_per_token = total_time / 256\n> > > > \n> > > > # Calculate tokens\
          \ per second\n> > > > tokens_per_second = 256 / total_time\n> > > > \n>\
          \ > > > # Print the results\n> > > > print(\"Total inference time: {:.2f}\
          \ ms\".format(total_time))\n> > > > print(\"Number of tokens generated:\
          \ {}\".format(256))\n> > > > print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n\
          > > > > print(\"Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n\
          > > > > ```\n> > > > \n> > > > Results:\n> > > > ```\n> > > > Total inference\
          \ time: 2.22 ms\n> > > > Number of tokens generated: 256\n> > > > Time per\
          \ token: 0.01 ms/token\n> > > > Tokens per second: 115.40 token/s\n> > >\
          \ > ```\n> > > \n> > > That's impressive but the numbers do not appear to\
          \ line up well ?\n> > > 0.01ms /token would be 100k tokens/second and the\
          \ total inference time of 2.22 would be 450 tokens/second. That needs an\
          \ actual stopwatch to make sure :)\n> > > \n> > > I ran the same one, though\
          \ I can not fit more than 8 bit of 40B on my cheap desktop PC, I'm getting\
          \ ~16 tokens/sec for the same prompt and generation. (64ms/token)\n> > \n\
          > > That makes sense. I copied it from a github issue so I thought it was\
          \ solid. I now see it just assumes the `output` has 256 tokens! This is\
          \ a new code which has much lower inference time:\n> > \n> > ```python\n\
          > > import time\n> > \n> > prompt = \"Write a long story about alpaca\"\n\
          > > prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\n>\
          \ > \n> > start = time.time()\n> > output = client_falcon40.generate(prompt_template,\
          \ max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text\n\
          > > delay = time.time()\n> > \n> > real_total_tokens = len(output.split(\"\
          \ \"))\n> > total_time = (delay - start)\n> > time_per_token = total_time\
          \ / real_total_tokens\n> > \n> > # Calculate tokens per second\n> > tokens_per_second\
          \ = real_total_tokens / total_time\n> > \n> > # Print the results\n> > print(\"\
          Total inference time: {:.2f} ms\".format(total_time))\n> > print(\"Number\
          \ of tokens generated: {}\".format(real_total_tokens))\n> > print(\"Time\
          \ per token: {:.2f} ms/token\".format(time_per_token))\n> > print(\"Tokens\
          \ per second: {:.2f} token/s\".format(tokens_per_second))\n> > ```\n> >\
          \ \n> > For Falcon-40B now I have\n> > \n> > ```\n> > Total inference time:\
          \ 3.15 ms\n> > Number of tokens generated: 106\n> > Time per token: 0.03\
          \ ms/token\n> > Tokens per second: 33.67 token/s\n> > ```\n> > \n> > For\
          \ Falcon-7B\n> > ```\n> > Total inference time: 3.70 ms\n> > Number of tokens\
          \ generated: 241\n> > Time per token: 0.02 ms/token\n> > Tokens per second:\
          \ 65.16 token/s\n> > ```\n> \n> I think in your benchmarking calculations\
          \ you're not doing the conversion from milliseconds to seconds, e.g.\n>\
          \ $$\\frac{241\\text{ tokens}}{3.70\\text{ ms}} = \\frac{241\\text{ tokens}}{.0037\\\
          text{ s}} \\approx 65135\\text{ tokens/s}\\text{ (or $65.135$ tokens/ms)}$$\n\
          \nThere is an issue with the `Total inference time: 3.70 ms`. This should\
          \ be `3.70s`. So the `60 tokens per second` check out considering the LLM\
          \ model and the hardware limitations. (I am not even sure I can tokenize\
          \ the text with 65k tokens/s :D)"
        updatedAt: '2023-07-04T17:46:13.022Z'
      numEdits: 0
      reactions: []
    id: 64a45ae59a90f70113375578
    type: comment
  author: MaziyarPanahi
  content: "> > > > I just did a benchmark of Falcon-40B on 4 A100 with 80G:\n> >\
    \ > > \n> > > > - text-generation-inference (no quantization - 16bit)\n> > > >\
    \ - falcon-40b-instruct\n> > > > \n> > > > ```python\n> > > > import time\n> >\
    \ > > \n> > > > prompt = \"Write a story about alpaca\"\n> > > > prompt_template\
    \ = f\"### Instruction: {prompt}\\n### Response:\"\n> > > > \n> > > > start =\
    \ time.time()\n> > > > output = client_falcon40.generate(prompt_template, max_new_tokens=256,\
    \ temperature=1, top_k=1, top_p=0.90)\n> > > > \n> > > > delay = time.time()\n\
    > > > > total_time = (delay - start)\n> > > > time_per_token = total_time / 256\n\
    > > > > \n> > > > # Calculate tokens per second\n> > > > tokens_per_second = 256\
    \ / total_time\n> > > > \n> > > > # Print the results\n> > > > print(\"Total inference\
    \ time: {:.2f} ms\".format(total_time))\n> > > > print(\"Number of tokens generated:\
    \ {}\".format(256))\n> > > > print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n\
    > > > > print(\"Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n\
    > > > > ```\n> > > > \n> > > > Results:\n> > > > ```\n> > > > Total inference\
    \ time: 2.22 ms\n> > > > Number of tokens generated: 256\n> > > > Time per token:\
    \ 0.01 ms/token\n> > > > Tokens per second: 115.40 token/s\n> > > > ```\n> > >\
    \ \n> > > That's impressive but the numbers do not appear to line up well ?\n\
    > > > 0.01ms /token would be 100k tokens/second and the total inference time of\
    \ 2.22 would be 450 tokens/second. That needs an actual stopwatch to make sure\
    \ :)\n> > > \n> > > I ran the same one, though I can not fit more than 8 bit of\
    \ 40B on my cheap desktop PC, I'm getting ~16 tokens/sec for the same prompt and\
    \ generation. (64ms/token)\n> > \n> > That makes sense. I copied it from a github\
    \ issue so I thought it was solid. I now see it just assumes the `output` has\
    \ 256 tokens! This is a new code which has much lower inference time:\n> > \n\
    > > ```python\n> > import time\n> > \n> > prompt = \"Write a long story about\
    \ alpaca\"\n> > prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\
    \n> > \n> > start = time.time()\n> > output = client_falcon40.generate(prompt_template,\
    \ max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text\n\
    > > delay = time.time()\n> > \n> > real_total_tokens = len(output.split(\" \"\
    ))\n> > total_time = (delay - start)\n> > time_per_token = total_time / real_total_tokens\n\
    > > \n> > # Calculate tokens per second\n> > tokens_per_second = real_total_tokens\
    \ / total_time\n> > \n> > # Print the results\n> > print(\"Total inference time:\
    \ {:.2f} ms\".format(total_time))\n> > print(\"Number of tokens generated: {}\"\
    .format(real_total_tokens))\n> > print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n\
    > > print(\"Tokens per second: {:.2f} token/s\".format(tokens_per_second))\n>\
    \ > ```\n> > \n> > For Falcon-40B now I have\n> > \n> > ```\n> > Total inference\
    \ time: 3.15 ms\n> > Number of tokens generated: 106\n> > Time per token: 0.03\
    \ ms/token\n> > Tokens per second: 33.67 token/s\n> > ```\n> > \n> > For Falcon-7B\n\
    > > ```\n> > Total inference time: 3.70 ms\n> > Number of tokens generated: 241\n\
    > > Time per token: 0.02 ms/token\n> > Tokens per second: 65.16 token/s\n> > ```\n\
    > \n> I think in your benchmarking calculations you're not doing the conversion\
    \ from milliseconds to seconds, e.g.\n> $$\\frac{241\\text{ tokens}}{3.70\\text{\
    \ ms}} = \\frac{241\\text{ tokens}}{.0037\\text{ s}} \\approx 65135\\text{ tokens/s}\\\
    text{ (or $65.135$ tokens/ms)}$$\n\nThere is an issue with the `Total inference\
    \ time: 3.70 ms`. This should be `3.70s`. So the `60 tokens per second` check\
    \ out considering the LLM model and the hardware limitations. (I am not even sure\
    \ I can tokenize the text with 65k tokens/s :D)"
  created_at: 2023-07-04 16:46:13+00:00
  edited: false
  hidden: false
  id: 64a45ae59a90f70113375578
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674225232524-6327bbf0bfc72a1d59ce6377.png?w=200&h=200&f=face
      fullname: Eamonn Tweedy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: etweedy
      type: user
    createdAt: '2023-07-04T17:50:38.000Z'
    data:
      edited: false
      editors:
      - etweedy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9624024033546448
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674225232524-6327bbf0bfc72a1d59ce6377.png?w=200&h=200&f=face
          fullname: Eamonn Tweedy
          isHf: false
          isPro: false
          name: etweedy
          type: user
        html: '<p>I see, yes that is more reasonable! Somehow the units are not reporting
          correctly on total inference time.</p>

          '
        raw: I see, yes that is more reasonable! Somehow the units are not reporting
          correctly on total inference time.
        updatedAt: '2023-07-04T17:50:38.075Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MaziyarPanahi
    id: 64a45beee3cf200cbf8e6c63
    type: comment
  author: etweedy
  content: I see, yes that is more reasonable! Somehow the units are not reporting
    correctly on total inference time.
  created_at: 2023-07-04 16:50:38+00:00
  edited: false
  hidden: false
  id: 64a45beee3cf200cbf8e6c63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1607085671166-noauth.jpeg?w=200&h=200&f=face
      fullname: Shiv
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shivaraj
      type: user
    createdAt: '2023-07-28T05:41:30.000Z'
    data:
      edited: false
      editors:
      - Shivaraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9323374629020691
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1607085671166-noauth.jpeg?w=200&h=200&f=face
          fullname: Shiv
          isHf: false
          isPro: false
          name: Shivaraj
          type: user
        html: '<p>Is Falcon 7b instruct model sensitive to the Prompt template? As
          per my experiment I did several times different types of prompt templates
          I don''t it see it considers all the guidelines provided in prompt, Specially
          when context doesn''t have answer to the asked question it still tries to
          make answer out that, despite telling "if context is empty then do not generate
          answer, say sorry I cannot answer!."</p>

          <p>Please share your experience with prompt template</p>

          '
        raw: 'Is Falcon 7b instruct model sensitive to the Prompt template? As per
          my experiment I did several times different types of prompt templates I
          don''t it see it considers all the guidelines provided in prompt, Specially
          when context doesn''t have answer to the asked question it still tries to
          make answer out that, despite telling "if context is empty then do not generate
          answer, say sorry I cannot answer!."


          Please share your experience with prompt template'
        updatedAt: '2023-07-28T05:41:30.080Z'
      numEdits: 0
      reactions: []
    id: 64c3550a2a5eaefd000920e4
    type: comment
  author: Shivaraj
  content: 'Is Falcon 7b instruct model sensitive to the Prompt template? As per my
    experiment I did several times different types of prompt templates I don''t it
    see it considers all the guidelines provided in prompt, Specially when context
    doesn''t have answer to the asked question it still tries to make answer out that,
    despite telling "if context is empty then do not generate answer, say sorry I
    cannot answer!."


    Please share your experience with prompt template'
  created_at: 2023-07-28 04:41:30+00:00
  edited: false
  hidden: false
  id: 64c3550a2a5eaefd000920e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/9x3ZOeD0s0aQ4eb7gFfXn.png?w=200&h=200&f=face
      fullname: Muhammad Shaffay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shaffay00
      type: user
    createdAt: '2023-08-01T15:26:57.000Z'
    data:
      edited: false
      editors:
      - Shaffay00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.829253613948822
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/9x3ZOeD0s0aQ4eb7gFfXn.png?w=200&h=200&f=face
          fullname: Muhammad Shaffay
          isHf: false
          isPro: false
          name: Shaffay00
          type: user
        html: '<p>Hi,<br>What improvements should I make in this prompt to get an
          exciting story with 1000+ tokens?</p>

          <p>template = """<br>You are a story writer who writes long and captivating
          stories on a given topic.</p>

          <p>The output format of the story should be the following 6 paragraphs.<br>Paragraph
          1: Long Engaging Introduction (400 words)<br>Paragraph 2: Tell three main
          points about the topic.<br>Paragraph 3: Explain point 1 in detail (400 words).<br>Paragraph
          4: Explain point 2 in detail (400 words).<br>Paragraph 5: Explain point
          3 in detail (400 words).<br>Paragraph 6: Long Conclusion (400 words)</p>

          <p>The story should be 1000 words minimum.</p>

          <p>Topic: {question}<br>Story:"""</p>

          '
        raw: "Hi,\nWhat improvements should I make in this prompt to get an exciting\
          \ story with 1000+ tokens?\n\ntemplate = \"\"\"\nYou are a story writer\
          \ who writes long and captivating stories on a given topic.\n\nThe output\
          \ format of the story should be the following 6 paragraphs.\nParagraph 1:\
          \ Long Engaging Introduction (400 words)\nParagraph 2: Tell three main points\
          \ about the topic. \nParagraph 3: Explain point 1 in detail (400 words).\n\
          Paragraph 4: Explain point 2 in detail (400 words).\nParagraph 5: Explain\
          \ point 3 in detail (400 words).\nParagraph 6: Long Conclusion (400 words)\n\
          \nThe story should be 1000 words minimum.\n\nTopic: {question}\nStory:\"\
          \"\""
        updatedAt: '2023-08-01T15:26:57.914Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - krisha05
    id: 64c924414524c2aea71910a4
    type: comment
  author: Shaffay00
  content: "Hi,\nWhat improvements should I make in this prompt to get an exciting\
    \ story with 1000+ tokens?\n\ntemplate = \"\"\"\nYou are a story writer who writes\
    \ long and captivating stories on a given topic.\n\nThe output format of the story\
    \ should be the following 6 paragraphs.\nParagraph 1: Long Engaging Introduction\
    \ (400 words)\nParagraph 2: Tell three main points about the topic. \nParagraph\
    \ 3: Explain point 1 in detail (400 words).\nParagraph 4: Explain point 2 in detail\
    \ (400 words).\nParagraph 5: Explain point 3 in detail (400 words).\nParagraph\
    \ 6: Long Conclusion (400 words)\n\nThe story should be 1000 words minimum.\n\n\
    Topic: {question}\nStory:\"\"\""
  created_at: 2023-08-01 14:26:57+00:00
  edited: false
  hidden: false
  id: 64c924414524c2aea71910a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b39239d52d67c01ce2efb2/403xzE9y55Okb2RYi2zQS.jpeg?w=200&h=200&f=face
      fullname: Matan Gilat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matangilat
      type: user
    createdAt: '2023-08-09T14:12:56.000Z'
    data:
      edited: true
      editors:
      - matangilat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8840459585189819
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b39239d52d67c01ce2efb2/403xzE9y55Okb2RYi2zQS.jpeg?w=200&h=200&f=face
          fullname: Matan Gilat
          isHf: false
          isPro: false
          name: matangilat
          type: user
        html: '<p>The following prompt works for me :<br>"""Answer the question using
          the provided text, and if the answer is not contained within the text below,
          respond with "I can''t answer that<br>     &gt;&gt;CONTEXT&lt;&lt;<br>      {context}<br>      &gt;&gt;QUESTION&lt;&lt;
          {question}<br>"""</p>

          '
        raw: "The following prompt works for me : \n\"\"\"Answer the question using\
          \ the provided text, and if the answer is not contained within the text\
          \ below, respond with \"I can't answer that\n     >>CONTEXT<<\n      {context}\
          \ \n      >>QUESTION<< {question}\n\"\"\""
        updatedAt: '2023-08-09T14:14:26.920Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - danny3
    id: 64d39ee8f63b01b7f662aa61
    type: comment
  author: matangilat
  content: "The following prompt works for me : \n\"\"\"Answer the question using\
    \ the provided text, and if the answer is not contained within the text below,\
    \ respond with \"I can't answer that\n     >>CONTEXT<<\n      {context} \n   \
    \   >>QUESTION<< {question}\n\"\"\""
  created_at: 2023-08-09 13:12:56+00:00
  edited: true
  hidden: false
  id: 64d39ee8f63b01b7f662aa61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-BmI-M3yjyA93EeGspwpY.jpeg?w=200&h=200&f=face
      fullname: Daniel Enemona Adama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danny3
      type: user
    createdAt: '2023-08-11T20:37:45.000Z'
    data:
      edited: true
      editors:
      - danny3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553667306900024
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-BmI-M3yjyA93EeGspwpY.jpeg?w=200&h=200&f=face
          fullname: Daniel Enemona Adama
          isHf: false
          isPro: false
          name: danny3
          type: user
        html: "<p>My falcon-7b do not seem to remember it context even using ConversationBufferMemory\
          \ with langchain. Please help</p>\n<p>class JacqChatbot:<br>  def <strong>init</strong>(self,\
          \ tokenizer, model, generation_config):<br>    self.tokenizer = tokenizer<br>\
          \    self.model = model<br>    # self.stopping_criteria = stopping_criteria<br>\
          \    self.generation_config = generation_config<br>    self.DEFAULT_TEMPLATE\
          \ = <br>    \"\"\"The following is a conversation between a human and AI,\
          \ <br>    called Jacq. Jacq is an experienced and very successful Legal\
          \ professional. <br>    Jacq was built to be respectful, polite, inclusive\
          \ and practical. <br>    Jacq was built by the Lex-ai in Lagos, Nigeria\
          \ in the year 2023. <br>    Jacq helps with every legal task given to him.\
          \ <br>    When requesting for physical documents refer them to upload those\
          \ documents before submission. <br>    If Jacq does not know the answer\
          \ to a question, he truthfully says he does not know and does not <br> \
          \   ask for the answer to the question. The conversation begins.</p>\n<pre><code>Current\
          \ conversation:\n{chat_history}\n### Instruction:\n{input}\n\n### Response:\"\
          \"\"\n\nself.prompt_template   = self._create_prompt_template()\nself.llm\
          \ = self._create_llm_pipeline()\nself.llm_chain = self._create_llm_chain()\n\
          self.STOP_SUSPECT_LIST = [\"### Instruction:\", \"&lt;/s&gt;\",\n      \
          \                    \"\\nUser\", \"\\n### Response:\",\n              \
          \            \"\\n&lt;person&gt;:\", \"\\n##Response:\", \"\\n&lt;human&gt;:\"\
          ,\n                          \"\\n&lt;assistant&gt;:\"]\n</code></pre>\n\
          <p>  def _create_prompt_template(self):<br>    return PromptTemplate(input_variables=[\"\
          chat_history\", \"input\"], template=self.DEFAULT_TEMPLATE)</p>\n<p>  def\
          \ _create_llm_pipeline(self):<br>    pipeline_instance = pipeline(<br> \
          \       \"text-generation\",<br>        generation_config=self.generation_config,<br>\
          \        model=self.model,<br>        tokenizer=self.tokenizer,<br>    \
          \    # stopping_criteria=self.stopping_criteria,<br>        framework=\"\
          pt\"<br>    )<br>    return HuggingFacePipeline(pipeline=pipeline_instance)</p>\n\
          <p>  def _create_llm_chain(self):<br>    config = {<br>        \"memory_key\"\
          : \"chat_history\",<br>        \"ai_prefix\": \"\",<br>        \"human_prefix\"\
          : \"\",<br>        \"input_key\": \"input\",<br>        \"return_messages\"\
          : False<br>    }</p>\n<pre><code>memory = ConversationBufferMemory(**config)\n\
          \nreturn LLMChain(llm=self.llm,\n                prompt=self.prompt_template,\n\
          \                # output_parser=OutputParser(),\n                verbose=True,\n\
          \                memory=memory)\n</code></pre>\n<p>  def chat(self, message):<br>\
          \      if message:<br>        return self.llm_chain.predict(input=message,\
          \ stop=self.STOP_SUSPECT_LIST).strip()</p>\n<h1 id=\"stop_tokens--\">stop_tokens\
          \ = [</h1>\n<h1 id=\"-instruction-\">[\"###\", \"Instruction\", \":\"],</h1>\n\
          <h1 id=\"-response-\">[\"###\", \"Response\", \":\"],</h1>\n<h1 id=\"-nuser\"\
          ># [\"\\\",\"n\",'User']</h1>\n<h1 id=\"\">]</h1>\n<p>generation_config\
          \ = GenerationConfig()<br>generation_config.max_new_tokens = 128<br>generation_config.temperature\
          \ = 0.5</p>\n<h1 id=\"generation_configtop--015\">generation_config.top\
          \ = 0.15</h1>\n<p>generation_config.num_return_sequences = 1<br>generation_config.repetition_penalty\
          \ = 6.5</p>\n<h1 id=\"generation_configtop_k50\">generation_config.top_k=50</h1>\n\
          <h1 id=\"generation_configtop_p092\">generation_config.top_p=0.92</h1>\n\
          <h1 id=\"generation_configreturn_full_text--true-uncomment-when-using-langchain\"\
          >#generation_config.return_full_text = True Uncomment when using Langchain</h1>\n\
          <p>generation_config.pad_token_id = tokenizer.pad_token_id<br>generation_config.eos_token_id\
          \ = tokenizer.eos_token_id</p>\n<h1 id=\"stopping_criteria--stoppingcriterialiststopgenerationcriteriastop_tokens-tokenizer-modeldevice\"\
          >stopping_criteria = StoppingCriteriaList([StopGenerationCriteria(stop_tokens,\
          \ tokenizer, model.device)])</h1>\n<p>chatbot = JacqChatbot(tokenizer, model,\
          \ generation_config) #stopping_criteria</p>\n"
        raw: "My falcon-7b do not seem to remember it context even using ConversationBufferMemory\
          \ with langchain. Please help\n\nclass JacqChatbot:\n  def __init__(self,\
          \ tokenizer, model, generation_config):\n    self.tokenizer = tokenizer\n\
          \    self.model = model\n    # self.stopping_criteria = stopping_criteria\n\
          \    self.generation_config = generation_config\n    self.DEFAULT_TEMPLATE\
          \ = \\\n    \"\"\"The following is a conversation between a human and AI,\
          \ \\\n    called Jacq. Jacq is an experienced and very successful Legal\
          \ professional. \\\n    Jacq was built to be respectful, polite, inclusive\
          \ and practical. \\\n    Jacq was built by the Lex-ai in Lagos, Nigeria\
          \ in the year 2023. \\\n    Jacq helps with every legal task given to him.\
          \ \\\n    When requesting for physical documents refer them to upload those\
          \ documents before submission. \\\n    If Jacq does not know the answer\
          \ to a question, he truthfully says he does not know and does not \\\n \
          \   ask for the answer to the question. The conversation begins.\n\n   \
          \ Current conversation:\n    {chat_history}\n    ### Instruction:\n    {input}\n\
          \n    ### Response:\"\"\"\n\n    self.prompt_template   = self._create_prompt_template()\n\
          \    self.llm = self._create_llm_pipeline()\n    self.llm_chain = self._create_llm_chain()\n\
          \    self.STOP_SUSPECT_LIST = [\"### Instruction:\", \"</s>\",\n       \
          \                       \"\\nUser\", \"\\n### Response:\",\n           \
          \                   \"\\n<person>:\", \"\\n##Response:\", \"\\n<human>:\"\
          ,\n                              \"\\n<assistant>:\"]\n\n  def _create_prompt_template(self):\n\
          \    return PromptTemplate(input_variables=[\"chat_history\", \"input\"\
          ], template=self.DEFAULT_TEMPLATE)\n\n  def _create_llm_pipeline(self):\n\
          \    pipeline_instance = pipeline(\n        \"text-generation\",\n     \
          \   generation_config=self.generation_config,\n        model=self.model,\n\
          \        tokenizer=self.tokenizer,\n        # stopping_criteria=self.stopping_criteria,\n\
          \        framework=\"pt\"\n    )\n    return HuggingFacePipeline(pipeline=pipeline_instance)\n\
          \n  def _create_llm_chain(self):\n    config = {\n        \"memory_key\"\
          : \"chat_history\",\n        \"ai_prefix\": \"<assistant>\",\n        \"\
          human_prefix\": \"<human>\",\n        \"input_key\": \"input\",\n      \
          \  \"return_messages\": False\n    }\n\n    memory = ConversationBufferMemory(**config)\n\
          \n    return LLMChain(llm=self.llm,\n                    prompt=self.prompt_template,\n\
          \                    # output_parser=OutputParser(),\n                 \
          \   verbose=True,\n                    memory=memory)\n\n  def chat(self,\
          \ message):\n      if message:\n        return self.llm_chain.predict(input=message,\
          \ stop=self.STOP_SUSPECT_LIST).strip()\n\n# stop_tokens = [\n#     [\"###\"\
          , \"Instruction\", \":\"],\n#     [\"###\", \"Response\", \":\"],\n#   \
          \  # [\"\\\\\",\"n\",'User']\n# ]\n\ngeneration_config = GenerationConfig()\n\
          generation_config.max_new_tokens = 128\ngeneration_config.temperature =\
          \ 0.5\n# generation_config.top = 0.15\ngeneration_config.num_return_sequences\
          \ = 1\ngeneration_config.repetition_penalty = 6.5\n# generation_config.top_k=50\n\
          # generation_config.top_p=0.92\n# #generation_config.return_full_text =\
          \ True Uncomment when using Langchain\ngeneration_config.pad_token_id =\
          \ tokenizer.pad_token_id\ngeneration_config.eos_token_id = tokenizer.eos_token_id\n\
          \n# stopping_criteria = StoppingCriteriaList([StopGenerationCriteria(stop_tokens,\
          \ tokenizer, model.device)])\nchatbot = JacqChatbot(tokenizer, model, generation_config)\
          \ #stopping_criteria"
        updatedAt: '2023-08-11T21:25:29.624Z'
      numEdits: 1
      reactions: []
    id: 64d69c193ca2924d6e7908de
    type: comment
  author: danny3
  content: "My falcon-7b do not seem to remember it context even using ConversationBufferMemory\
    \ with langchain. Please help\n\nclass JacqChatbot:\n  def __init__(self, tokenizer,\
    \ model, generation_config):\n    self.tokenizer = tokenizer\n    self.model =\
    \ model\n    # self.stopping_criteria = stopping_criteria\n    self.generation_config\
    \ = generation_config\n    self.DEFAULT_TEMPLATE = \\\n    \"\"\"The following\
    \ is a conversation between a human and AI, \\\n    called Jacq. Jacq is an experienced\
    \ and very successful Legal professional. \\\n    Jacq was built to be respectful,\
    \ polite, inclusive and practical. \\\n    Jacq was built by the Lex-ai in Lagos,\
    \ Nigeria in the year 2023. \\\n    Jacq helps with every legal task given to\
    \ him. \\\n    When requesting for physical documents refer them to upload those\
    \ documents before submission. \\\n    If Jacq does not know the answer to a question,\
    \ he truthfully says he does not know and does not \\\n    ask for the answer\
    \ to the question. The conversation begins.\n\n    Current conversation:\n   \
    \ {chat_history}\n    ### Instruction:\n    {input}\n\n    ### Response:\"\"\"\
    \n\n    self.prompt_template   = self._create_prompt_template()\n    self.llm\
    \ = self._create_llm_pipeline()\n    self.llm_chain = self._create_llm_chain()\n\
    \    self.STOP_SUSPECT_LIST = [\"### Instruction:\", \"</s>\",\n             \
    \                 \"\\nUser\", \"\\n### Response:\",\n                       \
    \       \"\\n<person>:\", \"\\n##Response:\", \"\\n<human>:\",\n             \
    \                 \"\\n<assistant>:\"]\n\n  def _create_prompt_template(self):\n\
    \    return PromptTemplate(input_variables=[\"chat_history\", \"input\"], template=self.DEFAULT_TEMPLATE)\n\
    \n  def _create_llm_pipeline(self):\n    pipeline_instance = pipeline(\n     \
    \   \"text-generation\",\n        generation_config=self.generation_config,\n\
    \        model=self.model,\n        tokenizer=self.tokenizer,\n        # stopping_criteria=self.stopping_criteria,\n\
    \        framework=\"pt\"\n    )\n    return HuggingFacePipeline(pipeline=pipeline_instance)\n\
    \n  def _create_llm_chain(self):\n    config = {\n        \"memory_key\": \"chat_history\"\
    ,\n        \"ai_prefix\": \"<assistant>\",\n        \"human_prefix\": \"<human>\"\
    ,\n        \"input_key\": \"input\",\n        \"return_messages\": False\n   \
    \ }\n\n    memory = ConversationBufferMemory(**config)\n\n    return LLMChain(llm=self.llm,\n\
    \                    prompt=self.prompt_template,\n                    # output_parser=OutputParser(),\n\
    \                    verbose=True,\n                    memory=memory)\n\n  def\
    \ chat(self, message):\n      if message:\n        return self.llm_chain.predict(input=message,\
    \ stop=self.STOP_SUSPECT_LIST).strip()\n\n# stop_tokens = [\n#     [\"###\", \"\
    Instruction\", \":\"],\n#     [\"###\", \"Response\", \":\"],\n#     # [\"\\\\\
    \",\"n\",'User']\n# ]\n\ngeneration_config = GenerationConfig()\ngeneration_config.max_new_tokens\
    \ = 128\ngeneration_config.temperature = 0.5\n# generation_config.top = 0.15\n\
    generation_config.num_return_sequences = 1\ngeneration_config.repetition_penalty\
    \ = 6.5\n# generation_config.top_k=50\n# generation_config.top_p=0.92\n# #generation_config.return_full_text\
    \ = True Uncomment when using Langchain\ngeneration_config.pad_token_id = tokenizer.pad_token_id\n\
    generation_config.eos_token_id = tokenizer.eos_token_id\n\n# stopping_criteria\
    \ = StoppingCriteriaList([StopGenerationCriteria(stop_tokens, tokenizer, model.device)])\n\
    chatbot = JacqChatbot(tokenizer, model, generation_config) #stopping_criteria"
  created_at: 2023-08-11 19:37:45+00:00
  edited: true
  hidden: false
  id: 64d69c193ca2924d6e7908de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13ad3c8a4cb031ffa9284a9e45862544.svg
      fullname: Joshua Watsom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Joshin
      type: user
    createdAt: '2023-09-04T13:57:12.000Z'
    data:
      edited: false
      editors:
      - Joshin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9259504079818726
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13ad3c8a4cb031ffa9284a9e45862544.svg
          fullname: Joshua Watsom
          isHf: false
          isPro: true
          name: Joshin
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Check out the special tokens near the top of the tokenizer.json file.
          There are special tokens for &gt;&gt;QUESTION&lt;&lt;, &gt;&gt;ANSWER&lt;&lt;,
          and a few other types you can play with. I''ve had the best results prompting
          it like this:</p>

          <blockquote>

          <blockquote>

          <p>QUESTION&lt;&lt;In Python, I want to write a simple HTTP API that receives
          an object via POST and responds with another object. The request object
          contains a string prompt, float temperature, and int max_tokens. The response
          object contains a string response, int prompt_tokens, int completion_tokens.
          For now just set hard-coded values and get the response out. I''ll add the
          logic myself afterward.<br>ANSWER&lt;&lt;</p>

          </blockquote>

          </blockquote>

          </blockquote>

          <p>Great find. I was wondering which of these tokens would be best to utilise
          if I wanted to constrain the LLM''s answer to just the given context? No
          matter what I use, the LLM seems to want to answer from knowledge it knows
          instead of just responding with a generic "I don''t know" e.g.</p>

          <pre><code class="language-python"><span class="hljs-string">"""Answer the
          question as truthfully as possible using the provided text, and if the answer
          is not contained within the text below, respond with "I can''t answer that"</span>

          <span class="hljs-string"></span>

          <span class="hljs-string">&gt;&gt;CONTEXT&lt;&lt;</span>

          <span class="hljs-string">Home and contents insurance covers your house
          and other structures on your property as well as your belongings for loss
          or damage due to events like fire, theft and storms.</span>

          <span class="hljs-string"></span>

          <span class="hljs-string">&gt;&gt;QUESTION&lt;&lt; What is 2+2</span>

          <span class="hljs-string">"""</span>

          </code></pre>

          <p>The answer is 4</p>

          </blockquote>

          <p>You can use &gt;&gt;INTRODUCTION&lt;&lt; for the system prompt and &gt;&gt;SUMMARY&lt;&lt;
          for the context</p>

          '
        raw: "> > Check out the special tokens near the top of the tokenizer.json\
          \ file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few\
          \ other types you can play with. I've had the best results prompting it\
          \ like this:\n> > \n> > >>QUESTION<<In Python, I want to write a simple\
          \ HTTP API that receives an object via POST and responds with another object.\
          \ The request object contains a string prompt, float temperature, and int\
          \ max_tokens. The response object contains a string response, int prompt_tokens,\
          \ int completion_tokens. For now just set hard-coded values and get the\
          \ response out. I'll add the logic myself afterward.\n> > >>ANSWER<<\n>\
          \ \n> \n> \n> Great find. I was wondering which of these tokens would be\
          \ best to utilise if I wanted to constrain the LLM's answer to just the\
          \ given context? No matter what I use, the LLM seems to want to answer from\
          \ knowledge it knows instead of just responding with a generic \"I don't\
          \ know\" e.g.\n> \n> \n> \n> ```python\n> \"\"\"Answer the question as truthfully\
          \ as possible using the provided text, and if the answer is not contained\
          \ within the text below, respond with \"I can't answer that\"\n> \n> >>CONTEXT<<\n\
          > Home and contents insurance covers your house and other structures on\
          \ your property as well as your belongings for loss or damage due to events\
          \ like fire, theft and storms.\n> \n> >>QUESTION<< What is 2+2\n> \"\"\"\
          \n> ```\n> \n> The answer is 4\n\nYou can use >>INTRODUCTION<< for the system\
          \ prompt and >>SUMMARY<< for the context"
        updatedAt: '2023-09-04T13:57:12.060Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Joshin
    id: 64f5e238561d3030ba2b3559
    type: comment
  author: Joshin
  content: "> > Check out the special tokens near the top of the tokenizer.json file.\
    \ There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types\
    \ you can play with. I've had the best results prompting it like this:\n> > \n\
    > > >>QUESTION<<In Python, I want to write a simple HTTP API that receives an\
    \ object via POST and responds with another object. The request object contains\
    \ a string prompt, float temperature, and int max_tokens. The response object\
    \ contains a string response, int prompt_tokens, int completion_tokens. For now\
    \ just set hard-coded values and get the response out. I'll add the logic myself\
    \ afterward.\n> > >>ANSWER<<\n> \n> \n> \n> Great find. I was wondering which\
    \ of these tokens would be best to utilise if I wanted to constrain the LLM's\
    \ answer to just the given context? No matter what I use, the LLM seems to want\
    \ to answer from knowledge it knows instead of just responding with a generic\
    \ \"I don't know\" e.g.\n> \n> \n> \n> ```python\n> \"\"\"Answer the question\
    \ as truthfully as possible using the provided text, and if the answer is not\
    \ contained within the text below, respond with \"I can't answer that\"\n> \n\
    > >>CONTEXT<<\n> Home and contents insurance covers your house and other structures\
    \ on your property as well as your belongings for loss or damage due to events\
    \ like fire, theft and storms.\n> \n> >>QUESTION<< What is 2+2\n> \"\"\"\n> ```\n\
    > \n> The answer is 4\n\nYou can use >>INTRODUCTION<< for the system prompt and\
    \ >>SUMMARY<< for the context"
  created_at: 2023-09-04 12:57:12+00:00
  edited: false
  hidden: false
  id: 64f5e238561d3030ba2b3559
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8ed016c6d496e6205f60b3e26ea4586.svg
      fullname: Yoyo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YoYo1234Qwerty
      type: user
    createdAt: '2023-09-26T21:34:54.000Z'
    data:
      edited: false
      editors:
      - YoYo1234Qwerty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8829682469367981
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8ed016c6d496e6205f60b3e26ea4586.svg
          fullname: Yoyo
          isHf: false
          isPro: false
          name: YoYo1234Qwerty
          type: user
        html: '<p>I think everyone''s wrong about <code>&gt;&gt;QUESTION&lt;&lt;</code>
          and <code>&gt;&gt;ANSWER&lt;&lt;</code> etc.</p>

          <p>If you prompt it with a <code>&gt;&gt;QUESTION&lt;&lt;</code> it never
          outputs the <code>&gt;&gt;ANSWER&lt;&lt;</code> token on its own, meaning
          it likely wasn''t trained to use these tokens.</p>

          <p>They apparently trained on Baise: <a rel="nofollow" href="https://github.com/project-baize/baize-chatbot/blob/main/demo/app.py#L59">https://github.com/project-baize/baize-chatbot/blob/main/demo/app.py#L59</a></p>

          <p>Which suggests a prompting scheme of <code>[|Human|]</code> and <code>[|AI|]</code>.</p>

          <p>But then in experiments, it really wants to output:</p>

          <pre><code>User How''s the weather?

          Mini So so, kinda gloomy.

          </code></pre>

          '
        raw: 'I think everyone''s wrong about `>>QUESTION<<` and `>>ANSWER<<` etc.


          If you prompt it with a `>>QUESTION<<` it never outputs the `>>ANSWER<<`
          token on its own, meaning it likely wasn''t trained to use these tokens.


          They apparently trained on Baise: https://github.com/project-baize/baize-chatbot/blob/main/demo/app.py#L59


          Which suggests a prompting scheme of `[|Human|]` and `[|AI|]`.


          But then in experiments, it really wants to output:


          ```

          User How''s the weather?

          Mini So so, kinda gloomy.

          ```'
        updatedAt: '2023-09-26T21:34:54.211Z'
      numEdits: 0
      reactions: []
    id: 65134e7e330c55fdc53944b2
    type: comment
  author: YoYo1234Qwerty
  content: 'I think everyone''s wrong about `>>QUESTION<<` and `>>ANSWER<<` etc.


    If you prompt it with a `>>QUESTION<<` it never outputs the `>>ANSWER<<` token
    on its own, meaning it likely wasn''t trained to use these tokens.


    They apparently trained on Baise: https://github.com/project-baize/baize-chatbot/blob/main/demo/app.py#L59


    Which suggests a prompting scheme of `[|Human|]` and `[|AI|]`.


    But then in experiments, it really wants to output:


    ```

    User How''s the weather?

    Mini So so, kinda gloomy.

    ```'
  created_at: 2023-09-26 20:34:54+00:00
  edited: false
  hidden: false
  id: 65134e7e330c55fdc53944b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02cd941db15296a3d5e5c3502abe5df9.svg
      fullname: jayanth vallu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jay0911
      type: user
    createdAt: '2023-10-18T04:02:40.000Z'
    data:
      edited: true
      editors:
      - jay0911
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6022001504898071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02cd941db15296a3d5e5c3502abe5df9.svg
          fullname: jayanth vallu
          isHf: false
          isPro: false
          name: jay0911
          type: user
        html: '<p>How to use FalconForSequenceClassification for simple sentiment
          analysis task? I tried the following:</p>

          <p>import torch<br>from transformers import AutoTokenizer, FalconForSequenceClassification</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("Rocketknight1/falcon-rw-1b")<br>model
          = FalconForSequenceClassification.from_pretrained("Rocketknight1/falcon-rw-1b",num_labels=2)<br>model.config.id2label
          = {"0" : " positive" , "1":"Negative:}<br>inputs = tokenizer("Hello, my
          dog is cute", return_tensors="pt")</p>

          <p>with torch.no_grad():<br>    logits = model(**inputs).logits</p>

          <p>predicted_class_id = logits.argmax().item()</p>

          <p>It was giving it as negative sentence</p>

          '
        raw: "How to use FalconForSequenceClassification for simple sentiment analysis\
          \ task? I tried the following:\n\nimport torch\nfrom transformers import\
          \ AutoTokenizer, FalconForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          Rocketknight1/falcon-rw-1b\")\nmodel = FalconForSequenceClassification.from_pretrained(\"\
          Rocketknight1/falcon-rw-1b\",num_labels=2)\nmodel.config.id2label = {\"\
          0\" : \" positive\" , \"1\":\"Negative:}\ninputs = tokenizer(\"Hello, my\
          \ dog is cute\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits\
          \ = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\n\
          \nIt was giving it as negative sentence"
        updatedAt: '2023-10-18T04:06:40.935Z'
      numEdits: 1
      reactions: []
    id: 652f58e024241244377a58f2
    type: comment
  author: jay0911
  content: "How to use FalconForSequenceClassification for simple sentiment analysis\
    \ task? I tried the following:\n\nimport torch\nfrom transformers import AutoTokenizer,\
    \ FalconForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    Rocketknight1/falcon-rw-1b\")\nmodel = FalconForSequenceClassification.from_pretrained(\"\
    Rocketknight1/falcon-rw-1b\",num_labels=2)\nmodel.config.id2label = {\"0\" : \"\
    \ positive\" , \"1\":\"Negative:}\ninputs = tokenizer(\"Hello, my dog is cute\"\
    , return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\
    \npredicted_class_id = logits.argmax().item()\n\nIt was giving it as negative\
    \ sentence"
  created_at: 2023-10-18 03:02:40+00:00
  edited: true
  hidden: false
  id: 652f58e024241244377a58f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78785a24e18f6f014eec8f8a577ea503.svg
      fullname: PITAMBAR MUDULI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pitambarmuduli
      type: user
    createdAt: '2023-11-02T05:15:17.000Z'
    data:
      edited: false
      editors:
      - Pitambarmuduli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9751489162445068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78785a24e18f6f014eec8f8a577ea503.svg
          fullname: PITAMBAR MUDULI
          isHf: false
          isPro: false
          name: Pitambarmuduli
          type: user
        html: '<p>How is this model applied to translation (One language to other
          like German to English )?</p>

          '
        raw: How is this model applied to translation (One language to other like
          German to English )?
        updatedAt: '2023-11-02T05:15:17.669Z'
      numEdits: 0
      reactions: []
    id: 654330650d2e27122a5fc280
    type: comment
  author: Pitambarmuduli
  content: How is this model applied to translation (One language to other like German
    to English )?
  created_at: 2023-11-02 04:15:17+00:00
  edited: false
  hidden: false
  id: 654330650d2e27122a5fc280
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b09b5a69b4188446e685a0261cad3ece.svg
      fullname: Krisha Sanjay Borana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krisha05
      type: user
    createdAt: '2023-12-10T09:09:19.000Z'
    data:
      edited: false
      editors:
      - krisha05
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8401933908462524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b09b5a69b4188446e685a0261cad3ece.svg
          fullname: Krisha Sanjay Borana
          isHf: false
          isPro: false
          name: krisha05
          type: user
        html: '<blockquote>

          <p>Hi,<br>What improvements should I make in this prompt to get an exciting
          story with 1000+ tokens?</p>

          <p>template = """<br>You are a story writer who writes long and captivating
          stories on a given topic.</p>

          <p>The output format of the story should be the following 6 paragraphs.<br>Paragraph
          1: Long Engaging Introduction (400 words)<br>Paragraph 2: Tell three main
          points about the topic.<br>Paragraph 3: Explain point 1 in detail (400 words).<br>Paragraph
          4: Explain point 2 in detail (400 words).<br>Paragraph 5: Explain point
          3 in detail (400 words).<br>Paragraph 6: Long Conclusion (400 words)</p>

          <p>The story should be 1000 words minimum.</p>

          <p>Topic: {question}<br>Story:"""</p>

          </blockquote>

          <p>I would love to know more about this</p>

          '
        raw: "> Hi,\n> What improvements should I make in this prompt to get an exciting\
          \ story with 1000+ tokens?\n> \n> template = \"\"\"\n> You are a story writer\
          \ who writes long and captivating stories on a given topic.\n> \n> The output\
          \ format of the story should be the following 6 paragraphs.\n> Paragraph\
          \ 1: Long Engaging Introduction (400 words)\n> Paragraph 2: Tell three main\
          \ points about the topic. \n> Paragraph 3: Explain point 1 in detail (400\
          \ words).\n> Paragraph 4: Explain point 2 in detail (400 words).\n> Paragraph\
          \ 5: Explain point 3 in detail (400 words).\n> Paragraph 6: Long Conclusion\
          \ (400 words)\n> \n> The story should be 1000 words minimum.\n> \n> Topic:\
          \ {question}\n> Story:\"\"\"\n\nI would love to know more about this"
        updatedAt: '2023-12-10T09:09:19.170Z'
      numEdits: 0
      reactions: []
    id: 6575803f30a4401dfb83668b
    type: comment
  author: krisha05
  content: "> Hi,\n> What improvements should I make in this prompt to get an exciting\
    \ story with 1000+ tokens?\n> \n> template = \"\"\"\n> You are a story writer\
    \ who writes long and captivating stories on a given topic.\n> \n> The output\
    \ format of the story should be the following 6 paragraphs.\n> Paragraph 1: Long\
    \ Engaging Introduction (400 words)\n> Paragraph 2: Tell three main points about\
    \ the topic. \n> Paragraph 3: Explain point 1 in detail (400 words).\n> Paragraph\
    \ 4: Explain point 2 in detail (400 words).\n> Paragraph 5: Explain point 3 in\
    \ detail (400 words).\n> Paragraph 6: Long Conclusion (400 words)\n> \n> The story\
    \ should be 1000 words minimum.\n> \n> Topic: {question}\n> Story:\"\"\"\n\nI\
    \ would love to know more about this"
  created_at: 2023-12-10 09:09:19+00:00
  edited: false
  hidden: false
  id: 6575803f30a4401dfb83668b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: Example Use
