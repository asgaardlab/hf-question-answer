!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dgallitelli
conflicting_files: null
created_at: 2023-05-30 13:10:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/199b26b6b993e56297682f2d628d1f82.svg
      fullname: Davide Gallitelli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dgallitelli
      type: user
    createdAt: '2023-05-30T14:10:54.000Z'
    data:
      edited: false
      editors:
      - dgallitelli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/199b26b6b993e56297682f2d628d1f82.svg
          fullname: Davide Gallitelli
          isHf: false
          isPro: false
          name: dgallitelli
          type: user
        html: "<p>I'm trying to deploy the model to Amazon SageMaker via the <code>HuggingFaceModel</code>\
          \ class. When I try to use the code snippet provided with this model, I\
          \ can deploy the endpoint, however the <code>predict</code> query fails\
          \ with the following error:</p>\n<pre><code>\"{\n  \"code\": 400,\n  \"\
          type\": \"InternalServerException\",\n  \"message\": \"Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code\\u003dTrue` to remove this error.\"\
          \n}\"\n</code></pre>\n<p>I tried passing myself the <code>trust_remote_code=True</code>\
          \ in the Pipeline code with a custom inference python script, but nothing\
          \ changes - which makes me think that it just gets ignored by the Multi-Model\
          \ Server in SageMaker. </p>\n<p>Any suggestions on how to solve it? </p>\n"
        raw: "I'm trying to deploy the model to Amazon SageMaker via the `HuggingFaceModel`\
          \ class. When I try to use the code snippet provided with this model, I\
          \ can deploy the endpoint, however the `predict` query fails with the following\
          \ error:\r\n\r\n```\r\n\"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\"\
          ,\r\n  \"message\": \"Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code\\u003dTrue` to remove this error.\"\
          \r\n}\"\r\n```\r\n\r\nI tried passing myself the `trust_remote_code=True`\
          \ in the Pipeline code with a custom inference python script, but nothing\
          \ changes - which makes me think that it just gets ignored by the Multi-Model\
          \ Server in SageMaker. \r\n\r\nAny suggestions on how to solve it? "
        updatedAt: '2023-05-30T14:10:54.680Z'
      numEdits: 0
      reactions: []
    id: 647603ee06092adb82b69c80
    type: comment
  author: dgallitelli
  content: "I'm trying to deploy the model to Amazon SageMaker via the `HuggingFaceModel`\
    \ class. When I try to use the code snippet provided with this model, I can deploy\
    \ the endpoint, however the `predict` query fails with the following error:\r\n\
    \r\n```\r\n\"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\"\
    ,\r\n  \"message\": \"Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct\
    \ requires you to execute the configuration file in that repo on your local machine.\
    \ Make sure you have read the code there to avoid malicious use, then set the\
    \ option `trust_remote_code\\u003dTrue` to remove this error.\"\r\n}\"\r\n```\r\
    \n\r\nI tried passing myself the `trust_remote_code=True` in the Pipeline code\
    \ with a custom inference python script, but nothing changes - which makes me\
    \ think that it just gets ignored by the Multi-Model Server in SageMaker. \r\n\
    \r\nAny suggestions on how to solve it? "
  created_at: 2023-05-30 13:10:54+00:00
  edited: false
  hidden: false
  id: 647603ee06092adb82b69c80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afa1207eb61ab7d2c01bea241d5e20e7.svg
      fullname: "Attila Sz\xE1sz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rq-aszasz
      type: user
    createdAt: '2023-06-08T12:58:01.000Z'
    data:
      edited: false
      editors:
      - rq-aszasz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.897876501083374
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afa1207eb61ab7d2c01bea241d5e20e7.svg
          fullname: "Attila Sz\xE1sz"
          isHf: false
          isPro: false
          name: rq-aszasz
          type: user
        html: '<p>I reproduced this error too.</p>

          <p>I also tried setting the <code>container_startup_health_check_timeout</code>
          parameter to 1 hour, it didn''t help.</p>

          '
        raw: 'I reproduced this error too.


          I also tried setting the `container_startup_health_check_timeout` parameter
          to 1 hour, it didn''t help.'
        updatedAt: '2023-06-08T12:58:01.850Z'
      numEdits: 0
      reactions: []
    id: 6481d05917f2fba0008ca33b
    type: comment
  author: rq-aszasz
  content: 'I reproduced this error too.


    I also tried setting the `container_startup_health_check_timeout` parameter to
    1 hour, it didn''t help.'
  created_at: 2023-06-08 11:58:01+00:00
  edited: false
  hidden: false
  id: 6481d05917f2fba0008ca33b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afa1207eb61ab7d2c01bea241d5e20e7.svg
      fullname: "Attila Sz\xE1sz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rq-aszasz
      type: user
    createdAt: '2023-06-08T18:10:51.000Z'
    data:
      edited: false
      editors:
      - rq-aszasz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6769030690193176
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afa1207eb61ab7d2c01bea241d5e20e7.svg
          fullname: "Attila Sz\xE1sz"
          isHf: false
          isPro: false
          name: rq-aszasz
          type: user
        html: '<p>Update:</p>

          <p>Try with <code>sagemaker&gt;=2.163.0</code>.</p>

          '
        raw: 'Update:


          Try with `sagemaker>=2.163.0`.'
        updatedAt: '2023-06-08T18:10:51.991Z'
      numEdits: 0
      reactions: []
    id: 648219ab2a1e051fc4318363
    type: comment
  author: rq-aszasz
  content: 'Update:


    Try with `sagemaker>=2.163.0`.'
  created_at: 2023-06-08 17:10:51+00:00
  edited: false
  hidden: false
  id: 648219ab2a1e051fc4318363
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:12:12.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7840975522994995
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>You can leverage the HuggingFace LLM Inference Container, as documented
          <a rel="nofollow" href="https://www.philschmid.de/sagemaker-falcon-llm">here</a>,
          to deploy Falcon easily in SageMaker.</p>

          '
        raw: You can leverage the HuggingFace LLM Inference Container, as documented
          [here](https://www.philschmid.de/sagemaker-falcon-llm), to deploy Falcon
          easily in SageMaker.
        updatedAt: '2023-06-09T14:12:12.266Z'
      numEdits: 0
      reactions: []
    id: 6483333c0b2cd6f14c6c8fa3
    type: comment
  author: FalconLLM
  content: You can leverage the HuggingFace LLM Inference Container, as documented
    [here](https://www.philschmid.de/sagemaker-falcon-llm), to deploy Falcon easily
    in SageMaker.
  created_at: 2023-06-09 13:12:12+00:00
  edited: false
  hidden: false
  id: 6483333c0b2cd6f14c6c8fa3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: Deployment to Amazon SageMaker - `trust_remote_code` issue
