!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AmlanSamanta
conflicting_files: null
created_at: 2023-07-27 13:48:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/-LeCTMuzJuBJDK80GRy76.jpeg?w=200&h=200&f=face
      fullname: Amlan Samanta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AmlanSamanta
      type: user
    createdAt: '2023-07-27T14:48:24.000Z'
    data:
      edited: false
      editors:
      - AmlanSamanta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6975100636482239
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/-LeCTMuzJuBJDK80GRy76.jpeg?w=200&h=200&f=face
          fullname: Amlan Samanta
          isHf: false
          isPro: false
          name: AmlanSamanta
          type: user
        html: '<p>Hi team, </p>

          <p>We are facing issues while using this model on the aforementioned machine.
          We were able to run the same experiment on G5 instance successfully but
          we are observing that the same code is not working on Inf2 machine instance.
          We are aware that it has Accelerator instead of NVIDIA GPU. Hence we tried
          the neuron-core''s capability and added required helper code for using the
          capability of neuron-cores of the instance by using the torch-neuronx library.
          The code changes and respective error screenshots are provided below for
          your reference:</p>

          <p>Code without any torch-neuronx usage - Generation code snippet:</p>

          <p>generation_output = model.generate(<br>                                  input_ids
          = input_ids,<br>                                  attention_mask = attention_mask,<br>                                  generation_config
          = generation_config,<br>                                  return_dict_in_generate
          = True,<br>                                  output_scores = False,<br>                                  max_new_tokens
          = max_new_tokens,<br>                                  early_stopping =
          True<br>                        )<br>    #print("generation_output")<br>    #print(generation_output)<br>    s
          = generation_output.sequences[0]<br>    output = tokenizer.decode(s)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/s7KaTvBtWNf1V8_09PGIF.png"><img
          alt="without any changes.png" src="https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/s7KaTvBtWNf1V8_09PGIF.png"></a></p>

          <p>Code using torch-neuronx - helper function code snippet:</p>

          <p>def generate_sample_inputs(tokenizer, sequence_length):<br>  dummy_input
          = "dummy"<br>  embeddings = tokenizer(dummy_input, max_length=sequence_length,
          padding="max_length",return_tensors="pt")<br>  return tuple(embeddings.values())</p>

          <p>def compile_model_inf2(model, tokenizer, sequence_length, num_neuron_cores):<br>    #
          use only one neuron core<br>    os.environ["NEURON_RT_NUM_CORES"] = str(num_neuron_cores)<br>    import
          torch_neuronx<br>    payload = generate_sample_inputs(tokenizer, sequence_length)<br>    return
          torch_neuronx.trace(model, payload)</p>

          <p>model = compile_model_inf2(model, tokenizer, sequence_length=512, num_neuron_cores=1)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/BZiVcDJgFbnqmcVjQXT8l.png"><img
          alt="with torch-neuron related code1.png" src="https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/BZiVcDJgFbnqmcVjQXT8l.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/4K8aTuumigeqPNhGpSSej.png"><img
          alt="with torch-neuron related code2.png" src="https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/4K8aTuumigeqPNhGpSSej.png"></a></p>

          <p>Can this github issue address our specific problems mentioned above?<br><a
          rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/2260">https://github.com/oobabooga/text-generation-webui/issues/2260</a>
          </p>

          <p>My queries are basically:</p>

          <ol>

          <li>Is it even feasible to do inference on this machine or should we go
          for G4dn.8xlarge as we are facing so many issues in Inf2?</li>

          <li>Can we try Llama 2 on Inferentia 2 8xlarge machine or this is not supported?
          If not, which machine instance we should try considering cost-effectiveness?</li>

          </ol>

          '
        raw: "Hi team, \r\n\r\nWe are facing issues while using this model on the\
          \ aforementioned machine. We were able to run the same experiment on G5\
          \ instance successfully but we are observing that the same code is not working\
          \ on Inf2 machine instance. We are aware that it has Accelerator instead\
          \ of NVIDIA GPU. Hence we tried the neuron-core's capability and added required\
          \ helper code for using the capability of neuron-cores of the instance by\
          \ using the torch-neuronx library. The code changes and respective error\
          \ screenshots are provided below for your reference:\r\n\r\nCode without\
          \ any torch-neuronx usage - Generation code snippet:\r\n\r\ngeneration_output\
          \ = model.generate(\r\n                                  input_ids = input_ids,\
          \ \r\n                                  attention_mask = attention_mask,\
          \ \r\n                                  generation_config = generation_config,\r\
          \n                                  return_dict_in_generate = True, \r\n\
          \                                  output_scores = False, \r\n         \
          \                         max_new_tokens = max_new_tokens, \r\n        \
          \                          early_stopping = True\r\n                   \
          \     )\r\n    #print(\"generation_output\")\r\n    #print(generation_output)\r\
          \n    s = generation_output.sequences[0]\r\n    output = tokenizer.decode(s)\r\
          \n\r\n![without any changes.png](https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/s7KaTvBtWNf1V8_09PGIF.png)\r\
          \n\r\n\r\nCode using torch-neuronx - helper function code snippet:\r\n\r\
          \ndef generate_sample_inputs(tokenizer, sequence_length):\r\n  dummy_input\
          \ = \"dummy\"\r\n  embeddings = tokenizer(dummy_input, max_length=sequence_length,\
          \ padding=\"max_length\",return_tensors=\"pt\")\r\n  return tuple(embeddings.values())\r\
          \n\r\ndef compile_model_inf2(model, tokenizer, sequence_length, num_neuron_cores):\r\
          \n    # use only one neuron core\r\n    os.environ[\"NEURON_RT_NUM_CORES\"\
          ] = str(num_neuron_cores)\r\n    import torch_neuronx\r\n    payload = generate_sample_inputs(tokenizer,\
          \ sequence_length)\r\n    return torch_neuronx.trace(model, payload)\r\n\
          \r\nmodel = compile_model_inf2(model, tokenizer, sequence_length=512, num_neuron_cores=1)\r\
          \n\r\n\r\n![with torch-neuron related code1.png](https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/BZiVcDJgFbnqmcVjQXT8l.png)\r\
          \n\r\n\r\n![with torch-neuron related code2.png](https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/4K8aTuumigeqPNhGpSSej.png)\r\
          \n\r\n\r\nCan this github issue address our specific problems mentioned\
          \ above? \r\nhttps://github.com/oobabooga/text-generation-webui/issues/2260\
          \ \r\n\r\nMy queries are basically:\r\n1. Is it even feasible to do inference\
          \ on this machine or should we go for G4dn.8xlarge as we are facing so many\
          \ issues in Inf2?\r\n2. Can we try Llama 2 on Inferentia 2 8xlarge machine\
          \ or this is not supported? If not, which machine instance we should try\
          \ considering cost-effectiveness?\r\n"
        updatedAt: '2023-07-27T14:48:24.294Z'
      numEdits: 0
      reactions: []
    id: 64c283b8ca709fb385274722
    type: comment
  author: AmlanSamanta
  content: "Hi team, \r\n\r\nWe are facing issues while using this model on the aforementioned\
    \ machine. We were able to run the same experiment on G5 instance successfully\
    \ but we are observing that the same code is not working on Inf2 machine instance.\
    \ We are aware that it has Accelerator instead of NVIDIA GPU. Hence we tried the\
    \ neuron-core's capability and added required helper code for using the capability\
    \ of neuron-cores of the instance by using the torch-neuronx library. The code\
    \ changes and respective error screenshots are provided below for your reference:\r\
    \n\r\nCode without any torch-neuronx usage - Generation code snippet:\r\n\r\n\
    generation_output = model.generate(\r\n                                  input_ids\
    \ = input_ids, \r\n                                  attention_mask = attention_mask,\
    \ \r\n                                  generation_config = generation_config,\r\
    \n                                  return_dict_in_generate = True, \r\n     \
    \                             output_scores = False, \r\n                    \
    \              max_new_tokens = max_new_tokens, \r\n                         \
    \         early_stopping = True\r\n                        )\r\n    #print(\"\
    generation_output\")\r\n    #print(generation_output)\r\n    s = generation_output.sequences[0]\r\
    \n    output = tokenizer.decode(s)\r\n\r\n![without any changes.png](https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/s7KaTvBtWNf1V8_09PGIF.png)\r\
    \n\r\n\r\nCode using torch-neuronx - helper function code snippet:\r\n\r\ndef\
    \ generate_sample_inputs(tokenizer, sequence_length):\r\n  dummy_input = \"dummy\"\
    \r\n  embeddings = tokenizer(dummy_input, max_length=sequence_length, padding=\"\
    max_length\",return_tensors=\"pt\")\r\n  return tuple(embeddings.values())\r\n\
    \r\ndef compile_model_inf2(model, tokenizer, sequence_length, num_neuron_cores):\r\
    \n    # use only one neuron core\r\n    os.environ[\"NEURON_RT_NUM_CORES\"] =\
    \ str(num_neuron_cores)\r\n    import torch_neuronx\r\n    payload = generate_sample_inputs(tokenizer,\
    \ sequence_length)\r\n    return torch_neuronx.trace(model, payload)\r\n\r\nmodel\
    \ = compile_model_inf2(model, tokenizer, sequence_length=512, num_neuron_cores=1)\r\
    \n\r\n\r\n![with torch-neuron related code1.png](https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/BZiVcDJgFbnqmcVjQXT8l.png)\r\
    \n\r\n\r\n![with torch-neuron related code2.png](https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/4K8aTuumigeqPNhGpSSej.png)\r\
    \n\r\n\r\nCan this github issue address our specific problems mentioned above?\
    \ \r\nhttps://github.com/oobabooga/text-generation-webui/issues/2260 \r\n\r\n\
    My queries are basically:\r\n1. Is it even feasible to do inference on this machine\
    \ or should we go for G4dn.8xlarge as we are facing so many issues in Inf2?\r\n\
    2. Can we try Llama 2 on Inferentia 2 8xlarge machine or this is not supported?\
    \ If not, which machine instance we should try considering cost-effectiveness?\r\
    \n"
  created_at: 2023-07-27 13:48:24+00:00
  edited: false
  hidden: false
  id: 64c283b8ca709fb385274722
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/-LeCTMuzJuBJDK80GRy76.jpeg?w=200&h=200&f=face
      fullname: Amlan Samanta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AmlanSamanta
      type: user
    createdAt: '2023-08-02T16:56:05.000Z'
    data:
      from: Issue with Falcon LLM while trying to use it on AWS EC2 Inferentia 2.8xlarge  Instance
      to: Issue with Falcon LLM while trying to use it on AWS EC2 Inferentia 2.8xlarge
        Instance
    id: 64ca8aa5d469fc2cf81fc4d8
    type: title-change
  author: AmlanSamanta
  created_at: 2023-08-02 15:56:05+00:00
  id: 64ca8aa5d469fc2cf81fc4d8
  new_title: Issue with Falcon LLM while trying to use it on AWS EC2 Inferentia 2.8xlarge
    Instance
  old_title: Issue with Falcon LLM while trying to use it on AWS EC2 Inferentia 2.8xlarge  Instance
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: Issue with Falcon LLM while trying to use it on AWS EC2 Inferentia 2.8xlarge
  Instance
