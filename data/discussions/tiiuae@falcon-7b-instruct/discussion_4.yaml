!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tarungupta83
conflicting_files: null
created_at: 2023-05-27 18:53:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/982e91dffeabfcc42dd78236918d0110.svg
      fullname: Tarun Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarungupta83
      type: user
    createdAt: '2023-05-27T19:53:00.000Z'
    data:
      edited: true
      editors:
      - tarungupta83
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/982e91dffeabfcc42dd78236918d0110.svg
          fullname: Tarun Gupta
          isHf: false
          isPro: false
          name: tarungupta83
          type: user
        html: "<h1>How use it with LangChain Library </h1>\n <h2>what is wrong in\
          \ this code </h2>\n\n<pre><code>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\nimport transformers\nimport torch\nfrom langchain\
          \ import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\
          \nmodel = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\n\n# template for an instruction with no input\n\
          prompt = PromptTemplate(\n    input_variables=[\"instruction\"],\n    template=\"\
          {instruction}\"\n)\n\nllm = HuggingFacePipeline(pipeline=pipeline)\nllm_chain\
          \ = LLMChain(llm=llm, prompt=prompt)\n\nprint(llm_chain.predict(\n    instruction=\"\
          Explain to me the difference between nuclear fission and fusion.\"\n).lstrip())\n\
          </code></pre>\n"
        raw: "<h1>How use it with LangChain Library </h1>\n <h2>what is wrong in this\
          \ code </h2>\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\nfrom langchain import PromptTemplate,\
          \ LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\nmodel = \"\
          tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\n\n# template for an instruction with no input\n\
          prompt = PromptTemplate(\n    input_variables=[\"instruction\"],\n    template=\"\
          {instruction}\"\n)\n\nllm = HuggingFacePipeline(pipeline=pipeline)\nllm_chain\
          \ = LLMChain(llm=llm, prompt=prompt)\n\nprint(llm_chain.predict(\n    instruction=\"\
          Explain to me the difference between nuclear fission and fusion.\"\n).lstrip())\n\
          ```"
        updatedAt: '2023-05-27T19:58:35.474Z'
      numEdits: 5
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - arshad615
      - count: 1
        reaction: "\U0001F91D"
        users:
        - dqurious1
    id: 64725f9c6facfb01d8af9cce
    type: comment
  author: tarungupta83
  content: "<h1>How use it with LangChain Library </h1>\n <h2>what is wrong in this\
    \ code </h2>\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    import transformers\nimport torch\nfrom langchain import PromptTemplate, LLMChain\n\
    from langchain.llms import HuggingFacePipeline\n\nmodel = \"tiiuae/falcon-7b-instruct\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
    \    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\n# template for an\
    \ instruction with no input\nprompt = PromptTemplate(\n    input_variables=[\"\
    instruction\"],\n    template=\"{instruction}\"\n)\n\nllm = HuggingFacePipeline(pipeline=pipeline)\n\
    llm_chain = LLMChain(llm=llm, prompt=prompt)\n\nprint(llm_chain.predict(\n   \
    \ instruction=\"Explain to me the difference between nuclear fission and fusion.\"\
    \n).lstrip())\n```"
  created_at: 2023-05-27 18:53:00+00:00
  edited: true
  hidden: false
  id: 64725f9c6facfb01d8af9cce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T14:58:30.000Z'
    data:
      pinned: true
    id: 64760f16b2ef3a5a82fe596a
    type: pinning-change
  author: FalconLLM
  created_at: 2023-05-30 13:58:30+00:00
  id: 64760f16b2ef3a5a82fe596a
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T14:58:36.000Z'
    data:
      pinned: false
    id: 64760f1cbf6b09dc72867ee5
    type: pinning-change
  author: FalconLLM
  created_at: 2023-05-30 13:58:36+00:00
  id: 64760f1cbf6b09dc72867ee5
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6092f7205f7ae77c35e3a7143f4ff726.svg
      fullname: Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: its-eric-liu
      type: user
    createdAt: '2023-09-06T22:55:36.000Z'
    data:
      edited: false
      editors:
      - its-eric-liu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5061749219894409
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6092f7205f7ae77c35e3a7143f4ff726.svg
          fullname: Liu
          isHf: false
          isPro: false
          name: its-eric-liu
          type: user
        html: '<hr>

          <p>ValueError                                Traceback (most recent call
          last)<br> in &lt;cell line: 10&gt;()<br>      8<br>      9 tokenizer = AutoTokenizer.from_pretrained(model)<br>---&gt;
          10 pipeline = transformers.pipeline(<br>     11     "text-generation",<br>     12     model=model,</p>

          <p>1 frames<br>/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py
          in infer_framework_load_model(model, config, model_classes, task, framework,
          **model_kwargs)<br>    280             for class_name, trace in all_traceback.items():<br>    281                 error
          += f"while loading with {class_name}, an error is thrown:\n{trace}\n"<br>--&gt;
          282             raise ValueError(<br>    283                 f"Could not
          load model {model} with any of the following classes: {class_tuple}. See
          the original errors:\n\n{error}\n"<br>    284             )</p>

          <p>ValueError: Could not load model tiiuae/falcon-7b-instruct with any of
          the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class ''transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM''&gt;).
          See the original errors:</p>

          <p>while loading with AutoModelForCausalLM, an error is thrown:<br>Traceback
          (most recent call last):<br>  File "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py",
          line 269, in infer_framework_load_model<br>    model = model_class.from_pretrained(model,
          **kwargs)<br>  File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py",
          line 558, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
          line 3165, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>  File
          "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
          line 3286, in _load_pretrained_model<br>    raise ValueError(<br>ValueError:
          The current <code>device_map</code> had weights offloaded to the disk. Please
          provide an <code>offload_folder</code> for them. Alternatively, make sure
          you have <code>safetensors</code> installed if the model you are using offers
          the weights in this format.</p>

          <p>while loading with TFAutoModelForCausalLM, an error is thrown:<br>Traceback
          (most recent call last):<br>  File "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py",
          line 269, in infer_framework_load_model<br>    model = model_class.from_pretrained(model,
          **kwargs)<br>  File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py",
          line 566, in from_pretrained<br>    raise ValueError(<br>ValueError: Unrecognized
          configuration class &lt;class ''transformers_modules.tiiuae.falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07.configuration_RW.RWConfig''&gt;
          for this kind of AutoModel: TFAutoModelForCausalLM.<br>Model type should
          be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,
          GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig,
          RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig,
          XLNetConfig.</p>

          '
        raw: "---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          <ipython-input-13-5723accdc0f3> in <cell line: 10>()\n      8 \n      9\
          \ tokenizer = AutoTokenizer.from_pretrained(model)\n---> 10 pipeline = transformers.pipeline(\n\
          \     11     \"text-generation\",\n     12     model=model,\n\n1 frames\n\
          /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py in\
          \ infer_framework_load_model(model, config, model_classes, task, framework,\
          \ **model_kwargs)\n    280             for class_name, trace in all_traceback.items():\n\
          \    281                 error += f\"while loading with {class_name}, an\
          \ error is thrown:\\n{trace}\\n\"\n--> 282             raise ValueError(\n\
          \    283                 f\"Could not load model {model} with any of the\
          \ following classes: {class_tuple}. See the original errors:\\n\\n{error}\\\
          n\"\n    284             )\n\nValueError: Could not load model tiiuae/falcon-7b-instruct\
          \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\
          \ See the original errors:\n\nwhile loading with AutoModelForCausalLM, an\
          \ error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3165, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3286, in _load_pretrained_model\n    raise ValueError(\nValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers the weights in this format.\n\
          \nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback\
          \ (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized\
          \ configuration class <class 'transformers_modules.tiiuae.falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07.configuration_RW.RWConfig'>\
          \ for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should\
          \ be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,\
          \ GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig,\
          \ XLMConfig, XLMRobertaConfig, XLNetConfig.\n"
        updatedAt: '2023-09-06T22:55:36.834Z'
      numEdits: 0
      reactions: []
    id: 64f903684110f1806f2b8e1f
    type: comment
  author: its-eric-liu
  content: "---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    <ipython-input-13-5723accdc0f3> in <cell line: 10>()\n      8 \n      9 tokenizer\
    \ = AutoTokenizer.from_pretrained(model)\n---> 10 pipeline = transformers.pipeline(\n\
    \     11     \"text-generation\",\n     12     model=model,\n\n1 frames\n/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\
    \ in infer_framework_load_model(model, config, model_classes, task, framework,\
    \ **model_kwargs)\n    280             for class_name, trace in all_traceback.items():\n\
    \    281                 error += f\"while loading with {class_name}, an error\
    \ is thrown:\\n{trace}\\n\"\n--> 282             raise ValueError(\n    283  \
    \               f\"Could not load model {model} with any of the following classes:\
    \ {class_tuple}. See the original errors:\\n\\n{error}\\n\"\n    284         \
    \    )\n\nValueError: Could not load model tiiuae/falcon-7b-instruct with any\
    \ of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\
    \ See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error\
    \ is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
    , line 558, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\",\
    \ line 3165, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line\
    \ 3286, in _load_pretrained_model\n    raise ValueError(\nValueError: The current\
    \ `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\
    \ for them. Alternatively, make sure you have `safetensors` installed if the model\
    \ you are using offers the weights in this format.\n\nwhile loading with TFAutoModelForCausalLM,\
    \ an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
    , line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized\
    \ configuration class <class 'transformers_modules.tiiuae.falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07.configuration_RW.RWConfig'>\
    \ for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one\
    \ of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig,\
    \ OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig,\
    \ RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n"
  created_at: 2023-09-06 21:55:36+00:00
  edited: false
  hidden: false
  id: 64f903684110f1806f2b8e1f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: How use it with LangChain Library
