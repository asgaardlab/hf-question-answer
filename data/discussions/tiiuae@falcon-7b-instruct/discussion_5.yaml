!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ThreeBlessings
conflicting_files: null
created_at: 2023-05-27 19:55:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/511705723ec776a91bdff4fd4bd37c12.svg
      fullname: Volodymyr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThreeBlessings
      type: user
    createdAt: '2023-05-27T20:55:02.000Z'
    data:
      edited: false
      editors:
      - ThreeBlessings
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/511705723ec776a91bdff4fd4bd37c12.svg
          fullname: Volodymyr
          isHf: false
          isPro: false
          name: ThreeBlessings
          type: user
        html: '<p>Hi!</p>

          <p>I''m updating a <a rel="nofollow" href="https://github.com/dcai-course/dcai-lab/blob/master/prompt_engineering/Lab_Prompt_Engineering.ipynb">lab</a>
          for Data-Centric AI course and it would be cool to use this model with <code>load_in_8bit=True</code>
          parameter and have it sharded in 2Gb weights for easy use with free tier
          Colab GPUs.</p>

          <p>Is it planned to add this features?</p>

          '
        raw: "Hi!\r\n\r\nI'm updating a [lab](https://github.com/dcai-course/dcai-lab/blob/master/prompt_engineering/Lab_Prompt_Engineering.ipynb)\
          \ for Data-Centric AI course and it would be cool to use this model with\
          \ `load_in_8bit=True` parameter and have it sharded in 2Gb weights for easy\
          \ use with free tier Colab GPUs.\r\n\r\nIs it planned to add this features?\r\
          \n"
        updatedAt: '2023-05-27T20:55:02.714Z'
      numEdits: 0
      reactions: []
    id: 64726e2622016353ae3bf150
    type: comment
  author: ThreeBlessings
  content: "Hi!\r\n\r\nI'm updating a [lab](https://github.com/dcai-course/dcai-lab/blob/master/prompt_engineering/Lab_Prompt_Engineering.ipynb)\
    \ for Data-Centric AI course and it would be cool to use this model with `load_in_8bit=True`\
    \ parameter and have it sharded in 2Gb weights for easy use with free tier Colab\
    \ GPUs.\r\n\r\nIs it planned to add this features?\r\n"
  created_at: 2023-05-27 19:55:02+00:00
  edited: false
  hidden: false
  id: 64726e2622016353ae3bf150
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e644992d17fe0a6f97d8930a6aa64aa5.svg
      fullname: Vladimir P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CleverShovel
      type: user
    createdAt: '2023-05-28T12:36:20.000Z'
    data:
      edited: false
      editors:
      - CleverShovel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e644992d17fe0a6f97d8930a6aa64aa5.svg
          fullname: Vladimir P
          isHf: false
          isPro: false
          name: CleverShovel
          type: user
        html: '<p>Hi! </p>

          <p>I''m not an author of this model but I sharded this model, you can check
          it <a href="https://huggingface.co/CleverShovel/falcon-7b-instruct-sharded-bf16">here</a>.<br>And
          I did it using free tier Colab environment with no GPUs, in this environment
          Colab give you enough RAM to load models up to 7B.</p>

          '
        raw: "Hi! \n\nI'm not an author of this model but I sharded this model, you\
          \ can check it [here](https://huggingface.co/CleverShovel/falcon-7b-instruct-sharded-bf16).\
          \ \nAnd I did it using free tier Colab environment with no GPUs, in this\
          \ environment Colab give you enough RAM to load models up to 7B."
        updatedAt: '2023-05-28T12:36:20.496Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FalconLLM
    id: 64734ac46cff2f8672029ce4
    type: comment
  author: CleverShovel
  content: "Hi! \n\nI'm not an author of this model but I sharded this model, you\
    \ can check it [here](https://huggingface.co/CleverShovel/falcon-7b-instruct-sharded-bf16).\
    \ \nAnd I did it using free tier Colab environment with no GPUs, in this environment\
    \ Colab give you enough RAM to load models up to 7B."
  created_at: 2023-05-28 11:36:20+00:00
  edited: false
  hidden: false
  id: 64734ac46cff2f8672029ce4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:09:37.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.887657642364502
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>There has been some support for 4-bit in a great external library,
          <a rel="nofollow" href="https://github.com/rmihaylov/falcontune">FalconTune</a>.
          You can also check-out this <a href="https://huggingface.co/blog/falcon">blog
          post</a> from HuggingFace.</p>

          '
        raw: There has been some support for 4-bit in a great external library, [FalconTune](https://github.com/rmihaylov/falcontune).
          You can also check-out this [blog post](https://huggingface.co/blog/falcon)
          from HuggingFace.
        updatedAt: '2023-06-09T14:09:37.620Z'
      numEdits: 0
      reactions: []
    id: 648332a10b2cd6f14c6c56b8
    type: comment
  author: FalconLLM
  content: There has been some support for 4-bit in a great external library, [FalconTune](https://github.com/rmihaylov/falcontune).
    You can also check-out this [blog post](https://huggingface.co/blog/falcon) from
    HuggingFace.
  created_at: 2023-06-09 13:09:37+00:00
  edited: false
  hidden: false
  id: 648332a10b2cd6f14c6c56b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/519f08996a075bcb155bd67e8cb60372.svg
      fullname: Arsenii Shatokhin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vrsen
      type: user
    createdAt: '2023-06-14T16:56:15.000Z'
    data:
      edited: false
      editors:
      - vrsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6291282773017883
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/519f08996a075bcb155bd67e8cb60372.svg
          fullname: Arsenii Shatokhin
          isHf: false
          isPro: false
          name: vrsen
          type: user
        html: '<p>Hey, check out my video on how to fine tune and use instruct on
          a single gpu in free google colab: <a rel="nofollow" href="https://youtu.be/AXG7TA7vIQ8">https://youtu.be/AXG7TA7vIQ8</a></p>

          '
        raw: 'Hey, check out my video on how to fine tune and use instruct on a single
          gpu in free google colab: https://youtu.be/AXG7TA7vIQ8'
        updatedAt: '2023-06-14T16:56:15.166Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - vrsen
        - ThreeBlessings
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vrsen
      - count: 1
        reaction: "\U0001F92F"
        users:
        - vrsen
    id: 6489f12f67890b4083e5e27d
    type: comment
  author: vrsen
  content: 'Hey, check out my video on how to fine tune and use instruct on a single
    gpu in free google colab: https://youtu.be/AXG7TA7vIQ8'
  created_at: 2023-06-14 15:56:15+00:00
  edited: false
  hidden: false
  id: 6489f12f67890b4083e5e27d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: 8bit and sharded weights
