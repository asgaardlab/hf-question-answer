!!python/object:huggingface_hub.community.DiscussionWithDetails
author: beejay
conflicting_files: null
created_at: 2023-06-18 03:28:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a563a2b4878296d8bcfff54aad63335.svg
      fullname: Baskar Jayaraman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: beejay
      type: user
    createdAt: '2023-06-18T04:28:45.000Z'
    data:
      edited: true
      editors:
      - beejay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9773492217063904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a563a2b4878296d8bcfff54aad63335.svg
          fullname: Baskar Jayaraman
          isHf: false
          isPro: false
          name: beejay
          type: user
        html: '<p>Has anyone been able to successfully use any of the Falcon models  and
          produce a question-answering system where the answer comes from a document
          for a given question? If there is no answer in the given document, the application
          should say something to the effect of "No answer in the document/I don''t
          know". I have been trying, rather unsuccessfully, and anything I have tried
          always returns an answer that is clearly not in the document, which is basically
          what the model has seen in the training data. If anyone has had success,
          it would be great if the code that achieves it can be shared. My need is
          to run this on my own GPU without relying on a hosted inference API such
          as a HF API.</p>

          '
        raw: Has anyone been able to successfully use any of the Falcon models  and
          produce a question-answering system where the answer comes from a document
          for a given question? If there is no answer in the given document, the application
          should say something to the effect of "No answer in the document/I don't
          know". I have been trying, rather unsuccessfully, and anything I have tried
          always returns an answer that is clearly not in the document, which is basically
          what the model has seen in the training data. If anyone has had success,
          it would be great if the code that achieves it can be shared. My need is
          to run this on my own GPU without relying on a hosted inference API such
          as a HF API.
        updatedAt: '2023-06-18T05:45:28.858Z'
      numEdits: 2
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - rustamg
        - vipulad
        - inspectablock
        - dangkhoa99
    id: 648e87fda3090498da4ef217
    type: comment
  author: beejay
  content: Has anyone been able to successfully use any of the Falcon models  and
    produce a question-answering system where the answer comes from a document for
    a given question? If there is no answer in the given document, the application
    should say something to the effect of "No answer in the document/I don't know".
    I have been trying, rather unsuccessfully, and anything I have tried always returns
    an answer that is clearly not in the document, which is basically what the model
    has seen in the training data. If anyone has had success, it would be great if
    the code that achieves it can be shared. My need is to run this on my own GPU
    without relying on a hosted inference API such as a HF API.
  created_at: 2023-06-18 03:28:45+00:00
  edited: true
  hidden: false
  id: 648e87fda3090498da4ef217
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1607085671166-noauth.jpeg?w=200&h=200&f=face
      fullname: Shiv
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shivaraj
      type: user
    createdAt: '2023-07-31T14:24:35.000Z'
    data:
      edited: false
      editors:
      - Shivaraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9633352160453796
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1607085671166-noauth.jpeg?w=200&h=200&f=face
          fullname: Shiv
          isHf: false
          isPro: false
          name: Shivaraj
          type: user
        html: '<p>I am facing the same issue, The work around I have is I put cosine
          similarity cut-off for query and available snippets any question out of
          context is asked cosine similarity will less, hence I don''t even trigger
          the LLM. </p>

          '
        raw: 'I am facing the same issue, The work around I have is I put cosine similarity
          cut-off for query and available snippets any question out of context is
          asked cosine similarity will less, hence I don''t even trigger the LLM. '
        updatedAt: '2023-07-31T14:24:35.257Z'
      numEdits: 0
      reactions: []
    id: 64c7c423904317f42d96ed28
    type: comment
  author: Shivaraj
  content: 'I am facing the same issue, The work around I have is I put cosine similarity
    cut-off for query and available snippets any question out of context is asked
    cosine similarity will less, hence I don''t even trigger the LLM. '
  created_at: 2023-07-31 13:24:35+00:00
  edited: false
  hidden: false
  id: 64c7c423904317f42d96ed28
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: any success in In-context question-answering?
