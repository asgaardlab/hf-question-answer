!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ibrahim-Ola
conflicting_files: null
created_at: 2023-06-08 15:31:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d200a5159f7d086f38438c50e29b4f73.svg
      fullname: Ibrahim Olalekan Alabi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ibrahim-Ola
      type: user
    createdAt: '2023-06-08T16:31:04.000Z'
    data:
      edited: false
      editors:
      - Ibrahim-Ola
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5794273018836975
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d200a5159f7d086f38438c50e29b4f73.svg
          fullname: Ibrahim Olalekan Alabi
          isHf: false
          isPro: false
          name: Ibrahim-Ola
          type: user
        html: '<p>I''m getting the following the error below when I try to load my
          model on Ubuntu and MacOS (i7, 2018)</p>

          <p><code>ValueError: Could not load model tiiuae/falcon-7b-instruct with
          any of the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</code></p>

          <p>Note: all my packages are up to date. My pipeline is:</p>

          <p>model = "tiiuae/falcon-7b-instruct" </p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model)</p>

          <p>pipeline = pipeline(<br>    task="text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map="auto",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id<br>)</p>

          <p>Any help is appreciated.</p>

          <p>Thanks!</p>

          '
        raw: "I'm getting the following the error below when I try to load my model\
          \ on Ubuntu and MacOS (i7, 2018)\r\n\r\n```ValueError: Could not load model\
          \ tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```\r\
          \n\r\nNote: all my packages are up to date. My pipeline is:\r\n\r\nmodel\
          \ = \"tiiuae/falcon-7b-instruct\" \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
          \n\r\npipeline = pipeline(\r\n    task=\"text-generation\", \r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
          \n    device_map=\"auto\",\r\n    max_length=200,\r\n    do_sample=True,\r\
          \n    top_k=10,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id\r\
          \n)\r\n\r\nAny help is appreciated.\r\n\r\nThanks!\r\n"
        updatedAt: '2023-06-08T16:31:04.505Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - jithinrocs
        - RafaelMPereira
        - EwenVCC
        - neoneye
        - ChinniAjay
        - Rohitvj
    id: 648202480da080ec246e641c
    type: comment
  author: Ibrahim-Ola
  content: "I'm getting the following the error below when I try to load my model\
    \ on Ubuntu and MacOS (i7, 2018)\r\n\r\n```ValueError: Could not load model tiiuae/falcon-7b-instruct\
    \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```\r\
    \n\r\nNote: all my packages are up to date. My pipeline is:\r\n\r\nmodel = \"\
    tiiuae/falcon-7b-instruct\" \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
    \n\r\npipeline = pipeline(\r\n    task=\"text-generation\", \r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
    \n    device_map=\"auto\",\r\n    max_length=200,\r\n    do_sample=True,\r\n \
    \   top_k=10,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id\r\
    \n)\r\n\r\nAny help is appreciated.\r\n\r\nThanks!\r\n"
  created_at: 2023-06-08 15:31:04+00:00
  edited: false
  hidden: false
  id: 648202480da080ec246e641c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f42c2209f2780d3320a10f4758f26303.svg
      fullname: Will Tejeda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thewilltejeda
      type: user
    createdAt: '2023-06-09T01:08:36.000Z'
    data:
      edited: true
      editors:
      - thewilltejeda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6897875666618347
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f42c2209f2780d3320a10f4758f26303.svg
          fullname: Will Tejeda
          isHf: false
          isPro: false
          name: thewilltejeda
          type: user
        html: "<p>Same issue here </p>\n<p>I'm on a 16\" M1 Pro macbook 16GB RAM 16Core\
          \ GPU , </p>\n<p>Python3.9.2</p>\n<pre><code>\nTraceback (most recent call\
          \ last):\n  File \"/Users/__/Code/FalconLLM/./main.py\", line 11, in &lt;module&gt;\n\
          \    pipeline = transformers.pipeline(\n               ^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
          , line 788, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 278, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model tiiuae/falcon-7b with any of the following\
          \ classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,).\n\
          </code></pre>\n"
        raw: "Same issue here \n\nI'm on a 16\" M1 Pro macbook 16GB RAM 16Core GPU\
          \ , \n\nPython3.9.2\n\n```\n\nTraceback (most recent call last):\n  File\
          \ \"/Users/__/Code/FalconLLM/./main.py\", line 11, in <module>\n    pipeline\
          \ = transformers.pipeline(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
          , line 788, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 278, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model tiiuae/falcon-7b with any of the following\
          \ classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
          ```"
        updatedAt: '2023-06-09T01:20:48.944Z'
      numEdits: 4
      reactions: []
    id: 64827b94ba6c556892f5a3df
    type: comment
  author: thewilltejeda
  content: "Same issue here \n\nI'm on a 16\" M1 Pro macbook 16GB RAM 16Core GPU ,\
    \ \n\nPython3.9.2\n\n```\n\nTraceback (most recent call last):\n  File \"/Users/__/Code/FalconLLM/./main.py\"\
    , line 11, in <module>\n    pipeline = transformers.pipeline(\n              \
    \ ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
    , line 788, in pipeline\n    framework, model = infer_framework_load_model(\n\
    \                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
    , line 278, in infer_framework_load_model\n    raise ValueError(f\"Could not load\
    \ model {model} with any of the following classes: {class_tuple}.\")\nValueError:\
    \ Could not load model tiiuae/falcon-7b with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n```"
  created_at: 2023-06-09 00:08:36+00:00
  edited: true
  hidden: false
  id: 64827b94ba6c556892f5a3df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d04c6f23ac9caaca7eba61575cbb631.svg
      fullname: Imran Hendley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ihendley
      type: user
    createdAt: '2023-06-09T02:14:49.000Z'
    data:
      edited: false
      editors:
      - ihendley
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9035215973854065
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d04c6f23ac9caaca7eba61575cbb631.svg
          fullname: Imran Hendley
          isHf: false
          isPro: false
          name: ihendley
          type: user
        html: '<p>I was able to load it and generate text on my 64GB M1 Max after
          upgrading torch to the latest 2.01 via <code>pip install --upgrade torch</code>
          and then changing <code>torch_dtype=torch.bfloat16</code> to <code>torch_dtype=torch.float32</code>
          in the pipeline. However the generation was extremely slow.</p>

          '
        raw: I was able to load it and generate text on my 64GB M1 Max after upgrading
          torch to the latest 2.01 via `pip install --upgrade torch` and then changing
          `torch_dtype=torch.bfloat16` to `torch_dtype=torch.float32` in the pipeline.
          However the generation was extremely slow.
        updatedAt: '2023-06-09T02:14:49.477Z'
      numEdits: 0
      reactions: []
    id: 64828b19d1bc22de4d92ce03
    type: comment
  author: ihendley
  content: I was able to load it and generate text on my 64GB M1 Max after upgrading
    torch to the latest 2.01 via `pip install --upgrade torch` and then changing `torch_dtype=torch.bfloat16`
    to `torch_dtype=torch.float32` in the pipeline. However the generation was extremely
    slow.
  created_at: 2023-06-09 01:14:49+00:00
  edited: false
  hidden: false
  id: 64828b19d1bc22de4d92ce03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d200a5159f7d086f38438c50e29b4f73.svg
      fullname: Ibrahim Olalekan Alabi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ibrahim-Ola
      type: user
    createdAt: '2023-06-09T16:55:31.000Z'
    data:
      edited: false
      editors:
      - Ibrahim-Ola
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8732522130012512
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d200a5159f7d086f38438c50e29b4f73.svg
          fullname: Ibrahim Olalekan Alabi
          isHf: false
          isPro: false
          name: Ibrahim-Ola
          type: user
        html: '<blockquote>

          <p>I was able to load it and generate text on my 64GB M1 Max after upgrading
          torch to the latest 2.01 via <code>pip install --upgrade torch</code> and
          then changing <code>torch_dtype=torch.bfloat16</code> to <code>torch_dtype=torch.float32</code>
          in the pipeline. However the generation was extremely slow.</p>

          </blockquote>

          <p>I changed <code>torch_dtype=torch.bfloat16</code> to  <code>torch_dtype=torch.float32</code>,
          but I still get the same error. My torch is the latest. I am on 16GB RAM,
          though.</p>

          '
        raw: '> I was able to load it and generate text on my 64GB M1 Max after upgrading
          torch to the latest 2.01 via `pip install --upgrade torch` and then changing
          `torch_dtype=torch.bfloat16` to `torch_dtype=torch.float32` in the pipeline.
          However the generation was extremely slow.


          I changed ```torch_dtype=torch.bfloat16``` to  ```torch_dtype=torch.float32```,
          but I still get the same error. My torch is the latest. I am on 16GB RAM,
          though.'
        updatedAt: '2023-06-09T16:55:31.525Z'
      numEdits: 0
      reactions: []
    id: 648359835a6b3d904bdf496d
    type: comment
  author: Ibrahim-Ola
  content: '> I was able to load it and generate text on my 64GB M1 Max after upgrading
    torch to the latest 2.01 via `pip install --upgrade torch` and then changing `torch_dtype=torch.bfloat16`
    to `torch_dtype=torch.float32` in the pipeline. However the generation was extremely
    slow.


    I changed ```torch_dtype=torch.bfloat16``` to  ```torch_dtype=torch.float32```,
    but I still get the same error. My torch is the latest. I am on 16GB RAM, though.'
  created_at: 2023-06-09 15:55:31+00:00
  edited: false
  hidden: false
  id: 648359835a6b3d904bdf496d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666575791150-62e6ce3b7af5c995d8859911.png?w=200&h=200&f=face
      fullname: Xingyu Bian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: therealcyberlord
      type: user
    createdAt: '2023-06-12T14:47:52.000Z'
    data:
      edited: false
      editors:
      - therealcyberlord
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9989177584648132
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666575791150-62e6ce3b7af5c995d8859911.png?w=200&h=200&f=face
          fullname: Xingyu Bian
          isHf: false
          isPro: false
          name: therealcyberlord
          type: user
        html: '<p>having the same issue as well</p>

          '
        raw: having the same issue as well
        updatedAt: '2023-06-12T14:47:52.133Z'
      numEdits: 0
      reactions: []
    id: 648730183dde5f34bbb33984
    type: comment
  author: therealcyberlord
  content: having the same issue as well
  created_at: 2023-06-12 13:47:52+00:00
  edited: false
  hidden: false
  id: 648730183dde5f34bbb33984
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/553954cbe49c4f92c04c674d9dc55729.svg
      fullname: Prasanta Panja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prasantapanja
      type: user
    createdAt: '2023-06-12T20:43:06.000Z'
    data:
      edited: true
      editors:
      - prasantapanja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8962505459785461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/553954cbe49c4f92c04c674d9dc55729.svg
          fullname: Prasanta Panja
          isHf: false
          isPro: false
          name: prasantapanja
          type: user
        html: '<p>Facing same issue. Running on Mac M1. Can it be due to low memory
          as I''m using 8 GB RAM?</p>

          '
        raw: Facing same issue. Running on Mac M1. Can it be due to low memory as
          I'm using 8 GB RAM?
        updatedAt: '2023-06-12T21:03:07.215Z'
      numEdits: 1
      reactions: []
    id: 6487835ade9f68b81bfd8285
    type: comment
  author: prasantapanja
  content: Facing same issue. Running on Mac M1. Can it be due to low memory as I'm
    using 8 GB RAM?
  created_at: 2023-06-12 19:43:06+00:00
  edited: true
  hidden: false
  id: 6487835ade9f68b81bfd8285
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/be4216f188baccc8bdfa7687ae030691.svg
      fullname: Manuel Rech
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: manuelrech
      type: user
    createdAt: '2023-06-13T11:07:40.000Z'
    data:
      edited: false
      editors:
      - manuelrech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.961325466632843
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/be4216f188baccc8bdfa7687ae030691.svg
          fullname: Manuel Rech
          isHf: false
          isPro: false
          name: manuelrech
          type: user
        html: '<p>Same issue here, I was using it smoothly and suddenly it threw this
          error, no changes, no upgrades, downgrades.</p>

          '
        raw: Same issue here, I was using it smoothly and suddenly it threw this error,
          no changes, no upgrades, downgrades.
        updatedAt: '2023-06-13T11:07:40.283Z'
      numEdits: 0
      reactions: []
    id: 64884dfc50acfbfbd4b2246f
    type: comment
  author: manuelrech
  content: Same issue here, I was using it smoothly and suddenly it threw this error,
    no changes, no upgrades, downgrades.
  created_at: 2023-06-13 10:07:40+00:00
  edited: false
  hidden: false
  id: 64884dfc50acfbfbd4b2246f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e68648f481bac0767ebf30d54ae75676.svg
      fullname: Samantha Silva
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nessian
      type: user
    createdAt: '2023-06-15T19:51:49.000Z'
    data:
      edited: false
      editors:
      - nessian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9941520094871521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e68648f481bac0767ebf30d54ae75676.svg
          fullname: Samantha Silva
          isHf: false
          isPro: false
          name: nessian
          type: user
        html: '<p>I am on Mac M1 as well and same issue</p>

          '
        raw: I am on Mac M1 as well and same issue
        updatedAt: '2023-06-15T19:51:49.721Z'
      numEdits: 0
      reactions: []
    id: 648b6bd55fc2cceee9e13558
    type: comment
  author: nessian
  content: I am on Mac M1 as well and same issue
  created_at: 2023-06-15 18:51:49+00:00
  edited: false
  hidden: false
  id: 648b6bd55fc2cceee9e13558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d200a5159f7d086f38438c50e29b4f73.svg
      fullname: Ibrahim Olalekan Alabi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ibrahim-Ola
      type: user
    createdAt: '2023-06-15T19:59:22.000Z'
    data:
      edited: false
      editors:
      - Ibrahim-Ola
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.522057056427002
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d200a5159f7d086f38438c50e29b4f73.svg
          fullname: Ibrahim Olalekan Alabi
          isHf: false
          isPro: false
          name: Ibrahim-Ola
          type: user
        html: '<blockquote>

          <p>I''m getting the following the error below when I try to load my model
          on Ubuntu and MacOS (i7, 2018)</p>

          <p><code>ValueError: Could not load model tiiuae/falcon-7b-instruct with
          any of the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</code></p>

          <p>Note: all my packages are up to date. My pipeline is:</p>

          <p>model = "tiiuae/falcon-7b-instruct" </p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model)</p>

          <p>pipeline = pipeline(<br>    task="text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map="auto",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id<br>)</p>

          <p>Any help is appreciated.</p>

          <p>Thanks!</p>

          </blockquote>

          <p>I found a way to make it work:</p>

          <pre><code>from transformers import AutoModelForCausalLM


          model_id="tiiuae/falcon-7b-instruct"

          tokenizer=AutoTokenizer.from_pretrained(model_id)

          model=AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)

          </code></pre>

          '
        raw: "> I'm getting the following the error below when I try to load my model\
          \ on Ubuntu and MacOS (i7, 2018)\n> \n> ```ValueError: Could not load model\
          \ tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```\n\
          > \n> Note: all my packages are up to date. My pipeline is:\n> \n> model\
          \ = \"tiiuae/falcon-7b-instruct\" \n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > \n> pipeline = pipeline(\n>     task=\"text-generation\", \n>     model=model,\n\
          >     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n\
          >     device_map=\"auto\",\n>     max_length=200,\n>     do_sample=True,\n\
          >     top_k=10,\n>     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id\n\
          > )\n> \n> Any help is appreciated.\n> \n> Thanks!\n\nI found a way to make\
          \ it work:\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel_id=\"\
          tiiuae/falcon-7b-instruct\"\ntokenizer=AutoTokenizer.from_pretrained(model_id)\n\
          model=AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n\
          ```"
        updatedAt: '2023-06-15T19:59:22.742Z'
      numEdits: 0
      reactions:
      - count: 17
        reaction: "\U0001F44D"
        users:
        - neoneye
        - amin-nejad
        - prasantapanja
        - clemdaniel
        - vjangale
        - randytorres
        - justinsvegliato
        - issacnitin
        - sidroy
        - dj38fj3ksj0
        - NavKau
        - itunscode
        - bmorrishome
        - OwOmeister
        - yearstarter
        - stephco
        - ZolS
      relatedEventId: 648b6d9a351b155f02e3a0a0
    id: 648b6d9a351b155f02e3a09f
    type: comment
  author: Ibrahim-Ola
  content: "> I'm getting the following the error below when I try to load my model\
    \ on Ubuntu and MacOS (i7, 2018)\n> \n> ```ValueError: Could not load model tiiuae/falcon-7b-instruct\
    \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```\n\
    > \n> Note: all my packages are up to date. My pipeline is:\n> \n> model = \"\
    tiiuae/falcon-7b-instruct\" \n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > \n> pipeline = pipeline(\n>     task=\"text-generation\", \n>     model=model,\n\
    >     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n\
    >     device_map=\"auto\",\n>     max_length=200,\n>     do_sample=True,\n>  \
    \   top_k=10,\n>     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id\n\
    > )\n> \n> Any help is appreciated.\n> \n> Thanks!\n\nI found a way to make it\
    \ work:\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel_id=\"tiiuae/falcon-7b-instruct\"\
    \ntokenizer=AutoTokenizer.from_pretrained(model_id)\nmodel=AutoModelForCausalLM.from_pretrained(model_id,\
    \ trust_remote_code=True)\n```"
  created_at: 2023-06-15 18:59:22+00:00
  edited: false
  hidden: false
  id: 648b6d9a351b155f02e3a09f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d200a5159f7d086f38438c50e29b4f73.svg
      fullname: Ibrahim Olalekan Alabi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ibrahim-Ola
      type: user
    createdAt: '2023-06-15T19:59:22.000Z'
    data:
      status: closed
    id: 648b6d9a351b155f02e3a0a0
    type: status-change
  author: Ibrahim-Ola
  created_at: 2023-06-15 18:59:22+00:00
  id: 648b6d9a351b155f02e3a0a0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626763753657-5f6ddf835e78cc6b0ed31e5d.jpeg?w=200&h=200&f=face
      fullname: Prasanna Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vpkprasanna
      type: user
    createdAt: '2023-08-10T08:15:20.000Z'
    data:
      edited: false
      editors:
      - vpkprasanna
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5603741407394409
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626763753657-5f6ddf835e78cc6b0ed31e5d.jpeg?w=200&h=200&f=face
          fullname: Prasanna Kumar V
          isHf: false
          isPro: false
          name: vpkprasanna
          type: user
        html: '<blockquote>

          <p>I''m getting the following the error below when I try to load my model
          on Ubuntu and MacOS (i7, 2018)</p>

          <p><code>ValueError: Could not load model tiiuae/falcon-7b-instruct with
          any of the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</code></p>

          <p>Note: all my packages are up to date. My pipeline is:</p>

          <p>model = "tiiuae/falcon-7b-instruct" </p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model)</p>

          <p>pipeline = pipeline(<br>    task="text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map="auto",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id<br>)</p>

          <p>Any help is appreciated.</p>

          <p>Thanks!<br>Even i got the same error but when i specify device manually
          as "cuda" in device_map parameter it starts to load the model . try this
          method once </p>

          </blockquote>

          '
        raw: "> I'm getting the following the error below when I try to load my model\
          \ on Ubuntu and MacOS (i7, 2018)\n> \n> ```ValueError: Could not load model\
          \ tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```\n\
          > \n> Note: all my packages are up to date. My pipeline is:\n> \n> model\
          \ = \"tiiuae/falcon-7b-instruct\" \n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > \n> pipeline = pipeline(\n>     task=\"text-generation\", \n>     model=model,\n\
          >     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n\
          >     device_map=\"auto\",\n>     max_length=200,\n>     do_sample=True,\n\
          >     top_k=10,\n>     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id\n\
          > )\n> \n> Any help is appreciated.\n> \n> Thanks!\nEven i got the same\
          \ error but when i specify device manually as \"cuda\" in device_map parameter\
          \ it starts to load the model . try this method once "
        updatedAt: '2023-08-10T08:15:20.984Z'
      numEdits: 0
      reactions: []
    id: 64d49c9888871f11f3ca2cee
    type: comment
  author: vpkprasanna
  content: "> I'm getting the following the error below when I try to load my model\
    \ on Ubuntu and MacOS (i7, 2018)\n> \n> ```ValueError: Could not load model tiiuae/falcon-7b-instruct\
    \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```\n\
    > \n> Note: all my packages are up to date. My pipeline is:\n> \n> model = \"\
    tiiuae/falcon-7b-instruct\" \n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > \n> pipeline = pipeline(\n>     task=\"text-generation\", \n>     model=model,\n\
    >     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n\
    >     device_map=\"auto\",\n>     max_length=200,\n>     do_sample=True,\n>  \
    \   top_k=10,\n>     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id\n\
    > )\n> \n> Any help is appreciated.\n> \n> Thanks!\nEven i got the same error\
    \ but when i specify device manually as \"cuda\" in device_map parameter it starts\
    \ to load the model . try this method once "
  created_at: 2023-08-10 07:15:20+00:00
  edited: false
  hidden: false
  id: 64d49c9888871f11f3ca2cee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Could not load Model
