!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ferraria
conflicting_files: null
created_at: 2023-08-01 17:05:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5efaadd729b69c5b512a089a5a81bcb.svg
      fullname: Faria Khandaker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ferraria
      type: user
    createdAt: '2023-08-01T18:05:19.000Z'
    data:
      edited: false
      editors:
      - Ferraria
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4934123456478119
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5efaadd729b69c5b512a089a5a81bcb.svg
          fullname: Faria Khandaker
          isHf: false
          isPro: false
          name: Ferraria
          type: user
        html: "<p>Hi There,</p>\n<p>I tried to use both chunks of code from the falcon-7b-instruct\
          \ model card but both code chunks return  a \"no space left on device\"\
          \ error message. I am using a virtual machine with 128GB of ram so i don't\
          \ understand why this would be happening. </p>\n<p>the code chunks i used:</p>\n\
          <pre><code># Use a pipeline as a high-level helper\nfrom transformers import\
          \ pipeline\n\npipe = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\"\
          , trust_remote_code=True)\n</code></pre>\n<p>and </p>\n<pre><code># Load\
          \ model directly\nfrom transformers import AutoModelForCausalLM\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n\
          </code></pre>\n<p>can someone please point me in the right direction for\
          \ resolving this error?</p>\n<p>Thank you!</p>\n<p>this is the full error\
          \ message</p>\n<pre><code>---------------------------------------------------------------------------\n\
          OSError                                   Traceback (most recent call last)\n\
          Cell In[2], line 3\n      1 # Load model directly\n      2 from transformers\
          \ import AutoModelForCausalLM\n----&gt; 3 model = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n\nFile c:\\Users\\\
          gg\\Documents\\generative_ai\\genai\\lib\\site-packages\\transformers\\\
          models\\auto\\auto_factory.py:488, in _BaseAutoModelClass.from_pretrained(cls,\
          \ pretrained_model_name_or_path, *model_args, **kwargs)\n    486     else:\n\
          \    487         cls.register(config.__class__, model_class, exist_ok=True)\n\
          --&gt; 488     return model_class.from_pretrained(\n    489         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    490     )\n  \
          \  491 elif type(config) in cls._model_mapping.keys():\n    492     model_class\
          \ = _get_model_class(config, cls._model_mapping)\n\nFile c:\\Users\\gg\\\
          Documents\\generative_ai\\genai\\lib\\site-packages\\transformers\\modeling_utils.py:2610,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   2607 # We'll\
          \ need to download and cache each checkpoint shard if the checkpoint is\
          \ sharded.\n   2608 if is_sharded:\n   2609     # rsolved_archive_file becomes\
          \ a list of files that point to the different checkpoint shards in this\
          \ case.\n-&gt; 2610     resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\
          \   2611         pretrained_model_name_or_path,\n   2612         resolved_archive_file,\n\
          \   2613         cache_dir=cache_dir,\n   2614         force_download=force_download,\n\
          \   2615         proxies=proxies,\n   2616         resume_download=resume_download,\n\
          \   2617         local_files_only=local_files_only,\n   2618         use_auth_token=token,\n\
          \   2619         user_agent=user_agent,\n   2620         revision=revision,\n\
          \   2621         subfolder=subfolder,\n   2622         _commit_hash=commit_hash,\n\
          \   2623     )\n   2625 # load pt weights early so that we know which dtype\
          \ to init the model under\n   2626 if from_pt:\n\nFile c:\\Users\\gg\\Documents\\\
          generative_ai\\genai\\lib\\site-packages\\transformers\\utils\\hub.py:958,\
          \ in get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename,\
          \ cache_dir, force_download, proxies, resume_download, local_files_only,\
          \ use_auth_token, user_agent, revision, subfolder, _commit_hash)\n    955\
          \ for shard_filename in tqdm(shard_filenames, desc=\"Downloading shards\"\
          , disable=not show_progress_bar):\n    956     try:\n    957         # Load\
          \ from URL\n--&gt; 958         cached_filename = cached_file(\n    959 \
          \            pretrained_model_name_or_path,\n    960             shard_filename,\n\
          \    961             cache_dir=cache_dir,\n...\n    481 @_functools.wraps(func)\n\
          \    482 def func_wrapper(*args, **kwargs):\n--&gt; 483     return func(*args,\
          \ **kwargs)\n\nOSError: [Errno 28] No space left on device\n\n</code></pre>\n"
        raw: "Hi There,\r\n\r\nI tried to use both chunks of code from the falcon-7b-instruct\
          \ model card but both code chunks return  a \"no space left on device\"\
          \ error message. I am using a virtual machine with 128GB of ram so i don't\
          \ understand why this would be happening. \r\n\r\nthe code chunks i used:\r\
          \n```\r\n# Use a pipeline as a high-level helper\r\nfrom transformers import\
          \ pipeline\r\n\r\npipe = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\"\
          , trust_remote_code=True)\r\n```\r\nand \r\n\r\n```\r\n# Load model directly\r\
          \nfrom transformers import AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-7b-instruct\", trust_remote_code=True)\r\n\r\n```\r\ncan someone\
          \ please point me in the right direction for resolving this error?\r\n\r\
          \nThank you!\r\n\r\nthis is the full error message\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
          \nOSError                                   Traceback (most recent call\
          \ last)\r\nCell In[2], line 3\r\n      1 # Load model directly\r\n     \
          \ 2 from transformers import AutoModelForCausalLM\r\n----> 3 model = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-7b-instruct\", trust_remote_code=True)\r\n\r\nFile c:\\Users\\\
          gg\\Documents\\generative_ai\\genai\\lib\\site-packages\\transformers\\\
          models\\auto\\auto_factory.py:488, in _BaseAutoModelClass.from_pretrained(cls,\
          \ pretrained_model_name_or_path, *model_args, **kwargs)\r\n    486     else:\r\
          \n    487         cls.register(config.__class__, model_class, exist_ok=True)\r\
          \n--> 488     return model_class.from_pretrained(\r\n    489         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\r\n    490     )\r\n\
          \    491 elif type(config) in cls._model_mapping.keys():\r\n    492    \
          \ model_class = _get_model_class(config, cls._model_mapping)\r\n\r\nFile\
          \ c:\\Users\\gg\\Documents\\generative_ai\\genai\\lib\\site-packages\\transformers\\\
          modeling_utils.py:2610, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\r\n   2607 #\
          \ We'll need to download and cache each checkpoint shard if the checkpoint\
          \ is sharded.\r\n   2608 if is_sharded:\r\n   2609     # rsolved_archive_file\
          \ becomes a list of files that point to the different checkpoint shards\
          \ in this case.\r\n-> 2610     resolved_archive_file, sharded_metadata =\
          \ get_checkpoint_shard_files(\r\n   2611         pretrained_model_name_or_path,\r\
          \n   2612         resolved_archive_file,\r\n   2613         cache_dir=cache_dir,\r\
          \n   2614         force_download=force_download,\r\n   2615         proxies=proxies,\r\
          \n   2616         resume_download=resume_download,\r\n   2617         local_files_only=local_files_only,\r\
          \n   2618         use_auth_token=token,\r\n   2619         user_agent=user_agent,\r\
          \n   2620         revision=revision,\r\n   2621         subfolder=subfolder,\r\
          \n   2622         _commit_hash=commit_hash,\r\n   2623     )\r\n   2625\
          \ # load pt weights early so that we know which dtype to init the model\
          \ under\r\n   2626 if from_pt:\r\n\r\nFile c:\\Users\\gg\\Documents\\generative_ai\\\
          genai\\lib\\site-packages\\transformers\\utils\\hub.py:958, in get_checkpoint_shard_files(pretrained_model_name_or_path,\
          \ index_filename, cache_dir, force_download, proxies, resume_download, local_files_only,\
          \ use_auth_token, user_agent, revision, subfolder, _commit_hash)\r\n   \
          \ 955 for shard_filename in tqdm(shard_filenames, desc=\"Downloading shards\"\
          , disable=not show_progress_bar):\r\n    956     try:\r\n    957       \
          \  # Load from URL\r\n--> 958         cached_filename = cached_file(\r\n\
          \    959             pretrained_model_name_or_path,\r\n    960         \
          \    shard_filename,\r\n    961             cache_dir=cache_dir,\r\n...\r\
          \n    481 @_functools.wraps(func)\r\n    482 def func_wrapper(*args, **kwargs):\r\
          \n--> 483     return func(*args, **kwargs)\r\n\r\nOSError: [Errno 28] No\
          \ space left on device\r\n\r\n\r\n```"
        updatedAt: '2023-08-01T18:05:19.041Z'
      numEdits: 0
      reactions: []
    id: 64c9495f4524c2aea71f4df3
    type: comment
  author: Ferraria
  content: "Hi There,\r\n\r\nI tried to use both chunks of code from the falcon-7b-instruct\
    \ model card but both code chunks return  a \"no space left on device\" error\
    \ message. I am using a virtual machine with 128GB of ram so i don't understand\
    \ why this would be happening. \r\n\r\nthe code chunks i used:\r\n```\r\n# Use\
    \ a pipeline as a high-level helper\r\nfrom transformers import pipeline\r\n\r\
    \npipe = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\r\
    \n```\r\nand \r\n\r\n```\r\n# Load model directly\r\nfrom transformers import\
    \ AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\"\
    , trust_remote_code=True)\r\n\r\n```\r\ncan someone please point me in the right\
    \ direction for resolving this error?\r\n\r\nThank you!\r\n\r\nthis is the full\
    \ error message\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
    \nOSError                                   Traceback (most recent call last)\r\
    \nCell In[2], line 3\r\n      1 # Load model directly\r\n      2 from transformers\
    \ import AutoModelForCausalLM\r\n----> 3 model = AutoModelForCausalLM.from_pretrained(\"\
    tiiuae/falcon-7b-instruct\", trust_remote_code=True)\r\n\r\nFile c:\\Users\\gg\\\
    Documents\\generative_ai\\genai\\lib\\site-packages\\transformers\\models\\auto\\\
    auto_factory.py:488, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
    \ *model_args, **kwargs)\r\n    486     else:\r\n    487         cls.register(config.__class__,\
    \ model_class, exist_ok=True)\r\n--> 488     return model_class.from_pretrained(\r\
    \n    489         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \ **kwargs\r\n    490     )\r\n    491 elif type(config) in cls._model_mapping.keys():\r\
    \n    492     model_class = _get_model_class(config, cls._model_mapping)\r\n\r\
    \nFile c:\\Users\\gg\\Documents\\generative_ai\\genai\\lib\\site-packages\\transformers\\\
    modeling_utils.py:2610, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
    \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
    \ token, revision, use_safetensors, *model_args, **kwargs)\r\n   2607 # We'll\
    \ need to download and cache each checkpoint shard if the checkpoint is sharded.\r\
    \n   2608 if is_sharded:\r\n   2609     # rsolved_archive_file becomes a list\
    \ of files that point to the different checkpoint shards in this case.\r\n-> 2610\
    \     resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\r\n\
    \   2611         pretrained_model_name_or_path,\r\n   2612         resolved_archive_file,\r\
    \n   2613         cache_dir=cache_dir,\r\n   2614         force_download=force_download,\r\
    \n   2615         proxies=proxies,\r\n   2616         resume_download=resume_download,\r\
    \n   2617         local_files_only=local_files_only,\r\n   2618         use_auth_token=token,\r\
    \n   2619         user_agent=user_agent,\r\n   2620         revision=revision,\r\
    \n   2621         subfolder=subfolder,\r\n   2622         _commit_hash=commit_hash,\r\
    \n   2623     )\r\n   2625 # load pt weights early so that we know which dtype\
    \ to init the model under\r\n   2626 if from_pt:\r\n\r\nFile c:\\Users\\gg\\Documents\\\
    generative_ai\\genai\\lib\\site-packages\\transformers\\utils\\hub.py:958, in\
    \ get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename, cache_dir,\
    \ force_download, proxies, resume_download, local_files_only, use_auth_token,\
    \ user_agent, revision, subfolder, _commit_hash)\r\n    955 for shard_filename\
    \ in tqdm(shard_filenames, desc=\"Downloading shards\", disable=not show_progress_bar):\r\
    \n    956     try:\r\n    957         # Load from URL\r\n--> 958         cached_filename\
    \ = cached_file(\r\n    959             pretrained_model_name_or_path,\r\n   \
    \ 960             shard_filename,\r\n    961             cache_dir=cache_dir,\r\
    \n...\r\n    481 @_functools.wraps(func)\r\n    482 def func_wrapper(*args, **kwargs):\r\
    \n--> 483     return func(*args, **kwargs)\r\n\r\nOSError: [Errno 28] No space\
    \ left on device\r\n\r\n\r\n```"
  created_at: 2023-08-01 17:05:19+00:00
  edited: false
  hidden: false
  id: 64c9495f4524c2aea71f4df3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5efaadd729b69c5b512a089a5a81bcb.svg
      fullname: Faria Khandaker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ferraria
      type: user
    createdAt: '2023-08-03T17:48:30.000Z'
    data:
      edited: false
      editors:
      - Ferraria
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9925386905670166
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5efaadd729b69c5b512a089a5a81bcb.svg
          fullname: Faria Khandaker
          isHf: false
          isPro: false
          name: Ferraria
          type: user
        html: '<p>i just saw that the actual memory of my VM was less than what i
          assumed. it had nothing to do with RAM</p>

          '
        raw: i just saw that the actual memory of my VM was less than what i assumed.
          it had nothing to do with RAM
        updatedAt: '2023-08-03T17:48:30.869Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64cbe86e00377e2848878ff8
    id: 64cbe86e00377e2848878ff5
    type: comment
  author: Ferraria
  content: i just saw that the actual memory of my VM was less than what i assumed.
    it had nothing to do with RAM
  created_at: 2023-08-03 16:48:30+00:00
  edited: false
  hidden: false
  id: 64cbe86e00377e2848878ff5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d5efaadd729b69c5b512a089a5a81bcb.svg
      fullname: Faria Khandaker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ferraria
      type: user
    createdAt: '2023-08-03T17:48:30.000Z'
    data:
      status: closed
    id: 64cbe86e00377e2848878ff8
    type: status-change
  author: Ferraria
  created_at: 2023-08-03 16:48:30+00:00
  id: 64cbe86e00377e2848878ff8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 72
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: closed
target_branch: null
title: no space left on device error for falcon-7b-instruct
