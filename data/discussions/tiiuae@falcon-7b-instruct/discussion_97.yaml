!!python/object:huggingface_hub.community.DiscussionWithDetails
author: airedwin
conflicting_files: null
created_at: 2023-11-01 19:23:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/584422349730e73cd8607fe861765708.svg
      fullname: Edwin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: airedwin
      type: user
    createdAt: '2023-11-01T20:23:19.000Z'
    data:
      edited: false
      editors:
      - airedwin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5118892192840576
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/584422349730e73cd8607fe861765708.svg
          fullname: Edwin
          isHf: false
          isPro: false
          name: airedwin
          type: user
        html: "<pre><code>def load(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
          \        model_8bit = AutoModelForCausalLM.from_pretrained(\n          \
          \  MODEL_NAME,\n            device_map=\"auto\",\n            load_in_8bit=True,\n\
          \            trust_remote_code=True)\n\n        self.pipeline = pipeline(\n\
          \            \"text-generation\",\n            model=model_8bit,\n     \
          \       tokenizer=self.tokenizer,\n            torch_dtype=torch.bfloat16,\n\
          \            trust_remote_code=True,\n            device_map=\"auto\",\n\
          \        )\n\ndef predict(self, request: Dict) -&gt; Dict:\n        with\
          \ torch.no_grad():\n            try:\n                prompt = request.pop(\"\
          prompt\")\n                data = self.pipeline(\n                    prompt,\n\
          \                    eos_token_id=self.tokenizer.eos_token_id,\n       \
          \             max_length=DEFAULT_MAX_LENGTH,\n                    **request\n\
          \                )[0]\n                return {\"data\": data}\n</code></pre>\n\
          <p>I am using this code to get an answer after a prompt. It works locally\
          \ when I run this model in a docker container on a NVIDIA RTX GPU. When\
          \ I deploy my container to an AWS instance p3.2xlarge which uses NVIDIA\
          \ TESLA V100 GPU, the generated_text it returns is empty. I am also using\
          \ a local copy of the falcon-7b-instruct snapshot with TRANSFORMERS_OFFLINE=1.</p>\n\
          <p>Any ideas why this works local but not on an EC2 instance?</p>\n"
        raw: "```\r\ndef load(self):\r\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\
          \n        model_8bit = AutoModelForCausalLM.from_pretrained(\r\n       \
          \     MODEL_NAME,\r\n            device_map=\"auto\",\r\n            load_in_8bit=True,\r\
          \n            trust_remote_code=True)\r\n\r\n        self.pipeline = pipeline(\r\
          \n            \"text-generation\",\r\n            model=model_8bit,\r\n\
          \            tokenizer=self.tokenizer,\r\n            torch_dtype=torch.bfloat16,\r\
          \n            trust_remote_code=True,\r\n            device_map=\"auto\"\
          ,\r\n        )\r\n\r\ndef predict(self, request: Dict) -> Dict:\r\n    \
          \    with torch.no_grad():\r\n            try:\r\n                prompt\
          \ = request.pop(\"prompt\")\r\n                data = self.pipeline(\r\n\
          \                    prompt,\r\n                    eos_token_id=self.tokenizer.eos_token_id,\r\
          \n                    max_length=DEFAULT_MAX_LENGTH,\r\n               \
          \     **request\r\n                )[0]\r\n                return {\"data\"\
          : data}\r\n\r\n```\r\n\r\nI am using this code to get an answer after a\
          \ prompt. It works locally when I run this model in a docker container on\
          \ a NVIDIA RTX GPU. When I deploy my container to an AWS instance p3.2xlarge\
          \ which uses NVIDIA TESLA V100 GPU, the generated_text it returns is empty.\
          \ I am also using a local copy of the falcon-7b-instruct snapshot with TRANSFORMERS_OFFLINE=1.\r\
          \n\r\nAny ideas why this works local but not on an EC2 instance?"
        updatedAt: '2023-11-01T20:23:19.204Z'
      numEdits: 0
      reactions: []
    id: 6542b3b7af8e15b8dbedb248
    type: comment
  author: airedwin
  content: "```\r\ndef load(self):\r\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\
    \n        model_8bit = AutoModelForCausalLM.from_pretrained(\r\n            MODEL_NAME,\r\
    \n            device_map=\"auto\",\r\n            load_in_8bit=True,\r\n     \
    \       trust_remote_code=True)\r\n\r\n        self.pipeline = pipeline(\r\n \
    \           \"text-generation\",\r\n            model=model_8bit,\r\n        \
    \    tokenizer=self.tokenizer,\r\n            torch_dtype=torch.bfloat16,\r\n\
    \            trust_remote_code=True,\r\n            device_map=\"auto\",\r\n \
    \       )\r\n\r\ndef predict(self, request: Dict) -> Dict:\r\n        with torch.no_grad():\r\
    \n            try:\r\n                prompt = request.pop(\"prompt\")\r\n   \
    \             data = self.pipeline(\r\n                    prompt,\r\n       \
    \             eos_token_id=self.tokenizer.eos_token_id,\r\n                  \
    \  max_length=DEFAULT_MAX_LENGTH,\r\n                    **request\r\n       \
    \         )[0]\r\n                return {\"data\": data}\r\n\r\n```\r\n\r\nI\
    \ am using this code to get an answer after a prompt. It works locally when I\
    \ run this model in a docker container on a NVIDIA RTX GPU. When I deploy my container\
    \ to an AWS instance p3.2xlarge which uses NVIDIA TESLA V100 GPU, the generated_text\
    \ it returns is empty. I am also using a local copy of the falcon-7b-instruct\
    \ snapshot with TRANSFORMERS_OFFLINE=1.\r\n\r\nAny ideas why this works local\
    \ but not on an EC2 instance?"
  created_at: 2023-11-01 19:23:19+00:00
  edited: false
  hidden: false
  id: 6542b3b7af8e15b8dbedb248
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 97
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: model not generating text
