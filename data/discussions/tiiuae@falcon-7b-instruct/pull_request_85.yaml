!!python/object:huggingface_hub.community.DiscussionWithDetails
author: matthewmrichter
conflicting_files: []
created_at: 2023-09-15 19:14:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6e5798964d1a06c34f0674c0a46e7fa.svg
      fullname: Matt Richter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthewmrichter
      type: user
    createdAt: '2023-09-15T20:14:57.000Z'
    data:
      edited: false
      editors:
      - matthewmrichter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6228694319725037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6e5798964d1a06c34f0674c0a46e7fa.svg
          fullname: Matt Richter
          isHf: false
          isPro: false
          name: matthewmrichter
          type: user
        html: "<p>I'm trying to deploy this model into AWS SageMaker. Per this link\
          \ (<a rel=\"nofollow\" href=\"https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers\"\
          >https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers</a>)\
          \ using this model image:<br>763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04</p>\n\
          <p>Cloudwatch shows this error upon launch of the model, before even invoking\
          \ the endpoint: </p>\n<pre><code>W-9000-tiiuae__falcon-7b-instruc-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\n</code></pre>\n\
          <p>Similarly when I try and invoke the endpoint via python/boto it confirms\
          \ the issue:</p>\n<pre><code>Traceback (most recent call last):\n  File\
          \ \"/../sagemaker.py\", line 44, in &lt;module&gt;\n    main()\n  File \"\
          /../sagemaker.py\", line 39, in main\n    response = predict_data(sagemaker_runtime,\
          \ endpoint_name, request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/../sagemaker.py\", line 7, in predict_data\n    response = sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,\n\
          \               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/opt/homebrew/lib/python3.11/site-packages/botocore/client.py\"\
          , line 535, in _api_call\n    return self._make_api_call(operation_name,\
          \ kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"/opt/homebrew/lib/python3.11/site-packages/botocore/client.py\", line\
          \ 980, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n\
          botocore.errorfactory.ModelError: An error occurred (ModelError) when calling\
          \ the InvokeEndpoint operation: Received client error (400) from primary\
          \ with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code\\u003dTrue` to remove this error.\"\
          \n}\n</code></pre>\n<p>Here's some of the Terraform I'm using to configure\
          \ the AWS resources for this:</p>\n<pre><code># module.huggingface_sagemaker_falcon.data.aws_sagemaker_prebuilt_ecr_image.deploy_image:\n\
          data \"aws_sagemaker_prebuilt_ecr_image\" \"deploy_image\" {\n  image_tag\
          \       = \"2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\"\n  repository_name\
          \ = \"huggingface-pytorch-inference\"\n}\n\n# module.huggingface_sagemaker_falcon.aws_sagemaker_model.model_with_hub_model[0]:\n\
          resource \"aws_sagemaker_model\" \"model_with_hub_model\" {\n  enable_network_isolation\
          \ = false\n  execution_role_arn       = aws_iam_role.new_role.arn\n  name\
          \                     = \"falcon-model\"\n\n  primary_container {\n    environment\
          \ = {\n      \"HF_MODEL_ID\"          = \"tiiuae/falcon-7b-instruct\"\n\
          \      \"HF_TASK\"              = \"text-generation\"\n      \"HF_TRUST_REMOTE_CODE\"\
          \ = \"True\"\n      \"HF_MODEL_REVISION\"    = \"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\"\
          \n    }\n    image = data.aws_sagemaker_prebuilt_ecr_image.deploy_image.registry_path\n\
          \    mode  = \"SingleModel\"\n  }\n}\n</code></pre>\n<p>Here's my Python:</p>\n\
          <pre><code>import boto3\nimport json\n\n\ndef predict_data(sagemaker_runtime,\
          \ endpoint_name, input_data):\n    json_input_data = json.dumps(input_data).encode('utf-8')\n\
          \    response = sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,\n\
          \                                                 ContentType='application/json',\n\
          \                                                 Body=json_input_data)\n\
          \    return response\n\n\ndef main():\n    region_name = 'us-east-2'\n \
          \   session = boto3.Session(region_name=region_name)\n    sagemaker_runtime\
          \ = session.client('sagemaker-runtime')\n\n    endpoint_name = '&lt;endpoint\
          \ name&gt;'\n\n    # define prompt\n    prompt = \"\"\"You are the most\
          \ advanced AI assistant on the planet, called Falcon.\n\n    User: How can\
          \ we set up Kubernetes cluster on AWS? Think step by step.\n    Falcon:\"\
          \"\"\n\n    # hyperparameters for llm\n    request = {\n        \"inputs\"\
          : prompt\n    }\n\n    response = predict_data(sagemaker_runtime, endpoint_name,\
          \ request)\n    print(response)\n\n\nif __name__ == \"__main__\":\n    main()\n\
          </code></pre>\n<p>Seems that the config.json needs this added? From the\
          \ AWS side, I couldn't find a way to configure the model, endpoint configuration,\
          \ or endpoint resources to override that.</p>\n<p>Also making the assumption\
          \ that true is treated like a bool in that configuration and the capitalization\
          \ <code>true</code> vs <code>True</code> does not matter.</p>\n<p>I'm very\
          \ much an ML novice so if this is a security concern or if there is in fact\
          \ a way to configure our AWS Sagemaker resources, or my python request,\
          \ to trust remote code, or if I am completely in the wrong stratosphere\
          \ as to how this all works, please feel free to reject this and let me know.</p>\n"
        raw: "I'm trying to deploy this model into AWS SageMaker. Per this link (https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers)\
          \ using this model image:\n763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n\
          \nCloudwatch shows this error upon launch of the model, before even invoking\
          \ the endpoint: \n\n```\nW-9000-tiiuae__falcon-7b-instruc-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ - Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\n```\n\
          \nSimilarly when I try and invoke the endpoint via python/boto it confirms\
          \ the issue:\n\n```\nTraceback (most recent call last):\n  File \"/../sagemaker.py\"\
          , line 44, in <module>\n    main()\n  File \"/../sagemaker.py\", line 39,\
          \ in main\n    response = predict_data(sagemaker_runtime, endpoint_name,\
          \ request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/../sagemaker.py\", line 7, in predict_data\n    response = sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,\n\
          \               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/opt/homebrew/lib/python3.11/site-packages/botocore/client.py\"\
          , line 535, in _api_call\n    return self._make_api_call(operation_name,\
          \ kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"/opt/homebrew/lib/python3.11/site-packages/botocore/client.py\", line\
          \ 980, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n\
          botocore.errorfactory.ModelError: An error occurred (ModelError) when calling\
          \ the InvokeEndpoint operation: Received client error (400) from primary\
          \ with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code\\u003dTrue` to remove this error.\"\
          \n}\n```\n\nHere's some of the Terraform I'm using to configure the AWS\
          \ resources for this:\n\n```\n# module.huggingface_sagemaker_falcon.data.aws_sagemaker_prebuilt_ecr_image.deploy_image:\n\
          data \"aws_sagemaker_prebuilt_ecr_image\" \"deploy_image\" {\n  image_tag\
          \       = \"2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\"\n  repository_name\
          \ = \"huggingface-pytorch-inference\"\n}\n\n# module.huggingface_sagemaker_falcon.aws_sagemaker_model.model_with_hub_model[0]:\n\
          resource \"aws_sagemaker_model\" \"model_with_hub_model\" {\n  enable_network_isolation\
          \ = false\n  execution_role_arn       = aws_iam_role.new_role.arn\n  name\
          \                     = \"falcon-model\"\n\n  primary_container {\n    environment\
          \ = {\n      \"HF_MODEL_ID\"          = \"tiiuae/falcon-7b-instruct\"\n\
          \      \"HF_TASK\"              = \"text-generation\"\n      \"HF_TRUST_REMOTE_CODE\"\
          \ = \"True\"\n      \"HF_MODEL_REVISION\"    = \"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\"\
          \n    }\n    image = data.aws_sagemaker_prebuilt_ecr_image.deploy_image.registry_path\n\
          \    mode  = \"SingleModel\"\n  }\n}\n```\n\nHere's my Python:\n\n```\n\
          import boto3\nimport json\n\n\ndef predict_data(sagemaker_runtime, endpoint_name,\
          \ input_data):\n    json_input_data = json.dumps(input_data).encode('utf-8')\n\
          \    response = sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,\n\
          \                                                 ContentType='application/json',\n\
          \                                                 Body=json_input_data)\n\
          \    return response\n\n\ndef main():\n    region_name = 'us-east-2'\n \
          \   session = boto3.Session(region_name=region_name)\n    sagemaker_runtime\
          \ = session.client('sagemaker-runtime')\n\n    endpoint_name = '<endpoint\
          \ name>'\n\n    # define prompt\n    prompt = \"\"\"You are the most advanced\
          \ AI assistant on the planet, called Falcon.\n\n    User: How can we set\
          \ up Kubernetes cluster on AWS? Think step by step.\n    Falcon:\"\"\"\n\
          \n    # hyperparameters for llm\n    request = {\n        \"inputs\": prompt\n\
          \    }\n\n    response = predict_data(sagemaker_runtime, endpoint_name,\
          \ request)\n    print(response)\n\n\nif __name__ == \"__main__\":\n    main()\n\
          ```\n\nSeems that the config.json needs this added? From the AWS side, I\
          \ couldn't find a way to configure the model, endpoint configuration, or\
          \ endpoint resources to override that.\n\nAlso making the assumption that\
          \ true is treated like a bool in that configuration and the capitalization\
          \ `true` vs `True` does not matter.\n\nI'm very much an ML novice so if\
          \ this is a security concern or if there is in fact a way to configure our\
          \ AWS Sagemaker resources, or my python request, to trust remote code, or\
          \ if I am completely in the wrong stratosphere as to how this all works,\
          \ please feel free to reject this and let me know."
        updatedAt: '2023-09-15T20:14:57.349Z'
      numEdits: 0
      reactions: []
    id: 6504bb41d3219dc63c5478bb
    type: comment
  author: matthewmrichter
  content: "I'm trying to deploy this model into AWS SageMaker. Per this link (https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers)\
    \ using this model image:\n763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n\
    \nCloudwatch shows this error upon launch of the model, before even invoking the\
    \ endpoint: \n\n```\nW-9000-tiiuae__falcon-7b-instruc-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ - Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\
    \ requires you to execute the configuration file in that repo on your local machine.\
    \ Make sure you have read the code there to avoid malicious use, then set the\
    \ option `trust_remote_code=True` to remove this error.\n```\n\nSimilarly when\
    \ I try and invoke the endpoint via python/boto it confirms the issue:\n\n```\n\
    Traceback (most recent call last):\n  File \"/../sagemaker.py\", line 44, in <module>\n\
    \    main()\n  File \"/../sagemaker.py\", line 39, in main\n    response = predict_data(sagemaker_runtime,\
    \ endpoint_name, request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/../sagemaker.py\", line 7, in predict_data\n    response = sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,\n\
    \               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/opt/homebrew/lib/python3.11/site-packages/botocore/client.py\", line\
    \ 535, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/botocore/client.py\"\
    , line 980, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n\
    botocore.errorfactory.ModelError: An error occurred (ModelError) when calling\
    \ the InvokeEndpoint operation: Received client error (400) from primary with\
    \ message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"\
    message\": \"Loading /.sagemaker/mms/models/tiiuae__falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\
    \ requires you to execute the configuration file in that repo on your local machine.\
    \ Make sure you have read the code there to avoid malicious use, then set the\
    \ option `trust_remote_code\\u003dTrue` to remove this error.\"\n}\n```\n\nHere's\
    \ some of the Terraform I'm using to configure the AWS resources for this:\n\n\
    ```\n# module.huggingface_sagemaker_falcon.data.aws_sagemaker_prebuilt_ecr_image.deploy_image:\n\
    data \"aws_sagemaker_prebuilt_ecr_image\" \"deploy_image\" {\n  image_tag    \
    \   = \"2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\"\n  repository_name\
    \ = \"huggingface-pytorch-inference\"\n}\n\n# module.huggingface_sagemaker_falcon.aws_sagemaker_model.model_with_hub_model[0]:\n\
    resource \"aws_sagemaker_model\" \"model_with_hub_model\" {\n  enable_network_isolation\
    \ = false\n  execution_role_arn       = aws_iam_role.new_role.arn\n  name    \
    \                 = \"falcon-model\"\n\n  primary_container {\n    environment\
    \ = {\n      \"HF_MODEL_ID\"          = \"tiiuae/falcon-7b-instruct\"\n      \"\
    HF_TASK\"              = \"text-generation\"\n      \"HF_TRUST_REMOTE_CODE\" =\
    \ \"True\"\n      \"HF_MODEL_REVISION\"    = \"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\"\
    \n    }\n    image = data.aws_sagemaker_prebuilt_ecr_image.deploy_image.registry_path\n\
    \    mode  = \"SingleModel\"\n  }\n}\n```\n\nHere's my Python:\n\n```\nimport\
    \ boto3\nimport json\n\n\ndef predict_data(sagemaker_runtime, endpoint_name, input_data):\n\
    \    json_input_data = json.dumps(input_data).encode('utf-8')\n    response =\
    \ sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,\n            \
    \                                     ContentType='application/json',\n      \
    \                                           Body=json_input_data)\n    return\
    \ response\n\n\ndef main():\n    region_name = 'us-east-2'\n    session = boto3.Session(region_name=region_name)\n\
    \    sagemaker_runtime = session.client('sagemaker-runtime')\n\n    endpoint_name\
    \ = '<endpoint name>'\n\n    # define prompt\n    prompt = \"\"\"You are the most\
    \ advanced AI assistant on the planet, called Falcon.\n\n    User: How can we\
    \ set up Kubernetes cluster on AWS? Think step by step.\n    Falcon:\"\"\"\n\n\
    \    # hyperparameters for llm\n    request = {\n        \"inputs\": prompt\n\
    \    }\n\n    response = predict_data(sagemaker_runtime, endpoint_name, request)\n\
    \    print(response)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nSeems\
    \ that the config.json needs this added? From the AWS side, I couldn't find a\
    \ way to configure the model, endpoint configuration, or endpoint resources to\
    \ override that.\n\nAlso making the assumption that true is treated like a bool\
    \ in that configuration and the capitalization `true` vs `True` does not matter.\n\
    \nI'm very much an ML novice so if this is a security concern or if there is in\
    \ fact a way to configure our AWS Sagemaker resources, or my python request, to\
    \ trust remote code, or if I am completely in the wrong stratosphere as to how\
    \ this all works, please feel free to reject this and let me know."
  created_at: 2023-09-15 19:14:57+00:00
  edited: false
  hidden: false
  id: 6504bb41d3219dc63c5478bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/a6e5798964d1a06c34f0674c0a46e7fa.svg
      fullname: Matt Richter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthewmrichter
      type: user
    createdAt: '2023-09-15T20:14:58.000Z'
    data:
      oid: 777a465507c47b7c7377c6bff3fb783ee81dd787
      parents:
      - eb410fb6ffa9028e97adb801f0d6ec46d02f8b07
      subject: Trust remote code for SageMaker execution
    id: 6504bb420000000000000000
    type: commit
  author: matthewmrichter
  created_at: 2023-09-15 19:14:58+00:00
  id: 6504bb420000000000000000
  oid: 777a465507c47b7c7377c6bff3fb783ee81dd787
  summary: Trust remote code for SageMaker execution
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6e5798964d1a06c34f0674c0a46e7fa.svg
      fullname: Matt Richter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthewmrichter
      type: user
    createdAt: '2023-09-18T19:07:21.000Z'
    data:
      edited: false
      editors:
      - matthewmrichter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9422048926353455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6e5798964d1a06c34f0674c0a46e7fa.svg
          fullname: Matt Richter
          isHf: false
          isPro: false
          name: matthewmrichter
          type: user
        html: '<p>I''m closing this. I figured out how to test, and this does not
          actually accomplish what I was looking to do. For those curious, the key
          was to run the model on a text-generation specific pre-baked image: <code>763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.0.3-gpu-py39-cu118-ubuntu20.04</code>.</p>

          '
        raw: 'I''m closing this. I figured out how to test, and this does not actually
          accomplish what I was looking to do. For those curious, the key was to run
          the model on a text-generation specific pre-baked image: `763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.0.3-gpu-py39-cu118-ubuntu20.04`.'
        updatedAt: '2023-09-18T19:07:21.301Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65089fe95b34509e16c4b550
    id: 65089fe95b34509e16c4b54f
    type: comment
  author: matthewmrichter
  content: 'I''m closing this. I figured out how to test, and this does not actually
    accomplish what I was looking to do. For those curious, the key was to run the
    model on a text-generation specific pre-baked image: `763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.0.3-gpu-py39-cu118-ubuntu20.04`.'
  created_at: 2023-09-18 18:07:21+00:00
  edited: false
  hidden: false
  id: 65089fe95b34509e16c4b54f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a6e5798964d1a06c34f0674c0a46e7fa.svg
      fullname: Matt Richter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthewmrichter
      type: user
    createdAt: '2023-09-18T19:07:21.000Z'
    data:
      status: closed
    id: 65089fe95b34509e16c4b550
    type: status-change
  author: matthewmrichter
  created_at: 2023-09-18 18:07:21+00:00
  id: 65089fe95b34509e16c4b550
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 85
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: closed
target_branch: refs/heads/main
title: Trust remote code for SageMaker execution
