!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kycrowe
conflicting_files: null
created_at: 2023-06-02 20:48:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4ad6d52ad9a62a2c9ef283399f457b8d.svg
      fullname: kycrowe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kycrowe
      type: user
    createdAt: '2023-06-02T21:48:54.000Z'
    data:
      edited: false
      editors:
      - kycrowe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.798007071018219
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4ad6d52ad9a62a2c9ef283399f457b8d.svg
          fullname: kycrowe
          isHf: false
          isPro: false
          name: kycrowe
          type: user
        html: "<p>Hello, I want to avoid re-downaloding the model every time for the\
          \ code below</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ AutoModelForCausalLM\n<span class=\"hljs-keyword\">import</span> transformers\n\
          <span class=\"hljs-keyword\">import</span> torch\n\nmodel = <span class=\"\
          hljs-string\">\"tiiuae/falcon-7b-instruct\"</span>\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=<span class=\"hljs-literal\">True</span>,\n    device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>,\n)\n</code></pre>\n<p> so I saved\
          \ the pipeline by doing </p>\n<pre><code class=\"language-python\">pipeline.save_pretrained(<span\
          \ class=\"hljs-string\">\"./pipeline_path/\"</span>)\n</code></pre>\n<p>However,\
          \ I am unable to simply reload the pipeline with </p>\n<pre><code class=\"\
          language-python\">pipe_load = transformers.pipeline(<span class=\"hljs-string\"\
          >\"text-generation\"</span>, model = <span class=\"hljs-string\">\"./pipeline_path/\"\
          </span>)\n</code></pre>\n<p>Getting </p>\n<pre><code>ValueError: Loading\
          \ ./pipeline_path/ requires you to execute the configuration file in that\
          \ repo on your local machine. Make sure you have read the code there to\
          \ avoid malicious use, then set the option `trust_remote_code=True` to remove\
          \ this error.\n</code></pre>\n<p>I added <code>trust_remote_code=True</code>\
          \ in to avoid this error,  but my jupyter kernel dies. </p>\n<p>My questions\
          \ are:</p>\n<ol>\n<li>Any idea how to resolve this? Should I just not run\
          \ this stuff in a jupyter notebook?</li>\n<li>I'm also not quite understand\
          \  <code>execute the configuration file in that repo on your local machine</code>.\
          \ </li>\n<li>Is there any other better ways to avoid re-downaloding the\
          \ model?</li>\n</ol>\n<p>Any help would be greatly appreciated!</p>\n"
        raw: "Hello, I want to avoid re-downaloding the model every time for the code\
          \ below\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \nimport transformers\r\nimport torch\r\n\r\nmodel = \"tiiuae/falcon-7b-instruct\"\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\",\r\n)\r\n```\r\n so I saved the pipeline by doing\
          \ \r\n```python\r\npipeline.save_pretrained(\"./pipeline_path/\")\r\n```\r\
          \nHowever, I am unable to simply reload the pipeline with \r\n```python\r\
          \npipe_load = transformers.pipeline(\"text-generation\", model = \"./pipeline_path/\"\
          )\r\n```\r\nGetting \r\n```\r\nValueError: Loading ./pipeline_path/ requires\
          \ you to execute the configuration file in that repo on your local machine.\
          \ Make sure you have read the code there to avoid malicious use, then set\
          \ the option `trust_remote_code=True` to remove this error.\r\n```\r\nI\
          \ added `trust_remote_code=True` in to avoid this error,  but my jupyter\
          \ kernel dies. \r\n\r\nMy questions are:\r\n1. Any idea how to resolve this?\
          \ Should I just not run this stuff in a jupyter notebook?\r\n2. I'm also\
          \ not quite understand  `execute the configuration file in that repo on\
          \ your local machine`. \r\n3. Is there any other better ways to avoid re-downaloding\
          \ the model? \r\n\r\nAny help would be greatly appreciated!"
        updatedAt: '2023-06-02T21:48:54.778Z'
      numEdits: 0
      reactions: []
    id: 647a63c64c1d1da20dd524a8
    type: comment
  author: kycrowe
  content: "Hello, I want to avoid re-downaloding the model every time for the code\
    \ below\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \nimport transformers\r\nimport torch\r\n\r\nmodel = \"tiiuae/falcon-7b-instruct\"\
    \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n    device_map=\"\
    auto\",\r\n)\r\n```\r\n so I saved the pipeline by doing \r\n```python\r\npipeline.save_pretrained(\"\
    ./pipeline_path/\")\r\n```\r\nHowever, I am unable to simply reload the pipeline\
    \ with \r\n```python\r\npipe_load = transformers.pipeline(\"text-generation\"\
    , model = \"./pipeline_path/\")\r\n```\r\nGetting \r\n```\r\nValueError: Loading\
    \ ./pipeline_path/ requires you to execute the configuration file in that repo\
    \ on your local machine. Make sure you have read the code there to avoid malicious\
    \ use, then set the option `trust_remote_code=True` to remove this error.\r\n\
    ```\r\nI added `trust_remote_code=True` in to avoid this error,  but my jupyter\
    \ kernel dies. \r\n\r\nMy questions are:\r\n1. Any idea how to resolve this? Should\
    \ I just not run this stuff in a jupyter notebook?\r\n2. I'm also not quite understand\
    \  `execute the configuration file in that repo on your local machine`. \r\n3.\
    \ Is there any other better ways to avoid re-downaloding the model? \r\n\r\nAny\
    \ help would be greatly appreciated!"
  created_at: 2023-06-02 20:48:54+00:00
  edited: false
  hidden: false
  id: 647a63c64c1d1da20dd524a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FR8fbI8gvVQ-w82MDXKvj.jpeg?w=200&h=200&f=face
      fullname: "fAle\u0161"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: designfailure
      type: user
    createdAt: '2023-06-03T13:26:58.000Z'
    data:
      edited: false
      editors:
      - designfailure
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7463153004646301
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FR8fbI8gvVQ-w82MDXKvj.jpeg?w=200&h=200&f=face
          fullname: "fAle\u0161"
          isHf: false
          isPro: false
          name: designfailure
          type: user
        html: "<p>ChatGPT suggest this:</p>\n<p>To save and load models using the\
          \ Hugging Face's Transformers library, you might want to save both the model\
          \ and the tokenizer, not the pipeline, as they are the primary components.\
          \ Here's how to do it:</p>\n<p>To save:</p>\n<pre><code class=\"language-python\"\
          >tokenizer = AutoTokenizer.from_pretrained(model)\nmodel = AutoModelForCausalLM.from_pretrained(model)\n\
          \ntokenizer.save_pretrained(<span class=\"hljs-string\">\"./model_path/\"\
          </span>)\nmodel.save_pretrained(<span class=\"hljs-string\">\"./model_path/\"\
          </span>)\n</code></pre>\n<p>To load:</p>\n<pre><code class=\"language-python\"\
          >tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"\
          ./model_path/\"</span>)\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"./model_path/\"</span>)\n\npipeline = transformers.pipeline(\n\
          \    <span class=\"hljs-string\">\"text-generation\"</span>,\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>,\n)\n</code></pre>\n<p>In this way,\
          \ you don't need to download the model each time you run your script, and\
          \ it should resolve the issues you are encountering with the <code>trust_remote_code=True</code>\
          \ setting.</p>\n<p>#designfailure</p>\n"
        raw: "ChatGPT suggest this:\n\nTo save and load models using the Hugging Face's\
          \ Transformers library, you might want to save both the model and the tokenizer,\
          \ not the pipeline, as they are the primary components. Here's how to do\
          \ it:\n\nTo save:\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          model = AutoModelForCausalLM.from_pretrained(model)\n\ntokenizer.save_pretrained(\"\
          ./model_path/\")\nmodel.save_pretrained(\"./model_path/\")\n```\n\nTo load:\n\
          \n```python\ntokenizer = AutoTokenizer.from_pretrained(\"./model_path/\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"./model_path/\")\n\npipeline\
          \ = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"\
          auto\",\n)\n```\n\nIn this way, you don't need to download the model each\
          \ time you run your script, and it should resolve the issues you are encountering\
          \ with the `trust_remote_code=True` setting.\n\n#designfailure"
        updatedAt: '2023-06-03T13:26:58.760Z'
      numEdits: 0
      reactions: []
    id: 647b3fa26dbad6ab0578d6b4
    type: comment
  author: designfailure
  content: "ChatGPT suggest this:\n\nTo save and load models using the Hugging Face's\
    \ Transformers library, you might want to save both the model and the tokenizer,\
    \ not the pipeline, as they are the primary components. Here's how to do it:\n\
    \nTo save:\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(model)\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(model)\n\ntokenizer.save_pretrained(\"\
    ./model_path/\")\nmodel.save_pretrained(\"./model_path/\")\n```\n\nTo load:\n\n\
    ```python\ntokenizer = AutoTokenizer.from_pretrained(\"./model_path/\")\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\"./model_path/\")\n\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
    \    device_map=\"auto\",\n)\n```\n\nIn this way, you don't need to download the\
    \ model each time you run your script, and it should resolve the issues you are\
    \ encountering with the `trust_remote_code=True` setting.\n\n#designfailure"
  created_at: 2023-06-03 12:26:58+00:00
  edited: false
  hidden: false
  id: 647b3fa26dbad6ab0578d6b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4ad6d52ad9a62a2c9ef283399f457b8d.svg
      fullname: kycrowe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kycrowe
      type: user
    createdAt: '2023-06-05T14:22:21.000Z'
    data:
      edited: true
      editors:
      - kycrowe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7555480599403381
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4ad6d52ad9a62a2c9ef283399f457b8d.svg
          fullname: kycrowe
          isHf: false
          isPro: false
          name: kycrowe
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;designfailure&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/designfailure\"\
          >@<span class=\"underline\">designfailure</span></a></span>\n\n\t</span></span><br>I\
          \ tried loading model and tokenizer separately like below</p>\n<blockquote>\n\
          <pre><code class=\"language-python\">tokenizer = AutoTokenizer.from_pretrained(model)\n\
          model = AutoModelForCausalLM.from_pretrained(model)\n</code></pre>\n</blockquote>\n\
          <p>But getting the require execute the configuration file again on the line\
          \ of loading model:</p>\n<pre><code>ValueError: Loading tiiuae/falcon-7b-instruct\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\n</code></pre>\n"
        raw: "Thank you @designfailure \nI tried loading model and tokenizer separately\
          \ like below\n> ```python\n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > model = AutoModelForCausalLM.from_pretrained(model)\n> ```\n\nBut getting\
          \ the require execute the configuration file again on the line of loading\
          \ model:\n```\nValueError: Loading tiiuae/falcon-7b-instruct requires you\
          \ to execute the configuration file in that repo on your local machine.\
          \ Make sure you have read the code there to avoid malicious use, then set\
          \ the option `trust_remote_code=True` to remove this error.\n```"
        updatedAt: '2023-06-05T14:23:42.160Z'
      numEdits: 2
      reactions: []
    id: 647def9df14eafc3b4521092
    type: comment
  author: kycrowe
  content: "Thank you @designfailure \nI tried loading model and tokenizer separately\
    \ like below\n> ```python\n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > model = AutoModelForCausalLM.from_pretrained(model)\n> ```\n\nBut getting the\
    \ require execute the configuration file again on the line of loading model:\n\
    ```\nValueError: Loading tiiuae/falcon-7b-instruct requires you to execute the\
    \ configuration file in that repo on your local machine. Make sure you have read\
    \ the code there to avoid malicious use, then set the option `trust_remote_code=True`\
    \ to remove this error.\n```"
  created_at: 2023-06-05 13:22:21+00:00
  edited: true
  hidden: false
  id: 647def9df14eafc3b4521092
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afa1207eb61ab7d2c01bea241d5e20e7.svg
      fullname: "Attila Sz\xE1sz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rq-aszasz
      type: user
    createdAt: '2023-06-08T13:00:40.000Z'
    data:
      edited: false
      editors:
      - rq-aszasz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6535874009132385
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afa1207eb61ab7d2c01bea241d5e20e7.svg
          fullname: "Attila Sz\xE1sz"
          isHf: false
          isPro: false
          name: rq-aszasz
          type: user
        html: '<p><a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/10">https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/10</a>
          flags the same issue when running on AWS Sagemaker.</p>

          '
        raw: https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/10 flags
          the same issue when running on AWS Sagemaker.
        updatedAt: '2023-06-08T13:00:40.241Z'
      numEdits: 0
      reactions: []
    id: 6481d0f86f283a746865d1c4
    type: comment
  author: rq-aszasz
  content: https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/10 flags the
    same issue when running on AWS Sagemaker.
  created_at: 2023-06-08 12:00:40+00:00
  edited: false
  hidden: false
  id: 6481d0f86f283a746865d1c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f54e7aa54f16e51a0f8aab6a820cb5e.svg
      fullname: Meili Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mwgupta
      type: user
    createdAt: '2023-08-05T01:16:21.000Z'
    data:
      edited: false
      editors:
      - mwgupta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7993093132972717
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f54e7aa54f16e51a0f8aab6a820cb5e.svg
          fullname: Meili Gupta
          isHf: false
          isPro: false
          name: mwgupta
          type: user
        html: "<blockquote>\n<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;designfailure&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/designfailure\"\
          >@<span class=\"underline\">designfailure</span></a></span>\n\n\t</span></span><br>I\
          \ tried loading model and tokenizer separately like below</p>\n<blockquote>\n\
          <pre><code class=\"language-python\">tokenizer = AutoTokenizer.from_pretrained(model)\n\
          model = AutoModelForCausalLM.from_pretrained(model)\n</code></pre>\n</blockquote>\n\
          <p>But getting the require execute the configuration file again on the line\
          \ of loading model:</p>\n<pre><code>ValueError: Loading tiiuae/falcon-7b-instruct\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\n</code></pre>\n\
          </blockquote>\n<p>What was the fix in the end?</p>\n"
        raw: "> Thank you @designfailure \n> I tried loading model and tokenizer separately\
          \ like below\n> > ```python\n> > tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > > model = AutoModelForCausalLM.from_pretrained(model)\n> > ```\n> \n>\
          \ But getting the require execute the configuration file again on the line\
          \ of loading model:\n> ```\n> ValueError: Loading tiiuae/falcon-7b-instruct\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\n>\
          \ ```\n\nWhat was the fix in the end?"
        updatedAt: '2023-08-05T01:16:21.472Z'
      numEdits: 0
      reactions: []
    id: 64cda2e5bf39f9c8be7766e1
    type: comment
  author: mwgupta
  content: "> Thank you @designfailure \n> I tried loading model and tokenizer separately\
    \ like below\n> > ```python\n> > tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > > model = AutoModelForCausalLM.from_pretrained(model)\n> > ```\n> \n> But getting\
    \ the require execute the configuration file again on the line of loading model:\n\
    > ```\n> ValueError: Loading tiiuae/falcon-7b-instruct requires you to execute\
    \ the configuration file in that repo on your local machine. Make sure you have\
    \ read the code there to avoid malicious use, then set the option `trust_remote_code=True`\
    \ to remove this error.\n> ```\n\nWhat was the fix in the end?"
  created_at: 2023-08-05 00:16:21+00:00
  edited: false
  hidden: false
  id: 64cda2e5bf39f9c8be7766e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/220bb4099234e70b578f8f180610160b.svg
      fullname: Carlos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cazz1
      type: user
    createdAt: '2023-09-02T05:08:18.000Z'
    data:
      edited: false
      editors:
      - cazz1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.925349771976471
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/220bb4099234e70b578f8f180610160b.svg
          fullname: Carlos
          isHf: false
          isPro: false
          name: cazz1
          type: user
        html: '<p>I am encountering the same problem. What was the fix plz?</p>

          '
        raw: I am encountering the same problem. What was the fix plz?
        updatedAt: '2023-09-02T05:08:18.828Z'
      numEdits: 0
      reactions: []
    id: 64f2c342b0f30d4ccf2cb229
    type: comment
  author: cazz1
  content: I am encountering the same problem. What was the fix plz?
  created_at: 2023-09-02 04:08:18+00:00
  edited: false
  hidden: false
  id: 64f2c342b0f30d4ccf2cb229
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/996b8944f24973e38868a2c4fadd911e.svg
      fullname: Alfred Lueurt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alflur
      type: user
    createdAt: '2023-09-04T20:53:52.000Z'
    data:
      edited: false
      editors:
      - alflur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.926261305809021
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/996b8944f24973e38868a2c4fadd911e.svg
          fullname: Alfred Lueurt
          isHf: false
          isPro: false
          name: alflur
          type: user
        html: '<p>hello ! same problem too. Someone have a fix ?Thanks</p>

          '
        raw: hello ! same problem too. Someone have a fix ?Thanks
        updatedAt: '2023-09-04T20:53:52.122Z'
      numEdits: 0
      reactions: []
    id: 64f643e05afaa96886587d43
    type: comment
  author: alflur
  content: hello ! same problem too. Someone have a fix ?Thanks
  created_at: 2023-09-04 19:53:52+00:00
  edited: false
  hidden: false
  id: 64f643e05afaa96886587d43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf426055928a283d5cf72781dbf2f2e3.svg
      fullname: Aria
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Khyn
      type: user
    createdAt: '2023-09-25T02:49:24.000Z'
    data:
      edited: true
      editors:
      - Khyn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7897631525993347
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf426055928a283d5cf72781dbf2f2e3.svg
          fullname: Aria
          isHf: false
          isPro: false
          name: Khyn
          type: user
        html: "<blockquote>\n<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;designfailure&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/designfailure\"\
          >@<span class=\"underline\">designfailure</span></a></span>\n\n\t</span></span><br>I\
          \ tried loading model and tokenizer separately like below</p>\n<blockquote>\n\
          <pre><code class=\"language-python\">tokenizer = AutoTokenizer.from_pretrained(model)\n\
          model = AutoModelForCausalLM.from_pretrained(model)\n</code></pre>\n</blockquote>\n\
          <p>But getting the require execute the configuration file again on the line\
          \ of loading model:</p>\n<pre><code>ValueError: Loading tiiuae/falcon-7b-instruct\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\n</code></pre>\n\
          </blockquote>\n<p>hello ! same problem too.What was the fix in the end?\
          \ Thanks</p>\n"
        raw: "> Thank you @designfailure \n> I tried loading model and tokenizer separately\
          \ like below\n> > ```python\n> > tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > > model = AutoModelForCausalLM.from_pretrained(model)\n> > ```\n> \n>\
          \ But getting the require execute the configuration file again on the line\
          \ of loading model:\n> ```\n> ValueError: Loading tiiuae/falcon-7b-instruct\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\n>\
          \ ```\n\nhello ! same problem too.What was the fix in the end? Thanks\n\n"
        updatedAt: '2023-09-25T02:49:46.028Z'
      numEdits: 1
      reactions: []
    id: 6510f534f874d950df6bf4eb
    type: comment
  author: Khyn
  content: "> Thank you @designfailure \n> I tried loading model and tokenizer separately\
    \ like below\n> > ```python\n> > tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > > model = AutoModelForCausalLM.from_pretrained(model)\n> > ```\n> \n> But getting\
    \ the require execute the configuration file again on the line of loading model:\n\
    > ```\n> ValueError: Loading tiiuae/falcon-7b-instruct requires you to execute\
    \ the configuration file in that repo on your local machine. Make sure you have\
    \ read the code there to avoid malicious use, then set the option `trust_remote_code=True`\
    \ to remove this error.\n> ```\n\nhello ! same problem too.What was the fix in\
    \ the end? Thanks\n\n"
  created_at: 2023-09-25 01:49:24+00:00
  edited: true
  hidden: false
  id: 6510f534f874d950df6bf4eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a0a97bdeb59d15bbfe54a6d80a75937.svg
      fullname: Anaelia Ovalle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aovalle
      type: user
    createdAt: '2023-10-18T12:51:08.000Z'
    data:
      edited: false
      editors:
      - aovalle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9835432767868042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a0a97bdeb59d15bbfe54a6d80a75937.svg
          fullname: Anaelia Ovalle
          isHf: false
          isPro: false
          name: aovalle
          type: user
        html: '<p>I had the same issue, resolved it with updating to transformers==4.34.0</p>

          '
        raw: 'I had the same issue, resolved it with updating to transformers==4.34.0


          '
        updatedAt: '2023-10-18T12:51:08.721Z'
      numEdits: 0
      reactions: []
    id: 652fd4bc17a853e9bd1f9b31
    type: comment
  author: aovalle
  content: 'I had the same issue, resolved it with updating to transformers==4.34.0


    '
  created_at: 2023-10-18 11:51:08+00:00
  edited: false
  hidden: false
  id: 652fd4bc17a853e9bd1f9b31
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: Loading ./pipeline/ requires you to execute the configuration file in that
  repo on your local machine
