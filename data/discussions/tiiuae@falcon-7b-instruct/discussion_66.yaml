!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kvmukilan
conflicting_files: null
created_at: 2023-07-20 02:09:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11f48cd55623643f5b77771c7abde0f7.svg
      fullname: Karmukilan V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kvmukilan
      type: user
    createdAt: '2023-07-20T03:09:10.000Z'
    data:
      edited: false
      editors:
      - kvmukilan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7314983010292053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11f48cd55623643f5b77771c7abde0f7.svg
          fullname: Karmukilan V
          isHf: false
          isPro: false
          name: kvmukilan
          type: user
        html: "<p>I have trained LLM on my PDF file now I am asking questions related\
          \ to same, but if a question is being asked out of the context I want the\
          \ answer as \" I dont know \" or \" out of context \" </p>\n<p>Right now\
          \ it is answering even out of context<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/649939984ecdc6fbd6a4168d/R44KRNKHDywKJIcCn6Jbe.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/649939984ecdc6fbd6a4168d/R44KRNKHDywKJIcCn6Jbe.png\"\
          ></a></p>\n<p>I have used follwing embeddings:</p>\n<ol>\n<li>sentence-transformers/all-mpnet-base-v2</li>\n\
          <li>hkunlp/instructor-xl</li>\n</ol>\n<p>and tried with following LLMs:</p>\n\
          <ol>\n<li>lmsys/fastchat-t5-3b-v1.0</li>\n<li>falcon-7b-instruct</li>\n\
          </ol>\n<p>Here is the </p>\n<h1 id=\"prompt-template\">Prompt template</h1>\n\
          <pre><code class=\"language-question_t5_template\">                context:\
          \ {context}\n                question: {question}\n                answer:\
          \ \n                \"\"\"\n                QUESTION_T5_PROMPT = PromptTemplate(\n\
          \                    template=question_t5_template, input_variables=[\"\
          context\", \"question\"]\n                )\n            qa.combine_documents_chain.llm_chain.prompt\
          \ = QUESTION_T5_PROMPT\n            qa.combine_documents_chain.verbose =\
          \ True\n            qa.return_source_documents = True\n</code></pre>\n<p>Function\
          \ calling the query </p>\n<pre><code>    def answer_query(self,question:str)\
          \ -&gt;str:\n        \"\"\"\n        Answer the question\n        \"\"\"\
          \n\n        answer_dict = self.qa({\"query\":question,})\n        print(answer_dict)\n\
          \        answer = answer_dict[\"result\"]\n</code></pre>\n<h1 id=\"please-access-full-code-here\"\
          ><a rel=\"nofollow\" href=\"https://replit.com/join/lxaofshjga-kvmukilan\"\
          >Please access full code here</a></h1>\n"
        raw: "I have trained LLM on my PDF file now I am asking questions related\
          \ to same, but if a question is being asked out of the context I want the\
          \ answer as \" I dont know \" or \" out of context \" \r\n\r\nRight now\
          \ it is answering even out of context \r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/649939984ecdc6fbd6a4168d/R44KRNKHDywKJIcCn6Jbe.png)\r\
          \n\r\nI have used follwing embeddings:\r\n\r\n1. sentence-transformers/all-mpnet-base-v2\r\
          \n2. hkunlp/instructor-xl\r\n\r\nand tried with following LLMs:\r\n1. lmsys/fastchat-t5-3b-v1.0\r\
          \n2. falcon-7b-instruct\r\n\r\nHere is the \r\n# Prompt template \r\n```question_t5_template\
          \ = \"\"\"\r\n                context: {context}\r\n                question:\
          \ {question}\r\n                answer: \r\n                \"\"\"\r\n \
          \               QUESTION_T5_PROMPT = PromptTemplate(\r\n               \
          \     template=question_t5_template, input_variables=[\"context\", \"question\"\
          ]\r\n                )\r\n            qa.combine_documents_chain.llm_chain.prompt\
          \ = QUESTION_T5_PROMPT\r\n            qa.combine_documents_chain.verbose\
          \ = True\r\n            qa.return_source_documents = True\r\n```\r\nFunction\
          \ calling the query \r\n```\r\n    def answer_query(self,question:str) ->str:\r\
          \n        \"\"\"\r\n        Answer the question\r\n        \"\"\"\r\n\r\n\
          \        answer_dict = self.qa({\"query\":question,})\r\n        print(answer_dict)\r\
          \n        answer = answer_dict[\"result\"]\r\n```\r\n\r\n# [Please access\
          \ full code here](https://replit.com/join/lxaofshjga-kvmukilan) \r\n\r\n\
          \r\n\r\n"
        updatedAt: '2023-07-20T03:09:10.791Z'
      numEdits: 0
      reactions: []
    id: 64b8a556f62a2c23a6fb0dbb
    type: comment
  author: kvmukilan
  content: "I have trained LLM on my PDF file now I am asking questions related to\
    \ same, but if a question is being asked out of the context I want the answer\
    \ as \" I dont know \" or \" out of context \" \r\n\r\nRight now it is answering\
    \ even out of context \r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/649939984ecdc6fbd6a4168d/R44KRNKHDywKJIcCn6Jbe.png)\r\
    \n\r\nI have used follwing embeddings:\r\n\r\n1. sentence-transformers/all-mpnet-base-v2\r\
    \n2. hkunlp/instructor-xl\r\n\r\nand tried with following LLMs:\r\n1. lmsys/fastchat-t5-3b-v1.0\r\
    \n2. falcon-7b-instruct\r\n\r\nHere is the \r\n# Prompt template \r\n```question_t5_template\
    \ = \"\"\"\r\n                context: {context}\r\n                question:\
    \ {question}\r\n                answer: \r\n                \"\"\"\r\n       \
    \         QUESTION_T5_PROMPT = PromptTemplate(\r\n                    template=question_t5_template,\
    \ input_variables=[\"context\", \"question\"]\r\n                )\r\n       \
    \     qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT\r\n   \
    \         qa.combine_documents_chain.verbose = True\r\n            qa.return_source_documents\
    \ = True\r\n```\r\nFunction calling the query \r\n```\r\n    def answer_query(self,question:str)\
    \ ->str:\r\n        \"\"\"\r\n        Answer the question\r\n        \"\"\"\r\n\
    \r\n        answer_dict = self.qa({\"query\":question,})\r\n        print(answer_dict)\r\
    \n        answer = answer_dict[\"result\"]\r\n```\r\n\r\n# [Please access full\
    \ code here](https://replit.com/join/lxaofshjga-kvmukilan) \r\n\r\n\r\n\r\n"
  created_at: 2023-07-20 02:09:10+00:00
  edited: false
  hidden: false
  id: 64b8a556f62a2c23a6fb0dbb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 66
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: 'falcon-7b-instruct is answering out of context '
