!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ankity09
conflicting_files: null
created_at: 2023-08-30 03:14:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a200765c9624b97611a3b8/Ah5RIIW6RXznYwXPw8ONi.png?w=200&h=200&f=face
      fullname: Ankit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ankity09
      type: user
    createdAt: '2023-08-30T04:14:31.000Z'
    data:
      edited: false
      editors:
      - ankity09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8697126507759094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a200765c9624b97611a3b8/Ah5RIIW6RXznYwXPw8ONi.png?w=200&h=200&f=face
          fullname: Ankit
          isHf: false
          isPro: true
          name: ankity09
          type: user
        html: '<p>I am implementing RAG architecture with ChromaDB as my Vector Store
          and Falcon-7B as my LLM. I have used Langchains retriever to tie these together.
          While testing with a single PDF and search results set to return the top
          3 matches, I face a number of issues. </p>

          <ol>

          <li>The returned answers are not accurate (Tried different Temperature settings)</li>

          <li>The model takes a long time and then responds with the same sentence
          repeated multiple times. (Increasing repetition penalty mitigated this to
          an extent)</li>

          <li>Model does not return with an answer for extended period of times, sometimes
          greater than 10-15 mins.</li>

          <li>Model response is slow. 5X slow in some cases when compared to models
          like Llama-2 7B or 13B</li>

          </ol>

          <p>I reduced the returned search results from 3 to 1, which improved parts
          of the accuracy and time, however the model stops responding after being
          queried 3-4 times. </p>

          <p>All of these issues have been reported in some form or the other previously</p>

          <h2 id="wrong-output">Wrong Output</h2>

          <p><a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/69">while
          giving a input but getting the wrong output for the particular input</a><br><a
          href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/66">falcon-7b-instruct
          is answering out of context</a><br><a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/62">Repeats
          the same sentence</a><br><a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/34">any
          success in In-context question-answering?</a><br><a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/17">Model
          keeps generating multiple rounds of conversation</a></p>

          <h2 id="model-is-slow-or-does-not-give-output">Model is Slow or does not
          give output</h2>

          <p><a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/33">Slow
          inference</a><br><a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/31">4th
          inference in a row does not work for Falcon7B in 8 or 4 bit</a></p>

          <p>I am using the 16bit version of the model and running on two T4 GPUs
          on AWS.</p>

          <p>Please let me know if there are any workarounds or fixes for the above.</p>

          <p>Thanks</p>

          '
        raw: "I am implementing RAG architecture with ChromaDB as my Vector Store\
          \ and Falcon-7B as my LLM. I have used Langchains retriever to tie these\
          \ together. While testing with a single PDF and search results set to return\
          \ the top 3 matches, I face a number of issues. \r\n1. The returned answers\
          \ are not accurate (Tried different Temperature settings)\r\n2. The model\
          \ takes a long time and then responds with the same sentence repeated multiple\
          \ times. (Increasing repetition penalty mitigated this to an extent)\r\n\
          3. Model does not return with an answer for extended period of times, sometimes\
          \ greater than 10-15 mins.\r\n4. Model response is slow. 5X slow in some\
          \ cases when compared to models like Llama-2 7B or 13B\r\n\r\nI reduced\
          \ the returned search results from 3 to 1, which improved parts of the accuracy\
          \ and time, however the model stops responding after being queried 3-4 times.\
          \ \r\n\r\nAll of these issues have been reported in some form or the other\
          \ previously\r\n\r\n## Wrong Output\r\n[while giving a input but getting\
          \ the wrong output for the particular input](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/69)\r\
          \n[falcon-7b-instruct is answering out of context](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/66)\r\
          \n[Repeats the same sentence](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/62)\r\
          \n[any success in In-context question-answering?](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/34)\r\
          \n[Model keeps generating multiple rounds of conversation](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/17)\r\
          \n\r\n## Model is Slow or does not give output\r\n[Slow inference](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/33)\r\
          \n[4th inference in a row does not work for Falcon7B in 8 or 4 bit](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/31)\r\
          \n\r\nI am using the 16bit version of the model and running on two T4 GPUs\
          \ on AWS.\r\n\r\nPlease let me know if there are any workarounds or fixes\
          \ for the above.\r\n\r\nThanks"
        updatedAt: '2023-08-30T04:14:31.186Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - samos123
    id: 64eec22712f6864885ff2737
    type: comment
  author: ankity09
  content: "I am implementing RAG architecture with ChromaDB as my Vector Store and\
    \ Falcon-7B as my LLM. I have used Langchains retriever to tie these together.\
    \ While testing with a single PDF and search results set to return the top 3 matches,\
    \ I face a number of issues. \r\n1. The returned answers are not accurate (Tried\
    \ different Temperature settings)\r\n2. The model takes a long time and then responds\
    \ with the same sentence repeated multiple times. (Increasing repetition penalty\
    \ mitigated this to an extent)\r\n3. Model does not return with an answer for\
    \ extended period of times, sometimes greater than 10-15 mins.\r\n4. Model response\
    \ is slow. 5X slow in some cases when compared to models like Llama-2 7B or 13B\r\
    \n\r\nI reduced the returned search results from 3 to 1, which improved parts\
    \ of the accuracy and time, however the model stops responding after being queried\
    \ 3-4 times. \r\n\r\nAll of these issues have been reported in some form or the\
    \ other previously\r\n\r\n## Wrong Output\r\n[while giving a input but getting\
    \ the wrong output for the particular input](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/69)\r\
    \n[falcon-7b-instruct is answering out of context](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/66)\r\
    \n[Repeats the same sentence](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/62)\r\
    \n[any success in In-context question-answering?](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/34)\r\
    \n[Model keeps generating multiple rounds of conversation](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/17)\r\
    \n\r\n## Model is Slow or does not give output\r\n[Slow inference](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/33)\r\
    \n[4th inference in a row does not work for Falcon7B in 8 or 4 bit](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/31)\r\
    \n\r\nI am using the 16bit version of the model and running on two T4 GPUs on\
    \ AWS.\r\n\r\nPlease let me know if there are any workarounds or fixes for the\
    \ above.\r\n\r\nThanks"
  created_at: 2023-08-30 03:14:31+00:00
  edited: false
  hidden: false
  id: 64eec22712f6864885ff2737
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a200765c9624b97611a3b8/Ah5RIIW6RXznYwXPw8ONi.png?w=200&h=200&f=face
      fullname: Ankit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ankity09
      type: user
    createdAt: '2023-08-30T04:35:52.000Z'
    data:
      edited: false
      editors:
      - ankity09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9232428073883057
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a200765c9624b97611a3b8/Ah5RIIW6RXznYwXPw8ONi.png?w=200&h=200&f=face
          fullname: Ankit
          isHf: false
          isPro: true
          name: ankity09
          type: user
        html: '<p>When I set the returned search results from VecDB to 3(larger prompt),
          the model takes<br>1st Question(Answer is wrong)<br><code>CPU times: user
          2min 39s, sys: 391 ms, total: 2min 39s</code><br>2nd Question (Answer is
          wrong)<br><code>CPU times: user 47.4 s, sys: 7.26 ms, total: 47.4 s</code><br>and
          then does not respond from the third onwards</p>

          <p>When I decrease the results to 1 (smaller prompt)<br>1st question takes(Answer
          is right)<br><code>CPU times: user 44.6 s, sys: 288 ms, total: 44.9 s</code><br>2nd
          Question takes(Answer is somewhat right)<br><code>CPU times: user 17.4 s,
          sys: 0 ns, total: 17.4 s</code><br>and then does not respond from the third
          onwards as above.</p>

          '
        raw: "When I set the returned search results from VecDB to 3(larger prompt),\
          \ the model takes \n1st Question(Answer is wrong)\n```CPU times: user 2min\
          \ 39s, sys: 391 ms, total: 2min 39s```\n2nd Question (Answer is wrong)\n\
          ```CPU times: user 47.4 s, sys: 7.26 ms, total: 47.4 s```\nand then does\
          \ not respond from the third onwards\n\nWhen I decrease the results to 1\
          \ (smaller prompt)\n1st question takes(Answer is right)\n```CPU times: user\
          \ 44.6 s, sys: 288 ms, total: 44.9 s```\n2nd Question takes(Answer is somewhat\
          \ right)\n```CPU times: user 17.4 s, sys: 0 ns, total: 17.4 s```\nand then\
          \ does not respond from the third onwards as above."
        updatedAt: '2023-08-30T04:35:52.745Z'
      numEdits: 0
      reactions: []
    id: 64eec7286f1e085c1ead8291
    type: comment
  author: ankity09
  content: "When I set the returned search results from VecDB to 3(larger prompt),\
    \ the model takes \n1st Question(Answer is wrong)\n```CPU times: user 2min 39s,\
    \ sys: 391 ms, total: 2min 39s```\n2nd Question (Answer is wrong)\n```CPU times:\
    \ user 47.4 s, sys: 7.26 ms, total: 47.4 s```\nand then does not respond from\
    \ the third onwards\n\nWhen I decrease the results to 1 (smaller prompt)\n1st\
    \ question takes(Answer is right)\n```CPU times: user 44.6 s, sys: 288 ms, total:\
    \ 44.9 s```\n2nd Question takes(Answer is somewhat right)\n```CPU times: user\
    \ 17.4 s, sys: 0 ns, total: 17.4 s```\nand then does not respond from the third\
    \ onwards as above."
  created_at: 2023-08-30 03:35:52+00:00
  edited: false
  hidden: false
  id: 64eec7286f1e085c1ead8291
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 78
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: Facing Issues with Model Output and Inference Times
