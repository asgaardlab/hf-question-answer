!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pranavnerurkar
conflicting_files: null
created_at: 2023-07-12 08:04:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5b6fcf3ed718471c05c237a1651622e.svg
      fullname: pranav nerurkar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pranavnerurkar
      type: user
    createdAt: '2023-07-12T09:04:03.000Z'
    data:
      edited: false
      editors:
      - pranavnerurkar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45152145624160767
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5b6fcf3ed718471c05c237a1651622e.svg
          fullname: pranav nerurkar
          isHf: false
          isPro: false
          name: pranavnerurkar
          type: user
        html: '<p>Followed<br><a rel="nofollow" href="https://www.philschmid.de/sagemaker-falcon-llm">https://www.philschmid.de/sagemaker-falcon-llm</a></p>

          <p>Crashed at</p>

          <h1 id="deploy-model-to-an-endpoint">Deploy model to an endpoint</h1>

          <h1 id="httpssagemakerreadthedocsioenstableapiinferencemodelhtmlsagemakermodelmodeldeploy"><a
          rel="nofollow" href="https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy">https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy</a></h1>

          <p>llm = llm_model.deploy(<br>  initial_instance_count=1,<br>  instance_type=instance_type,</p>

          <h1 id="volume_size400--if-using-an-instance-with-local-ssd-storage-volume_size-must-be-none-eg-p4-but-not-p3">volume_size=400,
          # If using an instance with local SSD storage, volume_size must be None,
          e.g. p4 but not p3</h1>

          <p>  container_startup_health_check_timeout=health_check_timeout, # 10 minutes
          to be able to load the model<br>)</p>

          <p>I used instance: ml.g4dn.xlarge</p>

          <hr>

          <p>UnexpectedStatusException                 Traceback (most recent call
          last)<br> in <br>      5   instance_type=instance_type,<br>      6   # volume_size=400,
          # If using an instance with local SSD storage, volume_size must be None,
          e.g. p4 but not p3<br>----&gt; 7   container_startup_health_check_timeout=health_check_timeout,
          # 10 minutes to be able to load the model<br>      8 )</p>

          <p>/opt/conda/lib/python3.7/site-packages/sagemaker/huggingface/model.py
          in deploy(self, initial_instance_count, instance_type, serializer, deserializer,
          accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config,
          async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout,
          container_startup_health_check_timeout, inference_recommendation_id, explainer_config,
          **kwargs)<br>    326             container_startup_health_check_timeout=container_startup_health_check_timeout,<br>    327             inference_recommendation_id=inference_recommendation_id,<br>--&gt;
          328             explainer_config=explainer_config,<br>    329         )<br>    330
          </p>

          <p>/opt/conda/lib/python3.7/site-packages/sagemaker/model.py in deploy(self,
          initial_instance_count, instance_type, serializer, deserializer, accelerator_type,
          endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config,
          serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout,
          inference_recommendation_id, explainer_config, **kwargs)<br>   1334             data_capture_config_dict=data_capture_config_dict,<br>   1335             explainer_config_dict=explainer_config_dict,<br>-&gt;
          1336             async_inference_config_dict=async_inference_config_dict,<br>   1337         )<br>   1338
          </p>

          <p>/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in endpoint_from_production_variants(self,
          name, production_variants, tags, kms_key, wait, data_capture_config_dict,
          async_inference_config_dict, explainer_config_dict)<br>   4575         self.sagemaker_client.create_endpoint_config(**config_options)<br>   4576<br>-&gt;
          4577         return self.create_endpoint(endpoint_name=name, config_name=name,
          tags=tags, wait=wait)<br>   4578<br>   4579     def expand_role(self, role):</p>

          <p>/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in create_endpoint(self,
          endpoint_name, config_name, tags, wait)<br>   3968         )<br>   3969         if
          wait:<br>-&gt; 3970             self.wait_for_endpoint(endpoint_name)<br>   3971         return
          endpoint_name<br>   3972 </p>

          <p>/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in wait_for_endpoint(self,
          endpoint, poll)<br>   4323                 message=message,<br>   4324                 allowed_statuses=["InService"],<br>-&gt;
          4325                 actual_status=status,<br>   4326             )<br>   4327         return
          desc</p>

          <p>UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-07-12-08-18-29-406:
          Failed. Reason: The primary container for production variant AllTraffic
          did not pass the ping health check. Please check CloudWatch logs for this
          endpoint..</p>

          '
        raw: "Followed\r\nhttps://www.philschmid.de/sagemaker-falcon-llm\r\n\r\nCrashed\
          \ at\r\n# Deploy model to an endpoint\r\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\r\
          \nllm = llm_model.deploy(\r\n  initial_instance_count=1,\r\n  instance_type=instance_type,\r\
          \n  # volume_size=400, # If using an instance with local SSD storage, volume_size\
          \ must be None, e.g. p4 but not p3\r\n  container_startup_health_check_timeout=health_check_timeout,\
          \ # 10 minutes to be able to load the model\r\n)\r\n\r\n\r\nI used instance:\
          \ ml.g4dn.xlarge\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\
          \nUnexpectedStatusException                 Traceback (most recent call\
          \ last)\r\n<ipython-input-16-9b8b7d5ce10b> in <module>\r\n      5   instance_type=instance_type,\r\
          \n      6   # volume_size=400, # If using an instance with local SSD storage,\
          \ volume_size must be None, e.g. p4 but not p3\r\n----> 7   container_startup_health_check_timeout=health_check_timeout,\
          \ # 10 minutes to be able to load the model\r\n      8 )\r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/huggingface/model.py\
          \ in deploy(self, initial_instance_count, instance_type, serializer, deserializer,\
          \ accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config,\
          \ async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout,\
          \ container_startup_health_check_timeout, inference_recommendation_id, explainer_config,\
          \ **kwargs)\r\n    326             container_startup_health_check_timeout=container_startup_health_check_timeout,\r\
          \n    327             inference_recommendation_id=inference_recommendation_id,\r\
          \n--> 328             explainer_config=explainer_config,\r\n    329    \
          \     )\r\n    330 \r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/model.py\
          \ in deploy(self, initial_instance_count, instance_type, serializer, deserializer,\
          \ accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config,\
          \ async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout,\
          \ container_startup_health_check_timeout, inference_recommendation_id, explainer_config,\
          \ **kwargs)\r\n   1334             data_capture_config_dict=data_capture_config_dict,\r\
          \n   1335             explainer_config_dict=explainer_config_dict,\r\n->\
          \ 1336             async_inference_config_dict=async_inference_config_dict,\r\
          \n   1337         )\r\n   1338 \r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\
          \ in endpoint_from_production_variants(self, name, production_variants,\
          \ tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict,\
          \ explainer_config_dict)\r\n   4575         self.sagemaker_client.create_endpoint_config(**config_options)\r\
          \n   4576 \r\n-> 4577         return self.create_endpoint(endpoint_name=name,\
          \ config_name=name, tags=tags, wait=wait)\r\n   4578 \r\n   4579     def\
          \ expand_role(self, role):\r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\
          \ in create_endpoint(self, endpoint_name, config_name, tags, wait)\r\n \
          \  3968         )\r\n   3969         if wait:\r\n-> 3970             self.wait_for_endpoint(endpoint_name)\r\
          \n   3971         return endpoint_name\r\n   3972 \r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\
          \ in wait_for_endpoint(self, endpoint, poll)\r\n   4323                \
          \ message=message,\r\n   4324                 allowed_statuses=[\"InService\"\
          ],\r\n-> 4325                 actual_status=status,\r\n   4326         \
          \    )\r\n   4327         return desc\r\n\r\nUnexpectedStatusException:\
          \ Error hosting endpoint huggingface-pytorch-tgi-inference-2023-07-12-08-18-29-406:\
          \ Failed. Reason: The primary container for production variant AllTraffic\
          \ did not pass the ping health check. Please check CloudWatch logs for this\
          \ endpoint.."
        updatedAt: '2023-07-12T09:04:03.635Z'
      numEdits: 0
      reactions: []
    id: 64ae6c832a530cbdeea51c7a
    type: comment
  author: pranavnerurkar
  content: "Followed\r\nhttps://www.philschmid.de/sagemaker-falcon-llm\r\n\r\nCrashed\
    \ at\r\n# Deploy model to an endpoint\r\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\r\
    \nllm = llm_model.deploy(\r\n  initial_instance_count=1,\r\n  instance_type=instance_type,\r\
    \n  # volume_size=400, # If using an instance with local SSD storage, volume_size\
    \ must be None, e.g. p4 but not p3\r\n  container_startup_health_check_timeout=health_check_timeout,\
    \ # 10 minutes to be able to load the model\r\n)\r\n\r\n\r\nI used instance: ml.g4dn.xlarge\r\
    \n\r\n\r\n\r\n---------------------------------------------------------------------------\r\
    \nUnexpectedStatusException                 Traceback (most recent call last)\r\
    \n<ipython-input-16-9b8b7d5ce10b> in <module>\r\n      5   instance_type=instance_type,\r\
    \n      6   # volume_size=400, # If using an instance with local SSD storage,\
    \ volume_size must be None, e.g. p4 but not p3\r\n----> 7   container_startup_health_check_timeout=health_check_timeout,\
    \ # 10 minutes to be able to load the model\r\n      8 )\r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/huggingface/model.py\
    \ in deploy(self, initial_instance_count, instance_type, serializer, deserializer,\
    \ accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config,\
    \ serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout,\
    \ inference_recommendation_id, explainer_config, **kwargs)\r\n    326        \
    \     container_startup_health_check_timeout=container_startup_health_check_timeout,\r\
    \n    327             inference_recommendation_id=inference_recommendation_id,\r\
    \n--> 328             explainer_config=explainer_config,\r\n    329         )\r\
    \n    330 \r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/model.py in\
    \ deploy(self, initial_instance_count, instance_type, serializer, deserializer,\
    \ accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config,\
    \ serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout,\
    \ inference_recommendation_id, explainer_config, **kwargs)\r\n   1334        \
    \     data_capture_config_dict=data_capture_config_dict,\r\n   1335          \
    \   explainer_config_dict=explainer_config_dict,\r\n-> 1336             async_inference_config_dict=async_inference_config_dict,\r\
    \n   1337         )\r\n   1338 \r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\
    \ in endpoint_from_production_variants(self, name, production_variants, tags,\
    \ kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\r\
    \n   4575         self.sagemaker_client.create_endpoint_config(**config_options)\r\
    \n   4576 \r\n-> 4577         return self.create_endpoint(endpoint_name=name,\
    \ config_name=name, tags=tags, wait=wait)\r\n   4578 \r\n   4579     def expand_role(self,\
    \ role):\r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in\
    \ create_endpoint(self, endpoint_name, config_name, tags, wait)\r\n   3968   \
    \      )\r\n   3969         if wait:\r\n-> 3970             self.wait_for_endpoint(endpoint_name)\r\
    \n   3971         return endpoint_name\r\n   3972 \r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\
    \ in wait_for_endpoint(self, endpoint, poll)\r\n   4323                 message=message,\r\
    \n   4324                 allowed_statuses=[\"InService\"],\r\n-> 4325       \
    \          actual_status=status,\r\n   4326             )\r\n   4327         return\
    \ desc\r\n\r\nUnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-07-12-08-18-29-406:\
    \ Failed. Reason: The primary container for production variant AllTraffic did\
    \ not pass the ping health check. Please check CloudWatch logs for this endpoint.."
  created_at: 2023-07-12 08:04:03+00:00
  edited: false
  hidden: false
  id: 64ae6c832a530cbdeea51c7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/097df7a744da527a58b40d21260f1f8d.svg
      fullname: Dan Ryan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danryan
      type: user
    createdAt: '2023-07-17T18:25:16.000Z'
    data:
      edited: false
      editors:
      - danryan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9096437692642212
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/097df7a744da527a58b40d21260f1f8d.svg
          fullname: Dan Ryan
          isHf: false
          isPro: false
          name: danryan
          type: user
        html: '<p>The endpoint container is not healthy and is restarting. Check the
          endpoint cloudwatch logs for details.</p>

          '
        raw: The endpoint container is not healthy and is restarting. Check the endpoint
          cloudwatch logs for details.
        updatedAt: '2023-07-17T18:25:16.865Z'
      numEdits: 0
      reactions: []
    id: 64b5878c7dc0a21163303410
    type: comment
  author: danryan
  content: The endpoint container is not healthy and is restarting. Check the endpoint
    cloudwatch logs for details.
  created_at: 2023-07-17 17:25:16+00:00
  edited: false
  hidden: false
  id: 64b5878c7dc0a21163303410
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ac1704b0ce96a279645fe63e7d7c4f6.svg
      fullname: guglielmo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gpaoletti
      type: user
    createdAt: '2023-08-08T09:55:18.000Z'
    data:
      edited: false
      editors:
      - gpaoletti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5976722240447998
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ac1704b0ce96a279645fe63e7d7c4f6.svg
          fullname: guglielmo
          isHf: false
          isPro: false
          name: gpaoletti
          type: user
        html: "<p>I am tryning to deploy falcon-40b and experiencing the same error\
          \ since I moved to </p>\n<pre><code># install supported sagemaker SDK\n\
          !pip install \"sagemaker==2.175.0\" --upgrade --quiet\n</code></pre>\n<p>and</p>\n\
          <pre><code># retrieve the llm image uri\nllm_image = get_huggingface_llm_image_uri(\n\
          \  \"huggingface\",\n  version=\"0.9.3\"\n)\n</code></pre>\n<p>all working\
          \ fine with llm_image version \"0.8.2\"</p>\n<p>both tests done with </p>\n\
          <pre><code>import json\nfrom sagemaker.huggingface import HuggingFaceModel\n\
          \n# sagemaker config\ninstance_type = \"ml.g5.12xlarge\"\nnumber_of_gpu\
          \ = 4\nhealth_check_timeout = 300\n\n# TGI config\nconfig = {\n  'HF_MODEL_ID':\
          \ \"tiiuae/falcon-40b-instruct\", # model_id from hf.co/models\n  'SM_NUM_GPUS':\
          \ json.dumps(number_of_gpu), # Number of GPU used per replica\n  'MAX_INPUT_LENGTH':\
          \ json.dumps(1024),  # Max length of input text\n  'MAX_TOTAL_TOKENS': json.dumps(2048),\
          \  # Max length of the generation (including input text)\n  # 'HF_MODEL_QUANTIZE':\
          \ \"bitsandbytes\", # comment in to quantize\n}\n\n# create HuggingFaceModel\n\
          llm_model = HuggingFaceModel(\n  role=role,\n  image_uri=llm_image,\n  env=config\n\
          )\n</code></pre>\n<p>with falcon-7b I am able to succesfully deploy using\
          \ version \"0.9.3\"</p>\n"
        raw: "I am tryning to deploy falcon-40b and experiencing the same error since\
          \ I moved to \n```\n# install supported sagemaker SDK\n!pip install \"sagemaker==2.175.0\"\
          \ --upgrade --quiet\n```\nand\n```\n# retrieve the llm image uri\nllm_image\
          \ = get_huggingface_llm_image_uri(\n  \"huggingface\",\n  version=\"0.9.3\"\
          \n)\n```\nall working fine with llm_image version \"0.8.2\"\n\nboth tests\
          \ done with \n```\nimport json\nfrom sagemaker.huggingface import HuggingFaceModel\n\
          \n# sagemaker config\ninstance_type = \"ml.g5.12xlarge\"\nnumber_of_gpu\
          \ = 4\nhealth_check_timeout = 300\n\n# TGI config\nconfig = {\n  'HF_MODEL_ID':\
          \ \"tiiuae/falcon-40b-instruct\", # model_id from hf.co/models\n  'SM_NUM_GPUS':\
          \ json.dumps(number_of_gpu), # Number of GPU used per replica\n  'MAX_INPUT_LENGTH':\
          \ json.dumps(1024),  # Max length of input text\n  'MAX_TOTAL_TOKENS': json.dumps(2048),\
          \  # Max length of the generation (including input text)\n  # 'HF_MODEL_QUANTIZE':\
          \ \"bitsandbytes\", # comment in to quantize\n}\n\n# create HuggingFaceModel\n\
          llm_model = HuggingFaceModel(\n  role=role,\n  image_uri=llm_image,\n  env=config\n\
          )\n```\nwith falcon-7b I am able to succesfully deploy using version \"\
          0.9.3\""
        updatedAt: '2023-08-08T09:55:18.870Z'
      numEdits: 0
      reactions: []
    id: 64d211063dc82444ac3a9300
    type: comment
  author: gpaoletti
  content: "I am tryning to deploy falcon-40b and experiencing the same error since\
    \ I moved to \n```\n# install supported sagemaker SDK\n!pip install \"sagemaker==2.175.0\"\
    \ --upgrade --quiet\n```\nand\n```\n# retrieve the llm image uri\nllm_image =\
    \ get_huggingface_llm_image_uri(\n  \"huggingface\",\n  version=\"0.9.3\"\n)\n\
    ```\nall working fine with llm_image version \"0.8.2\"\n\nboth tests done with\
    \ \n```\nimport json\nfrom sagemaker.huggingface import HuggingFaceModel\n\n#\
    \ sagemaker config\ninstance_type = \"ml.g5.12xlarge\"\nnumber_of_gpu = 4\nhealth_check_timeout\
    \ = 300\n\n# TGI config\nconfig = {\n  'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\"\
    , # model_id from hf.co/models\n  'SM_NUM_GPUS': json.dumps(number_of_gpu), #\
    \ Number of GPU used per replica\n  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max\
    \ length of input text\n  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length\
    \ of the generation (including input text)\n  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\"\
    , # comment in to quantize\n}\n\n# create HuggingFaceModel\nllm_model = HuggingFaceModel(\n\
    \  role=role,\n  image_uri=llm_image,\n  env=config\n)\n```\nwith falcon-7b I\
    \ am able to succesfully deploy using version \"0.9.3\""
  created_at: 2023-08-08 08:55:18+00:00
  edited: false
  hidden: false
  id: 64d211063dc82444ac3a9300
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ac1704b0ce96a279645fe63e7d7c4f6.svg
      fullname: guglielmo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gpaoletti
      type: user
    createdAt: '2023-08-08T15:43:19.000Z'
    data:
      edited: false
      editors:
      - gpaoletti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7376818060874939
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ac1704b0ce96a279645fe63e7d7c4f6.svg
          fullname: guglielmo
          isHf: false
          isPro: false
          name: gpaoletti
          type: user
        html: '<p>details of my error </p>

          <pre><code>UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-08-08-09-21-35-398:
          Failed. Reason: The primary container for production variant AllTraffic
          did not pass the ping health check. Please check CloudWatch logs for this
          endpoint..

          </code></pre>

          '
        raw: "details of my error \n```\nUnexpectedStatusException: Error hosting\
          \ endpoint huggingface-pytorch-tgi-inference-2023-08-08-09-21-35-398: Failed.\
          \ Reason: The primary container for production variant AllTraffic did not\
          \ pass the ping health check. Please check CloudWatch logs for this endpoint..\n\
          ```\n"
        updatedAt: '2023-08-08T15:43:19.940Z'
      numEdits: 0
      reactions: []
    id: 64d26297ad5294e2937a02af
    type: comment
  author: gpaoletti
  content: "details of my error \n```\nUnexpectedStatusException: Error hosting endpoint\
    \ huggingface-pytorch-tgi-inference-2023-08-08-09-21-35-398: Failed. Reason: The\
    \ primary container for production variant AllTraffic did not pass the ping health\
    \ check. Please check CloudWatch logs for this endpoint..\n```\n"
  created_at: 2023-08-08 14:43:19+00:00
  edited: false
  hidden: false
  id: 64d26297ad5294e2937a02af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-08-08T16:21:52.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9720572233200073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: '<p>What is the error you see in cloudwatch? </p>

          '
        raw: 'What is the error you see in cloudwatch? '
        updatedAt: '2023-08-08T16:21:52.790Z'
      numEdits: 0
      reactions: []
    id: 64d26ba092474b17cb2cae99
    type: comment
  author: philschmid
  content: 'What is the error you see in cloudwatch? '
  created_at: 2023-08-08 15:21:52+00:00
  edited: false
  hidden: false
  id: 64d26ba092474b17cb2cae99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ac1704b0ce96a279645fe63e7d7c4f6.svg
      fullname: guglielmo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gpaoletti
      type: user
    createdAt: '2023-08-08T16:38:23.000Z'
    data:
      edited: false
      editors:
      - gpaoletti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8549469709396362
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ac1704b0ce96a279645fe63e7d7c4f6.svg
          fullname: guglielmo
          isHf: false
          isPro: false
          name: gpaoletti
          type: user
        html: '<p>I have also the full CSV but this is  a screenshot which looks like
          some relevant part.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64be82038e051085ba21ed0f/9oItuf1PuuKezH8CqWAHD.png"><img
          alt="Screenshot 2023-08-08 at 18.35.34.png" src="https://cdn-uploads.huggingface.co/production/uploads/64be82038e051085ba21ed0f/9oItuf1PuuKezH8CqWAHD.png"></a></p>

          '
        raw: "I have also the full CSV but this is  a screenshot which looks like\
          \ some relevant part. \n![Screenshot 2023-08-08 at 18.35.34.png](https://cdn-uploads.huggingface.co/production/uploads/64be82038e051085ba21ed0f/9oItuf1PuuKezH8CqWAHD.png)\n"
        updatedAt: '2023-08-08T16:38:23.657Z'
      numEdits: 0
      reactions: []
    id: 64d26f7f35577e62a9e22a23
    type: comment
  author: gpaoletti
  content: "I have also the full CSV but this is  a screenshot which looks like some\
    \ relevant part. \n![Screenshot 2023-08-08 at 18.35.34.png](https://cdn-uploads.huggingface.co/production/uploads/64be82038e051085ba21ed0f/9oItuf1PuuKezH8CqWAHD.png)\n"
  created_at: 2023-08-08 15:38:23+00:00
  edited: false
  hidden: false
  id: 64d26f7f35577e62a9e22a23
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 61
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: 'UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-07-12-08-18-29-406'
