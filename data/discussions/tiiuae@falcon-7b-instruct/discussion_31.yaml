!!python/object:huggingface_hub.community.DiscussionWithDetails
author: max0uu
conflicting_files: null
created_at: 2023-06-13 12:21:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb82524e004e156bbcbfc0d8a5175a9a.svg
      fullname: Max Poyatos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: max0uu
      type: user
    createdAt: '2023-06-13T13:21:19.000Z'
    data:
      edited: true
      editors:
      - max0uu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6541157364845276
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb82524e004e156bbcbfc0d8a5175a9a.svg
          fullname: Max Poyatos
          isHf: false
          isPro: false
          name: max0uu
          type: user
        html: "<p>I built a tool that generates inferences in a loop with Falcon 7B\
          \ in 16/8/4 bit. Weirdly, Falcon generates the first 3 inferences rapidly\
          \ and then it blocks and never returns the 4th inference when I use it in\
          \ 8 or 4 bit. Plus, the problem persists when I change the set of instructions\
          \ I use to generate inferences. </p>\n<p>Here is the code I use to load\
          \ the model and generate inferences:</p>\n<p>def load_falcon(weight_encoding):<br>\
          \    model_path = \"/falcon-7b-instruct\"<br>    eight_bit = weight_encoding\
          \ == \"8-bit\"<br>    four_bit = weight_encoding == \"4-bit\"</p>\n<pre><code>#creating\
          \ a model \nfmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n\
          \    load_in_8bit = eight_bit,\n    load_in_4bit = four_bit,\n    trust_remote_code\
          \ = True,\n    torch_dtype= torch.float16,\n    device_map = \"auto\")\n\
          \nfmodel.eval() \n# fmodel.to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\
          \ngen_text = transformers.pipeline(\nmodel=fmodel, \ntokenizer=tokenizer,\
          \ \ntask='text-generation', \nreturn_full_text=False,     \nmax_length=5000,\
          \ \ntemperature=0.1, \ntop_p=0.75, #select from top tokens whose probability\
          \ adds up to 15%\ntop_k=40, #selecting from top 0 tokens \nrepetition_penalty=1.9,\
          \ #without a penalty, output starts to repeat \ndo_sample=True, \nnum_return_sequences=1,\n\
          eos_token_id=tokenizer.eos_token_id\n</code></pre>\n<p>)<br>    return gen_text,\
          \ tokenizer</p>\n<p>def falcon_inference(instruction, gen_text):<br>   \
          \ response = gen_text(instruction)<br>    texts = [ seq['generated_text']\
          \ for seq in response]<br>    text = '\\n'.join(texts)<br>    return text</p>\n\
          <p>Configuration:<br>CUDA Version: 12.1<br>NVIDIA GeForce RTX 4090 (24 GO\
          \ VRAM)<br>I have the latest versions of bitsandbytes (0.39.0), transformers\
          \ (4.31.0.dev0) and accelerate (0.21.0.dev0).</p>\n<p>Thank you for your\
          \ help !</p>\n"
        raw: "I built a tool that generates inferences in a loop with Falcon 7B in\
          \ 16/8/4 bit. Weirdly, Falcon generates the first 3 inferences rapidly and\
          \ then it blocks and never returns the 4th inference when I use it in 8\
          \ or 4 bit. Plus, the problem persists when I change the set of instructions\
          \ I use to generate inferences. \n\nHere is the code I use to load the model\
          \ and generate inferences:\n\ndef load_falcon(weight_encoding):\n    model_path\
          \ = \"/falcon-7b-instruct\"\n    eight_bit = weight_encoding == \"8-bit\"\
          \n    four_bit = weight_encoding == \"4-bit\"\n\n    #creating a model \n\
          \    fmodel = AutoModelForCausalLM.from_pretrained(\n        model_path,\n\
          \        load_in_8bit = eight_bit,\n        load_in_4bit = four_bit,\n \
          \       trust_remote_code = True,\n        torch_dtype= torch.float16,\n\
          \        device_map = \"auto\")\n\n    fmodel.eval() \n    # fmodel.to(device)\n\
          \    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    gen_text\
          \ = transformers.pipeline(\n    model=fmodel, \n    tokenizer=tokenizer,\
          \ \n    task='text-generation', \n    return_full_text=False,     \n   \
          \ max_length=5000, \n    temperature=0.1, \n    top_p=0.75, #select from\
          \ top tokens whose probability adds up to 15%\n    top_k=40, #selecting\
          \ from top 0 tokens \n    repetition_penalty=1.9, #without a penalty, output\
          \ starts to repeat \n    do_sample=True, \n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id\n)\n    return gen_text, tokenizer\n\
          \ndef falcon_inference(instruction, gen_text):\n    response = gen_text(instruction)\n\
          \    texts = [ seq['generated_text'] for seq in response]\n    text = '\\\
          n'.join(texts)\n    return text\n\n\nConfiguration:\nCUDA Version: 12.1\
          \     \nNVIDIA GeForce RTX 4090 (24 GO VRAM)\nI have the latest versions\
          \ of bitsandbytes (0.39.0), transformers (4.31.0.dev0) and accelerate (0.21.0.dev0).\n\
          \nThank you for your help !\n\n\n"
        updatedAt: '2023-06-13T13:22:10.500Z'
      numEdits: 1
      reactions: []
    id: 64886d4f79c1481aba8dd660
    type: comment
  author: max0uu
  content: "I built a tool that generates inferences in a loop with Falcon 7B in 16/8/4\
    \ bit. Weirdly, Falcon generates the first 3 inferences rapidly and then it blocks\
    \ and never returns the 4th inference when I use it in 8 or 4 bit. Plus, the problem\
    \ persists when I change the set of instructions I use to generate inferences.\
    \ \n\nHere is the code I use to load the model and generate inferences:\n\ndef\
    \ load_falcon(weight_encoding):\n    model_path = \"/falcon-7b-instruct\"\n  \
    \  eight_bit = weight_encoding == \"8-bit\"\n    four_bit = weight_encoding ==\
    \ \"4-bit\"\n\n    #creating a model \n    fmodel = AutoModelForCausalLM.from_pretrained(\n\
    \        model_path,\n        load_in_8bit = eight_bit,\n        load_in_4bit\
    \ = four_bit,\n        trust_remote_code = True,\n        torch_dtype= torch.float16,\n\
    \        device_map = \"auto\")\n\n    fmodel.eval() \n    # fmodel.to(device)\n\
    \    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    gen_text = transformers.pipeline(\n\
    \    model=fmodel, \n    tokenizer=tokenizer, \n    task='text-generation', \n\
    \    return_full_text=False,     \n    max_length=5000, \n    temperature=0.1,\
    \ \n    top_p=0.75, #select from top tokens whose probability adds up to 15%\n\
    \    top_k=40, #selecting from top 0 tokens \n    repetition_penalty=1.9, #without\
    \ a penalty, output starts to repeat \n    do_sample=True, \n    num_return_sequences=1,\n\
    \    eos_token_id=tokenizer.eos_token_id\n)\n    return gen_text, tokenizer\n\n\
    def falcon_inference(instruction, gen_text):\n    response = gen_text(instruction)\n\
    \    texts = [ seq['generated_text'] for seq in response]\n    text = '\\n'.join(texts)\n\
    \    return text\n\n\nConfiguration:\nCUDA Version: 12.1     \nNVIDIA GeForce\
    \ RTX 4090 (24 GO VRAM)\nI have the latest versions of bitsandbytes (0.39.0),\
    \ transformers (4.31.0.dev0) and accelerate (0.21.0.dev0).\n\nThank you for your\
    \ help !\n\n\n"
  created_at: 2023-06-13 12:21:19+00:00
  edited: true
  hidden: false
  id: 64886d4f79c1481aba8dd660
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31d4056d85425d918f8c326c4e022dff.svg
      fullname: Michael O'Mahony
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: michaelomahony
      type: user
    createdAt: '2023-06-15T11:53:32.000Z'
    data:
      edited: false
      editors:
      - michaelomahony
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9121449589729309
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31d4056d85425d918f8c326c4e022dff.svg
          fullname: Michael O'Mahony
          isHf: false
          isPro: false
          name: michaelomahony
          type: user
        html: '<p>I am also having a similar issue where the 8-bit Falcon-7b-instruct
          model will not generate anything. Even the ''poem about Valencia'' example
          from their article (<a href="https://huggingface.co/blog/falcon#inference">https://huggingface.co/blog/falcon#inference</a>)
          does not work. There is no error code, it just returns the prompt with no
          generated text. The regular Falcon-7b-instruct model works fine for me but
          is very slow.</p>

          '
        raw: I am also having a similar issue where the 8-bit Falcon-7b-instruct model
          will not generate anything. Even the 'poem about Valencia' example from
          their article (https://huggingface.co/blog/falcon#inference) does not work.
          There is no error code, it just returns the prompt with no generated text.
          The regular Falcon-7b-instruct model works fine for me but is very slow.
        updatedAt: '2023-06-15T11:53:32.162Z'
      numEdits: 0
      reactions: []
    id: 648afbbc17d92d8cc9c3d854
    type: comment
  author: michaelomahony
  content: I am also having a similar issue where the 8-bit Falcon-7b-instruct model
    will not generate anything. Even the 'poem about Valencia' example from their
    article (https://huggingface.co/blog/falcon#inference) does not work. There is
    no error code, it just returns the prompt with no generated text. The regular
    Falcon-7b-instruct model works fine for me but is very slow.
  created_at: 2023-06-15 10:53:32+00:00
  edited: false
  hidden: false
  id: 648afbbc17d92d8cc9c3d854
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31d4056d85425d918f8c326c4e022dff.svg
      fullname: Michael O'Mahony
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: michaelomahony
      type: user
    createdAt: '2023-06-19T08:00:08.000Z'
    data:
      edited: true
      editors:
      - michaelomahony
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9515706300735474
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31d4056d85425d918f8c326c4e022dff.svg
          fullname: Michael O'Mahony
          isHf: false
          isPro: false
          name: michaelomahony
          type: user
        html: '<blockquote>

          <p>I am also having a similar issue where the 8-bit Falcon-7b-instruct model
          will not generate anything. Even the ''poem about Valencia'' example from
          their article (<a href="https://huggingface.co/blog/falcon#inference">https://huggingface.co/blog/falcon#inference</a>)
          does not work. There is no error code, it just returns the prompt with no
          generated text. The regular Falcon-7b-instruct model works fine for me but
          is very slow.</p>

          </blockquote>

          <p>My issue was with the 8-bit model which was loaded from locally saved
          files, when I load it from the hub it works fine. There must be some issue
          with how Huggingface saves the 8-bit model.</p>

          '
        raw: '> I am also having a similar issue where the 8-bit Falcon-7b-instruct
          model will not generate anything. Even the ''poem about Valencia'' example
          from their article (https://huggingface.co/blog/falcon#inference) does not
          work. There is no error code, it just returns the prompt with no generated
          text. The regular Falcon-7b-instruct model works fine for me but is very
          slow.


          My issue was with the 8-bit model which was loaded from locally saved files,
          when I load it from the hub it works fine. There must be some issue with
          how Huggingface saves the 8-bit model.'
        updatedAt: '2023-06-19T08:00:35.126Z'
      numEdits: 1
      reactions: []
    id: 64900b08d0b9f97fb206c392
    type: comment
  author: michaelomahony
  content: '> I am also having a similar issue where the 8-bit Falcon-7b-instruct
    model will not generate anything. Even the ''poem about Valencia'' example from
    their article (https://huggingface.co/blog/falcon#inference) does not work. There
    is no error code, it just returns the prompt with no generated text. The regular
    Falcon-7b-instruct model works fine for me but is very slow.


    My issue was with the 8-bit model which was loaded from locally saved files, when
    I load it from the hub it works fine. There must be some issue with how Huggingface
    saves the 8-bit model.'
  created_at: 2023-06-19 07:00:08+00:00
  edited: true
  hidden: false
  id: 64900b08d0b9f97fb206c392
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 31
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: '4th inference in a row does not work for Falcon7B in  8 or 4 bit '
