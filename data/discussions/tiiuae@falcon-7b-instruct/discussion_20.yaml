!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CalumPlays
conflicting_files: null
created_at: 2023-06-02 19:06:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5c4eb23594573d36a34ceb63de58d14.svg
      fullname: Calum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CalumPlays
      type: user
    createdAt: '2023-06-02T20:06:25.000Z'
    data:
      edited: true
      editors:
      - CalumPlays
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4667957127094269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5c4eb23594573d36a34ceb63de58d14.svg
          fullname: Calum
          isHf: false
          isPro: false
          name: CalumPlays
          type: user
        html: "<p>Hello all I keep getting this error everytime I run the example\
          \ python file on the page.</p>\n<p>Full log below:<br>~/falcon-chat$ python\
          \ falcon-small.py<br>Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 2/2 [00:18&lt;00:00,  9.36s/it]<br>The model 'RWForCausalLM'\
          \ is not supported for text-generation. Supported models are ['BartForCausalLM',\
          \ 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
          \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
          \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
          \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
          \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
          \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
          \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
          \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
          \ 'XLNetLMHeadModel', 'XmodForCausalLM'].<br>/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py:1255:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation\"\
          >https://huggingface.co/docs/transformers/main_classes/text_generation</a>)<br>\
          \  warnings.warn(<br>Setting <code>pad_token_id</code> to <code>eos_token_id</code>:11\
          \ for open-end generation.<br>Traceback (most recent call last):<br>  File\
          \ \"/home/cosmos/falcon-chat/falcon-small.py\", line 16, in <br>    sequences\
          \ = pipeline(<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 201, in <strong>call</strong><br>    return super().<strong>call</strong>(text_inputs,\
          \ **kwargs)<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1119, in <strong>call</strong><br>    return self.run_single(inputs,\
          \ preprocess_params, forward_params, postprocess_params)<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1126, in run_single<br>    model_outputs = self.forward(model_inputs,\
          \ **forward_params)<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1025, in forward<br>    model_outputs = self._forward(model_inputs,\
          \ **forward_params)<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 263, in _forward<br>    generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate<br>    return self.sample(<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample<br>    outputs = self(<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 753, in forward<br>    transformer_outputs = self.transformer(<br>\
          \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 648, in forward<br>    outputs = block(<br>  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 385, in forward<br>    attn_outputs = self.self_attention(<br>  File\
          \ \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 279, in forward<br>    attn_output = F.scaled_dot_product_attention(<br>RuntimeError:\
          \ CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling <code>cublasGemmStridedBatchedExFix(handle,\
          \ opa, opb, (int)m, (int)n, (int)k, (void*)&amp;falpha, a, CUDA_R_16BF,\
          \ (int)lda, stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&amp;fbeta,\
          \ c, CUDA_R_16BF, (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)</code></p>\n\
          <p>EDIT: I do have CUBLAS installed in my annaconda environment along with\
          \ cuDNN and cudatoolkit</p>\n<p>UPDATE: I fixed this by using float16 instead\
          \ of bfloat16.</p>\n"
        raw: "Hello all I keep getting this error everytime I run the example python\
          \ file on the page.\n\nFull log below:\n~/falcon-chat$ python falcon-small.py\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 2/2 [00:18<00:00,  9.36s/it]\nThe model 'RWForCausalLM' is not supported\
          \ for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
          \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
          \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
          \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
          \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
          \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
          \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
          \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
          \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
          \ 'XLNetLMHeadModel', 'XmodForCausalLM'].\n/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py:1255:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
          \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
          \ generation.\nTraceback (most recent call last):\n  File \"/home/cosmos/falcon-chat/falcon-small.py\"\
          , line 16, in <module>\n    sequences = pipeline(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 201, in __call__\n    return super().__call__(text_inputs, **kwargs)\n\
          \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1119, in __call__\n    return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1126, in run_single\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1025, in forward\n    model_outputs = self._forward(model_inputs,\
          \ **forward_params)\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 263, in _forward\n    generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\n    return self.sample(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\n    outputs = self(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 753, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 648, in forward\n    outputs = block(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 385, in forward\n    attn_outputs = self.self_attention(\n  File\
          \ \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
          , line 279, in forward\n    attn_output = F.scaled_dot_product_attention(\n\
          RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedExFix(handle,\
          \ opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda,\
          \ stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF,\
          \ (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\n\
          \nEDIT: I do have CUBLAS installed in my annaconda environment along with\
          \ cuDNN and cudatoolkit\n\nUPDATE: I fixed this by using float16 instead\
          \ of bfloat16."
        updatedAt: '2023-06-02T20:32:17.749Z'
      numEdits: 2
      reactions: []
    id: 647a4bc18de08112d71be68a
    type: comment
  author: CalumPlays
  content: "Hello all I keep getting this error everytime I run the example python\
    \ file on the page.\n\nFull log below:\n~/falcon-chat$ python falcon-small.py\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
    \ [00:18<00:00,  9.36s/it]\nThe model 'RWForCausalLM' is not supported for text-generation.\
    \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
    \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM',\
    \ 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM',\
    \ 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM',\
    \ 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel',\
    \ 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
    \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
    \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
    \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
    \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
    \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
    \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
    \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
    \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n\
    /home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py:1255:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use a generation configuration file\
    \ (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
    \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\
    Traceback (most recent call last):\n  File \"/home/cosmos/falcon-chat/falcon-small.py\"\
    , line 16, in <module>\n    sequences = pipeline(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
    , line 201, in __call__\n    return super().__call__(text_inputs, **kwargs)\n\
    \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1119, in __call__\n    return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1126, in run_single\n    model_outputs = self.forward(model_inputs, **forward_params)\n\
    \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1025, in forward\n    model_outputs = self._forward(model_inputs, **forward_params)\n\
    \  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
    , line 263, in _forward\n    generated_sequence = self.model.generate(input_ids=input_ids,\
    \ attention_mask=attention_mask, **generate_kwargs)\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1565, in generate\n    return self.sample(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2612, in sample\n    outputs = self(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
    , line 753, in forward\n    transformer_outputs = self.transformer(\n  File \"\
    /home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
    , line 648, in forward\n    outputs = block(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
    , line 385, in forward\n    attn_outputs = self.self_attention(\n  File \"/home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/cosmos/miniconda3/envs/ttd/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/cosmos/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/22225c3ac76bdddc1c6c44ebea0e3109468de29f/modelling_RW.py\"\
    , line 279, in forward\n    attn_output = F.scaled_dot_product_attention(\nRuntimeError:\
    \ CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedExFix(handle,\
    \ opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda,\
    \ stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc,\
    \ stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\n\nEDIT:\
    \ I do have CUBLAS installed in my annaconda environment along with cuDNN and\
    \ cudatoolkit\n\nUPDATE: I fixed this by using float16 instead of bfloat16."
  created_at: 2023-06-02 19:06:25+00:00
  edited: true
  hidden: false
  id: 647a4bc18de08112d71be68a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:15:46.000Z'
    data:
      status: closed
    id: 64833412823496a7c3110c48
    type: status-change
  author: FalconLLM
  created_at: 2023-06-09 13:15:46+00:00
  id: 64833412823496a7c3110c48
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a3b5b76fa700fd6172fc8319f00a9d.svg
      fullname: Roberto Congiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sardodazione
      type: user
    createdAt: '2023-12-07T00:10:11.000Z'
    data:
      edited: false
      editors:
      - sardodazione
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9439824223518372
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84a3b5b76fa700fd6172fc8319f00a9d.svg
          fullname: Roberto Congiu
          isHf: false
          isPro: false
          name: sardodazione
          type: user
        html: '<p>Changing bfloat to float worked for me too... thanks!</p>

          '
        raw: Changing bfloat to float worked for me too... thanks!
        updatedAt: '2023-12-07T00:10:11.233Z'
      numEdits: 0
      reactions: []
    id: 65710d638489a9ee97000bff
    type: comment
  author: sardodazione
  content: Changing bfloat to float worked for me too... thanks!
  created_at: 2023-12-07 00:10:11+00:00
  edited: false
  hidden: false
  id: 65710d638489a9ee97000bff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: closed
target_branch: null
title: 'CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedFx...'
