!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mrscoopers
conflicting_files: null
created_at: 2023-09-03 14:52:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648ce65ee4591d14d4024c65/RhOSbcaghgEcTvQ2csPf-.jpeg?w=200&h=200&f=face
      fullname: Evgeniya Sukhodolskaya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrscoopers
      type: user
    createdAt: '2023-09-03T15:52:12.000Z'
    data:
      edited: false
      editors:
      - mrscoopers
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4340406358242035
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648ce65ee4591d14d4024c65/RhOSbcaghgEcTvQ2csPf-.jpeg?w=200&h=200&f=face
          fullname: Evgeniya Sukhodolskaya
          isHf: false
          isPro: false
          name: mrscoopers
          type: user
        html: '<p>While using inference API, for me any alternation of ''num_beams''
          and ''num_return_sequences'' parameters does not change the output: it always
          returns the same (one) generated text.</p>

          <p>Could anybody explain to me, please, why so?</p>

          <p>For some models (such as gpt-2, both parameters work. For some (e.g.,
          bloom), only one does (e.g., num_beams)</p>

          <p>%%<br>import requests<br>headers = {''Content-type'': ''application/json'',
          "Authorization": f"Bearer hf_bearer"}</p>

          <p>def query_falcon(prompt):<br>    parameters = {''max_new_tokens'':25,
          ''early_stopping'':True, ''return_full_text'': False,<br>                  ''do_sample'':
          False, ''num_beams'':10, ''num_return_sequences'':2}<br>    options = {''use_cache'':
          False}<br>    payload = {''inputs'': prompt,<br>               ''parameters'':
          parameters,<br>               ''options'': options}<br>    data = json.dumps(payload)<br>    response
          = requests.request("POST",<br>                                "<a rel="nofollow"
          href="https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct&quot;">https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct"</a>,<br>                                headers=headers,<br>                                data=data)<br>    try:<br>        return
          json.loads(response.content.decode("utf-8"))<br>    except Exception as
          e:<br>        ...<br>        return ''Model error''<br>%%</p>

          '
        raw: "While using inference API, for me any alternation of 'num_beams' and\
          \ 'num_return_sequences' parameters does not change the output: it always\
          \ returns the same (one) generated text.\r\n\r\nCould anybody explain to\
          \ me, please, why so?\r\n\r\nFor some models (such as gpt-2, both parameters\
          \ work. For some (e.g., bloom), only one does (e.g., num_beams)\r\n\r\n\r\
          \n%%\r\nimport requests\r\nheaders = {'Content-type': 'application/json',\
          \ \"Authorization\": f\"Bearer hf_bearer\"}\r\n\r\ndef query_falcon(prompt):\r\
          \n    parameters = {'max_new_tokens':25, 'early_stopping':True, 'return_full_text':\
          \ False,\r\n                  'do_sample': False, 'num_beams':10, 'num_return_sequences':2}\
          \ \r\n    options = {'use_cache': False}\r\n    payload = {'inputs': prompt,\r\
          \n               'parameters': parameters,\r\n               'options':\
          \ options}\r\n    data = json.dumps(payload)\r\n    response = requests.request(\"\
          POST\",\r\n                                \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
          ,\r\n                                headers=headers,\r\n              \
          \                  data=data)\r\n    try:\r\n        return json.loads(response.content.decode(\"\
          utf-8\"))\r\n    except Exception as e:\r\n        ...\r\n        return\
          \ 'Model error'\r\n%%"
        updatedAt: '2023-09-03T15:52:12.003Z'
      numEdits: 0
      reactions: []
    id: 64f4abac84345e67f57e379d
    type: comment
  author: mrscoopers
  content: "While using inference API, for me any alternation of 'num_beams' and 'num_return_sequences'\
    \ parameters does not change the output: it always returns the same (one) generated\
    \ text.\r\n\r\nCould anybody explain to me, please, why so?\r\n\r\nFor some models\
    \ (such as gpt-2, both parameters work. For some (e.g., bloom), only one does\
    \ (e.g., num_beams)\r\n\r\n\r\n%%\r\nimport requests\r\nheaders = {'Content-type':\
    \ 'application/json', \"Authorization\": f\"Bearer hf_bearer\"}\r\n\r\ndef query_falcon(prompt):\r\
    \n    parameters = {'max_new_tokens':25, 'early_stopping':True, 'return_full_text':\
    \ False,\r\n                  'do_sample': False, 'num_beams':10, 'num_return_sequences':2}\
    \ \r\n    options = {'use_cache': False}\r\n    payload = {'inputs': prompt,\r\
    \n               'parameters': parameters,\r\n               'options': options}\r\
    \n    data = json.dumps(payload)\r\n    response = requests.request(\"POST\",\r\
    \n                                \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
    ,\r\n                                headers=headers,\r\n                    \
    \            data=data)\r\n    try:\r\n        return json.loads(response.content.decode(\"\
    utf-8\"))\r\n    except Exception as e:\r\n        ...\r\n        return 'Model\
    \ error'\r\n%%"
  created_at: 2023-09-03 14:52:12+00:00
  edited: false
  hidden: false
  id: 64f4abac84345e67f57e379d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 80
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: '''num_return_sequences'' & ''num_beams'' can''t be changed in inference API
  calls'
