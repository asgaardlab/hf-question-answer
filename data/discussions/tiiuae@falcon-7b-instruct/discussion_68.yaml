!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jurecucek
conflicting_files: null
created_at: 2023-07-21 10:45:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/678c04590ca8be9b9f9799a9b6f14683.svg
      fullname: jurecucek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jurecucek
      type: user
    createdAt: '2023-07-21T11:45:04.000Z'
    data:
      edited: false
      editors:
      - jurecucek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6293371319770813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/678c04590ca8be9b9f9799a9b6f14683.svg
          fullname: jurecucek
          isHf: false
          isPro: false
          name: jurecucek
          type: user
        html: '<p>So I have 8x 1080Ti in my machine. (also i5 and 16GB ram).<br>1080Ti
          is 11GB graphic card. Falcon 7B  is in 2 parts and it should work. Vicuna
          model works on this machine.<br>so this is my Python code.<br>from transformers
          import AutoTokenizer, AutoModelForCausalLM<br>import transformers<br>import
          torch</p>

          <p>model_name = "tiiuae/falcon-7b-instruct"</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to("cuda")  #
          Move the model to the GPU</p>

          <h1 id="wrap-the-model-with-dataparallel-to-use-multiple-gpus">Wrap the
          model with DataParallel to use multiple GPUs</h1>

          <p>if torch.cuda.device_count() &gt; 1:<br>    model = torch.nn.DataParallel(model)</p>

          <p>pipeline = transformers.pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map="auto",<br>)</p>

          <p>sequences = pipeline(<br>    "tell me a joke.",<br>    max_length=100,<br>    do_sample=True,<br>    top_k=10,<br>    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for
          seq in sequences:<br>    print(f"Result: {seq[''generated_text'']}")</p>

          <p>when I run it, I am just getting message "Killed".<br>thanks for help!</p>

          '
        raw: "So I have 8x 1080Ti in my machine. (also i5 and 16GB ram).\r\n1080Ti\
          \ is 11GB graphic card. Falcon 7B  is in 2 parts and it should work. Vicuna\
          \ model works on this machine.\r\nso this is my Python code.\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\n\
          import torch\r\n\r\nmodel_name = \"tiiuae/falcon-7b-instruct\"\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\r\n\
          model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(\"\
          cuda\")  # Move the model to the GPU\r\n\r\n# Wrap the model with DataParallel\
          \ to use multiple GPUs\r\nif torch.cuda.device_count() > 1:\r\n    model\
          \ = torch.nn.DataParallel(model)\r\n\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\",\r\n)\r\n\r\nsequences = pipeline(\r\n    \"tell me\
          \ a joke.\",\r\n    max_length=100,\r\n    do_sample=True,\r\n    top_k=10,\r\
          \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n\r\nwhen I run it, I am just getting message \"Killed\".\r\nthanks\
          \ for help!"
        updatedAt: '2023-07-21T11:45:04.336Z'
      numEdits: 0
      reactions: []
    id: 64ba6fc0b8b57f83125cbc3a
    type: comment
  author: jurecucek
  content: "So I have 8x 1080Ti in my machine. (also i5 and 16GB ram).\r\n1080Ti is\
    \ 11GB graphic card. Falcon 7B  is in 2 parts and it should work. Vicuna model\
    \ works on this machine.\r\nso this is my Python code.\r\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\nimport torch\r\
    \n\r\nmodel_name = \"tiiuae/falcon-7b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name,\
    \ trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
    \ trust_remote_code=True).to(\"cuda\")  # Move the model to the GPU\r\n\r\n# Wrap\
    \ the model with DataParallel to use multiple GPUs\r\nif torch.cuda.device_count()\
    \ > 1:\r\n    model = torch.nn.DataParallel(model)\r\n\r\npipeline = transformers.pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n    device_map=\"\
    auto\",\r\n)\r\n\r\nsequences = pipeline(\r\n    \"tell me a joke.\",\r\n    max_length=100,\r\
    \n    do_sample=True,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
    \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
    )\r\n\r\nwhen I run it, I am just getting message \"Killed\".\r\nthanks for help!"
  created_at: 2023-07-21 10:45:04+00:00
  edited: false
  hidden: false
  id: 64ba6fc0b8b57f83125cbc3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/678c04590ca8be9b9f9799a9b6f14683.svg
      fullname: jurecucek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jurecucek
      type: user
    createdAt: '2023-07-21T12:39:59.000Z'
    data:
      edited: false
      editors:
      - jurecucek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9033985137939453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/678c04590ca8be9b9f9799a9b6f14683.svg
          fullname: jurecucek
          isHf: false
          isPro: false
          name: jurecucek
          type: user
        html: '<p>Falcon 7B does load using fastchat. So i guess my code is wrong
          :D</p>

          '
        raw: 'Falcon 7B does load using fastchat. So i guess my code is wrong :D

          '
        updatedAt: '2023-07-21T12:39:59.607Z'
      numEdits: 0
      reactions: []
    id: 64ba7c9f30533c2492f2394e
    type: comment
  author: jurecucek
  content: 'Falcon 7B does load using fastchat. So i guess my code is wrong :D

    '
  created_at: 2023-07-21 11:39:59+00:00
  edited: false
  hidden: false
  id: 64ba7c9f30533c2492f2394e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/SdIbbDIuuSbHR46ULmFuf.png?w=200&h=200&f=face
      fullname: Adam Englander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adam-zettafi
      type: user
    createdAt: '2023-07-28T15:04:10.000Z'
    data:
      edited: false
      editors:
      - adam-zettafi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9699193835258484
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/SdIbbDIuuSbHR46ULmFuf.png?w=200&h=200&f=face
          fullname: Adam Englander
          isHf: false
          isPro: false
          name: adam-zettafi
          type: user
        html: '<p>In my experience, killed usually means it used too much RAM and
          was shut down. Are there specific reasons you are providing so many configuration
          options, or was this from a code snippet? My best experiences with Hugging
          Face libraries have been when starting with only the bare necessities to
          get it running and then modifying from there for tuning. I suggest removing
          all the extras and letting the Transformer and Pipeline figure it out. You
          can always add more for tuning later.</p>

          '
        raw: In my experience, killed usually means it used too much RAM and was shut
          down. Are there specific reasons you are providing so many configuration
          options, or was this from a code snippet? My best experiences with Hugging
          Face libraries have been when starting with only the bare necessities to
          get it running and then modifying from there for tuning. I suggest removing
          all the extras and letting the Transformer and Pipeline figure it out. You
          can always add more for tuning later.
        updatedAt: '2023-07-28T15:04:10.896Z'
      numEdits: 0
      reactions: []
    id: 64c3d8ea5e5bc55a92dc0d5a
    type: comment
  author: adam-zettafi
  content: In my experience, killed usually means it used too much RAM and was shut
    down. Are there specific reasons you are providing so many configuration options,
    or was this from a code snippet? My best experiences with Hugging Face libraries
    have been when starting with only the bare necessities to get it running and then
    modifying from there for tuning. I suggest removing all the extras and letting
    the Transformer and Pipeline figure it out. You can always add more for tuning
    later.
  created_at: 2023-07-28 14:04:10+00:00
  edited: false
  hidden: false
  id: 64c3d8ea5e5bc55a92dc0d5a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 68
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: Getting message Killed when loading on multi-gpu
