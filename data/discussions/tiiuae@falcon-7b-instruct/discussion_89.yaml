!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vkajjam
conflicting_files: null
created_at: 2023-09-30 00:58:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a4a0c90b8593cc0c0dba8fb81064304.svg
      fullname: vishal kajjam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vkajjam
      type: user
    createdAt: '2023-09-30T01:58:22.000Z'
    data:
      edited: false
      editors:
      - vkajjam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6247681379318237
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a4a0c90b8593cc0c0dba8fb81064304.svg
          fullname: vishal kajjam
          isHf: false
          isPro: false
          name: vkajjam
          type: user
        html: "<p>Hi All.</p>\n<p>I am getting the following Could not locate the\
          \ configuration_RW.py inside tiiuae/falcon-7b-instruct. error since this\
          \ morning. From my understanding it is related to the following <a href=\"\
          https://huggingface.co/tiiuae/falcon-7b-instruct/commit/cf4b3c42ce2fdfe24f753f0f0d179202fea59c99\"\
          >change</a> to the Falcon model.</p>\n<p>Usecase<br>I have fine tuned the\
          \ falcon-7b-instruct model using SFT and use the following snippet to load\
          \ the model:</p>\n<pre><code>sft_model_16 = AutoModelForCausalLM.from_pretrained(\n\
          \    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True,\n\
          \    torch_dtype=torch.float16,\n)\n</code></pre>\n<p>The <code>model_dir</code>\
          \ includes the base model along with the adaptor. </p>\n<p>I am using <code>4.27.4</code>\
          \ of the transformers library. Prior to the latest commit, the instantiated\
          \ model was of the <code>RWForCausalLM</code> type.</p>\n<pre><code>RWForCausalLM(\n\
          \  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n\
          \    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n        (input_layernorm):\
          \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n        (self_attention):\
          \ Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value):\
          \ Linear(in_features=4544, out_features=4672, bias=False)\n          (dense):\
          \ Linear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout):\
          \ Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n      \
          \    (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n\
          \          (act): GELU(approximate='none')\n          (dense_4h_to_h): Linear(in_features=18176,\
          \ out_features=4544, bias=False)\n        )\n      )\n    )\n    (ln_f):\
          \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head):\
          \ Linear(in_features=4544, out_features=65024, bias=False)\n)\n</code></pre>\n\
          <p>After the latest commit, the instantiated model was of the <code>FalconForCausalLM</code>\
          \ type.</p>\n<pre><code>FalconForCausalLM(\n  (transformer): FalconModel(\n\
          \    (word_embeddings): Embedding(65024, 4544)\n    (h): ModuleList(\n \
          \     (0-31): 32 x FalconDecoderLayer(\n        (self_attention): FalconAttention(\n\
          \          (maybe_rotary): FalconRotaryEmbedding()\n          (query_key_value):\
          \ FalconLinear(in_features=4544, out_features=4672, bias=False)\n      \
          \    (dense): FalconLinear(in_features=4544, out_features=4544, bias=False)\n\
          \          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n\
          \        (mlp): FalconMLP(\n          (dense_h_to_4h): FalconLinear(in_features=4544,\
          \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
          \          (dense_4h_to_h): FalconLinear(in_features=18176, out_features=4544,\
          \ bias=False)\n        )\n        (input_layernorm): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\n)\n</code></pre>\n<p>What is the recommended\
          \ way to resolve this error and safe guarding against this scenario in the\
          \ future? </p>\n<p>Thanks in advance.</p>\n"
        raw: "Hi All.\r\n\r\nI am getting the following Could not locate the configuration_RW.py\
          \ inside tiiuae/falcon-7b-instruct. error since this morning. From my understanding\
          \ it is related to the following [change](https://huggingface.co/tiiuae/falcon-7b-instruct/commit/cf4b3c42ce2fdfe24f753f0f0d179202fea59c99)\
          \ to the Falcon model.\r\n\r\nUsecase\r\nI have fine tuned the falcon-7b-instruct\
          \ model using SFT and use the following snippet to load the model:\r\n\r\
          \n```\r\nsft_model_16 = AutoModelForCausalLM.from_pretrained(\r\n    model_dir,\r\
          \n    device_map=\"auto\",\r\n    trust_remote_code=True,\r\n    torch_dtype=torch.float16,\r\
          \n)\r\n```\r\n\r\nThe `model_dir` includes the base model along with the\
          \ adaptor. \r\n\r\nI am using `4.27.4` of the transformers library. Prior\
          \ to the latest commit, the instantiated model was of the `RWForCausalLM`\
          \ type.\r\n\r\n```\r\nRWForCausalLM(\r\n  (transformer): RWModel(\r\n  \
          \  (word_embeddings): Embedding(65024, 4544)\r\n    (h): ModuleList(\r\n\
          \      (0-31): 32 x DecoderLayer(\r\n        (input_layernorm): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\r\n        (self_attention): Attention(\r\
          \n          (maybe_rotary): RotaryEmbedding()\r\n          (query_key_value):\
          \ Linear(in_features=4544, out_features=4672, bias=False)\r\n          (dense):\
          \ Linear(in_features=4544, out_features=4544, bias=False)\r\n          (attention_dropout):\
          \ Dropout(p=0.0, inplace=False)\r\n        )\r\n        (mlp): MLP(\r\n\
          \          (dense_h_to_4h): Linear(in_features=4544, out_features=18176,\
          \ bias=False)\r\n          (act): GELU(approximate='none')\r\n         \
          \ (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\r\
          \n        )\r\n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,), eps=1e-05,\
          \ elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\r\n)\r\n```\r\n\r\nAfter the latest commit,\
          \ the instantiated model was of the `FalconForCausalLM` type.\r\n\r\n```\r\
          \nFalconForCausalLM(\r\n  (transformer): FalconModel(\r\n    (word_embeddings):\
          \ Embedding(65024, 4544)\r\n    (h): ModuleList(\r\n      (0-31): 32 x FalconDecoderLayer(\r\
          \n        (self_attention): FalconAttention(\r\n          (maybe_rotary):\
          \ FalconRotaryEmbedding()\r\n          (query_key_value): FalconLinear(in_features=4544,\
          \ out_features=4672, bias=False)\r\n          (dense): FalconLinear(in_features=4544,\
          \ out_features=4544, bias=False)\r\n          (attention_dropout): Dropout(p=0.0,\
          \ inplace=False)\r\n        )\r\n        (mlp): FalconMLP(\r\n         \
          \ (dense_h_to_4h): FalconLinear(in_features=4544, out_features=18176, bias=False)\r\
          \n          (act): GELU(approximate='none')\r\n          (dense_4h_to_h):\
          \ FalconLinear(in_features=18176, out_features=4544, bias=False)\r\n   \
          \     )\r\n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
          \n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
          \n  )\r\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\r\
          \n)\r\n```\r\n\r\nWhat is the recommended way to resolve this error and\
          \ safe guarding against this scenario in the future? \r\n\r\nThanks in advance.\r\
          \n"
        updatedAt: '2023-09-30T01:58:22.648Z'
      numEdits: 0
      reactions: []
    id: 651780be8876a95d5a201367
    type: comment
  author: vkajjam
  content: "Hi All.\r\n\r\nI am getting the following Could not locate the configuration_RW.py\
    \ inside tiiuae/falcon-7b-instruct. error since this morning. From my understanding\
    \ it is related to the following [change](https://huggingface.co/tiiuae/falcon-7b-instruct/commit/cf4b3c42ce2fdfe24f753f0f0d179202fea59c99)\
    \ to the Falcon model.\r\n\r\nUsecase\r\nI have fine tuned the falcon-7b-instruct\
    \ model using SFT and use the following snippet to load the model:\r\n\r\n```\r\
    \nsft_model_16 = AutoModelForCausalLM.from_pretrained(\r\n    model_dir,\r\n \
    \   device_map=\"auto\",\r\n    trust_remote_code=True,\r\n    torch_dtype=torch.float16,\r\
    \n)\r\n```\r\n\r\nThe `model_dir` includes the base model along with the adaptor.\
    \ \r\n\r\nI am using `4.27.4` of the transformers library. Prior to the latest\
    \ commit, the instantiated model was of the `RWForCausalLM` type.\r\n\r\n```\r\
    \nRWForCausalLM(\r\n  (transformer): RWModel(\r\n    (word_embeddings): Embedding(65024,\
    \ 4544)\r\n    (h): ModuleList(\r\n      (0-31): 32 x DecoderLayer(\r\n      \
    \  (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
    \n        (self_attention): Attention(\r\n          (maybe_rotary): RotaryEmbedding()\r\
    \n          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\r\
    \n          (dense): Linear(in_features=4544, out_features=4544, bias=False)\r\
    \n          (attention_dropout): Dropout(p=0.0, inplace=False)\r\n        )\r\n\
    \        (mlp): MLP(\r\n          (dense_h_to_4h): Linear(in_features=4544, out_features=18176,\
    \ bias=False)\r\n          (act): GELU(approximate='none')\r\n          (dense_4h_to_h):\
    \ Linear(in_features=18176, out_features=4544, bias=False)\r\n        )\r\n  \
    \    )\r\n    )\r\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
    \n  )\r\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\r\
    \n)\r\n```\r\n\r\nAfter the latest commit, the instantiated model was of the `FalconForCausalLM`\
    \ type.\r\n\r\n```\r\nFalconForCausalLM(\r\n  (transformer): FalconModel(\r\n\
    \    (word_embeddings): Embedding(65024, 4544)\r\n    (h): ModuleList(\r\n   \
    \   (0-31): 32 x FalconDecoderLayer(\r\n        (self_attention): FalconAttention(\r\
    \n          (maybe_rotary): FalconRotaryEmbedding()\r\n          (query_key_value):\
    \ FalconLinear(in_features=4544, out_features=4672, bias=False)\r\n          (dense):\
    \ FalconLinear(in_features=4544, out_features=4544, bias=False)\r\n          (attention_dropout):\
    \ Dropout(p=0.0, inplace=False)\r\n        )\r\n        (mlp): FalconMLP(\r\n\
    \          (dense_h_to_4h): FalconLinear(in_features=4544, out_features=18176,\
    \ bias=False)\r\n          (act): GELU(approximate='none')\r\n          (dense_4h_to_h):\
    \ FalconLinear(in_features=18176, out_features=4544, bias=False)\r\n        )\r\
    \n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
    \n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
    \n  )\r\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\r\
    \n)\r\n```\r\n\r\nWhat is the recommended way to resolve this error and safe guarding\
    \ against this scenario in the future? \r\n\r\nThanks in advance.\r\n"
  created_at: 2023-09-30 00:58:22+00:00
  edited: false
  hidden: false
  id: 651780be8876a95d5a201367
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f61a5d91ce77656fa36c3576cf73528.svg
      fullname: Zak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zak
      type: user
    createdAt: '2023-10-02T06:19:35.000Z'
    data:
      edited: false
      editors:
      - Zak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9779930710792542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f61a5d91ce77656fa36c3576cf73528.svg
          fullname: Zak
          isHf: false
          isPro: false
          name: Zak
          type: user
        html: '<p>Same issue for me.<br>it seems that the Falcon team has modified
          the code and removed the configuration_RW.py file</p>

          '
        raw: 'Same issue for me.

          it seems that the Falcon team has modified the code and removed the configuration_RW.py
          file'
        updatedAt: '2023-10-02T06:19:35.267Z'
      numEdits: 0
      reactions: []
    id: 651a60f7107446b24cd0ae40
    type: comment
  author: Zak
  content: 'Same issue for me.

    it seems that the Falcon team has modified the code and removed the configuration_RW.py
    file'
  created_at: 2023-10-02 05:19:35+00:00
  edited: false
  hidden: false
  id: 651a60f7107446b24cd0ae40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824c22e393d4765737b778cf30ade0b.svg
      fullname: Victor Ashioya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ashioyajotham
      type: user
    createdAt: '2023-10-05T09:53:14.000Z'
    data:
      edited: false
      editors:
      - ashioyajotham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8274878263473511
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824c22e393d4765737b778cf30ade0b.svg
          fullname: Victor Ashioya
          isHf: false
          isPro: false
          name: ashioyajotham
          type: user
        html: '<p>Same issue. Welp!</p>

          '
        raw: Same issue. Welp!
        updatedAt: '2023-10-05T09:53:14.856Z'
      numEdits: 0
      reactions: []
    id: 651e878a606211cf9446d222
    type: comment
  author: ashioyajotham
  content: Same issue. Welp!
  created_at: 2023-10-05 08:53:14+00:00
  edited: false
  hidden: false
  id: 651e878a606211cf9446d222
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824c22e393d4765737b778cf30ade0b.svg
      fullname: Victor Ashioya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ashioyajotham
      type: user
    createdAt: '2023-10-05T09:57:28.000Z'
    data:
      edited: true
      editors:
      - ashioyajotham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5087892413139343
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824c22e393d4765737b778cf30ade0b.svg
          fullname: Victor Ashioya
          isHf: false
          isPro: false
          name: ashioyajotham
          type: user
        html: '<p>Using FalconForCasualLm seems to be the solution <a href="https://huggingface.co/tiiuae/falcon-7b/discussions/60#651988650e3a5553d4aac013">https://huggingface.co/tiiuae/falcon-7b/discussions/60#651988650e3a5553d4aac013</a></p>

          '
        raw: Using FalconForCasualLm seems to be the solution https://huggingface.co/tiiuae/falcon-7b/discussions/60#651988650e3a5553d4aac013
        updatedAt: '2023-10-05T09:58:01.708Z'
      numEdits: 1
      reactions: []
    id: 651e8888567de56557e2b6fb
    type: comment
  author: ashioyajotham
  content: Using FalconForCasualLm seems to be the solution https://huggingface.co/tiiuae/falcon-7b/discussions/60#651988650e3a5553d4aac013
  created_at: 2023-10-05 08:57:28+00:00
  edited: true
  hidden: false
  id: 651e8888567de56557e2b6fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4a4a0c90b8593cc0c0dba8fb81064304.svg
      fullname: vishal kajjam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vkajjam
      type: user
    createdAt: '2023-10-06T02:27:32.000Z'
    data:
      status: closed
    id: 651f70949b25c6baa48d1aac
    type: status-change
  author: vkajjam
  created_at: 2023-10-06 01:27:32+00:00
  id: 651f70949b25c6baa48d1aac
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 89
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Could not locate the configuration_RW.py inside tiiuae/falcon-7b-instruct.
