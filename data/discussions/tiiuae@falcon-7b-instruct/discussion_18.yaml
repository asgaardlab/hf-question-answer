!!python/object:huggingface_hub.community.DiscussionWithDetails
author: domid10
conflicting_files: null
created_at: 2023-06-02 06:19:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e713304c0549dced6a808021444d00e.svg
      fullname: Dom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: domid10
      type: user
    createdAt: '2023-06-02T07:19:26.000Z'
    data:
      edited: false
      editors:
      - domid10
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e713304c0549dced6a808021444d00e.svg
          fullname: Dom
          isHf: false
          isPro: false
          name: domid10
          type: user
        html: "<p>The reply always seems to be under 70 characters. Even when setting\
          \ a higher max_length. Any ideas?<br>ex reply:<br>\"Life is a journey, a\
          \ path we must take.<br>To find our way, we\"</p>\n<p>from langchain import\
          \ PromptTemplate, HuggingFaceHub, LLMChain<br>from dotenv import load_dotenv<br>load_dotenv()</p>\n\
          <p>template = \"\"\"Question: {question}<br>Answer: Let's think step by\
          \ step.\"\"\"<br>prompt = PromptTemplate(template=template, input_variables=[\"\
          question\"])</p>\n<p>llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\"\
          , model_kwargs={\"temperature\":0.1, \"max_length\":2000,})<br>llm_chain\
          \ = LLMChain(prompt=prompt, llm=llm)</p>\n<p>question = \"Write a poem about\
          \ life\"<br>print(question)<br>print('\u27A1\uFE0F ', llm_chain.run(question))</p>\n"
        raw: "The reply always seems to be under 70 characters. Even when setting\
          \ a higher max_length. Any ideas?\r\nex reply:\r\n\"Life is a journey, a\
          \ path we must take.\r\nTo find our way, we\"\r\n\r\nfrom langchain import\
          \ PromptTemplate, HuggingFaceHub, LLMChain\r\nfrom dotenv import load_dotenv\r\
          \nload_dotenv()\r\n\r\ntemplate = \"\"\"Question: {question}\r\nAnswer:\
          \ Let's think step by step.\"\"\"\r\nprompt = PromptTemplate(template=template,\
          \ input_variables=[\"question\"])\r\n\r\nllm = HuggingFaceHub(repo_id=\"\
          tiiuae/falcon-7b-instruct\", model_kwargs={\"temperature\":0.1, \"max_length\"\
          :2000,})\r\nllm_chain = LLMChain(prompt=prompt, llm=llm)\r\n\r\nquestion\
          \ = \"Write a poem about life\"\r\nprint(question)\r\nprint('\u27A1\uFE0F\
          \ ', llm_chain.run(question))\r\n"
        updatedAt: '2023-06-02T07:19:26.452Z'
      numEdits: 0
      reactions: []
    id: 647997fe7ca7c879b8431105
    type: comment
  author: domid10
  content: "The reply always seems to be under 70 characters. Even when setting a\
    \ higher max_length. Any ideas?\r\nex reply:\r\n\"Life is a journey, a path we\
    \ must take.\r\nTo find our way, we\"\r\n\r\nfrom langchain import PromptTemplate,\
    \ HuggingFaceHub, LLMChain\r\nfrom dotenv import load_dotenv\r\nload_dotenv()\r\
    \n\r\ntemplate = \"\"\"Question: {question}\r\nAnswer: Let's think step by step.\"\
    \"\"\r\nprompt = PromptTemplate(template=template, input_variables=[\"question\"\
    ])\r\n\r\nllm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"\
    temperature\":0.1, \"max_length\":2000,})\r\nllm_chain = LLMChain(prompt=prompt,\
    \ llm=llm)\r\n\r\nquestion = \"Write a poem about life\"\r\nprint(question)\r\n\
    print('\u27A1\uFE0F ', llm_chain.run(question))\r\n"
  created_at: 2023-06-02 06:19:26+00:00
  edited: false
  hidden: false
  id: 647997fe7ca7c879b8431105
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65d1630cdff7494bbbfe4e3e1338e6ce.svg
      fullname: Keno Teppris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aditunoe
      type: user
    createdAt: '2023-06-12T16:06:29.000Z'
    data:
      edited: false
      editors:
      - aditunoe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9744073152542114
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65d1630cdff7494bbbfe4e3e1338e6ce.svg
          fullname: Keno Teppris
          isHf: false
          isPro: false
          name: aditunoe
          type: user
        html: '<p>I have the same issue, at least in combination with LangChain the
          mdel tends to only ouput a few Tokens and than just stops in the middle
          of the sentence.</p>

          <p>Would be nice to know if we are just doing something wrong or its just
          the way this model works?</p>

          '
        raw: 'I have the same issue, at least in combination with LangChain the mdel
          tends to only ouput a few Tokens and than just stops in the middle of the
          sentence.


          Would be nice to know if we are just doing something wrong or its just the
          way this model works?'
        updatedAt: '2023-06-12T16:06:29.173Z'
      numEdits: 0
      reactions: []
    id: 6487428526add86d79fb946d
    type: comment
  author: aditunoe
  content: 'I have the same issue, at least in combination with LangChain the mdel
    tends to only ouput a few Tokens and than just stops in the middle of the sentence.


    Would be nice to know if we are just doing something wrong or its just the way
    this model works?'
  created_at: 2023-06-12 15:06:29+00:00
  edited: false
  hidden: false
  id: 6487428526add86d79fb946d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65d1630cdff7494bbbfe4e3e1338e6ce.svg
      fullname: Keno Teppris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aditunoe
      type: user
    createdAt: '2023-06-13T13:58:55.000Z'
    data:
      edited: false
      editors:
      - aditunoe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5832358002662659
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65d1630cdff7494bbbfe4e3e1338e6ce.svg
          fullname: Keno Teppris
          isHf: false
          isPro: false
          name: aditunoe
          type: user
        html: "<p>I found our mistake. <span data-props=\"{&quot;user&quot;:&quot;domid10&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/domid10\"\
          >@<span class=\"underline\">domid10</span></a></span>\n\n\t</span></span>\
          \  you need to add max_new_tokens and set it higher to get better results.<br>Example:</p>\n\
          <pre><code class=\"language-python\">llm = HuggingFaceEndpoint(\n      \
          \      endpoint_url= <span class=\"hljs-string\">\"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
          </span>,\n            huggingfacehub_api_token=HUGGINFACE_KEY,\n       \
          \     task=<span class=\"hljs-string\">\"text-generation\"</span>,\n   \
          \         model_kwargs = {\n                <span class=\"hljs-string\"\
          >\"temperature\"</span>:<span class=\"hljs-number\">0.2</span>,\n      \
          \          <span class=\"hljs-string\">\"max_new_tokens\"</span>:<span class=\"\
          hljs-number\">400</span>,\n                <span class=\"hljs-string\">\"\
          num_return_sequences\"</span>:<span class=\"hljs-number\">1</span>\n   \
          \         }\n        )\n</code></pre>\n"
        raw: "I found our mistake. @domid10  you need to add max_new_tokens and set\
          \ it higher to get better results.\nExample:\n```python\nllm = HuggingFaceEndpoint(\n\
          \            endpoint_url= \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
          ,\n            huggingfacehub_api_token=HUGGINFACE_KEY,\n            task=\"\
          text-generation\",\n            model_kwargs = {\n                \"temperature\"\
          :0.2,\n                \"max_new_tokens\":400,\n                \"num_return_sequences\"\
          :1\n            }\n        )\n```"
        updatedAt: '2023-06-13T13:58:55.400Z'
      numEdits: 0
      reactions: []
    id: 6488761fa834dc7a2b27ac9d
    type: comment
  author: aditunoe
  content: "I found our mistake. @domid10  you need to add max_new_tokens and set\
    \ it higher to get better results.\nExample:\n```python\nllm = HuggingFaceEndpoint(\n\
    \            endpoint_url= \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\
    ,\n            huggingfacehub_api_token=HUGGINFACE_KEY,\n            task=\"text-generation\"\
    ,\n            model_kwargs = {\n                \"temperature\":0.2,\n      \
    \          \"max_new_tokens\":400,\n                \"num_return_sequences\":1\n\
    \            }\n        )\n```"
  created_at: 2023-06-13 12:58:55+00:00
  edited: false
  hidden: false
  id: 6488761fa834dc7a2b27ac9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
      fullname: Muhammad Ajmal Siddiqui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajmalsiddiqui
      type: user
    createdAt: '2023-06-13T15:36:54.000Z'
    data:
      edited: true
      editors:
      - ajmalsiddiqui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7887667417526245
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
          fullname: Muhammad Ajmal Siddiqui
          isHf: false
          isPro: false
          name: ajmalsiddiqui
          type: user
        html: '<p>Hello All, I am interested to know falcon performance benchmarking
          on A100 and T4. I will be thankful if someone can share the inference statistics.<br>a)
          GPU type<br>b) Average inference time per request</p>

          '
        raw: 'Hello All, I am interested to know falcon performance benchmarking on
          A100 and T4. I will be thankful if someone can share the inference statistics.

          a) GPU type

          b) Average inference time per request'
        updatedAt: '2023-06-13T15:37:40.702Z'
      numEdits: 1
      reactions: []
    id: 64888d16a834dc7a2b2debe7
    type: comment
  author: ajmalsiddiqui
  content: 'Hello All, I am interested to know falcon performance benchmarking on
    A100 and T4. I will be thankful if someone can share the inference statistics.

    a) GPU type

    b) Average inference time per request'
  created_at: 2023-06-13 14:36:54+00:00
  edited: true
  hidden: false
  id: 64888d16a834dc7a2b2debe7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e713304c0549dced6a808021444d00e.svg
      fullname: Dom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: domid10
      type: user
    createdAt: '2023-06-15T12:04:21.000Z'
    data:
      edited: false
      editors:
      - domid10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9580837488174438
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e713304c0549dced6a808021444d00e.svg
          fullname: Dom
          isHf: false
          isPro: false
          name: domid10
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aditunoe&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aditunoe\">@<span class=\"\
          underline\">aditunoe</span></a></span>\n\n\t</span></span> Thank you! This\
          \ should really be added to the docs.</p>\n"
        raw: '@aditunoe Thank you! This should really be added to the docs.'
        updatedAt: '2023-06-15T12:04:21.286Z'
      numEdits: 0
      reactions: []
    id: 648afe452d8d81db5328d119
    type: comment
  author: domid10
  content: '@aditunoe Thank you! This should really be added to the docs.'
  created_at: 2023-06-15 11:04:21+00:00
  edited: false
  hidden: false
  id: 648afe452d8d81db5328d119
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: tiiuae/falcon-7b-instruct
repo_type: model
status: open
target_branch: null
title: max_length not working?
