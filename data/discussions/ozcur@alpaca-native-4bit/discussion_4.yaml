!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joncc
conflicting_files: null
created_at: 2023-07-10 14:31:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cdd3e80c173b07db2621329b8210d3c6.svg
      fullname: Jon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joncc
      type: user
    createdAt: '2023-07-10T15:31:08.000Z'
    data:
      edited: false
      editors:
      - joncc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.70691978931427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cdd3e80c173b07db2621329b8210d3c6.svg
          fullname: Jon
          isHf: false
          isPro: false
          name: joncc
          type: user
        html: '<p>Newbie question - how do I load the tokenizer and ".pt" model into
          a Huggingface pipeline using the HF API?</p>

          <p>I''ve been trying stuff like "</p>

          <p>model = LlamaForCausalLM.from_pretrained("./alpaca7b-4bit.pt")</p>

          <p>and </p>

          <p>tokenizer = LlamaTokenizer.from_pretrained("./tokenizer.model")</p>

          <p>But no success</p>

          '
        raw: "Newbie question - how do I load the tokenizer and \".pt\" model into\
          \ a Huggingface pipeline using the HF API?\r\n\r\nI've been trying stuff\
          \ like \"\r\n\r\nmodel = LlamaForCausalLM.from_pretrained(\"./alpaca7b-4bit.pt\"\
          )\r\n\r\nand \r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"./tokenizer.model\"\
          )\r\n\r\nBut no success\r\n"
        updatedAt: '2023-07-10T15:31:08.354Z'
      numEdits: 0
      reactions: []
    id: 64ac243cc08741fc97c0c15a
    type: comment
  author: joncc
  content: "Newbie question - how do I load the tokenizer and \".pt\" model into a\
    \ Huggingface pipeline using the HF API?\r\n\r\nI've been trying stuff like \"\
    \r\n\r\nmodel = LlamaForCausalLM.from_pretrained(\"./alpaca7b-4bit.pt\")\r\n\r\
    \nand \r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"./tokenizer.model\"\
    )\r\n\r\nBut no success\r\n"
  created_at: 2023-07-10 14:31:08+00:00
  edited: false
  hidden: false
  id: 64ac243cc08741fc97c0c15a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
      fullname: ozcur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ozcur
      type: user
    createdAt: '2023-07-12T15:42:36.000Z'
    data:
      edited: false
      editors:
      - ozcur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9848594069480896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
          fullname: ozcur
          isHf: false
          isPro: false
          name: ozcur
          type: user
        html: '<p>This is a very old quant at this point, I don''t recommend using
          it.  Take a look at TheBloke''s work isntead.</p>

          '
        raw: This is a very old quant at this point, I don't recommend using it.  Take
          a look at TheBloke's work isntead.
        updatedAt: '2023-07-12T15:42:36.195Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64aec9ec03246ffd04b53b9c
    id: 64aec9ec03246ffd04b53b99
    type: comment
  author: ozcur
  content: This is a very old quant at this point, I don't recommend using it.  Take
    a look at TheBloke's work isntead.
  created_at: 2023-07-12 14:42:36+00:00
  edited: false
  hidden: false
  id: 64aec9ec03246ffd04b53b99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
      fullname: ozcur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ozcur
      type: user
    createdAt: '2023-07-12T15:42:36.000Z'
    data:
      status: closed
    id: 64aec9ec03246ffd04b53b9c
    type: status-change
  author: ozcur
  created_at: 2023-07-12 14:42:36+00:00
  id: 64aec9ec03246ffd04b53b9c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cdd3e80c173b07db2621329b8210d3c6.svg
      fullname: Jon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joncc
      type: user
    createdAt: '2023-07-12T15:45:45.000Z'
    data:
      edited: false
      editors:
      - joncc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445480108261108
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cdd3e80c173b07db2621329b8210d3c6.svg
          fullname: Jon
          isHf: false
          isPro: false
          name: joncc
          type: user
        html: '<p>Thanks for the reply</p>

          '
        raw: Thanks for the reply
        updatedAt: '2023-07-12T15:45:45.006Z'
      numEdits: 0
      reactions: []
    id: 64aecaa99135f79b0912a44b
    type: comment
  author: joncc
  content: Thanks for the reply
  created_at: 2023-07-12 14:45:45+00:00
  edited: false
  hidden: false
  id: 64aecaa99135f79b0912a44b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: ozcur/alpaca-native-4bit
repo_type: model
status: closed
target_branch: null
title: Use this inside Hugging face API
