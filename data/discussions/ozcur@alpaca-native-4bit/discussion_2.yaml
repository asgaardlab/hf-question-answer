!!python/object:huggingface_hub.community.DiscussionWithDetails
author: athu16
conflicting_files: null
created_at: 2023-03-23 04:35:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f1de3c4ce422c0876c7cf116bab3eb74.svg
      fullname: Atharva Vaidya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: athu16
      type: user
    createdAt: '2023-03-23T05:35:39.000Z'
    data:
      edited: false
      editors:
      - athu16
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f1de3c4ce422c0876c7cf116bab3eb74.svg
          fullname: Atharva Vaidya
          isHf: false
          isPro: false
          name: athu16
          type: user
        html: '<p>I get multiple size mismatch errors while trying to load the model.
          To my knowledge, the alpaca-native model was finetuned from the llama-13b
          model (given its file size of about 24 GB). Yet the model name in this repo
          has "alpaca7b" in it. Where can I find the original 7b alpaca model?</p>

          '
        raw: I get multiple size mismatch errors while trying to load the model. To
          my knowledge, the alpaca-native model was finetuned from the llama-13b model
          (given its file size of about 24 GB). Yet the model name in this repo has
          "alpaca7b" in it. Where can I find the original 7b alpaca model?
        updatedAt: '2023-03-23T05:35:39.639Z'
      numEdits: 0
      reactions: []
    id: 641be52b7c21ab946bf6fe6a
    type: comment
  author: athu16
  content: I get multiple size mismatch errors while trying to load the model. To
    my knowledge, the alpaca-native model was finetuned from the llama-13b model (given
    its file size of about 24 GB). Yet the model name in this repo has "alpaca7b"
    in it. Where can I find the original 7b alpaca model?
  created_at: 2023-03-23 04:35:39+00:00
  edited: false
  hidden: false
  id: 641be52b7c21ab946bf6fe6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-03-23T12:27:29.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>It is the same size as point-alpaca''s weights when I applied the
          diffs so it''s definitly alpaca-7B.<br>I''m not an expert but it seems that
          the issue lies between GPTQ-for-llama and alpaca which is llama finetuned
          and pruned because I''ve seen a warning about a lenght missmatch when I
          tried to quantize it myself (it aborted since the free colab ran out of
          ram unfortunately).</p>

          '
        raw: "It is the same size as point-alpaca's weights when I applied the diffs\
          \ so it's definitly alpaca-7B. \nI'm not an expert but it seems that the\
          \ issue lies between GPTQ-for-llama and alpaca which is llama finetuned\
          \ and pruned because I've seen a warning about a lenght missmatch when I\
          \ tried to quantize it myself (it aborted since the free colab ran out of\
          \ ram unfortunately)."
        updatedAt: '2023-03-23T13:05:53.253Z'
      numEdits: 1
      reactions: []
    id: 641c45b16cd42f5cd471b5a3
    type: comment
  author: cmh
  content: "It is the same size as point-alpaca's weights when I applied the diffs\
    \ so it's definitly alpaca-7B. \nI'm not an expert but it seems that the issue\
    \ lies between GPTQ-for-llama and alpaca which is llama finetuned and pruned because\
    \ I've seen a warning about a lenght missmatch when I tried to quantize it myself\
    \ (it aborted since the free colab ran out of ram unfortunately)."
  created_at: 2023-03-23 11:27:29+00:00
  edited: true
  hidden: false
  id: 641c45b16cd42f5cd471b5a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-23T12:53:37.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>I tried quantizing this model as well and it''s not possible because
          of the embedded size when it was trained. Would require modifying keys to
          the appropriate values before converting or it won''t quant correctly. LoRA
          seems to only convert fine at the moment without any extra work.</p>

          '
        raw: I tried quantizing this model as well and it's not possible because of
          the embedded size when it was trained. Would require modifying keys to the
          appropriate values before converting or it won't quant correctly. LoRA seems
          to only convert fine at the moment without any extra work.
        updatedAt: '2023-03-23T12:53:37.562Z'
      numEdits: 0
      reactions: []
    id: 641c4bd1aa941743c6c4a0c2
    type: comment
  author: elinas
  content: I tried quantizing this model as well and it's not possible because of
    the embedded size when it was trained. Would require modifying keys to the appropriate
    values before converting or it won't quant correctly. LoRA seems to only convert
    fine at the moment without any extra work.
  created_at: 2023-03-23 11:53:37+00:00
  edited: false
  hidden: false
  id: 641c4bd1aa941743c6c4a0c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
      fullname: ozcur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ozcur
      type: user
    createdAt: '2023-03-23T15:31:59.000Z'
    data:
      edited: false
      editors:
      - ozcur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
          fullname: ozcur
          isHf: false
          isPro: false
          name: ozcur
          type: user
        html: '<p>As stated on the model card, this was quantized from the fine-tuned
          7b model at chavinlo/alpaca-native @cecc16dc15544ee626ae3dfb9dfc5cea8851cf1e.  The
          original alpaca-native model is available there.</p>

          '
        raw: As stated on the model card, this was quantized from the fine-tuned 7b
          model at chavinlo/alpaca-native @cecc16dc15544ee626ae3dfb9dfc5cea8851cf1e.  The
          original alpaca-native model is available there.
        updatedAt: '2023-03-23T15:31:59.936Z'
      numEdits: 0
      reactions: []
      relatedEventId: 641c70ef3d67778aae23f327
    id: 641c70ef3d67778aae23f326
    type: comment
  author: ozcur
  content: As stated on the model card, this was quantized from the fine-tuned 7b
    model at chavinlo/alpaca-native @cecc16dc15544ee626ae3dfb9dfc5cea8851cf1e.  The
    original alpaca-native model is available there.
  created_at: 2023-03-23 14:31:59+00:00
  edited: false
  hidden: false
  id: 641c70ef3d67778aae23f326
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
      fullname: ozcur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ozcur
      type: user
    createdAt: '2023-03-23T15:31:59.000Z'
    data:
      status: closed
    id: 641c70ef3d67778aae23f327
    type: status-change
  author: ozcur
  created_at: 2023-03-23 14:31:59+00:00
  id: 641c70ef3d67778aae23f327
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-24T01:47:19.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Did you actually test this after quantizing it?</p>

          '
        raw: Did you actually test this after quantizing it?
        updatedAt: '2023-03-24T01:47:19.418Z'
      numEdits: 0
      reactions: []
    id: 641d01275de221d81464e2dc
    type: comment
  author: elinas
  content: Did you actually test this after quantizing it?
  created_at: 2023-03-24 00:47:19+00:00
  edited: false
  hidden: false
  id: 641d01275de221d81464e2dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
      fullname: ozcur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ozcur
      type: user
    createdAt: '2023-03-24T04:08:22.000Z'
    data:
      edited: false
      editors:
      - ozcur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
          fullname: ozcur
          isHf: false
          isPro: false
          name: ozcur
          type: user
        html: '<p>Yes.  The inference script in the quant repo provided coherent results.</p>

          '
        raw: Yes.  The inference script in the quant repo provided coherent results.
        updatedAt: '2023-03-24T04:08:22.616Z'
      numEdits: 0
      reactions: []
    id: 641d2236d1e1671e3f2000a4
    type: comment
  author: ozcur
  content: Yes.  The inference script in the quant repo provided coherent results.
  created_at: 2023-03-24 03:08:22+00:00
  edited: false
  hidden: false
  id: 641d2236d1e1671e3f2000a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
      fullname: ozcur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ozcur
      type: user
    createdAt: '2023-03-24T04:50:02.000Z'
    data:
      edited: false
      editors:
      - ozcur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
          fullname: ozcur
          isHf: false
          isPro: false
          name: ozcur
          type: user
        html: '<p>Here''s an example invocation:</p>

          <p><a rel="nofollow" href="https://pastebin.com/K1XqG7Aa">https://pastebin.com/K1XqG7Aa</a></p>

          '
        raw: 'Here''s an example invocation:


          https://pastebin.com/K1XqG7Aa'
        updatedAt: '2023-03-24T04:50:02.186Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - cmh
        - elinas
      relatedEventId: 641d2bfaa073e0d293257c00
    id: 641d2bfaa073e0d293257bff
    type: comment
  author: ozcur
  content: 'Here''s an example invocation:


    https://pastebin.com/K1XqG7Aa'
  created_at: 2023-03-24 03:50:02+00:00
  edited: false
  hidden: false
  id: 641d2bfaa073e0d293257bff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7df11d77c0d74cffb2b6337a1b21b2da.svg
      fullname: ozcur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ozcur
      type: user
    createdAt: '2023-03-24T04:50:02.000Z'
    data:
      status: open
    id: 641d2bfaa073e0d293257c00
    type: status-change
  author: ozcur
  created_at: 2023-03-24 03:50:02+00:00
  id: 641d2bfaa073e0d293257c00
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641bc541af42e9b7dd4667f4/SjXpGsauOYjNJTF6nb7EQ.jpeg?w=200&h=200&f=face
      fullname: Tarun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarunchand
      type: user
    createdAt: '2023-03-25T18:17:15.000Z'
    data:
      edited: false
      editors:
      - tarunchand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641bc541af42e9b7dd4667f4/SjXpGsauOYjNJTF6nb7EQ.jpeg?w=200&h=200&f=face
          fullname: Tarun
          isHf: false
          isPro: false
          name: tarunchand
          type: user
        html: "<p>\u2514\u2500$ CUDA_VISIBLE_DEVICES=0 python llama_inference.py /home/me/GPT/text-generation-webui/models/alpaca-7b\
          \ --wbits 4 --load /home/me/GPT/text-generation-webui/models/alpaca-7b/alpaca-7b-4bit.pt\
          \ --max_length 300 --text \"$(cat test_prompt.txt)\"<br>Loading model ...<br>Traceback\
          \ (most recent call last):<br>  File \"/home/me/GPT/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference.py\"\
          , line 108, in <br>    model = load_quant(args.model, args.load, args.wbits)<br>\
          \  File \"/home/me/GPT/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference.py\"\
          , line 52, in load_quant<br>    model.load_state_dict(torch.load(checkpoint))<br>\
          \  File \"/home/me/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict<br>    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(<br>RuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:<br>    Missing key(s) in state_dict:\
          \ \"model.layers.0.self_attn.q_proj.zeros\", \"model.layers.0.self_attn.k_proj.zeros\"\
          , \"model.layers.0.self_attn.v_proj.zeros\", \"model.layers.0.self_attn.o_proj.zeros\"\
          , \"model.layers.0.mlp.gate_proj.zeros\", \"model.layers.0.mlp.down_proj.zeros\"\
          , \"model.layers.0.mlp.up_proj.zeros\", \"model.layers.1.self_attn.q_proj.zeros\"\
          , \"model.layers.1.self_attn.k_proj.zeros\", \"model.layers.1.self_attn.v_proj.zeros\"\
          , \"model.layers.1.self_attn.o_proj.zeros\", \"model.layers.1.mlp.gate_proj.zeros\"\
          , \"model.layers.1.mlp.down_proj.zeros\", \"model.layers.1.mlp.up_proj.zeros\"\
          , \"model.layers.2.self_attn.q_proj.zeros\", \"model.layers.2.self_attn.k_proj.zeros\"\
          , \"model.layers.2.self_attn.v_proj.zeros\", \"model.layers.2.self_attn.o_proj.zeros\"\
          , \"model.layers.2.mlp.gate_proj.zeros\", \"model.layers.2.mlp.down_proj.zeros\"\
          , \"model.layers.2.mlp.up_proj.zeros\", \"model.layers.3.self_attn.q_proj.zeros\"\
          , \"model.layers.3.self_attn.k_proj.zeros\", \"model.layers.3.self_attn.v_proj.zeros\"\
          , \"model.layers.3.self_attn.o_proj.zeros\", \"model.layers.3.mlp.gate_proj.zeros\"\
          , \"model.layers.3.mlp.down_proj.zeros\", \"model.layers.3.mlp.up_proj.zeros\"\
          , \"model.layers.4.self_attn.q_proj.zeros\", \"model.layers.4.self_attn.k_proj.zeros\"\
          , \"model.layers.4.self_attn.v_proj.zeros\", \"model.layers.4.self_attn.o_proj.zeros\"\
          , \"model.layers.4.mlp.gate_proj.zeros\", \"model.layers.4.mlp.down_proj.zeros\"\
          , \"model.layers.4.mlp.up_proj.zeros\", \"model.layers.5.self_attn.q_proj.zeros\"\
          , \"model.layers.5.self_attn.k_proj.zeros\", \"model.layers.5.self_attn.v_proj.zeros\"\
          , \"model.layers.5.self_attn.o_proj.zeros\", \"model.layers.5.mlp.gate_proj.zeros\"\
          , \"model.layers.5.mlp.down_proj.zeros\", \"model.layers.5.mlp.up_proj.zeros\"\
          , \"model.layers.6.self_attn.q_proj.zeros\", \"model.layers.6.self_attn.k_proj.zeros\"\
          , \"model.layers.6.self_attn.v_proj.zeros\", \"model.layers.6.self_attn.o_proj.zeros\"\
          , \"model.layers.6.mlp.gate_proj.zeros\", \"model.layers.6.mlp.down_proj.zeros\"\
          , \"model.layers.6.mlp.up_proj.zeros\", \"model.layers.7.self_attn.q_proj.zeros\"\
          , \"model.layers.7.self_attn.k_proj.zeros\", \"model.layers.7.self_attn.v_proj.zeros\"\
          , \"model.layers.7.self_attn.o_proj.zeros\", \"model.layers.7.mlp.gate_proj.zeros\"\
          , \"model.layers.7.mlp.down_proj.zeros\", \"model.layers.7.mlp.up_proj.zeros\"\
          , \"model.layers.8.self_attn.q_proj.zeros\", \"model.layers.8.self_attn.k_proj.zeros\"\
          , \"model.layers.8.self_attn.v_proj.zeros\", \"model.layers.8.self_attn.o_proj.zeros\"\
          , \"model.layers.8.mlp.gate_proj.zeros\", \"model.layers.8.mlp.down_proj.zeros\"\
          , \"model.layers.8.mlp.up_proj.zeros\", \"model.layers.9.self_attn.q_proj.zeros\"\
          , \"model.layers.9.self_attn.k_proj.zeros\", \"model.layers.9.self_attn.v_proj.zeros\"\
          , \"model.layers.9.self_attn.o_proj.zeros\", \"model.layers.9.mlp.gate_proj.zeros\"\
          , \"model.layers.9.mlp.down_proj.zeros\", \"model.layers.9.mlp.up_proj.zeros\"\
          , \"model.layers.10.self_attn.q_proj.zeros\", \"model.layers.10.self_attn.k_proj.zeros\"\
          , \"model.layers.10.self_attn.v_proj.zeros\", \"model.layers.10.self_attn.o_proj.zeros\"\
          , \"model.layers.10.mlp.gate_proj.zeros\", \"model.layers.10.mlp.down_proj.zeros\"\
          , \"model.layers.10.mlp.up_proj.zeros\", \"model.layers.11.self_attn.q_proj.zeros\"\
          , \"model.layers.11.self_attn.k_proj.zeros\", \"model.layers.11.self_attn.v_proj.zeros\"\
          , \"model.layers.11.self_attn.o_proj.zeros\", \"model.layers.11.mlp.gate_proj.zeros\"\
          , \"model.layers.11.mlp.down_proj.zeros\", \"model.layers.11.mlp.up_proj.zeros\"\
          , \"model.layers.12.self_attn.q_proj.zeros\", \"model.layers.12.self_attn.k_proj.zeros\"\
          , \"model.layers.12.self_attn.v_proj.zeros\", \"model.layers.12.self_attn.o_proj.zeros\"\
          , \"model.layers.12.mlp.gate_proj.zeros\", \"model.layers.12.mlp.down_proj.zeros\"\
          , \"model.layers.12.mlp.up_proj.zeros\", \"model.layers.13.self_attn.q_proj.zeros\"\
          , \"model.layers.13.self_attn.k_proj.zeros\", \"model.layers.13.self_attn.v_proj.zeros\"\
          , \"model.layers.13.self_attn.o_proj.zeros\", \"model.layers.13.mlp.gate_proj.zeros\"\
          , \"model.layers.13.mlp.down_proj.zeros\", \"model.layers.13.mlp.up_proj.zeros\"\
          , \"model.layers.14.self_attn.q_proj.zeros\", \"model.layers.14.self_attn.k_proj.zeros\"\
          , \"model.layers.14.self_attn.v_proj.zeros\", \"model.layers.14.self_attn.o_proj.zeros\"\
          , \"model.layers.14.mlp.gate_proj.zeros\", \"model.layers.14.mlp.down_proj.zeros\"\
          , \"model.layers.14.mlp.up_proj.zeros\", \"model.layers.15.self_attn.q_proj.zeros\"\
          , \"model.layers.15.self_attn.k_proj.zeros\", \"model.layers.15.self_attn.v_proj.zeros\"\
          , \"model.layers.15.self_attn.o_proj.zeros\", \"model.layers.15.mlp.gate_proj.zeros\"\
          , \"model.layers.15.mlp.down_proj.zeros\", \"model.layers.15.mlp.up_proj.zeros\"\
          , \"model.layers.16.self_attn.q_proj.zeros\", \"model.layers.16.self_attn.k_proj.zeros\"\
          , \"model.layers.16.self_attn.v_proj.zeros\", \"model.layers.16.self_attn.o_proj.zeros\"\
          , \"model.layers.16.mlp.gate_proj.zeros\", \"model.layers.16.mlp.down_proj.zeros\"\
          , \"model.layers.16.mlp.up_proj.zeros\", \"model.layers.17.self_attn.q_proj.zeros\"\
          , \"model.layers.17.self_attn.k_proj.zeros\", \"model.layers.17.self_attn.v_proj.zeros\"\
          , \"model.layers.17.self_attn.o_proj.zeros\", \"model.layers.17.mlp.gate_proj.zeros\"\
          , \"model.layers.17.mlp.down_proj.zeros\", \"model.layers.17.mlp.up_proj.zeros\"\
          , \"model.layers.18.self_attn.q_proj.zeros\", \"model.layers.18.self_attn.k_proj.zeros\"\
          , \"model.layers.18.self_attn.v_proj.zeros\", \"model.layers.18.self_attn.o_proj.zeros\"\
          , \"model.layers.18.mlp.gate_proj.zeros\", \"model.layers.18.mlp.down_proj.zeros\"\
          , \"model.layers.18.mlp.up_proj.zeros\", \"model.layers.19.self_attn.q_proj.zeros\"\
          , \"model.layers.19.self_attn.k_proj.zeros\", \"model.layers.19.self_attn.v_proj.zeros\"\
          , \"model.layers.19.self_attn.o_proj.zeros\", \"model.layers.19.mlp.gate_proj.zeros\"\
          , \"model.layers.19.mlp.down_proj.zeros\", \"model.layers.19.mlp.up_proj.zeros\"\
          , \"model.layers.20.self_attn.q_proj.zeros\", \"model.layers.20.self_attn.k_proj.zeros\"\
          , \"model.layers.20.self_attn.v_proj.zeros\", \"model.layers.20.self_attn.o_proj.zeros\"\
          , \"model.layers.20.mlp.gate_proj.zeros\", \"model.layers.20.mlp.down_proj.zeros\"\
          , \"model.layers.20.mlp.up_proj.zeros\", \"model.layers.21.self_attn.q_proj.zeros\"\
          , \"model.layers.21.self_attn.k_proj.zeros\", \"model.layers.21.self_attn.v_proj.zeros\"\
          , \"model.layers.21.self_attn.o_proj.zeros\", \"model.layers.21.mlp.gate_proj.zeros\"\
          , \"model.layers.21.mlp.down_proj.zeros\", \"model.layers.21.mlp.up_proj.zeros\"\
          , \"model.layers.22.self_attn.q_proj.zeros\", \"model.layers.22.self_attn.k_proj.zeros\"\
          , \"model.layers.22.self_attn.v_proj.zeros\", \"model.layers.22.self_attn.o_proj.zeros\"\
          , \"model.layers.22.mlp.gate_proj.zeros\", \"model.layers.22.mlp.down_proj.zeros\"\
          , \"model.layers.22.mlp.up_proj.zeros\", \"model.layers.23.self_attn.q_proj.zeros\"\
          , \"model.layers.23.self_attn.k_proj.zeros\", \"model.layers.23.self_attn.v_proj.zeros\"\
          , \"model.layers.23.self_attn.o_proj.zeros\", \"model.layers.23.mlp.gate_proj.zeros\"\
          , \"model.layers.23.mlp.down_proj.zeros\", \"model.layers.23.mlp.up_proj.zeros\"\
          , \"model.layers.24.self_attn.q_proj.zeros\", \"model.layers.24.self_attn.k_proj.zeros\"\
          , \"model.layers.24.self_attn.v_proj.zeros\", \"model.layers.24.self_attn.o_proj.zeros\"\
          , \"model.layers.24.mlp.gate_proj.zeros\", \"model.layers.24.mlp.down_proj.zeros\"\
          , \"model.layers.24.mlp.up_proj.zeros\", \"model.layers.25.self_attn.q_proj.zeros\"\
          , \"model.layers.25.self_attn.k_proj.zeros\", \"model.layers.25.self_attn.v_proj.zeros\"\
          , \"model.layers.25.self_attn.o_proj.zeros\", \"model.layers.25.mlp.gate_proj.zeros\"\
          , \"model.layers.25.mlp.down_proj.zeros\", \"model.layers.25.mlp.up_proj.zeros\"\
          , \"model.layers.26.self_attn.q_proj.zeros\", \"model.layers.26.self_attn.k_proj.zeros\"\
          , \"model.layers.26.self_attn.v_proj.zeros\", \"model.layers.26.self_attn.o_proj.zeros\"\
          , \"model.layers.26.mlp.gate_proj.zeros\", \"model.layers.26.mlp.down_proj.zeros\"\
          , \"model.layers.26.mlp.up_proj.zeros\", \"model.layers.27.self_attn.q_proj.zeros\"\
          , \"model.layers.27.self_attn.k_proj.zeros\", \"model.layers.27.self_attn.v_proj.zeros\"\
          , \"model.layers.27.self_attn.o_proj.zeros\", \"model.layers.27.mlp.gate_proj.zeros\"\
          , \"model.layers.27.mlp.down_proj.zeros\", \"model.layers.27.mlp.up_proj.zeros\"\
          , \"model.layers.28.self_attn.q_proj.zeros\", \"model.layers.28.self_attn.k_proj.zeros\"\
          , \"model.layers.28.self_attn.v_proj.zeros\", \"model.layers.28.self_attn.o_proj.zeros\"\
          , \"model.layers.28.mlp.gate_proj.zeros\", \"model.layers.28.mlp.down_proj.zeros\"\
          , \"model.layers.28.mlp.up_proj.zeros\", \"model.layers.29.self_attn.q_proj.zeros\"\
          , \"model.layers.29.self_attn.k_proj.zeros\", \"model.layers.29.self_attn.v_proj.zeros\"\
          , \"model.layers.29.self_attn.o_proj.zeros\", \"model.layers.29.mlp.gate_proj.zeros\"\
          , \"model.layers.29.mlp.down_proj.zeros\", \"model.layers.29.mlp.up_proj.zeros\"\
          , \"model.layers.30.self_attn.q_proj.zeros\", \"model.layers.30.self_attn.k_proj.zeros\"\
          , \"model.layers.30.self_attn.v_proj.zeros\", \"model.layers.30.self_attn.o_proj.zeros\"\
          , \"model.layers.30.mlp.gate_proj.zeros\", \"model.layers.30.mlp.down_proj.zeros\"\
          , \"model.layers.30.mlp.up_proj.zeros\", \"model.layers.31.self_attn.q_proj.zeros\"\
          , \"model.layers.31.self_attn.k_proj.zeros\", \"model.layers.31.self_attn.v_proj.zeros\"\
          , \"model.layers.31.self_attn.o_proj.zeros\", \"model.layers.31.mlp.gate_proj.zeros\"\
          , \"model.layers.31.mlp.down_proj.zeros\", \"model.layers.31.mlp.up_proj.zeros\"\
          .<br>    Unexpected key(s) in state_dict: \"model.layers.0.self_attn.q_proj.qzeros\"\
          , \"model.layers.0.self_attn.k_proj.qzeros\", \"model.layers.0.self_attn.v_proj.qzeros\"\
          , \"model.layers.0.self_attn.o_proj.qzeros\", \"model.layers.0.mlp.gate_proj.qzeros\"\
          , \"model.layers.0.mlp.down_proj.qzeros\", \"model.layers.0.mlp.up_proj.qzeros\"\
          , \"model.layers.1.self_attn.q_proj.qzeros\", \"model.layers.1.self_attn.k_proj.qzeros\"\
          , \"model.layers.1.self_attn.v_proj.qzeros\", \"model.layers.1.self_attn.o_proj.qzeros\"\
          , \"model.layers.1.mlp.gate_proj.qzeros\", \"model.layers.1.mlp.down_proj.qzeros\"\
          , \"model.layers.1.mlp.up_proj.qzeros\", \"model.layers.2.self_attn.q_proj.qzeros\"\
          , \"model.layers.2.self_attn.k_proj.qzeros\", \"model.layers.2.self_attn.v_proj.qzeros\"\
          , \"model.layers.2.self_attn.o_proj.qzeros\", \"model.layers.2.mlp.gate_proj.qzeros\"\
          , \"model.layers.2.mlp.down_proj.qzeros\", \"model.layers.2.mlp.up_proj.qzeros\"\
          , \"model.layers.3.self_attn.q_proj.qzeros\", \"model.layers.3.self_attn.k_proj.qzeros\"\
          , \"model.layers.3.self_attn.v_proj.qzeros\", \"model.layers.3.self_attn.o_proj.qzeros\"\
          , \"model.layers.3.mlp.gate_proj.qzeros\", \"model.layers.3.mlp.down_proj.qzeros\"\
          , \"model.layers.3.mlp.up_proj.qzeros\", \"model.layers.4.self_attn.q_proj.qzeros\"\
          , \"model.layers.4.self_attn.k_proj.qzeros\", \"model.layers.4.self_attn.v_proj.qzeros\"\
          , \"model.layers.4.self_attn.o_proj.qzeros\", \"model.layers.4.mlp.gate_proj.qzeros\"\
          , \"model.layers.4.mlp.down_proj.qzeros\", \"model.layers.4.mlp.up_proj.qzeros\"\
          , \"model.layers.5.self_attn.q_proj.qzeros\", \"model.layers.5.self_attn.k_proj.qzeros\"\
          , \"model.layers.5.self_attn.v_proj.qzeros\", \"model.layers.5.self_attn.o_proj.qzeros\"\
          , \"model.layers.5.mlp.gate_proj.qzeros\", \"model.layers.5.mlp.down_proj.qzeros\"\
          , \"model.layers.5.mlp.up_proj.qzeros\", \"model.layers.6.self_attn.q_proj.qzeros\"\
          , \"model.layers.6.self_attn.k_proj.qzeros\", \"model.layers.6.self_attn.v_proj.qzeros\"\
          , \"model.layers.6.self_attn.o_proj.qzeros\", \"model.layers.6.mlp.gate_proj.qzeros\"\
          , \"model.layers.6.mlp.down_proj.qzeros\", \"model.layers.6.mlp.up_proj.qzeros\"\
          , \"model.layers.7.self_attn.q_proj.qzeros\", \"model.layers.7.self_attn.k_proj.qzeros\"\
          , \"model.layers.7.self_attn.v_proj.qzeros\", \"model.layers.7.self_attn.o_proj.qzeros\"\
          , \"model.layers.7.mlp.gate_proj.qzeros\", \"model.layers.7.mlp.down_proj.qzeros\"\
          , \"model.layers.7.mlp.up_proj.qzeros\", \"model.layers.8.self_attn.q_proj.qzeros\"\
          , \"model.layers.8.self_attn.k_proj.qzeros\", \"model.layers.8.self_attn.v_proj.qzeros\"\
          , \"model.layers.8.self_attn.o_proj.qzeros\", \"model.layers.8.mlp.gate_proj.qzeros\"\
          , \"model.layers.8.mlp.down_proj.qzeros\", \"model.layers.8.mlp.up_proj.qzeros\"\
          , \"model.layers.9.self_attn.q_proj.qzeros\", \"model.layers.9.self_attn.k_proj.qzeros\"\
          , \"model.layers.9.self_attn.v_proj.qzeros\", \"model.layers.9.self_attn.o_proj.qzeros\"\
          , \"model.layers.9.mlp.gate_proj.qzeros\", \"model.layers.9.mlp.down_proj.qzeros\"\
          , \"model.layers.9.mlp.up_proj.qzeros\", \"model.layers.10.self_attn.q_proj.qzeros\"\
          , \"model.layers.10.self_attn.k_proj.qzeros\", \"model.layers.10.self_attn.v_proj.qzeros\"\
          , \"model.layers.10.self_attn.o_proj.qzeros\", \"model.layers.10.mlp.gate_proj.qzeros\"\
          , \"model.layers.10.mlp.down_proj.qzeros\", \"model.layers.10.mlp.up_proj.qzeros\"\
          , \"model.layers.11.self_attn.q_proj.qzeros\", \"model.layers.11.self_attn.k_proj.qzeros\"\
          , \"model.layers.11.self_attn.v_proj.qzeros\", \"model.layers.11.self_attn.o_proj.qzeros\"\
          , \"model.layers.11.mlp.gate_proj.qzeros\", \"model.layers.11.mlp.down_proj.qzeros\"\
          , \"model.layers.11.mlp.up_proj.qzeros\", \"model.layers.12.self_attn.q_proj.qzeros\"\
          , \"model.layers.12.self_attn.k_proj.qzeros\", \"model.layers.12.self_attn.v_proj.qzeros\"\
          , \"model.layers.12.self_attn.o_proj.qzeros\", \"model.layers.12.mlp.gate_proj.qzeros\"\
          , \"model.layers.12.mlp.down_proj.qzeros\", \"model.layers.12.mlp.up_proj.qzeros\"\
          , \"model.layers.13.self_attn.q_proj.qzeros\", \"model.layers.13.self_attn.k_proj.qzeros\"\
          , \"model.layers.13.self_attn.v_proj.qzeros\", \"model.layers.13.self_attn.o_proj.qzeros\"\
          , \"model.layers.13.mlp.gate_proj.qzeros\", \"model.layers.13.mlp.down_proj.qzeros\"\
          , \"model.layers.13.mlp.up_proj.qzeros\", \"model.layers.14.self_attn.q_proj.qzeros\"\
          , \"model.layers.14.self_attn.k_proj.qzeros\", \"model.layers.14.self_attn.v_proj.qzeros\"\
          , \"model.layers.14.self_attn.o_proj.qzeros\", \"model.layers.14.mlp.gate_proj.qzeros\"\
          , \"model.layers.14.mlp.down_proj.qzeros\", \"model.layers.14.mlp.up_proj.qzeros\"\
          , \"model.layers.15.self_attn.q_proj.qzeros\", \"model.layers.15.self_attn.k_proj.qzeros\"\
          , \"model.layers.15.self_attn.v_proj.qzeros\", \"model.layers.15.self_attn.o_proj.qzeros\"\
          , \"model.layers.15.mlp.gate_proj.qzeros\", \"model.layers.15.mlp.down_proj.qzeros\"\
          , \"model.layers.15.mlp.up_proj.qzeros\", \"model.layers.16.self_attn.q_proj.qzeros\"\
          , \"model.layers.16.self_attn.k_proj.qzeros\", \"model.layers.16.self_attn.v_proj.qzeros\"\
          , \"model.layers.16.self_attn.o_proj.qzeros\", \"model.layers.16.mlp.gate_proj.qzeros\"\
          , \"model.layers.16.mlp.down_proj.qzeros\", \"model.layers.16.mlp.up_proj.qzeros\"\
          , \"model.layers.17.self_attn.q_proj.qzeros\", \"model.layers.17.self_attn.k_proj.qzeros\"\
          , \"model.layers.17.self_attn.v_proj.qzeros\", \"model.layers.17.self_attn.o_proj.qzeros\"\
          , \"model.layers.17.mlp.gate_proj.qzeros\", \"model.layers.17.mlp.down_proj.qzeros\"\
          , \"model.layers.17.mlp.up_proj.qzeros\", \"model.layers.18.self_attn.q_proj.qzeros\"\
          , \"model.layers.18.self_attn.k_proj.qzeros\", \"model.layers.18.self_attn.v_proj.qzeros\"\
          , \"model.layers.18.self_attn.o_proj.qzeros\", \"model.layers.18.mlp.gate_proj.qzeros\"\
          , \"model.layers.18.mlp.down_proj.qzeros\", \"model.layers.18.mlp.up_proj.qzeros\"\
          , \"model.layers.19.self_attn.q_proj.qzeros\", \"model.layers.19.self_attn.k_proj.qzeros\"\
          , \"model.layers.19.self_attn.v_proj.qzeros\", \"model.layers.19.self_attn.o_proj.qzeros\"\
          , \"model.layers.19.mlp.gate_proj.qzeros\", \"model.layers.19.mlp.down_proj.qzeros\"\
          , \"model.layers.19.mlp.up_proj.qzeros\", \"model.layers.20.self_attn.q_proj.qzeros\"\
          , \"model.layers.20.self_attn.k_proj.qzeros\", \"model.layers.20.self_attn.v_proj.qzeros\"\
          , \"model.layers.20.self_attn.o_proj.qzeros\", \"model.layers.20.mlp.gate_proj.qzeros\"\
          , \"model.layers.20.mlp.down_proj.qzeros\", \"model.layers.20.mlp.up_proj.qzeros\"\
          , \"model.layers.21.self_attn.q_proj.qzeros\", \"model.layers.21.self_attn.k_proj.qzeros\"\
          , \"model.layers.21.self_attn.v_proj.qzeros\", \"model.layers.21.self_attn.o_proj.qzeros\"\
          , \"model.layers.21.mlp.gate_proj.qzeros\", \"model.layers.21.mlp.down_proj.qzeros\"\
          , \"model.layers.21.mlp.up_proj.qzeros\", \"model.layers.22.self_attn.q_proj.qzeros\"\
          , \"model.layers.22.self_attn.k_proj.qzeros\", \"model.layers.22.self_attn.v_proj.qzeros\"\
          , \"model.layers.22.self_attn.o_proj.qzeros\", \"model.layers.22.mlp.gate_proj.qzeros\"\
          , \"model.layers.22.mlp.down_proj.qzeros\", \"model.layers.22.mlp.up_proj.qzeros\"\
          , \"model.layers.23.self_attn.q_proj.qzeros\", \"model.layers.23.self_attn.k_proj.qzeros\"\
          , \"model.layers.23.self_attn.v_proj.qzeros\", \"model.layers.23.self_attn.o_proj.qzeros\"\
          , \"model.layers.23.mlp.gate_proj.qzeros\", \"model.layers.23.mlp.down_proj.qzeros\"\
          , \"model.layers.23.mlp.up_proj.qzeros\", \"model.layers.24.self_attn.q_proj.qzeros\"\
          , \"model.layers.24.self_attn.k_proj.qzeros\", \"model.layers.24.self_attn.v_proj.qzeros\"\
          , \"model.layers.24.self_attn.o_proj.qzeros\", \"model.layers.24.mlp.gate_proj.qzeros\"\
          , \"model.layers.24.mlp.down_proj.qzeros\", \"model.layers.24.mlp.up_proj.qzeros\"\
          , \"model.layers.25.self_attn.q_proj.qzeros\", \"model.layers.25.self_attn.k_proj.qzeros\"\
          , \"model.layers.25.self_attn.v_proj.qzeros\", \"model.layers.25.self_attn.o_proj.qzeros\"\
          , \"model.layers.25.mlp.gate_proj.qzeros\", \"model.layers.25.mlp.down_proj.qzeros\"\
          , \"model.layers.25.mlp.up_proj.qzeros\", \"model.layers.26.self_attn.q_proj.qzeros\"\
          , \"model.layers.26.self_attn.k_proj.qzeros\", \"model.layers.26.self_attn.v_proj.qzeros\"\
          , \"model.layers.26.self_attn.o_proj.qzeros\", \"model.layers.26.mlp.gate_proj.qzeros\"\
          , \"model.layers.26.mlp.down_proj.qzeros\", \"model.layers.26.mlp.up_proj.qzeros\"\
          , \"model.layers.27.self_attn.q_proj.qzeros\", \"model.layers.27.self_attn.k_proj.qzeros\"\
          , \"model.layers.27.self_attn.v_proj.qzeros\", \"model.layers.27.self_attn.o_proj.qzeros\"\
          , \"model.layers.27.mlp.gate_proj.qzeros\", \"model.layers.27.mlp.down_proj.qzeros\"\
          , \"model.layers.27.mlp.up_proj.qzeros\", \"model.layers.28.self_attn.q_proj.qzeros\"\
          , \"model.layers.28.self_attn.k_proj.qzeros\", \"model.layers.28.self_attn.v_proj.qzeros\"\
          , \"model.layers.28.self_attn.o_proj.qzeros\", \"model.layers.28.mlp.gate_proj.qzeros\"\
          , \"model.layers.28.mlp.down_proj.qzeros\", \"model.layers.28.mlp.up_proj.qzeros\"\
          , \"model.layers.29.self_attn.q_proj.qzeros\", \"model.layers.29.self_attn.k_proj.qzeros\"\
          , \"model.layers.29.self_attn.v_proj.qzeros\", \"model.layers.29.self_attn.o_proj.qzeros\"\
          , \"model.layers.29.mlp.gate_proj.qzeros\", \"model.layers.29.mlp.down_proj.qzeros\"\
          , \"model.layers.29.mlp.up_proj.qzeros\", \"model.layers.30.self_attn.q_proj.qzeros\"\
          , \"model.layers.30.self_attn.k_proj.qzeros\", \"model.layers.30.self_attn.v_proj.qzeros\"\
          , \"model.layers.30.self_attn.o_proj.qzeros\", \"model.layers.30.mlp.gate_proj.qzeros\"\
          , \"model.layers.30.mlp.down_proj.qzeros\", \"model.layers.30.mlp.up_proj.qzeros\"\
          , \"model.layers.31.self_attn.q_proj.qzeros\", \"model.layers.31.self_attn.k_proj.qzeros\"\
          , \"model.layers.31.self_attn.v_proj.qzeros\", \"model.layers.31.self_attn.o_proj.qzeros\"\
          , \"model.layers.31.mlp.gate_proj.qzeros\", \"model.layers.31.mlp.down_proj.qzeros\"\
          , \"model.layers.31.mlp.up_proj.qzeros\".<br>    size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.0.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.0.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.0.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.0.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.0.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.0.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.1.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.1.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.1.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.1.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.1.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.1.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.1.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.2.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.2.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.2.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.2.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.2.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.2.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.2.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.3.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.3.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.3.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.3.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.3.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.3.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.3.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.4.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.4.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.4.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.4.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.4.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.4.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.4.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.5.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.5.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.5.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.5.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.5.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.5.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.5.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.6.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.6.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.6.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.6.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.6.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.6.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.6.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.7.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.7.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.7.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.7.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.7.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.7.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.7.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.8.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.8.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.8.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.8.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.8.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.8.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.8.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.9.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.9.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.9.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.9.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.9.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.9.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.9.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.10.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.10.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.10.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.10.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.10.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.10.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.10.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.11.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.11.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.11.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.11.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.11.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.11.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.11.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.12.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.12.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.12.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.12.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.12.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.12.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.12.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.13.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.13.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.13.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.13.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.13.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.13.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.13.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.14.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.14.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.14.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.14.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.14.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.14.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.14.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.15.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.15.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.15.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.15.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.15.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.15.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.15.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.16.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.16.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.16.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.16.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.16.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.16.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.16.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.17.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.17.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.17.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.17.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.17.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.17.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.17.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.18.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.18.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.18.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.18.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.18.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.18.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.18.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.19.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.19.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.19.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.19.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.19.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.19.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.19.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.20.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.20.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.20.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.20.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.20.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.20.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.20.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.21.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.21.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.21.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.21.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.21.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.21.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.21.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.22.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.22.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.22.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.22.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.22.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.22.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.22.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.23.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.23.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.23.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.23.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.23.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.23.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.23.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.24.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.24.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.24.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.24.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.24.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.24.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.24.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.25.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.25.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.25.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.25.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.25.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.25.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.25.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.26.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.26.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.26.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.26.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.26.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.26.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.26.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.27.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.27.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.27.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.27.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.27.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.27.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.27.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.28.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.28.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.28.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.28.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.28.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.28.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.28.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.29.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.29.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.29.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.29.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.29.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.29.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.29.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.30.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.30.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.30.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.30.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.30.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.30.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.30.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).<br>    size mismatch for model.layers.31.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.31.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.31.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).<br>    size mismatch\
          \ for model.layers.31.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([32, 4096]) from checkpoint, the shape in current model is\
          \ torch.Size([4096, 1]).<br>    size mismatch for model.layers.31.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).<br>    size mismatch\
          \ for model.layers.31.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).<br>    size mismatch for model.layers.31.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).</p>\n"
        raw: "\u2514\u2500$ CUDA_VISIBLE_DEVICES=0 python llama_inference.py /home/me/GPT/text-generation-webui/models/alpaca-7b\
          \ --wbits 4 --load /home/me/GPT/text-generation-webui/models/alpaca-7b/alpaca-7b-4bit.pt\
          \ --max_length 300 --text \"$(cat test_prompt.txt)\"         \nLoading model\
          \ ...\nTraceback (most recent call last):\n  File \"/home/me/GPT/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference.py\"\
          , line 108, in <module>\n    model = load_quant(args.model, args.load, args.wbits)\n\
          \  File \"/home/me/GPT/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference.py\"\
          , line 52, in load_quant\n    model.load_state_dict(torch.load(checkpoint))\n\
          \  File \"/home/me/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n\tMissing key(s) in state_dict: \"model.layers.0.self_attn.q_proj.zeros\"\
          , \"model.layers.0.self_attn.k_proj.zeros\", \"model.layers.0.self_attn.v_proj.zeros\"\
          , \"model.layers.0.self_attn.o_proj.zeros\", \"model.layers.0.mlp.gate_proj.zeros\"\
          , \"model.layers.0.mlp.down_proj.zeros\", \"model.layers.0.mlp.up_proj.zeros\"\
          , \"model.layers.1.self_attn.q_proj.zeros\", \"model.layers.1.self_attn.k_proj.zeros\"\
          , \"model.layers.1.self_attn.v_proj.zeros\", \"model.layers.1.self_attn.o_proj.zeros\"\
          , \"model.layers.1.mlp.gate_proj.zeros\", \"model.layers.1.mlp.down_proj.zeros\"\
          , \"model.layers.1.mlp.up_proj.zeros\", \"model.layers.2.self_attn.q_proj.zeros\"\
          , \"model.layers.2.self_attn.k_proj.zeros\", \"model.layers.2.self_attn.v_proj.zeros\"\
          , \"model.layers.2.self_attn.o_proj.zeros\", \"model.layers.2.mlp.gate_proj.zeros\"\
          , \"model.layers.2.mlp.down_proj.zeros\", \"model.layers.2.mlp.up_proj.zeros\"\
          , \"model.layers.3.self_attn.q_proj.zeros\", \"model.layers.3.self_attn.k_proj.zeros\"\
          , \"model.layers.3.self_attn.v_proj.zeros\", \"model.layers.3.self_attn.o_proj.zeros\"\
          , \"model.layers.3.mlp.gate_proj.zeros\", \"model.layers.3.mlp.down_proj.zeros\"\
          , \"model.layers.3.mlp.up_proj.zeros\", \"model.layers.4.self_attn.q_proj.zeros\"\
          , \"model.layers.4.self_attn.k_proj.zeros\", \"model.layers.4.self_attn.v_proj.zeros\"\
          , \"model.layers.4.self_attn.o_proj.zeros\", \"model.layers.4.mlp.gate_proj.zeros\"\
          , \"model.layers.4.mlp.down_proj.zeros\", \"model.layers.4.mlp.up_proj.zeros\"\
          , \"model.layers.5.self_attn.q_proj.zeros\", \"model.layers.5.self_attn.k_proj.zeros\"\
          , \"model.layers.5.self_attn.v_proj.zeros\", \"model.layers.5.self_attn.o_proj.zeros\"\
          , \"model.layers.5.mlp.gate_proj.zeros\", \"model.layers.5.mlp.down_proj.zeros\"\
          , \"model.layers.5.mlp.up_proj.zeros\", \"model.layers.6.self_attn.q_proj.zeros\"\
          , \"model.layers.6.self_attn.k_proj.zeros\", \"model.layers.6.self_attn.v_proj.zeros\"\
          , \"model.layers.6.self_attn.o_proj.zeros\", \"model.layers.6.mlp.gate_proj.zeros\"\
          , \"model.layers.6.mlp.down_proj.zeros\", \"model.layers.6.mlp.up_proj.zeros\"\
          , \"model.layers.7.self_attn.q_proj.zeros\", \"model.layers.7.self_attn.k_proj.zeros\"\
          , \"model.layers.7.self_attn.v_proj.zeros\", \"model.layers.7.self_attn.o_proj.zeros\"\
          , \"model.layers.7.mlp.gate_proj.zeros\", \"model.layers.7.mlp.down_proj.zeros\"\
          , \"model.layers.7.mlp.up_proj.zeros\", \"model.layers.8.self_attn.q_proj.zeros\"\
          , \"model.layers.8.self_attn.k_proj.zeros\", \"model.layers.8.self_attn.v_proj.zeros\"\
          , \"model.layers.8.self_attn.o_proj.zeros\", \"model.layers.8.mlp.gate_proj.zeros\"\
          , \"model.layers.8.mlp.down_proj.zeros\", \"model.layers.8.mlp.up_proj.zeros\"\
          , \"model.layers.9.self_attn.q_proj.zeros\", \"model.layers.9.self_attn.k_proj.zeros\"\
          , \"model.layers.9.self_attn.v_proj.zeros\", \"model.layers.9.self_attn.o_proj.zeros\"\
          , \"model.layers.9.mlp.gate_proj.zeros\", \"model.layers.9.mlp.down_proj.zeros\"\
          , \"model.layers.9.mlp.up_proj.zeros\", \"model.layers.10.self_attn.q_proj.zeros\"\
          , \"model.layers.10.self_attn.k_proj.zeros\", \"model.layers.10.self_attn.v_proj.zeros\"\
          , \"model.layers.10.self_attn.o_proj.zeros\", \"model.layers.10.mlp.gate_proj.zeros\"\
          , \"model.layers.10.mlp.down_proj.zeros\", \"model.layers.10.mlp.up_proj.zeros\"\
          , \"model.layers.11.self_attn.q_proj.zeros\", \"model.layers.11.self_attn.k_proj.zeros\"\
          , \"model.layers.11.self_attn.v_proj.zeros\", \"model.layers.11.self_attn.o_proj.zeros\"\
          , \"model.layers.11.mlp.gate_proj.zeros\", \"model.layers.11.mlp.down_proj.zeros\"\
          , \"model.layers.11.mlp.up_proj.zeros\", \"model.layers.12.self_attn.q_proj.zeros\"\
          , \"model.layers.12.self_attn.k_proj.zeros\", \"model.layers.12.self_attn.v_proj.zeros\"\
          , \"model.layers.12.self_attn.o_proj.zeros\", \"model.layers.12.mlp.gate_proj.zeros\"\
          , \"model.layers.12.mlp.down_proj.zeros\", \"model.layers.12.mlp.up_proj.zeros\"\
          , \"model.layers.13.self_attn.q_proj.zeros\", \"model.layers.13.self_attn.k_proj.zeros\"\
          , \"model.layers.13.self_attn.v_proj.zeros\", \"model.layers.13.self_attn.o_proj.zeros\"\
          , \"model.layers.13.mlp.gate_proj.zeros\", \"model.layers.13.mlp.down_proj.zeros\"\
          , \"model.layers.13.mlp.up_proj.zeros\", \"model.layers.14.self_attn.q_proj.zeros\"\
          , \"model.layers.14.self_attn.k_proj.zeros\", \"model.layers.14.self_attn.v_proj.zeros\"\
          , \"model.layers.14.self_attn.o_proj.zeros\", \"model.layers.14.mlp.gate_proj.zeros\"\
          , \"model.layers.14.mlp.down_proj.zeros\", \"model.layers.14.mlp.up_proj.zeros\"\
          , \"model.layers.15.self_attn.q_proj.zeros\", \"model.layers.15.self_attn.k_proj.zeros\"\
          , \"model.layers.15.self_attn.v_proj.zeros\", \"model.layers.15.self_attn.o_proj.zeros\"\
          , \"model.layers.15.mlp.gate_proj.zeros\", \"model.layers.15.mlp.down_proj.zeros\"\
          , \"model.layers.15.mlp.up_proj.zeros\", \"model.layers.16.self_attn.q_proj.zeros\"\
          , \"model.layers.16.self_attn.k_proj.zeros\", \"model.layers.16.self_attn.v_proj.zeros\"\
          , \"model.layers.16.self_attn.o_proj.zeros\", \"model.layers.16.mlp.gate_proj.zeros\"\
          , \"model.layers.16.mlp.down_proj.zeros\", \"model.layers.16.mlp.up_proj.zeros\"\
          , \"model.layers.17.self_attn.q_proj.zeros\", \"model.layers.17.self_attn.k_proj.zeros\"\
          , \"model.layers.17.self_attn.v_proj.zeros\", \"model.layers.17.self_attn.o_proj.zeros\"\
          , \"model.layers.17.mlp.gate_proj.zeros\", \"model.layers.17.mlp.down_proj.zeros\"\
          , \"model.layers.17.mlp.up_proj.zeros\", \"model.layers.18.self_attn.q_proj.zeros\"\
          , \"model.layers.18.self_attn.k_proj.zeros\", \"model.layers.18.self_attn.v_proj.zeros\"\
          , \"model.layers.18.self_attn.o_proj.zeros\", \"model.layers.18.mlp.gate_proj.zeros\"\
          , \"model.layers.18.mlp.down_proj.zeros\", \"model.layers.18.mlp.up_proj.zeros\"\
          , \"model.layers.19.self_attn.q_proj.zeros\", \"model.layers.19.self_attn.k_proj.zeros\"\
          , \"model.layers.19.self_attn.v_proj.zeros\", \"model.layers.19.self_attn.o_proj.zeros\"\
          , \"model.layers.19.mlp.gate_proj.zeros\", \"model.layers.19.mlp.down_proj.zeros\"\
          , \"model.layers.19.mlp.up_proj.zeros\", \"model.layers.20.self_attn.q_proj.zeros\"\
          , \"model.layers.20.self_attn.k_proj.zeros\", \"model.layers.20.self_attn.v_proj.zeros\"\
          , \"model.layers.20.self_attn.o_proj.zeros\", \"model.layers.20.mlp.gate_proj.zeros\"\
          , \"model.layers.20.mlp.down_proj.zeros\", \"model.layers.20.mlp.up_proj.zeros\"\
          , \"model.layers.21.self_attn.q_proj.zeros\", \"model.layers.21.self_attn.k_proj.zeros\"\
          , \"model.layers.21.self_attn.v_proj.zeros\", \"model.layers.21.self_attn.o_proj.zeros\"\
          , \"model.layers.21.mlp.gate_proj.zeros\", \"model.layers.21.mlp.down_proj.zeros\"\
          , \"model.layers.21.mlp.up_proj.zeros\", \"model.layers.22.self_attn.q_proj.zeros\"\
          , \"model.layers.22.self_attn.k_proj.zeros\", \"model.layers.22.self_attn.v_proj.zeros\"\
          , \"model.layers.22.self_attn.o_proj.zeros\", \"model.layers.22.mlp.gate_proj.zeros\"\
          , \"model.layers.22.mlp.down_proj.zeros\", \"model.layers.22.mlp.up_proj.zeros\"\
          , \"model.layers.23.self_attn.q_proj.zeros\", \"model.layers.23.self_attn.k_proj.zeros\"\
          , \"model.layers.23.self_attn.v_proj.zeros\", \"model.layers.23.self_attn.o_proj.zeros\"\
          , \"model.layers.23.mlp.gate_proj.zeros\", \"model.layers.23.mlp.down_proj.zeros\"\
          , \"model.layers.23.mlp.up_proj.zeros\", \"model.layers.24.self_attn.q_proj.zeros\"\
          , \"model.layers.24.self_attn.k_proj.zeros\", \"model.layers.24.self_attn.v_proj.zeros\"\
          , \"model.layers.24.self_attn.o_proj.zeros\", \"model.layers.24.mlp.gate_proj.zeros\"\
          , \"model.layers.24.mlp.down_proj.zeros\", \"model.layers.24.mlp.up_proj.zeros\"\
          , \"model.layers.25.self_attn.q_proj.zeros\", \"model.layers.25.self_attn.k_proj.zeros\"\
          , \"model.layers.25.self_attn.v_proj.zeros\", \"model.layers.25.self_attn.o_proj.zeros\"\
          , \"model.layers.25.mlp.gate_proj.zeros\", \"model.layers.25.mlp.down_proj.zeros\"\
          , \"model.layers.25.mlp.up_proj.zeros\", \"model.layers.26.self_attn.q_proj.zeros\"\
          , \"model.layers.26.self_attn.k_proj.zeros\", \"model.layers.26.self_attn.v_proj.zeros\"\
          , \"model.layers.26.self_attn.o_proj.zeros\", \"model.layers.26.mlp.gate_proj.zeros\"\
          , \"model.layers.26.mlp.down_proj.zeros\", \"model.layers.26.mlp.up_proj.zeros\"\
          , \"model.layers.27.self_attn.q_proj.zeros\", \"model.layers.27.self_attn.k_proj.zeros\"\
          , \"model.layers.27.self_attn.v_proj.zeros\", \"model.layers.27.self_attn.o_proj.zeros\"\
          , \"model.layers.27.mlp.gate_proj.zeros\", \"model.layers.27.mlp.down_proj.zeros\"\
          , \"model.layers.27.mlp.up_proj.zeros\", \"model.layers.28.self_attn.q_proj.zeros\"\
          , \"model.layers.28.self_attn.k_proj.zeros\", \"model.layers.28.self_attn.v_proj.zeros\"\
          , \"model.layers.28.self_attn.o_proj.zeros\", \"model.layers.28.mlp.gate_proj.zeros\"\
          , \"model.layers.28.mlp.down_proj.zeros\", \"model.layers.28.mlp.up_proj.zeros\"\
          , \"model.layers.29.self_attn.q_proj.zeros\", \"model.layers.29.self_attn.k_proj.zeros\"\
          , \"model.layers.29.self_attn.v_proj.zeros\", \"model.layers.29.self_attn.o_proj.zeros\"\
          , \"model.layers.29.mlp.gate_proj.zeros\", \"model.layers.29.mlp.down_proj.zeros\"\
          , \"model.layers.29.mlp.up_proj.zeros\", \"model.layers.30.self_attn.q_proj.zeros\"\
          , \"model.layers.30.self_attn.k_proj.zeros\", \"model.layers.30.self_attn.v_proj.zeros\"\
          , \"model.layers.30.self_attn.o_proj.zeros\", \"model.layers.30.mlp.gate_proj.zeros\"\
          , \"model.layers.30.mlp.down_proj.zeros\", \"model.layers.30.mlp.up_proj.zeros\"\
          , \"model.layers.31.self_attn.q_proj.zeros\", \"model.layers.31.self_attn.k_proj.zeros\"\
          , \"model.layers.31.self_attn.v_proj.zeros\", \"model.layers.31.self_attn.o_proj.zeros\"\
          , \"model.layers.31.mlp.gate_proj.zeros\", \"model.layers.31.mlp.down_proj.zeros\"\
          , \"model.layers.31.mlp.up_proj.zeros\". \n\tUnexpected key(s) in state_dict:\
          \ \"model.layers.0.self_attn.q_proj.qzeros\", \"model.layers.0.self_attn.k_proj.qzeros\"\
          , \"model.layers.0.self_attn.v_proj.qzeros\", \"model.layers.0.self_attn.o_proj.qzeros\"\
          , \"model.layers.0.mlp.gate_proj.qzeros\", \"model.layers.0.mlp.down_proj.qzeros\"\
          , \"model.layers.0.mlp.up_proj.qzeros\", \"model.layers.1.self_attn.q_proj.qzeros\"\
          , \"model.layers.1.self_attn.k_proj.qzeros\", \"model.layers.1.self_attn.v_proj.qzeros\"\
          , \"model.layers.1.self_attn.o_proj.qzeros\", \"model.layers.1.mlp.gate_proj.qzeros\"\
          , \"model.layers.1.mlp.down_proj.qzeros\", \"model.layers.1.mlp.up_proj.qzeros\"\
          , \"model.layers.2.self_attn.q_proj.qzeros\", \"model.layers.2.self_attn.k_proj.qzeros\"\
          , \"model.layers.2.self_attn.v_proj.qzeros\", \"model.layers.2.self_attn.o_proj.qzeros\"\
          , \"model.layers.2.mlp.gate_proj.qzeros\", \"model.layers.2.mlp.down_proj.qzeros\"\
          , \"model.layers.2.mlp.up_proj.qzeros\", \"model.layers.3.self_attn.q_proj.qzeros\"\
          , \"model.layers.3.self_attn.k_proj.qzeros\", \"model.layers.3.self_attn.v_proj.qzeros\"\
          , \"model.layers.3.self_attn.o_proj.qzeros\", \"model.layers.3.mlp.gate_proj.qzeros\"\
          , \"model.layers.3.mlp.down_proj.qzeros\", \"model.layers.3.mlp.up_proj.qzeros\"\
          , \"model.layers.4.self_attn.q_proj.qzeros\", \"model.layers.4.self_attn.k_proj.qzeros\"\
          , \"model.layers.4.self_attn.v_proj.qzeros\", \"model.layers.4.self_attn.o_proj.qzeros\"\
          , \"model.layers.4.mlp.gate_proj.qzeros\", \"model.layers.4.mlp.down_proj.qzeros\"\
          , \"model.layers.4.mlp.up_proj.qzeros\", \"model.layers.5.self_attn.q_proj.qzeros\"\
          , \"model.layers.5.self_attn.k_proj.qzeros\", \"model.layers.5.self_attn.v_proj.qzeros\"\
          , \"model.layers.5.self_attn.o_proj.qzeros\", \"model.layers.5.mlp.gate_proj.qzeros\"\
          , \"model.layers.5.mlp.down_proj.qzeros\", \"model.layers.5.mlp.up_proj.qzeros\"\
          , \"model.layers.6.self_attn.q_proj.qzeros\", \"model.layers.6.self_attn.k_proj.qzeros\"\
          , \"model.layers.6.self_attn.v_proj.qzeros\", \"model.layers.6.self_attn.o_proj.qzeros\"\
          , \"model.layers.6.mlp.gate_proj.qzeros\", \"model.layers.6.mlp.down_proj.qzeros\"\
          , \"model.layers.6.mlp.up_proj.qzeros\", \"model.layers.7.self_attn.q_proj.qzeros\"\
          , \"model.layers.7.self_attn.k_proj.qzeros\", \"model.layers.7.self_attn.v_proj.qzeros\"\
          , \"model.layers.7.self_attn.o_proj.qzeros\", \"model.layers.7.mlp.gate_proj.qzeros\"\
          , \"model.layers.7.mlp.down_proj.qzeros\", \"model.layers.7.mlp.up_proj.qzeros\"\
          , \"model.layers.8.self_attn.q_proj.qzeros\", \"model.layers.8.self_attn.k_proj.qzeros\"\
          , \"model.layers.8.self_attn.v_proj.qzeros\", \"model.layers.8.self_attn.o_proj.qzeros\"\
          , \"model.layers.8.mlp.gate_proj.qzeros\", \"model.layers.8.mlp.down_proj.qzeros\"\
          , \"model.layers.8.mlp.up_proj.qzeros\", \"model.layers.9.self_attn.q_proj.qzeros\"\
          , \"model.layers.9.self_attn.k_proj.qzeros\", \"model.layers.9.self_attn.v_proj.qzeros\"\
          , \"model.layers.9.self_attn.o_proj.qzeros\", \"model.layers.9.mlp.gate_proj.qzeros\"\
          , \"model.layers.9.mlp.down_proj.qzeros\", \"model.layers.9.mlp.up_proj.qzeros\"\
          , \"model.layers.10.self_attn.q_proj.qzeros\", \"model.layers.10.self_attn.k_proj.qzeros\"\
          , \"model.layers.10.self_attn.v_proj.qzeros\", \"model.layers.10.self_attn.o_proj.qzeros\"\
          , \"model.layers.10.mlp.gate_proj.qzeros\", \"model.layers.10.mlp.down_proj.qzeros\"\
          , \"model.layers.10.mlp.up_proj.qzeros\", \"model.layers.11.self_attn.q_proj.qzeros\"\
          , \"model.layers.11.self_attn.k_proj.qzeros\", \"model.layers.11.self_attn.v_proj.qzeros\"\
          , \"model.layers.11.self_attn.o_proj.qzeros\", \"model.layers.11.mlp.gate_proj.qzeros\"\
          , \"model.layers.11.mlp.down_proj.qzeros\", \"model.layers.11.mlp.up_proj.qzeros\"\
          , \"model.layers.12.self_attn.q_proj.qzeros\", \"model.layers.12.self_attn.k_proj.qzeros\"\
          , \"model.layers.12.self_attn.v_proj.qzeros\", \"model.layers.12.self_attn.o_proj.qzeros\"\
          , \"model.layers.12.mlp.gate_proj.qzeros\", \"model.layers.12.mlp.down_proj.qzeros\"\
          , \"model.layers.12.mlp.up_proj.qzeros\", \"model.layers.13.self_attn.q_proj.qzeros\"\
          , \"model.layers.13.self_attn.k_proj.qzeros\", \"model.layers.13.self_attn.v_proj.qzeros\"\
          , \"model.layers.13.self_attn.o_proj.qzeros\", \"model.layers.13.mlp.gate_proj.qzeros\"\
          , \"model.layers.13.mlp.down_proj.qzeros\", \"model.layers.13.mlp.up_proj.qzeros\"\
          , \"model.layers.14.self_attn.q_proj.qzeros\", \"model.layers.14.self_attn.k_proj.qzeros\"\
          , \"model.layers.14.self_attn.v_proj.qzeros\", \"model.layers.14.self_attn.o_proj.qzeros\"\
          , \"model.layers.14.mlp.gate_proj.qzeros\", \"model.layers.14.mlp.down_proj.qzeros\"\
          , \"model.layers.14.mlp.up_proj.qzeros\", \"model.layers.15.self_attn.q_proj.qzeros\"\
          , \"model.layers.15.self_attn.k_proj.qzeros\", \"model.layers.15.self_attn.v_proj.qzeros\"\
          , \"model.layers.15.self_attn.o_proj.qzeros\", \"model.layers.15.mlp.gate_proj.qzeros\"\
          , \"model.layers.15.mlp.down_proj.qzeros\", \"model.layers.15.mlp.up_proj.qzeros\"\
          , \"model.layers.16.self_attn.q_proj.qzeros\", \"model.layers.16.self_attn.k_proj.qzeros\"\
          , \"model.layers.16.self_attn.v_proj.qzeros\", \"model.layers.16.self_attn.o_proj.qzeros\"\
          , \"model.layers.16.mlp.gate_proj.qzeros\", \"model.layers.16.mlp.down_proj.qzeros\"\
          , \"model.layers.16.mlp.up_proj.qzeros\", \"model.layers.17.self_attn.q_proj.qzeros\"\
          , \"model.layers.17.self_attn.k_proj.qzeros\", \"model.layers.17.self_attn.v_proj.qzeros\"\
          , \"model.layers.17.self_attn.o_proj.qzeros\", \"model.layers.17.mlp.gate_proj.qzeros\"\
          , \"model.layers.17.mlp.down_proj.qzeros\", \"model.layers.17.mlp.up_proj.qzeros\"\
          , \"model.layers.18.self_attn.q_proj.qzeros\", \"model.layers.18.self_attn.k_proj.qzeros\"\
          , \"model.layers.18.self_attn.v_proj.qzeros\", \"model.layers.18.self_attn.o_proj.qzeros\"\
          , \"model.layers.18.mlp.gate_proj.qzeros\", \"model.layers.18.mlp.down_proj.qzeros\"\
          , \"model.layers.18.mlp.up_proj.qzeros\", \"model.layers.19.self_attn.q_proj.qzeros\"\
          , \"model.layers.19.self_attn.k_proj.qzeros\", \"model.layers.19.self_attn.v_proj.qzeros\"\
          , \"model.layers.19.self_attn.o_proj.qzeros\", \"model.layers.19.mlp.gate_proj.qzeros\"\
          , \"model.layers.19.mlp.down_proj.qzeros\", \"model.layers.19.mlp.up_proj.qzeros\"\
          , \"model.layers.20.self_attn.q_proj.qzeros\", \"model.layers.20.self_attn.k_proj.qzeros\"\
          , \"model.layers.20.self_attn.v_proj.qzeros\", \"model.layers.20.self_attn.o_proj.qzeros\"\
          , \"model.layers.20.mlp.gate_proj.qzeros\", \"model.layers.20.mlp.down_proj.qzeros\"\
          , \"model.layers.20.mlp.up_proj.qzeros\", \"model.layers.21.self_attn.q_proj.qzeros\"\
          , \"model.layers.21.self_attn.k_proj.qzeros\", \"model.layers.21.self_attn.v_proj.qzeros\"\
          , \"model.layers.21.self_attn.o_proj.qzeros\", \"model.layers.21.mlp.gate_proj.qzeros\"\
          , \"model.layers.21.mlp.down_proj.qzeros\", \"model.layers.21.mlp.up_proj.qzeros\"\
          , \"model.layers.22.self_attn.q_proj.qzeros\", \"model.layers.22.self_attn.k_proj.qzeros\"\
          , \"model.layers.22.self_attn.v_proj.qzeros\", \"model.layers.22.self_attn.o_proj.qzeros\"\
          , \"model.layers.22.mlp.gate_proj.qzeros\", \"model.layers.22.mlp.down_proj.qzeros\"\
          , \"model.layers.22.mlp.up_proj.qzeros\", \"model.layers.23.self_attn.q_proj.qzeros\"\
          , \"model.layers.23.self_attn.k_proj.qzeros\", \"model.layers.23.self_attn.v_proj.qzeros\"\
          , \"model.layers.23.self_attn.o_proj.qzeros\", \"model.layers.23.mlp.gate_proj.qzeros\"\
          , \"model.layers.23.mlp.down_proj.qzeros\", \"model.layers.23.mlp.up_proj.qzeros\"\
          , \"model.layers.24.self_attn.q_proj.qzeros\", \"model.layers.24.self_attn.k_proj.qzeros\"\
          , \"model.layers.24.self_attn.v_proj.qzeros\", \"model.layers.24.self_attn.o_proj.qzeros\"\
          , \"model.layers.24.mlp.gate_proj.qzeros\", \"model.layers.24.mlp.down_proj.qzeros\"\
          , \"model.layers.24.mlp.up_proj.qzeros\", \"model.layers.25.self_attn.q_proj.qzeros\"\
          , \"model.layers.25.self_attn.k_proj.qzeros\", \"model.layers.25.self_attn.v_proj.qzeros\"\
          , \"model.layers.25.self_attn.o_proj.qzeros\", \"model.layers.25.mlp.gate_proj.qzeros\"\
          , \"model.layers.25.mlp.down_proj.qzeros\", \"model.layers.25.mlp.up_proj.qzeros\"\
          , \"model.layers.26.self_attn.q_proj.qzeros\", \"model.layers.26.self_attn.k_proj.qzeros\"\
          , \"model.layers.26.self_attn.v_proj.qzeros\", \"model.layers.26.self_attn.o_proj.qzeros\"\
          , \"model.layers.26.mlp.gate_proj.qzeros\", \"model.layers.26.mlp.down_proj.qzeros\"\
          , \"model.layers.26.mlp.up_proj.qzeros\", \"model.layers.27.self_attn.q_proj.qzeros\"\
          , \"model.layers.27.self_attn.k_proj.qzeros\", \"model.layers.27.self_attn.v_proj.qzeros\"\
          , \"model.layers.27.self_attn.o_proj.qzeros\", \"model.layers.27.mlp.gate_proj.qzeros\"\
          , \"model.layers.27.mlp.down_proj.qzeros\", \"model.layers.27.mlp.up_proj.qzeros\"\
          , \"model.layers.28.self_attn.q_proj.qzeros\", \"model.layers.28.self_attn.k_proj.qzeros\"\
          , \"model.layers.28.self_attn.v_proj.qzeros\", \"model.layers.28.self_attn.o_proj.qzeros\"\
          , \"model.layers.28.mlp.gate_proj.qzeros\", \"model.layers.28.mlp.down_proj.qzeros\"\
          , \"model.layers.28.mlp.up_proj.qzeros\", \"model.layers.29.self_attn.q_proj.qzeros\"\
          , \"model.layers.29.self_attn.k_proj.qzeros\", \"model.layers.29.self_attn.v_proj.qzeros\"\
          , \"model.layers.29.self_attn.o_proj.qzeros\", \"model.layers.29.mlp.gate_proj.qzeros\"\
          , \"model.layers.29.mlp.down_proj.qzeros\", \"model.layers.29.mlp.up_proj.qzeros\"\
          , \"model.layers.30.self_attn.q_proj.qzeros\", \"model.layers.30.self_attn.k_proj.qzeros\"\
          , \"model.layers.30.self_attn.v_proj.qzeros\", \"model.layers.30.self_attn.o_proj.qzeros\"\
          , \"model.layers.30.mlp.gate_proj.qzeros\", \"model.layers.30.mlp.down_proj.qzeros\"\
          , \"model.layers.30.mlp.up_proj.qzeros\", \"model.layers.31.self_attn.q_proj.qzeros\"\
          , \"model.layers.31.self_attn.k_proj.qzeros\", \"model.layers.31.self_attn.v_proj.qzeros\"\
          , \"model.layers.31.self_attn.o_proj.qzeros\", \"model.layers.31.mlp.gate_proj.qzeros\"\
          , \"model.layers.31.mlp.down_proj.qzeros\", \"model.layers.31.mlp.up_proj.qzeros\"\
          . \n\tsize mismatch for model.layers.0.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.0.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.0.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.0.mlp.up_proj.scales: copying a\
          \ param with shape torch.Size([32, 11008]) from checkpoint, the shape in\
          \ current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.1.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.1.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.1.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.1.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.2.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.2.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.2.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.2.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.3.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.3.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.3.mlp.up_proj.scales: copying a\
          \ param with shape torch.Size([32, 11008]) from checkpoint, the shape in\
          \ current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.4.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.4.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.4.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.4.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.5.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.5.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.5.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.5.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.6.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.6.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.6.mlp.up_proj.scales: copying a\
          \ param with shape torch.Size([32, 11008]) from checkpoint, the shape in\
          \ current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.7.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.7.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.7.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.7.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.8.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.8.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.8.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.8.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.9.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.9.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.9.mlp.up_proj.scales: copying a\
          \ param with shape torch.Size([32, 11008]) from checkpoint, the shape in\
          \ current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.10.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.10.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.10.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.10.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.11.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.11.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.11.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.11.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.12.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.12.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.12.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.13.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.13.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.13.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.13.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.14.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.14.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.14.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.14.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.15.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.15.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.15.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.16.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.16.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.16.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.16.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.17.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.17.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.17.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.17.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.18.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.18.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.18.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.19.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.19.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.19.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.19.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.20.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.20.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.20.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.20.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.21.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.21.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.21.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.22.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.22.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.22.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.22.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.23.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.23.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.23.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.23.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.24.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.24.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.24.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.25.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.25.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.25.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.25.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.26.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.26.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.26.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.26.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.27.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.27.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.27.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.28.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.28.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.28.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.28.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.29.self_attn.q_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.29.self_attn.o_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.29.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.29.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.29.mlp.up_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.30.self_attn.v_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.30.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1]).\n\tsize mismatch for\
          \ model.layers.30.mlp.down_proj.scales: copying a param with shape torch.Size([86,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.30.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.31.self_attn.k_proj.scales: copying a param with shape torch.Size([32,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([4096,\
          \ 1]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([32, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([4096, 1]).\n\tsize mismatch for\
          \ model.layers.31.mlp.gate_proj.scales: copying a param with shape torch.Size([32,\
          \ 11008]) from checkpoint, the shape in current model is torch.Size([11008,\
          \ 1]).\n\tsize mismatch for model.layers.31.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([86, 4096]) from checkpoint, the shape in\
          \ current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.31.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the\
          \ shape in current model is torch.Size([11008, 1])."
        updatedAt: '2023-03-25T18:17:15.645Z'
      numEdits: 0
      reactions: []
    id: 641f3aab445976852e9cd2e9
    type: comment
  author: tarunchand
  content: "\u2514\u2500$ CUDA_VISIBLE_DEVICES=0 python llama_inference.py /home/me/GPT/text-generation-webui/models/alpaca-7b\
    \ --wbits 4 --load /home/me/GPT/text-generation-webui/models/alpaca-7b/alpaca-7b-4bit.pt\
    \ --max_length 300 --text \"$(cat test_prompt.txt)\"         \nLoading model ...\n\
    Traceback (most recent call last):\n  File \"/home/me/GPT/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference.py\"\
    , line 108, in <module>\n    model = load_quant(args.model, args.load, args.wbits)\n\
    \  File \"/home/me/GPT/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference.py\"\
    , line 52, in load_quant\n    model.load_state_dict(torch.load(checkpoint))\n\
    \  File \"/home/me/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
    \tMissing key(s) in state_dict: \"model.layers.0.self_attn.q_proj.zeros\", \"\
    model.layers.0.self_attn.k_proj.zeros\", \"model.layers.0.self_attn.v_proj.zeros\"\
    , \"model.layers.0.self_attn.o_proj.zeros\", \"model.layers.0.mlp.gate_proj.zeros\"\
    , \"model.layers.0.mlp.down_proj.zeros\", \"model.layers.0.mlp.up_proj.zeros\"\
    , \"model.layers.1.self_attn.q_proj.zeros\", \"model.layers.1.self_attn.k_proj.zeros\"\
    , \"model.layers.1.self_attn.v_proj.zeros\", \"model.layers.1.self_attn.o_proj.zeros\"\
    , \"model.layers.1.mlp.gate_proj.zeros\", \"model.layers.1.mlp.down_proj.zeros\"\
    , \"model.layers.1.mlp.up_proj.zeros\", \"model.layers.2.self_attn.q_proj.zeros\"\
    , \"model.layers.2.self_attn.k_proj.zeros\", \"model.layers.2.self_attn.v_proj.zeros\"\
    , \"model.layers.2.self_attn.o_proj.zeros\", \"model.layers.2.mlp.gate_proj.zeros\"\
    , \"model.layers.2.mlp.down_proj.zeros\", \"model.layers.2.mlp.up_proj.zeros\"\
    , \"model.layers.3.self_attn.q_proj.zeros\", \"model.layers.3.self_attn.k_proj.zeros\"\
    , \"model.layers.3.self_attn.v_proj.zeros\", \"model.layers.3.self_attn.o_proj.zeros\"\
    , \"model.layers.3.mlp.gate_proj.zeros\", \"model.layers.3.mlp.down_proj.zeros\"\
    , \"model.layers.3.mlp.up_proj.zeros\", \"model.layers.4.self_attn.q_proj.zeros\"\
    , \"model.layers.4.self_attn.k_proj.zeros\", \"model.layers.4.self_attn.v_proj.zeros\"\
    , \"model.layers.4.self_attn.o_proj.zeros\", \"model.layers.4.mlp.gate_proj.zeros\"\
    , \"model.layers.4.mlp.down_proj.zeros\", \"model.layers.4.mlp.up_proj.zeros\"\
    , \"model.layers.5.self_attn.q_proj.zeros\", \"model.layers.5.self_attn.k_proj.zeros\"\
    , \"model.layers.5.self_attn.v_proj.zeros\", \"model.layers.5.self_attn.o_proj.zeros\"\
    , \"model.layers.5.mlp.gate_proj.zeros\", \"model.layers.5.mlp.down_proj.zeros\"\
    , \"model.layers.5.mlp.up_proj.zeros\", \"model.layers.6.self_attn.q_proj.zeros\"\
    , \"model.layers.6.self_attn.k_proj.zeros\", \"model.layers.6.self_attn.v_proj.zeros\"\
    , \"model.layers.6.self_attn.o_proj.zeros\", \"model.layers.6.mlp.gate_proj.zeros\"\
    , \"model.layers.6.mlp.down_proj.zeros\", \"model.layers.6.mlp.up_proj.zeros\"\
    , \"model.layers.7.self_attn.q_proj.zeros\", \"model.layers.7.self_attn.k_proj.zeros\"\
    , \"model.layers.7.self_attn.v_proj.zeros\", \"model.layers.7.self_attn.o_proj.zeros\"\
    , \"model.layers.7.mlp.gate_proj.zeros\", \"model.layers.7.mlp.down_proj.zeros\"\
    , \"model.layers.7.mlp.up_proj.zeros\", \"model.layers.8.self_attn.q_proj.zeros\"\
    , \"model.layers.8.self_attn.k_proj.zeros\", \"model.layers.8.self_attn.v_proj.zeros\"\
    , \"model.layers.8.self_attn.o_proj.zeros\", \"model.layers.8.mlp.gate_proj.zeros\"\
    , \"model.layers.8.mlp.down_proj.zeros\", \"model.layers.8.mlp.up_proj.zeros\"\
    , \"model.layers.9.self_attn.q_proj.zeros\", \"model.layers.9.self_attn.k_proj.zeros\"\
    , \"model.layers.9.self_attn.v_proj.zeros\", \"model.layers.9.self_attn.o_proj.zeros\"\
    , \"model.layers.9.mlp.gate_proj.zeros\", \"model.layers.9.mlp.down_proj.zeros\"\
    , \"model.layers.9.mlp.up_proj.zeros\", \"model.layers.10.self_attn.q_proj.zeros\"\
    , \"model.layers.10.self_attn.k_proj.zeros\", \"model.layers.10.self_attn.v_proj.zeros\"\
    , \"model.layers.10.self_attn.o_proj.zeros\", \"model.layers.10.mlp.gate_proj.zeros\"\
    , \"model.layers.10.mlp.down_proj.zeros\", \"model.layers.10.mlp.up_proj.zeros\"\
    , \"model.layers.11.self_attn.q_proj.zeros\", \"model.layers.11.self_attn.k_proj.zeros\"\
    , \"model.layers.11.self_attn.v_proj.zeros\", \"model.layers.11.self_attn.o_proj.zeros\"\
    , \"model.layers.11.mlp.gate_proj.zeros\", \"model.layers.11.mlp.down_proj.zeros\"\
    , \"model.layers.11.mlp.up_proj.zeros\", \"model.layers.12.self_attn.q_proj.zeros\"\
    , \"model.layers.12.self_attn.k_proj.zeros\", \"model.layers.12.self_attn.v_proj.zeros\"\
    , \"model.layers.12.self_attn.o_proj.zeros\", \"model.layers.12.mlp.gate_proj.zeros\"\
    , \"model.layers.12.mlp.down_proj.zeros\", \"model.layers.12.mlp.up_proj.zeros\"\
    , \"model.layers.13.self_attn.q_proj.zeros\", \"model.layers.13.self_attn.k_proj.zeros\"\
    , \"model.layers.13.self_attn.v_proj.zeros\", \"model.layers.13.self_attn.o_proj.zeros\"\
    , \"model.layers.13.mlp.gate_proj.zeros\", \"model.layers.13.mlp.down_proj.zeros\"\
    , \"model.layers.13.mlp.up_proj.zeros\", \"model.layers.14.self_attn.q_proj.zeros\"\
    , \"model.layers.14.self_attn.k_proj.zeros\", \"model.layers.14.self_attn.v_proj.zeros\"\
    , \"model.layers.14.self_attn.o_proj.zeros\", \"model.layers.14.mlp.gate_proj.zeros\"\
    , \"model.layers.14.mlp.down_proj.zeros\", \"model.layers.14.mlp.up_proj.zeros\"\
    , \"model.layers.15.self_attn.q_proj.zeros\", \"model.layers.15.self_attn.k_proj.zeros\"\
    , \"model.layers.15.self_attn.v_proj.zeros\", \"model.layers.15.self_attn.o_proj.zeros\"\
    , \"model.layers.15.mlp.gate_proj.zeros\", \"model.layers.15.mlp.down_proj.zeros\"\
    , \"model.layers.15.mlp.up_proj.zeros\", \"model.layers.16.self_attn.q_proj.zeros\"\
    , \"model.layers.16.self_attn.k_proj.zeros\", \"model.layers.16.self_attn.v_proj.zeros\"\
    , \"model.layers.16.self_attn.o_proj.zeros\", \"model.layers.16.mlp.gate_proj.zeros\"\
    , \"model.layers.16.mlp.down_proj.zeros\", \"model.layers.16.mlp.up_proj.zeros\"\
    , \"model.layers.17.self_attn.q_proj.zeros\", \"model.layers.17.self_attn.k_proj.zeros\"\
    , \"model.layers.17.self_attn.v_proj.zeros\", \"model.layers.17.self_attn.o_proj.zeros\"\
    , \"model.layers.17.mlp.gate_proj.zeros\", \"model.layers.17.mlp.down_proj.zeros\"\
    , \"model.layers.17.mlp.up_proj.zeros\", \"model.layers.18.self_attn.q_proj.zeros\"\
    , \"model.layers.18.self_attn.k_proj.zeros\", \"model.layers.18.self_attn.v_proj.zeros\"\
    , \"model.layers.18.self_attn.o_proj.zeros\", \"model.layers.18.mlp.gate_proj.zeros\"\
    , \"model.layers.18.mlp.down_proj.zeros\", \"model.layers.18.mlp.up_proj.zeros\"\
    , \"model.layers.19.self_attn.q_proj.zeros\", \"model.layers.19.self_attn.k_proj.zeros\"\
    , \"model.layers.19.self_attn.v_proj.zeros\", \"model.layers.19.self_attn.o_proj.zeros\"\
    , \"model.layers.19.mlp.gate_proj.zeros\", \"model.layers.19.mlp.down_proj.zeros\"\
    , \"model.layers.19.mlp.up_proj.zeros\", \"model.layers.20.self_attn.q_proj.zeros\"\
    , \"model.layers.20.self_attn.k_proj.zeros\", \"model.layers.20.self_attn.v_proj.zeros\"\
    , \"model.layers.20.self_attn.o_proj.zeros\", \"model.layers.20.mlp.gate_proj.zeros\"\
    , \"model.layers.20.mlp.down_proj.zeros\", \"model.layers.20.mlp.up_proj.zeros\"\
    , \"model.layers.21.self_attn.q_proj.zeros\", \"model.layers.21.self_attn.k_proj.zeros\"\
    , \"model.layers.21.self_attn.v_proj.zeros\", \"model.layers.21.self_attn.o_proj.zeros\"\
    , \"model.layers.21.mlp.gate_proj.zeros\", \"model.layers.21.mlp.down_proj.zeros\"\
    , \"model.layers.21.mlp.up_proj.zeros\", \"model.layers.22.self_attn.q_proj.zeros\"\
    , \"model.layers.22.self_attn.k_proj.zeros\", \"model.layers.22.self_attn.v_proj.zeros\"\
    , \"model.layers.22.self_attn.o_proj.zeros\", \"model.layers.22.mlp.gate_proj.zeros\"\
    , \"model.layers.22.mlp.down_proj.zeros\", \"model.layers.22.mlp.up_proj.zeros\"\
    , \"model.layers.23.self_attn.q_proj.zeros\", \"model.layers.23.self_attn.k_proj.zeros\"\
    , \"model.layers.23.self_attn.v_proj.zeros\", \"model.layers.23.self_attn.o_proj.zeros\"\
    , \"model.layers.23.mlp.gate_proj.zeros\", \"model.layers.23.mlp.down_proj.zeros\"\
    , \"model.layers.23.mlp.up_proj.zeros\", \"model.layers.24.self_attn.q_proj.zeros\"\
    , \"model.layers.24.self_attn.k_proj.zeros\", \"model.layers.24.self_attn.v_proj.zeros\"\
    , \"model.layers.24.self_attn.o_proj.zeros\", \"model.layers.24.mlp.gate_proj.zeros\"\
    , \"model.layers.24.mlp.down_proj.zeros\", \"model.layers.24.mlp.up_proj.zeros\"\
    , \"model.layers.25.self_attn.q_proj.zeros\", \"model.layers.25.self_attn.k_proj.zeros\"\
    , \"model.layers.25.self_attn.v_proj.zeros\", \"model.layers.25.self_attn.o_proj.zeros\"\
    , \"model.layers.25.mlp.gate_proj.zeros\", \"model.layers.25.mlp.down_proj.zeros\"\
    , \"model.layers.25.mlp.up_proj.zeros\", \"model.layers.26.self_attn.q_proj.zeros\"\
    , \"model.layers.26.self_attn.k_proj.zeros\", \"model.layers.26.self_attn.v_proj.zeros\"\
    , \"model.layers.26.self_attn.o_proj.zeros\", \"model.layers.26.mlp.gate_proj.zeros\"\
    , \"model.layers.26.mlp.down_proj.zeros\", \"model.layers.26.mlp.up_proj.zeros\"\
    , \"model.layers.27.self_attn.q_proj.zeros\", \"model.layers.27.self_attn.k_proj.zeros\"\
    , \"model.layers.27.self_attn.v_proj.zeros\", \"model.layers.27.self_attn.o_proj.zeros\"\
    , \"model.layers.27.mlp.gate_proj.zeros\", \"model.layers.27.mlp.down_proj.zeros\"\
    , \"model.layers.27.mlp.up_proj.zeros\", \"model.layers.28.self_attn.q_proj.zeros\"\
    , \"model.layers.28.self_attn.k_proj.zeros\", \"model.layers.28.self_attn.v_proj.zeros\"\
    , \"model.layers.28.self_attn.o_proj.zeros\", \"model.layers.28.mlp.gate_proj.zeros\"\
    , \"model.layers.28.mlp.down_proj.zeros\", \"model.layers.28.mlp.up_proj.zeros\"\
    , \"model.layers.29.self_attn.q_proj.zeros\", \"model.layers.29.self_attn.k_proj.zeros\"\
    , \"model.layers.29.self_attn.v_proj.zeros\", \"model.layers.29.self_attn.o_proj.zeros\"\
    , \"model.layers.29.mlp.gate_proj.zeros\", \"model.layers.29.mlp.down_proj.zeros\"\
    , \"model.layers.29.mlp.up_proj.zeros\", \"model.layers.30.self_attn.q_proj.zeros\"\
    , \"model.layers.30.self_attn.k_proj.zeros\", \"model.layers.30.self_attn.v_proj.zeros\"\
    , \"model.layers.30.self_attn.o_proj.zeros\", \"model.layers.30.mlp.gate_proj.zeros\"\
    , \"model.layers.30.mlp.down_proj.zeros\", \"model.layers.30.mlp.up_proj.zeros\"\
    , \"model.layers.31.self_attn.q_proj.zeros\", \"model.layers.31.self_attn.k_proj.zeros\"\
    , \"model.layers.31.self_attn.v_proj.zeros\", \"model.layers.31.self_attn.o_proj.zeros\"\
    , \"model.layers.31.mlp.gate_proj.zeros\", \"model.layers.31.mlp.down_proj.zeros\"\
    , \"model.layers.31.mlp.up_proj.zeros\". \n\tUnexpected key(s) in state_dict:\
    \ \"model.layers.0.self_attn.q_proj.qzeros\", \"model.layers.0.self_attn.k_proj.qzeros\"\
    , \"model.layers.0.self_attn.v_proj.qzeros\", \"model.layers.0.self_attn.o_proj.qzeros\"\
    , \"model.layers.0.mlp.gate_proj.qzeros\", \"model.layers.0.mlp.down_proj.qzeros\"\
    , \"model.layers.0.mlp.up_proj.qzeros\", \"model.layers.1.self_attn.q_proj.qzeros\"\
    , \"model.layers.1.self_attn.k_proj.qzeros\", \"model.layers.1.self_attn.v_proj.qzeros\"\
    , \"model.layers.1.self_attn.o_proj.qzeros\", \"model.layers.1.mlp.gate_proj.qzeros\"\
    , \"model.layers.1.mlp.down_proj.qzeros\", \"model.layers.1.mlp.up_proj.qzeros\"\
    , \"model.layers.2.self_attn.q_proj.qzeros\", \"model.layers.2.self_attn.k_proj.qzeros\"\
    , \"model.layers.2.self_attn.v_proj.qzeros\", \"model.layers.2.self_attn.o_proj.qzeros\"\
    , \"model.layers.2.mlp.gate_proj.qzeros\", \"model.layers.2.mlp.down_proj.qzeros\"\
    , \"model.layers.2.mlp.up_proj.qzeros\", \"model.layers.3.self_attn.q_proj.qzeros\"\
    , \"model.layers.3.self_attn.k_proj.qzeros\", \"model.layers.3.self_attn.v_proj.qzeros\"\
    , \"model.layers.3.self_attn.o_proj.qzeros\", \"model.layers.3.mlp.gate_proj.qzeros\"\
    , \"model.layers.3.mlp.down_proj.qzeros\", \"model.layers.3.mlp.up_proj.qzeros\"\
    , \"model.layers.4.self_attn.q_proj.qzeros\", \"model.layers.4.self_attn.k_proj.qzeros\"\
    , \"model.layers.4.self_attn.v_proj.qzeros\", \"model.layers.4.self_attn.o_proj.qzeros\"\
    , \"model.layers.4.mlp.gate_proj.qzeros\", \"model.layers.4.mlp.down_proj.qzeros\"\
    , \"model.layers.4.mlp.up_proj.qzeros\", \"model.layers.5.self_attn.q_proj.qzeros\"\
    , \"model.layers.5.self_attn.k_proj.qzeros\", \"model.layers.5.self_attn.v_proj.qzeros\"\
    , \"model.layers.5.self_attn.o_proj.qzeros\", \"model.layers.5.mlp.gate_proj.qzeros\"\
    , \"model.layers.5.mlp.down_proj.qzeros\", \"model.layers.5.mlp.up_proj.qzeros\"\
    , \"model.layers.6.self_attn.q_proj.qzeros\", \"model.layers.6.self_attn.k_proj.qzeros\"\
    , \"model.layers.6.self_attn.v_proj.qzeros\", \"model.layers.6.self_attn.o_proj.qzeros\"\
    , \"model.layers.6.mlp.gate_proj.qzeros\", \"model.layers.6.mlp.down_proj.qzeros\"\
    , \"model.layers.6.mlp.up_proj.qzeros\", \"model.layers.7.self_attn.q_proj.qzeros\"\
    , \"model.layers.7.self_attn.k_proj.qzeros\", \"model.layers.7.self_attn.v_proj.qzeros\"\
    , \"model.layers.7.self_attn.o_proj.qzeros\", \"model.layers.7.mlp.gate_proj.qzeros\"\
    , \"model.layers.7.mlp.down_proj.qzeros\", \"model.layers.7.mlp.up_proj.qzeros\"\
    , \"model.layers.8.self_attn.q_proj.qzeros\", \"model.layers.8.self_attn.k_proj.qzeros\"\
    , \"model.layers.8.self_attn.v_proj.qzeros\", \"model.layers.8.self_attn.o_proj.qzeros\"\
    , \"model.layers.8.mlp.gate_proj.qzeros\", \"model.layers.8.mlp.down_proj.qzeros\"\
    , \"model.layers.8.mlp.up_proj.qzeros\", \"model.layers.9.self_attn.q_proj.qzeros\"\
    , \"model.layers.9.self_attn.k_proj.qzeros\", \"model.layers.9.self_attn.v_proj.qzeros\"\
    , \"model.layers.9.self_attn.o_proj.qzeros\", \"model.layers.9.mlp.gate_proj.qzeros\"\
    , \"model.layers.9.mlp.down_proj.qzeros\", \"model.layers.9.mlp.up_proj.qzeros\"\
    , \"model.layers.10.self_attn.q_proj.qzeros\", \"model.layers.10.self_attn.k_proj.qzeros\"\
    , \"model.layers.10.self_attn.v_proj.qzeros\", \"model.layers.10.self_attn.o_proj.qzeros\"\
    , \"model.layers.10.mlp.gate_proj.qzeros\", \"model.layers.10.mlp.down_proj.qzeros\"\
    , \"model.layers.10.mlp.up_proj.qzeros\", \"model.layers.11.self_attn.q_proj.qzeros\"\
    , \"model.layers.11.self_attn.k_proj.qzeros\", \"model.layers.11.self_attn.v_proj.qzeros\"\
    , \"model.layers.11.self_attn.o_proj.qzeros\", \"model.layers.11.mlp.gate_proj.qzeros\"\
    , \"model.layers.11.mlp.down_proj.qzeros\", \"model.layers.11.mlp.up_proj.qzeros\"\
    , \"model.layers.12.self_attn.q_proj.qzeros\", \"model.layers.12.self_attn.k_proj.qzeros\"\
    , \"model.layers.12.self_attn.v_proj.qzeros\", \"model.layers.12.self_attn.o_proj.qzeros\"\
    , \"model.layers.12.mlp.gate_proj.qzeros\", \"model.layers.12.mlp.down_proj.qzeros\"\
    , \"model.layers.12.mlp.up_proj.qzeros\", \"model.layers.13.self_attn.q_proj.qzeros\"\
    , \"model.layers.13.self_attn.k_proj.qzeros\", \"model.layers.13.self_attn.v_proj.qzeros\"\
    , \"model.layers.13.self_attn.o_proj.qzeros\", \"model.layers.13.mlp.gate_proj.qzeros\"\
    , \"model.layers.13.mlp.down_proj.qzeros\", \"model.layers.13.mlp.up_proj.qzeros\"\
    , \"model.layers.14.self_attn.q_proj.qzeros\", \"model.layers.14.self_attn.k_proj.qzeros\"\
    , \"model.layers.14.self_attn.v_proj.qzeros\", \"model.layers.14.self_attn.o_proj.qzeros\"\
    , \"model.layers.14.mlp.gate_proj.qzeros\", \"model.layers.14.mlp.down_proj.qzeros\"\
    , \"model.layers.14.mlp.up_proj.qzeros\", \"model.layers.15.self_attn.q_proj.qzeros\"\
    , \"model.layers.15.self_attn.k_proj.qzeros\", \"model.layers.15.self_attn.v_proj.qzeros\"\
    , \"model.layers.15.self_attn.o_proj.qzeros\", \"model.layers.15.mlp.gate_proj.qzeros\"\
    , \"model.layers.15.mlp.down_proj.qzeros\", \"model.layers.15.mlp.up_proj.qzeros\"\
    , \"model.layers.16.self_attn.q_proj.qzeros\", \"model.layers.16.self_attn.k_proj.qzeros\"\
    , \"model.layers.16.self_attn.v_proj.qzeros\", \"model.layers.16.self_attn.o_proj.qzeros\"\
    , \"model.layers.16.mlp.gate_proj.qzeros\", \"model.layers.16.mlp.down_proj.qzeros\"\
    , \"model.layers.16.mlp.up_proj.qzeros\", \"model.layers.17.self_attn.q_proj.qzeros\"\
    , \"model.layers.17.self_attn.k_proj.qzeros\", \"model.layers.17.self_attn.v_proj.qzeros\"\
    , \"model.layers.17.self_attn.o_proj.qzeros\", \"model.layers.17.mlp.gate_proj.qzeros\"\
    , \"model.layers.17.mlp.down_proj.qzeros\", \"model.layers.17.mlp.up_proj.qzeros\"\
    , \"model.layers.18.self_attn.q_proj.qzeros\", \"model.layers.18.self_attn.k_proj.qzeros\"\
    , \"model.layers.18.self_attn.v_proj.qzeros\", \"model.layers.18.self_attn.o_proj.qzeros\"\
    , \"model.layers.18.mlp.gate_proj.qzeros\", \"model.layers.18.mlp.down_proj.qzeros\"\
    , \"model.layers.18.mlp.up_proj.qzeros\", \"model.layers.19.self_attn.q_proj.qzeros\"\
    , \"model.layers.19.self_attn.k_proj.qzeros\", \"model.layers.19.self_attn.v_proj.qzeros\"\
    , \"model.layers.19.self_attn.o_proj.qzeros\", \"model.layers.19.mlp.gate_proj.qzeros\"\
    , \"model.layers.19.mlp.down_proj.qzeros\", \"model.layers.19.mlp.up_proj.qzeros\"\
    , \"model.layers.20.self_attn.q_proj.qzeros\", \"model.layers.20.self_attn.k_proj.qzeros\"\
    , \"model.layers.20.self_attn.v_proj.qzeros\", \"model.layers.20.self_attn.o_proj.qzeros\"\
    , \"model.layers.20.mlp.gate_proj.qzeros\", \"model.layers.20.mlp.down_proj.qzeros\"\
    , \"model.layers.20.mlp.up_proj.qzeros\", \"model.layers.21.self_attn.q_proj.qzeros\"\
    , \"model.layers.21.self_attn.k_proj.qzeros\", \"model.layers.21.self_attn.v_proj.qzeros\"\
    , \"model.layers.21.self_attn.o_proj.qzeros\", \"model.layers.21.mlp.gate_proj.qzeros\"\
    , \"model.layers.21.mlp.down_proj.qzeros\", \"model.layers.21.mlp.up_proj.qzeros\"\
    , \"model.layers.22.self_attn.q_proj.qzeros\", \"model.layers.22.self_attn.k_proj.qzeros\"\
    , \"model.layers.22.self_attn.v_proj.qzeros\", \"model.layers.22.self_attn.o_proj.qzeros\"\
    , \"model.layers.22.mlp.gate_proj.qzeros\", \"model.layers.22.mlp.down_proj.qzeros\"\
    , \"model.layers.22.mlp.up_proj.qzeros\", \"model.layers.23.self_attn.q_proj.qzeros\"\
    , \"model.layers.23.self_attn.k_proj.qzeros\", \"model.layers.23.self_attn.v_proj.qzeros\"\
    , \"model.layers.23.self_attn.o_proj.qzeros\", \"model.layers.23.mlp.gate_proj.qzeros\"\
    , \"model.layers.23.mlp.down_proj.qzeros\", \"model.layers.23.mlp.up_proj.qzeros\"\
    , \"model.layers.24.self_attn.q_proj.qzeros\", \"model.layers.24.self_attn.k_proj.qzeros\"\
    , \"model.layers.24.self_attn.v_proj.qzeros\", \"model.layers.24.self_attn.o_proj.qzeros\"\
    , \"model.layers.24.mlp.gate_proj.qzeros\", \"model.layers.24.mlp.down_proj.qzeros\"\
    , \"model.layers.24.mlp.up_proj.qzeros\", \"model.layers.25.self_attn.q_proj.qzeros\"\
    , \"model.layers.25.self_attn.k_proj.qzeros\", \"model.layers.25.self_attn.v_proj.qzeros\"\
    , \"model.layers.25.self_attn.o_proj.qzeros\", \"model.layers.25.mlp.gate_proj.qzeros\"\
    , \"model.layers.25.mlp.down_proj.qzeros\", \"model.layers.25.mlp.up_proj.qzeros\"\
    , \"model.layers.26.self_attn.q_proj.qzeros\", \"model.layers.26.self_attn.k_proj.qzeros\"\
    , \"model.layers.26.self_attn.v_proj.qzeros\", \"model.layers.26.self_attn.o_proj.qzeros\"\
    , \"model.layers.26.mlp.gate_proj.qzeros\", \"model.layers.26.mlp.down_proj.qzeros\"\
    , \"model.layers.26.mlp.up_proj.qzeros\", \"model.layers.27.self_attn.q_proj.qzeros\"\
    , \"model.layers.27.self_attn.k_proj.qzeros\", \"model.layers.27.self_attn.v_proj.qzeros\"\
    , \"model.layers.27.self_attn.o_proj.qzeros\", \"model.layers.27.mlp.gate_proj.qzeros\"\
    , \"model.layers.27.mlp.down_proj.qzeros\", \"model.layers.27.mlp.up_proj.qzeros\"\
    , \"model.layers.28.self_attn.q_proj.qzeros\", \"model.layers.28.self_attn.k_proj.qzeros\"\
    , \"model.layers.28.self_attn.v_proj.qzeros\", \"model.layers.28.self_attn.o_proj.qzeros\"\
    , \"model.layers.28.mlp.gate_proj.qzeros\", \"model.layers.28.mlp.down_proj.qzeros\"\
    , \"model.layers.28.mlp.up_proj.qzeros\", \"model.layers.29.self_attn.q_proj.qzeros\"\
    , \"model.layers.29.self_attn.k_proj.qzeros\", \"model.layers.29.self_attn.v_proj.qzeros\"\
    , \"model.layers.29.self_attn.o_proj.qzeros\", \"model.layers.29.mlp.gate_proj.qzeros\"\
    , \"model.layers.29.mlp.down_proj.qzeros\", \"model.layers.29.mlp.up_proj.qzeros\"\
    , \"model.layers.30.self_attn.q_proj.qzeros\", \"model.layers.30.self_attn.k_proj.qzeros\"\
    , \"model.layers.30.self_attn.v_proj.qzeros\", \"model.layers.30.self_attn.o_proj.qzeros\"\
    , \"model.layers.30.mlp.gate_proj.qzeros\", \"model.layers.30.mlp.down_proj.qzeros\"\
    , \"model.layers.30.mlp.up_proj.qzeros\", \"model.layers.31.self_attn.q_proj.qzeros\"\
    , \"model.layers.31.self_attn.k_proj.qzeros\", \"model.layers.31.self_attn.v_proj.qzeros\"\
    , \"model.layers.31.self_attn.o_proj.qzeros\", \"model.layers.31.mlp.gate_proj.qzeros\"\
    , \"model.layers.31.mlp.down_proj.qzeros\", \"model.layers.31.mlp.up_proj.qzeros\"\
    . \n\tsize mismatch for model.layers.0.self_attn.q_proj.scales: copying a param\
    \ with shape torch.Size([32, 4096]) from checkpoint, the shape in current model\
    \ is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.0.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.0.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.1.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.1.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.2.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.2.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.3.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.3.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.4.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.4.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.5.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.5.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.6.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.6.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.7.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.7.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.8.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.8.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.9.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.9.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.10.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.10.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.11.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.11.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.12.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.12.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.13.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.13.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.14.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.14.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.15.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.15.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.16.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.16.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.17.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.17.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.18.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.18.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.19.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.19.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.20.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.20.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.21.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.21.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.22.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.22.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.23.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.23.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.24.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.24.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.25.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.25.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.26.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.26.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.27.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.27.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.28.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.28.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.28.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.29.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.29.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.29.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.29.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.30.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.30.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.30.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.31.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1]).\n\tsize mismatch for model.layers.31.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([86, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 1]).\n\tsize mismatch for model.layers.31.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([32, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([11008, 1])."
  created_at: 2023-03-25 17:17:15+00:00
  edited: false
  hidden: false
  id: 641f3aab445976852e9cd2e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-03-25T18:26:14.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>At first everybody thought it was broken but ozcur was right it''s
          not (sorry about that).<br>Anyway, there''s two methods: </p>

          <ol>

          <li><p>Use GPTQ-for-LLaMa''s inference script like in the exemple provided
          on the model card:<br>python llama_inference.py /path/alpaca-native-4bit
          --wbits 4 --groupsize 128 --load /path/alpaca-native-4bit/alpaca7b-4bit.pt
          --max_length 300 --text "your text"</p>

          </li>

          <li><p>Try the gptq-group-size branch of text-generation-webui (wip, supports
          groupsize 128):<br>python server.py --model alpaca-native-4bit --gptq-bits
          4 --gptq-model-type llama</p>

          </li>

          </ol>

          <p><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/pull/530">https://github.com/oobabooga/text-generation-webui/pull/530</a></p>

          <p>edit: with the new update of text-generation-webui it is:<br>python server.py
          --model alpaca-native-4bit --wbits 4 --model_type llama --groupsize 128</p>

          '
        raw: "At first everybody thought it was broken but ozcur was right it's not\
          \ (sorry about that).\nAnyway, there's two methods: \n\n1. Use GPTQ-for-LLaMa's\
          \ inference script like in the exemple provided on the model card:\npython\
          \ llama_inference.py /path/alpaca-native-4bit --wbits 4 --groupsize 128\
          \ --load /path/alpaca-native-4bit/alpaca7b-4bit.pt --max_length 300 --text\
          \ \"your text\"\n\n2. Try the gptq-group-size branch of text-generation-webui\
          \ (wip, supports groupsize 128):\npython server.py --model alpaca-native-4bit\
          \ --gptq-bits 4 --gptq-model-type llama\n\nhttps://github.com/oobabooga/text-generation-webui/pull/530\n\
          \nedit: with the new update of text-generation-webui it is:\npython server.py\
          \ --model alpaca-native-4bit --wbits 4 --model_type llama --groupsize 128"
        updatedAt: '2023-03-26T01:15:16.630Z'
      numEdits: 7
      reactions: []
    id: 641f3cc6ce000d321828de5d
    type: comment
  author: cmh
  content: "At first everybody thought it was broken but ozcur was right it's not\
    \ (sorry about that).\nAnyway, there's two methods: \n\n1. Use GPTQ-for-LLaMa's\
    \ inference script like in the exemple provided on the model card:\npython llama_inference.py\
    \ /path/alpaca-native-4bit --wbits 4 --groupsize 128 --load /path/alpaca-native-4bit/alpaca7b-4bit.pt\
    \ --max_length 300 --text \"your text\"\n\n2. Try the gptq-group-size branch of\
    \ text-generation-webui (wip, supports groupsize 128):\npython server.py --model\
    \ alpaca-native-4bit --gptq-bits 4 --gptq-model-type llama\n\nhttps://github.com/oobabooga/text-generation-webui/pull/530\n\
    \nedit: with the new update of text-generation-webui it is:\npython server.py\
    \ --model alpaca-native-4bit --wbits 4 --model_type llama --groupsize 128"
  created_at: 2023-03-25 17:26:14+00:00
  edited: true
  hidden: false
  id: 641f3cc6ce000d321828de5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641bc541af42e9b7dd4667f4/SjXpGsauOYjNJTF6nb7EQ.jpeg?w=200&h=200&f=face
      fullname: Tarun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarunchand
      type: user
    createdAt: '2023-03-27T21:17:25.000Z'
    data:
      edited: false
      editors:
      - tarunchand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641bc541af42e9b7dd4667f4/SjXpGsauOYjNJTF6nb7EQ.jpeg?w=200&h=200&f=face
          fullname: Tarun
          isHf: false
          isPro: false
          name: tarunchand
          type: user
        html: '<p>Thanks</p>

          '
        raw: Thanks
        updatedAt: '2023-03-27T21:17:25.666Z'
      numEdits: 0
      reactions: []
    id: 642207e53a33330eabb410ac
    type: comment
  author: tarunchand
  content: Thanks
  created_at: 2023-03-27 20:17:25+00:00
  edited: false
  hidden: false
  id: 642207e53a33330eabb410ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-03T16:57:39.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Not sure what I''m missing here but I keep getting the missing key(s)
          error.<br>This is the command I''m running, which is pretty much the same
          as the README: python llama_inference.py /content/models/ozcur/alpaca-native-4bit
          --wbits 4 --groupsize 128 --load /content/models/ozcur/alpaca-native-4bit/alpaca7b-4bit.pt
          --max_length 500 --text "Instruction: What is an alpaca? How is it different
          from a llama?"</p>

          <p>Can anyone help please.</p>

          '
        raw: 'Not sure what I''m missing here but I keep getting the missing key(s)
          error.

          This is the command I''m running, which is pretty much the same as the README:
          python llama_inference.py /content/models/ozcur/alpaca-native-4bit --wbits
          4 --groupsize 128 --load /content/models/ozcur/alpaca-native-4bit/alpaca7b-4bit.pt
          --max_length 500 --text "Instruction: What is an alpaca? How is it different
          from a llama?"


          Can anyone help please.'
        updatedAt: '2023-04-03T16:57:39.149Z'
      numEdits: 0
      reactions: []
    id: 642b0583469b80a9a8bb4020
    type: comment
  author: Reggie
  content: 'Not sure what I''m missing here but I keep getting the missing key(s)
    error.

    This is the command I''m running, which is pretty much the same as the README:
    python llama_inference.py /content/models/ozcur/alpaca-native-4bit --wbits 4 --groupsize
    128 --load /content/models/ozcur/alpaca-native-4bit/alpaca7b-4bit.pt --max_length
    500 --text "Instruction: What is an alpaca? How is it different from a llama?"


    Can anyone help please.'
  created_at: 2023-04-03 15:57:39+00:00
  edited: false
  hidden: false
  id: 642b0583469b80a9a8bb4020
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-03T17:45:13.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>GPTQ-for-llama have seen some changes lately, are you sure you are
          not using the<br>default triton git branch instead of the cuda one?</p>

          <p>Edit: Check llama''s entry on the textgen-webui wiki for more infos.</p>

          '
        raw: "GPTQ-for-llama have seen some changes lately, are you sure you are not\
          \ using the \ndefault triton git branch instead of the cuda one?\n\nEdit:\
          \ Check llama's entry on the textgen-webui wiki for more infos."
        updatedAt: '2023-04-03T17:46:34.375Z'
      numEdits: 2
      reactions: []
    id: 642b10a97f041440f3e3c550
    type: comment
  author: cmh
  content: "GPTQ-for-llama have seen some changes lately, are you sure you are not\
    \ using the \ndefault triton git branch instead of the cuda one?\n\nEdit: Check\
    \ llama's entry on the textgen-webui wiki for more infos."
  created_at: 2023-04-03 16:45:13+00:00
  edited: true
  hidden: false
  id: 642b10a97f041440f3e3c550
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-04T16:02:22.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Ya I''m on the latest triton branch.<br>I did check the textgen
          wiki. But no luck still. I have this issue with other 4-bit models as well.</p>

          '
        raw: "Ya I'm on the latest triton branch. \nI did check the textgen wiki.\
          \ But no luck still. I have this issue with other 4-bit models as well."
        updatedAt: '2023-04-04T16:02:22.951Z'
      numEdits: 0
      reactions: []
    id: 642c4a0e1ab1afcbb56fd50d
    type: comment
  author: Reggie
  content: "Ya I'm on the latest triton branch. \nI did check the textgen wiki. But\
    \ no luck still. I have this issue with other 4-bit models as well."
  created_at: 2023-04-04 15:02:22+00:00
  edited: false
  hidden: false
  id: 642c4a0e1ab1afcbb56fd50d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-04T16:05:25.000Z'
    data:
      edited: false
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>That''s probably the issue, switch to the cuda branch.</p>

          '
        raw: That's probably the issue, switch to the cuda branch.
        updatedAt: '2023-04-04T16:05:25.392Z'
      numEdits: 0
      reactions: []
    id: 642c4ac51ab1afcbb56fd9d8
    type: comment
  author: cmh
  content: That's probably the issue, switch to the cuda branch.
  created_at: 2023-04-04 15:05:25+00:00
  edited: false
  hidden: false
  id: 642c4ac51ab1afcbb56fd9d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-05T07:31:56.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Cuda branch also fails. Me (&amp; others) seem to be having this
          problem with other models as well: <a href="https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/4">https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/4</a><br>Should
          I be on a specific commit of the Cuda branch. This stuff is developing so
          fast I can barely keep up!</p>

          '
        raw: 'Cuda branch also fails. Me (& others) seem to be having this problem
          with other models as well: https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/4

          Should I be on a specific commit of the Cuda branch. This stuff is developing
          so fast I can barely keep up!'
        updatedAt: '2023-04-05T07:31:56.522Z'
      numEdits: 0
      reactions: []
    id: 642d23ecb2dfc2cd7c4f5397
    type: comment
  author: Reggie
  content: 'Cuda branch also fails. Me (& others) seem to be having this problem with
    other models as well: https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/4

    Should I be on a specific commit of the Cuda branch. This stuff is developing
    so fast I can barely keep up!'
  created_at: 2023-04-05 06:31:56+00:00
  edited: false
  hidden: false
  id: 642d23ecb2dfc2cd7c4f5397
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ozcur/alpaca-native-4bit
repo_type: model
status: open
target_branch: null
title: Missing key(s) in state_dict
