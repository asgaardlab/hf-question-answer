!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-12-11 05:12:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-11T05:12:55.000Z'
    data:
      edited: true
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9434341192245483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>It''s incredible that you can merge 4 LLMs together yet the outputs
          remain coherent. I''m assuming that has something to do with how much better
          spherical linear interpolation is compared to weight averaging.</p>

          <p>This got me wondering if any combination of Mistrals can be merged? Are
          there compatibility issues (e.g. tokens)? Do you need to get permission
          first? Just wondering because the "smartest" Mistral I''ve come across is
          Dolphin 2.1, while the one that produced the most human-like responses is
          Starling alpha. Is there a reason these two couldn''t be merged?</p>

          <p>Edit: I guess this has kinda been done. Was looking around and OpenHermes2.5
          is very similar to Dolphin 2.1, and neural chat is similar to Starling.
          <a href="https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp">https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp</a></p>

          '
        raw: 'It''s incredible that you can merge 4 LLMs together yet the outputs
          remain coherent. I''m assuming that has something to do with how much better
          spherical linear interpolation is compared to weight averaging.


          This got me wondering if any combination of Mistrals can be merged? Are
          there compatibility issues (e.g. tokens)? Do you need to get permission
          first? Just wondering because the "smartest" Mistral I''ve come across is
          Dolphin 2.1, while the one that produced the most human-like responses is
          Starling alpha. Is there a reason these two couldn''t be merged?


          Edit: I guess this has kinda been done. Was looking around and OpenHermes2.5
          is very similar to Dolphin 2.1, and neural chat is similar to Starling.
          https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp'
        updatedAt: '2023-12-11T05:34:54.939Z'
      numEdits: 1
      reactions: []
    id: 65769a570cfadb728ada9fe7
    type: comment
  author: Phil337
  content: 'It''s incredible that you can merge 4 LLMs together yet the outputs remain
    coherent. I''m assuming that has something to do with how much better spherical
    linear interpolation is compared to weight averaging.


    This got me wondering if any combination of Mistrals can be merged? Are there
    compatibility issues (e.g. tokens)? Do you need to get permission first? Just
    wondering because the "smartest" Mistral I''ve come across is Dolphin 2.1, while
    the one that produced the most human-like responses is Starling alpha. Is there
    a reason these two couldn''t be merged?


    Edit: I guess this has kinda been done. Was looking around and OpenHermes2.5 is
    very similar to Dolphin 2.1, and neural chat is similar to Starling. https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp'
  created_at: 2023-12-11 05:12:55+00:00
  edited: true
  hidden: false
  id: 65769a570cfadb728ada9fe7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2023-12-12T17:24:58.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9405004978179932
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: '<blockquote>

          <p>It''s incredible that you can merge 4 LLMs together yet the outputs remain
          coherent. I''m assuming that has something to do with how much better spherical
          linear interpolation is compared to weight averaging.</p>

          <p>This got me wondering if any combination of Mistrals can be merged? Are
          there compatibility issues (e.g. tokens)? Do you need to get permission
          first? Just wondering because the "smartest" Mistral I''ve come across is
          Dolphin 2.1, while the one that produced the most human-like responses is
          Starling alpha. Is there a reason these two couldn''t be merged?</p>

          <p>Edit: I guess this has kinda been done. Was looking around and OpenHermes2.5
          is very similar to Dolphin 2.1, and neural chat is similar to Starling.
          <a href="https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp">https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp</a></p>

          </blockquote>

          <p>Good idea. I''m gonna this.</p>

          '
        raw: "> It's incredible that you can merge 4 LLMs together yet the outputs\
          \ remain coherent. I'm assuming that has something to do with how much better\
          \ spherical linear interpolation is compared to weight averaging.\n> \n\
          > This got me wondering if any combination of Mistrals can be merged? Are\
          \ there compatibility issues (e.g. tokens)? Do you need to get permission\
          \ first? Just wondering because the \"smartest\" Mistral I've come across\
          \ is Dolphin 2.1, while the one that produced the most human-like responses\
          \ is Starling alpha. Is there a reason these two couldn't be merged?\n>\
          \ \n> Edit: I guess this has kinda been done. Was looking around and OpenHermes2.5\
          \ is very similar to Dolphin 2.1, and neural chat is similar to Starling.\
          \ https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp\n\n\
          Good idea. I'm gonna this."
        updatedAt: '2023-12-12T17:24:58.564Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Phil337
    id: 6578976aeb02736add7f8f2d
    type: comment
  author: Q-bert
  content: "> It's incredible that you can merge 4 LLMs together yet the outputs remain\
    \ coherent. I'm assuming that has something to do with how much better spherical\
    \ linear interpolation is compared to weight averaging.\n> \n> This got me wondering\
    \ if any combination of Mistrals can be merged? Are there compatibility issues\
    \ (e.g. tokens)? Do you need to get permission first? Just wondering because the\
    \ \"smartest\" Mistral I've come across is Dolphin 2.1, while the one that produced\
    \ the most human-like responses is Starling alpha. Is there a reason these two\
    \ couldn't be merged?\n> \n> Edit: I guess this has kinda been done. Was looking\
    \ around and OpenHermes2.5 is very similar to Dolphin 2.1, and neural chat is\
    \ similar to Starling. https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp\n\
    \nGood idea. I'm gonna this."
  created_at: 2023-12-12 17:24:58+00:00
  edited: false
  hidden: false
  id: 6578976aeb02736add7f8f2d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Q-bert/Merged-AGI-7B
repo_type: model
status: open
target_branch: null
title: Slerp Merging
