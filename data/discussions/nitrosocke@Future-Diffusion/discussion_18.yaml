!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LunSei
conflicting_files: null
created_at: 2022-12-22 09:46:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e23a414e922ee07bd44ec04345852b72.svg
      fullname: LunSei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LunSei
      type: user
    createdAt: '2022-12-22T09:46:21.000Z'
    data:
      edited: false
      editors:
      - LunSei
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e23a414e922ee07bd44ec04345852b72.svg
          fullname: LunSei
          isHf: false
          isPro: false
          name: LunSei
          type: user
        html: '<p>Hey, it hasn''t been working for days now! What''s going on? This
          was my favorite Stable tool. :(</p>

          '
        raw: Hey, it hasn't been working for days now! What's going on? This was my
          favorite Stable tool. :(
        updatedAt: '2022-12-22T09:46:21.188Z'
      numEdits: 0
      reactions: []
    id: 63a4276d412fd71fb7edaf97
    type: comment
  author: LunSei
  content: Hey, it hasn't been working for days now! What's going on? This was my
    favorite Stable tool. :(
  created_at: 2022-12-22 09:46:21+00:00
  edited: false
  hidden: false
  id: 63a4276d412fd71fb7edaf97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/479789e550a7dd83a12bf035055e01df.svg
      fullname: Yuki Laneige
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YukiLaneige
      type: user
    createdAt: '2022-12-22T10:39:02.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/479789e550a7dd83a12bf035055e01df.svg
          fullname: Yuki Laneige
          isHf: false
          isPro: false
          name: YukiLaneige
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-12-22T10:48:04.598Z'
      numEdits: 0
      reactions: []
    id: 63a433c684a6a25c65c04621
    type: comment
  author: YukiLaneige
  content: This comment has been hidden
  created_at: 2022-12-22 10:39:02+00:00
  edited: true
  hidden: true
  id: 63a433c684a6a25c65c04621
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52c541c6ff886b2224117fbb016797e5.svg
      fullname: "Pelle Svansl\xF6s"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Watchdog87
      type: user
    createdAt: '2023-01-05T08:56:43.000Z'
    data:
      edited: false
      editors:
      - Watchdog87
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52c541c6ff886b2224117fbb016797e5.svg
          fullname: "Pelle Svansl\xF6s"
          isHf: false
          isPro: false
          name: Watchdog87
          type: user
        html: '<p>Getting a bunch of errors!</p>

          '
        raw: Getting a bunch of errors!
        updatedAt: '2023-01-05T08:56:43.353Z'
      numEdits: 0
      reactions: []
    id: 63b690cb1d2cb750d7543671
    type: comment
  author: Watchdog87
  content: Getting a bunch of errors!
  created_at: 2023-01-05 08:56:43+00:00
  edited: false
  hidden: false
  id: 63b690cb1d2cb750d7543671
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676511834011-63741ca4c1142685da8a0226.jpeg?w=200&h=200&f=face
      fullname: Joshua Frank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: frankjoshua
      type: user
    createdAt: '2023-01-06T22:59:21.000Z'
    data:
      edited: false
      editors:
      - frankjoshua
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676511834011-63741ca4c1142685da8a0226.jpeg?w=200&h=200&f=face
          fullname: Joshua Frank
          isHf: false
          isPro: false
          name: frankjoshua
          type: user
        html: '<p>Also giving me problems all of the sudden:</p>

          <p>Exception: Expected is_sm8x || is_sm75 to be true, but got false.  (Could
          this error message be improved?  If so, please report an enhancement request
          to PyTorch.)<br>Traceback (most recent call last):<br>  File "/root/app/app/server.py",
          line 66, in createimage<br>    stableDiffusion = getStableDiffusionImage(data[''input''])<br>  File
          "/root/app/app/models.py", line 56, in getStableDiffusionImage<br>    output
          = gpus[gpu][model](<br>  File "/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py",
          line 27, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
          line 517, in <strong>call</strong><br>    noise_pred = self.unet(latent_model_input,
          t, encoder_hidden_states=text_embeddings).sample<br>  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1190, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py",
          line 392, in forward<br>    sample = self.mid_block(sample, emb, encoder_hidden_states=encoder_hidden_states)<br>  File
          "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line
          1190, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py",
          line 414, in forward<br>    hidden_states = attn(hidden_states, encoder_hidden_states).sample<br>  File
          "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line
          1190, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py",
          line 216, in forward<br>    hidden_states = block(hidden_states, context=encoder_hidden_states,
          timestep=timestep)<br>  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1190, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py",
          line 484, in forward<br>    hidden_states = self.attn1(norm_hidden_states)
          + hidden_states<br>  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1190, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py",
          line 584, in forward<br>    hidden_states = self._memory_efficient_attention_xformers(query,
          key, value)<br>  File "/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py",
          line 663, in _memory_efficient_attention_xformers<br>    hidden_states =
          xformers.ops.memory_efficient_attention(query, key, value, attn_bias=None)<br>  File
          "/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/<strong>init</strong>.py",
          line 191, in memory_efficient_attention<br>    return _memory_efficient_attention(<br>  File
          "/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/<strong>init</strong>.py",
          line 287, in _memory_efficient_attention<br>    return _memory_efficient_attention_forward(<br>  File
          "/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/<strong>init</strong>.py",
          line 308, in <em>memory_efficient_attention_forward<br>    out, *</em> =
          op.apply(inp, needs_gradient=False)<br>  File "/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/flash.py",
          line 155, in apply<br>    softmax_lse, *rest = cls.OPERATOR(<br>RuntimeError:
          Expected is_sm8x || is_sm75 to be true, but got false.  (Could this error
          message be improved?  If so, please report an enhancement request to PyTorch.)</p>

          '
        raw: "Also giving me problems all of the sudden:\n\nException: Expected is_sm8x\
          \ || is_sm75 to be true, but got false.  (Could this error message be improved?\
          \  If so, please report an enhancement request to PyTorch.)\nTraceback (most\
          \ recent call last):\n  File \"/root/app/app/server.py\", line 66, in createimage\n\
          \    stableDiffusion = getStableDiffusionImage(data['input'])\n  File \"\
          /root/app/app/models.py\", line 56, in getStableDiffusionImage\n    output\
          \ = gpus[gpu][model](\n  File \"/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\"\
          , line 517, in __call__\n    noise_pred = self.unet(latent_model_input,\
          \ t, encoder_hidden_states=text_embeddings).sample\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py\"\
          , line 392, in forward\n    sample = self.mid_block(sample, emb, encoder_hidden_states=encoder_hidden_states)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py\"\
          , line 414, in forward\n    hidden_states = attn(hidden_states, encoder_hidden_states).sample\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\"\
          , line 216, in forward\n    hidden_states = block(hidden_states, context=encoder_hidden_states,\
          \ timestep=timestep)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\"\
          , line 484, in forward\n    hidden_states = self.attn1(norm_hidden_states)\
          \ + hidden_states\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\"\
          , line 584, in forward\n    hidden_states = self._memory_efficient_attention_xformers(query,\
          \ key, value)\n  File \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\"\
          , line 663, in _memory_efficient_attention_xformers\n    hidden_states =\
          \ xformers.ops.memory_efficient_attention(query, key, value, attn_bias=None)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\"\
          , line 191, in memory_efficient_attention\n    return _memory_efficient_attention(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\"\
          , line 287, in _memory_efficient_attention\n    return _memory_efficient_attention_forward(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\"\
          , line 308, in _memory_efficient_attention_forward\n    out, *_ = op.apply(inp,\
          \ needs_gradient=False)\n  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/flash.py\"\
          , line 155, in apply\n    softmax_lse, *rest = cls.OPERATOR(\nRuntimeError:\
          \ Expected is_sm8x || is_sm75 to be true, but got false.  (Could this error\
          \ message be improved?  If so, please report an enhancement request to PyTorch.)"
        updatedAt: '2023-01-06T22:59:21.869Z'
      numEdits: 0
      reactions: []
    id: 63b8a7c9a83d33eaaed120ee
    type: comment
  author: frankjoshua
  content: "Also giving me problems all of the sudden:\n\nException: Expected is_sm8x\
    \ || is_sm75 to be true, but got false.  (Could this error message be improved?\
    \  If so, please report an enhancement request to PyTorch.)\nTraceback (most recent\
    \ call last):\n  File \"/root/app/app/server.py\", line 66, in createimage\n \
    \   stableDiffusion = getStableDiffusionImage(data['input'])\n  File \"/root/app/app/models.py\"\
    , line 56, in getStableDiffusionImage\n    output = gpus[gpu][model](\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line\
    \ 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\"\
    , line 517, in __call__\n    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py\"\
    , line 392, in forward\n    sample = self.mid_block(sample, emb, encoder_hidden_states=encoder_hidden_states)\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py\"\
    , line 414, in forward\n    hidden_states = attn(hidden_states, encoder_hidden_states).sample\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\", line\
    \ 216, in forward\n    hidden_states = block(hidden_states, context=encoder_hidden_states,\
    \ timestep=timestep)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\", line\
    \ 484, in forward\n    hidden_states = self.attn1(norm_hidden_states) + hidden_states\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\", line\
    \ 584, in forward\n    hidden_states = self._memory_efficient_attention_xformers(query,\
    \ key, value)\n  File \"/opt/conda/lib/python3.9/site-packages/diffusers/models/attention.py\"\
    , line 663, in _memory_efficient_attention_xformers\n    hidden_states = xformers.ops.memory_efficient_attention(query,\
    \ key, value, attn_bias=None)\n  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\"\
    , line 191, in memory_efficient_attention\n    return _memory_efficient_attention(\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\"\
    , line 287, in _memory_efficient_attention\n    return _memory_efficient_attention_forward(\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\"\
    , line 308, in _memory_efficient_attention_forward\n    out, *_ = op.apply(inp,\
    \ needs_gradient=False)\n  File \"/opt/conda/lib/python3.9/site-packages/xformers/ops/fmha/flash.py\"\
    , line 155, in apply\n    softmax_lse, *rest = cls.OPERATOR(\nRuntimeError: Expected\
    \ is_sm8x || is_sm75 to be true, but got false.  (Could this error message be\
    \ improved?  If so, please report an enhancement request to PyTorch.)"
  created_at: 2023-01-06 22:59:21+00:00
  edited: false
  hidden: false
  id: 63b8a7c9a83d33eaaed120ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676511834011-63741ca4c1142685da8a0226.jpeg?w=200&h=200&f=face
      fullname: Joshua Frank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: frankjoshua
      type: user
    createdAt: '2023-01-08T22:09:09.000Z'
    data:
      edited: false
      editors:
      - frankjoshua
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676511834011-63741ca4c1142685da8a0226.jpeg?w=200&h=200&f=face
          fullname: Joshua Frank
          isHf: false
          isPro: false
          name: frankjoshua
          type: user
        html: '<p>My issue was I was using the xformers library.<br>enable_xformers_memory_efficient_attention()<br>Some
          model work it and some do not. It uses less memory and runs faster but seems
          a little unstable.</p>

          '
        raw: "My issue was I was using the xformers library. \nenable_xformers_memory_efficient_attention()\n\
          Some model work it and some do not. It uses less memory and runs faster\
          \ but seems a little unstable."
        updatedAt: '2023-01-08T22:09:09.406Z'
      numEdits: 0
      reactions: []
    id: 63bb3f05f948864bc8eba104
    type: comment
  author: frankjoshua
  content: "My issue was I was using the xformers library. \nenable_xformers_memory_efficient_attention()\n\
    Some model work it and some do not. It uses less memory and runs faster but seems\
    \ a little unstable."
  created_at: 2023-01-08 22:09:09+00:00
  edited: false
  hidden: false
  id: 63bb3f05f948864bc8eba104
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: nitrosocke/Future-Diffusion
repo_type: model
status: open
target_branch: null
title: Model not working now?
