!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PhelixZhen
conflicting_files: null
created_at: 2023-10-02 06:53:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c3d8871ef3b62525dd4dd8ba13b41b4.svg
      fullname: zhenjianuo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PhelixZhen
      type: user
    createdAt: '2023-10-02T07:53:22.000Z'
    data:
      edited: false
      editors:
      - PhelixZhen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5040547847747803
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c3d8871ef3b62525dd4dd8ba13b41b4.svg
          fullname: zhenjianuo
          isHf: false
          isPro: false
          name: PhelixZhen
          type: user
        html: "<p>The model is not responding. I loaded the model locally using text-generation-webui,\
          \ but when I try to have a conversation with the model in the chat tab,\
          \ it doesn't produce any output. Here are the settings I used when loading\
          \ it\uFF1A</p>\n<p>The AutoGPTQ params are: {'model_basename': 'model',\
          \ 'device': 'cuda:0', 'use_triton': True, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': {0: '23700MiB', 1: '20200MiB', 'cpu': '51400MiB'},\
          \ 'quantize_config': None, 'use_cuda_fp16': True, 'disable_exllama': False}</p>\n"
        raw: "The model is not responding. I loaded the model locally using text-generation-webui,\
          \ but when I try to have a conversation with the model in the chat tab,\
          \ it doesn't produce any output. Here are the settings I used when loading\
          \ it\uFF1A\r\n\r\nThe AutoGPTQ params are: {'model_basename': 'model', 'device':\
          \ 'cuda:0', 'use_triton': True, 'inject_fused_attention': True, 'inject_fused_mlp':\
          \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory':\
          \ {0: '23700MiB', 1: '20200MiB', 'cpu': '51400MiB'}, 'quantize_config':\
          \ None, 'use_cuda_fp16': True, 'disable_exllama': False}"
        updatedAt: '2023-10-02T07:53:22.838Z'
      numEdits: 0
      reactions: []
    id: 651a76f286fa8cb175c73c5d
    type: comment
  author: PhelixZhen
  content: "The model is not responding. I loaded the model locally using text-generation-webui,\
    \ but when I try to have a conversation with the model in the chat tab, it doesn't\
    \ produce any output. Here are the settings I used when loading it\uFF1A\r\n\r\
    \nThe AutoGPTQ params are: {'model_basename': 'model', 'device': 'cuda:0', 'use_triton':\
    \ True, 'inject_fused_attention': True, 'inject_fused_mlp': True, 'use_safetensors':\
    \ True, 'trust_remote_code': False, 'max_memory': {0: '23700MiB', 1: '20200MiB',\
    \ 'cpu': '51400MiB'}, 'quantize_config': None, 'use_cuda_fp16': True, 'disable_exllama':\
    \ False}"
  created_at: 2023-10-02 06:53:22+00:00
  edited: false
  hidden: false
  id: 651a76f286fa8cb175c73c5d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/Llama-2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'The model is not responding. '
