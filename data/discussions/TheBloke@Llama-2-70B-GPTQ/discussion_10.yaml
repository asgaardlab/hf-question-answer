!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tridungduong16
conflicting_files: null
created_at: 2023-07-25 12:10:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454fa48b27940efcb944bb9/3GcYK4RXljPSjBUgQEArL.png?w=200&h=200&f=face
      fullname: Duong Tri Dung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tridungduong16
      type: user
    createdAt: '2023-07-25T13:10:20.000Z'
    data:
      edited: true
      editors:
      - tridungduong16
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9733033180236816
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454fa48b27940efcb944bb9/3GcYK4RXljPSjBUgQEArL.png?w=200&h=200&f=face
          fullname: Duong Tri Dung
          isHf: false
          isPro: false
          name: tridungduong16
          type: user
        html: '<p>I can fine tune well with 13B model, but fails for 70B, errors with
          reshape. Does any one try to finetune it?</p>

          '
        raw: I can fine tune well with 13B model, but fails for 70B, errors with reshape.
          Does any one try to finetune it?
        updatedAt: '2023-07-27T10:57:17.983Z'
      numEdits: 1
      reactions: []
    id: 64bfc9bc484eccec03418a00
    type: comment
  author: tridungduong16
  content: I can fine tune well with 13B model, but fails for 70B, errors with reshape.
    Does any one try to finetune it?
  created_at: 2023-07-25 12:10:20+00:00
  edited: true
  hidden: false
  id: 64bfc9bc484eccec03418a00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454fa48b27940efcb944bb9/3GcYK4RXljPSjBUgQEArL.png?w=200&h=200&f=face
      fullname: Duong Tri Dung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tridungduong16
      type: user
    createdAt: '2023-07-27T10:55:58.000Z'
    data:
      edited: true
      editors:
      - tridungduong16
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5358200073242188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454fa48b27940efcb944bb9/3GcYK4RXljPSjBUgQEArL.png?w=200&h=200&f=face
          fullname: Duong Tri Dung
          isHf: false
          isPro: false
          name: tridungduong16
          type: user
        html: "<pre><code> File \"/envs/dtd_env/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_triton.py\"\
          , line 141, in forward\n    out = out.half().reshape(out_shape)\nRuntimeError:\
          \ shape '[1, 665, 24576]' is invalid for input of size 6809600\n</code></pre>\n"
        raw: "```\n File \"/envs/dtd_env/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_triton.py\"\
          , line 141, in forward\n    out = out.half().reshape(out_shape)\nRuntimeError:\
          \ shape '[1, 665, 24576]' is invalid for input of size 6809600\n```\n"
        updatedAt: '2023-07-27T10:56:55.238Z'
      numEdits: 1
      reactions: []
    id: 64c24d3e079d5d760e657492
    type: comment
  author: tridungduong16
  content: "```\n File \"/envs/dtd_env/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_triton.py\"\
    , line 141, in forward\n    out = out.half().reshape(out_shape)\nRuntimeError:\
    \ shape '[1, 665, 24576]' is invalid for input of size 6809600\n```\n"
  created_at: 2023-07-27 09:55:58+00:00
  edited: true
  hidden: false
  id: 64c24d3e079d5d760e657492
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
      fullname: jimsha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shawei3000
      type: user
    createdAt: '2023-10-01T22:15:38.000Z'
    data:
      edited: true
      editors:
      - shawei3000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7076358795166016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
          fullname: jimsha
          isHf: false
          isPro: false
          name: shawei3000
          type: user
        html: "<h2 id=\"i-got-similar-error-message-fine-tune-70b--gptq-model-tridungduong16--have-u-figured-out-a-solution\"\
          >I got similar error message, fine tune 70B  GPTQ model, <span data-props=\"\
          {&quot;user&quot;:&quot;tridungduong16&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/tridungduong16\">@<span class=\"underline\"\
          >tridungduong16</span></a></span>\n\n\t</span></span> , have u figured out\
          \ a solution?</h2>\n<pre><code>File ~/anaconda3/envs/py310_torch20/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:195\
          \ in forward\nkey_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads,\
          \ self.head_dim).transpose(1, 2)\nshape '[1, 768, 64, 128]' is invalid for\
          \ input of size 786432\n</code></pre>\n<hr>\n<p>its my recent understanding\
          \ adding 2 lora layers on base llama is bad idea, its better to mix both\
          \ lora training data, and that requires fine-tuning on llama 2 directly...\
          \  testing that...</p>\n"
        raw: "I got similar error message, fine tune 70B  GPTQ model, @tridungduong16\
          \ , have u figured out a solution?\n--------------------------\n    File\
          \ ~/anaconda3/envs/py310_torch20/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:195\
          \ in forward\n    key_states = self.k_proj(hidden_states).view(bsz, q_len,\
          \ self.num_heads, self.head_dim).transpose(1, 2)\n    shape '[1, 768, 64,\
          \ 128]' is invalid for input of size 786432\n------------------------------\n\
          \nits my recent understanding adding 2 lora layers on base llama is bad\
          \ idea, its better to mix both lora training data, and that requires fine-tuning\
          \ on llama 2 directly...  testing that..."
        updatedAt: '2023-10-01T22:16:32.357Z'
      numEdits: 2
      reactions: []
    id: 6519ef8ab447213cb557e728
    type: comment
  author: shawei3000
  content: "I got similar error message, fine tune 70B  GPTQ model, @tridungduong16\
    \ , have u figured out a solution?\n--------------------------\n    File ~/anaconda3/envs/py310_torch20/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:195\
    \ in forward\n    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads,\
    \ self.head_dim).transpose(1, 2)\n    shape '[1, 768, 64, 128]' is invalid for\
    \ input of size 786432\n------------------------------\n\nits my recent understanding\
    \ adding 2 lora layers on base llama is bad idea, its better to mix both lora\
    \ training data, and that requires fine-tuning on llama 2 directly...  testing\
    \ that..."
  created_at: 2023-10-01 21:15:38+00:00
  edited: true
  hidden: false
  id: 6519ef8ab447213cb557e728
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/Llama-2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: Wrong shape when loading with Peft-AutoGPTQ
