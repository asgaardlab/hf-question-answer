!!python/object:huggingface_hub.community.DiscussionWithDetails
author: muneerhanif7
conflicting_files: null
created_at: 2023-09-01 09:15:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e495e213b6ad8b38b1949310c6b120ae.svg
      fullname: Muhammad Muneer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muneerhanif7
      type: user
    createdAt: '2023-09-01T10:15:50.000Z'
    data:
      edited: true
      editors:
      - muneerhanif7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7361165285110474
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e495e213b6ad8b38b1949310c6b120ae.svg
          fullname: Muhammad Muneer
          isHf: false
          isPro: false
          name: muneerhanif7
          type: user
        html: "<h1 id=\"i-have-downloaded-the-repo-theblokellama-2-70b-gptq-to-local-using-snapshot_download-i-am-using-a100-80gb-gpu-\"\
          >I have downloaded the repo TheBloke/Llama-2-70B-GPTQ to local using snapshot_download.\
          \ I am using A100 80GB GPU :</h1>\n<p>from huggingface_hub import snapshot_download<br>snapshot_download(repo_id=\"\
          TheBloke/Llama-2-70B-GPTQ\",local_dir=\"./Llama-2-70B-GPTQ\")</p>\n<h1 id=\"\
          here-is-the-code-which-is-used-for-generation\">here is the code which is\
          \ used for generation:</h1>\n<p>from transformers import AutoTokenizer,\
          \ pipeline, logging<br>from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig</p>\n\
          <p>import torch<br>torch.cuda.is_available()<br>device = torch.device(\"\
          cuda\" if torch.cuda.is_available() else \"cpu\")</p>\n<p>def llama2_70b_model(job):<br>\
          \    job_input = job[\"input\"]<br>    job_text = job_input['text']<br>\
          \    context = job_text['context']<br>    prompt = job_text['prompt']<br>\
          \    temperature = job_text['temperature']<br>    max_new_tokens = job_text['max_new_tokens']<br>\
          \    top_p = job_text['top_p']<br>    repetition_penalty = job_text['repetition_penalty']<br>prompt_template=f'''[INST]\
          \ &lt;&gt;<br>{prompt}<br>&lt;&gt;<br>{context} [/INST]'''</p>\n<pre><code>input_ids\
          \ = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput\
          \ = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens,\
          \ top_p=top_p,  repetition_penalty=repetition_penalty)\n\noutputs = tokenizer.decode(output[0])\n\
          # output = outputs.split(\":\")[3]\ntorch.cuda.empty_cache()\n\nreturn outputs\n\
          </code></pre>\n<p>if <strong>name</strong> == \"<strong>main</strong>\"\
          :<br>    model_name_or_path = \"Llama-2-70B-GPTQ/\"<br>    model_basename\
          \ = \"model\"</p>\n<pre><code>use_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path\
          \ ,local_files_only=True)\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path\
          \ , model_basename=model_basename, use_safetensors=True, low_cpu_mem_usage=True,\
          \ local_files_only=True,device_map='auto', use_triton=use_triton, quantize_config=None,inject_fused_attention=False)\n\
          \nprompt = \"Extract the names of all characters mentioned in the text.\"\
          \ncontext = ''' \n</code></pre>\n<p>   In the novel 'Pride and Prejudice,'\
          \ Elizabeth Bennet, Mr. Darcy, and Jane Bennet are prominent characters.:<br>'''<br>\
          \    job = { \"input\": {<br>                \"text\":{ \"prompt\":prompt,\"\
          context\":context,\"temperature\":0.7, \"max_new_tokens\":4020, \"top_p\"\
          :5, \"repetition_penalty\":1}<br>                }<br>          }<br>  \
          \  print(llama2_70b_model(job))</p>\n<h1 id=\"here-is-the-output\">here\
          \ is the output:</h1>\n<p>Prompt: Extract the names of all characters mentioned\
          \ in the text. /n/ncontext:In the novel 'Pride and Prejudice,' Elizabeth\
          \ Bennet, Mr. Darcy, and Jane Bennet are prominent characters.:<br>Prompts\
          \ can be used to extract information from a document or set of documents\
          \ that is not explicitly stated but implied by contextual clues such as\
          \ character relationships (e.g., who knows whom), events occurring at certain\
          \ times during narratives etc.. For example if we want our model trained\
          \ on this prompt then it will learn how different people interact with each\
          \ other based upon their actions within stories written about them; thus\
          \ allowing us access into understanding human behavior better than ever\
          \ before!</p>\n<p>I have tried different parameters and values, but every\
          \ time, it gives very bad responses. Sometimes it repeats the same words\
          \ or gives a blank output. I have also tried using Llama 2 13B, and it performed\
          \ much better than this. So, does anyone have an idea of what could be the\
          \ issue? Why is the model producing irrelevant and poor responses?</p>\n"
        raw: "# I have downloaded the repo TheBloke/Llama-2-70B-GPTQ to local using\
          \ snapshot_download. I am using A100 80GB GPU :\nfrom huggingface_hub import\
          \ snapshot_download\nsnapshot_download(repo_id=\"TheBloke/Llama-2-70B-GPTQ\"\
          ,local_dir=\"./Llama-2-70B-GPTQ\")\n\n# here is the code which is used for\
          \ generation:\nfrom transformers import AutoTokenizer, pipeline, logging\n\
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nimport\
          \ torch\ntorch.cuda.is_available()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\")\n\n\ndef llama2_70b_model(job):\n    job_input = job[\"\
          input\"]\n    job_text = job_input['text']\n    context = job_text['context']\n\
          \    prompt = job_text['prompt']\n    temperature = job_text['temperature']\n\
          \    max_new_tokens = job_text['max_new_tokens']\n    top_p = job_text['top_p']\n\
          \    repetition_penalty = job_text['repetition_penalty']\nprompt_template=f'''[INST]\
          \ <<SYS>>\n{prompt}\n<</SYS>>\n{context} [/INST]'''\n    \n    input_ids\
          \ = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n \
          \   output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens,\
          \ top_p=top_p,  repetition_penalty=repetition_penalty)\n    \n    outputs\
          \ = tokenizer.decode(output[0])\n    # output = outputs.split(\":\")[3]\n\
          \    torch.cuda.empty_cache()\n\n    return outputs\n\nif __name__ == \"\
          __main__\":\n    model_name_or_path = \"Llama-2-70B-GPTQ/\"\n    model_basename\
          \ = \"model\"\n    \n    use_triton = False\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path\
          \ ,local_files_only=True)\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path\
          \ , model_basename=model_basename, use_safetensors=True, low_cpu_mem_usage=True,\
          \ local_files_only=True,device_map='auto', use_triton=use_triton, quantize_config=None,inject_fused_attention=False)\n\
          \n    prompt = \"Extract the names of all characters mentioned in the text.\"\
          \n    context = ''' \n   In the novel 'Pride and Prejudice,' Elizabeth Bennet,\
          \ Mr. Darcy, and Jane Bennet are prominent characters.:\n'''\n    job =\
          \ { \"input\": {\n                \"text\":{ \"prompt\":prompt,\"context\"\
          :context,\"temperature\":0.7, \"max_new_tokens\":4020, \"top_p\":5, \"repetition_penalty\"\
          :1}\n                }\n          }\n    print(llama2_70b_model(job))\n\n\
          # here is the output:\nPrompt: Extract the names of all characters mentioned\
          \ in the text. /n/ncontext:In the novel 'Pride and Prejudice,' Elizabeth\
          \ Bennet, Mr. Darcy, and Jane Bennet are prominent characters.:\nPrompts\
          \ can be used to extract information from a document or set of documents\
          \ that is not explicitly stated but implied by contextual clues such as\
          \ character relationships (e.g., who knows whom), events occurring at certain\
          \ times during narratives etc.. For example if we want our model trained\
          \ on this prompt then it will learn how different people interact with each\
          \ other based upon their actions within stories written about them; thus\
          \ allowing us access into understanding human behavior better than ever\
          \ before!</s>\n\n\nI have tried different parameters and values, but every\
          \ time, it gives very bad responses. Sometimes it repeats the same words\
          \ or gives a blank output. I have also tried using Llama 2 13B, and it performed\
          \ much better than this. So, does anyone have an idea of what could be the\
          \ issue? Why is the model producing irrelevant and poor responses?"
        updatedAt: '2023-09-01T13:52:05.622Z'
      numEdits: 4
      reactions: []
    id: 64f1b9d61a2ce177c6694b75
    type: comment
  author: muneerhanif7
  content: "# I have downloaded the repo TheBloke/Llama-2-70B-GPTQ to local using\
    \ snapshot_download. I am using A100 80GB GPU :\nfrom huggingface_hub import snapshot_download\n\
    snapshot_download(repo_id=\"TheBloke/Llama-2-70B-GPTQ\",local_dir=\"./Llama-2-70B-GPTQ\"\
    )\n\n# here is the code which is used for generation:\nfrom transformers import\
    \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\n\nimport torch\ntorch.cuda.is_available()\ndevice = torch.device(\"\
    cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef llama2_70b_model(job):\n\
    \    job_input = job[\"input\"]\n    job_text = job_input['text']\n    context\
    \ = job_text['context']\n    prompt = job_text['prompt']\n    temperature = job_text['temperature']\n\
    \    max_new_tokens = job_text['max_new_tokens']\n    top_p = job_text['top_p']\n\
    \    repetition_penalty = job_text['repetition_penalty']\nprompt_template=f'''[INST]\
    \ <<SYS>>\n{prompt}\n<</SYS>>\n{context} [/INST]'''\n    \n    input_ids = tokenizer(prompt_template,\
    \ return_tensors='pt').input_ids.cuda()\n    output = model.generate(inputs=input_ids,\
    \ temperature=temperature, max_new_tokens=max_new_tokens, top_p=top_p,  repetition_penalty=repetition_penalty)\n\
    \    \n    outputs = tokenizer.decode(output[0])\n    # output = outputs.split(\"\
    :\")[3]\n    torch.cuda.empty_cache()\n\n    return outputs\n\nif __name__ ==\
    \ \"__main__\":\n    model_name_or_path = \"Llama-2-70B-GPTQ/\"\n    model_basename\
    \ = \"model\"\n    \n    use_triton = False\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path\
    \ ,local_files_only=True)\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path\
    \ , model_basename=model_basename, use_safetensors=True, low_cpu_mem_usage=True,\
    \ local_files_only=True,device_map='auto', use_triton=use_triton, quantize_config=None,inject_fused_attention=False)\n\
    \n    prompt = \"Extract the names of all characters mentioned in the text.\"\n\
    \    context = ''' \n   In the novel 'Pride and Prejudice,' Elizabeth Bennet,\
    \ Mr. Darcy, and Jane Bennet are prominent characters.:\n'''\n    job = { \"input\"\
    : {\n                \"text\":{ \"prompt\":prompt,\"context\":context,\"temperature\"\
    :0.7, \"max_new_tokens\":4020, \"top_p\":5, \"repetition_penalty\":1}\n      \
    \          }\n          }\n    print(llama2_70b_model(job))\n\n# here is the output:\n\
    Prompt: Extract the names of all characters mentioned in the text. /n/ncontext:In\
    \ the novel 'Pride and Prejudice,' Elizabeth Bennet, Mr. Darcy, and Jane Bennet\
    \ are prominent characters.:\nPrompts can be used to extract information from\
    \ a document or set of documents that is not explicitly stated but implied by\
    \ contextual clues such as character relationships (e.g., who knows whom), events\
    \ occurring at certain times during narratives etc.. For example if we want our\
    \ model trained on this prompt then it will learn how different people interact\
    \ with each other based upon their actions within stories written about them;\
    \ thus allowing us access into understanding human behavior better than ever before!</s>\n\
    \n\nI have tried different parameters and values, but every time, it gives very\
    \ bad responses. Sometimes it repeats the same words or gives a blank output.\
    \ I have also tried using Llama 2 13B, and it performed much better than this.\
    \ So, does anyone have an idea of what could be the issue? Why is the model producing\
    \ irrelevant and poor responses?"
  created_at: 2023-09-01 09:15:50+00:00
  edited: true
  hidden: false
  id: 64f1b9d61a2ce177c6694b75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-09-17T15:45:29.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8039167523384094
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>most likely, the reason might be that you are using a BASE model.
          To get actual responses for your instructions or questions, use the 70b
          chat version</p>

          '
        raw: most likely, the reason might be that you are using a BASE model. To
          get actual responses for your instructions or questions, use the 70b chat
          version
        updatedAt: '2023-09-17T15:45:29.285Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nurb432
    id: 65071f1954b989666d4b8b12
    type: comment
  author: YaTharThShaRma999
  content: most likely, the reason might be that you are using a BASE model. To get
    actual responses for your instructions or questions, use the 70b chat version
  created_at: 2023-09-17 14:45:29+00:00
  edited: false
  hidden: false
  id: 65071f1954b989666d4b8b12
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Llama-2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: model responses not good.
