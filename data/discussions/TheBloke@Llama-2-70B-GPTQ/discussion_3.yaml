!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Aivean
conflicting_files: null
created_at: 2023-07-19 04:39:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d1f9a569a1c0af19fbe093e2ed751a7.svg
      fullname: Ivan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aivean
      type: user
    createdAt: '2023-07-19T05:39:26.000Z'
    data:
      edited: true
      editors:
      - Aivean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5710618495941162
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d1f9a569a1c0af19fbe093e2ed751a7.svg
          fullname: Ivan
          isHf: false
          isPro: false
          name: Aivean
          type: user
        html: "<p>Using the latest oobabooga/text-generation-webui on runpod. Tried\
          \ two different GPUs (L40 48 GB and A100 80GB), ExLLama loader.</p>\n<p>The\
          \ model loads successful (nothing in the logs), but fails during the inference:</p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 331, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
          \ state):\n  File \"/workspace/text-generation-webui/modules/exllama.py\"\
          , line 98, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\"\
          , line 186, in gen_begin_reuse\n    self.gen_begin(in_tokens)\n  File \"\
          /usr/local/lib/python3.10/dist-packages/exllama/generator.py\", line 171,\
          \ in gen_begin\n    self.model.forward(self.sequence[:, :-1], self.cache,\
          \ preprocess_only = True, lora = self.lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 849, in forward\n    r = self._forward(input_ids[:, chunk_begin :\
          \ chunk_end],\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 930, in _forward\n    hidden_states = decoder_layer.forward(hidden_states,\
          \ cache, buffers[device], lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 470, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 388, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
          \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 525, 64,\
          \ 128]' is invalid for input of size 537600\n</code></pre>\n<p>Interestingly\
          \ enough, a very small prompt (like 'Hello') works.</p>\n<p>Tried other\
          \ loaders, similar issues. Tried Llama 2 13b, and it worked.</p>\n<p>Tried\
          \ <code>gptq-4bit-64g-actorder_True</code> quantization on A100, same error.\
          \ All settings are default. My steps are literally: start pod, download\
          \ model, load it, try generate.</p>\n"
        raw: "Using the latest oobabooga/text-generation-webui on runpod. Tried two\
          \ different GPUs (L40 48 GB and A100 80GB), ExLLama loader.\n\nThe model\
          \ loads successful (nothing in the logs), but fails during the inference:\n\
          ```\nTraceback (most recent call last):\n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 331, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
          \ state):\n  File \"/workspace/text-generation-webui/modules/exllama.py\"\
          , line 98, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\"\
          , line 186, in gen_begin_reuse\n    self.gen_begin(in_tokens)\n  File \"\
          /usr/local/lib/python3.10/dist-packages/exllama/generator.py\", line 171,\
          \ in gen_begin\n    self.model.forward(self.sequence[:, :-1], self.cache,\
          \ preprocess_only = True, lora = self.lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 849, in forward\n    r = self._forward(input_ids[:, chunk_begin :\
          \ chunk_end],\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 930, in _forward\n    hidden_states = decoder_layer.forward(hidden_states,\
          \ cache, buffers[device], lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 470, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 388, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
          \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 525, 64,\
          \ 128]' is invalid for input of size 537600\n```\n\nInterestingly enough,\
          \ a very small prompt (like 'Hello') works.\n\nTried other loaders, similar\
          \ issues. Tried Llama 2 13b, and it worked.\n\nTried `gptq-4bit-64g-actorder_True`\
          \ quantization on A100, same error. All settings are default. My steps are\
          \ literally: start pod, download model, load it, try generate."
        updatedAt: '2023-07-19T05:53:15.336Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eemotgs
    id: 64b7770efe6a108d03041c68
    type: comment
  author: Aivean
  content: "Using the latest oobabooga/text-generation-webui on runpod. Tried two\
    \ different GPUs (L40 48 GB and A100 80GB), ExLLama loader.\n\nThe model loads\
    \ successful (nothing in the logs), but fails during the inference:\n```\nTraceback\
    \ (most recent call last):\n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
    , line 331, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
    \ state):\n  File \"/workspace/text-generation-webui/modules/exllama.py\", line\
    \ 98, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\", line 186,\
    \ in gen_begin_reuse\n    self.gen_begin(in_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\"\
    , line 171, in gen_begin\n    self.model.forward(self.sequence[:, :-1], self.cache,\
    \ preprocess_only = True, lora = self.lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
    , line 849, in forward\n    r = self._forward(input_ids[:, chunk_begin : chunk_end],\n\
    \  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\", line 930,\
    \ in _forward\n    hidden_states = decoder_layer.forward(hidden_states, cache,\
    \ buffers[device], lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
    , line 470, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
    \ cache, buffer, lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
    , line 388, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
    \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 525, 64, 128]'\
    \ is invalid for input of size 537600\n```\n\nInterestingly enough, a very small\
    \ prompt (like 'Hello') works.\n\nTried other loaders, similar issues. Tried Llama\
    \ 2 13b, and it worked.\n\nTried `gptq-4bit-64g-actorder_True` quantization on\
    \ A100, same error. All settings are default. My steps are literally: start pod,\
    \ download model, load it, try generate."
  created_at: 2023-07-19 04:39:26+00:00
  edited: true
  hidden: false
  id: 64b7770efe6a108d03041c68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b32fa65053b1189f82adfef74569e9a.svg
      fullname: Tristan Webb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: olafthefrog
      type: user
    createdAt: '2023-07-19T05:48:15.000Z'
    data:
      edited: false
      editors:
      - olafthefrog
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40769338607788086
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b32fa65053b1189f82adfef74569e9a.svg
          fullname: Tristan Webb
          isHf: false
          isPro: false
          name: olafthefrog
          type: user
        html: '<p>Same error here on a A100 80GB.</p>

          '
        raw: Same error here on a A100 80GB.
        updatedAt: '2023-07-19T05:48:15.327Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Aivean
    id: 64b7791fbdf37897307e0f84
    type: comment
  author: olafthefrog
  content: Same error here on a A100 80GB.
  created_at: 2023-07-19 04:48:15+00:00
  edited: false
  hidden: false
  id: 64b7791fbdf37897307e0f84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58bcc09ad80b7254b386ef1d1a94a993.svg
      fullname: Courtney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rabidaught
      type: user
    createdAt: '2023-07-19T06:10:42.000Z'
    data:
      edited: false
      editors:
      - rabidaught
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.941727340221405
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58bcc09ad80b7254b386ef1d1a94a993.svg
          fullname: Courtney
          isHf: false
          isPro: false
          name: rabidaught
          type: user
        html: '<p>There''s an architecture change with 70B.<br>ExLLaMA and AutoGPTQ
          issue.</p>

          '
        raw: 'There''s an architecture change with 70B.

          ExLLaMA and AutoGPTQ issue.'
        updatedAt: '2023-07-19T06:10:42.893Z'
      numEdits: 0
      reactions: []
    id: 64b77e62509b109d042fa139
    type: comment
  author: rabidaught
  content: 'There''s an architecture change with 70B.

    ExLLaMA and AutoGPTQ issue.'
  created_at: 2023-07-19 05:10:42+00:00
  edited: false
  hidden: false
  id: 64b77e62509b109d042fa139
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d1f9a569a1c0af19fbe093e2ed751a7.svg
      fullname: Ivan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aivean
      type: user
    createdAt: '2023-07-19T06:18:11.000Z'
    data:
      edited: true
      editors:
      - Aivean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7052903771400452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d1f9a569a1c0af19fbe093e2ed751a7.svg
          fullname: Ivan
          isHf: false
          isPro: false
          name: Aivean
          type: user
        html: '<blockquote>

          <p>There''s an architecture change with 70B.<br>ExLLaMA and AutoGPTQ issue.</p>

          </blockquote>

          <p>Do you mean there is a difference between 13b and 70b (former works fine)?</p>

          <p>In this case usage instructions and compatibility info should be updated:<br><a
          href="https://huggingface.co/TheBloke/Llama-2-70B-GPTQ#how-to-easily-download-and-use-this-model-in-text-generation-webui">https://huggingface.co/TheBloke/Llama-2-70B-GPTQ#how-to-easily-download-and-use-this-model-in-text-generation-webui</a></p>

          '
        raw: '> There''s an architecture change with 70B.

          > ExLLaMA and AutoGPTQ issue.


          Do you mean there is a difference between 13b and 70b (former works fine)?


          In this case usage instructions and compatibility info should be updated:

          https://huggingface.co/TheBloke/Llama-2-70B-GPTQ#how-to-easily-download-and-use-this-model-in-text-generation-webui'
        updatedAt: '2023-07-19T06:22:13.863Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eemotgs
    id: 64b78023954ae43365971244
    type: comment
  author: Aivean
  content: '> There''s an architecture change with 70B.

    > ExLLaMA and AutoGPTQ issue.


    Do you mean there is a difference between 13b and 70b (former works fine)?


    In this case usage instructions and compatibility info should be updated:

    https://huggingface.co/TheBloke/Llama-2-70B-GPTQ#how-to-easily-download-and-use-this-model-in-text-generation-webui'
  created_at: 2023-07-19 05:18:11+00:00
  edited: true
  hidden: false
  id: 64b78023954ae43365971244
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da407d29d74849ef7a30be8e84be498c.svg
      fullname: Eduard Martini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eemotgs
      type: user
    createdAt: '2023-07-19T07:01:13.000Z'
    data:
      edited: false
      editors:
      - eemotgs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7672526240348816
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da407d29d74849ef7a30be8e84be498c.svg
          fullname: Eduard Martini
          isHf: false
          isPro: false
          name: eemotgs
          type: user
        html: '<p>Same issue on 2xA6000. </p>

          '
        raw: 'Same issue on 2xA6000. '
        updatedAt: '2023-07-19T07:01:13.479Z'
      numEdits: 0
      reactions: []
    id: 64b78a3990b38df83384e03e
    type: comment
  author: eemotgs
  content: 'Same issue on 2xA6000. '
  created_at: 2023-07-19 06:01:13+00:00
  edited: false
  hidden: false
  id: 64b78a3990b38df83384e03e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-19T07:55:17.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9523099660873413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>This is because the <code>num_head</code> of <code>key</code> and
          <code>value</code> in attention for llama 70B is different with <code>num_attention_head</code>
          (you can check it from <code>config.json</code> in model uploaded by meta).
          That''s why in transformers there is new function named <code>repeat_kv</code>
          to accomodate this. Exllama and GPTQ not yet done it.</p>

          '
        raw: This is because the `num_head` of `key` and `value` in attention for
          llama 70B is different with `num_attention_head` (you can check it from
          `config.json` in model uploaded by meta). That's why in transformers there
          is new function named `repeat_kv` to accomodate this. Exllama and GPTQ not
          yet done it.
        updatedAt: '2023-07-19T07:55:17.148Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - algorithm
        - Aivean
    id: 64b796e5fdb702b3d86a9bca
    type: comment
  author: fahadh4ilyas
  content: This is because the `num_head` of `key` and `value` in attention for llama
    70B is different with `num_attention_head` (you can check it from `config.json`
    in model uploaded by meta). That's why in transformers there is new function named
    `repeat_kv` to accomodate this. Exllama and GPTQ not yet done it.
  created_at: 2023-07-19 06:55:17+00:00
  edited: false
  hidden: false
  id: 64b796e5fdb702b3d86a9bca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fbc9431b4b7a98b1d986e2787b36c853.svg
      fullname: Tea Lover
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tea-lover-418
      type: user
    createdAt: '2023-07-19T08:39:47.000Z'
    data:
      edited: false
      editors:
      - tea-lover-418
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7356981039047241
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fbc9431b4b7a98b1d986e2787b36c853.svg
          fullname: Tea Lover
          isHf: false
          isPro: false
          name: tea-lover-418
          type: user
        html: '<p>Same here on an A100 80gb. </p>

          '
        raw: 'Same here on an A100 80gb. '
        updatedAt: '2023-07-19T08:39:47.404Z'
      numEdits: 0
      reactions: []
    id: 64b7a153b727f8771ac30491
    type: comment
  author: tea-lover-418
  content: 'Same here on an A100 80gb. '
  created_at: 2023-07-19 07:39:47+00:00
  edited: false
  hidden: false
  id: 64b7a153b727f8771ac30491
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T11:19:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609853625297546
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, you need to update Transformers to the latest version. I should
          have mentioned that in the README, but it was already 4am and I forgot.</p>

          <p>Please run:</p>

          <pre><code>pip3 install git+https://github.com/huggingface/transformers

          </code></pre>

          <p>and try again.</p>

          '
        raw: 'Yes, you need to update Transformers to the latest version. I should
          have mentioned that in the README, but it was already 4am and I forgot.


          Please run:

          ```

          pip3 install git+https://github.com/huggingface/transformers

          ```


          and try again.

          '
        updatedAt: '2023-07-19T11:19:32.524Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Aivean
        - Toaster496
    id: 64b7c6c41d913123e6e57047
    type: comment
  author: TheBloke
  content: 'Yes, you need to update Transformers to the latest version. I should have
    mentioned that in the README, but it was already 4am and I forgot.


    Please run:

    ```

    pip3 install git+https://github.com/huggingface/transformers

    ```


    and try again.

    '
  created_at: 2023-07-19 10:19:32+00:00
  edited: false
  hidden: false
  id: 64b7c6c41d913123e6e57047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6383dc174c48969dcf1b4fce/4N-GY7jVvdk08kp2B8DLh.jpeg?w=200&h=200&f=face
      fullname: turboderp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: turboderp
      type: user
    createdAt: '2023-07-19T12:03:36.000Z'
    data:
      edited: false
      editors:
      - turboderp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9507122039794922
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6383dc174c48969dcf1b4fce/4N-GY7jVvdk08kp2B8DLh.jpeg?w=200&h=200&f=face
          fullname: turboderp
          isHf: false
          isPro: false
          name: turboderp
          type: user
        html: '<p>There is an architectural change to 70b, yes. They added grouped-query
          attention which needs to be added to ExLlama. It''s not a big change, though,
          and I''m on it, so be patient. Downloading all these models takes a while.
          And yes, 7b and 13b don''t have this change.</p>

          '
        raw: There is an architectural change to 70b, yes. They added grouped-query
          attention which needs to be added to ExLlama. It's not a big change, though,
          and I'm on it, so be patient. Downloading all these models takes a while.
          And yes, 7b and 13b don't have this change.
        updatedAt: '2023-07-19T12:03:36.906Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - martyn
        - tea-lover-418
        - Aivean
        - Toaster496
      - count: 1
        reaction: "\U0001F917"
        users:
        - rabidaught
    id: 64b7d118901f417d48674ce5
    type: comment
  author: turboderp
  content: There is an architectural change to 70b, yes. They added grouped-query
    attention which needs to be added to ExLlama. It's not a big change, though, and
    I'm on it, so be patient. Downloading all these models takes a while. And yes,
    7b and 13b don't have this change.
  created_at: 2023-07-19 11:03:36+00:00
  edited: false
  hidden: false
  id: 64b7d118901f417d48674ce5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T12:06:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594048857688904
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great, looking forward to! GfL and AutoGPTQ are slow as shit with
          this ;)</p>

          '
        raw: Great, looking forward to! GfL and AutoGPTQ are slow as shit with this
          ;)
        updatedAt: '2023-07-19T12:06:29.504Z'
      numEdits: 0
      reactions: []
    id: 64b7d1c58c632fbca9505853
    type: comment
  author: TheBloke
  content: Great, looking forward to! GfL and AutoGPTQ are slow as shit with this
    ;)
  created_at: 2023-07-19 11:06:29+00:00
  edited: false
  hidden: false
  id: 64b7d1c58c632fbca9505853
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b32fa65053b1189f82adfef74569e9a.svg
      fullname: Tristan Webb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: olafthefrog
      type: user
    createdAt: '2023-07-19T16:25:35.000Z'
    data:
      edited: false
      editors:
      - olafthefrog
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9841663837432861
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b32fa65053b1189f82adfef74569e9a.svg
          fullname: Tristan Webb
          isHf: false
          isPro: false
          name: olafthefrog
          type: user
        html: '<p>These turn around times are amazing guys, it looks like llama2 support
          was added to ExLlama. What''s that, 24 hours since the OG model dropped?</p>

          '
        raw: These turn around times are amazing guys, it looks like llama2 support
          was added to ExLlama. What's that, 24 hours since the OG model dropped?
        updatedAt: '2023-07-19T16:25:35.224Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Aivean
        - tea-lover-418
    id: 64b80e7fb78381f38f55a854
    type: comment
  author: olafthefrog
  content: These turn around times are amazing guys, it looks like llama2 support
    was added to ExLlama. What's that, 24 hours since the OG model dropped?
  created_at: 2023-07-19 15:25:35+00:00
  edited: false
  hidden: false
  id: 64b80e7fb78381f38f55a854
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d1f9a569a1c0af19fbe093e2ed751a7.svg
      fullname: Ivan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aivean
      type: user
    createdAt: '2023-07-20T01:47:47.000Z'
    data:
      edited: false
      editors:
      - Aivean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7642842531204224
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d1f9a569a1c0af19fbe093e2ed751a7.svg
          fullname: Ivan
          isHf: false
          isPro: false
          name: Aivean
          type: user
        html: '<p>Awesome! Can confirm that after updating <code>text-generation-webui</code>
          and updating pip deps, ExLlama loader worked! Thanks, everyone!</p>

          '
        raw: Awesome! Can confirm that after updating `text-generation-webui` and
          updating pip deps, ExLlama loader worked! Thanks, everyone!
        updatedAt: '2023-07-20T01:47:47.684Z'
      numEdits: 0
      reactions: []
    id: 64b892432fccad9f5f002640
    type: comment
  author: Aivean
  content: Awesome! Can confirm that after updating `text-generation-webui` and updating
    pip deps, ExLlama loader worked! Thanks, everyone!
  created_at: 2023-07-20 00:47:47+00:00
  edited: false
  hidden: false
  id: 64b892432fccad9f5f002640
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: The `main` branch for TheBloke/Llama-2-70B-GPTQ appears borked
