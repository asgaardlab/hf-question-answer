!!python/object:huggingface_hub.community.DiscussionWithDetails
author: OSK-Creative-Tech
conflicting_files: null
created_at: 2023-10-24 07:29:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2e525b2d41a76d442a1bf3ba572c02b.svg
      fullname: Gregor Schermuly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OSK-Creative-Tech
      type: user
    createdAt: '2023-10-24T08:29:54.000Z'
    data:
      edited: true
      editors:
      - OSK-Creative-Tech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9529144763946533
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2e525b2d41a76d442a1bf3ba572c02b.svg
          fullname: Gregor Schermuly
          isHf: false
          isPro: false
          name: OSK-Creative-Tech
          type: user
        html: '<p>Hello. I am in need to fine-tune Llama 70b for a specific task.<br>It
          was done on Llama 7b, 13b, and Mistral 7b before, but seems like not enough
          power with these smaller models.<br>Fine-tuning was done with HF Autotrain
          and with QLorA (bitsandbytes, peft) approach.</p>

          <p>Could the same way of fine-tuning be applied to this quantized model?<br>Or
          should I use the base 70b llama2 model and quantize it after fine-tuning?</p>

          '
        raw: "Hello. I am in need to fine-tune Llama 70b for a specific task. \nIt\
          \ was done on Llama 7b, 13b, and Mistral 7b before, but seems like not enough\
          \ power with these smaller models.\nFine-tuning was done with HF Autotrain\
          \ and with QLorA (bitsandbytes, peft) approach.\n\nCould the same way of\
          \ fine-tuning be applied to this quantized model?\nOr should I use the base\
          \ 70b llama2 model and quantize it after fine-tuning?"
        updatedAt: '2023-10-30T09:10:00.536Z'
      numEdits: 1
      reactions: []
    id: 65378082fb6eb06b92c0dd79
    type: comment
  author: OSK-Creative-Tech
  content: "Hello. I am in need to fine-tune Llama 70b for a specific task. \nIt was\
    \ done on Llama 7b, 13b, and Mistral 7b before, but seems like not enough power\
    \ with these smaller models.\nFine-tuning was done with HF Autotrain and with\
    \ QLorA (bitsandbytes, peft) approach.\n\nCould the same way of fine-tuning be\
    \ applied to this quantized model?\nOr should I use the base 70b llama2 model\
    \ and quantize it after fine-tuning?"
  created_at: 2023-10-24 07:29:54+00:00
  edited: true
  hidden: false
  id: 65378082fb6eb06b92c0dd79
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/Llama-2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: Fine-tuning?
