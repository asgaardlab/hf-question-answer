!!python/object:huggingface_hub.community.DiscussionWithDetails
author: matchaslime
conflicting_files: null
created_at: 2023-07-19 20:23:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d502cf72928522e813be0c9d058278b.svg
      fullname: Bernard Wan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matchaslime
      type: user
    createdAt: '2023-07-19T21:23:08.000Z'
    data:
      edited: false
      editors:
      - matchaslime
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.962146520614624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d502cf72928522e813be0c9d058278b.svg
          fullname: Bernard Wan
          isHf: false
          isPro: false
          name: matchaslime
          type: user
        html: '<p>Hi, I am following the instructions to use in python code, but the
          model is always outputting the same response to the same prompt. Changing
          the temperature does not seem to do anything. What could be the issue here?</p>

          '
        raw: Hi, I am following the instructions to use in python code, but the model
          is always outputting the same response to the same prompt. Changing the
          temperature does not seem to do anything. What could be the issue here?
        updatedAt: '2023-07-19T21:23:08.168Z'
      numEdits: 0
      reactions: []
    id: 64b8543ca771ebc065eb7a27
    type: comment
  author: matchaslime
  content: Hi, I am following the instructions to use in python code, but the model
    is always outputting the same response to the same prompt. Changing the temperature
    does not seem to do anything. What could be the issue here?
  created_at: 2023-07-19 20:23:08+00:00
  edited: false
  hidden: false
  id: 64b8543ca771ebc065eb7a27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95f63350daf3939fc375abc6b8421543.svg
      fullname: Robin van den Hoogen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robinlowen
      type: user
    createdAt: '2023-07-20T18:36:27.000Z'
    data:
      edited: false
      editors:
      - robinlowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9807426333427429
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95f63350daf3939fc375abc6b8421543.svg
          fullname: Robin van den Hoogen
          isHf: false
          isPro: false
          name: robinlowen
          type: user
        html: '<p>could you provide some code?</p>

          '
        raw: could you provide some code?
        updatedAt: '2023-07-20T18:36:27.395Z'
      numEdits: 0
      reactions: []
    id: 64b97eab4168d2a8d541b0af
    type: comment
  author: robinlowen
  content: could you provide some code?
  created_at: 2023-07-20 17:36:27+00:00
  edited: false
  hidden: false
  id: 64b97eab4168d2a8d541b0af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d502cf72928522e813be0c9d058278b.svg
      fullname: Bernard Wan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matchaslime
      type: user
    createdAt: '2023-07-21T06:29:22.000Z'
    data:
      edited: false
      editors:
      - matchaslime
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7513574361801147
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d502cf72928522e813be0c9d058278b.svg
          fullname: Bernard Wan
          isHf: false
          isPro: false
          name: matchaslime
          type: user
        html: '<p>I''m just using the example code in the readme for the autogptq
          section</p>

          '
        raw: I'm just using the example code in the readme for the autogptq section
        updatedAt: '2023-07-21T06:29:22.638Z'
      numEdits: 0
      reactions: []
    id: 64ba25c2594d62250b05c9c0
    type: comment
  author: matchaslime
  content: I'm just using the example code in the readme for the autogptq section
  created_at: 2023-07-21 05:29:22+00:00
  edited: false
  hidden: false
  id: 64ba25c2594d62250b05c9c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T08:33:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5190426707267761
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>When I have tested inference before, I have had code to change the\
          \ seed, like so:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-meta\">    @property</span>\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">seed</span>(<span class=\"hljs-params\"\
          >self</span>):\n        <span class=\"hljs-keyword\">return</span> self._current_seed\n\
          \n<span class=\"hljs-meta\">    <span data-props=\"{&quot;user&quot;:&quot;seed&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/seed\"\
          >@<span class=\"underline\">seed</span></a></span>\n\n\t</span></span>.setter</span>\n\
          \    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >seed</span>(<span class=\"hljs-params\">self, seed</span>):\n        self._seed\
          \ = <span class=\"hljs-built_in\">int</span>(seed)\n\n    <span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">update_seed</span>(<span\
          \ class=\"hljs-params\">self</span>):\n        self._current_seed = (self._seed\
          \ == -<span class=\"hljs-number\">1</span> ) <span class=\"hljs-keyword\"\
          >and</span> random.randint(<span class=\"hljs-number\">1</span>, <span class=\"\
          hljs-number\">2</span>**<span class=\"hljs-number\">31</span>) <span class=\"\
          hljs-keyword\">or</span> self._seed\n        random.seed(self._current_seed)\n\
          \        torch.manual_seed(self._current_seed)\n        torch.cuda.manual_seed_all(self._current_seed)\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >generate</span>(<span class=\"hljs-params\">self, prompt</span>):\n   \
          \     self.update_seed()\n        input_ids, len_input_ids = self.encode(prompt)\n\
          \n        <span class=\"hljs-keyword\">with</span> self.do_timing(<span\
          \ class=\"hljs-literal\">True</span>) <span class=\"hljs-keyword\">as</span>\
          \ timing:\n            <span class=\"hljs-keyword\">with</span> torch.no_grad():\n\
          \                tokens = self.model.generate(inputs=input_ids, generation_config=self.generation_config)[<span\
          \ class=\"hljs-number\">0</span>].cuda()\n            len_reply = <span\
          \ class=\"hljs-built_in\">len</span>(tokens) - len_input_ids\n         \
          \   response = self.tokenizer.decode(tokens)\n            reply_tokens =\
          \ tokens[-len_reply:]\n            reply = self.tokenizer.decode(reply_tokens)\n\
          \n        result = {\n            <span class=\"hljs-string\">'response'</span>:\
          \ response,   <span class=\"hljs-comment\"># The response in full, including\
          \ prompt</span>\n            <span class=\"hljs-string\">'reply'</span>:\
          \ reply,         <span class=\"hljs-comment\"># Just the reply, no prompt</span>\n\
          \            <span class=\"hljs-string\">'len_reply'</span>: len_reply,\
          \ <span class=\"hljs-comment\"># The length of the reply tokens</span>\n\
          \            <span class=\"hljs-string\">'seed'</span>: self.seed,     \
          \ <span class=\"hljs-comment\"># The seed used to generate this response</span>\n\
          \            <span class=\"hljs-string\">'time'</span>: timing[<span class=\"\
          hljs-string\">'time'</span>]  <span class=\"hljs-comment\"># The time in\
          \ seconds to generate the response</span>\n        }\n        <span class=\"\
          hljs-keyword\">return</span> result\n</code></pre>\n<p>You could try the\
          \ same to get a different seed for each generation.</p>\n"
        raw: "When I have tested inference before, I have had code to change the seed,\
          \ like so:\n\n```python\n    @property\n    def seed(self):\n        return\
          \ self._current_seed\n\n    @seed.setter\n    def seed(self, seed):\n  \
          \      self._seed = int(seed)\n\n    def update_seed(self):\n        self._current_seed\
          \ = (self._seed == -1 ) and random.randint(1, 2**31) or self._seed\n   \
          \     random.seed(self._current_seed)\n        torch.manual_seed(self._current_seed)\n\
          \        torch.cuda.manual_seed_all(self._current_seed)\n\n    def generate(self,\
          \ prompt):\n        self.update_seed()\n        input_ids, len_input_ids\
          \ = self.encode(prompt)\n\n        with self.do_timing(True) as timing:\n\
          \            with torch.no_grad():\n                tokens = self.model.generate(inputs=input_ids,\
          \ generation_config=self.generation_config)[0].cuda()\n            len_reply\
          \ = len(tokens) - len_input_ids\n            response = self.tokenizer.decode(tokens)\n\
          \            reply_tokens = tokens[-len_reply:]\n            reply = self.tokenizer.decode(reply_tokens)\n\
          \n        result = {\n            'response': response,   # The response\
          \ in full, including prompt\n            'reply': reply,         # Just\
          \ the reply, no prompt\n            'len_reply': len_reply, # The length\
          \ of the reply tokens\n            'seed': self.seed,      # The seed used\
          \ to generate this response\n            'time': timing['time']  # The time\
          \ in seconds to generate the response\n        }\n        return result\n\
          ```\n\nYou could try the same to get a different seed for each generation."
        updatedAt: '2023-07-21T08:33:01.755Z'
      numEdits: 0
      reactions: []
    id: 64ba42bdcccaf002e051bbd7
    type: comment
  author: TheBloke
  content: "When I have tested inference before, I have had code to change the seed,\
    \ like so:\n\n```python\n    @property\n    def seed(self):\n        return self._current_seed\n\
    \n    @seed.setter\n    def seed(self, seed):\n        self._seed = int(seed)\n\
    \n    def update_seed(self):\n        self._current_seed = (self._seed == -1 )\
    \ and random.randint(1, 2**31) or self._seed\n        random.seed(self._current_seed)\n\
    \        torch.manual_seed(self._current_seed)\n        torch.cuda.manual_seed_all(self._current_seed)\n\
    \n    def generate(self, prompt):\n        self.update_seed()\n        input_ids,\
    \ len_input_ids = self.encode(prompt)\n\n        with self.do_timing(True) as\
    \ timing:\n            with torch.no_grad():\n                tokens = self.model.generate(inputs=input_ids,\
    \ generation_config=self.generation_config)[0].cuda()\n            len_reply =\
    \ len(tokens) - len_input_ids\n            response = self.tokenizer.decode(tokens)\n\
    \            reply_tokens = tokens[-len_reply:]\n            reply = self.tokenizer.decode(reply_tokens)\n\
    \n        result = {\n            'response': response,   # The response in full,\
    \ including prompt\n            'reply': reply,         # Just the reply, no prompt\n\
    \            'len_reply': len_reply, # The length of the reply tokens\n      \
    \      'seed': self.seed,      # The seed used to generate this response\n   \
    \         'time': timing['time']  # The time in seconds to generate the response\n\
    \        }\n        return result\n```\n\nYou could try the same to get a different\
    \ seed for each generation."
  created_at: 2023-07-21 07:33:01+00:00
  edited: false
  hidden: false
  id: 64ba42bdcccaf002e051bbd7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Llama-2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: Problems with temperature when using with python code.
