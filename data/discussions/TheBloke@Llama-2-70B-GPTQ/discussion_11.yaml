!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tonycloud
conflicting_files: null
created_at: 2023-08-04 01:54:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
      fullname: zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonycloud
      type: user
    createdAt: '2023-08-04T02:54:04.000Z'
    data:
      edited: false
      editors:
      - tonycloud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8646398782730103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
          fullname: zhao
          isHf: false
          isPro: false
          name: tonycloud
          type: user
        html: '<p>now I want quant llama2 70b model, and I use AutoGPTQ, but I can
          not success.</p>

          '
        raw: now I want quant llama2 70b model, and I use AutoGPTQ, but I can not
          success.
        updatedAt: '2023-08-04T02:54:04.918Z'
      numEdits: 0
      reactions: []
    id: 64cc684c4dcdaead7a27dd57
    type: comment
  author: tonycloud
  content: now I want quant llama2 70b model, and I use AutoGPTQ, but I can not success.
  created_at: 2023-08-04 01:54:04+00:00
  edited: false
  hidden: false
  id: 64cc684c4dcdaead7a27dd57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
      fullname: zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonycloud
      type: user
    createdAt: '2023-08-04T02:54:54.000Z'
    data:
      edited: false
      editors:
      - tonycloud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5838757753372192
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
          fullname: zhao
          isHf: false
          isPro: false
          name: tonycloud
          type: user
        html: "<p>Can you describe in detail how to quantify the llama270b model using\
          \ AutoGPTQ\uFF1F</p>\n"
        raw: "Can you describe in detail how to quantify the llama270b model using\
          \ AutoGPTQ\uFF1F"
        updatedAt: '2023-08-04T02:54:54.386Z'
      numEdits: 0
      reactions: []
    id: 64cc687e9172674336485606
    type: comment
  author: tonycloud
  content: "Can you describe in detail how to quantify the llama270b model using AutoGPTQ\uFF1F"
  created_at: 2023-08-04 01:54:54+00:00
  edited: false
  hidden: false
  id: 64cc687e9172674336485606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T09:38:27.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7582855224609375
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Firstly you need to update to AutoGPTQ 0.3.2, which I currently\
          \ recommend is done by building from source:</p>\n<pre><code>pip3 uninstall\
          \ -y auto-gptq\ngit clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\n\
          pip3 install .\n</code></pre>\n<p>There's no special steps for quantising\
          \ Llama 2 70B compared to other models - as long as you're running AutoGPTQ\
          \ 0.3.2 with Transformers 4.31.0 or later (which will be automatically installed\
          \ when you update AutoGPTQ), it will work.</p>\n<p>You can use my AutoGPTQ\
          \ wrapper script: <a rel=\"nofollow\" href=\"https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py\"\
          >https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py</a></p>\n\
          <p>Example execution, to produce a 4-bit, 128g, act-order model:</p>\n<pre><code>\
          \ python3 quant_autogptq.py /path/to/Llama-2-70B /path/to/save/gptq wikitext\
          \ --bits 4 --group_size 128 --desc_act 1 --damp 0.1 --dtype float16 --seqlen\
          \ 4096 --num_samples 128 --use_fast \n</code></pre>\n<p>To quantise at sequence\
          \ length 4096 (recommended) you will need a 48GB GPU.  And you will need\
          \ at least 200GB RAM in order to quantise and pack the model.  Expect it\
          \ to take 2-4 hours, depending on the speed of the system.</p>\n<p>If you\
          \ only have a 24GB GPU you can try <code>--seqlen 2048</code> instead, which\
          \ will also be quicker.  But the quality of the quantisation won't be as\
          \ good.</p>\n"
        raw: "Firstly you need to update to AutoGPTQ 0.3.2, which I currently recommend\
          \ is done by building from source:\n```\npip3 uninstall -y auto-gptq\ngit\
          \ clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip3 install\
          \ .\n```\n\nThere's no special steps for quantising Llama 2 70B compared\
          \ to other models - as long as you're running AutoGPTQ 0.3.2 with Transformers\
          \ 4.31.0 or later (which will be automatically installed when you update\
          \ AutoGPTQ), it will work.\n\nYou can use my AutoGPTQ wrapper script: https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py\n\
          \nExample execution, to produce a 4-bit, 128g, act-order model:\n```\n python3\
          \ quant_autogptq.py /path/to/Llama-2-70B /path/to/save/gptq wikitext --bits\
          \ 4 --group_size 128 --desc_act 1 --damp 0.1 --dtype float16 --seqlen 4096\
          \ --num_samples 128 --use_fast \n```\n\nTo quantise at sequence length 4096\
          \ (recommended) you will need a 48GB GPU.  And you will need at least 200GB\
          \ RAM in order to quantise and pack the model.  Expect it to take 2-4 hours,\
          \ depending on the speed of the system.\n\nIf you only have a 24GB GPU you\
          \ can try `--seqlen 2048` instead, which will also be quicker.  But the\
          \ quality of the quantisation won't be as good."
        updatedAt: '2023-08-05T09:39:30.819Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - GralchemOz
        - lockon
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
    id: 64ce189315b26cc7f7b06a12
    type: comment
  author: TheBloke
  content: "Firstly you need to update to AutoGPTQ 0.3.2, which I currently recommend\
    \ is done by building from source:\n```\npip3 uninstall -y auto-gptq\ngit clone\
    \ https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip3 install .\n```\n\nThere's\
    \ no special steps for quantising Llama 2 70B compared to other models - as long\
    \ as you're running AutoGPTQ 0.3.2 with Transformers 4.31.0 or later (which will\
    \ be automatically installed when you update AutoGPTQ), it will work.\n\nYou can\
    \ use my AutoGPTQ wrapper script: https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py\n\
    \nExample execution, to produce a 4-bit, 128g, act-order model:\n```\n python3\
    \ quant_autogptq.py /path/to/Llama-2-70B /path/to/save/gptq wikitext --bits 4\
    \ --group_size 128 --desc_act 1 --damp 0.1 --dtype float16 --seqlen 4096 --num_samples\
    \ 128 --use_fast \n```\n\nTo quantise at sequence length 4096 (recommended) you\
    \ will need a 48GB GPU.  And you will need at least 200GB RAM in order to quantise\
    \ and pack the model.  Expect it to take 2-4 hours, depending on the speed of\
    \ the system.\n\nIf you only have a 24GB GPU you can try `--seqlen 2048` instead,\
    \ which will also be quicker.  But the quality of the quantisation won't be as\
    \ good."
  created_at: 2023-08-05 08:38:27+00:00
  edited: true
  hidden: false
  id: 64ce189315b26cc7f7b06a12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
      fullname: zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonycloud
      type: user
    createdAt: '2023-08-07T02:49:36.000Z'
    data:
      edited: false
      editors:
      - tonycloud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.994533896446228
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
          fullname: zhao
          isHf: false
          isPro: false
          name: tonycloud
          type: user
        html: '<p>ok, thank you very much. I will test it.<br>Thank you very much
          for your work.</p>

          '
        raw: 'ok, thank you very much. I will test it.

          Thank you very much for your work.'
        updatedAt: '2023-08-07T02:49:36.737Z'
      numEdits: 0
      reactions: []
    id: 64d05bc04dfd5df70734fd51
    type: comment
  author: tonycloud
  content: 'ok, thank you very much. I will test it.

    Thank you very much for your work.'
  created_at: 2023-08-07 01:49:36+00:00
  edited: false
  hidden: false
  id: 64d05bc04dfd5df70734fd51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
      fullname: zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonycloud
      type: user
    createdAt: '2023-08-07T07:27:26.000Z'
    data:
      edited: false
      editors:
      - tonycloud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9694307446479797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1afc074f88ccb5a1694cb6212c6b8d0d.svg
          fullname: zhao
          isHf: false
          isPro: false
          name: tonycloud
          type: user
        html: '<p>I have obtained the 4-bit quantized model, and would like to know
          whether this model can be loaded by text-generation-inference</p>

          '
        raw: I have obtained the 4-bit quantized model, and would like to know whether
          this model can be loaded by text-generation-inference
        updatedAt: '2023-08-07T07:27:26.674Z'
      numEdits: 0
      reactions: []
    id: 64d09cdec9d00e38470492c7
    type: comment
  author: tonycloud
  content: I have obtained the 4-bit quantized model, and would like to know whether
    this model can be loaded by text-generation-inference
  created_at: 2023-08-07 06:27:26+00:00
  edited: false
  hidden: false
  id: 64d09cdec9d00e38470492c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/Llama-2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: how to quant llama2 70b model with AutoGPTQ
