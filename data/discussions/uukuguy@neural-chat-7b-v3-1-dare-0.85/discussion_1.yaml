!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bartowski
conflicting_files: null
created_at: 2023-11-23 03:58:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bartowski
      type: user
    createdAt: '2023-11-23T03:58:36.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9735366106033325
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: '<p>From reading the DARE paper, I''m not sure I understand what it
          is these models are</p>

          <p>Are these a merge of the base model and something else? Or is it SFT?
          And with what dataset?</p>

          '
        raw: "From reading the DARE paper, I'm not sure I understand what it is these\
          \ models are\r\n\r\nAre these a merge of the base model and something else?\
          \ Or is it SFT? And with what dataset?"
        updatedAt: '2023-11-23T03:58:36.225Z'
      numEdits: 0
      reactions: []
    id: 655ecdec02567cbe4bb51f03
    type: comment
  author: bartowski
  content: "From reading the DARE paper, I'm not sure I understand what it is these\
    \ models are\r\n\r\nAre these a merge of the base model and something else? Or\
    \ is it SFT? And with what dataset?"
  created_at: 2023-11-23 03:58:36+00:00
  edited: false
  hidden: false
  id: 655ecdec02567cbe4bb51f03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64298a9b8852afdf89bd8846/it61DWZSkPbM4clfrLaPw.png?w=200&h=200&f=face
      fullname: Jiangwen Su
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: uukuguy
      type: user
    createdAt: '2023-11-23T04:52:13.000Z'
    data:
      edited: false
      editors:
      - uukuguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.893669843673706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64298a9b8852afdf89bd8846/it61DWZSkPbM4clfrLaPw.png?w=200&h=200&f=face
          fullname: Jiangwen Su
          isHf: false
          isPro: false
          name: uukuguy
          type: user
        html: '<p>I am trying to use the DARE method to mitigate or eliminate the
          mutual interference between models when merging multiple homologous PEFT
          models, 0.85 means the drop rate in DARE. <a rel="nofollow" href="https://github.com/uukuguy/multi_loras#mixture-of-multi-loras">https://github.com/uukuguy/multi_loras#mixture-of-multi-loras</a></p>

          '
        raw: I am trying to use the DARE method to mitigate or eliminate the mutual
          interference between models when merging multiple homologous PEFT models,
          0.85 means the drop rate in DARE. https://github.com/uukuguy/multi_loras#mixture-of-multi-loras
        updatedAt: '2023-11-23T04:52:13.103Z'
      numEdits: 0
      reactions: []
    id: 655eda7d0bda1e8ff82d5632
    type: comment
  author: uukuguy
  content: I am trying to use the DARE method to mitigate or eliminate the mutual
    interference between models when merging multiple homologous PEFT models, 0.85
    means the drop rate in DARE. https://github.com/uukuguy/multi_loras#mixture-of-multi-loras
  created_at: 2023-11-23 04:52:13+00:00
  edited: false
  hidden: false
  id: 655eda7d0bda1e8ff82d5632
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bartowski
      type: user
    createdAt: '2023-11-23T07:23:40.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9869359731674194
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: '<p>Are these meant to be used as-is or do they still need to be merged?
          Are you basically re-tuning these models with DARE?</p>

          <p>My confusion is that I thought DARE was for merging or fine tuning, but
          don''t know what this is a merge/tune of</p>

          '
        raw: 'Are these meant to be used as-is or do they still need to be merged?
          Are you basically re-tuning these models with DARE?


          My confusion is that I thought DARE was for merging or fine tuning, but
          don''t know what this is a merge/tune of'
        updatedAt: '2023-11-23T07:23:40.607Z'
      numEdits: 0
      reactions: []
    id: 655efdfcc073ea9cf85bb15c
    type: comment
  author: bartowski
  content: 'Are these meant to be used as-is or do they still need to be merged? Are
    you basically re-tuning these models with DARE?


    My confusion is that I thought DARE was for merging or fine tuning, but don''t
    know what this is a merge/tune of'
  created_at: 2023-11-23 07:23:40+00:00
  edited: false
  hidden: false
  id: 655efdfcc073ea9cf85bb15c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64298a9b8852afdf89bd8846/it61DWZSkPbM4clfrLaPw.png?w=200&h=200&f=face
      fullname: Jiangwen Su
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: uukuguy
      type: user
    createdAt: '2023-11-23T07:43:18.000Z'
    data:
      edited: false
      editors:
      - uukuguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9309107661247253
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64298a9b8852afdf89bd8846/it61DWZSkPbM4clfrLaPw.png?w=200&h=200&f=face
          fullname: Jiangwen Su
          isHf: false
          isPro: false
          name: uukuguy
          type: user
        html: '<p>As the paper said,  we can "obtain new capabilities by assimilating
          the parameters of homologous models without the need for retraining or GPUs".<br>By
          drop the redundant delta parameters, it''s possible to mitigate the mutual
          interference between merging models. What I want to do is try to verify
          this point.  If the verification is successful, then I may have the possibility
          to merge multiple homologous models and maintain the prominent advantages
          of each model. And all of this does not require retraining the model, which
          is the most appealing aspect to me.</p>

          '
        raw: 'As the paper said,  we can "obtain new capabilities by assimilating
          the parameters of homologous models without the need for retraining or GPUs".

          By drop the redundant delta parameters, it''s possible to mitigate the mutual
          interference between merging models. What I want to do is try to verify
          this point.  If the verification is successful, then I may have the possibility
          to merge multiple homologous models and maintain the prominent advantages
          of each model. And all of this does not require retraining the model, which
          is the most appealing aspect to me.'
        updatedAt: '2023-11-23T07:43:18.150Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - waldie
        - bartowski
        - sethuiyer
    id: 655f0296f74fa124d1137a60
    type: comment
  author: uukuguy
  content: 'As the paper said,  we can "obtain new capabilities by assimilating the
    parameters of homologous models without the need for retraining or GPUs".

    By drop the redundant delta parameters, it''s possible to mitigate the mutual
    interference between merging models. What I want to do is try to verify this point.  If
    the verification is successful, then I may have the possibility to merge multiple
    homologous models and maintain the prominent advantages of each model. And all
    of this does not require retraining the model, which is the most appealing aspect
    to me.'
  created_at: 2023-11-23 07:43:18+00:00
  edited: false
  hidden: false
  id: 655f0296f74fa124d1137a60
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: uukuguy/neural-chat-7b-v3-1-dare-0.85
repo_type: model
status: open
target_branch: null
title: What exactly is the 0.85?
