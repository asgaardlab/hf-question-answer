!!python/object:huggingface_hub.community.DiscussionWithDetails
author: robert1968
conflicting_files: null
created_at: 2023-10-16 17:32:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-10-16T18:32:08.000Z'
    data:
      edited: false
      editors:
      - robert1968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9166292548179626
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
          fullname: Robert Szabo
          isHf: false
          isPro: false
          name: robert1968
          type: user
        html: '<p>Hi,<br>i have RTX3060 12GB GPU, but get CUDA out of memory. i was
          able to load and run TheBloke_samantha-1.2-mistral-7B-AWQ which is ~ similar
          size.<br>i used AutoAWQ Model loader.<br>any help appreciated.<br>thx</p>

          '
        raw: "Hi, \r\ni have RTX3060 12GB GPU, but get CUDA out of memory. i was able\
          \ to load and run TheBloke_samantha-1.2-mistral-7B-AWQ which is ~ similar\
          \ size. \r\ni used AutoAWQ Model loader.\r\nany help appreciated. \r\nthx"
        updatedAt: '2023-10-16T18:32:08.138Z'
      numEdits: 0
      reactions: []
    id: 652d81a8956d6a4244e56bf2
    type: comment
  author: robert1968
  content: "Hi, \r\ni have RTX3060 12GB GPU, but get CUDA out of memory. i was able\
    \ to load and run TheBloke_samantha-1.2-mistral-7B-AWQ which is ~ similar size.\
    \ \r\ni used AutoAWQ Model loader.\r\nany help appreciated. \r\nthx"
  created_at: 2023-10-16 17:32:08+00:00
  edited: false
  hidden: false
  id: 652d81a8956d6a4244e56bf2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Mistral-11B-OmniMix-AWQ
repo_type: model
status: open
target_branch: null
title: Why cant load to 12GB GPU?
