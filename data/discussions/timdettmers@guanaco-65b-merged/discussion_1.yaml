!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zzman
conflicting_files: null
created_at: 2023-05-30 03:18:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16a92b2db9a8fb07e323862d7109b958.svg
      fullname: Alex Zivkovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zzman
      type: user
    createdAt: '2023-05-30T04:18:11.000Z'
    data:
      edited: false
      editors:
      - zzman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16a92b2db9a8fb07e323862d7109b958.svg
          fullname: Alex Zivkovic
          isHf: false
          isPro: false
          name: zzman
          type: user
        html: '<p>tokenizer = AutoTokenizer.from_pretrained("timdettmers/guanaco-65b-merged")<br>gives
          following error:<br>ValueError: Couldn''t instantiate the backend tokenizer
          from one of:<br>(1) a <code>tokenizers</code> library serialization file,<br>(2)
          a slow tokenizer instance to convert or<br>(3) an equivalent slow tokenizer
          class to instantiate and convert.<br>You need to have sentencepiece installed
          to convert a slow tokenizer to a fast one.</p>

          <p>I made sure that sentencepiece is installed. </p>

          '
        raw: "tokenizer = AutoTokenizer.from_pretrained(\"timdettmers/guanaco-65b-merged\"\
          )\r\ngives following error:\r\nValueError: Couldn't instantiate the backend\
          \ tokenizer from one of: \r\n(1) a `tokenizers` library serialization file,\
          \ \r\n(2) a slow tokenizer instance to convert or \r\n(3) an equivalent\
          \ slow tokenizer class to instantiate and convert. \r\nYou need to have\
          \ sentencepiece installed to convert a slow tokenizer to a fast one.\r\n\
          \r\nI made sure that sentencepiece is installed. "
        updatedAt: '2023-05-30T04:18:11.036Z'
      numEdits: 0
      reactions: []
    id: 64757903a855203d8fef7665
    type: comment
  author: zzman
  content: "tokenizer = AutoTokenizer.from_pretrained(\"timdettmers/guanaco-65b-merged\"\
    )\r\ngives following error:\r\nValueError: Couldn't instantiate the backend tokenizer\
    \ from one of: \r\n(1) a `tokenizers` library serialization file, \r\n(2) a slow\
    \ tokenizer instance to convert or \r\n(3) an equivalent slow tokenizer class\
    \ to instantiate and convert. \r\nYou need to have sentencepiece installed to\
    \ convert a slow tokenizer to a fast one.\r\n\r\nI made sure that sentencepiece\
    \ is installed. "
  created_at: 2023-05-30 03:18:11+00:00
  edited: false
  hidden: false
  id: 64757903a855203d8fef7665
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16a92b2db9a8fb07e323862d7109b958.svg
      fullname: Alex Zivkovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zzman
      type: user
    createdAt: '2023-05-30T12:09:29.000Z'
    data:
      edited: false
      editors:
      - zzman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16a92b2db9a8fb07e323862d7109b958.svg
          fullname: Alex Zivkovic
          isHf: false
          isPro: false
          name: zzman
          type: user
        html: '<p>I do have a workaround by using the tokenizer from TheBloke model:<br>tokenizer
          = AutoTokenizer.from_pretrained("TheBloke/guanaco-65B-HF")</p>

          '
        raw: "I do have a workaround by using the tokenizer from TheBloke model: \n\
          tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/guanaco-65B-HF\")"
        updatedAt: '2023-05-30T12:09:29.633Z'
      numEdits: 0
      reactions: []
    id: 6475e779c894b5c9cf7500ca
    type: comment
  author: zzman
  content: "I do have a workaround by using the tokenizer from TheBloke model: \n\
    tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/guanaco-65B-HF\")"
  created_at: 2023-05-30 11:09:29+00:00
  edited: false
  hidden: false
  id: 6475e779c894b5c9cf7500ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7bed7de9ddba8db95c1e150d57b24f7.svg
      fullname: Michael Simkin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: simsim314
      type: user
    createdAt: '2023-06-07T13:03:58.000Z'
    data:
      edited: true
      editors:
      - simsim314
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8998050689697266
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7bed7de9ddba8db95c1e150d57b24f7.svg
          fullname: Michael Simkin
          isHf: false
          isPro: false
          name: simsim314
          type: user
        html: '<p>He didn''t need to post a tokenizer as it''s just fine-tune from
          llama-65b-hf model, with the exact same tokenizer.<br>Just use:<br><code>model_name
          = "decapoda-research/llama-65b-hf"</code><br><code>tokenizer = LlamaTokenizer.from_pretrained(model_name)</code></p>

          <p>PS. You will need to <code>import LlamaTokenizer</code></p>

          '
        raw: 'He didn''t need to post a tokenizer as it''s just fine-tune from llama-65b-hf
          model, with the exact same tokenizer.

          Just use:

          `model_name = "decapoda-research/llama-65b-hf"`

          `tokenizer = LlamaTokenizer.from_pretrained(model_name)`


          PS. You will need to `import LlamaTokenizer`'
        updatedAt: '2023-06-07T13:06:59.705Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Drshafi
        - Tom-Neverwinter
    id: 6480803e9aafd41918a7e6f8
    type: comment
  author: simsim314
  content: 'He didn''t need to post a tokenizer as it''s just fine-tune from llama-65b-hf
    model, with the exact same tokenizer.

    Just use:

    `model_name = "decapoda-research/llama-65b-hf"`

    `tokenizer = LlamaTokenizer.from_pretrained(model_name)`


    PS. You will need to `import LlamaTokenizer`'
  created_at: 2023-06-07 12:03:58+00:00
  edited: true
  hidden: false
  id: 6480803e9aafd41918a7e6f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2dcec944e9eb3c196981b79d302ab563.svg
      fullname: shafiullah qureshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Drshafi
      type: user
    createdAt: '2023-06-15T21:04:21.000Z'
    data:
      edited: true
      editors:
      - Drshafi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7676662802696228
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2dcec944e9eb3c196981b79d302ab563.svg
          fullname: shafiullah qureshi
          isHf: false
          isPro: false
          name: Drshafi
          type: user
        html: '<p>Why can''t we used guanaco-65b-merged or esle we will have to merge
          by ourselves as given below. How much GPU power is needed to run this model?</p>

          <p>model_name = "decapoda-research/llama-65b-hf"<br>adapters_name = ''timdettmers/guanaco-65b''</p>

          <p>model = AutoModelForCausalLM.from_pretrained(<br>    model_name,<br>    #load_in_4bit=True,<br>    torch_dtype=torch.bfloat16,<br>    device_map={"":
          0}<br>)<br>model = PeftModel.from_pretrained(model, adapters_name)<br>model
          = model.merge_and_unload()<br>tokenizer = LlamaTokenizer.from_pretrained(model_name)</p>

          '
        raw: "Why can't we used guanaco-65b-merged or esle we will have to merge by\
          \ ourselves as given below. How much GPU power is needed to run this model?\n\
          \nmodel_name = \"decapoda-research/llama-65b-hf\"\nadapters_name = 'timdettmers/guanaco-65b'\n\
          \n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n \
          \   #load_in_4bit=True,\n    torch_dtype=torch.bfloat16,\n    device_map={\"\
          \": 0}\n)\nmodel = PeftModel.from_pretrained(model, adapters_name)\nmodel\
          \ = model.merge_and_unload()\ntokenizer = LlamaTokenizer.from_pretrained(model_name)"
        updatedAt: '2023-06-15T21:06:06.001Z'
      numEdits: 1
      reactions: []
    id: 648b7cd508c4a9d807b617e7
    type: comment
  author: Drshafi
  content: "Why can't we used guanaco-65b-merged or esle we will have to merge by\
    \ ourselves as given below. How much GPU power is needed to run this model?\n\n\
    model_name = \"decapoda-research/llama-65b-hf\"\nadapters_name = 'timdettmers/guanaco-65b'\n\
    \n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    #load_in_4bit=True,\n\
    \    torch_dtype=torch.bfloat16,\n    device_map={\"\": 0}\n)\nmodel = PeftModel.from_pretrained(model,\
    \ adapters_name)\nmodel = model.merge_and_unload()\ntokenizer = LlamaTokenizer.from_pretrained(model_name)"
  created_at: 2023-06-15 20:04:21+00:00
  edited: true
  hidden: false
  id: 648b7cd508c4a9d807b617e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: timdettmers/guanaco-65b-merged
repo_type: model
status: open
target_branch: null
title: Tokenizer gives an error
