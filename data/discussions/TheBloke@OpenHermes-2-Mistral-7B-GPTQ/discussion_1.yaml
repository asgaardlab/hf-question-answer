!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HAvietisov
conflicting_files: null
created_at: 2023-10-17 09:28:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-10-17T10:28:32.000Z'
    data:
      edited: false
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4271126687526703
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: "<p>Tried to load model using the sample code : </p>\n<pre><code>from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n\
          model_name_or_path = \"TheBloke/OpenHermes-2-Mistral-7B-GPTQ\"\n# To use\
          \ a different branch, change revision\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n   \
          \                                          device_map=\"auto\",\n      \
          \                                       trust_remote_code=False,\n     \
          \                                        revision=\"main\")\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          system_message = \"You are helpful and kind assistant.\"\n\nprompt = \"\
          Tell me about AI\"\nprompt_template=f'''&lt;|im_start|&gt;system\n{system_message}&lt;|im_end|&gt;\n\
          &lt;|im_start|&gt;user\n{prompt}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\
          '''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n  \
          \  top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
          </code></pre>\n<p>Got following error instead : </p>\n<pre><code>SafetensorError\
          \                           Traceback (most recent call last)\nCell In[2],\
          \ line 6\n      3 model_name_or_path = \"TheBloke/OpenHermes-2-Mistral-7B-GPTQ\"\
          \n      4 # To use a different branch, change revision\n      5 # For example:\
          \ revision=\"gptq-4bit-32g-actorder_True\"\n----&gt; 6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \      7                                              device_map=\"auto\"\
          ,\n      8                                              trust_remote_code=False,\n\
          \      9                                              revision=\"main\"\
          )\n     11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n     13 system_message = \"You are helpful and kind assistant.\"\
          \n\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:565,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    563 elif type(config) in cls._model_mapping.keys():\n\
          \    564     model_class = _get_model_class(config, cls._model_mapping)\n\
          --&gt; 565     return model_class.from_pretrained(\n    566         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    567     )\n  \
          \  568 raise ValueError(\n    569     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    570     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    571 )\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3019,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n...\n--&gt;\
          \ 463     with safe_open(checkpoint_file, framework=\"pt\") as f:\n    464\
          \         metadata = f.metadata()\n    465     if metadata.get(\"format\"\
          ) not in [\"pt\", \"tf\", \"flax\"]:\n\nSafetensorError: Error while deserializing\
          \ header: MetadataIncompleteBuffer\n</code></pre>\n<p>What I did wrong?</p>\n"
        raw: "Tried to load model using the sample code : \r\n```\r\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, pipeline\r\n\r\nmodel_name_or_path\
          \ = \"TheBloke/OpenHermes-2-Mistral-7B-GPTQ\"\r\n# To use a different branch,\
          \ change revision\r\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\
          \r\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\n\
          \                                             device_map=\"auto\",\r\n \
          \                                            trust_remote_code=False,\r\n\
          \                                             revision=\"main\")\r\n\r\n\
          tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\
          \n\r\nsystem_message = \"You are helpful and kind assistant.\"\r\n\r\nprompt\
          \ = \"Tell me about AI\"\r\nprompt_template=f'''<|im_start|>system\r\n{system_message}<|im_end|>\r\
          \n<|im_start|>user\r\n{prompt}<|im_end|>\r\n<|im_start|>assistant\r\n'''\r\
          \n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\r\
          \nprint(tokenizer.decode(output[0]))\r\n\r\n# Inference can also be done\
          \ using transformers' pipeline\r\n\r\nprint(\"*** Pipeline:\")\r\npipe =\
          \ pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    max_new_tokens=512,\r\n    do_sample=True,\r\n    temperature=0.7,\r\
          \n    top_p=0.95,\r\n    top_k=40,\r\n    repetition_penalty=1.1\r\n)\r\n\
          \r\nprint(pipe(prompt_template)[0]['generated_text'])\r\n```\r\nGot following\
          \ error instead : \r\n```\r\nSafetensorError                           Traceback\
          \ (most recent call last)\r\nCell In[2], line 6\r\n      3 model_name_or_path\
          \ = \"TheBloke/OpenHermes-2-Mistral-7B-GPTQ\"\r\n      4 # To use a different\
          \ branch, change revision\r\n      5 # For example: revision=\"gptq-4bit-32g-actorder_True\"\
          \r\n----> 6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
          \n      7                                              device_map=\"auto\"\
          ,\r\n      8                                              trust_remote_code=False,\r\
          \n      9                                              revision=\"main\"\
          )\r\n     11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n     13 system_message = \"You are helpful and kind\
          \ assistant.\"\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:565,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n    563 elif type(config) in cls._model_mapping.keys():\r\
          \n    564     model_class = _get_model_class(config, cls._model_mapping)\r\
          \n--> 565     return model_class.from_pretrained(\r\n    566         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\r\n    567     )\r\n\
          \    568 raise ValueError(\r\n    569     f\"Unrecognized configuration\
          \ class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\\
          n\"\r\n    570     f\"Model type should be one of {', '.join(c.__name__\
          \ for c in cls._model_mapping.keys())}.\"\r\n    571 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3019,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\r\n...\r\n-->\
          \ 463     with safe_open(checkpoint_file, framework=\"pt\") as f:\r\n  \
          \  464         metadata = f.metadata()\r\n    465     if metadata.get(\"\
          format\") not in [\"pt\", \"tf\", \"flax\"]:\r\n\r\nSafetensorError: Error\
          \ while deserializing header: MetadataIncompleteBuffer\r\n```\r\nWhat I\
          \ did wrong?"
        updatedAt: '2023-10-17T10:28:32.493Z'
      numEdits: 0
      reactions: []
    id: 652e61d0d8fadff4ceb820d5
    type: comment
  author: HAvietisov
  content: "Tried to load model using the sample code : \r\n```\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer, pipeline\r\n\r\nmodel_name_or_path\
    \ = \"TheBloke/OpenHermes-2-Mistral-7B-GPTQ\"\r\n# To use a different branch,\
    \ change revision\r\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\n       \
    \                                      device_map=\"auto\",\r\n              \
    \                               trust_remote_code=False,\r\n                 \
    \                            revision=\"main\")\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nsystem_message = \"You are helpful and kind assistant.\"\
    \r\n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''<|im_start|>system\r\
    \n{system_message}<|im_end|>\r\n<|im_start|>user\r\n{prompt}<|im_end|>\r\n<|im_start|>assistant\r\
    \n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template,\
    \ return_tensors='pt').input_ids.cuda()\r\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\r\
    \nprint(tokenizer.decode(output[0]))\r\n\r\n# Inference can also be done using\
    \ transformers' pipeline\r\n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    max_new_tokens=512,\r\n    do_sample=True,\r\n    temperature=0.7,\r\n  \
    \  top_p=0.95,\r\n    top_k=40,\r\n    repetition_penalty=1.1\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\
    \n```\r\nGot following error instead : \r\n```\r\nSafetensorError            \
    \               Traceback (most recent call last)\r\nCell In[2], line 6\r\n  \
    \    3 model_name_or_path = \"TheBloke/OpenHermes-2-Mistral-7B-GPTQ\"\r\n    \
    \  4 # To use a different branch, change revision\r\n      5 # For example: revision=\"\
    gptq-4bit-32g-actorder_True\"\r\n----> 6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
    \n      7                                              device_map=\"auto\",\r\n\
    \      8                                              trust_remote_code=False,\r\
    \n      9                                              revision=\"main\")\r\n\
    \     11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\
    \n     13 system_message = \"You are helpful and kind assistant.\"\r\n\r\nFile\
    \ /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:565,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n    563 elif type(config) in cls._model_mapping.keys():\r\n   \
    \ 564     model_class = _get_model_class(config, cls._model_mapping)\r\n--> 565\
    \     return model_class.from_pretrained(\r\n    566         pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\r\n    567     )\r\n    568\
    \ raise ValueError(\r\n    569     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\r\n    570     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \r\n    571 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3019,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\r\n...\r\n--> 463     with\
    \ safe_open(checkpoint_file, framework=\"pt\") as f:\r\n    464         metadata\
    \ = f.metadata()\r\n    465     if metadata.get(\"format\") not in [\"pt\", \"\
    tf\", \"flax\"]:\r\n\r\nSafetensorError: Error while deserializing header: MetadataIncompleteBuffer\r\
    \n```\r\nWhat I did wrong?"
  created_at: 2023-10-17 09:28:32+00:00
  edited: false
  hidden: false
  id: 652e61d0d8fadff4ceb820d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-17T10:51:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.941042959690094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This error usually means the download failed - one or more of the
          files is incomplete and corrupt. Try downloading  again.</p>

          '
        raw: This error usually means the download failed - one or more of the files
          is incomplete and corrupt. Try downloading  again.
        updatedAt: '2023-10-17T10:51:51.830Z'
      numEdits: 0
      reactions: []
    id: 652e674789e73c5e69a089c5
    type: comment
  author: TheBloke
  content: This error usually means the download failed - one or more of the files
    is incomplete and corrupt. Try downloading  again.
  created_at: 2023-10-17 09:51:51+00:00
  edited: false
  hidden: false
  id: 652e674789e73c5e69a089c5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/OpenHermes-2-Mistral-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Error while deserializing header
