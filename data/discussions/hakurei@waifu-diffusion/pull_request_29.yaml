!!python/object:huggingface_hub.community.DiscussionWithDetails
author: skytnt
conflicting_files: []
created_at: 2022-10-19 01:49:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650375870480-noauth.png?w=200&h=200&f=face
      fullname: skytnt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skytnt
      type: user
    createdAt: '2022-10-19T02:49:39.000Z'
    data:
      edited: false
      editors:
      - skytnt
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650375870480-noauth.png?w=200&h=200&f=face
          fullname: skytnt
          isHf: false
          isPro: false
          name: skytnt
          type: user
        html: '<p>Now we can input prompt without 77 tokens limit and adjust weighting  by
          using custom_pipeline="waifu-research-department/long-prompt-weighting-pipeline".</p>

          <p>It requires diffusers&gt;=0.4.0</p>

          <p>Check out <a href="https://huggingface.co/waifu-research-department/long-prompt-weighting-pipeline">waifu-research-department/long-prompt-weighting-pipeline</a>
          for detial.</p>

          '
        raw: 'Now we can input prompt without 77 tokens limit and adjust weighting  by
          using custom_pipeline="waifu-research-department/long-prompt-weighting-pipeline".


          It requires diffusers>=0.4.0


          Check out [waifu-research-department/long-prompt-weighting-pipeline](https://huggingface.co/waifu-research-department/long-prompt-weighting-pipeline)
          for detial.'
        updatedAt: '2022-10-19T02:49:39.726Z'
      numEdits: 0
      reactions: []
    id: 634f65c307e669188d3f0fd5
    type: comment
  author: skytnt
  content: 'Now we can input prompt without 77 tokens limit and adjust weighting  by
    using custom_pipeline="waifu-research-department/long-prompt-weighting-pipeline".


    It requires diffusers>=0.4.0


    Check out [waifu-research-department/long-prompt-weighting-pipeline](https://huggingface.co/waifu-research-department/long-prompt-weighting-pipeline)
    for detial.'
  created_at: 2022-10-19 01:49:39+00:00
  edited: false
  hidden: false
  id: 634f65c307e669188d3f0fd5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650375870480-noauth.png?w=200&h=200&f=face
      fullname: skytnt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skytnt
      type: user
    createdAt: '2022-10-19T02:49:40.000Z'
    data:
      oid: 0a4486b8341f8ce3e92cdc02d32d9fafdb9f2ba1
      parents:
      - cc452ee4c493761a0c5d6e92394729dbdcd2cb28
      subject: support for longer prompt and weighting using custom_pipeline
    id: 634f65c40000000000000000
    type: commit
  author: skytnt
  created_at: 2022-10-19 01:49:40+00:00
  id: 634f65c40000000000000000
  oid: 0a4486b8341f8ce3e92cdc02d32d9fafdb9f2ba1
  summary: support for longer prompt and weighting using custom_pipeline
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 29
repo_id: hakurei/waifu-diffusion
repo_type: model
status: open
target_branch: refs/heads/main
title: support for longer prompt and weighting using custom_pipeline
