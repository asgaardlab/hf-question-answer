!!python/object:huggingface_hub.community.DiscussionWithDetails
author: goodromka
conflicting_files: null
created_at: 2023-11-13 20:12:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-11-13T20:12:10.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447598457336426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>Thank you for providing the gguf version of the model. I followed
          your suggestion and utilized make-ggml.py as instructed earlier. However,
          I encountered an error: FileNotFoundError - the tokenizer.model file could
          not be found. It seems that this file is not present in their directory.
          Can you confirm if you used the same make-ggml.py script as previously recommended?
          If affirmative, could you please specify where you obtained the tokenizer.model
          file?</p>

          '
        raw: 'Thank you for providing the gguf version of the model. I followed your
          suggestion and utilized make-ggml.py as instructed earlier. However, I encountered
          an error: FileNotFoundError - the tokenizer.model file could not be found.
          It seems that this file is not present in their directory. Can you confirm
          if you used the same make-ggml.py script as previously recommended? If affirmative,
          could you please specify where you obtained the tokenizer.model file?'
        updatedAt: '2023-11-13T20:12:10.784Z'
      numEdits: 0
      reactions: []
    id: 6552831a27b74d7f6d99cf5d
    type: comment
  author: goodromka
  content: 'Thank you for providing the gguf version of the model. I followed your
    suggestion and utilized make-ggml.py as instructed earlier. However, I encountered
    an error: FileNotFoundError - the tokenizer.model file could not be found. It
    seems that this file is not present in their directory. Can you confirm if you
    used the same make-ggml.py script as previously recommended? If affirmative, could
    you please specify where you obtained the tokenizer.model file?'
  created_at: 2023-11-13 20:12:10+00:00
  edited: false
  hidden: false
  id: 6552831a27b74d7f6d99cf5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-13T21:48:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8940330147743225
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This model uses a different tokenizer format, without tokenizer.model.  Pass
          <code>--vocabtype bpe</code> to <code>convert.py</code> and it should work
          to make the FP16, from which you can make quantisations as usual.</p>

          <p>Or just use my GGUFs?  Or are you trying to make GGUF of an AquilaChat
          fine tune?</p>

          '
        raw: 'This model uses a different tokenizer format, without tokenizer.model.  Pass
          `--vocabtype bpe` to `convert.py` and it should work to make the FP16, from
          which you can make quantisations as usual.


          Or just use my GGUFs?  Or are you trying to make GGUF of an AquilaChat fine
          tune?'
        updatedAt: '2023-11-13T21:48:38.414Z'
      numEdits: 0
      reactions: []
    id: 655299b6764637dae0804aa3
    type: comment
  author: TheBloke
  content: 'This model uses a different tokenizer format, without tokenizer.model.  Pass
    `--vocabtype bpe` to `convert.py` and it should work to make the FP16, from which
    you can make quantisations as usual.


    Or just use my GGUFs?  Or are you trying to make GGUF of an AquilaChat fine tune?'
  created_at: 2023-11-13 21:48:38+00:00
  edited: false
  hidden: false
  id: 655299b6764637dae0804aa3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-11-15T09:45:26.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9274327158927917
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>I would like to express my appreciation for your advice. I successfully
          quantized my fine-tuned AquilaChat model using --vocabtype bpe.</p>

          '
        raw: I would like to express my appreciation for your advice. I successfully
          quantized my fine-tuned AquilaChat model using --vocabtype bpe.
        updatedAt: '2023-11-15T09:45:26.914Z'
      numEdits: 0
      reactions: []
    id: 6554933627ba688d1b95347e
    type: comment
  author: goodromka
  content: I would like to express my appreciation for your advice. I successfully
    quantized my fine-tuned AquilaChat model using --vocabtype bpe.
  created_at: 2023-11-15 09:45:26+00:00
  edited: false
  hidden: false
  id: 6554933627ba688d1b95347e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-11-15T09:45:29.000Z'
    data:
      status: closed
    id: 6554933937b3d85662a9e16a
    type: status-change
  author: goodromka
  created_at: 2023-11-15 09:45:29+00:00
  id: 6554933937b3d85662a9e16a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/AquilaChat2-34B-AWQ
repo_type: model
status: closed
target_branch: null
title: FileNotFoundError - the tokenizer.model file could not be found
