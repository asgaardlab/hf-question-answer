!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sreag
conflicting_files: null
created_at: 2023-03-04 12:51:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2318d3791eca366773bca16c662fe53b.svg
      fullname: m
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sreag
      type: user
    createdAt: '2023-03-04T12:51:22.000Z'
    data:
      edited: false
      editors:
      - sreag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2318d3791eca366773bca16c662fe53b.svg
          fullname: m
          isHf: false
          isPro: false
          name: sreag
          type: user
        html: '<p>What is the expected processing time in Google Colab? I am trying
          this in Colab and it shows the expected time to be half an hour. Bit confused.</p>

          '
        raw: What is the expected processing time in Google Colab? I am trying this
          in Colab and it shows the expected time to be half an hour. Bit confused.
        updatedAt: '2023-03-04T12:51:22.825Z'
      numEdits: 0
      reactions: []
    id: 64033eca56038547951e94d4
    type: comment
  author: sreag
  content: What is the expected processing time in Google Colab? I am trying this
    in Colab and it shows the expected time to be half an hour. Bit confused.
  created_at: 2023-03-04 12:51:22+00:00
  edited: false
  hidden: false
  id: 64033eca56038547951e94d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641968925128-noauth.jpeg?w=200&h=200&f=face
      fullname: lu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cq
      type: user
    createdAt: '2023-03-05T13:05:23.000Z'
    data:
      edited: false
      editors:
      - cq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641968925128-noauth.jpeg?w=200&h=200&f=face
          fullname: lu
          isHf: false
          isPro: false
          name: cq
          type: user
        html: '<p>The expected time depends on the device you use in Colab. If you
          run it on A100 GPU, it takes just a few seconds. However, 30 minutes is
          still too long even for CPU inference. Could you provide more info (GPU/CPU
          type, timesteps, batch size) about it?</p>

          '
        raw: The expected time depends on the device you use in Colab. If you run
          it on A100 GPU, it takes just a few seconds. However, 30 minutes is still
          too long even for CPU inference. Could you provide more info (GPU/CPU type,
          timesteps, batch size) about it?
        updatedAt: '2023-03-05T13:05:23.895Z'
      numEdits: 0
      reactions: []
    id: 640493939abef9e3939b561a
    type: comment
  author: cq
  content: The expected time depends on the device you use in Colab. If you run it
    on A100 GPU, it takes just a few seconds. However, 30 minutes is still too long
    even for CPU inference. Could you provide more info (GPU/CPU type, timesteps,
    batch size) about it?
  created_at: 2023-03-05 13:05:23+00:00
  edited: false
  hidden: false
  id: 640493939abef9e3939b561a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fc6879e1c5ee87b1164876d/gyT4t-tvya6yXbjwnLm0l.png?w=200&h=200&f=face
      fullname: Ontocord.AI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ontocord
      type: user
    createdAt: '2023-09-02T04:13:38.000Z'
    data:
      edited: false
      editors:
      - ontocord
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6017512679100037
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fc6879e1c5ee87b1164876d/gyT4t-tvya6yXbjwnLm0l.png?w=200&h=200&f=face
          fullname: Ontocord.AI
          isHf: false
          isPro: false
          name: ontocord
          type: user
        html: "<p>running on cpu on colab for me required 6mins to generate the apple</p>\n\
          <p>Downloading (\u2026)ain/model_index.json: 100%<br>560/560 [00:00&lt;00:00,\
          \ 33.4kB/s]<br>safety_checker/model.safetensors not found<br>Fetching 15\
          \ files: 100%<br>15/15 [02:00&lt;00:00, 8.69s/it]<br>Downloading (\u2026\
          )rocessor_config.json: 100%<br>518/518 [00:00&lt;00:00, 17.9kB/s]<br>Downloading\
          \ (\u2026)_checker/config.json: 100%<br>4.79k/4.79k [00:00&lt;00:00, 159kB/s]<br>Downloading\
          \ (\u2026)cial_tokens_map.json: 100%<br>472/472 [00:00&lt;00:00, 27.0kB/s]<br>Downloading\
          \ (\u2026)cheduler_config.json: 100%<br>459/459 [00:00&lt;00:00, 10.4kB/s]<br>Downloading\
          \ (\u2026)tokenizer/vocab.json: 100%<br>1.06M/1.06M [00:00&lt;00:00, 1.36MB/s]<br>Downloading\
          \ (\u2026)okenizer_config.json: 100%<br>836/836 [00:00&lt;00:00, 12.3kB/s]<br>Downloading\
          \ (\u2026)_encoder/config.json: 100%<br>560/560 [00:00&lt;00:00, 9.16kB/s]<br>Downloading\
          \ (\u2026)tokenizer/merges.txt: 100%<br>525k/525k [00:00&lt;00:00, 894kB/s]<br>Downloading\
          \ (\u2026)140/unet/config.json: 100%<br>748/748 [00:00&lt;00:00, 54.7kB/s]<br>Downloading\
          \ (\u2026)8140/vae/config.json: 100%<br>581/581 [00:00&lt;00:00, 29.4kB/s]<br>Downloading\
          \ pytorch_model.bin: 100%<br>1.22G/1.22G [01:02&lt;00:00, 20.7MB/s]<br>Downloading\
          \ pytorch_model.bin: 100%<br>492M/492M [00:25&lt;00:00, 21.4MB/s]<br>Downloading\
          \ (\u2026)on_pytorch_model.bin: 100%<br>335M/335M [00:17&lt;00:00, 20.2MB/s]<br>Downloading\
          \ (\u2026)on_pytorch_model.bin: 100%<br>2.32G/2.32G [01:56&lt;00:00, 21.0MB/s]<br>Loading\
          \ pipeline components...: 100%<br>7/7 [00:05&lt;00:00, 1.50it/s]<br><code>text_config_dict</code>\
          \ is provided which will be used to initialize <code>CLIPTextConfig</code>.\
          \ The value <code>text_config[\"id2label\"]</code> will be overriden.<br><code>text_config_dict</code>\
          \ is provided which will be used to initialize <code>CLIPTextConfig</code>.\
          \ The value <code>text_config[\"bos_token_id\"]</code> will be overriden.<br><code>text_config_dict</code>\
          \ is provided which will be used to initialize <code>CLIPTextConfig</code>.\
          \ The value <code>text_config[\"eos_token_id\"]</code> will be overriden.<br>The\
          \ config attributes {'predict_epsilon': True} were passed to DPMSolverMultistepScheduler,\
          \ but are not expected and will be ignored. Please verify your scheduler_config.json\
          \ configuration file.<br>/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:128:\
          \ FutureWarning: The configuration file of this scheduler: DPMSolverMultistepScheduler\
          \ {<br>  \"_class_name\": \"DPMSolverMultistepScheduler\",<br>  \"_diffusers_version\"\
          : \"0.20.2\",<br>  \"algorithm_type\": \"dpmsolver++\",<br>  \"beta_end\"\
          : 0.012,<br>  \"beta_schedule\": \"scaled_linear\",<br>  \"beta_start\"\
          : 0.00085,<br>  \"dynamic_thresholding_ratio\": 0.995,<br>  \"lambda_min_clipped\"\
          : -Infinity,<br>  \"lower_order_final\": true,<br>  \"num_train_timesteps\"\
          : 1000,<br>  \"predict_epsilon\": true,<br>  \"prediction_type\": \"epsilon\"\
          ,<br>  \"sample_max_value\": 1.0,<br>  \"solver_order\": 2,<br>  \"solver_type\"\
          : \"midpoint\",<br>  \"steps_offset\": 0,<br>  \"thresholding\": false,<br>\
          \  \"timestep_spacing\": \"linspace\",<br>  \"trained_betas\": null,<br>\
          \  \"use_karras_sigmas\": false,<br>  \"variance_type\": null<br>}<br> is\
          \ outdated. <code>steps_offset</code> should be set to 1 instead of 0. Please\
          \ make sure to update the config accordingly as leaving <code>steps_offset</code>\
          \ might led to incorrect results in future versions. If you have downloaded\
          \ this checkpoint from the Hugging Face Hub, it would be very nice if you\
          \ could open a Pull request for the <code>scheduler/scheduler_config.json</code>\
          \ file<br>  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message,\
          \ standard_warn=False)<br>100%<br>50/50 [06:46&lt;00:00, 7.46s/it]</p>\n"
        raw: "running on cpu on colab for me required 6mins to generate the apple\n\
          \nDownloading (\u2026)ain/model_index.json: 100%\n560/560 [00:00<00:00,\
          \ 33.4kB/s]\nsafety_checker/model.safetensors not found\nFetching 15 files:\
          \ 100%\n15/15 [02:00<00:00, 8.69s/it]\nDownloading (\u2026)rocessor_config.json:\
          \ 100%\n518/518 [00:00<00:00, 17.9kB/s]\nDownloading (\u2026)_checker/config.json:\
          \ 100%\n4.79k/4.79k [00:00<00:00, 159kB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%\n472/472 [00:00<00:00, 27.0kB/s]\nDownloading (\u2026)cheduler_config.json:\
          \ 100%\n459/459 [00:00<00:00, 10.4kB/s]\nDownloading (\u2026)tokenizer/vocab.json:\
          \ 100%\n1.06M/1.06M [00:00<00:00, 1.36MB/s]\nDownloading (\u2026)okenizer_config.json:\
          \ 100%\n836/836 [00:00<00:00, 12.3kB/s]\nDownloading (\u2026)_encoder/config.json:\
          \ 100%\n560/560 [00:00<00:00, 9.16kB/s]\nDownloading (\u2026)tokenizer/merges.txt:\
          \ 100%\n525k/525k [00:00<00:00, 894kB/s]\nDownloading (\u2026)140/unet/config.json:\
          \ 100%\n748/748 [00:00<00:00, 54.7kB/s]\nDownloading (\u2026)8140/vae/config.json:\
          \ 100%\n581/581 [00:00<00:00, 29.4kB/s]\nDownloading pytorch_model.bin:\
          \ 100%\n1.22G/1.22G [01:02<00:00, 20.7MB/s]\nDownloading pytorch_model.bin:\
          \ 100%\n492M/492M [00:25<00:00, 21.4MB/s]\nDownloading (\u2026)on_pytorch_model.bin:\
          \ 100%\n335M/335M [00:17<00:00, 20.2MB/s]\nDownloading (\u2026)on_pytorch_model.bin:\
          \ 100%\n2.32G/2.32G [01:56<00:00, 21.0MB/s]\nLoading pipeline components...:\
          \ 100%\n7/7 [00:05<00:00, 1.50it/s]\n`text_config_dict` is provided which\
          \ will be used to initialize `CLIPTextConfig`. The value `text_config[\"\
          id2label\"]` will be overriden.\n`text_config_dict` is provided which will\
          \ be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"\
          ]` will be overriden.\n`text_config_dict` is provided which will be used\
          \ to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"\
          ]` will be overriden.\nThe config attributes {'predict_epsilon': True} were\
          \ passed to DPMSolverMultistepScheduler, but are not expected and will be\
          \ ignored. Please verify your scheduler_config.json configuration file.\n\
          /usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:128:\
          \ FutureWarning: The configuration file of this scheduler: DPMSolverMultistepScheduler\
          \ {\n  \"_class_name\": \"DPMSolverMultistepScheduler\",\n  \"_diffusers_version\"\
          : \"0.20.2\",\n  \"algorithm_type\": \"dpmsolver++\",\n  \"beta_end\": 0.012,\n\
          \  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n \
          \ \"dynamic_thresholding_ratio\": 0.995,\n  \"lambda_min_clipped\": -Infinity,\n\
          \  \"lower_order_final\": true,\n  \"num_train_timesteps\": 1000,\n  \"\
          predict_epsilon\": true,\n  \"prediction_type\": \"epsilon\",\n  \"sample_max_value\"\
          : 1.0,\n  \"solver_order\": 2,\n  \"solver_type\": \"midpoint\",\n  \"steps_offset\"\
          : 0,\n  \"thresholding\": false,\n  \"timestep_spacing\": \"linspace\",\n\
          \  \"trained_betas\": null,\n  \"use_karras_sigmas\": false,\n  \"variance_type\"\
          : null\n}\n is outdated. `steps_offset` should be set to 1 instead of 0.\
          \ Please make sure to update the config accordingly as leaving `steps_offset`\
          \ might led to incorrect results in future versions. If you have downloaded\
          \ this checkpoint from the Hugging Face Hub, it would be very nice if you\
          \ could open a Pull request for the `scheduler/scheduler_config.json` file\n\
          \  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n\
          100%\n50/50 [06:46<00:00, 7.46s/it]"
        updatedAt: '2023-09-02T04:13:38.749Z'
      numEdits: 0
      reactions: []
    id: 64f2b672b0da53a80e4ed231
    type: comment
  author: ontocord
  content: "running on cpu on colab for me required 6mins to generate the apple\n\n\
    Downloading (\u2026)ain/model_index.json: 100%\n560/560 [00:00<00:00, 33.4kB/s]\n\
    safety_checker/model.safetensors not found\nFetching 15 files: 100%\n15/15 [02:00<00:00,\
    \ 8.69s/it]\nDownloading (\u2026)rocessor_config.json: 100%\n518/518 [00:00<00:00,\
    \ 17.9kB/s]\nDownloading (\u2026)_checker/config.json: 100%\n4.79k/4.79k [00:00<00:00,\
    \ 159kB/s]\nDownloading (\u2026)cial_tokens_map.json: 100%\n472/472 [00:00<00:00,\
    \ 27.0kB/s]\nDownloading (\u2026)cheduler_config.json: 100%\n459/459 [00:00<00:00,\
    \ 10.4kB/s]\nDownloading (\u2026)tokenizer/vocab.json: 100%\n1.06M/1.06M [00:00<00:00,\
    \ 1.36MB/s]\nDownloading (\u2026)okenizer_config.json: 100%\n836/836 [00:00<00:00,\
    \ 12.3kB/s]\nDownloading (\u2026)_encoder/config.json: 100%\n560/560 [00:00<00:00,\
    \ 9.16kB/s]\nDownloading (\u2026)tokenizer/merges.txt: 100%\n525k/525k [00:00<00:00,\
    \ 894kB/s]\nDownloading (\u2026)140/unet/config.json: 100%\n748/748 [00:00<00:00,\
    \ 54.7kB/s]\nDownloading (\u2026)8140/vae/config.json: 100%\n581/581 [00:00<00:00,\
    \ 29.4kB/s]\nDownloading pytorch_model.bin: 100%\n1.22G/1.22G [01:02<00:00, 20.7MB/s]\n\
    Downloading pytorch_model.bin: 100%\n492M/492M [00:25<00:00, 21.4MB/s]\nDownloading\
    \ (\u2026)on_pytorch_model.bin: 100%\n335M/335M [00:17<00:00, 20.2MB/s]\nDownloading\
    \ (\u2026)on_pytorch_model.bin: 100%\n2.32G/2.32G [01:56<00:00, 21.0MB/s]\nLoading\
    \ pipeline components...: 100%\n7/7 [00:05<00:00, 1.50it/s]\n`text_config_dict`\
    \ is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"\
    id2label\"]` will be overriden.\n`text_config_dict` is provided which will be\
    \ used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"\
    ]` will be overriden.\n`text_config_dict` is provided which will be used to initialize\
    \ `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n\
    The config attributes {'predict_epsilon': True} were passed to DPMSolverMultistepScheduler,\
    \ but are not expected and will be ignored. Please verify your scheduler_config.json\
    \ configuration file.\n/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:128:\
    \ FutureWarning: The configuration file of this scheduler: DPMSolverMultistepScheduler\
    \ {\n  \"_class_name\": \"DPMSolverMultistepScheduler\",\n  \"_diffusers_version\"\
    : \"0.20.2\",\n  \"algorithm_type\": \"dpmsolver++\",\n  \"beta_end\": 0.012,\n\
    \  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"dynamic_thresholding_ratio\"\
    : 0.995,\n  \"lambda_min_clipped\": -Infinity,\n  \"lower_order_final\": true,\n\
    \  \"num_train_timesteps\": 1000,\n  \"predict_epsilon\": true,\n  \"prediction_type\"\
    : \"epsilon\",\n  \"sample_max_value\": 1.0,\n  \"solver_order\": 2,\n  \"solver_type\"\
    : \"midpoint\",\n  \"steps_offset\": 0,\n  \"thresholding\": false,\n  \"timestep_spacing\"\
    : \"linspace\",\n  \"trained_betas\": null,\n  \"use_karras_sigmas\": false,\n\
    \  \"variance_type\": null\n}\n is outdated. `steps_offset` should be set to 1\
    \ instead of 0. Please make sure to update the config accordingly as leaving `steps_offset`\
    \ might led to incorrect results in future versions. If you have downloaded this\
    \ checkpoint from the Hugging Face Hub, it would be very nice if you could open\
    \ a Pull request for the `scheduler/scheduler_config.json` file\n  deprecate(\"\
    steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n100%\n\
    50/50 [06:46<00:00, 7.46s/it]"
  created_at: 2023-09-02 03:13:38+00:00
  edited: false
  hidden: false
  id: 64f2b672b0da53a80e4ed231
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: OFA-Sys/small-stable-diffusion-v0
repo_type: model
status: open
target_branch: null
title: Expected speed on Colab
