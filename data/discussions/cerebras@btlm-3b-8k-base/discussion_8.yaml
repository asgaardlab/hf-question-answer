!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ZQ-Dev
conflicting_files: null
created_at: 2023-07-24 19:54:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6169a814cc21d3c0aa086ad4/bVzEucmlFk4olghdyQ-Y_.png?w=200&h=200&f=face
      fullname: ZQ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZQ-Dev
      type: user
    createdAt: '2023-07-24T20:54:28.000Z'
    data:
      edited: false
      editors:
      - ZQ-Dev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9174070358276367
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6169a814cc21d3c0aa086ad4/bVzEucmlFk4olghdyQ-Y_.png?w=200&h=200&f=face
          fullname: ZQ
          isHf: false
          isPro: false
          name: ZQ-Dev
          type: user
        html: '<p>First off, thanks for the amazing contribution! </p>

          <p>Based on internal knowledge of the model and its training process, do
          you have any recommendations for users seeking to perform further domain-specific
          pretraining? For example, as seen here for the medical domain?</p>

          <p><a rel="nofollow" href="https://arxiv.org/abs/2304.14454">https://arxiv.org/abs/2304.14454</a></p>

          '
        raw: "First off, thanks for the amazing contribution! \r\n\r\nBased on internal\
          \ knowledge of the model and its training process, do you have any recommendations\
          \ for users seeking to perform further domain-specific pretraining? For\
          \ example, as seen here for the medical domain?\r\n\r\nhttps://arxiv.org/abs/2304.14454"
        updatedAt: '2023-07-24T20:54:28.471Z'
      numEdits: 0
      reactions: []
    id: 64bee5041d40292dd318195a
    type: comment
  author: ZQ-Dev
  content: "First off, thanks for the amazing contribution! \r\n\r\nBased on internal\
    \ knowledge of the model and its training process, do you have any recommendations\
    \ for users seeking to perform further domain-specific pretraining? For example,\
    \ as seen here for the medical domain?\r\n\r\nhttps://arxiv.org/abs/2304.14454"
  created_at: 2023-07-24 19:54:28+00:00
  edited: false
  hidden: false
  id: 64bee5041d40292dd318195a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
      fullname: Daria Soboleva
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daria-soboleva
      type: user
    createdAt: '2023-07-25T16:21:39.000Z'
    data:
      edited: false
      editors:
      - daria-soboleva
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.95767742395401
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
          fullname: Daria Soboleva
          isHf: false
          isPro: false
          name: daria-soboleva
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;ZQ-Dev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ZQ-Dev\">@<span class=\"\
          underline\">ZQ-Dev</span></a></span>\n\n\t</span></span> thanks for reaching\
          \ out. We are def interested in learning more what ppl plan to build on\
          \ top. Are you interested in the medical domain cont. pre-training? </p>\n"
        raw: 'Hey @ZQ-Dev thanks for reaching out. We are def interested in learning
          more what ppl plan to build on top. Are you interested in the medical domain
          cont. pre-training? '
        updatedAt: '2023-07-25T16:21:39.541Z'
      numEdits: 0
      reactions: []
    id: 64bff69360df53fd4e2ef2bb
    type: comment
  author: daria-soboleva
  content: 'Hey @ZQ-Dev thanks for reaching out. We are def interested in learning
    more what ppl plan to build on top. Are you interested in the medical domain cont.
    pre-training? '
  created_at: 2023-07-25 15:21:39+00:00
  edited: false
  hidden: false
  id: 64bff69360df53fd4e2ef2bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6169a814cc21d3c0aa086ad4/bVzEucmlFk4olghdyQ-Y_.png?w=200&h=200&f=face
      fullname: ZQ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZQ-Dev
      type: user
    createdAt: '2023-07-25T16:32:22.000Z'
    data:
      edited: false
      editors:
      - ZQ-Dev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8712936639785767
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6169a814cc21d3c0aa086ad4/bVzEucmlFk4olghdyQ-Y_.png?w=200&h=200&f=face
          fullname: ZQ
          isHf: false
          isPro: false
          name: ZQ-Dev
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;daria-soboleva&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daria-soboleva\"\
          >@<span class=\"underline\">daria-soboleva</span></a></span>\n\n\t</span></span>,\
          \ thanks for the quick response! I\u2019m targeting a different domain (not\
          \ medical, feel free to DM for more context), but it\u2019s essentially\
          \  the same problem as the medical domain, i.e. heavy use of domain-specific\
          \ jargon, acronyms, vocab etc. that I would like to incorporate into a model\u2019\
          s pretraining before instruction tuning for downstream tasks.</p>\n"
        raw: "Hi @daria-soboleva, thanks for the quick response! I\u2019m targeting\
          \ a different domain (not medical, feel free to DM for more context), but\
          \ it\u2019s essentially  the same problem as the medical domain, i.e. heavy\
          \ use of domain-specific jargon, acronyms, vocab etc. that I would like\
          \ to incorporate into a model\u2019s pretraining before instruction tuning\
          \ for downstream tasks."
        updatedAt: '2023-07-25T16:32:22.510Z'
      numEdits: 0
      reactions: []
    id: 64bff916b693f2bebd4fe540
    type: comment
  author: ZQ-Dev
  content: "Hi @daria-soboleva, thanks for the quick response! I\u2019m targeting\
    \ a different domain (not medical, feel free to DM for more context), but it\u2019\
    s essentially  the same problem as the medical domain, i.e. heavy use of domain-specific\
    \ jargon, acronyms, vocab etc. that I would like to incorporate into a model\u2019\
    s pretraining before instruction tuning for downstream tasks."
  created_at: 2023-07-25 15:32:22+00:00
  edited: false
  hidden: false
  id: 64bff916b693f2bebd4fe540
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
      fullname: Daria Soboleva
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daria-soboleva
      type: user
    createdAt: '2023-07-26T17:32:59.000Z'
    data:
      edited: false
      editors:
      - daria-soboleva
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9589915871620178
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
          fullname: Daria Soboleva
          isHf: false
          isPro: false
          name: daria-soboleva
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ZQ-Dev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ZQ-Dev\">@<span class=\"\
          underline\">ZQ-Dev</span></a></span>\n\n\t</span></span> gotcha, yeah that\
          \ makes sense. I would recommend splitting your cont. pre-training dataset\
          \ into the train and holdout to make sure that you can assess the quality\
          \ without instruction based fine-tuning. Make sure that you don't have any\
          \ overlap between the training set and your new holdout set from domain\
          \ specific data. Feel free to use our scripts for document-level decontamination:\
          \ <a rel=\"nofollow\" href=\"https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama\"\
          >https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama</a>.\
          \ Ideally you don't want to repeat a lot of examples from SlimPJ and your\
          \ in-domain train dataset since this would not be optimal, but I think there\
          \ is more hustle than an outcome, so I would not be worried about the duplicates\
          \ there. In terms of the extending vocab, I believe this is still a not\
          \ very-well researched part, but my recommendation would be to check how\
          \ many tokens you have to add and if it is a small amount, I would just\
          \ extend the vocab. Another tip would be to check the fertility score on\
          \ your tokenized dataset in case you need to re-train tokenizer from scratch\
          \ for your vocab. In that case you might want to make sure that old tokens\
          \ and their ids are matching with the new vocab. Hope that helps. </p>\n"
        raw: '@ZQ-Dev gotcha, yeah that makes sense. I would recommend splitting your
          cont. pre-training dataset into the train and holdout to make sure that
          you can assess the quality without instruction based fine-tuning. Make sure
          that you don''t have any overlap between the training set and your new holdout
          set from domain specific data. Feel free to use our scripts for document-level
          decontamination: https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama.
          Ideally you don''t want to repeat a lot of examples from SlimPJ and your
          in-domain train dataset since this would not be optimal, but I think there
          is more hustle than an outcome, so I would not be worried about the duplicates
          there. In terms of the extending vocab, I believe this is still a not very-well
          researched part, but my recommendation would be to check how many tokens
          you have to add and if it is a small amount, I would just extend the vocab.
          Another tip would be to check the fertility score on your tokenized dataset
          in case you need to re-train tokenizer from scratch for your vocab. In that
          case you might want to make sure that old tokens and their ids are matching
          with the new vocab. Hope that helps. '
        updatedAt: '2023-07-26T17:32:59.640Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ZQ-Dev
        - Joseph717171
    id: 64c158cb08f5bfefcef1613c
    type: comment
  author: daria-soboleva
  content: '@ZQ-Dev gotcha, yeah that makes sense. I would recommend splitting your
    cont. pre-training dataset into the train and holdout to make sure that you can
    assess the quality without instruction based fine-tuning. Make sure that you don''t
    have any overlap between the training set and your new holdout set from domain
    specific data. Feel free to use our scripts for document-level decontamination:
    https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama.
    Ideally you don''t want to repeat a lot of examples from SlimPJ and your in-domain
    train dataset since this would not be optimal, but I think there is more hustle
    than an outcome, so I would not be worried about the duplicates there. In terms
    of the extending vocab, I believe this is still a not very-well researched part,
    but my recommendation would be to check how many tokens you have to add and if
    it is a small amount, I would just extend the vocab. Another tip would be to check
    the fertility score on your tokenized dataset in case you need to re-train tokenizer
    from scratch for your vocab. In that case you might want to make sure that old
    tokens and their ids are matching with the new vocab. Hope that helps. '
  created_at: 2023-07-26 16:32:59+00:00
  edited: false
  hidden: false
  id: 64c158cb08f5bfefcef1613c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6169a814cc21d3c0aa086ad4/bVzEucmlFk4olghdyQ-Y_.png?w=200&h=200&f=face
      fullname: ZQ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZQ-Dev
      type: user
    createdAt: '2023-07-26T17:41:07.000Z'
    data:
      edited: false
      editors:
      - ZQ-Dev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6034637093544006
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6169a814cc21d3c0aa086ad4/bVzEucmlFk4olghdyQ-Y_.png?w=200&h=200&f=face
          fullname: ZQ
          isHf: false
          isPro: false
          name: ZQ-Dev
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;daria-soboleva&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daria-soboleva\"\
          >@<span class=\"underline\">daria-soboleva</span></a></span>\n\n\t</span></span>\
          \ super helpful, thank you!</p>\n"
        raw: '@daria-soboleva super helpful, thank you!'
        updatedAt: '2023-07-26T17:41:07.197Z'
      numEdits: 0
      reactions: []
    id: 64c15ab3c091ad685afeb4f5
    type: comment
  author: ZQ-Dev
  content: '@daria-soboleva super helpful, thank you!'
  created_at: 2023-07-26 16:41:07+00:00
  edited: false
  hidden: false
  id: 64c15ab3c091ad685afeb4f5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: cerebras/btlm-3b-8k-base
repo_type: model
status: open
target_branch: null
title: Recommendations for additional pretraining?
