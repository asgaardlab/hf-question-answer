!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CUIGuy
conflicting_files: null
created_at: 2023-07-25 15:18:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
      fullname: Xiaoyun Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CUIGuy
      type: user
    createdAt: '2023-07-25T16:18:45.000Z'
    data:
      edited: false
      editors:
      - CUIGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8158418536186218
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
          fullname: Xiaoyun Wu
          isHf: false
          isPro: false
          name: CUIGuy
          type: user
        html: '<p>Loading cerebras/btlm-3b-8k-base requires to execute some code in
          that repo, you can inspect the content of the repository at <a rel="nofollow"
          href="https://hf.co/cerebras/btlm-3b-8k-base">https://hf.co/cerebras/btlm-3b-8k-base</a>.
          You can dismiss this prompt by passing <code>trust_remote_code=True</code>.</p>

          <p>It will be nice if people can just run it without checking these details.
          </p>

          '
        raw: "Loading cerebras/btlm-3b-8k-base requires to execute some code in that\
          \ repo, you can inspect the content of the repository at https://hf.co/cerebras/btlm-3b-8k-base.\
          \ You can dismiss this prompt by passing `trust_remote_code=True`.\r\n\r\
          \nIt will be nice if people can just run it without checking these details. "
        updatedAt: '2023-07-25T16:18:45.435Z'
      numEdits: 0
      reactions: []
    id: 64bff5e5661694889faeb980
    type: comment
  author: CUIGuy
  content: "Loading cerebras/btlm-3b-8k-base requires to execute some code in that\
    \ repo, you can inspect the content of the repository at https://hf.co/cerebras/btlm-3b-8k-base.\
    \ You can dismiss this prompt by passing `trust_remote_code=True`.\r\n\r\nIt will\
    \ be nice if people can just run it without checking these details. "
  created_at: 2023-07-25 15:18:45+00:00
  edited: false
  hidden: false
  id: 64bff5e5661694889faeb980
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
      fullname: Daria Soboleva
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daria-soboleva
      type: user
    createdAt: '2023-07-25T16:26:52.000Z'
    data:
      edited: false
      editors:
      - daria-soboleva
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9531995058059692
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
          fullname: Daria Soboleva
          isHf: false
          isPro: false
          name: daria-soboleva
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CUIGuy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CUIGuy\">@<span class=\"\
          underline\">CUIGuy</span></a></span>\n\n\t</span></span> I agree :) One\
          \ of the constraints that we had is that HuggingFace does not support MuP\
          \ implementation that we shared with our BTLM model in a custom class implementation.\
          \ We believe it can greatly benefit your fine-tuning regime. Once MuP is\
          \ fully adopted by the HuggingFace, it should just work without adding additional\
          \ flags. We are in close communication with HuggingFace, so we believe it\
          \ should happen soon. </p>\n"
        raw: '@CUIGuy I agree :) One of the constraints that we had is that HuggingFace
          does not support MuP implementation that we shared with our BTLM model in
          a custom class implementation. We believe it can greatly benefit your fine-tuning
          regime. Once MuP is fully adopted by the HuggingFace, it should just work
          without adding additional flags. We are in close communication with HuggingFace,
          so we believe it should happen soon. '
        updatedAt: '2023-07-25T16:26:52.019Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jingwora
    id: 64bff7cc1a984181d889e7db
    type: comment
  author: daria-soboleva
  content: '@CUIGuy I agree :) One of the constraints that we had is that HuggingFace
    does not support MuP implementation that we shared with our BTLM model in a custom
    class implementation. We believe it can greatly benefit your fine-tuning regime.
    Once MuP is fully adopted by the HuggingFace, it should just work without adding
    additional flags. We are in close communication with HuggingFace, so we believe
    it should happen soon. '
  created_at: 2023-07-25 15:26:52+00:00
  edited: false
  hidden: false
  id: 64bff7cc1a984181d889e7db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
      fullname: Xiaoyun Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CUIGuy
      type: user
    createdAt: '2023-07-25T16:29:40.000Z'
    data:
      edited: true
      editors:
      - CUIGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.707025945186615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
          fullname: Xiaoyun Wu
          isHf: false
          isPro: false
          name: CUIGuy
          type: user
        html: "<p>Is there more information on your claim on MuP implementation will\
          \ benefit fine-tuning ?  I can understand it is useful for pretraining.\
          \ <span data-props=\"{&quot;user&quot;:&quot;daria-soboleva&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/daria-soboleva\">@<span\
          \ class=\"underline\">daria-soboleva</span></a></span>\n\n\t</span></span>\
          \  </p>\n"
        raw: 'Is there more information on your claim on MuP implementation will benefit
          fine-tuning ?  I can understand it is useful for pretraining. @daria-soboleva  '
        updatedAt: '2023-07-26T15:27:13.944Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Handgun1773
    id: 64bff87489592d4f21c369b4
    type: comment
  author: CUIGuy
  content: 'Is there more information on your claim on MuP implementation will benefit
    fine-tuning ?  I can understand it is useful for pretraining. @daria-soboleva  '
  created_at: 2023-07-25 15:29:40+00:00
  edited: true
  hidden: false
  id: 64bff87489592d4f21c369b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
      fullname: Xiaoyun Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CUIGuy
      type: user
    createdAt: '2023-07-25T16:30:15.000Z'
    data:
      edited: false
      editors:
      - CUIGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8267583847045898
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
          fullname: Xiaoyun Wu
          isHf: false
          isPro: false
          name: CUIGuy
          type: user
        html: '<p>also, will this be compatible with something like vllm down the
          road?</p>

          '
        raw: also, will this be compatible with something like vllm down the road?
        updatedAt: '2023-07-25T16:30:15.707Z'
      numEdits: 0
      reactions: []
    id: 64bff897227250691ab3c0dd
    type: comment
  author: CUIGuy
  content: also, will this be compatible with something like vllm down the road?
  created_at: 2023-07-25 15:30:15+00:00
  edited: false
  hidden: false
  id: 64bff897227250691ab3c0dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
      fullname: Daria Soboleva
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daria-soboleva
      type: user
    createdAt: '2023-07-26T17:37:01.000Z'
    data:
      edited: false
      editors:
      - daria-soboleva
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9461171627044678
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
          fullname: Daria Soboleva
          isHf: false
          isPro: false
          name: daria-soboleva
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CUIGuy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CUIGuy\">@<span class=\"\
          underline\">CUIGuy</span></a></span>\n\n\t</span></span> we are releasing\
          \ our paper soon with all the details on how MuP is helpful, but for now\
          \ feel free to take a look at <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2304.03208\"\
          >https://arxiv.org/abs/2304.03208</a> or <a rel=\"nofollow\" href=\"https://github.com/microsoft/mup\"\
          >https://github.com/microsoft/mup</a> for details on how it works. On a\
          \ high-level, it should drastically reduce amount of experiments that you\
          \ want to try out with HP. You can do that with smaller scale and zero shot\
          \ those params into a larger scale. Saves you compute needed to find best\
          \ HP for the large scale. </p>\n<p>Thank you for the recommendation to support\
          \ on the vllm, for now we have support on HF, but if there is more demand\
          \ on adding it to vllm codebase, can certainly do that :) </p>\n"
        raw: "@CUIGuy we are releasing our paper soon with all the details on how\
          \ MuP is helpful, but for now feel free to take a look at https://arxiv.org/abs/2304.03208\
          \ or https://github.com/microsoft/mup for details on how it works. On a\
          \ high-level, it should drastically reduce amount of experiments that you\
          \ want to try out with HP. You can do that with smaller scale and zero shot\
          \ those params into a larger scale. Saves you compute needed to find best\
          \ HP for the large scale. \n\nThank you for the recommendation to support\
          \ on the vllm, for now we have support on HF, but if there is more demand\
          \ on adding it to vllm codebase, can certainly do that :) "
        updatedAt: '2023-07-26T17:37:01.054Z'
      numEdits: 0
      reactions: []
    id: 64c159bd369e2f615f5a0d7d
    type: comment
  author: daria-soboleva
  content: "@CUIGuy we are releasing our paper soon with all the details on how MuP\
    \ is helpful, but for now feel free to take a look at https://arxiv.org/abs/2304.03208\
    \ or https://github.com/microsoft/mup for details on how it works. On a high-level,\
    \ it should drastically reduce amount of experiments that you want to try out\
    \ with HP. You can do that with smaller scale and zero shot those params into\
    \ a larger scale. Saves you compute needed to find best HP for the large scale.\
    \ \n\nThank you for the recommendation to support on the vllm, for now we have\
    \ support on HF, but if there is more demand on adding it to vllm codebase, can\
    \ certainly do that :) "
  created_at: 2023-07-26 16:37:01+00:00
  edited: false
  hidden: false
  id: 64c159bd369e2f615f5a0d7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
      fullname: Xiaoyun Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CUIGuy
      type: user
    createdAt: '2023-07-26T17:41:37.000Z'
    data:
      edited: false
      editors:
      - CUIGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9244198799133301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
          fullname: Xiaoyun Wu
          isHf: false
          isPro: false
          name: CUIGuy
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;CUIGuy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CUIGuy\"\
          >@<span class=\"underline\">CUIGuy</span></a></span>\n\n\t</span></span>\
          \ we are releasing our paper soon with all the details on how MuP is helpful,\
          \ but for now feel free to take a look at <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2304.03208\"\
          >https://arxiv.org/abs/2304.03208</a> or <a rel=\"nofollow\" href=\"https://github.com/microsoft/mup\"\
          >https://github.com/microsoft/mup</a> for details on how it works. On a\
          \ high-level, it should drastically reduce amount of experiments that you\
          \ want to try out with HP. You can do that with smaller scale and zero shot\
          \ those params into a larger scale. Saves you compute needed to find best\
          \ HP for the large scale. </p>\n<p>Thank you for the recommendation to support\
          \ on the vllm, for now we have support on HF, but if there is more demand\
          \ on adding it to vllm codebase, can certainly do that :)</p>\n</blockquote>\n\
          <p>Thanks. By the way, do you have a timeline on when the hf version will\
          \ be ready? Also, when it (hf version will be better) will be on the <a\
          \ href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\"\
          >https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a>?</p>\n"
        raw: "> @CUIGuy we are releasing our paper soon with all the details on how\
          \ MuP is helpful, but for now feel free to take a look at https://arxiv.org/abs/2304.03208\
          \ or https://github.com/microsoft/mup for details on how it works. On a\
          \ high-level, it should drastically reduce amount of experiments that you\
          \ want to try out with HP. You can do that with smaller scale and zero shot\
          \ those params into a larger scale. Saves you compute needed to find best\
          \ HP for the large scale. \n> \n> Thank you for the recommendation to support\
          \ on the vllm, for now we have support on HF, but if there is more demand\
          \ on adding it to vllm codebase, can certainly do that :)\n\nThanks. By\
          \ the way, do you have a timeline on when the hf version will be ready?\
          \ Also, when it (hf version will be better) will be on the https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?"
        updatedAt: '2023-07-26T17:41:37.452Z'
      numEdits: 0
      reactions: []
    id: 64c15ad1d9bdb0d64d57bc0d
    type: comment
  author: CUIGuy
  content: "> @CUIGuy we are releasing our paper soon with all the details on how\
    \ MuP is helpful, but for now feel free to take a look at https://arxiv.org/abs/2304.03208\
    \ or https://github.com/microsoft/mup for details on how it works. On a high-level,\
    \ it should drastically reduce amount of experiments that you want to try out\
    \ with HP. You can do that with smaller scale and zero shot those params into\
    \ a larger scale. Saves you compute needed to find best HP for the large scale.\
    \ \n> \n> Thank you for the recommendation to support on the vllm, for now we\
    \ have support on HF, but if there is more demand on adding it to vllm codebase,\
    \ can certainly do that :)\n\nThanks. By the way, do you have a timeline on when\
    \ the hf version will be ready? Also, when it (hf version will be better) will\
    \ be on the https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?"
  created_at: 2023-07-26 16:41:37+00:00
  edited: false
  hidden: false
  id: 64c15ad1d9bdb0d64d57bc0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
      fullname: Daria Soboleva
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daria-soboleva
      type: user
    createdAt: '2023-07-26T17:45:56.000Z'
    data:
      edited: false
      editors:
      - daria-soboleva
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9282328486442566
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
          fullname: Daria Soboleva
          isHf: false
          isPro: false
          name: daria-soboleva
          type: user
        html: "<blockquote>\n<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;CUIGuy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CUIGuy\"\
          >@<span class=\"underline\">CUIGuy</span></a></span>\n\n\t</span></span>\
          \ we are releasing our paper soon with all the details on how MuP is helpful,\
          \ but for now feel free to take a look at <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2304.03208\"\
          >https://arxiv.org/abs/2304.03208</a> or <a rel=\"nofollow\" href=\"https://github.com/microsoft/mup\"\
          >https://github.com/microsoft/mup</a> for details on how it works. On a\
          \ high-level, it should drastically reduce amount of experiments that you\
          \ want to try out with HP. You can do that with smaller scale and zero shot\
          \ those params into a larger scale. Saves you compute needed to find best\
          \ HP for the large scale. </p>\n<p>Thank you for the recommendation to support\
          \ on the vllm, for now we have support on HF, but if there is more demand\
          \ on adding it to vllm codebase, can certainly do that :)</p>\n</blockquote>\n\
          <p>Thanks. By the way, do you have a timeline on when the hf version will\
          \ be ready? Also, when it (hf version will be better) will be on the <a\
          \ href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\"\
          >https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a>?</p>\n\
          </blockquote>\n<p>I guess you mean when HF version without trust remote\
          \ code flag will be ready? I would imagine in the next few months, but unfortunately\
          \ cannot provide any more concrete deadlines. </p>\n"
        raw: "> > @CUIGuy we are releasing our paper soon with all the details on\
          \ how MuP is helpful, but for now feel free to take a look at https://arxiv.org/abs/2304.03208\
          \ or https://github.com/microsoft/mup for details on how it works. On a\
          \ high-level, it should drastically reduce amount of experiments that you\
          \ want to try out with HP. You can do that with smaller scale and zero shot\
          \ those params into a larger scale. Saves you compute needed to find best\
          \ HP for the large scale. \n> > \n> > Thank you for the recommendation to\
          \ support on the vllm, for now we have support on HF, but if there is more\
          \ demand on adding it to vllm codebase, can certainly do that :)\n> \n>\
          \ Thanks. By the way, do you have a timeline on when the hf version will\
          \ be ready? Also, when it (hf version will be better) will be on the https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?\n\
          \nI guess you mean when HF version without trust remote code flag will be\
          \ ready? I would imagine in the next few months, but unfortunately cannot\
          \ provide any more concrete deadlines. "
        updatedAt: '2023-07-26T17:45:56.449Z'
      numEdits: 0
      reactions: []
    id: 64c15bd42b37de867c3182eb
    type: comment
  author: daria-soboleva
  content: "> > @CUIGuy we are releasing our paper soon with all the details on how\
    \ MuP is helpful, but for now feel free to take a look at https://arxiv.org/abs/2304.03208\
    \ or https://github.com/microsoft/mup for details on how it works. On a high-level,\
    \ it should drastically reduce amount of experiments that you want to try out\
    \ with HP. You can do that with smaller scale and zero shot those params into\
    \ a larger scale. Saves you compute needed to find best HP for the large scale.\
    \ \n> > \n> > Thank you for the recommendation to support on the vllm, for now\
    \ we have support on HF, but if there is more demand on adding it to vllm codebase,\
    \ can certainly do that :)\n> \n> Thanks. By the way, do you have a timeline on\
    \ when the hf version will be ready? Also, when it (hf version will be better)\
    \ will be on the https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?\n\
    \nI guess you mean when HF version without trust remote code flag will be ready?\
    \ I would imagine in the next few months, but unfortunately cannot provide any\
    \ more concrete deadlines. "
  created_at: 2023-07-26 16:45:56+00:00
  edited: false
  hidden: false
  id: 64c15bd42b37de867c3182eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
      fullname: Xiaoyun Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CUIGuy
      type: user
    createdAt: '2023-07-26T17:49:26.000Z'
    data:
      edited: false
      editors:
      - CUIGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9411938786506653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af5e8947845d198701e452cd8a514cce.svg
          fullname: Xiaoyun Wu
          isHf: false
          isPro: false
          name: CUIGuy
          type: user
        html: "<p>ok, meanwhile, is it possible to release a frozen version (without\
          \ MuP) so that we can be use it without poking into it? Many people only\
          \ care about fine tuning on the specific sized models, so MuP is not that\
          \ useful at all. <span data-props=\"{&quot;user&quot;:&quot;daria-soboleva&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daria-soboleva\"\
          >@<span class=\"underline\">daria-soboleva</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'ok, meanwhile, is it possible to release a frozen version (without MuP)
          so that we can be use it without poking into it? Many people only care about
          fine tuning on the specific sized models, so MuP is not that useful at all.
          @daria-soboleva '
        updatedAt: '2023-07-26T17:49:26.166Z'
      numEdits: 0
      reactions: []
    id: 64c15ca6defa69121210e9a6
    type: comment
  author: CUIGuy
  content: 'ok, meanwhile, is it possible to release a frozen version (without MuP)
    so that we can be use it without poking into it? Many people only care about fine
    tuning on the specific sized models, so MuP is not that useful at all. @daria-soboleva '
  created_at: 2023-07-26 16:49:26+00:00
  edited: false
  hidden: false
  id: 64c15ca6defa69121210e9a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60c908f5d3f493296bae29fb/vlczac90Je1dd826vObeS.jpeg?w=200&h=200&f=face
      fullname: Richard Kuzma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rskuzma
      type: user
    createdAt: '2023-07-26T19:37:50.000Z'
    data:
      edited: true
      editors:
      - rskuzma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9647632241249084
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60c908f5d3f493296bae29fb/vlczac90Je1dd826vObeS.jpeg?w=200&h=200&f=face
          fullname: Richard Kuzma
          isHf: false
          isPro: false
          name: rskuzma
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;CUIGuy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CUIGuy\">@<span class=\"\
          underline\">CUIGuy</span></a></span>\n\n\t</span></span>, thanks for your\
          \ interest! I don't believe HF currently supports SwiGLU and ALiBi for its\
          \ GPT2 model class that we use (though maybe I've missed an alternative)\
          \ so even without muP a custom class and <code>trust_remote_code</code>\
          \ may be required for models with the BTLM architecture</p>\n"
        raw: Hi @CUIGuy, thanks for your interest! I don't believe HF currently supports
          SwiGLU and ALiBi for its GPT2 model class that we use (though maybe I've
          missed an alternative) so even without muP a custom class and `trust_remote_code`
          may be required for models with the BTLM architecture
        updatedAt: '2023-07-26T19:39:42.487Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - daria-soboleva
    id: 64c1760e561398def7625d7a
    type: comment
  author: rskuzma
  content: Hi @CUIGuy, thanks for your interest! I don't believe HF currently supports
    SwiGLU and ALiBi for its GPT2 model class that we use (though maybe I've missed
    an alternative) so even without muP a custom class and `trust_remote_code` may
    be required for models with the BTLM architecture
  created_at: 2023-07-26 18:37:50+00:00
  edited: true
  hidden: false
  id: 64c1760e561398def7625d7a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: cerebras/btlm-3b-8k-base
repo_type: model
status: open
target_branch: null
title: 'why  we can not make this fully HF ready? '
