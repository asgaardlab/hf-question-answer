!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vedtam
conflicting_files: null
created_at: 2023-07-27 05:33:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
      fullname: Edmond Varga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vedtam
      type: user
    createdAt: '2023-07-27T06:33:40.000Z'
    data:
      edited: false
      editors:
      - vedtam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960408627986908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
          fullname: Edmond Varga
          isHf: false
          isPro: false
          name: vedtam
          type: user
        html: '<p>Hi,</p>

          <p>I''m wondering what others have to say about inference time, am I doing
          something wrong or is it normal to have inference time &gt; 6 minutes on
          average for single line prompts?</p>

          '
        raw: "Hi,\r\n\r\nI'm wondering what others have to say about inference time,\
          \ am I doing something wrong or is it normal to have inference time > 6\
          \ minutes on average for single line prompts?"
        updatedAt: '2023-07-27T06:33:40.230Z'
      numEdits: 0
      reactions: []
    id: 64c20fc4b005aab93d65ce02
    type: comment
  author: vedtam
  content: "Hi,\r\n\r\nI'm wondering what others have to say about inference time,\
    \ am I doing something wrong or is it normal to have inference time > 6 minutes\
    \ on average for single line prompts?"
  created_at: 2023-07-27 05:33:40+00:00
  edited: false
  hidden: false
  id: 64c20fc4b005aab93d65ce02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6132c6c146a28b5058423152/uCf-HNZnG3CNo9mNwzgQ-.jpeg?w=200&h=200&f=face
      fullname: Rob Myers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robertmyers
      type: user
    createdAt: '2023-07-27T20:36:33.000Z'
    data:
      edited: false
      editors:
      - robertmyers
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9937283992767334
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6132c6c146a28b5058423152/uCf-HNZnG3CNo9mNwzgQ-.jpeg?w=200&h=200&f=face
          fullname: Rob Myers
          isHf: false
          isPro: false
          name: robertmyers
          type: user
        html: '<p>are you quantizing at all? could you share the code you are using?</p>

          '
        raw: are you quantizing at all? could you share the code you are using?
        updatedAt: '2023-07-27T20:36:33.907Z'
      numEdits: 0
      reactions: []
    id: 64c2d551a2f9706201467908
    type: comment
  author: robertmyers
  content: are you quantizing at all? could you share the code you are using?
  created_at: 2023-07-27 19:36:33+00:00
  edited: false
  hidden: false
  id: 64c2d551a2f9706201467908
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
      fullname: Edmond Varga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vedtam
      type: user
    createdAt: '2023-07-28T05:36:02.000Z'
    data:
      edited: false
      editors:
      - vedtam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8768046498298645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
          fullname: Edmond Varga
          isHf: false
          isPro: false
          name: vedtam
          type: user
        html: '<p>I''m using the sample code from the model''s page here on HF, the
          first block using the transformer library (copy/paste).</p>

          <p>I''m not quantitizing.</p>

          '
        raw: 'I''m using the sample code from the model''s page here on HF, the first
          block using the transformer library (copy/paste).


          I''m not quantitizing.'
        updatedAt: '2023-07-28T05:36:02.464Z'
      numEdits: 0
      reactions: []
    id: 64c353c2f560da9195baf3ed
    type: comment
  author: vedtam
  content: 'I''m using the sample code from the model''s page here on HF, the first
    block using the transformer library (copy/paste).


    I''m not quantitizing.'
  created_at: 2023-07-28 04:36:02+00:00
  edited: false
  hidden: false
  id: 64c353c2f560da9195baf3ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2790bc47ca5aee55f00efd2d454ae82.svg
      fullname: Doug
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DukeFerdinand
      type: user
    createdAt: '2023-08-02T01:42:00.000Z'
    data:
      edited: false
      editors:
      - DukeFerdinand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9801622033119202
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2790bc47ca5aee55f00efd2d454ae82.svg
          fullname: Doug
          isHf: false
          isPro: false
          name: DukeFerdinand
          type: user
        html: '<p>I''m getting the same experience from just running the sample code
          as well. Seems like a cool model so I''d love for this to work, but a few
          minutes completion times for basic prompts isn''t gonna cut it :(</p>

          '
        raw: I'm getting the same experience from just running the sample code as
          well. Seems like a cool model so I'd love for this to work, but a few minutes
          completion times for basic prompts isn't gonna cut it :(
        updatedAt: '2023-08-02T01:42:00.460Z'
      numEdits: 0
      reactions: []
    id: 64c9b4684cc4849813726d90
    type: comment
  author: DukeFerdinand
  content: I'm getting the same experience from just running the sample code as well.
    Seems like a cool model so I'd love for this to work, but a few minutes completion
    times for basic prompts isn't gonna cut it :(
  created_at: 2023-08-02 00:42:00+00:00
  edited: false
  hidden: false
  id: 64c9b4684cc4849813726d90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
      fullname: Edmond Varga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vedtam
      type: user
    createdAt: '2023-08-02T04:55:09.000Z'
    data:
      edited: false
      editors:
      - vedtam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9557839035987854
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
          fullname: Edmond Varga
          isHf: false
          isPro: false
          name: vedtam
          type: user
        html: '<p>I''ve posted the same question onnthe Cerebras Discord too. It was
          ignored which brings me to the assumption that you either need quantitasation
          or the M1 chip is not supported and the inference runs purely on CPU instead
          of the graphic acceleration.</p>

          <p>Hopefully it''s the second because if you need to quantitise the model,
          it defeats the purpose of running 3b as I can run just fine quantitised
          7b models on my local infrastructure.</p>

          '
        raw: 'I''ve posted the same question onnthe Cerebras Discord too. It was ignored
          which brings me to the assumption that you either need quantitasation or
          the M1 chip is not supported and the inference runs purely on CPU instead
          of the graphic acceleration.


          Hopefully it''s the second because if you need to quantitise the model,
          it defeats the purpose of running 3b as I can run just fine quantitised
          7b models on my local infrastructure.'
        updatedAt: '2023-08-02T04:55:09.092Z'
      numEdits: 0
      reactions: []
    id: 64c9e1ad36c11430f342e413
    type: comment
  author: vedtam
  content: 'I''ve posted the same question onnthe Cerebras Discord too. It was ignored
    which brings me to the assumption that you either need quantitasation or the M1
    chip is not supported and the inference runs purely on CPU instead of the graphic
    acceleration.


    Hopefully it''s the second because if you need to quantitise the model, it defeats
    the purpose of running 3b as I can run just fine quantitised 7b models on my local
    infrastructure.'
  created_at: 2023-08-02 03:55:09+00:00
  edited: false
  hidden: false
  id: 64c9e1ad36c11430f342e413
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57908591b00256c1a185ad8a165b8d70.svg
      fullname: Chris K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nudelbrot
      type: user
    createdAt: '2023-08-02T18:07:14.000Z'
    data:
      edited: true
      editors:
      - nudelbrot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8909927010536194
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57908591b00256c1a185ad8a165b8d70.svg
          fullname: Chris K
          isHf: false
          isPro: false
          name: nudelbrot
          type: user
        html: '<p>can confirm inference is extremely slow for me too (Pascal Card,
          8 GB) compared to quantized llama2-hf-7b.<br>Using the example inference
          code.</p>

          '
        raw: 'can confirm inference is extremely slow for me too (Pascal Card, 8 GB)
          compared to quantized llama2-hf-7b.

          Using the example inference code.'
        updatedAt: '2023-08-02T18:07:32.705Z'
      numEdits: 1
      reactions: []
    id: 64ca9b52667f4f80851e8721
    type: comment
  author: nudelbrot
  content: 'can confirm inference is extremely slow for me too (Pascal Card, 8 GB)
    compared to quantized llama2-hf-7b.

    Using the example inference code.'
  created_at: 2023-08-02 17:07:14+00:00
  edited: true
  hidden: false
  id: 64ca9b52667f4f80851e8721
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60c908f5d3f493296bae29fb/vlczac90Je1dd826vObeS.jpeg?w=200&h=200&f=face
      fullname: Richard Kuzma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rskuzma
      type: user
    createdAt: '2023-08-02T18:40:53.000Z'
    data:
      edited: false
      editors:
      - rskuzma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8168231844902039
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60c908f5d3f493296bae29fb/vlczac90Je1dd826vObeS.jpeg?w=200&h=200&f=face
          fullname: Richard Kuzma
          isHf: false
          isPro: false
          name: rskuzma
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;vedtam&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vedtam\">@<span class=\"\
          underline\">vedtam</span></a></span>\n\n\t</span></span>, I am able to generate\
          \ using a T4 GPU in Colab in a few seconds using <code>load_in_8bit=True</code>\
          \ (which requires installing <code>accelerate</code> and <code>bitsandbytes</code>)\
          \ and the default Hugging Face transformers <code>model.generate()</code></p>\n\
          <p>As I understand it, the default transformers library isn't great for\
          \ inference latency compared to redpajama.cpp, vLLM, etc (<a rel=\"nofollow\"\
          \ href=\"https://hamel.dev/notes/llm/03_inference.html\">https://hamel.dev/notes/llm/03_inference.html</a>)</p>\n\
          <p>We're also looking into integrating with popular quantization and inference\
          \ tools</p>\n"
        raw: 'Hi @vedtam, I am able to generate using a T4 GPU in Colab in a few seconds
          using `load_in_8bit=True` (which requires installing `accelerate` and `bitsandbytes`)
          and the default Hugging Face transformers `model.generate()`


          As I understand it, the default transformers library isn''t great for inference
          latency compared to redpajama.cpp, vLLM, etc (https://hamel.dev/notes/llm/03_inference.html)


          We''re also looking into integrating with popular quantization and inference
          tools'
        updatedAt: '2023-08-02T18:40:53.744Z'
      numEdits: 0
      reactions: []
    id: 64caa335c58bea735b512726
    type: comment
  author: rskuzma
  content: 'Hi @vedtam, I am able to generate using a T4 GPU in Colab in a few seconds
    using `load_in_8bit=True` (which requires installing `accelerate` and `bitsandbytes`)
    and the default Hugging Face transformers `model.generate()`


    As I understand it, the default transformers library isn''t great for inference
    latency compared to redpajama.cpp, vLLM, etc (https://hamel.dev/notes/llm/03_inference.html)


    We''re also looking into integrating with popular quantization and inference tools'
  created_at: 2023-08-02 17:40:53+00:00
  edited: false
  hidden: false
  id: 64caa335c58bea735b512726
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
      fullname: Edmond Varga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vedtam
      type: user
    createdAt: '2023-08-03T06:02:27.000Z'
    data:
      edited: true
      editors:
      - vedtam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581407904624939
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
          fullname: Edmond Varga
          isHf: false
          isPro: false
          name: vedtam
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rskuzma&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rskuzma\">@<span class=\"\
          underline\">rskuzma</span></a></span>\n\n\t</span></span> thanks for the\
          \ update. I'm looking forward to have other means running the model for\
          \ benchmarking, I'm keen to see how it could run on consumer hardware as\
          \ for the T4 GPU, that's significantly more powerful than my Apple M1, not\
          \ to mention a ~4 GB mobile device that the announcement of this model emphases.</p>\n\
          <p>Btw, that's a useful link, thanks for sharing!</p>\n"
        raw: '@rskuzma thanks for the update. I''m looking forward to have other means
          running the model for benchmarking, I''m keen to see how it could run on
          consumer hardware as for the T4 GPU, that''s significantly more powerful
          than my Apple M1, not to mention a ~4 GB mobile device that the announcement
          of this model emphases.


          Btw, that''s a useful link, thanks for sharing!'
        updatedAt: '2023-08-03T06:04:47.307Z'
      numEdits: 1
      reactions: []
    id: 64cb42f35aa1ab065c12c31e
    type: comment
  author: vedtam
  content: '@rskuzma thanks for the update. I''m looking forward to have other means
    running the model for benchmarking, I''m keen to see how it could run on consumer
    hardware as for the T4 GPU, that''s significantly more powerful than my Apple
    M1, not to mention a ~4 GB mobile device that the announcement of this model emphases.


    Btw, that''s a useful link, thanks for sharing!'
  created_at: 2023-08-03 05:02:27+00:00
  edited: true
  hidden: false
  id: 64cb42f35aa1ab065c12c31e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c47a267877fdc687c613da/G0uoHHgEuND3Ro5zU1Kk5.jpeg?w=200&h=200&f=face
      fullname: Nolan Dey
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ndey96
      type: user
    createdAt: '2023-08-03T19:17:41.000Z'
    data:
      edited: false
      editors:
      - ndey96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.786454439163208
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c47a267877fdc687c613da/G0uoHHgEuND3Ro5zU1Kk5.jpeg?w=200&h=200&f=face
          fullname: Nolan Dey
          isHf: false
          isPro: false
          name: ndey96
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vedtam&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vedtam\">@<span class=\"\
          underline\">vedtam</span></a></span>\n\n\t</span></span> thanks for your\
          \ interest in BTLM! It sounds like the inference implementation you are\
          \ using is not well optimized. To get rough estimates of throughput for\
          \ BTLM I used <a rel=\"nofollow\" href=\"https://github.com/togethercomputer/redpajama.cpp\"\
          >redpajama.cpp</a> to collect throughputs for <a href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\"\
          >RedPajama-INCITE-Base-3B-v1</a> which has a nearly identical cost to the\
          \ BTLM-3B-8K model. I used <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp\"\
          >llama.cpp</a> to collect throughput numbers for LLaMA 7B.<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/62c47a267877fdc687c613da/fd2wCbjRII8VqpBPArxe_.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/62c47a267877fdc687c613da/fd2wCbjRII8VqpBPArxe_.png\"\
          ></a></p>\n<p>We are working with maintainers of popular inference libraries\
          \ to support BTLM-3B-8K but you will have to stay tuned for that!</p>\n"
        raw: '@vedtam thanks for your interest in BTLM! It sounds like the inference
          implementation you are using is not well optimized. To get rough estimates
          of throughput for BTLM I used [redpajama.cpp](https://github.com/togethercomputer/redpajama.cpp)
          to collect throughputs for [RedPajama-INCITE-Base-3B-v1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1)
          which has a nearly identical cost to the BTLM-3B-8K model. I used [llama.cpp](https://github.com/ggerganov/llama.cpp)
          to collect throughput numbers for LLaMA 7B.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62c47a267877fdc687c613da/fd2wCbjRII8VqpBPArxe_.png)


          We are working with maintainers of popular inference libraries to support
          BTLM-3B-8K but you will have to stay tuned for that!

          '
        updatedAt: '2023-08-03T19:17:41.354Z'
      numEdits: 0
      reactions: []
    id: 64cbfd559e30a46f7ba523c4
    type: comment
  author: ndey96
  content: '@vedtam thanks for your interest in BTLM! It sounds like the inference
    implementation you are using is not well optimized. To get rough estimates of
    throughput for BTLM I used [redpajama.cpp](https://github.com/togethercomputer/redpajama.cpp)
    to collect throughputs for [RedPajama-INCITE-Base-3B-v1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1)
    which has a nearly identical cost to the BTLM-3B-8K model. I used [llama.cpp](https://github.com/ggerganov/llama.cpp)
    to collect throughput numbers for LLaMA 7B.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62c47a267877fdc687c613da/fd2wCbjRII8VqpBPArxe_.png)


    We are working with maintainers of popular inference libraries to support BTLM-3B-8K
    but you will have to stay tuned for that!

    '
  created_at: 2023-08-03 18:17:41+00:00
  edited: false
  hidden: false
  id: 64cbfd559e30a46f7ba523c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
      fullname: Edmond Varga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vedtam
      type: user
    createdAt: '2023-08-04T04:48:48.000Z'
    data:
      edited: false
      editors:
      - vedtam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.958549439907074
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83155816efb9dd2dc8f12f6e9bbd9680.svg
          fullname: Edmond Varga
          isHf: false
          isPro: false
          name: vedtam
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ndey96&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ndey96\">@<span class=\"\
          underline\">ndey96</span></a></span>\n\n\t</span></span> thanks for the\
          \ numbers and the update, really appreciated! Will keep an eye on the news\
          \ for when BTLM becomes picked up by the other libs.</p>\n"
        raw: '@ndey96 thanks for the numbers and the update, really appreciated! Will
          keep an eye on the news for when BTLM becomes picked up by the other libs.'
        updatedAt: '2023-08-04T04:48:48.079Z'
      numEdits: 0
      reactions: []
    id: 64cc833040ce646cc7fef4ac
    type: comment
  author: vedtam
  content: '@ndey96 thanks for the numbers and the update, really appreciated! Will
    keep an eye on the news for when BTLM becomes picked up by the other libs.'
  created_at: 2023-08-04 03:48:48+00:00
  edited: false
  hidden: false
  id: 64cc833040ce646cc7fef4ac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: cerebras/btlm-3b-8k-base
repo_type: model
status: open
target_branch: null
title: What is the inference time? On my Apple M1 Max completions take > 6 min
