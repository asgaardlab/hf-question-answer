!!python/object:huggingface_hub.community.DiscussionWithDetails
author: baffo32
conflicting_files: null
created_at: 2023-09-23 00:20:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
      fullname: Baffo 32
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baffo32
      type: user
    createdAt: '2023-09-23T01:20:46.000Z'
    data:
      edited: false
      editors:
      - baffo32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9520657062530518
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
          fullname: Baffo 32
          isHf: false
          isPro: false
          name: baffo32
          type: user
        html: "<p>Hey,</p>\n<p>I\u2019m looking at your chart showing incredible performance\
          \ improvement greatly extending the context length with a smaller portion\
          \ of training at the end.</p>\n<p>It\u2019s quite notable most of the gains\
          \ are in the untrained context lengths.</p>\n<p>It looks to me like steadily\
          \ increasing the context length throughout training could possibly flatline\
          \ the chart, these relative gains are so big.</p>\n<p>Has anyone tried training\
          \ on steadily increasing context lengths?</p>\n"
        raw: "Hey,\r\n\r\nI\u2019m looking at your chart showing incredible performance\
          \ improvement greatly extending the context length with a smaller portion\
          \ of training at the end.\r\n\r\nIt\u2019s quite notable most of the gains\
          \ are in the untrained context lengths.\r\n\r\nIt looks to me like steadily\
          \ increasing the context length throughout training could possibly flatline\
          \ the chart, these relative gains are so big.\r\n\r\nHas anyone tried training\
          \ on steadily increasing context lengths?"
        updatedAt: '2023-09-23T01:20:46.100Z'
      numEdits: 0
      reactions: []
    id: 650e3d6e2a45730c3fcf5113
    type: comment
  author: baffo32
  content: "Hey,\r\n\r\nI\u2019m looking at your chart showing incredible performance\
    \ improvement greatly extending the context length with a smaller portion of training\
    \ at the end.\r\n\r\nIt\u2019s quite notable most of the gains are in the untrained\
    \ context lengths.\r\n\r\nIt looks to me like steadily increasing the context\
    \ length throughout training could possibly flatline the chart, these relative\
    \ gains are so big.\r\n\r\nHas anyone tried training on steadily increasing context\
    \ lengths?"
  created_at: 2023-09-23 00:20:46+00:00
  edited: false
  hidden: false
  id: 650e3d6e2a45730c3fcf5113
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
      fullname: Daria Soboleva
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daria-soboleva
      type: user
    createdAt: '2023-09-25T18:50:46.000Z'
    data:
      edited: true
      editors:
      - daria-soboleva
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9257342219352722
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679080567124-6414bbe095fb6f824b2035a5.jpeg?w=200&h=200&f=face
          fullname: Daria Soboleva
          isHf: false
          isPro: false
          name: daria-soboleva
          type: user
        html: "<blockquote>\n<p>Hey,</p>\n<p>I\u2019m looking at your chart showing\
          \ incredible performance improvement greatly extending the context length\
          \ with a smaller portion of training at the end.</p>\n<p>It\u2019s quite\
          \ notable most of the gains are in the untrained context lengths.</p>\n\
          <p>It looks to me like steadily increasing the context length throughout\
          \ training could possibly flatline the chart, these relative gains are so\
          \ big.</p>\n<p>Has anyone tried training on steadily increasing context\
          \ lengths?</p>\n</blockquote>\n<p>Yes, this is a good idea. One of the examples\
          \ is Xgen long sequence models: <a rel=\"nofollow\" href=\"https://blog.salesforceairesearch.com/xgen/\"\
          >https://blog.salesforceairesearch.com/xgen/</a>, they trained with 2k,\
          \ 4k and 8k sequence lengths. One downside: you need to perform more granular\
          \ experiments on the smaller scale to find the best combination. Hope that\
          \ helps!</p>\n"
        raw: "> Hey,\n> \n> I\u2019m looking at your chart showing incredible performance\
          \ improvement greatly extending the context length with a smaller portion\
          \ of training at the end.\n> \n> It\u2019s quite notable most of the gains\
          \ are in the untrained context lengths.\n> \n> It looks to me like steadily\
          \ increasing the context length throughout training could possibly flatline\
          \ the chart, these relative gains are so big.\n> \n> Has anyone tried training\
          \ on steadily increasing context lengths?\n\nYes, this is a good idea. One\
          \ of the examples is Xgen long sequence models: https://blog.salesforceairesearch.com/xgen/,\
          \ they trained with 2k, 4k and 8k sequence lengths. One downside: you need\
          \ to perform more granular experiments on the smaller scale to find the\
          \ best combination. Hope that helps!"
        updatedAt: '2023-09-25T18:51:11.255Z'
      numEdits: 1
      reactions: []
    id: 6511d6865d5b59b9c3d24092
    type: comment
  author: daria-soboleva
  content: "> Hey,\n> \n> I\u2019m looking at your chart showing incredible performance\
    \ improvement greatly extending the context length with a smaller portion of training\
    \ at the end.\n> \n> It\u2019s quite notable most of the gains are in the untrained\
    \ context lengths.\n> \n> It looks to me like steadily increasing the context\
    \ length throughout training could possibly flatline the chart, these relative\
    \ gains are so big.\n> \n> Has anyone tried training on steadily increasing context\
    \ lengths?\n\nYes, this is a good idea. One of the examples is Xgen long sequence\
    \ models: https://blog.salesforceairesearch.com/xgen/, they trained with 2k, 4k\
    \ and 8k sequence lengths. One downside: you need to perform more granular experiments\
    \ on the smaller scale to find the best combination. Hope that helps!"
  created_at: 2023-09-25 17:50:46+00:00
  edited: true
  hidden: false
  id: 6511d6865d5b59b9c3d24092
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
      fullname: Baffo 32
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baffo32
      type: user
    createdAt: '2023-09-26T00:42:32.000Z'
    data:
      edited: false
      editors:
      - baffo32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7278123497962952
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
          fullname: Baffo 32
          isHf: false
          isPro: false
          name: baffo32
          type: user
        html: "<div class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n\
          <th align=\"center\">xgen</th>\n<th align=\"center\">btlm</th>\n</tr>\n\n\
          \t\t</thead><tbody><tr>\n<td align=\"center\"><img width=\"256\" src=\"\
          https://lh6.googleusercontent.com/HVZ52gGXXhP3S1A6zmCNPCQjR_0zPJdEa0Piw34K92wPGhcNnmUkvvqmMivyqWtObaB8intWIo79fp9jYdeETOUtgfK96WpHL-X4_VVGSfuahAw-ef1FgqGCVe6fzHKWpJv-sQsPd5Ed4mBorBwg7fg\"\
          ></td>\n<td align=\"center\"><img width=\"256\" src=\"https://huggingface.co/cerebras/btlm-3b-8k-base/resolve/main/figure_5_xentropy_with_sequence_lengths.svg\"\
          ></td>\n</tr>\n</tbody>\n\t</table>\n</div>\n<p>The XGen chart does not\
          \ give the appearance of transferring to untrained context lengths the way\
          \ the BTLM chart does. It's notable they trained for more tokens on the\
          \ shorter contexts, and plotted against a logarithmic context length axis.</p>\n\
          <p>It still seems very few instances of increasing the context length. Has\
          \ anyone tried ramping up the context length one token at a time during\
          \ training, with ALiBi? Or is there a reason if not?</p>\n"
        raw: '| xgen | btlm |

          |:----:|:----:|

          |<img src="https://lh6.googleusercontent.com/HVZ52gGXXhP3S1A6zmCNPCQjR_0zPJdEa0Piw34K92wPGhcNnmUkvvqmMivyqWtObaB8intWIo79fp9jYdeETOUtgfK96WpHL-X4_VVGSfuahAw-ef1FgqGCVe6fzHKWpJv-sQsPd5Ed4mBorBwg7fg"
          width="256"/>|<img src="https://huggingface.co/cerebras/btlm-3b-8k-base/resolve/main/figure_5_xentropy_with_sequence_lengths.svg"
          width="256"/>|


          The XGen chart does not give the appearance of transferring to untrained
          context lengths the way the BTLM chart does. It''s notable they trained
          for more tokens on the shorter contexts, and plotted against a logarithmic
          context length axis.


          It still seems very few instances of increasing the context length. Has
          anyone tried ramping up the context length one token at a time during training,
          with ALiBi? Or is there a reason if not?

          '
        updatedAt: '2023-09-26T00:42:32.625Z'
      numEdits: 0
      reactions: []
    id: 651228f82f54c75d69ef7337
    type: comment
  author: baffo32
  content: '| xgen | btlm |

    |:----:|:----:|

    |<img src="https://lh6.googleusercontent.com/HVZ52gGXXhP3S1A6zmCNPCQjR_0zPJdEa0Piw34K92wPGhcNnmUkvvqmMivyqWtObaB8intWIo79fp9jYdeETOUtgfK96WpHL-X4_VVGSfuahAw-ef1FgqGCVe6fzHKWpJv-sQsPd5Ed4mBorBwg7fg"
    width="256"/>|<img src="https://huggingface.co/cerebras/btlm-3b-8k-base/resolve/main/figure_5_xentropy_with_sequence_lengths.svg"
    width="256"/>|


    The XGen chart does not give the appearance of transferring to untrained context
    lengths the way the BTLM chart does. It''s notable they trained for more tokens
    on the shorter contexts, and plotted against a logarithmic context length axis.


    It still seems very few instances of increasing the context length. Has anyone
    tried ramping up the context length one token at a time during training, with
    ALiBi? Or is there a reason if not?

    '
  created_at: 2023-09-25 23:42:32+00:00
  edited: false
  hidden: false
  id: 651228f82f54c75d69ef7337
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
      fullname: Baffo 32
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baffo32
      type: user
    createdAt: '2023-09-26T00:43:04.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
          fullname: Baffo 32
          isHf: false
          isPro: false
          name: baffo32
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-26T00:46:12.140Z'
      numEdits: 1
      reactions: []
    id: 65122918b6bdfa5be9ce4218
    type: comment
  author: baffo32
  content: This comment has been hidden
  created_at: 2023-09-25 23:43:04+00:00
  edited: true
  hidden: true
  id: 65122918b6bdfa5be9ce4218
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: cerebras/btlm-3b-8k-base
repo_type: model
status: open
target_branch: null
title: Context length schedule and performance
