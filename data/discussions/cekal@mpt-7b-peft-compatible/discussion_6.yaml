!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yuanchun99
conflicting_files: null
created_at: 2023-06-30 00:28:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baaefc35cdbb78bcffc98be1c58eb13b.svg
      fullname: Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yuanchun99
      type: user
    createdAt: '2023-06-30T01:28:37.000Z'
    data:
      edited: false
      editors:
      - Yuanchun99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8388198614120483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baaefc35cdbb78bcffc98be1c58eb13b.svg
          fullname: Liang
          isHf: false
          isPro: false
          name: Yuanchun99
          type: user
        html: '<p>Thanks for your shared colab!<br><a rel="nofollow" href="https://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing#scrollTo=VXNEwdDL83NK">https://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing#scrollTo=VXNEwdDL83NK</a></p>

          <p>I managed to get the trainable params as follows:<br>trainable params:
          4194304 || all params: 6653480960 || trainable%: 0.0630392425441013</p>

          <p>Two question need your guidance:<br>a. Is that possible to runs it with
          ''PromptTuningConfig'' instead of ''LoraConfig'' ?<br>It need me to input
          the parameters of num_attention_heads and  num_layers, but I don''t know
          where to find it.</p>

          <p>b. for the stated trainable params, how do we know when the training
          data is larger the the ''trainable params'' ?</p>

          <p>Appreciate for your work, MosaicML seems to more focus on the the whole
          finetuning approach instead of PEFT.</p>

          '
        raw: "Thanks for your shared colab!\r\nhttps://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing#scrollTo=VXNEwdDL83NK\r\
          \n\r\nI managed to get the trainable params as follows:\r\ntrainable params:\
          \ 4194304 || all params: 6653480960 || trainable%: 0.0630392425441013\r\n\
          \r\nTwo question need your guidance:\r\na. Is that possible to runs it with\
          \ 'PromptTuningConfig' instead of 'LoraConfig' ? \r\nIt need me to input\
          \ the parameters of num_attention_heads and  num_layers, but I don't know\
          \ where to find it.\r\n\r\nb. for the stated trainable params, how do we\
          \ know when the training data is larger the the 'trainable params' ?\r\n\
          \r\nAppreciate for your work, MosaicML seems to more focus on the the whole\
          \ finetuning approach instead of PEFT."
        updatedAt: '2023-06-30T01:28:37.829Z'
      numEdits: 0
      reactions: []
    id: 649e2fc5c36e24ac5d6e73ac
    type: comment
  author: Yuanchun99
  content: "Thanks for your shared colab!\r\nhttps://colab.research.google.com/drive/1iBeY5UTLHE3aL6yNLiCIJHOBDqWBYbi5?usp=sharing#scrollTo=VXNEwdDL83NK\r\
    \n\r\nI managed to get the trainable params as follows:\r\ntrainable params: 4194304\
    \ || all params: 6653480960 || trainable%: 0.0630392425441013\r\n\r\nTwo question\
    \ need your guidance:\r\na. Is that possible to runs it with 'PromptTuningConfig'\
    \ instead of 'LoraConfig' ? \r\nIt need me to input the parameters of num_attention_heads\
    \ and  num_layers, but I don't know where to find it.\r\n\r\nb. for the stated\
    \ trainable params, how do we know when the training data is larger the the 'trainable\
    \ params' ?\r\n\r\nAppreciate for your work, MosaicML seems to more focus on the\
    \ the whole finetuning approach instead of PEFT."
  created_at: 2023-06-30 00:28:37+00:00
  edited: false
  hidden: false
  id: 649e2fc5c36e24ac5d6e73ac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: cekal/mpt-7b-peft-compatible
repo_type: model
status: open
target_branch: null
title: Two question on your shared google colab
