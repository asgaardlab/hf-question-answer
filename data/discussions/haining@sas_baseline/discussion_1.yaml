!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArthurParkerhouse
conflicting_files: null
created_at: 2022-12-04 18:48:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
      fullname: Parkerhouse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurParkerhouse
      type: user
    createdAt: '2022-12-04T18:48:51.000Z'
    data:
      edited: false
      editors:
      - ArthurParkerhouse
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
          fullname: Parkerhouse
          isHf: false
          isPro: false
          name: ArthurParkerhouse
          type: user
        html: '<p>Fantastic model! I''ve been playing with it for the last week. I
          was wondering, is the "summarize, simplify, and contextualize: " instruction
          the only instruction that works with this model, or could you just do something
          line "summarize and contextualize:", or "contextualize and correct punctuation:"?
          Is there a list of all possible instructions that you can provide to flan?
          </p>

          '
        raw: "Fantastic model! I've been playing with it for the last week. I was\
          \ wondering, is the \"summarize, simplify, and contextualize: \" instruction\
          \ the only instruction that works with this model, or could you just do\
          \ something line \"summarize and contextualize:\", or \"contextualize and\
          \ correct punctuation:\"? Is there a list of all possible instructions that\
          \ you can provide to flan? \r\n\r\n"
        updatedAt: '2022-12-04T18:48:51.749Z'
      numEdits: 0
      reactions: []
    id: 638ceb93092a73a14a33716c
    type: comment
  author: ArthurParkerhouse
  content: "Fantastic model! I've been playing with it for the last week. I was wondering,\
    \ is the \"summarize, simplify, and contextualize: \" instruction the only instruction\
    \ that works with this model, or could you just do something line \"summarize\
    \ and contextualize:\", or \"contextualize and correct punctuation:\"? Is there\
    \ a list of all possible instructions that you can provide to flan? \r\n\r\n"
  created_at: 2022-12-04 18:48:51+00:00
  edited: false
  hidden: false
  id: 638ceb93092a73a14a33716c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643384634931-noauth.png?w=200&h=200&f=face
      fullname: Haining Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: haining
      type: user
    createdAt: '2022-12-04T19:36:45.000Z'
    data:
      edited: true
      editors:
      - haining
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643384634931-noauth.png?w=200&h=200&f=face
          fullname: Haining Wang
          isHf: false
          isPro: false
          name: haining
          type: user
        html: "<p>Hi, thanks for your attention and comment!  </p>\n<p>This is our\
          \ baseline model. The only reason we put the instruction \"summarize, simplify,\
          \ and contextualize: \" before an abstract is for a fair comparison to our\
          \ full model. We don't need to prepend an instruction/task prefix if there\
          \ is only one task. From <a href=\"https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/t5\"\
          >t5 page</a> :</p>\n<blockquote>\n<p>According to this forum post, task\
          \ prefixes matter when (1) doing multi-task training (2) your task is similar\
          \ or related to one of the supervised tasks used in T5\u2019s pre-training\
          \ mixture (see Appendix D of the paper for the task prefixes used).</p>\n\
          </blockquote>\n<p>In our full model, we paired each instruction verb with\
          \ a corpus</p>\n<ul>\n<li>\"simplify: \" with wikiauto</li>\n<li>\"summarize:\
          \ \" with cnn_dm</li>\n<li>\"contextualize: \" with a new corpus from Science,\
          \ where the context part of an editor abstract is grafted onto the corresponding\
          \ paper's abstract for the purpose of recontextualization.</li>\n<li>\"\
          summarize, simplify, and contextualize:  \"  with PNAS SAS corpus.<br>We\
          \ show that the performance of main task (SAS) is boosted when adding more\
          \ relevant tasks prepended with natural language instruction.</li>\n</ul>\n\
          <p>We haven't tested the instruction of \"summarize and contextualize: \"\
          , but I like your idea! In theory, the model should generate shorter but\
          \ contextualized summary with roughly the same readability. I think modularizing\
          \ tasks with human language instruction is a fantastic idea and should be\
          \ concerned by neural/data science researchers. We also haven' tested the\
          \ \"contextualize and correct punctuation:\" instruction. But I think this\
          \ is possible: with the recontextualization corpus from Science and randomly\
          \ add/delete/substitute punctation, you can force the model to predict the\
          \ corresponding recontextualized, clean version. If the goal is to test\
          \ how well models like flan t5 can understand instructions and perform task\
          \ mixtures (or even for zero-shot generalization), those ideas are all terrific\
          \ and perhaps understudied. For now, we concern the last mile of scientific\
          \ understanding. </p>\n<p>I hope I made myself clear. And we will release\
          \ our full model shortly, stay tuned!</p>\n"
        raw: "Hi, thanks for your attention and comment!  \n\nThis is our baseline\
          \ model. The only reason we put the instruction \"summarize, simplify, and\
          \ contextualize: \" before an abstract is for a fair comparison to our full\
          \ model. We don't need to prepend an instruction/task prefix if there is\
          \ only one task. From [t5 page](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/t5)\
          \ :\n>According to this forum post, task prefixes matter when (1) doing\
          \ multi-task training (2) your task is similar or related to one of the\
          \ supervised tasks used in T5\u2019s pre-training mixture (see Appendix\
          \ D of the paper for the task prefixes used).\n\nIn our full model, we paired\
          \ each instruction verb with a corpus\n- \"simplify: \" with wikiauto\n\
          - \"summarize: \" with cnn_dm\n- \"contextualize: \" with a new corpus from\
          \ Science, where the context part of an editor abstract is grafted onto\
          \ the corresponding paper's abstract for the purpose of recontextualization.\n\
          - \"summarize, simplify, and contextualize:  \"  with PNAS SAS corpus.\n\
          We show that the performance of main task (SAS) is boosted when adding more\
          \ relevant tasks prepended with natural language instruction.\n\nWe haven't\
          \ tested the instruction of \"summarize and contextualize: \", but I like\
          \ your idea! In theory, the model should generate shorter but contextualized\
          \ summary with roughly the same readability. I think modularizing tasks\
          \ with human language instruction is a fantastic idea and should be concerned\
          \ by neural/data science researchers. We also haven' tested the \"contextualize\
          \ and correct punctuation:\" instruction. But I think this is possible:\
          \ with the recontextualization corpus from Science and randomly add/delete/substitute\
          \ punctation, you can force the model to predict the corresponding recontextualized,\
          \ clean version. If the goal is to test how well models like flan t5 can\
          \ understand instructions and perform task mixtures (or even for zero-shot\
          \ generalization), those ideas are all terrific and perhaps understudied.\
          \ For now, we concern the last mile of scientific understanding. \n\nI hope\
          \ I made myself clear. And we will release our full model shortly, stay\
          \ tuned!"
        updatedAt: '2022-12-04T20:05:46.999Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurParkerhouse
    id: 638cf6cdebda86f24b5d05bb
    type: comment
  author: haining
  content: "Hi, thanks for your attention and comment!  \n\nThis is our baseline model.\
    \ The only reason we put the instruction \"summarize, simplify, and contextualize:\
    \ \" before an abstract is for a fair comparison to our full model. We don't need\
    \ to prepend an instruction/task prefix if there is only one task. From [t5 page](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/t5)\
    \ :\n>According to this forum post, task prefixes matter when (1) doing multi-task\
    \ training (2) your task is similar or related to one of the supervised tasks\
    \ used in T5\u2019s pre-training mixture (see Appendix D of the paper for the\
    \ task prefixes used).\n\nIn our full model, we paired each instruction verb with\
    \ a corpus\n- \"simplify: \" with wikiauto\n- \"summarize: \" with cnn_dm\n- \"\
    contextualize: \" with a new corpus from Science, where the context part of an\
    \ editor abstract is grafted onto the corresponding paper's abstract for the purpose\
    \ of recontextualization.\n- \"summarize, simplify, and contextualize:  \"  with\
    \ PNAS SAS corpus.\nWe show that the performance of main task (SAS) is boosted\
    \ when adding more relevant tasks prepended with natural language instruction.\n\
    \nWe haven't tested the instruction of \"summarize and contextualize: \", but\
    \ I like your idea! In theory, the model should generate shorter but contextualized\
    \ summary with roughly the same readability. I think modularizing tasks with human\
    \ language instruction is a fantastic idea and should be concerned by neural/data\
    \ science researchers. We also haven' tested the \"contextualize and correct punctuation:\"\
    \ instruction. But I think this is possible: with the recontextualization corpus\
    \ from Science and randomly add/delete/substitute punctation, you can force the\
    \ model to predict the corresponding recontextualized, clean version. If the goal\
    \ is to test how well models like flan t5 can understand instructions and perform\
    \ task mixtures (or even for zero-shot generalization), those ideas are all terrific\
    \ and perhaps understudied. For now, we concern the last mile of scientific understanding.\
    \ \n\nI hope I made myself clear. And we will release our full model shortly,\
    \ stay tuned!"
  created_at: 2022-12-04 19:36:45+00:00
  edited: true
  hidden: false
  id: 638cf6cdebda86f24b5d05bb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: haining/sas_baseline
repo_type: model
status: open
target_branch: null
title: Love it!
