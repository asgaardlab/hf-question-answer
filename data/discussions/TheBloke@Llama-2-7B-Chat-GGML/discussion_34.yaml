!!python/object:huggingface_hub.community.DiscussionWithDetails
author: luissimoes
conflicting_files: null
created_at: 2023-11-13 15:23:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9edeb5f3c634b437ad3af1a5ad1d48c9.svg
      fullname: Luis Simoes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luissimoes
      type: user
    createdAt: '2023-11-13T15:23:56.000Z'
    data:
      edited: false
      editors:
      - luissimoes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9563347697257996
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9edeb5f3c634b437ad3af1a5ad1d48c9.svg
          fullname: Luis Simoes
          isHf: false
          isPro: false
          name: luissimoes
          type: user
        html: '<p>I am currently new to LLMs and its deployment model...<br>But I
          want to know if its possible to use this with TGI (text-generation-inference)
          in order to leverage APIs.</p>

          <p>Thank you</p>

          '
        raw: "I am currently new to LLMs and its deployment model...\r\nBut I want\
          \ to know if its possible to use this with TGI (text-generation-inference)\
          \ in order to leverage APIs.\r\n\r\nThank you"
        updatedAt: '2023-11-13T15:23:56.081Z'
      numEdits: 0
      reactions: []
    id: 65523f8c4a5191e1f19e497d
    type: comment
  author: luissimoes
  content: "I am currently new to LLMs and its deployment model...\r\nBut I want to\
    \ know if its possible to use this with TGI (text-generation-inference) in order\
    \ to leverage APIs.\r\n\r\nThank you"
  created_at: 2023-11-13 15:23:56+00:00
  edited: false
  hidden: false
  id: 65523f8c4a5191e1f19e497d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-13T15:28:04.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9340139031410217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Not this GGML variant, which is old and not recommended to use with
          anything now. GGML has been superseded by GGUF, but that is also not compatible
          with TGI. GGML/GGUF is a special format designed for use on smaller hardware,
          including without a GPU, but isn''t supported by inference servers like
          TGI or vLLM.  There are inference servers that support GGUF, like text-generation-webui,
          but that''s more for single-user operation, not multi-user like TGI and
          vLLM.</p>

          <p>Check out my Llama-2-7B-Chat-GPTQ or Llama-2-7B-Chat-AWQ repos, they
          both work with TGI.</p>

          '
        raw: 'Not this GGML variant, which is old and not recommended to use with
          anything now. GGML has been superseded by GGUF, but that is also not compatible
          with TGI. GGML/GGUF is a special format designed for use on smaller hardware,
          including without a GPU, but isn''t supported by inference servers like
          TGI or vLLM.  There are inference servers that support GGUF, like text-generation-webui,
          but that''s more for single-user operation, not multi-user like TGI and
          vLLM.


          Check out my Llama-2-7B-Chat-GPTQ or Llama-2-7B-Chat-AWQ repos, they both
          work with TGI.'
        updatedAt: '2023-11-13T15:28:04.336Z'
      numEdits: 0
      reactions: []
    id: 65524084e631439963093846
    type: comment
  author: TheBloke
  content: 'Not this GGML variant, which is old and not recommended to use with anything
    now. GGML has been superseded by GGUF, but that is also not compatible with TGI.
    GGML/GGUF is a special format designed for use on smaller hardware, including
    without a GPU, but isn''t supported by inference servers like TGI or vLLM.  There
    are inference servers that support GGUF, like text-generation-webui, but that''s
    more for single-user operation, not multi-user like TGI and vLLM.


    Check out my Llama-2-7B-Chat-GPTQ or Llama-2-7B-Chat-AWQ repos, they both work
    with TGI.'
  created_at: 2023-11-13 15:28:04+00:00
  edited: false
  hidden: false
  id: 65524084e631439963093846
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9edeb5f3c634b437ad3af1a5ad1d48c9.svg
      fullname: Luis Simoes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luissimoes
      type: user
    createdAt: '2023-11-13T16:22:42.000Z'
    data:
      edited: true
      editors:
      - luissimoes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9643417000770569
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9edeb5f3c634b437ad3af1a5ad1d48c9.svg
          fullname: Luis Simoes
          isHf: false
          isPro: false
          name: luissimoes
          type: user
        html: '<p>Thank you very much for the clarification.</p>

          <p>Do those run on TGI and using CPU? Are they quantized? I wanted to have
          same quality as 7B.</p>

          '
        raw: 'Thank you very much for the clarification.


          Do those run on TGI and using CPU? Are they quantized? I wanted to have
          same quality as 7B.'
        updatedAt: '2023-11-13T16:24:45.796Z'
      numEdits: 1
      reactions: []
    id: 65524d529e144c06dd025ca1
    type: comment
  author: luissimoes
  content: 'Thank you very much for the clarification.


    Do those run on TGI and using CPU? Are they quantized? I wanted to have same quality
    as 7B.'
  created_at: 2023-11-13 16:22:42+00:00
  edited: true
  hidden: false
  id: 65524d529e144c06dd025ca1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-13T16:27:04.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9408011436462402
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>All my uploads are quantised.</p>

          <p>Yes the GPTQ and AWQ models run on TGI, but they are GPU only.  TGI is
          GPU only.</p>

          <p>If you want CPU then you do want the GGUF models, but you can''t use
          TGI.  There are various options for providing an API using GGUF models,
          like text-generation-webui''s API. I''d recommend using that.</p>

          '
        raw: 'All my uploads are quantised.


          Yes the GPTQ and AWQ models run on TGI, but they are GPU only.  TGI is GPU
          only.


          If you want CPU then you do want the GGUF models, but you can''t use TGI.  There
          are various options for providing an API using GGUF models, like text-generation-webui''s
          API. I''d recommend using that.'
        updatedAt: '2023-11-13T16:27:22.505Z'
      numEdits: 1
      reactions: []
    id: 65524e58a9e0627208f83b38
    type: comment
  author: TheBloke
  content: 'All my uploads are quantised.


    Yes the GPTQ and AWQ models run on TGI, but they are GPU only.  TGI is GPU only.


    If you want CPU then you do want the GGUF models, but you can''t use TGI.  There
    are various options for providing an API using GGUF models, like text-generation-webui''s
    API. I''d recommend using that.'
  created_at: 2023-11-13 16:27:04+00:00
  edited: true
  hidden: false
  id: 65524e58a9e0627208f83b38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9edeb5f3c634b437ad3af1a5ad1d48c9.svg
      fullname: Luis Simoes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luissimoes
      type: user
    createdAt: '2023-11-13T17:08:09.000Z'
    data:
      edited: false
      editors:
      - luissimoes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8567876815795898
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9edeb5f3c634b437ad3af1a5ad1d48c9.svg
          fullname: Luis Simoes
          isHf: false
          isPro: false
          name: luissimoes
          type: user
        html: '<p>Thanks!</p>

          <p>I am doing some experiments using the GGML version now.<br>But struggling
          to understand why the approach using AutoModelForCausalLM from ctransformers
          gives a different result than using CTransformers from LangChain... Same
          prompt different results.</p>

          <p>AutoModelForCausalLM is more close to what I am looking for, just need
          to figure out what causes the difference...</p>

          '
        raw: 'Thanks!


          I am doing some experiments using the GGML version now.

          But struggling to understand why the approach using AutoModelForCausalLM
          from ctransformers gives a different result than using CTransformers from
          LangChain... Same prompt different results.


          AutoModelForCausalLM is more close to what I am looking for, just need to
          figure out what causes the difference...


          '
        updatedAt: '2023-11-13T17:08:09.609Z'
      numEdits: 0
      reactions: []
    id: 655257f9ded3140cd1f061a9
    type: comment
  author: luissimoes
  content: 'Thanks!


    I am doing some experiments using the GGML version now.

    But struggling to understand why the approach using AutoModelForCausalLM from
    ctransformers gives a different result than using CTransformers from LangChain...
    Same prompt different results.


    AutoModelForCausalLM is more close to what I am looking for, just need to figure
    out what causes the difference...


    '
  created_at: 2023-11-13 17:08:09+00:00
  edited: false
  hidden: false
  id: 655257f9ded3140cd1f061a9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: How to configure with TGI?
