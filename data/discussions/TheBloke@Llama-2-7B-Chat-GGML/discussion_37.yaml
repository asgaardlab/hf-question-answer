!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Lozzoya
conflicting_files: null
created_at: 2024-01-05 21:00:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33dfa1495b4cd39ef8e13b25b7a5299e.svg
      fullname: Christopher Lozoya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lozzoya
      type: user
    createdAt: '2024-01-05T21:00:54.000Z'
    data:
      edited: false
      editors:
      - Lozzoya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5825661420822144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33dfa1495b4cd39ef8e13b25b7a5299e.svg
          fullname: Christopher Lozoya
          isHf: false
          isPro: false
          name: Lozzoya
          type: user
        html: '<p>For some reason I can''t load the llama model from the correct file
          path? its in my downloads but says there is a (type=value_error)</p>

          <p>gguf_init_from_file: invalid magic characters ''tjgg''<br>error loading
          model: llama_model_loader: failed to load model from /Users/christopherlozoya/Downloads/llama-2-7b-chat.ggmlv3.q8_0.bin</p>

          <p>llama_load_model_from_file: failed to load model<br>AVX = 0 | AVX_VNNI
          = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA =
          0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS
          = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |<br>Traceback (most recent call last):<br>  File
          "/Users/christopherlozoya/PycharmProjects/TestOllama/.venv/mainllama.py",
          line 26, in <br>    llm = load_model()<br>          ^^^^^^^^^^^^<br>  File
          "/Users/christopherlozoya/PycharmProjects/TestOllama/.venv/mainllama.py",
          line 15, in load_model<br>    Llama_model = LlamaCpp(<br>                  ^^^^^^^^^<br>  File
          "/Users/christopherlozoya/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py",
          line 107, in <strong>init</strong><br>    super().<strong>init</strong>(**kwargs)<br>  File
          "pydantic/main.py", line 341, in pydantic.main.BaseModel.<strong>init</strong><br>pydantic.error_wrappers.ValidationError:
          1 validation error for LlamaCpp<br><strong>root</strong><br>  Could not
          load Llama model from path: /Users/christopherlozoya/Downloads/llama-2-7b-chat.ggmlv3.q8_0.bin.
          Received error  (type=value_error)</p>

          '
        raw: "For some reason I can't load the llama model from the correct file path?\
          \ its in my downloads but says there is a (type=value_error)\r\n\r\ngguf_init_from_file:\
          \ invalid magic characters 'tjgg'\r\nerror loading model: llama_model_loader:\
          \ failed to load model from /Users/christopherlozoya/Downloads/llama-2-7b-chat.ggmlv3.q8_0.bin\r\
          \n\r\nllama_load_model_from_file: failed to load model\r\nAVX = 0 | AVX_VNNI\
          \ = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA\
          \ = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0\
          \ | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \r\nTraceback (most recent\
          \ call last):\r\n  File \"/Users/christopherlozoya/PycharmProjects/TestOllama/.venv/mainllama.py\"\
          , line 26, in <module>\r\n    llm = load_model()\r\n          ^^^^^^^^^^^^\r\
          \n  File \"/Users/christopherlozoya/PycharmProjects/TestOllama/.venv/mainllama.py\"\
          , line 15, in load_model\r\n    Llama_model = LlamaCpp(\r\n            \
          \      ^^^^^^^^^\r\n  File \"/Users/christopherlozoya/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py\"\
          , line 107, in __init__\r\n    super().__init__(**kwargs)\r\n  File \"pydantic/main.py\"\
          , line 341, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError:\
          \ 1 validation error for LlamaCpp\r\n__root__\r\n  Could not load Llama\
          \ model from path: /Users/christopherlozoya/Downloads/llama-2-7b-chat.ggmlv3.q8_0.bin.\
          \ Received error  (type=value_error)\r\n"
        updatedAt: '2024-01-05T21:00:54.442Z'
      numEdits: 0
      reactions: []
    id: 65986e0658608c40442e6455
    type: comment
  author: Lozzoya
  content: "For some reason I can't load the llama model from the correct file path?\
    \ its in my downloads but says there is a (type=value_error)\r\n\r\ngguf_init_from_file:\
    \ invalid magic characters 'tjgg'\r\nerror loading model: llama_model_loader:\
    \ failed to load model from /Users/christopherlozoya/Downloads/llama-2-7b-chat.ggmlv3.q8_0.bin\r\
    \n\r\nllama_load_model_from_file: failed to load model\r\nAVX = 0 | AVX_VNNI =\
    \ 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON\
    \ = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
    \ = 0 | SSSE3 = 0 | VSX = 0 | \r\nTraceback (most recent call last):\r\n  File\
    \ \"/Users/christopherlozoya/PycharmProjects/TestOllama/.venv/mainllama.py\",\
    \ line 26, in <module>\r\n    llm = load_model()\r\n          ^^^^^^^^^^^^\r\n\
    \  File \"/Users/christopherlozoya/PycharmProjects/TestOllama/.venv/mainllama.py\"\
    , line 15, in load_model\r\n    Llama_model = LlamaCpp(\r\n                  ^^^^^^^^^\r\
    \n  File \"/Users/christopherlozoya/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py\"\
    , line 107, in __init__\r\n    super().__init__(**kwargs)\r\n  File \"pydantic/main.py\"\
    , line 341, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError:\
    \ 1 validation error for LlamaCpp\r\n__root__\r\n  Could not load Llama model\
    \ from path: /Users/christopherlozoya/Downloads/llama-2-7b-chat.ggmlv3.q8_0.bin.\
    \ Received error  (type=value_error)\r\n"
  created_at: 2024-01-05 21:00:54+00:00
  edited: false
  hidden: false
  id: 65986e0658608c40442e6455
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85e1c0a9219151cdacfe97e6f1c12b41.svg
      fullname: Kamila
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alrightkami
      type: user
    createdAt: '2024-01-08T14:25:32.000Z'
    data:
      edited: false
      editors:
      - alrightkami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8132703304290771
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85e1c0a9219151cdacfe97e6f1c12b41.svg
          fullname: Kamila
          isHf: false
          isPro: false
          name: alrightkami
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Lozzoya&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Lozzoya\">@<span class=\"\
          underline\">Lozzoya</span></a></span>\n\n\t</span></span> This is due to\
          \ the recent update to GGUF-format. Older model formats such as ggmlv3 are\
          \ no longer supported starting August 2023.<br>You can find new ones in\
          \ GGUF-format here: <a href=\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\"\
          >https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF</a></p>\n"
        raw: "@Lozzoya This is due to the recent update to GGUF-format. Older model\
          \ formats such as ggmlv3 are no longer supported starting August 2023. \n\
          You can find new ones in GGUF-format here: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF"
        updatedAt: '2024-01-08T14:25:32.857Z'
      numEdits: 0
      reactions: []
    id: 659c05dc6b8d5d1940b1c7ce
    type: comment
  author: alrightkami
  content: "@Lozzoya This is due to the recent update to GGUF-format. Older model\
    \ formats such as ggmlv3 are no longer supported starting August 2023. \nYou can\
    \ find new ones in GGUF-format here: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF"
  created_at: 2024-01-08 14:25:32+00:00
  edited: false
  hidden: false
  id: 659c05dc6b8d5d1940b1c7ce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: Could not load Llama model from path
