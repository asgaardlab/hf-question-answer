!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chaltik
conflicting_files: null
created_at: 2023-07-24 15:22:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/O6Iz5SSJSBQI_X44yJEkN.jpeg?w=200&h=200&f=face
      fullname: Karen Chaltikian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chaltik
      type: user
    createdAt: '2023-07-24T16:22:47.000Z'
    data:
      edited: false
      editors:
      - chaltik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6030439734458923
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/O6Iz5SSJSBQI_X44yJEkN.jpeg?w=200&h=200&f=face
          fullname: Karen Chaltikian
          isHf: false
          isPro: false
          name: chaltik
          type: user
        html: '<p>I have downloaded the file <code>llama-2-7b-chat.ggmlv3.q4_K_S.bin</code>
          and placed it in the folder <code>../models/llama-2-7b-chat.ggmlv3.q4_K_S</code>.<br>Calling
          <code>AutoModel.from_pretrained(''../models/llama-2-7b-chat.ggmlv3.q4_K_S'')
          gives the error about not finding the file named </code>pytorch_model.bin`.<br>Upon
          renaming the .bin file to such name I get this error:</p>

          <pre><code>OSError: Unable to load weights from pytorch checkpoint file
          for ''../models/llama-2-7b-chat.ggmlv3.q4_K_S/pytorch_model.bin'' at ''../models/llama-2-7b-chat.ggmlv3.q4_K_S/pytorch_model.bin''.
          If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set
          from_tf=True.

          </code></pre>

          <p>what is the correct way to load the model from binaries?<br>thanks very
          much!</p>

          '
        raw: "I have downloaded the file `llama-2-7b-chat.ggmlv3.q4_K_S.bin` and placed\
          \ it in the folder `../models/llama-2-7b-chat.ggmlv3.q4_K_S`. \r\nCalling\
          \ `AutoModel.from_pretrained('../models/llama-2-7b-chat.ggmlv3.q4_K_S')\
          \ gives the error about not finding the file named `pytorch_model.bin`.\r\
          \nUpon renaming the .bin file to such name I get this error:\r\n```\r\n\
          OSError: Unable to load weights from pytorch checkpoint file for '../models/llama-2-7b-chat.ggmlv3.q4_K_S/pytorch_model.bin'\
          \ at '../models/llama-2-7b-chat.ggmlv3.q4_K_S/pytorch_model.bin'. If you\
          \ tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\
          \n```\r\nwhat is the correct way to load the model from binaries?\r\nthanks\
          \ very much!"
        updatedAt: '2023-07-24T16:22:47.624Z'
      numEdits: 0
      reactions: []
    id: 64bea5572e66dc7b8beba0a9
    type: comment
  author: chaltik
  content: "I have downloaded the file `llama-2-7b-chat.ggmlv3.q4_K_S.bin` and placed\
    \ it in the folder `../models/llama-2-7b-chat.ggmlv3.q4_K_S`. \r\nCalling `AutoModel.from_pretrained('../models/llama-2-7b-chat.ggmlv3.q4_K_S')\
    \ gives the error about not finding the file named `pytorch_model.bin`.\r\nUpon\
    \ renaming the .bin file to such name I get this error:\r\n```\r\nOSError: Unable\
    \ to load weights from pytorch checkpoint file for '../models/llama-2-7b-chat.ggmlv3.q4_K_S/pytorch_model.bin'\
    \ at '../models/llama-2-7b-chat.ggmlv3.q4_K_S/pytorch_model.bin'. If you tried\
    \ to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\
    \n```\r\nwhat is the correct way to load the model from binaries?\r\nthanks very\
    \ much!"
  created_at: 2023-07-24 15:22:47+00:00
  edited: false
  hidden: false
  id: 64bea5572e66dc7b8beba0a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13bb6b205ed6ada66fc4312ed2d5117f.svg
      fullname: Daniel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spectral9
      type: user
    createdAt: '2023-07-26T05:35:27.000Z'
    data:
      edited: true
      editors:
      - spectral9
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4924508035182953
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13bb6b205ed6ada66fc4312ed2d5117f.svg
          fullname: Daniel
          isHf: false
          isPro: false
          name: spectral9
          type: user
        html: '<p>You can load up the model by just referencing the directory on GGML
          models using <a rel="nofollow" href="https://github.com/marella/ctransformers">c
          transformers</a>.</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          ctransformers <span class="hljs-keyword">import</span> AutoModelForCausalLM


          llm = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">''models/''</span>,
          model_type=<span class="hljs-string">''gpt2''</span>)


          <span class="hljs-built_in">print</span>(llm(<span class="hljs-string">''AI
          is going to''</span>))

          </code></pre>

          '
        raw: 'You can load up the model by just referencing the directory on GGML
          models using [c transformers](https://github.com/marella/ctransformers).


          ```python

          from ctransformers import AutoModelForCausalLM


          llm = AutoModelForCausalLM.from_pretrained(''models/'', model_type=''gpt2'')


          print(llm(''AI is going to''))

          ```'
        updatedAt: '2023-07-26T05:42:36.895Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - chaltik
        - eric3232332
    id: 64c0b09f672505119df2631b
    type: comment
  author: spectral9
  content: 'You can load up the model by just referencing the directory on GGML models
    using [c transformers](https://github.com/marella/ctransformers).


    ```python

    from ctransformers import AutoModelForCausalLM


    llm = AutoModelForCausalLM.from_pretrained(''models/'', model_type=''gpt2'')


    print(llm(''AI is going to''))

    ```'
  created_at: 2023-07-26 04:35:27+00:00
  edited: true
  hidden: false
  id: 64c0b09f672505119df2631b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2cb6ed38962a5c023c949b1e5b0f4e12.svg
      fullname: Zheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhh210
      type: user
    createdAt: '2023-07-28T04:35:40.000Z'
    data:
      edited: false
      editors:
      - zhh210
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24389788508415222
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2cb6ed38962a5c023c949b1e5b0f4e12.svg
          fullname: Zheng
          isHf: false
          isPro: false
          name: zhh210
          type: user
        html: "<p>Loading directly from huggingface doesn't seem to work either. The\
          \ mysterious error keeps suggesting using <code>from_tf=True</code> even\
          \ when I have already used it there:</p>\n<pre><code>from transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\n\nmodel_id=\"TheBloke/Llama-2-7B-Chat-GGML\"\
          .lower()\n\nmodel =AutoModelForCausalLM.from_pretrained(model_id, from_tf=True)\n\
          </code></pre>\n<pre><code>\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \ Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502 in &lt;module&gt;:5                         \
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502   2                        \
          \                                                                      \u2502\
          \n\u2502   3 model_id=\"TheBloke/Llama-2-7B-Chat-GGML\".lower()        \
          \                                     \u2502\n\u2502   4               \
          \                                                                      \
          \         \u2502\n\u2502 \u2771 5 model =AutoModelForCausalLM.from_pretrained(model_id,\
          \ from_tf=True)                          \u2502\n\u2502   6            \
          \                                                                      \
          \            \u2502\n\u2502                                            \
          \                                                      \u2502\n\u2502 /home/ec2-user/SageMaker/envs/py310/lib/python3.10/site-packages/transformers/models/auto/auto_f\
          \ \u2502\n\u2502 actory.py:493 in from_pretrained                      \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   490 \u2502   \u2502   \u2502   )      \
          \                                                                      \
          \  \u2502\n\u2502   491 \u2502   \u2502   elif type(config) in cls._model_mapping.keys():\
          \                                    \u2502\n\u2502   492 \u2502   \u2502\
          \   \u2502   model_class = _get_model_class(config, cls._model_mapping)\
          \                     \u2502\n\u2502 \u2771 493 \u2502   \u2502   \u2502\
          \   return model_class.from_pretrained(                                \
          \            \u2502\n\u2502   494 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs,   \u2502\n\u2502   495 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502\n\u2502   496 \u2502   \u2502   raise\
          \ ValueError(                                                          \
          \        \u2502\n\u2502                                                \
          \                                                  \u2502\n\u2502 /home/ec2-user/SageMaker/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:\
          \ \u2502\n\u2502 2560 in from_pretrained                               \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   2557 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \"use_auth_token\": token,              \
          \                        \u2502\n\u2502   2558 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   }                                       \
          \                          \u2502\n\u2502   2559 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   if has_file(pretrained_model_name_or_path,\
          \ TF2_WEIGHTS_NAME, **h  \u2502\n\u2502 \u2771 2560 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   raise EnvironmentError(        \
          \                               \u2502\n\u2502   2561 \u2502   \u2502  \
          \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   f\"{pretrained_model_name_or_path}\
          \ does not appear to hav  \u2502\n\u2502   2562 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   f\" {_add_variant(WEIGHTS_NAME,\
          \ variant)} but there is a   \u2502\n\u2502   2563 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \" Use `from_tf=True` to\
          \ load this model from those weigh  \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nOSError:\
          \ thebloke/llama-2-7b-chat-ggml does not appear to have a file named pytorch_model.bin\
          \ but there is a file \nfor TensorFlow weights. Use `from_tf=True` to load\
          \ this model from those weights.\n</code></pre>\n"
        raw: "Loading directly from huggingface doesn't seem to work either. The mysterious\
          \ error keeps suggesting using `from_tf=True` even when I have already used\
          \ it there:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \nmodel_id=\"TheBloke/Llama-2-7B-Chat-GGML\".lower()\n\nmodel =AutoModelForCausalLM.from_pretrained(model_id,\
          \ from_tf=True)\n```\n\n```\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \ Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502 in <module>:5                               \
          \                                                     \u2502\n\u2502   \
          \                                                                      \
          \                         \u2502\n\u2502   2                           \
          \                                                                   \u2502\
          \n\u2502   3 model_id=\"TheBloke/Llama-2-7B-Chat-GGML\".lower()        \
          \                                     \u2502\n\u2502   4               \
          \                                                                      \
          \         \u2502\n\u2502 \u2771 5 model =AutoModelForCausalLM.from_pretrained(model_id,\
          \ from_tf=True)                          \u2502\n\u2502   6            \
          \                                                                      \
          \            \u2502\n\u2502                                            \
          \                                                      \u2502\n\u2502 /home/ec2-user/SageMaker/envs/py310/lib/python3.10/site-packages/transformers/models/auto/auto_f\
          \ \u2502\n\u2502 actory.py:493 in from_pretrained                      \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   490 \u2502   \u2502   \u2502   )      \
          \                                                                      \
          \  \u2502\n\u2502   491 \u2502   \u2502   elif type(config) in cls._model_mapping.keys():\
          \                                    \u2502\n\u2502   492 \u2502   \u2502\
          \   \u2502   model_class = _get_model_class(config, cls._model_mapping)\
          \                     \u2502\n\u2502 \u2771 493 \u2502   \u2502   \u2502\
          \   return model_class.from_pretrained(                                \
          \            \u2502\n\u2502   494 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs,   \u2502\n\u2502   495 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502\n\u2502   496 \u2502   \u2502   raise\
          \ ValueError(                                                          \
          \        \u2502\n\u2502                                                \
          \                                                  \u2502\n\u2502 /home/ec2-user/SageMaker/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:\
          \ \u2502\n\u2502 2560 in from_pretrained                               \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   2557 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \"use_auth_token\": token,              \
          \                        \u2502\n\u2502   2558 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   }                                       \
          \                          \u2502\n\u2502   2559 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   if has_file(pretrained_model_name_or_path,\
          \ TF2_WEIGHTS_NAME, **h  \u2502\n\u2502 \u2771 2560 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   raise EnvironmentError(        \
          \                               \u2502\n\u2502   2561 \u2502   \u2502  \
          \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   f\"{pretrained_model_name_or_path}\
          \ does not appear to hav  \u2502\n\u2502   2562 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   f\" {_add_variant(WEIGHTS_NAME,\
          \ variant)} but there is a   \u2502\n\u2502   2563 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \" Use `from_tf=True` to\
          \ load this model from those weigh  \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nOSError:\
          \ thebloke/llama-2-7b-chat-ggml does not appear to have a file named pytorch_model.bin\
          \ but there is a file \nfor TensorFlow weights. Use `from_tf=True` to load\
          \ this model from those weights.\n```"
        updatedAt: '2023-07-28T04:35:40.392Z'
      numEdits: 0
      reactions: []
    id: 64c3459cf27ff500c4cf2069
    type: comment
  author: zhh210
  content: "Loading directly from huggingface doesn't seem to work either. The mysterious\
    \ error keeps suggesting using `from_tf=True` even when I have already used it\
    \ there:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\
    model_id=\"TheBloke/Llama-2-7B-Chat-GGML\".lower()\n\nmodel =AutoModelForCausalLM.from_pretrained(model_id,\
    \ from_tf=True)\n```\n\n```\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
    \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 in <module>:5 \
    \                                                                            \
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502   2               \
    \                                                                            \
    \   \u2502\n\u2502   3 model_id=\"TheBloke/Llama-2-7B-Chat-GGML\".lower()    \
    \                                         \u2502\n\u2502   4                 \
    \                                                                            \
    \ \u2502\n\u2502 \u2771 5 model =AutoModelForCausalLM.from_pretrained(model_id,\
    \ from_tf=True)                          \u2502\n\u2502   6                  \
    \                                                                            \u2502\
    \n\u2502                                                                     \
    \                             \u2502\n\u2502 /home/ec2-user/SageMaker/envs/py310/lib/python3.10/site-packages/transformers/models/auto/auto_f\
    \ \u2502\n\u2502 actory.py:493 in from_pretrained                            \
    \                                     \u2502\n\u2502                         \
    \                                                                         \u2502\
    \n\u2502   490 \u2502   \u2502   \u2502   )                                  \
    \                                            \u2502\n\u2502   491 \u2502   \u2502\
    \   elif type(config) in cls._model_mapping.keys():                          \
    \          \u2502\n\u2502   492 \u2502   \u2502   \u2502   model_class = _get_model_class(config,\
    \ cls._model_mapping)                     \u2502\n\u2502 \u2771 493 \u2502   \u2502\
    \   \u2502   return model_class.from_pretrained(                             \
    \               \u2502\n\u2502   494 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs,   \u2502\n\u2502   495 \u2502   \u2502\
    \   \u2502   )                                                               \
    \               \u2502\n\u2502   496 \u2502   \u2502   raise ValueError(     \
    \                                                             \u2502\n\u2502 \
    \                                                                            \
    \                     \u2502\n\u2502 /home/ec2-user/SageMaker/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:\
    \ \u2502\n\u2502 2560 in from_pretrained                                     \
    \                                     \u2502\n\u2502                         \
    \                                                                         \u2502\
    \n\u2502   2557 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502 \
    \  \"use_auth_token\": token,                                      \u2502\n\u2502\
    \   2558 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   }             \
    \                                                    \u2502\n\u2502   2559 \u2502\
    \   \u2502   \u2502   \u2502   \u2502   \u2502   if has_file(pretrained_model_name_or_path,\
    \ TF2_WEIGHTS_NAME, **h  \u2502\n\u2502 \u2771 2560 \u2502   \u2502   \u2502 \
    \  \u2502   \u2502   \u2502   \u2502   raise EnvironmentError(               \
    \                        \u2502\n\u2502   2561 \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \u2502   f\"{pretrained_model_name_or_path} does\
    \ not appear to hav  \u2502\n\u2502   2562 \u2502   \u2502   \u2502   \u2502 \
    \  \u2502   \u2502   \u2502   \u2502   f\" {_add_variant(WEIGHTS_NAME, variant)}\
    \ but there is a   \u2502\n\u2502   2563 \u2502   \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \" Use `from_tf=True` to load this model from those\
    \ weigh  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nOSError: thebloke/llama-2-7b-chat-ggml\
    \ does not appear to have a file named pytorch_model.bin but there is a file \n\
    for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\
    ```"
  created_at: 2023-07-28 03:35:40+00:00
  edited: false
  hidden: false
  id: 64c3459cf27ff500c4cf2069
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/O6Iz5SSJSBQI_X44yJEkN.jpeg?w=200&h=200&f=face
      fullname: Karen Chaltikian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chaltik
      type: user
    createdAt: '2023-07-28T06:31:56.000Z'
    data:
      edited: false
      editors:
      - chaltik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7788880467414856
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/O6Iz5SSJSBQI_X44yJEkN.jpeg?w=200&h=200&f=face
          fullname: Karen Chaltikian
          isHf: false
          isPro: false
          name: chaltik
          type: user
        html: '<p>Thanks!</p>

          '
        raw: Thanks!
        updatedAt: '2023-07-28T06:31:56.811Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c360dc7918ee895b9e8983
    id: 64c360dc7918ee895b9e8981
    type: comment
  author: chaltik
  content: Thanks!
  created_at: 2023-07-28 05:31:56+00:00
  edited: false
  hidden: false
  id: 64c360dc7918ee895b9e8981
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/O6Iz5SSJSBQI_X44yJEkN.jpeg?w=200&h=200&f=face
      fullname: Karen Chaltikian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chaltik
      type: user
    createdAt: '2023-07-28T06:31:56.000Z'
    data:
      status: closed
    id: 64c360dc7918ee895b9e8983
    type: status-change
  author: chaltik
  created_at: 2023-07-28 05:31:56+00:00
  id: 64c360dc7918ee895b9e8983
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: closed
target_branch: null
title: 'Newbie question on local model loading '
