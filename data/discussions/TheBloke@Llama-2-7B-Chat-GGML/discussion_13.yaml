!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zbruceli
conflicting_files: null
created_at: 2023-07-28 17:18:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-07-28T18:18:51.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8033155202865601
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>Hi, I have fine-tuned a LLaMA-2 7B model using the Philip Schmid
          tutorial (<a rel="nofollow" href="https://www.philschmid.de/instruction-tune-llama-2">https://www.philschmid.de/instruction-tune-llama-2</a>)
          and have merged the LoRa weights back into the original weights. Now how
          can I further convert the weights into GGML format and 4 bit quantization,
          so I can run in llama.cpp?</p>

          <p>These are the files in my merged model:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6425c5e2b1eab27cc28b6f3e/35ESqeFqlbd6yOHGwbfhA.png"><img
          alt="Screenshot 2023-07-28 at 11.17.41 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/6425c5e2b1eab27cc28b6f3e/35ESqeFqlbd6yOHGwbfhA.png"></a><br>Thanks!</p>

          '
        raw: "Hi, I have fine-tuned a LLaMA-2 7B model using the Philip Schmid tutorial\
          \ (https://www.philschmid.de/instruction-tune-llama-2) and have merged the\
          \ LoRa weights back into the original weights. Now how can I further convert\
          \ the weights into GGML format and 4 bit quantization, so I can run in llama.cpp?\r\
          \n\r\nThese are the files in my merged model:\r\n\r\n![Screenshot 2023-07-28\
          \ at 11.17.41 AM.png](https://cdn-uploads.huggingface.co/production/uploads/6425c5e2b1eab27cc28b6f3e/35ESqeFqlbd6yOHGwbfhA.png)\r\
          \nThanks!"
        updatedAt: '2023-07-28T18:18:51.762Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
        - jingwora
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - polykelvin
    id: 64c4068b47418a0a599567cd
    type: comment
  author: zbruceli
  content: "Hi, I have fine-tuned a LLaMA-2 7B model using the Philip Schmid tutorial\
    \ (https://www.philschmid.de/instruction-tune-llama-2) and have merged the LoRa\
    \ weights back into the original weights. Now how can I further convert the weights\
    \ into GGML format and 4 bit quantization, so I can run in llama.cpp?\r\n\r\n\
    These are the files in my merged model:\r\n\r\n![Screenshot 2023-07-28 at 11.17.41\
    \ AM.png](https://cdn-uploads.huggingface.co/production/uploads/6425c5e2b1eab27cc28b6f3e/35ESqeFqlbd6yOHGwbfhA.png)\r\
    \nThanks!"
  created_at: 2023-07-28 17:18:51+00:00
  edited: false
  hidden: false
  id: 64c4068b47418a0a599567cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-09T15:42:25.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8180792927742004
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></p>

          <p>ctrl+f for ''convert'' and you''ll find some script</p>

          '
        raw: 'https://github.com/ggerganov/llama.cpp


          ctrl+f for ''convert'' and you''ll find some script'
        updatedAt: '2023-08-09T15:42:25.755Z'
      numEdits: 0
      reactions: []
    id: 64d3b3e1e7699aac693b0f4f
    type: comment
  author: RonanMcGovern
  content: 'https://github.com/ggerganov/llama.cpp


    ctrl+f for ''convert'' and you''ll find some script'
  created_at: 2023-08-09 14:42:25+00:00
  edited: false
  hidden: false
  id: 64d3b3e1e7699aac693b0f4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-09T17:46:08.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.775484025478363
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Ok, I've dug in more on this and it's tricky...</p>\n<ol>\n<li><p>I\
          \ don't know what the format of the input model to convert.py needs to be.\
          \ float 32 or bf16? See this new <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/issues/2571\"\
          >issue</a></p>\n</li>\n<li><p>A lot of models on huggingface have shards\
          \ of 10GB max. I don't know how to handle shards with the convert.py script.</p>\n\
          </li>\n</ol>\n<p>I tried to do it with bf16 bin files, where I concatenated\
          \ the shards, but I ran into a key error (running in colab):</p>\n<pre><code>!python3\
          \ convert.py ../models/\nLoading...\n\nLoading model file ../models/pytorch_model.bin\n\
          vocabtype: spm\nLoading vocab file ../models/tokenizer.model\nparams: n_vocab:32000\
          \ n_embd:4096 n_mult:5504 n_head:32 n_layer:32\nTraceback (most recent call\
          \ last):\n  File \"/content/llama.cpp/convert.py\", line 1326, in &lt;module&gt;\n\
          \    main()\n  File \"/content/llama.cpp/convert.py\", line 1317, in main\n\
          \    model = do_necessary_conversions(model, params)\n  File \"/content/llama.cpp/convert.py\"\
          , line 1146, in do_necessary_conversions\n    model = convert_transformers_to_orig(model,\
          \ params)\n  File \"/content/llama.cpp/convert.py\", line 737, in convert_transformers_to_orig\n\
          \    out[\"tok_embeddings.weight\"] = model[\"model.embed_tokens.weight\"\
          ]\nKeyError: 'model.embed_tokens.weight'\n</code></pre>\n<p>After solving\
          \ that, I would have to see if the following works in a colab notebook:</p>\n\
          <pre><code>!./quantize ./models/ggml-model-f16.bin ./models/ggml-model-q3_K_M.bin\
          \ q3_K_M\n</code></pre>\n"
        raw: "Ok, I've dug in more on this and it's tricky...\n\n1. I don't know what\
          \ the format of the input model to convert.py needs to be. float 32 or bf16?\
          \ See this new [issue](https://github.com/ggerganov/llama.cpp/issues/2571)\n\
          \n2. A lot of models on huggingface have shards of 10GB max. I don't know\
          \ how to handle shards with the convert.py script.\n\nI tried to do it with\
          \ bf16 bin files, where I concatenated the shards, but I ran into a key\
          \ error (running in colab):\n```\n!python3 convert.py ../models/\nLoading...\n\
          \nLoading model file ../models/pytorch_model.bin\nvocabtype: spm\nLoading\
          \ vocab file ../models/tokenizer.model\nparams: n_vocab:32000 n_embd:4096\
          \ n_mult:5504 n_head:32 n_layer:32\nTraceback (most recent call last):\n\
          \  File \"/content/llama.cpp/convert.py\", line 1326, in <module>\n    main()\n\
          \  File \"/content/llama.cpp/convert.py\", line 1317, in main\n    model\
          \ = do_necessary_conversions(model, params)\n  File \"/content/llama.cpp/convert.py\"\
          , line 1146, in do_necessary_conversions\n    model = convert_transformers_to_orig(model,\
          \ params)\n  File \"/content/llama.cpp/convert.py\", line 737, in convert_transformers_to_orig\n\
          \    out[\"tok_embeddings.weight\"] = model[\"model.embed_tokens.weight\"\
          ]\nKeyError: 'model.embed_tokens.weight'\n```\n\nAfter solving that, I would\
          \ have to see if the following works in a colab notebook:\n```\n!./quantize\
          \ ./models/ggml-model-f16.bin ./models/ggml-model-q3_K_M.bin q3_K_M\n```"
        updatedAt: '2023-08-09T17:46:08.562Z'
      numEdits: 0
      reactions: []
    id: 64d3d0e0778b6c5cfd7500bd
    type: comment
  author: RonanMcGovern
  content: "Ok, I've dug in more on this and it's tricky...\n\n1. I don't know what\
    \ the format of the input model to convert.py needs to be. float 32 or bf16? See\
    \ this new [issue](https://github.com/ggerganov/llama.cpp/issues/2571)\n\n2. A\
    \ lot of models on huggingface have shards of 10GB max. I don't know how to handle\
    \ shards with the convert.py script.\n\nI tried to do it with bf16 bin files,\
    \ where I concatenated the shards, but I ran into a key error (running in colab):\n\
    ```\n!python3 convert.py ../models/\nLoading...\n\nLoading model file ../models/pytorch_model.bin\n\
    vocabtype: spm\nLoading vocab file ../models/tokenizer.model\nparams: n_vocab:32000\
    \ n_embd:4096 n_mult:5504 n_head:32 n_layer:32\nTraceback (most recent call last):\n\
    \  File \"/content/llama.cpp/convert.py\", line 1326, in <module>\n    main()\n\
    \  File \"/content/llama.cpp/convert.py\", line 1317, in main\n    model = do_necessary_conversions(model,\
    \ params)\n  File \"/content/llama.cpp/convert.py\", line 1146, in do_necessary_conversions\n\
    \    model = convert_transformers_to_orig(model, params)\n  File \"/content/llama.cpp/convert.py\"\
    , line 737, in convert_transformers_to_orig\n    out[\"tok_embeddings.weight\"\
    ] = model[\"model.embed_tokens.weight\"]\nKeyError: 'model.embed_tokens.weight'\n\
    ```\n\nAfter solving that, I would have to see if the following works in a colab\
    \ notebook:\n```\n!./quantize ./models/ggml-model-f16.bin ./models/ggml-model-q3_K_M.bin\
    \ q3_K_M\n```"
  created_at: 2023-08-09 16:46:08+00:00
  edited: false
  hidden: false
  id: 64d3d0e0778b6c5cfd7500bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-09T23:40:04.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.571571409702301
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>ok, issue resolved here:</p>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/2571">https://github.com/ggerganov/llama.cpp/issues/2571</a></p>

          <p>some code snippets here:</p>

          <pre><code>python3 convert.py ./ --outtype f16

          </code></pre>

          <p>and </p>

          <pre><code>./quantize ./ggml-model-f16.bin ./ggml-model-q3_K_M.bin q3_K_M

          </code></pre>

          '
        raw: "ok, issue resolved here:\n\nhttps://github.com/ggerganov/llama.cpp/issues/2571\n\
          \nsome code snippets here:\n```\npython3 convert.py ./ --outtype f16\n```\n\
          \nand \n```\n./quantize ./ggml-model-f16.bin ./ggml-model-q3_K_M.bin q3_K_M\n\
          ```"
        updatedAt: '2023-08-09T23:40:04.398Z'
      numEdits: 0
      reactions: []
    id: 64d423d4a9485f7adeeace2e
    type: comment
  author: RonanMcGovern
  content: "ok, issue resolved here:\n\nhttps://github.com/ggerganov/llama.cpp/issues/2571\n\
    \nsome code snippets here:\n```\npython3 convert.py ./ --outtype f16\n```\n\n\
    and \n```\n./quantize ./ggml-model-f16.bin ./ggml-model-q3_K_M.bin q3_K_M\n```"
  created_at: 2023-08-09 22:40:04+00:00
  edited: false
  hidden: false
  id: 64d423d4a9485f7adeeace2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-08-10T05:34:01.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5250610709190369
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'Thank you @RonanMcGovern '
        updatedAt: '2023-08-10T05:34:01.413Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
        - GenAI-Warrior
    id: 64d476c9256b2c3d857101ba
    type: comment
  author: zbruceli
  content: 'Thank you @RonanMcGovern '
  created_at: 2023-08-10 04:34:01+00:00
  edited: false
  hidden: false
  id: 64d476c9256b2c3d857101ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T09:24:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9756362438201904
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There''s a script included with llama.cpp that does everything for
          you. It''s called <code>make-ggml.py</code>. It''s based off an old Python
          script I used to produce my GGML models with.</p>

          '
        raw: There's a script included with llama.cpp that does everything for you.
          It's called `make-ggml.py`. It's based off an old Python script I used to
          produce my GGML models with.
        updatedAt: '2023-08-10T09:24:22.638Z'
      numEdits: 0
      reactions:
      - count: 8
        reaction: "\u2764\uFE0F"
        users:
        - RonanMcGovern
        - mikeee
        - elriel
        - polykelvin
        - PsiPi
        - shuver
        - Tatvajsh
        - sugatoray
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Zohaib28
        - pe65374
        - Tatvajsh
    id: 64d4acc6fecbf3520408f1e2
    type: comment
  author: TheBloke
  content: There's a script included with llama.cpp that does everything for you.
    It's called `make-ggml.py`. It's based off an old Python script I used to produce
    my GGML models with.
  created_at: 2023-08-10 08:24:22+00:00
  edited: false
  hidden: false
  id: 64d4acc6fecbf3520408f1e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-10T13:38:41.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7972071766853333
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>btw, why does ggml quantization require a tokenizer? Does the tokenizer
          end up influencing the way the quantization occurs?</p>

          '
        raw: btw, why does ggml quantization require a tokenizer? Does the tokenizer
          end up influencing the way the quantization occurs?
        updatedAt: '2023-08-10T13:38:41.372Z'
      numEdits: 0
      reactions: []
    id: 64d4e8618767727dff8897bd
    type: comment
  author: RonanMcGovern
  content: btw, why does ggml quantization require a tokenizer? Does the tokenizer
    end up influencing the way the quantization occurs?
  created_at: 2023-08-10 12:38:41+00:00
  edited: false
  hidden: false
  id: 64d4e8618767727dff8897bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
      fullname: Aaron Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liaaron1
      type: user
    createdAt: '2023-08-26T16:37:31.000Z'
    data:
      edited: false
      editors:
      - liaaron1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4732262194156647
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
          fullname: Aaron Li
          isHf: false
          isPro: false
          name: liaaron1
          type: user
        html: "<blockquote>\n<p>ok, issue resolved here:</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://github.com/ggerganov/llama.cpp/issues/2571\">https://github.com/ggerganov/llama.cpp/issues/2571</a></p>\n\
          <p>some code snippets here:</p>\n<pre><code>python3 convert.py ./ --outtype\
          \ f16\n</code></pre>\n<p>and </p>\n<pre><code>./quantize ./ggml-model-f16.bin\
          \ ./ggml-model-q3_K_M.bin q3_K_M\n</code></pre>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span\
          \ class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span><br>Thanks\
          \ a lot for the sharing. Do you know why the conver.py script doesn't recognize\
          \ the pytorch model bin file here?<br>It stopped at processing the 1st of\
          \ 7 bin model files.</p>\n<p>(lab) aaron@LIs-MacBook-Pro llama2 % python\
          \ llama.cpp/convert.py llama-2-7b-liaaron1 --outtype f16<br>Loading model\
          \ file llama-2-7b-liaaron1/pytorch_model-00001-of-00007.bin<br>Traceback\
          \ (most recent call last):<br>  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 1112, in <br>    main()<br>  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 1061, in main<br>    model_plus = load_some_model(args.model)<br>\
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 985, in load_some_model<br>    models_plus.append(lazy_load_file(path))<br>\
          \                       ^^^^^^^^^^^^^^^^^^^^<br>  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 720, in lazy_load_file<br>    raise ValueError(f\"unknown format:\
          \ {path}\")<br>ValueError: unknown format: llama-2-7b-liaaron1/pytorch_model-00001-of-00007.bin</p>\n\
          <p>Appreciate your help</p>\n<p>Aaron</p>\n"
        raw: "> ok, issue resolved here:\n> \n> https://github.com/ggerganov/llama.cpp/issues/2571\n\
          > \n> some code snippets here:\n> ```\n> python3 convert.py ./ --outtype\
          \ f16\n> ```\n> \n> and \n> ```\n> ./quantize ./ggml-model-f16.bin ./ggml-model-q3_K_M.bin\
          \ q3_K_M\n> ```\n\n@RonanMcGovern \nThanks a lot for the sharing. Do you\
          \ know why the conver.py script doesn't recognize the pytorch model bin\
          \ file here?\nIt stopped at processing the 1st of 7 bin model files.\n\n\
          (lab) aaron@LIs-MacBook-Pro llama2 % python llama.cpp/convert.py llama-2-7b-liaaron1\
          \ --outtype f16\nLoading model file llama-2-7b-liaaron1/pytorch_model-00001-of-00007.bin\n\
          Traceback (most recent call last):\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 1112, in <module>\n    main()\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 1061, in main\n    model_plus = load_some_model(args.model)\n   \
          \              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 985, in load_some_model\n    models_plus.append(lazy_load_file(path))\n\
          \                       ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
          , line 720, in lazy_load_file\n    raise ValueError(f\"unknown format: {path}\"\
          )\nValueError: unknown format: llama-2-7b-liaaron1/pytorch_model-00001-of-00007.bin\n\
          \nAppreciate your help\n\nAaron"
        updatedAt: '2023-08-26T16:37:31.545Z'
      numEdits: 0
      reactions: []
    id: 64ea2a4b8b59aa317e437922
    type: comment
  author: liaaron1
  content: "> ok, issue resolved here:\n> \n> https://github.com/ggerganov/llama.cpp/issues/2571\n\
    > \n> some code snippets here:\n> ```\n> python3 convert.py ./ --outtype f16\n\
    > ```\n> \n> and \n> ```\n> ./quantize ./ggml-model-f16.bin ./ggml-model-q3_K_M.bin\
    \ q3_K_M\n> ```\n\n@RonanMcGovern \nThanks a lot for the sharing. Do you know\
    \ why the conver.py script doesn't recognize the pytorch model bin file here?\n\
    It stopped at processing the 1st of 7 bin model files.\n\n(lab) aaron@LIs-MacBook-Pro\
    \ llama2 % python llama.cpp/convert.py llama-2-7b-liaaron1 --outtype f16\nLoading\
    \ model file llama-2-7b-liaaron1/pytorch_model-00001-of-00007.bin\nTraceback (most\
    \ recent call last):\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
    , line 1112, in <module>\n    main()\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
    , line 1061, in main\n    model_plus = load_some_model(args.model)\n         \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
    , line 985, in load_some_model\n    models_plus.append(lazy_load_file(path))\n\
    \                       ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aaron/Downloads/llama2/llama.cpp/convert.py\"\
    , line 720, in lazy_load_file\n    raise ValueError(f\"unknown format: {path}\"\
    )\nValueError: unknown format: llama-2-7b-liaaron1/pytorch_model-00001-of-00007.bin\n\
    \nAppreciate your help\n\nAaron"
  created_at: 2023-08-26 15:37:31+00:00
  edited: false
  hidden: false
  id: 64ea2a4b8b59aa317e437922
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
      fullname: Akarshan Biswas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akarshanbiswas
      type: user
    createdAt: '2023-08-26T16:51:16.000Z'
    data:
      edited: false
      editors:
      - akarshanbiswas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9477899074554443
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
          fullname: Akarshan Biswas
          isHf: false
          isPro: false
          name: akarshanbiswas
          type: user
        html: '<p>I have a colab notebook that I used to quantize LLaMA 2  13B chat
          model to gguf (available in my repo). </p>

          <p>I didn''t do Q3 unfortunately. If you want I can share the notebook.
          </p>

          <p>You have to just replace the existing model with appropriate model you
          want to quantize.</p>

          '
        raw: "I have a colab notebook that I used to quantize LLaMA 2  13B chat model\
          \ to gguf (available in my repo). \n\nI didn't do Q3 unfortunately. If you\
          \ want I can share the notebook. \n\nYou have to just replace the existing\
          \ model with appropriate model you want to quantize."
        updatedAt: '2023-08-26T16:51:16.382Z'
      numEdits: 0
      reactions: []
    id: 64ea2d8418d79efd53377745
    type: comment
  author: akarshanbiswas
  content: "I have a colab notebook that I used to quantize LLaMA 2  13B chat model\
    \ to gguf (available in my repo). \n\nI didn't do Q3 unfortunately. If you want\
    \ I can share the notebook. \n\nYou have to just replace the existing model with\
    \ appropriate model you want to quantize."
  created_at: 2023-08-26 15:51:16+00:00
  edited: false
  hidden: false
  id: 64ea2d8418d79efd53377745
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-26T17:41:02.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609937071800232
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;liaaron1&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/liaaron1\">@<span class=\"\
          underline\">liaaron1</span></a></span>\n\n\t</span></span> , there's nothing\
          \ obviously wrong to me, but it may be worth putting the .bin files into\
          \ the same folder as the script so you can run the exact command. Another\
          \ debug option I would try is to just the raw llama files as a test.</p>\n"
        raw: '@liaaron1 , there''s nothing obviously wrong to me, but it may be worth
          putting the .bin files into the same folder as the script so you can run
          the exact command. Another debug option I would try is to just the raw llama
          files as a test.'
        updatedAt: '2023-08-26T17:41:02.810Z'
      numEdits: 0
      reactions: []
    id: 64ea392e92d9db9a937f8c5c
    type: comment
  author: RonanMcGovern
  content: '@liaaron1 , there''s nothing obviously wrong to me, but it may be worth
    putting the .bin files into the same folder as the script so you can run the exact
    command. Another debug option I would try is to just the raw llama files as a
    test.'
  created_at: 2023-08-26 16:41:02+00:00
  edited: false
  hidden: false
  id: 64ea392e92d9db9a937f8c5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671660712992-noauth.jpeg?w=200&h=200&f=face
      fullname: Bhavneek Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blazingbhavneek
      type: user
    createdAt: '2023-08-26T19:23:15.000Z'
    data:
      edited: false
      editors:
      - blazingbhavneek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8041390776634216
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671660712992-noauth.jpeg?w=200&h=200&f=face
          fullname: Bhavneek Singh
          isHf: false
          isPro: false
          name: blazingbhavneek
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akarshanbiswas&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/akarshanbiswas\"\
          >@<span class=\"underline\">akarshanbiswas</span></a></span>\n\n\t</span></span>\
          \ Please do share!</p>\n"
        raw: '@akarshanbiswas Please do share!

          '
        updatedAt: '2023-08-26T19:23:15.423Z'
      numEdits: 0
      reactions: []
    id: 64ea5123f72e054cab4512db
    type: comment
  author: blazingbhavneek
  content: '@akarshanbiswas Please do share!

    '
  created_at: 2023-08-26 18:23:15+00:00
  edited: false
  hidden: false
  id: 64ea5123f72e054cab4512db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
      fullname: Aaron Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liaaron1
      type: user
    createdAt: '2023-08-27T02:16:29.000Z'
    data:
      edited: false
      editors:
      - liaaron1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9593732357025146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
          fullname: Aaron Li
          isHf: false
          isPro: false
          name: liaaron1
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;liaaron1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/liaaron1\"\
          >@<span class=\"underline\">liaaron1</span></a></span>\n\n\t</span></span>\
          \ , there's nothing obviously wrong to me, but it may be worth putting the\
          \ .bin files into the same folder as the script so you can run the exact\
          \ command. Another debug option I would try is to just the raw llama files\
          \ as a test.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ Got the same error after having moved the scripts into the same folder\
          \ as the .bin files. It seemed the scripts expecting .pt files instead?\
          \ Which format worked for you?</p>\n"
        raw: '> @liaaron1 , there''s nothing obviously wrong to me, but it may be
          worth putting the .bin files into the same folder as the script so you can
          run the exact command. Another debug option I would try is to just the raw
          llama files as a test.


          @RonanMcGovern Got the same error after having moved the scripts into the
          same folder as the .bin files. It seemed the scripts expecting .pt files
          instead? Which format worked for you?'
        updatedAt: '2023-08-27T02:16:29.845Z'
      numEdits: 0
      reactions: []
    id: 64eab1fdfb77a3eaa7f8da5d
    type: comment
  author: liaaron1
  content: '> @liaaron1 , there''s nothing obviously wrong to me, but it may be worth
    putting the .bin files into the same folder as the script so you can run the exact
    command. Another debug option I would try is to just the raw llama files as a
    test.


    @RonanMcGovern Got the same error after having moved the scripts into the same
    folder as the .bin files. It seemed the scripts expecting .pt files instead? Which
    format worked for you?'
  created_at: 2023-08-27 01:16:29+00:00
  edited: false
  hidden: false
  id: 64eab1fdfb77a3eaa7f8da5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
      fullname: Akarshan Biswas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akarshanbiswas
      type: user
    createdAt: '2023-08-27T02:16:39.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
          fullname: Akarshan Biswas
          isHf: false
          isPro: false
          name: akarshanbiswas
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-28T05:38:22.429Z'
      numEdits: 0
      reactions: []
    id: 64eab20714c57101f3b96ae0
    type: comment
  author: akarshanbiswas
  content: This comment has been hidden
  created_at: 2023-08-27 01:16:39+00:00
  edited: true
  hidden: true
  id: 64eab20714c57101f3b96ae0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
      fullname: Aaron Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liaaron1
      type: user
    createdAt: '2023-08-27T02:28:15.000Z'
    data:
      edited: false
      editors:
      - liaaron1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9440345168113708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
          fullname: Aaron Li
          isHf: false
          isPro: false
          name: liaaron1
          type: user
        html: '<p>The model I was trying to convert was fine tuned on top of <a href="https://huggingface.co/guardrail/llama-2-7b-guanaco-instruct-sharded/">https://huggingface.co/guardrail/llama-2-7b-guanaco-instruct-sharded/</a>
          - 4-bit precision using QLoRA by the way</p>

          '
        raw: The model I was trying to convert was fine tuned on top of https://huggingface.co/guardrail/llama-2-7b-guanaco-instruct-sharded/
          - 4-bit precision using QLoRA by the way
        updatedAt: '2023-08-27T02:28:15.381Z'
      numEdits: 0
      reactions: []
    id: 64eab4bf9be3dd2d52ea3bbc
    type: comment
  author: liaaron1
  content: The model I was trying to convert was fine tuned on top of https://huggingface.co/guardrail/llama-2-7b-guanaco-instruct-sharded/
    - 4-bit precision using QLoRA by the way
  created_at: 2023-08-27 01:28:15+00:00
  edited: false
  hidden: false
  id: 64eab4bf9be3dd2d52ea3bbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
      fullname: Aaron Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liaaron1
      type: user
    createdAt: '2023-08-27T11:52:29.000Z'
    data:
      edited: false
      editors:
      - liaaron1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9615345001220703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c85cbd531d48597ba20c8c9a6b661ad3.svg
          fullname: Aaron Li
          isHf: false
          isPro: false
          name: liaaron1
          type: user
        html: '<p>Problem solved after manually downloaded the model files to my locak
          disk again.  I was working on bin files with invalid contents. Sorry for
          the confusion.</p>

          '
        raw: Problem solved after manually downloaded the model files to my locak
          disk again.  I was working on bin files with invalid contents. Sorry for
          the confusion.
        updatedAt: '2023-08-27T11:52:29.802Z'
      numEdits: 0
      reactions: []
    id: 64eb38fdb96ff0e175515029
    type: comment
  author: liaaron1
  content: Problem solved after manually downloaded the model files to my locak disk
    again.  I was working on bin files with invalid contents. Sorry for the confusion.
  created_at: 2023-08-27 10:52:29+00:00
  edited: false
  hidden: false
  id: 64eb38fdb96ff0e175515029
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-07T17:23:35.000Z'
    data:
      edited: false
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7550680637359619
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: "<blockquote>\n<p>Ok, I've dug in more on this and it's tricky...</p>\n\
          <ol>\n<li><p>I don't know what the format of the input model to convert.py\
          \ needs to be. float 32 or bf16? See this new <a rel=\"nofollow\" href=\"\
          https://github.com/ggerganov/llama.cpp/issues/2571\">issue</a></p>\n</li>\n\
          <li><p>A lot of models on huggingface have shards of 10GB max. I don't know\
          \ how to handle shards with the convert.py script.</p>\n</li>\n</ol>\n<p>I\
          \ tried to do it with bf16 bin files, where I concatenated the shards, but\
          \ I ran into a key error (running in colab):</p>\n<pre><code>!python3 convert.py\
          \ ../models/\nLoading...\n\nLoading model file ../models/pytorch_model.bin\n\
          vocabtype: spm\nLoading vocab file ../models/tokenizer.model\nparams: n_vocab:32000\
          \ n_embd:4096 n_mult:5504 n_head:32 n_layer:32\nTraceback (most recent call\
          \ last):\n  File \"/content/llama.cpp/convert.py\", line 1326, in &lt;module&gt;\n\
          \    main()\n  File \"/content/llama.cpp/convert.py\", line 1317, in main\n\
          \    model = do_necessary_conversions(model, params)\n  File \"/content/llama.cpp/convert.py\"\
          , line 1146, in do_necessary_conversions\n    model = convert_transformers_to_orig(model,\
          \ params)\n  File \"/content/llama.cpp/convert.py\", line 737, in convert_transformers_to_orig\n\
          \    out[\"tok_embeddings.weight\"] = model[\"model.embed_tokens.weight\"\
          ]\nKeyError: 'model.embed_tokens.weight'\n</code></pre>\n<p>After solving\
          \ that, I would have to see if the following works in a colab notebook:</p>\n\
          <pre><code>!./quantize ./models/ggml-model-f16.bin ./models/ggml-model-q3_K_M.bin\
          \ q3_K_M\n</code></pre>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \  Can you help me out how did you resolved this embeddings error. I am\
          \ Stuck :) and do convert.py can convert pytorch model \"jphme/Llama-2-13b-chat-german\"\
          \ also as above Model has 3 .bin files  so we need to convert all files,\
          \ or converting 1 model can work effectively??<br><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/5lbF3GzHnRWuV5de70j7l.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/5lbF3GzHnRWuV5de70j7l.png\"\
          ></a></p>\n"
        raw: "> Ok, I've dug in more on this and it's tricky...\n> \n> 1. I don't\
          \ know what the format of the input model to convert.py needs to be. float\
          \ 32 or bf16? See this new [issue](https://github.com/ggerganov/llama.cpp/issues/2571)\n\
          > \n> 2. A lot of models on huggingface have shards of 10GB max. I don't\
          \ know how to handle shards with the convert.py script.\n> \n> I tried to\
          \ do it with bf16 bin files, where I concatenated the shards, but I ran\
          \ into a key error (running in colab):\n> ```\n> !python3 convert.py ../models/\n\
          > Loading...\n> \n> Loading model file ../models/pytorch_model.bin\n> vocabtype:\
          \ spm\n> Loading vocab file ../models/tokenizer.model\n> params: n_vocab:32000\
          \ n_embd:4096 n_mult:5504 n_head:32 n_layer:32\n> Traceback (most recent\
          \ call last):\n>   File \"/content/llama.cpp/convert.py\", line 1326, in\
          \ <module>\n>     main()\n>   File \"/content/llama.cpp/convert.py\", line\
          \ 1317, in main\n>     model = do_necessary_conversions(model, params)\n\
          >   File \"/content/llama.cpp/convert.py\", line 1146, in do_necessary_conversions\n\
          >     model = convert_transformers_to_orig(model, params)\n>   File \"/content/llama.cpp/convert.py\"\
          , line 737, in convert_transformers_to_orig\n>     out[\"tok_embeddings.weight\"\
          ] = model[\"model.embed_tokens.weight\"]\n> KeyError: 'model.embed_tokens.weight'\n\
          > ```\n> \n> After solving that, I would have to see if the following works\
          \ in a colab notebook:\n> ```\n> !./quantize ./models/ggml-model-f16.bin\
          \ ./models/ggml-model-q3_K_M.bin q3_K_M\n> ```\n\n@RonanMcGovern  Can you\
          \ help me out how did you resolved this embeddings error. I am Stuck :)\
          \ and do convert.py can convert pytorch model \"jphme/Llama-2-13b-chat-german\"\
          \ also as above Model has 3 .bin files  so we need to convert all files,\
          \ or converting 1 model can work effectively?? \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/5lbF3GzHnRWuV5de70j7l.png)\n\
          \ \n"
        updatedAt: '2023-09-07T17:23:35.650Z'
      numEdits: 0
      reactions: []
    id: 64fa0717fa644654223ba556
    type: comment
  author: komal-09
  content: "> Ok, I've dug in more on this and it's tricky...\n> \n> 1. I don't know\
    \ what the format of the input model to convert.py needs to be. float 32 or bf16?\
    \ See this new [issue](https://github.com/ggerganov/llama.cpp/issues/2571)\n>\
    \ \n> 2. A lot of models on huggingface have shards of 10GB max. I don't know\
    \ how to handle shards with the convert.py script.\n> \n> I tried to do it with\
    \ bf16 bin files, where I concatenated the shards, but I ran into a key error\
    \ (running in colab):\n> ```\n> !python3 convert.py ../models/\n> Loading...\n\
    > \n> Loading model file ../models/pytorch_model.bin\n> vocabtype: spm\n> Loading\
    \ vocab file ../models/tokenizer.model\n> params: n_vocab:32000 n_embd:4096 n_mult:5504\
    \ n_head:32 n_layer:32\n> Traceback (most recent call last):\n>   File \"/content/llama.cpp/convert.py\"\
    , line 1326, in <module>\n>     main()\n>   File \"/content/llama.cpp/convert.py\"\
    , line 1317, in main\n>     model = do_necessary_conversions(model, params)\n\
    >   File \"/content/llama.cpp/convert.py\", line 1146, in do_necessary_conversions\n\
    >     model = convert_transformers_to_orig(model, params)\n>   File \"/content/llama.cpp/convert.py\"\
    , line 737, in convert_transformers_to_orig\n>     out[\"tok_embeddings.weight\"\
    ] = model[\"model.embed_tokens.weight\"]\n> KeyError: 'model.embed_tokens.weight'\n\
    > ```\n> \n> After solving that, I would have to see if the following works in\
    \ a colab notebook:\n> ```\n> !./quantize ./models/ggml-model-f16.bin ./models/ggml-model-q3_K_M.bin\
    \ q3_K_M\n> ```\n\n@RonanMcGovern  Can you help me out how did you resolved this\
    \ embeddings error. I am Stuck :) and do convert.py can convert pytorch model\
    \ \"jphme/Llama-2-13b-chat-german\" also as above Model has 3 .bin files  so we\
    \ need to convert all files, or converting 1 model can work effectively?? \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/5lbF3GzHnRWuV5de70j7l.png)\n\
    \ \n"
  created_at: 2023-09-07 16:23:35+00:00
  edited: false
  hidden: false
  id: 64fa0717fa644654223ba556
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-08T07:58:25.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7558798789978027
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;komal-09&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/komal-09\">@<span class=\"\
          underline\">komal-09</span></a></span>\n\n\t</span></span> actually the\
          \ script handles all of this (multiple files), take a look at this github\
          \ issue: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/issues/2571\"\
          >https://github.com/ggerganov/llama.cpp/issues/2571</a></p>\n"
        raw: '@komal-09 actually the script handles all of this (multiple files),
          take a look at this github issue: https://github.com/ggerganov/llama.cpp/issues/2571'
        updatedAt: '2023-09-08T07:58:25.684Z'
      numEdits: 0
      reactions: []
    id: 64fad42152992431a6d1bcaf
    type: comment
  author: RonanMcGovern
  content: '@komal-09 actually the script handles all of this (multiple files), take
    a look at this github issue: https://github.com/ggerganov/llama.cpp/issues/2571'
  created_at: 2023-09-08 06:58:25+00:00
  edited: false
  hidden: false
  id: 64fad42152992431a6d1bcaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-08T10:48:16.000Z'
    data:
      edited: false
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8893989324569702
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: '<p>Ya That''s right, but My question is as mentioned in documentation
          I added tokenizer.model file into model''s folder and while running the
          convert.py scripts passed the path of my .bin file. then also this ''tok_embeddings.weight''
          error is coming.  </p>

          <p>This is the Repo hierarchy, what all should I include in Models folder
          of llama.cpp to get rid of this issue anything I am missing? </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/50zPL-JUqQridP3BZVajK.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/50zPL-JUqQridP3BZVajK.png"></a></p>

          '
        raw: "Ya That's right, but My question is as mentioned in documentation I\
          \ added tokenizer.model file into model's folder and while running the convert.py\
          \ scripts passed the path of my .bin file. then also this 'tok_embeddings.weight'\
          \ error is coming.  \n\nThis is the Repo hierarchy, what all should I include\
          \ in Models folder of llama.cpp to get rid of this issue anything I am missing?\
          \ \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/50zPL-JUqQridP3BZVajK.png)\n"
        updatedAt: '2023-09-08T10:48:16.958Z'
      numEdits: 0
      reactions: []
    id: 64fafbf08f76d45069056445
    type: comment
  author: komal-09
  content: "Ya That's right, but My question is as mentioned in documentation I added\
    \ tokenizer.model file into model's folder and while running the convert.py scripts\
    \ passed the path of my .bin file. then also this 'tok_embeddings.weight' error\
    \ is coming.  \n\nThis is the Repo hierarchy, what all should I include in Models\
    \ folder of llama.cpp to get rid of this issue anything I am missing? \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/50zPL-JUqQridP3BZVajK.png)\n"
  created_at: 2023-09-08 09:48:16+00:00
  edited: false
  hidden: false
  id: 64fafbf08f76d45069056445
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-08T12:31:53.000Z'
    data:
      edited: false
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9486509561538696
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ , currently I was trying to quantise the llama2 13b fine tuned model with\
          \ the help of llama.cpp<br>But I am just able to execute step of convert.py\
          \ but not able to run the ./quantize command and as well if I see the repo\
          \ I don\u2019t see any such file either</p>\n<p>Maybe this quantize is being\
          \ integrated inside of convert.py itself?</p>\n"
        raw: "Hi @RonanMcGovern , currently I was trying to quantise the llama2 13b\
          \ fine tuned model with the help of llama.cpp\nBut I am just able to execute\
          \ step of convert.py but not able to run the ./quantize command and as well\
          \ if I see the repo I don\u2019t see any such file either\n\nMaybe this\
          \ quantize is being integrated inside of convert.py itself?\n\n"
        updatedAt: '2023-09-08T12:31:53.575Z'
      numEdits: 0
      reactions: []
    id: 64fb14395ca946a0108a0016
    type: comment
  author: SanjuEpic
  content: "Hi @RonanMcGovern , currently I was trying to quantise the llama2 13b\
    \ fine tuned model with the help of llama.cpp\nBut I am just able to execute step\
    \ of convert.py but not able to run the ./quantize command and as well if I see\
    \ the repo I don\u2019t see any such file either\n\nMaybe this quantize is being\
    \ integrated inside of convert.py itself?\n\n"
  created_at: 2023-09-08 11:31:53+00:00
  edited: false
  hidden: false
  id: 64fb14395ca946a0108a0016
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-08T12:35:39.000Z'
    data:
      edited: false
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9332977533340454
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: '<p>But i see that with the current convert.py I am able to quantize
          it to q8_0 other than the fp32 and fp16</p>

          '
        raw: But i see that with the current convert.py I am able to quantize it to
          q8_0 other than the fp32 and fp16
        updatedAt: '2023-09-08T12:35:39.989Z'
      numEdits: 0
      reactions: []
    id: 64fb151bc7f04f7cee715428
    type: comment
  author: SanjuEpic
  content: But i see that with the current convert.py I am able to quantize it to
    q8_0 other than the fp32 and fp16
  created_at: 2023-09-08 11:35:39+00:00
  edited: false
  hidden: false
  id: 64fb151bc7f04f7cee715428
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-09-08T16:18:51.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9182519316673279
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>I was able to convert and quantize the fine-tuned model (llama2-7B,
          QLoRA, Dolly-15K dataset).But during inference there is an error</p>

          <blockquote>

          <p>error loading model: create_tensor: tensor ''output_norm.weight'' not
          found</p>

          </blockquote>

          <p>I''m a bit puzzled and cannot seem to find any info. Does any of you
          encounter this issues?</p>

          '
        raw: 'I was able to convert and quantize the fine-tuned model (llama2-7B,
          QLoRA, Dolly-15K dataset).But during inference there is an error


          > error loading model: create_tensor: tensor ''output_norm.weight'' not
          found


          I''m a bit puzzled and cannot seem to find any info. Does any of you encounter
          this issues?'
        updatedAt: '2023-09-08T16:18:51.923Z'
      numEdits: 0
      reactions: []
    id: 64fb496b010f41e4352ebae8
    type: comment
  author: zbruceli
  content: 'I was able to convert and quantize the fine-tuned model (llama2-7B, QLoRA,
    Dolly-15K dataset).But during inference there is an error


    > error loading model: create_tensor: tensor ''output_norm.weight'' not found


    I''m a bit puzzled and cannot seem to find any info. Does any of you encounter
    this issues?'
  created_at: 2023-09-08 15:18:51+00:00
  edited: false
  hidden: false
  id: 64fb496b010f41e4352ebae8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-09T03:31:50.000Z'
    data:
      edited: true
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9791840314865112
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<p>I think it\u2019s not properly quantized?<br>What was the code snippet\
          \ that you have used for the quantization of model?<br>And what technique\
          \ have u used?<br>Did u try with other quant methods and check whether u\
          \ were getting the same error? <span data-props=\"{&quot;user&quot;:&quot;zbruceli&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zbruceli\"\
          >@<span class=\"underline\">zbruceli</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: "I think it\u2019s not properly quantized?\nWhat was the code snippet\
          \ that you have used for the quantization of model?\nAnd what technique\
          \ have u used?\nDid u try with other quant methods and check whether u were\
          \ getting the same error? @zbruceli "
        updatedAt: '2023-09-09T03:32:11.655Z'
      numEdits: 1
      reactions: []
    id: 64fbe7269132c7f62a15a5a3
    type: comment
  author: SanjuEpic
  content: "I think it\u2019s not properly quantized?\nWhat was the code snippet that\
    \ you have used for the quantization of model?\nAnd what technique have u used?\n\
    Did u try with other quant methods and check whether u were getting the same error?\
    \ @zbruceli "
  created_at: 2023-09-09 02:31:50+00:00
  edited: true
  hidden: false
  id: 64fbe7269132c7f62a15a5a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-09T09:33:30.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9172768592834473
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Hi folks, haven''t had time to dig in deep here, but here is a <a
          rel="nofollow" href="https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb">gguf
          script</a> that may be of some help if you want to quantize with Colab.</p>

          <p>GGML is getting deprecated so probably it''s best to quantize to gguf.</p>

          '
        raw: 'Hi folks, haven''t had time to dig in deep here, but here is a [gguf
          script](https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb)
          that may be of some help if you want to quantize with Colab.


          GGML is getting deprecated so probably it''s best to quantize to gguf.'
        updatedAt: '2023-09-09T09:33:30.145Z'
      numEdits: 0
      reactions: []
    id: 64fc3bea4010eccccc323de2
    type: comment
  author: RonanMcGovern
  content: 'Hi folks, haven''t had time to dig in deep here, but here is a [gguf script](https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb)
    that may be of some help if you want to quantize with Colab.


    GGML is getting deprecated so probably it''s best to quantize to gguf.'
  created_at: 2023-09-09 08:33:30+00:00
  edited: false
  hidden: false
  id: 64fc3bea4010eccccc323de2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-09T13:00:41.000Z'
    data:
      edited: false
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9049458503723145
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<p>Thanks for the help <span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ , will give it a try!</p>\n"
        raw: 'Thanks for the help @RonanMcGovern , will give it a try!

          '
        updatedAt: '2023-09-09T13:00:41.021Z'
      numEdits: 0
      reactions: []
    id: 64fc6c797904ea30e68b5c78
    type: comment
  author: SanjuEpic
  content: 'Thanks for the help @RonanMcGovern , will give it a try!

    '
  created_at: 2023-09-09 12:00:41+00:00
  edited: false
  hidden: false
  id: 64fc6c797904ea30e68b5c78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-09-09T21:12:31.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8345580101013184
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: "<blockquote>\n<p>I think it\u2019s not properly quantized?<br>What\
          \ was the code snippet that you have used for the quantization of model?<br>And\
          \ what technique have u used?<br>Did u try with other quant methods and\
          \ check whether u were getting the same error? <span data-props=\"{&quot;user&quot;:&quot;zbruceli&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zbruceli\"\
          >@<span class=\"underline\">zbruceli</span></a></span>\n\n\t</span></span></p>\n\
          </blockquote>\n<p>I was using the llama.cpp instructions to convert models\
          \ to gguf format. It works perfectly with original meta-llama2-7B model,\
          \ but had the problems when converting QLoRA trained model (after merging).\
          \ I was using OVH Cloud tutorial and notebook for the QLoRA fine-tuning.\
          \ <a rel=\"nofollow\" href=\"https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/\"\
          >https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/</a></p>\n\
          <p>First step: use llama.cpp convert.py to convert model to F16. There was\
          \ one error</p>\n<blockquote>\n<p>\"Could not find tokenizer.model in models/ovh7b\
          \ or its parent\".</p>\n</blockquote>\n<p>So I copied the tokenizer.model\
          \ from original meta-llama2-7B model files. Then the convert script works\
          \ correctly.</p>\n<p>Then I quantize to q4.0 and it also worked.</p>\n<p>But\
          \ when I use llama.cpp to do intereference, I got the error of</p>\n<blockquote>\n\
          <p>error loading model: create_tensor: tensor 'output_norm.weight' not found<br>llama_load_model_from_file:\
          \ failed to load model<br>llama_init_from_gpt_params: error: failed to load\
          \ model 'models/ovh7b/ggml-model-q4_0.gguf'</p>\n</blockquote>\n"
        raw: "> I think it\u2019s not properly quantized?\n> What was the code snippet\
          \ that you have used for the quantization of model?\n> And what technique\
          \ have u used?\n> Did u try with other quant methods and check whether u\
          \ were getting the same error? @zbruceli\n\nI was using the llama.cpp instructions\
          \ to convert models to gguf format. It works perfectly with original meta-llama2-7B\
          \ model, but had the problems when converting QLoRA trained model (after\
          \ merging). I was using OVH Cloud tutorial and notebook for the QLoRA fine-tuning.\
          \ https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/\n\
          \nFirst step: use llama.cpp convert.py to convert model to F16. There was\
          \ one error\n\n> \"Could not find tokenizer.model in models/ovh7b or its\
          \ parent\".\n\nSo I copied the tokenizer.model from original meta-llama2-7B\
          \ model files. Then the convert script works correctly.\n\nThen I quantize\
          \ to q4.0 and it also worked.\n\nBut when I use llama.cpp to do intereference,\
          \ I got the error of\n\n> error loading model: create_tensor: tensor 'output_norm.weight'\
          \ not found\n> llama_load_model_from_file: failed to load model\n> llama_init_from_gpt_params:\
          \ error: failed to load model 'models/ovh7b/ggml-model-q4_0.gguf'"
        updatedAt: '2023-09-09T21:12:31.485Z'
      numEdits: 0
      reactions: []
    id: 64fcdfbf74574268a581a2e9
    type: comment
  author: zbruceli
  content: "> I think it\u2019s not properly quantized?\n> What was the code snippet\
    \ that you have used for the quantization of model?\n> And what technique have\
    \ u used?\n> Did u try with other quant methods and check whether u were getting\
    \ the same error? @zbruceli\n\nI was using the llama.cpp instructions to convert\
    \ models to gguf format. It works perfectly with original meta-llama2-7B model,\
    \ but had the problems when converting QLoRA trained model (after merging). I\
    \ was using OVH Cloud tutorial and notebook for the QLoRA fine-tuning. https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/\n\
    \nFirst step: use llama.cpp convert.py to convert model to F16. There was one\
    \ error\n\n> \"Could not find tokenizer.model in models/ovh7b or its parent\"\
    .\n\nSo I copied the tokenizer.model from original meta-llama2-7B model files.\
    \ Then the convert script works correctly.\n\nThen I quantize to q4.0 and it also\
    \ worked.\n\nBut when I use llama.cpp to do intereference, I got the error of\n\
    \n> error loading model: create_tensor: tensor 'output_norm.weight' not found\n\
    > llama_load_model_from_file: failed to load model\n> llama_init_from_gpt_params:\
    \ error: failed to load model 'models/ovh7b/ggml-model-q4_0.gguf'"
  created_at: 2023-09-09 20:12:31+00:00
  edited: false
  hidden: false
  id: 64fcdfbf74574268a581a2e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-10T05:28:27.000Z'
    data:
      edited: false
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8677351474761963
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<p>How did u quantize your model to q4_0?<br>in my case when i try\
          \ to execute ./quantize from llama.cpp repo i get \"no such file or directory\
          \ found\" error.<br>can u help me with this <span data-props=\"{&quot;user&quot;:&quot;zbruceli&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zbruceli\"\
          >@<span class=\"underline\">zbruceli</span></a></span>\n\n\t</span></span>\
          \ ?</p>\n<p>Just a naive queston, will changing the model format to .bin\
          \ work for your inference use? </p>\n"
        raw: 'How did u quantize your model to q4_0?

          in my case when i try to execute ./quantize from llama.cpp repo i get "no
          such file or directory found" error.

          can u help me with this @zbruceli ?


          Just a naive queston, will changing the model format to .bin work for your
          inference use? '
        updatedAt: '2023-09-10T05:28:27.552Z'
      numEdits: 0
      reactions: []
    id: 64fd53fbb961d0d12cc5ff91
    type: comment
  author: SanjuEpic
  content: 'How did u quantize your model to q4_0?

    in my case when i try to execute ./quantize from llama.cpp repo i get "no such
    file or directory found" error.

    can u help me with this @zbruceli ?


    Just a naive queston, will changing the model format to .bin work for your inference
    use? '
  created_at: 2023-09-10 04:28:27+00:00
  edited: false
  hidden: false
  id: 64fd53fbb961d0d12cc5ff91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-10T06:38:27.000Z'
    data:
      edited: false
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9280728697776794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<blockquote>\n<p>Hi folks, haven't had time to dig in deep here, but\
          \ here is a <a rel=\"nofollow\" href=\"https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb\"\
          >gguf script</a> that may be of some help if you want to quantize with Colab.</p>\n\
          <p>GGML is getting deprecated so probably it's best to quantize to gguf.</p>\n\
          </blockquote>\n<p>This works like a charm, thanks for the help <span data-props=\"\
          {&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span class=\"underline\"\
          >RonanMcGovern</span></a></span>\n\n\t</span></span> , earlier i missed\
          \ installing via cmake, hence was not able to find the quantize file.</p>\n"
        raw: "> Hi folks, haven't had time to dig in deep here, but here is a [gguf\
          \ script](https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb)\
          \ that may be of some help if you want to quantize with Colab.\n> \n> GGML\
          \ is getting deprecated so probably it's best to quantize to gguf.\n\nThis\
          \ works like a charm, thanks for the help @RonanMcGovern , earlier i missed\
          \ installing via cmake, hence was not able to find the quantize file."
        updatedAt: '2023-09-10T06:38:27.594Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - RonanMcGovern
        - RadarSISA
    id: 64fd6463d82fc6977db107e5
    type: comment
  author: SanjuEpic
  content: "> Hi folks, haven't had time to dig in deep here, but here is a [gguf\
    \ script](https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb)\
    \ that may be of some help if you want to quantize with Colab.\n> \n> GGML is\
    \ getting deprecated so probably it's best to quantize to gguf.\n\nThis works\
    \ like a charm, thanks for the help @RonanMcGovern , earlier i missed installing\
    \ via cmake, hence was not able to find the quantize file."
  created_at: 2023-09-10 05:38:27+00:00
  edited: false
  hidden: false
  id: 64fd6463d82fc6977db107e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-11T09:16:09.000Z'
    data:
      edited: false
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7777685523033142
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: '<p>Hi , </p>

          <p>I am not able to Quantize my model after running convert.py from Llama.cpp  the
          mode has been converted into gguf type but while running  ''<br>./quantize
          C:\PrivateGPT\privategpt\privateGPT-main\llama.cpp-master\models\ggml-model-f16.gguf
          C:\PrivateGPT\privategpt\privateGPT-main\llama.cpp-master\models\ggml-model-q4_0.gguf
          q4_0</p>

          <p>Error Occured :- ./quantize is not a cmdlet or script function.<br>Any
          Suggested solutions?<br>Also, I am trying to work on imartinez/privateGPT
          and trying to load model<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/LjxvcPIEZZJYkreYQ-gN7.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/LjxvcPIEZZJYkreYQ-gN7.png"></a><br>
          but this line is giving me Validation error.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/PHQ7LQNodyyr175F5Ngj9.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/PHQ7LQNodyyr175F5Ngj9.png"></a><br>Please
          Help I am very beginner in all these Need help in learning as no professional
          courses are available related to LLM''s and GPT.</p>

          '
        raw: "Hi , \n\nI am not able to Quantize my model after running convert.py\
          \ from Llama.cpp  the mode has been converted into gguf type but while running\
          \  '\n./quantize C:\\PrivateGPT\\privategpt\\privateGPT-main\\llama.cpp-master\\\
          models\\ggml-model-f16.gguf C:\\PrivateGPT\\privategpt\\privateGPT-main\\\
          llama.cpp-master\\models\\ggml-model-q4_0.gguf q4_0\n\nError Occured :-\
          \ ./quantize is not a cmdlet or script function. \nAny Suggested solutions?\
          \ \nAlso, I am trying to work on imartinez/privateGPT and trying to load\
          \ model \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/LjxvcPIEZZJYkreYQ-gN7.png)\n\
          \ but this line is giving me Validation error.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/PHQ7LQNodyyr175F5Ngj9.png)\n\
          Please Help I am very beginner in all these Need help in learning as no\
          \ professional courses are available related to LLM's and GPT."
        updatedAt: '2023-09-11T09:16:09.860Z'
      numEdits: 0
      reactions: []
    id: 64fedad9b5510e82282b147b
    type: comment
  author: komal-09
  content: "Hi , \n\nI am not able to Quantize my model after running convert.py from\
    \ Llama.cpp  the mode has been converted into gguf type but while running  '\n\
    ./quantize C:\\PrivateGPT\\privategpt\\privateGPT-main\\llama.cpp-master\\models\\\
    ggml-model-f16.gguf C:\\PrivateGPT\\privategpt\\privateGPT-main\\llama.cpp-master\\\
    models\\ggml-model-q4_0.gguf q4_0\n\nError Occured :- ./quantize is not a cmdlet\
    \ or script function. \nAny Suggested solutions? \nAlso, I am trying to work on\
    \ imartinez/privateGPT and trying to load model \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/LjxvcPIEZZJYkreYQ-gN7.png)\n\
    \ but this line is giving me Validation error.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/PHQ7LQNodyyr175F5Ngj9.png)\n\
    Please Help I am very beginner in all these Need help in learning as no professional\
    \ courses are available related to LLM's and GPT."
  created_at: 2023-09-11 08:16:09+00:00
  edited: false
  hidden: false
  id: 64fedad9b5510e82282b147b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-11T09:18:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9216335415840149
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>You guys know I've done all these models in GGUF now?  You could\
          \ just use mine:  <a href=\"https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF\"\
          >https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF</a> - no need to make\
          \ your own if you don't want to.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;komal-09&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/komal-09\"\
          >@<span class=\"underline\">komal-09</span></a></span>\n\n\t</span></span>\
          \ sorry I have no recent experience with PrivateGPT or GPT4All.  But if\
          \ you're trying to load GGML files with it, it might be that it only now\
          \ supports GGUF.  Try the GGUF files instead; GGML is no longer supported\
          \ by many tools.</p>\n"
        raw: 'You guys know I''ve done all these models in GGUF now?  You could just
          use mine:  https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF - no need
          to make your own if you don''t want to.


          @komal-09 sorry I have no recent experience with PrivateGPT or GPT4All.  But
          if you''re trying to load GGML files with it, it might be that it only now
          supports GGUF.  Try the GGUF files instead; GGML is no longer supported
          by many tools.'
        updatedAt: '2023-09-11T09:18:47.030Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - abrahamn
    id: 64fedb776f77556d68d4e049
    type: comment
  author: TheBloke
  content: 'You guys know I''ve done all these models in GGUF now?  You could just
    use mine:  https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF - no need to make
    your own if you don''t want to.


    @komal-09 sorry I have no recent experience with PrivateGPT or GPT4All.  But if
    you''re trying to load GGML files with it, it might be that it only now supports
    GGUF.  Try the GGUF files instead; GGML is no longer supported by many tools.'
  created_at: 2023-09-11 08:18:47+00:00
  edited: false
  hidden: false
  id: 64fedb776f77556d68d4e049
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-11T09:36:14.000Z'
    data:
      edited: false
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8791319131851196
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: '<p>yes, that''s possible reason and I have converted my model into
          GGUF  but not able to Quantize it.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/x8jJBeU3AAwQQuRaVA4uJ.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/x8jJBeU3AAwQQuRaVA4uJ.png"></a><br>
          as Written in Readme of  Llama.cpp</p>

          '
        raw: "yes, that's possible reason and I have converted my model into GGUF\
          \  but not able to Quantize it. \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/x8jJBeU3AAwQQuRaVA4uJ.png)\n\
          \ as Written in Readme of  Llama.cpp"
        updatedAt: '2023-09-11T09:36:14.870Z'
      numEdits: 0
      reactions: []
    id: 64fedf8ef441b226e114c224
    type: comment
  author: komal-09
  content: "yes, that's possible reason and I have converted my model into GGUF  but\
    \ not able to Quantize it. \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/x8jJBeU3AAwQQuRaVA4uJ.png)\n\
    \ as Written in Readme of  Llama.cpp"
  created_at: 2023-09-11 08:36:14+00:00
  edited: false
  hidden: false
  id: 64fedf8ef441b226e114c224
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-09-11T15:10:40.000Z'
    data:
      edited: true
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9715220928192139
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  I know i am\
          \ one who has appreciated greatly the work you have been doing for the community.\
          \ With the recent move to GGUF i started experimenting with doing it myself.\
          \ Basically 'self empowerment' for the next time they change formats on\
          \ us. And watching what you were doing, helped greatly in that venture.</p>\n\
          <p>Originally i honestly did not think i had the resources ( stuck with\
          \ an older 12gb vGPU titan, fried my 24gb tesla ), but the conversion+quantization\
          \ is not bad at all and only takes a few minutes on my non GPU machine,\
          \ for a 13B model. ( training, forget it, unless i lease GPU time.. ). </p>\n"
        raw: "@TheBloke  I know i am one who has appreciated greatly the work you\
          \ have been doing for the community. With the recent move to GGUF i started\
          \ experimenting with doing it myself. Basically 'self empowerment' for the\
          \ next time they change formats on us. And watching what you were doing,\
          \ helped greatly in that venture.\n \nOriginally i honestly did not think\
          \ i had the resources ( stuck with an older 12gb vGPU titan, fried my 24gb\
          \ tesla ), but the conversion+quantization is not bad at all and only takes\
          \ a few minutes on my non GPU machine, for a 13B model. ( training, forget\
          \ it, unless i lease GPU time.. ). \n\n"
        updatedAt: '2023-09-11T15:12:50.489Z'
      numEdits: 1
      reactions: []
    id: 64ff2df0416b2a83cf49a9dc
    type: comment
  author: Nurb432
  content: "@TheBloke  I know i am one who has appreciated greatly the work you have\
    \ been doing for the community. With the recent move to GGUF i started experimenting\
    \ with doing it myself. Basically 'self empowerment' for the next time they change\
    \ formats on us. And watching what you were doing, helped greatly in that venture.\n\
    \ \nOriginally i honestly did not think i had the resources ( stuck with an older\
    \ 12gb vGPU titan, fried my 24gb tesla ), but the conversion+quantization is not\
    \ bad at all and only takes a few minutes on my non GPU machine, for a 13B model.\
    \ ( training, forget it, unless i lease GPU time.. ). \n\n"
  created_at: 2023-09-11 14:10:40+00:00
  edited: true
  hidden: false
  id: 64ff2df0416b2a83cf49a9dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-09-11T15:15:14.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975303590297699
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;komal-09&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/komal-09\">@<span class=\"\
          underline\">komal-09</span></a></span>\n\n\t</span></span>   You did compile\
          \ it.. right? The python stuff works from the repository out of box since\
          \ its just a script, but that tool isn't an executable until you 'make'\
          \ it.</p>\n"
        raw: '@komal-09   You did compile it.. right? The python stuff works from
          the repository out of box since its just a script, but that tool isn''t
          an executable until you ''make'' it.'
        updatedAt: '2023-09-11T15:15:14.684Z'
      numEdits: 0
      reactions: []
    id: 64ff2f02f39f9f3c097cb798
    type: comment
  author: Nurb432
  content: '@komal-09   You did compile it.. right? The python stuff works from the
    repository out of box since its just a script, but that tool isn''t an executable
    until you ''make'' it.'
  created_at: 2023-09-11 14:15:14+00:00
  edited: false
  hidden: false
  id: 64ff2f02f39f9f3c097cb798
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-11T15:41:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9544830918312073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Nurb432&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Nurb432\">@<span class=\"\
          underline\">Nurb432</span></a></span>\n\n\t</span></span> great to hear\
          \ - and yeah making GGUFs is very light and efficient, needing very few\
          \ resources. They have done a great job on making it use as little RAM as\
          \ possible.  Pretty much any PC can make GGUFs, even of big models like\
          \ 70B.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;komal-09&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/komal-09\"\
          >@<span class=\"underline\">komal-09</span></a></span>\n\n\t</span></span>\
          \ Assuming you compiled or downloaded already-compiled binaries, then on\
          \ Windows it would be <code>quantize.exe</code>rather than <code>./quantize</code>\
          \ </p>\n"
        raw: '@Nurb432 great to hear - and yeah making GGUFs is very light and efficient,
          needing very few resources. They have done a great job on making it use
          as little RAM as possible.  Pretty much any PC can make GGUFs, even of big
          models like 70B.


          @komal-09 Assuming you compiled or downloaded already-compiled binaries,
          then on Windows it would be `quantize.exe`rather than `./quantize` '
        updatedAt: '2023-09-11T15:41:51.529Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Nurb432
        - RonanMcGovern
    id: 64ff353ffa4f2081707d6f5c
    type: comment
  author: TheBloke
  content: '@Nurb432 great to hear - and yeah making GGUFs is very light and efficient,
    needing very few resources. They have done a great job on making it use as little
    RAM as possible.  Pretty much any PC can make GGUFs, even of big models like 70B.


    @komal-09 Assuming you compiled or downloaded already-compiled binaries, then
    on Windows it would be `quantize.exe`rather than `./quantize` '
  created_at: 2023-09-11 14:41:51+00:00
  edited: false
  hidden: false
  id: 64ff353ffa4f2081707d6f5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-12T03:37:39.000Z'
    data:
      edited: true
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.823712944984436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  Yes<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pKCcp9_wGE8eNMkVho7U4.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pKCcp9_wGE8eNMkVho7U4.png\"\
          ></a><br>using above steps, I did build it.  but  'quantize.exe'  also didn't\
          \ work giving same error. Scripts I can see in my directory are as follows<br><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pJL_V8oxt3wjhSgD6VNJr.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pJL_V8oxt3wjhSgD6VNJr.png\"\
          ></a></p>\n<p>Ay way from where I can download quantize.exe direclty ?</p>\n"
        raw: "@TheBloke  Yes \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pKCcp9_wGE8eNMkVho7U4.png)\n\
          using above steps, I did build it.  but  'quantize.exe'  also didn't work\
          \ giving same error. Scripts I can see in my directory are as follows\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pJL_V8oxt3wjhSgD6VNJr.png)\n\
          \n\nAy way from where I can download quantize.exe direclty ?"
        updatedAt: '2023-09-12T04:03:49.083Z'
      numEdits: 1
      reactions: []
    id: 64ffdd037aa4d88f2064813b
    type: comment
  author: komal-09
  content: "@TheBloke  Yes \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pKCcp9_wGE8eNMkVho7U4.png)\n\
    using above steps, I did build it.  but  'quantize.exe'  also didn't work giving\
    \ same error. Scripts I can see in my directory are as follows\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/pJL_V8oxt3wjhSgD6VNJr.png)\n\
    \n\nAy way from where I can download quantize.exe direclty ?"
  created_at: 2023-09-12 02:37:39+00:00
  edited: true
  hidden: false
  id: 64ffdd037aa4d88f2064813b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-09-12T03:43:20.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8735622763633728
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>Afterr going through the entire process, I wrote down the successful
          path of fine-tuning and then convert to gguf for llama.cpp use</p>

          <p><a rel="nofollow" href="https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop">https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop</a></p>

          '
        raw: 'Afterr going through the entire process, I wrote down the successful
          path of fine-tuning and then convert to gguf for llama.cpp use


          https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop'
        updatedAt: '2023-09-12T03:43:20.387Z'
      numEdits: 0
      reactions: []
    id: 64ffde5869219ce3e48b22d9
    type: comment
  author: zbruceli
  content: 'Afterr going through the entire process, I wrote down the successful path
    of fine-tuning and then convert to gguf for llama.cpp use


    https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop'
  created_at: 2023-09-12 02:43:20+00:00
  edited: false
  hidden: false
  id: 64ffde5869219ce3e48b22d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-12T05:26:50.000Z'
    data:
      edited: false
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9249625205993652
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Hi folks, haven't had time to dig in\
          \ deep here, but here is a <a rel=\"nofollow\" href=\"https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb\"\
          >gguf script</a> that may be of some help if you want to quantize with Colab.</p>\n\
          <p>GGML is getting deprecated so probably it's best to quantize to gguf.</p>\n\
          </blockquote>\n<p>This works like a charm, thanks for the help <span data-props=\"\
          {&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span class=\"underline\"\
          >RonanMcGovern</span></a></span>\n\n\t</span></span> , earlier i missed\
          \ installing via cmake, hence was not able to find the quantize file.</p>\n\
          </blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;komal-09&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/komal-09\"\
          >@<span class=\"underline\">komal-09</span></a></span>\n\n\t</span></span>\
          \ , try installation steps from this script which ronan has provided and\
          \ see.</p>\n"
        raw: "> > Hi folks, haven't had time to dig in deep here, but here is a [gguf\
          \ script](https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb)\
          \ that may be of some help if you want to quantize with Colab.\n> > \n>\
          \ > GGML is getting deprecated so probably it's best to quantize to gguf.\n\
          > \n> This works like a charm, thanks for the help @RonanMcGovern , earlier\
          \ i missed installing via cmake, hence was not able to find the quantize\
          \ file.\n\nHi @komal-09 , try installation steps from this script which\
          \ ronan has provided and see."
        updatedAt: '2023-09-12T05:26:50.136Z'
      numEdits: 0
      reactions: []
    id: 64fff69a69219ce3e48e4247
    type: comment
  author: SanjuEpic
  content: "> > Hi folks, haven't had time to dig in deep here, but here is a [gguf\
    \ script](https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb)\
    \ that may be of some help if you want to quantize with Colab.\n> > \n> > GGML\
    \ is getting deprecated so probably it's best to quantize to gguf.\n> \n> This\
    \ works like a charm, thanks for the help @RonanMcGovern , earlier i missed installing\
    \ via cmake, hence was not able to find the quantize file.\n\nHi @komal-09 , try\
    \ installation steps from this script which ronan has provided and see."
  created_at: 2023-09-12 04:26:50+00:00
  edited: false
  hidden: false
  id: 64fff69a69219ce3e48e4247
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-12T06:05:34.000Z'
    data:
      edited: true
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9536969065666199
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;SanjuEpic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/SanjuEpic\">@<span class=\"\
          underline\">SanjuEpic</span></a></span>\n\n\t</span></span>  I did try this\
          \ approach by setting up again but as in <span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \  Collab file on ls -1 we can see Quantize file available just above readme\
          \ in mine directory it is not available :)<br>please if there is any source\
          \ from where I can download quantize file do Let me know it's urgent.</p>\n"
        raw: "@SanjuEpic  I did try this approach by setting up again but as in @RonanMcGovern\
          \  Collab file on ls -1 we can see Quantize file available just above readme\
          \ in mine directory it is not available :)  \nplease if there is any source\
          \ from where I can download quantize file do Let me know it's urgent."
        updatedAt: '2023-09-12T06:06:36.982Z'
      numEdits: 1
      reactions: []
    id: 64ffffae9d5687f825388d90
    type: comment
  author: komal-09
  content: "@SanjuEpic  I did try this approach by setting up again but as in @RonanMcGovern\
    \  Collab file on ls -1 we can see Quantize file available just above readme in\
    \ mine directory it is not available :)  \nplease if there is any source from\
    \ where I can download quantize file do Let me know it's urgent."
  created_at: 2023-09-12 05:05:34+00:00
  edited: true
  hidden: false
  id: 64ffffae9d5687f825388d90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-12T06:44:24.000Z'
    data:
      edited: false
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9184303879737854
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<p>Did u try that exact installation process but still getting the\
          \ error?<br>If so then I\u2019m not aware of how to resolve your problem\
          \ :(</p>\n"
        raw: "Did u try that exact installation process but still getting the error?\n\
          If so then I\u2019m not aware of how to resolve your problem :("
        updatedAt: '2023-09-12T06:44:24.453Z'
      numEdits: 0
      reactions: []
    id: 650008c8051fae19fc3d3b85
    type: comment
  author: SanjuEpic
  content: "Did u try that exact installation process but still getting the error?\n\
    If so then I\u2019m not aware of how to resolve your problem :("
  created_at: 2023-09-12 05:44:24+00:00
  edited: false
  hidden: false
  id: 650008c8051fae19fc3d3b85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
      fullname: Attili Sanjeet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjuEpic
      type: user
    createdAt: '2023-09-12T07:08:20.000Z'
    data:
      edited: true
      editors:
      - SanjuEpic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8296992182731628
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91f66c54f79887f80cbd6a7e34483a25.svg
          fullname: Attili Sanjeet
          isHf: false
          isPro: false
          name: SanjuEpic
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;komal-09&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/komal-09\">@<span class=\"\
          underline\">komal-09</span></a></span>\n\n\t</span></span> try these steps\
          \ once before doing ./quantize, even i had the similar issue previously,\
          \ once u build it then qunatize executable will be visible</p>\n<pre><code>cd\
          \ llama.cpp/\napt get update\napt install build-essential git cmake libopenblas-dev\
          \ libeigen3-dev\nmake LLAMA_OPENBLAS=1\nls\n</code></pre>\n"
        raw: '@komal-09 try these steps once before doing ./quantize, even i had the
          similar issue previously, once u build it then qunatize executable will
          be visible


          ```

          cd llama.cpp/

          apt get update

          apt install build-essential git cmake libopenblas-dev libeigen3-dev

          make LLAMA_OPENBLAS=1

          ls

          ```'
        updatedAt: '2023-09-12T07:09:06.934Z'
      numEdits: 1
      reactions: []
    id: 65000e641e14749e84e6f96e
    type: comment
  author: SanjuEpic
  content: '@komal-09 try these steps once before doing ./quantize, even i had the
    similar issue previously, once u build it then qunatize executable will be visible


    ```

    cd llama.cpp/

    apt get update

    apt install build-essential git cmake libopenblas-dev libeigen3-dev

    make LLAMA_OPENBLAS=1

    ls

    ```'
  created_at: 2023-09-12 06:08:20+00:00
  edited: true
  hidden: false
  id: 65000e641e14749e84e6f96e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-12T09:30:57.000Z'
    data:
      edited: false
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.882826030254364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: '<p>on windows apt command is not valid, can you give alternative command.</p>

          '
        raw: on windows apt command is not valid, can you give alternative command.
        updatedAt: '2023-09-12T09:30:57.665Z'
      numEdits: 0
      reactions: []
    id: 65002fd132d2159207ecaed6
    type: comment
  author: komal-09
  content: on windows apt command is not valid, can you give alternative command.
  created_at: 2023-09-12 08:30:57+00:00
  edited: false
  hidden: false
  id: 65002fd132d2159207ecaed6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-12T09:34:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8424482941627502
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;komal-09&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/komal-09\">@<span class=\"\
          underline\">komal-09</span></a></span>\n\n\t</span></span> just download\
          \ a pre-built release for Windows: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/releases\"\
          >https://github.com/ggerganov/llama.cpp/releases</a></p>\n<p>It will have\
          \ main.exe, quantize.exe, and everything else. No need to build it yourself.</p>\n\
          <p>If you have an NVidia GPU, pick the <code>cu11.7.1</code> version if\
          \ you use CUDA toolkit 11.x, or <code>cu12.1.0</code> version if you use\
          \ CUDA toolkit 12.x.  </p>\n<p>If you don't have an NVidia GPU or don't\
          \ plan to use it, pick <code>llama-b1215-bin-win-avx2-x64.zip</code>  if\
          \ you have a modern CPU, or <code>llama-b1215-bin-win-avx-x64.zip</code>\
          \ if you have an old CPU (more than 7+ years old)</p>\n"
        raw: "@komal-09 just download a pre-built release for Windows: https://github.com/ggerganov/llama.cpp/releases\n\
          \nIt will have main.exe, quantize.exe, and everything else. No need to build\
          \ it yourself.\n\nIf you have an NVidia GPU, pick the `cu11.7.1` version\
          \ if you use CUDA toolkit 11.x, or `cu12.1.0` version if you use CUDA toolkit\
          \ 12.x.  \n\nIf you don't have an NVidia GPU or don't plan to use it, pick\
          \ `llama-b1215-bin-win-avx2-x64.zip`  if you have a modern CPU, or `llama-b1215-bin-win-avx-x64.zip`\
          \ if you have an old CPU (more than 7+ years old)"
        updatedAt: '2023-09-12T09:34:50.662Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - PsiPi
        - sugatoray
    id: 650030ba868ac1994a17024c
    type: comment
  author: TheBloke
  content: "@komal-09 just download a pre-built release for Windows: https://github.com/ggerganov/llama.cpp/releases\n\
    \nIt will have main.exe, quantize.exe, and everything else. No need to build it\
    \ yourself.\n\nIf you have an NVidia GPU, pick the `cu11.7.1` version if you use\
    \ CUDA toolkit 11.x, or `cu12.1.0` version if you use CUDA toolkit 12.x.  \n\n\
    If you don't have an NVidia GPU or don't plan to use it, pick `llama-b1215-bin-win-avx2-x64.zip`\
    \  if you have a modern CPU, or `llama-b1215-bin-win-avx-x64.zip` if you have\
    \ an old CPU (more than 7+ years old)"
  created_at: 2023-09-12 08:34:50+00:00
  edited: false
  hidden: false
  id: 650030ba868ac1994a17024c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
      fullname: Komal Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komal-09
      type: user
    createdAt: '2023-09-12T09:39:20.000Z'
    data:
      edited: true
      editors:
      - komal-09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8557921051979065
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd446fd42b4094686aaacd75a77943.svg
          fullname: Komal Gupta
          isHf: false
          isPro: false
          name: komal-09
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>   Thank you so\
          \ much quantization worked \U0001F60A</p>\n<p>But Still Llamacpp not able\
          \ to load my model.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/1Hxp4-TGemYjD0EW5k26E.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/1Hxp4-TGemYjD0EW5k26E.png\"\
          ></a>\n </p>\n"
        raw: "@TheBloke   Thank you so much quantization worked \U0001F60A\n\nBut\
          \ Still Llamacpp not able to load my model.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/1Hxp4-TGemYjD0EW5k26E.png)\n\
          \ "
        updatedAt: '2023-09-12T10:09:18.076Z'
      numEdits: 1
      reactions: []
    id: 650031c818830fabea4c61c8
    type: comment
  author: komal-09
  content: "@TheBloke   Thank you so much quantization worked \U0001F60A\n\nBut Still\
    \ Llamacpp not able to load my model.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fa061c491fad0963f84863/1Hxp4-TGemYjD0EW5k26E.png)\n\
    \ "
  created_at: 2023-09-12 08:39:20+00:00
  edited: true
  hidden: false
  id: 650031c818830fabea4c61c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/173177a0b18382b71f201f367607061a.svg
      fullname: Kavita Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kavita08
      type: user
    createdAt: '2023-09-19T02:22:20.000Z'
    data:
      edited: false
      editors:
      - Kavita08
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8653785586357117
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/173177a0b18382b71f201f367607061a.svg
          fullname: Kavita Singh
          isHf: false
          isPro: false
          name: Kavita08
          type: user
        html: '<blockquote>

          <p>Afterr going through the entire process, I wrote down the successful
          path of fine-tuning and then convert to gguf for llama.cpp use</p>

          <p><a rel="nofollow" href="https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop">https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop</a></p>

          </blockquote>

          '
        raw: "> Afterr going through the entire process, I wrote down the successful\
          \ path of fine-tuning and then convert to gguf for llama.cpp use\n> \n>\
          \ https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop\n\
          \n"
        updatedAt: '2023-09-19T02:22:20.377Z'
      numEdits: 0
      reactions: []
    id: 650905dc5b34509e16d48aa1
    type: comment
  author: Kavita08
  content: "> Afterr going through the entire process, I wrote down the successful\
    \ path of fine-tuning and then convert to gguf for llama.cpp use\n> \n> https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop\n\
    \n"
  created_at: 2023-09-19 01:22:20+00:00
  edited: false
  hidden: false
  id: 650905dc5b34509e16d48aa1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/173177a0b18382b71f201f367607061a.svg
      fullname: Kavita Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kavita08
      type: user
    createdAt: '2023-09-19T02:24:11.000Z'
    data:
      edited: false
      editors:
      - Kavita08
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579119682312012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/173177a0b18382b71f201f367607061a.svg
          fullname: Kavita Singh
          isHf: false
          isPro: false
          name: Kavita08
          type: user
        html: '<p>Where to find those 3 ggml files ? And also first and second  steps
          commands are same in your post.</p>

          '
        raw: Where to find those 3 ggml files ? And also first and second  steps commands
          are same in your post.
        updatedAt: '2023-09-19T02:24:11.388Z'
      numEdits: 0
      reactions: []
    id: 6509064bf6a2bf0be7b4432b
    type: comment
  author: Kavita08
  content: Where to find those 3 ggml files ? And also first and second  steps commands
    are same in your post.
  created_at: 2023-09-19 01:24:11+00:00
  edited: false
  hidden: false
  id: 6509064bf6a2bf0be7b4432b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-09-19T02:56:12.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8240940570831299
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>My bad, the second step command was a copy paste error. I already
          updated the article and the correct one should be:</p>

          <p>python3 convert.py models/lora</p>

          '
        raw: 'My bad, the second step command was a copy paste error. I already updated
          the article and the correct one should be:


          python3 convert.py models/lora'
        updatedAt: '2023-09-19T02:56:12.790Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RadarSISA
    id: 65090dccf75ac8c6f96b5ad0
    type: comment
  author: zbruceli
  content: 'My bad, the second step command was a copy paste error. I already updated
    the article and the correct one should be:


    python3 convert.py models/lora'
  created_at: 2023-09-19 01:56:12+00:00
  edited: false
  hidden: false
  id: 65090dccf75ac8c6f96b5ad0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
      fullname: RadarTeam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RadarSISA
      type: user
    createdAt: '2023-09-19T17:39:11.000Z'
    data:
      edited: true
      editors:
      - RadarSISA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7077158093452454
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
          fullname: RadarTeam
          isHf: false
          isPro: false
          name: RadarSISA
          type: user
        html: "<p><a rel=\"nofollow\" href=\"https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb\"\
          >https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb</a><br><span\
          \ data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span\
          \ class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ after this how to push it to hub?</p>\n"
        raw: 'https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb

          @RonanMcGovern after this how to push it to hub?'
        updatedAt: '2023-09-19T17:59:54.296Z'
      numEdits: 1
      reactions: []
    id: 6509dcbf2257a3afbae899ee
    type: comment
  author: RadarSISA
  content: 'https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb

    @RonanMcGovern after this how to push it to hub?'
  created_at: 2023-09-19 16:39:11+00:00
  edited: true
  hidden: false
  id: 6509dcbf2257a3afbae899ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
      fullname: RadarTeam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RadarSISA
      type: user
    createdAt: '2023-09-19T19:39:08.000Z'
    data:
      edited: false
      editors:
      - RadarSISA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8110934495925903
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
          fullname: RadarTeam
          isHf: false
          isPro: false
          name: RadarSISA
          type: user
        html: '<p>how to solve this error  ?<br>llama_model_quantize: failed to quantize:
          failed to open ./ggml-model-f16.bin: No such file or directory</p>

          '
        raw: 'how to solve this error  ?

          llama_model_quantize: failed to quantize: failed to open ./ggml-model-f16.bin:
          No such file or directory


          '
        updatedAt: '2023-09-19T19:39:08.119Z'
      numEdits: 0
      reactions: []
    id: 6509f8dc6fb511ba9e9842b8
    type: comment
  author: RadarSISA
  content: 'how to solve this error  ?

    llama_model_quantize: failed to quantize: failed to open ./ggml-model-f16.bin:
    No such file or directory


    '
  created_at: 2023-09-19 18:39:08+00:00
  edited: false
  hidden: false
  id: 6509f8dc6fb511ba9e9842b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
      fullname: RadarTeam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RadarSISA
      type: user
    createdAt: '2023-09-19T20:55:04.000Z'
    data:
      edited: false
      editors:
      - RadarSISA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7993208169937134
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
          fullname: RadarTeam
          isHf: false
          isPro: false
          name: RadarSISA
          type: user
        html: '<p>Please anyone tell how to push to hub after this<br><a rel="nofollow"
          href="https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb">https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb</a></p>

          '
        raw: 'Please anyone tell how to push to hub after this

          https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb'
        updatedAt: '2023-09-19T20:55:04.861Z'
      numEdits: 0
      reactions: []
    id: 650a0aa8a6f657faa42a6d5a
    type: comment
  author: RadarSISA
  content: 'Please anyone tell how to push to hub after this

    https://github.com/TrelisResearch/gguf-quantization/blob/main/HuggingFace_to_GGUF.ipynb'
  created_at: 2023-09-19 19:55:04+00:00
  edited: false
  hidden: false
  id: 650a0aa8a6f657faa42a6d5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-20T08:24:51.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8616702556610107
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>It's kind of messy <span data-props=\"{&quot;user&quot;:&quot;RadarSISA&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RadarSISA\"\
          >@<span class=\"underline\">RadarSISA</span></a></span>\n\n\t</span></span>\
          \ as I don't believe you can use the push_to_hub command.</p>\n<p>It is\
          \ possible by connecting to the repo using git or using the huggingface\
          \ libraries here: <a href=\"https://huggingface.co/docs/huggingface_hub/v0.16.3/guides/upload\"\
          >https://huggingface.co/docs/huggingface_hub/v0.16.3/guides/upload</a></p>\n"
        raw: 'It''s kind of messy @RadarSISA as I don''t believe you can use the push_to_hub
          command.


          It is possible by connecting to the repo using git or using the huggingface
          libraries here: https://huggingface.co/docs/huggingface_hub/v0.16.3/guides/upload'
        updatedAt: '2023-09-20T08:24:51.931Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RadarSISA
    id: 650aac53f99720fea159a893
    type: comment
  author: RonanMcGovern
  content: 'It''s kind of messy @RadarSISA as I don''t believe you can use the push_to_hub
    command.


    It is possible by connecting to the repo using git or using the huggingface libraries
    here: https://huggingface.co/docs/huggingface_hub/v0.16.3/guides/upload'
  created_at: 2023-09-20 07:24:51+00:00
  edited: false
  hidden: false
  id: 650aac53f99720fea159a893
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
      fullname: RadarTeam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RadarSISA
      type: user
    createdAt: '2023-09-21T02:59:49.000Z'
    data:
      edited: false
      editors:
      - RadarSISA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6328124403953552
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
          fullname: RadarTeam
          isHf: false
          isPro: false
          name: RadarSISA
          type: user
        html: "<p>please any one tell, after making gguf file my models directory\
          \ has following files.</p>\n<p>ggml-vocab-llama.gguf<br>generation_config.json<br>pytorch_model-00001-of-00002.bin<br>pytorch_model-00002-of-00002.bin<br>pytorch_model.bin.index.json<br>tokenizer.model<br>ggml-model-f16.gguf<br>config.json<br>ggml-model.gguf</p>\n\
          <p>what are the necessary files for inferencing ? Actually it is taking\
          \ same amount of RAM as without gguf. And how to do inferencing. I'm using\
          \ following code for inferencing.</p>\n<p>from transformers import pipeline<br>pipe\
          \ = pipeline(\"text-generation\", model=\"/content/drive/MyDrive/my_llama_cpp/llama.cpp/models\"\
          )<br>user_prompt = \"What is a SISA Radar?\"<br>system_prompt = \"You are\
          \ a knowledgeable and helpful AI assistant at SISA Information Security\
          \ Private Limited.\"<br>result = pipe(f\"<s>[INST] &lt;&gt; {system_prompt}\
          \ &lt;&gt; {user_prompt} [/INST]\")<br>generated_text = result[0]['generated_text']<br>print(generated_text)</s></p><s>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span\
          \ class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;zbruceli&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zbruceli\">@<span class=\"\
          underline\">zbruceli</span></a></span>\n\n\t</span></span> </p>\n</s>"
        raw: 'please any one tell, after making gguf file my models directory has
          following files.


          ggml-vocab-llama.gguf

          generation_config.json

          pytorch_model-00001-of-00002.bin

          pytorch_model-00002-of-00002.bin

          pytorch_model.bin.index.json

          tokenizer.model

          ggml-model-f16.gguf

          config.json

          ggml-model.gguf


          what are the necessary files for inferencing ? Actually it is taking same
          amount of RAM as without gguf. And how to do inferencing. I''m using following
          code for inferencing.


          from transformers import pipeline

          pipe = pipeline("text-generation", model="/content/drive/MyDrive/my_llama_cpp/llama.cpp/models")

          user_prompt = "What is a SISA Radar?"

          system_prompt = "You are a knowledgeable and helpful AI assistant at SISA
          Information Security Private Limited."

          result = pipe(f"<s>[INST] <<SYS>> {system_prompt} <</SYS>> {user_prompt}
          [/INST]")

          generated_text = result[0][''generated_text'']

          print(generated_text)


          @RonanMcGovern @zbruceli '
        updatedAt: '2023-09-21T02:59:49.081Z'
      numEdits: 0
      reactions: []
    id: 650bb1a54edee1d630411479
    type: comment
  author: RadarSISA
  content: 'please any one tell, after making gguf file my models directory has following
    files.


    ggml-vocab-llama.gguf

    generation_config.json

    pytorch_model-00001-of-00002.bin

    pytorch_model-00002-of-00002.bin

    pytorch_model.bin.index.json

    tokenizer.model

    ggml-model-f16.gguf

    config.json

    ggml-model.gguf


    what are the necessary files for inferencing ? Actually it is taking same amount
    of RAM as without gguf. And how to do inferencing. I''m using following code for
    inferencing.


    from transformers import pipeline

    pipe = pipeline("text-generation", model="/content/drive/MyDrive/my_llama_cpp/llama.cpp/models")

    user_prompt = "What is a SISA Radar?"

    system_prompt = "You are a knowledgeable and helpful AI assistant at SISA Information
    Security Private Limited."

    result = pipe(f"<s>[INST] <<SYS>> {system_prompt} <</SYS>> {user_prompt} [/INST]")

    generated_text = result[0][''generated_text'']

    print(generated_text)


    @RonanMcGovern @zbruceli '
  created_at: 2023-09-21 01:59:49+00:00
  edited: false
  hidden: false
  id: 650bb1a54edee1d630411479
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
      fullname: RadarTeam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RadarSISA
      type: user
    createdAt: '2023-09-21T03:01:28.000Z'
    data:
      edited: false
      editors:
      - RadarSISA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5918246507644653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c7fae0888782568a0de674eacf9812b.svg
          fullname: RadarTeam
          isHf: false
          isPro: false
          name: RadarSISA
          type: user
        html: "<p>please any one tell, after making gguf file my models directory\
          \ has following files.</p>\n<p>ggml-vocab-llama.gguf<br>generation_config.json<br>pytorch_model-00001-of-00002.bin<br>pytorch_model-00002-of-00002.bin<br>pytorch_model.bin.index.json<br>tokenizer.model<br>ggml-model-f16.gguf<br>config.json<br>ggml-model.gguf</p>\n\
          <p>what are the necessary files for inferencing ? Actually it is taking\
          \ same amount of RAM as without gguf. And how to do inferencing. I'm using\
          \ following code for inferencing.</p>\n<p>from transformers import pipeline<br>pipe\
          \ = pipeline(\"text-generation\", model=\"/content/drive/MyDrive/my_llama_cpp/llama.cpp/models\"\
          )</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;zbruceli&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zbruceli\">@<span class=\"\
          underline\">zbruceli</span></a></span>\n\n\t</span></span> </p>\n"
        raw: 'please any one tell, after making gguf file my models directory has
          following files.


          ggml-vocab-llama.gguf

          generation_config.json

          pytorch_model-00001-of-00002.bin

          pytorch_model-00002-of-00002.bin

          pytorch_model.bin.index.json

          tokenizer.model

          ggml-model-f16.gguf

          config.json

          ggml-model.gguf


          what are the necessary files for inferencing ? Actually it is taking same
          amount of RAM as without gguf. And how to do inferencing. I''m using following
          code for inferencing.


          from transformers import pipeline

          pipe = pipeline("text-generation", model="/content/drive/MyDrive/my_llama_cpp/llama.cpp/models")


          @RonanMcGovern @zbruceli '
        updatedAt: '2023-09-21T03:01:28.143Z'
      numEdits: 0
      reactions: []
    id: 650bb208a158b41c71e0a5db
    type: comment
  author: RadarSISA
  content: 'please any one tell, after making gguf file my models directory has following
    files.


    ggml-vocab-llama.gguf

    generation_config.json

    pytorch_model-00001-of-00002.bin

    pytorch_model-00002-of-00002.bin

    pytorch_model.bin.index.json

    tokenizer.model

    ggml-model-f16.gguf

    config.json

    ggml-model.gguf


    what are the necessary files for inferencing ? Actually it is taking same amount
    of RAM as without gguf. And how to do inferencing. I''m using following code for
    inferencing.


    from transformers import pipeline

    pipe = pipeline("text-generation", model="/content/drive/MyDrive/my_llama_cpp/llama.cpp/models")


    @RonanMcGovern @zbruceli '
  created_at: 2023-09-21 02:01:28+00:00
  edited: false
  hidden: false
  id: 650bb208a158b41c71e0a5db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-09-23T05:35:34.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.702640950679779
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RadarSISA&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/RadarSISA\">@<span class=\"\
          underline\">RadarSISA</span></a></span>\n\n\t</span></span> I use llama.cpp\
          \ for inference, therefore I converted f16 gguf into q4.0 gguf. More details\
          \ in my blog article<br><a rel=\"nofollow\" href=\"https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop\"\
          >https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop</a></p>\n"
        raw: '@RadarSISA I use llama.cpp for inference, therefore I converted f16
          gguf into q4.0 gguf. More details in my blog article

          https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop'
        updatedAt: '2023-09-23T05:35:34.607Z'
      numEdits: 0
      reactions: []
    id: 650e792637698373cde6bae0
    type: comment
  author: zbruceli
  content: '@RadarSISA I use llama.cpp for inference, therefore I converted f16 gguf
    into q4.0 gguf. More details in my blog article

    https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop'
  created_at: 2023-09-23 04:35:34+00:00
  edited: false
  hidden: false
  id: 650e792637698373cde6bae0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/97b8dc2e2d0086abff24682b0f0af39a.svg
      fullname: Simon Strandgaard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neoneye
      type: user
    createdAt: '2023-09-26T18:16:04.000Z'
    data:
      edited: false
      editors:
      - neoneye
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6914376616477966
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/97b8dc2e2d0086abff24682b0f0af39a.svg
          fullname: Simon Strandgaard
          isHf: false
          isPro: false
          name: neoneye
          type: user
        html: "<p>I came across this discussion because I was experimenting fine tuning\
          \ the llama2 model, and is now having some <code>.bin</code> files, that\
          \ I want to convert to <code>.gguf</code> file format.</p>\n<pre><code class=\"\
          language-bash\">PROMPT&gt; <span class=\"hljs-built_in\">pwd</span>\n/Users/username/git\n\
          PROMPT&gt; git <span class=\"hljs-built_in\">clone</span> https://huggingface.co/neoneye/llama-2-7b-simonsolver\n\
          </code></pre>\n<p>I also had the problem with a missing <code>tokenizer.model</code>\
          \ file.<br>I downloaded the <code>tokenizer.model</code> from the original\
          \ model <a href=\"https://huggingface.co/NousResearch/Llama-2-7b-chat-hf/tree/main\"\
          >Llama-2-7b-chat-hf</a>, and place it inside my own fine tuned model <code>llama-2-7b-simonsolver</code></p>\n\
          <pre><code class=\"language-bash\">PROMPT&gt; <span class=\"hljs-built_in\"\
          >ls</span>\nllama-2-7b-simonsolver\nllama.cpp\nPROMPT&gt; <span class=\"\
          hljs-built_in\">cd</span> llama-2-7b-simonsolver\nPROMPT&gt; python3 ../llama.cpp/convert.py\
          \ ./ --outtype f16\nLoading model file pytorch_model-00001-of-00002.bin\n\
          Loading model file pytorch_model-00001-of-00002.bin\nLoading model file\
          \ pytorch_model-00002-of-00002.bin\n\u2026 snip \u2026\nWrote ggml-model-f16.gguf\n\
          PROMPT&gt; <span class=\"hljs-built_in\">ls</span> -la ggml-model-f16.gguf\
          \ \n13gb ggml-model-f16.gguf\n\nPROMPT&gt; <span class=\"hljs-built_in\"\
          >cd</span> /Users/username/git\nPROMPT&gt; ./llama.cpp/server --model llama-2-7b-simonsolver/ggml-model-f16.gguf\n\
          llama server listening at http://127.0.0.1:8080\n</code></pre>\n<p>Using\
          \ the llama.cpp web ui, I can verify that the llama2 indeed has learned\
          \ several things from the fine tuning.</p>\n<p>My hello world fine tuned\
          \ model is here, <a href=\"https://huggingface.co/neoneye/llama-2-7b-simonsolver\"\
          >llama-2-7b-simonsolver</a>.</p>\n<p>Also huge thanks to <span data-props=\"\
          {&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span class=\"underline\"\
          >RonanMcGovern</span></a></span>\n\n\t</span></span> for great videos about\
          \ fine tuning.</p>\n"
        raw: "I came across this discussion because I was experimenting fine tuning\
          \ the llama2 model, and is now having some `.bin` files, that I want to\
          \ convert to `.gguf` file format.\n\n\n```bash\nPROMPT> pwd\n/Users/username/git\n\
          PROMPT> git clone https://huggingface.co/neoneye/llama-2-7b-simonsolver\n\
          ```\n\nI also had the problem with a missing `tokenizer.model` file.\nI\
          \ downloaded the `tokenizer.model` from the original model [Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf/tree/main),\
          \ and place it inside my own fine tuned model `llama-2-7b-simonsolver`\n\
          \n```bash\nPROMPT> ls\nllama-2-7b-simonsolver\nllama.cpp\nPROMPT> cd llama-2-7b-simonsolver\n\
          PROMPT> python3 ../llama.cpp/convert.py ./ --outtype f16\nLoading model\
          \ file pytorch_model-00001-of-00002.bin\nLoading model file pytorch_model-00001-of-00002.bin\n\
          Loading model file pytorch_model-00002-of-00002.bin\n\u2026 snip \u2026\n\
          Wrote ggml-model-f16.gguf\nPROMPT> ls -la ggml-model-f16.gguf \n13gb ggml-model-f16.gguf\n\
          \nPROMPT> cd /Users/username/git\nPROMPT> ./llama.cpp/server --model llama-2-7b-simonsolver/ggml-model-f16.gguf\n\
          llama server listening at http://127.0.0.1:8080\n```\n\nUsing the llama.cpp\
          \ web ui, I can verify that the llama2 indeed has learned several things\
          \ from the fine tuning.\n\nMy hello world fine tuned model is here, [llama-2-7b-simonsolver](https://huggingface.co/neoneye/llama-2-7b-simonsolver).\n\
          \nAlso huge thanks to @RonanMcGovern for great videos about fine tuning."
        updatedAt: '2023-09-26T18:16:04.353Z'
      numEdits: 0
      reactions: []
    id: 65131fe492a52e002ac1ede0
    type: comment
  author: neoneye
  content: "I came across this discussion because I was experimenting fine tuning\
    \ the llama2 model, and is now having some `.bin` files, that I want to convert\
    \ to `.gguf` file format.\n\n\n```bash\nPROMPT> pwd\n/Users/username/git\nPROMPT>\
    \ git clone https://huggingface.co/neoneye/llama-2-7b-simonsolver\n```\n\nI also\
    \ had the problem with a missing `tokenizer.model` file.\nI downloaded the `tokenizer.model`\
    \ from the original model [Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf/tree/main),\
    \ and place it inside my own fine tuned model `llama-2-7b-simonsolver`\n\n```bash\n\
    PROMPT> ls\nllama-2-7b-simonsolver\nllama.cpp\nPROMPT> cd llama-2-7b-simonsolver\n\
    PROMPT> python3 ../llama.cpp/convert.py ./ --outtype f16\nLoading model file pytorch_model-00001-of-00002.bin\n\
    Loading model file pytorch_model-00001-of-00002.bin\nLoading model file pytorch_model-00002-of-00002.bin\n\
    \u2026 snip \u2026\nWrote ggml-model-f16.gguf\nPROMPT> ls -la ggml-model-f16.gguf\
    \ \n13gb ggml-model-f16.gguf\n\nPROMPT> cd /Users/username/git\nPROMPT> ./llama.cpp/server\
    \ --model llama-2-7b-simonsolver/ggml-model-f16.gguf\nllama server listening at\
    \ http://127.0.0.1:8080\n```\n\nUsing the llama.cpp web ui, I can verify that\
    \ the llama2 indeed has learned several things from the fine tuning.\n\nMy hello\
    \ world fine tuned model is here, [llama-2-7b-simonsolver](https://huggingface.co/neoneye/llama-2-7b-simonsolver).\n\
    \nAlso huge thanks to @RonanMcGovern for great videos about fine tuning."
  created_at: 2023-09-26 17:16:04+00:00
  edited: false
  hidden: false
  id: 65131fe492a52e002ac1ede0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
      fullname: Viorel Mirea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmirea
      type: user
    createdAt: '2023-10-25T13:57:10.000Z'
    data:
      edited: false
      editors:
      - vmirea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39497584104537964
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
          fullname: Viorel Mirea
          isHf: false
          isPro: false
          name: vmirea
          type: user
        html: '<p>How can I run the convert on the fine tuned model?</p>

          <p>(pt_source_2.0.1_cu12.2.1_535.86.10_cudnn8.9.5.29_intelpy310) vmirea@vmirea-Z390-GAMING-SLI:/media/vmirea/NTFS_8TB/projects/llama/llama-recipes/llm_qlora$
          ls -la models/open_llama_7b_qlora_uncensored_adapter<br>total 49245<br>drwxrwxrwx
          1 vmirea vmirea     4096 Oct 25 14:37 .<br>drwxrwxrwx 1 vmirea vmirea        0
          Oct 25 13:03 ..<br>-rwxrwxrwx 1 vmirea vmirea      470 Oct 25 12:54 adapter_config.json<br>-rwxrwxrwx
          1 vmirea vmirea 25234701 Oct 25 12:54 adapter_model.bin<br>-rwxrwxrwx 1
          vmirea vmirea 25178112 Oct 25 14:37 ggml-adapter-model.bin<br>-rwxrwxrwx
          1 vmirea vmirea      853 Oct 25 12:54 README.md<br>-rwxrwxrwx 1 vmirea vmirea     4091
          Oct 25 12:54 training_args.bin<br>(pt_source_2.0.1_cu12.2.1_535.86.10_cudnn8.9.5.29_intelpy310)
          vmirea@vmirea-Z390-GAMING-SLI:/media/vmirea/NTFS_8TB/projects/llama/llama-recipes/llm_qlora$
          python /media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py models/open_llama_7b_qlora_uncensored_adapter/adapter_model.bin<br>Loading
          model file models/open_llama_7b_qlora_uncensored_adapter/adapter_model.bin<br>Traceback
          (most recent call last):<br>  File "/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py",
          line 1208, in <br>    main()<br>  File "/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py",
          line 1157, in main<br>    params = Params.load(model_plus)<br>  File "/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py",
          line 292, in load<br>    params = Params.guessed(model_plus.model)<br>  File
          "/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py", line 166, in guessed<br>    n_vocab,
          n_embd = model["model.embed_tokens.weight"].shape if "model.embed_tokens.weight"
          in model else model["tok_embeddings.weight"].shape<br>KeyError: ''tok_embeddings.weight''</p>

          '
        raw: "How can I run the convert on the fine tuned model?\n\n(pt_source_2.0.1_cu12.2.1_535.86.10_cudnn8.9.5.29_intelpy310)\
          \ vmirea@vmirea-Z390-GAMING-SLI:/media/vmirea/NTFS_8TB/projects/llama/llama-recipes/llm_qlora$\
          \ ls -la models/open_llama_7b_qlora_uncensored_adapter\ntotal 49245\ndrwxrwxrwx\
          \ 1 vmirea vmirea     4096 Oct 25 14:37 .\ndrwxrwxrwx 1 vmirea vmirea  \
          \      0 Oct 25 13:03 ..\n-rwxrwxrwx 1 vmirea vmirea      470 Oct 25 12:54\
          \ adapter_config.json\n-rwxrwxrwx 1 vmirea vmirea 25234701 Oct 25 12:54\
          \ adapter_model.bin\n-rwxrwxrwx 1 vmirea vmirea 25178112 Oct 25 14:37 ggml-adapter-model.bin\n\
          -rwxrwxrwx 1 vmirea vmirea      853 Oct 25 12:54 README.md\n-rwxrwxrwx 1\
          \ vmirea vmirea     4091 Oct 25 12:54 training_args.bin\n(pt_source_2.0.1_cu12.2.1_535.86.10_cudnn8.9.5.29_intelpy310)\
          \ vmirea@vmirea-Z390-GAMING-SLI:/media/vmirea/NTFS_8TB/projects/llama/llama-recipes/llm_qlora$\
          \ python /media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py models/open_llama_7b_qlora_uncensored_adapter/adapter_model.bin\
          \ \nLoading model file models/open_llama_7b_qlora_uncensored_adapter/adapter_model.bin\n\
          Traceback (most recent call last):\n  File \"/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\"\
          , line 1208, in <module>\n    main()\n  File \"/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\"\
          , line 1157, in main\n    params = Params.load(model_plus)\n  File \"/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\"\
          , line 292, in load\n    params = Params.guessed(model_plus.model)\n  File\
          \ \"/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\", line 166, in\
          \ guessed\n    n_vocab, n_embd = model[\"model.embed_tokens.weight\"].shape\
          \ if \"model.embed_tokens.weight\" in model else model[\"tok_embeddings.weight\"\
          ].shape\nKeyError: 'tok_embeddings.weight'\n"
        updatedAt: '2023-10-25T13:57:10.195Z'
      numEdits: 0
      reactions: []
    id: 65391eb66913a31c5ff713bd
    type: comment
  author: vmirea
  content: "How can I run the convert on the fine tuned model?\n\n(pt_source_2.0.1_cu12.2.1_535.86.10_cudnn8.9.5.29_intelpy310)\
    \ vmirea@vmirea-Z390-GAMING-SLI:/media/vmirea/NTFS_8TB/projects/llama/llama-recipes/llm_qlora$\
    \ ls -la models/open_llama_7b_qlora_uncensored_adapter\ntotal 49245\ndrwxrwxrwx\
    \ 1 vmirea vmirea     4096 Oct 25 14:37 .\ndrwxrwxrwx 1 vmirea vmirea        0\
    \ Oct 25 13:03 ..\n-rwxrwxrwx 1 vmirea vmirea      470 Oct 25 12:54 adapter_config.json\n\
    -rwxrwxrwx 1 vmirea vmirea 25234701 Oct 25 12:54 adapter_model.bin\n-rwxrwxrwx\
    \ 1 vmirea vmirea 25178112 Oct 25 14:37 ggml-adapter-model.bin\n-rwxrwxrwx 1 vmirea\
    \ vmirea      853 Oct 25 12:54 README.md\n-rwxrwxrwx 1 vmirea vmirea     4091\
    \ Oct 25 12:54 training_args.bin\n(pt_source_2.0.1_cu12.2.1_535.86.10_cudnn8.9.5.29_intelpy310)\
    \ vmirea@vmirea-Z390-GAMING-SLI:/media/vmirea/NTFS_8TB/projects/llama/llama-recipes/llm_qlora$\
    \ python /media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py models/open_llama_7b_qlora_uncensored_adapter/adapter_model.bin\
    \ \nLoading model file models/open_llama_7b_qlora_uncensored_adapter/adapter_model.bin\n\
    Traceback (most recent call last):\n  File \"/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\"\
    , line 1208, in <module>\n    main()\n  File \"/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\"\
    , line 1157, in main\n    params = Params.load(model_plus)\n  File \"/media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\"\
    , line 292, in load\n    params = Params.guessed(model_plus.model)\n  File \"\
    /media/vmirea/NTFS_8TB/projects/llama.cpp/convert.py\", line 166, in guessed\n\
    \    n_vocab, n_embd = model[\"model.embed_tokens.weight\"].shape if \"model.embed_tokens.weight\"\
    \ in model else model[\"tok_embeddings.weight\"].shape\nKeyError: 'tok_embeddings.weight'\n"
  created_at: 2023-10-25 12:57:10+00:00
  edited: false
  hidden: false
  id: 65391eb66913a31c5ff713bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665398834110-noauth.png?w=200&h=200&f=face
      fullname: Zhang ning
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pe65374
      type: user
    createdAt: '2023-10-27T14:02:20.000Z'
    data:
      edited: false
      editors:
      - pe65374
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9478263258934021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665398834110-noauth.png?w=200&h=200&f=face
          fullname: Zhang ning
          isHf: false
          isPro: false
          name: pe65374
          type: user
        html: '<blockquote>

          <p>There''s a script included with llama.cpp that does everything for you.
          It''s called <code>make-ggml.py</code>. It''s based off an old Python script
          I used to produce my GGML models with.</p>

          </blockquote>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py">https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py</a><br>I
          guess that''s the script. Thanks.</p>

          '
        raw: '> There''s a script included with llama.cpp that does everything for
          you. It''s called `make-ggml.py`. It''s based off an old Python script I
          used to produce my GGML models with.


          https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py

          I guess that''s the script. Thanks.'
        updatedAt: '2023-10-27T14:02:20.164Z'
      numEdits: 0
      reactions: []
    id: 653bc2ec3630cec3eb509fca
    type: comment
  author: pe65374
  content: '> There''s a script included with llama.cpp that does everything for you.
    It''s called `make-ggml.py`. It''s based off an old Python script I used to produce
    my GGML models with.


    https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py

    I guess that''s the script. Thanks.'
  created_at: 2023-10-27 13:02:20+00:00
  edited: false
  hidden: false
  id: 653bc2ec3630cec3eb509fca
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: How to convert model into GGML format?
