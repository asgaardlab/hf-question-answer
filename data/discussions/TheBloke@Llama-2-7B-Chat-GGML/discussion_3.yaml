!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mike-ravkine
conflicting_files: null
created_at: 2023-07-18 21:44:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-18T22:44:25.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.93147212266922
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<p>Based on reverse engineering <a rel=\"nofollow\" href=\"https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212\"\
          >https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212</a>\
          \ here's what I've come up with:</p>\n<pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n\
          You are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible, while being safe.  Your answers should not include any harmful,\
          \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please\
          \ ensure that your responses are socially unbiased and positive in nature.\n\
          \nIf a question does not make any sense, or is not factually coherent, explain\
          \ why instead of answering something not correct. If you don't know the\
          \ answer to a question, please don't share false information.\n&lt;&lt;/SYS&gt;&gt;\n\
          \n [/INST]&lt;/s&gt;\n&lt;s&gt;[INST] {{prompt}} [/INST]\n</code></pre>\n\
          <p>It seems to work but holy hell is this 'chat' finetune lobotomized -\
          \ it will take any excuse it can find to refuse any and all requests.</p>\n\
          <p>I have also run some testing with the system prompt changed to:</p>\n\
          <pre><code>You are a helpful, respectful and honest assistant. Always answer\
          \ as helpfully as possible.\n\nIf a question does not make any sense, or\
          \ is not factually coherent, explain why instead of answering something\
          \ not correct. If you don't know the answer to a question, please don't\
          \ share false information.\n</code></pre>\n<p>This helps quite a bit with\
          \ the refusals, instead of a flat out \"this is unethical\" it will complain\
          \ a bit and then answer (what it imagines to be) a safer version of your\
          \ question. </p>\n"
        raw: "Based on reverse engineering https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212\
          \ here's what I've come up with:\r\n\r\n```\r\n<s>[INST] <<SYS>>\r\nYou\
          \ are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible, while being safe.  Your answers should not include any harmful,\
          \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please\
          \ ensure that your responses are socially unbiased and positive in nature.\r\
          \n\r\nIf a question does not make any sense, or is not factually coherent,\
          \ explain why instead of answering something not correct. If you don't know\
          \ the answer to a question, please don't share false information.\r\n<</SYS>>\r\
          \n\r\n [/INST]</s>\r\n<s>[INST] {{prompt}} [/INST]\r\n```\r\n\r\nIt seems\
          \ to work but holy hell is this 'chat' finetune lobotomized - it will take\
          \ any excuse it can find to refuse any and all requests.\r\n\r\nI have also\
          \ run some testing with the system prompt changed to:\r\n\r\n```\r\nYou\
          \ are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible.\r\n\r\nIf a question does not make any sense, or is not factually\
          \ coherent, explain why instead of answering something not correct. If you\
          \ don't know the answer to a question, please don't share false information.\r\
          \n```\r\n\r\nThis helps quite a bit with the refusals, instead of a flat\
          \ out \"this is unethical\" it will complain a bit and then answer (what\
          \ it imagines to be) a safer version of your question. "
        updatedAt: '2023-07-18T22:44:25.376Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - hmmhmmhm
        - Bon-Fire
        - chrislemke
        - jig-san
        - sujantkumarkv
        - zliu
    id: 64b715c932403871593b3d54
    type: comment
  author: mike-ravkine
  content: "Based on reverse engineering https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212\
    \ here's what I've come up with:\r\n\r\n```\r\n<s>[INST] <<SYS>>\r\nYou are a\
    \ helpful, respectful and honest assistant. Always answer as helpfully as possible,\
    \ while being safe.  Your answers should not include any harmful, unethical, racist,\
    \ sexist, toxic, dangerous, or illegal content. Please ensure that your responses\
    \ are socially unbiased and positive in nature.\r\n\r\nIf a question does not\
    \ make any sense, or is not factually coherent, explain why instead of answering\
    \ something not correct. If you don't know the answer to a question, please don't\
    \ share false information.\r\n<</SYS>>\r\n\r\n [/INST]</s>\r\n<s>[INST] {{prompt}}\
    \ [/INST]\r\n```\r\n\r\nIt seems to work but holy hell is this 'chat' finetune\
    \ lobotomized - it will take any excuse it can find to refuse any and all requests.\r\
    \n\r\nI have also run some testing with the system prompt changed to:\r\n\r\n\
    ```\r\nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
    \ as possible.\r\n\r\nIf a question does not make any sense, or is not factually\
    \ coherent, explain why instead of answering something not correct. If you don't\
    \ know the answer to a question, please don't share false information.\r\n```\r\
    \n\r\nThis helps quite a bit with the refusals, instead of a flat out \"this is\
    \ unethical\" it will complain a bit and then answer (what it imagines to be)\
    \ a safer version of your question. "
  created_at: 2023-07-18 21:44:25+00:00
  edited: false
  hidden: false
  id: 64b715c932403871593b3d54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-07-18T22:58:57.000Z'
    data:
      edited: true
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9384511113166809
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: "<p>That is similar to my conclusion about the format, but as far as\
          \ my understanding of the code goes the system message is attached to the\
          \ first prompt, rather than standing on it's own. Meaning the first [INST]\
          \ block contains both the system message and the first instruction. Then\
          \ all subsequent [INST] blocks contain only the instruction.</p>\n<p>That\
          \ also matches with the understanding that <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1640795572\"\
          >asgeir</a> and <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1641063329\"\
          >jxy</a> independently came up with which suggest it is probably correct.</p>\n\
          <p>Either way the current prompt template displayed in the model card is\
          \ definitively wrong. <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ it would be nice if you could replace it quickly since there will be a\
          \ lot of people trying out these models right now.</p>\n"
        raw: 'That is similar to my conclusion about the format, but as far as my
          understanding of the code goes the system message is attached to the first
          prompt, rather than standing on it''s own. Meaning the first [INST] block
          contains both the system message and the first instruction. Then all subsequent
          [INST] blocks contain only the instruction.


          That also matches with the understanding that [asgeir](https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1640795572)
          and [jxy](https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1641063329)
          independently came up with which suggest it is probably correct.


          Either way the current prompt template displayed in the model card is definitively
          wrong. @TheBloke it would be nice if you could replace it quickly since
          there will be a lot of people trying out these models right now.'
        updatedAt: '2023-07-19T00:59:24.091Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Bitbob
    id: 64b71931bdf378973073333c
    type: comment
  author: Mikael110
  content: 'That is similar to my conclusion about the format, but as far as my understanding
    of the code goes the system message is attached to the first prompt, rather than
    standing on it''s own. Meaning the first [INST] block contains both the system
    message and the first instruction. Then all subsequent [INST] blocks contain only
    the instruction.


    That also matches with the understanding that [asgeir](https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1640795572)
    and [jxy](https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1641063329)
    independently came up with which suggest it is probably correct.


    Either way the current prompt template displayed in the model card is definitively
    wrong. @TheBloke it would be nice if you could replace it quickly since there
    will be a lot of people trying out these models right now.'
  created_at: 2023-07-18 21:58:57+00:00
  edited: true
  hidden: false
  id: 64b71931bdf378973073333c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-18T23:25:41.000Z'
    data:
      edited: true
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8870164752006531
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<p>GIST: <a rel=\"nofollow\" href=\"https://gist.github.com/the-crypt-keeper/8d781a12ee515903edc89ef69383570f\"\
          >https://gist.github.com/the-crypt-keeper/8d781a12ee515903edc89ef69383570f</a></p>\n\
          <p>Result for a single <code>&lt;prompt&gt;</code> input:</p>\n<pre><code>&lt;s&gt;[INST]\
          \ &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful and honest assistant.\
          \ Always answer as helpfully as possible, while being safe.  Your answers\
          \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature.\n\nIf a question does not make any sense, or is\
          \ not factually coherent, explain why instead of answering something not\
          \ correct. If you don't know the answer to a question, please don't share\
          \ false information.\n&lt;&lt;/SYS&gt;&gt;\n\n&lt;prompt&gt; [/INST]\n</code></pre>\n\
          <p>Result for a <code>&lt;prompt&gt;</code> then assistant says <code>&lt;answer&gt;</code>\
          \ then user follows up with <code>&lt;prompt-second&gt;</code>:</p>\n<pre><code>&lt;s&gt;[INST]\
          \ &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful and honest assistant.\
          \ Always answer as helpfully as possible, while being safe.  Your answers\
          \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature.\n\nIf a question does not make any sense, or is\
          \ not factually coherent, explain why instead of answering something not\
          \ correct. If you don't know the answer to a question, please don't share\
          \ false information.\n&lt;&lt;/SYS&gt;&gt;\n\n&lt;prompt&gt; [/INST] &lt;answer&gt;\
          \ &lt;/s&gt;&lt;s&gt;[INST] &lt;prompt-second&gt; [/INST]\n</code></pre>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;Mikael110&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Mikael110\">@<span class=\"\
          underline\">Mikael110</span></a></span>\n\n\t</span></span> thanks.</p>\n"
        raw: 'GIST: https://gist.github.com/the-crypt-keeper/8d781a12ee515903edc89ef69383570f


          Result for a single `<prompt>` input:


          ```

          <s>[INST] <<SYS>>

          You are a helpful, respectful and honest assistant. Always answer as helpfully
          as possible, while being safe.  Your answers should not include any harmful,
          unethical, racist, sexist, toxic, dangerous, or illegal content. Please
          ensure that your responses are socially unbiased and positive in nature.


          If a question does not make any sense, or is not factually coherent, explain
          why instead of answering something not correct. If you don''t know the answer
          to a question, please don''t share false information.

          <</SYS>>


          <prompt> [/INST]

          ```


          Result for a `<prompt>` then assistant says `<answer>` then user follows
          up with `<prompt-second>`:


          ```

          <s>[INST] <<SYS>>

          You are a helpful, respectful and honest assistant. Always answer as helpfully
          as possible, while being safe.  Your answers should not include any harmful,
          unethical, racist, sexist, toxic, dangerous, or illegal content. Please
          ensure that your responses are socially unbiased and positive in nature.


          If a question does not make any sense, or is not factually coherent, explain
          why instead of answering something not correct. If you don''t know the answer
          to a question, please don''t share false information.

          <</SYS>>


          <prompt> [/INST] <answer> </s><s>[INST] <prompt-second> [/INST]

          ```


          @Mikael110 thanks.'
        updatedAt: '2023-07-18T23:26:01.517Z'
      numEdits: 1
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - limcheekin
        - viniciusarruda
        - omasoud
        - xiaol
        - sujantkumarkv
        - zhibinlu
        - rayleizhu
    id: 64b71f7588b86014d7e2dd71
    type: comment
  author: mike-ravkine
  content: 'GIST: https://gist.github.com/the-crypt-keeper/8d781a12ee515903edc89ef69383570f


    Result for a single `<prompt>` input:


    ```

    <s>[INST] <<SYS>>

    You are a helpful, respectful and honest assistant. Always answer as helpfully
    as possible, while being safe.  Your answers should not include any harmful, unethical,
    racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
    responses are socially unbiased and positive in nature.


    If a question does not make any sense, or is not factually coherent, explain why
    instead of answering something not correct. If you don''t know the answer to a
    question, please don''t share false information.

    <</SYS>>


    <prompt> [/INST]

    ```


    Result for a `<prompt>` then assistant says `<answer>` then user follows up with
    `<prompt-second>`:


    ```

    <s>[INST] <<SYS>>

    You are a helpful, respectful and honest assistant. Always answer as helpfully
    as possible, while being safe.  Your answers should not include any harmful, unethical,
    racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
    responses are socially unbiased and positive in nature.


    If a question does not make any sense, or is not factually coherent, explain why
    instead of answering something not correct. If you don''t know the answer to a
    question, please don''t share false information.

    <</SYS>>


    <prompt> [/INST] <answer> </s><s>[INST] <prompt-second> [/INST]

    ```


    @Mikael110 thanks.'
  created_at: 2023-07-18 22:25:41+00:00
  edited: true
  hidden: false
  id: 64b71f7588b86014d7e2dd71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-19T20:44:06.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7509762048721313
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: '<p>Created PR <a href="/TheBloke/Llama-2-7B-Chat-GGML/discussions/4">#4</a>.</p>

          '
        raw: 'Created PR #4.'
        updatedAt: '2023-07-19T20:44:06.272Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64b84b16cbb0af9bfb130591
    id: 64b84b16cbb0af9bfb13058d
    type: comment
  author: mike-ravkine
  content: 'Created PR #4.'
  created_at: 2023-07-19 19:44:06+00:00
  edited: false
  hidden: false
  id: 64b84b16cbb0af9bfb13058d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-19T20:44:06.000Z'
    data:
      status: closed
    id: 64b84b16cbb0af9bfb130591
    type: status-change
  author: mike-ravkine
  created_at: 2023-07-19 19:44:06+00:00
  id: 64b84b16cbb0af9bfb130591
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14af8bd7a590bd08a0098eafc9b774d9.svg
      fullname: Clay P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clayp
      type: user
    createdAt: '2023-07-19T21:45:15.000Z'
    data:
      edited: true
      editors:
      - clayp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9264116287231445
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14af8bd7a590bd08a0098eafc9b774d9.svg
          fullname: Clay P
          isHf: false
          isPro: false
          name: clayp
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mike-ravkine&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mike-ravkine\"\
          >@<span class=\"underline\">mike-ravkine</span></a></span>\n\n\t</span></span>\
          \ You're confident that there's no <code>&lt;/s&gt;</code> if it's only\
          \ a single prompt, is that right?</p>\n"
        raw: '@mike-ravkine You''re confident that there''s no ```</s>``` if it''s
          only a single prompt, is that right?'
        updatedAt: '2023-07-20T13:18:49.045Z'
      numEdits: 1
      reactions: []
    id: 64b8596b2f796fffbbef1aa8
    type: comment
  author: clayp
  content: '@mike-ravkine You''re confident that there''s no ```</s>``` if it''s only
    a single prompt, is that right?'
  created_at: 2023-07-19 20:45:15+00:00
  edited: true
  hidden: false
  id: 64b8596b2f796fffbbef1aa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
      fullname: Sven Heyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sven00
      type: user
    createdAt: '2023-07-20T07:06:12.000Z'
    data:
      edited: true
      editors:
      - Sven00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8359788060188293
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
          fullname: Sven Heyer
          isHf: false
          isPro: false
          name: Sven00
          type: user
        html: '<p>Do you know a good prompt (including SYS) in order to generate a
          concise/rephrased summary that captures the main points of a text based
          on?: <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</a>
          </p>

          <p>I would like to replicate the results I get from the demo: <a href="https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat">https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat</a></p>

          <p>There I only type "Summarize the following text: {text}" and it provides
          a great summary wile the same prompt with the template from above is not
          satisfying at all. </p>

          '
        raw: "Do you know a good prompt (including SYS) in order to generate a concise/rephrased\
          \ summary that captures the main points of a text based on?: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\
          \ \n\nI would like to replicate the results I get from the demo: https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat\n\
          \nThere I only type \"Summarize the following text: {text}\" and it provides\
          \ a great summary wile the same prompt with the template from above is not\
          \ satisfying at all. "
        updatedAt: '2023-07-20T07:16:19.676Z'
      numEdits: 1
      reactions: []
    id: 64b8dce4f8bf823a61ebaa48
    type: comment
  author: Sven00
  content: "Do you know a good prompt (including SYS) in order to generate a concise/rephrased\
    \ summary that captures the main points of a text based on?: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\
    \ \n\nI would like to replicate the results I get from the demo: https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat\n\
    \nThere I only type \"Summarize the following text: {text}\" and it provides a\
    \ great summary wile the same prompt with the template from above is not satisfying\
    \ at all. "
  created_at: 2023-07-20 06:06:12+00:00
  edited: true
  hidden: false
  id: 64b8dce4f8bf823a61ebaa48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14af8bd7a590bd08a0098eafc9b774d9.svg
      fullname: Clay P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clayp
      type: user
    createdAt: '2023-07-20T13:29:01.000Z'
    data:
      edited: false
      editors:
      - clayp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8607925772666931
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14af8bd7a590bd08a0098eafc9b774d9.svg
          fullname: Clay P
          isHf: false
          isPro: false
          name: clayp
          type: user
        html: '<blockquote>

          <p>Do you know a good prompt (including SYS) in order to generate a concise/rephrased
          summary that captures the main points of a text based on?: <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</a>
          </p>

          <p>I would like to replicate the results I get from the demo: <a href="https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat">https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat</a></p>

          <p>There I only type "Summarize the following text: {text}" and it provides
          a great summary wile the same prompt with the template from above is not
          satisfying at all.</p>

          </blockquote>

          <p>Can you provide an example with screenshots of both prompts and results?</p>

          <p>They''re using almost exactly the same prompt (though seemingly without
          the opening <code>&lt;s&gt;</code>): <a href="https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py">https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py</a></p>

          '
        raw: "> Do you know a good prompt (including SYS) in order to generate a concise/rephrased\
          \ summary that captures the main points of a text based on?: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\
          \ \n> \n> I would like to replicate the results I get from the demo: https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat\n\
          > \n> There I only type \"Summarize the following text: {text}\" and it\
          \ provides a great summary wile the same prompt with the template from above\
          \ is not satisfying at all.\n\nCan you provide an example with screenshots\
          \ of both prompts and results?\n\nThey're using almost exactly the same\
          \ prompt (though seemingly without the opening ```<s>```): https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py"
        updatedAt: '2023-07-20T13:29:01.252Z'
      numEdits: 0
      reactions: []
    id: 64b9369d6b5ee8c38870d5c5
    type: comment
  author: clayp
  content: "> Do you know a good prompt (including SYS) in order to generate a concise/rephrased\
    \ summary that captures the main points of a text based on?: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\
    \ \n> \n> I would like to replicate the results I get from the demo: https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat\n\
    > \n> There I only type \"Summarize the following text: {text}\" and it provides\
    \ a great summary wile the same prompt with the template from above is not satisfying\
    \ at all.\n\nCan you provide an example with screenshots of both prompts and results?\n\
    \nThey're using almost exactly the same prompt (though seemingly without the opening\
    \ ```<s>```): https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py"
  created_at: 2023-07-20 12:29:01+00:00
  edited: false
  hidden: false
  id: 64b9369d6b5ee8c38870d5c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-20T13:50:30.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8995760679244995
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;clayp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/clayp\">@<span class=\"\
          underline\">clayp</span></a></span>\n\n\t</span></span> Yes the EOS is only\
          \ involved when the conversation goes multi-turn and the leading BOS doesn't\
          \ seem to make any difference I get the same output.</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;Sven00&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Sven00\">@<span class=\"underline\">Sven00</span></a></span>\n\
          \n\t</span></span> See <a href=\"https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py#L24\"\
          >https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py#L24</a>\
          \ for the exact function they use to assemble the prompt.  It matches my\
          \ PR in <a href=\"/TheBloke/Llama-2-7B-Chat-GGML/discussions/4\">#4</a>\
          \ except for the BOS token that's appended at the beginning in their original\
          \ repo but not in this app.</p>\n"
        raw: '@clayp Yes the EOS is only involved when the conversation goes multi-turn
          and the leading BOS doesn''t seem to make any difference I get the same
          output.


          @Sven00 See https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py#L24
          for the exact function they use to assemble the prompt.  It matches my PR
          in #4 except for the BOS token that''s appended at the beginning in their
          original repo but not in this app.'
        updatedAt: '2023-07-20T13:50:30.318Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - clayp
    id: 64b93ba6126cfeb8fde303ed
    type: comment
  author: mike-ravkine
  content: '@clayp Yes the EOS is only involved when the conversation goes multi-turn
    and the leading BOS doesn''t seem to make any difference I get the same output.


    @Sven00 See https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py#L24
    for the exact function they use to assemble the prompt.  It matches my PR in #4
    except for the BOS token that''s appended at the beginning in their original repo
    but not in this app.'
  created_at: 2023-07-20 12:50:30+00:00
  edited: false
  hidden: false
  id: 64b93ba6126cfeb8fde303ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2d5207bd79bfe88218e1b5bbb1a2274.svg
      fullname: Ycros
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ycros
      type: user
    createdAt: '2023-07-21T04:49:10.000Z'
    data:
      edited: false
      editors:
      - ycros
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9635220170021057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2d5207bd79bfe88218e1b5bbb1a2274.svg
          fullname: Ycros
          isHf: false
          isPro: false
          name: ycros
          type: user
        html: '<p>I''m pretty sure llama.cpp''s tokenizer won''t currently tokenize
          <s> and </s> to BOS/EOS, you''d need to preprocess and poke in the tokens
          yourself.</p>

          '
        raw: I'm pretty sure llama.cpp's tokenizer won't currently tokenize <s> and
          </s> to BOS/EOS, you'd need to preprocess and poke in the tokens yourself.
        updatedAt: '2023-07-21T04:49:10.428Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - viniciusarruda
        - algorithm
    id: 64ba0e465c4deebf69a7bd20
    type: comment
  author: ycros
  content: I'm pretty sure llama.cpp's tokenizer won't currently tokenize <s> and
    </s> to BOS/EOS, you'd need to preprocess and poke in the tokens yourself.
  created_at: 2023-07-21 03:49:10+00:00
  edited: false
  hidden: false
  id: 64ba0e465c4deebf69a7bd20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
      fullname: "Vinicius Ferra\xE7o Arruda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusarruda
      type: user
    createdAt: '2023-07-21T04:52:49.000Z'
    data:
      edited: true
      editors:
      - viniciusarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8273534178733826
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
          fullname: "Vinicius Ferra\xE7o Arruda"
          isHf: false
          isPro: false
          name: viniciusarruda
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ycros&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ycros\">@<span class=\"\
          underline\">ycros</span></a></span>\n\n\t</span></span>  I'm trying to handle\
          \ that <a rel=\"nofollow\" href=\"https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/\"\
          >here</a>.</p>\n"
        raw: '@ycros  I''m trying to handle that [here](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/).'
        updatedAt: '2023-07-21T04:53:02.192Z'
      numEdits: 1
      reactions: []
    id: 64ba0f2184ddd52599ca891f
    type: comment
  author: viniciusarruda
  content: '@ycros  I''m trying to handle that [here](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/).'
  created_at: 2023-07-21 03:52:49+00:00
  edited: true
  hidden: false
  id: 64ba0f2184ddd52599ca891f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642d678777078db98b729188/UG42t8m7454PbDTSc7G5o.png?w=200&h=200&f=face
      fullname: algorithm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: algorithm
      type: user
    createdAt: '2023-07-21T23:34:07.000Z'
    data:
      edited: false
      editors:
      - algorithm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446172714233398
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642d678777078db98b729188/UG42t8m7454PbDTSc7G5o.png?w=200&h=200&f=face
          fullname: algorithm
          isHf: false
          isPro: false
          name: algorithm
          type: user
        html: '<p>I think this is related: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/2304">https://github.com/ggerganov/llama.cpp/pull/2304</a></p>

          '
        raw: 'I think this is related: https://github.com/ggerganov/llama.cpp/pull/2304'
        updatedAt: '2023-07-21T23:34:07.883Z'
      numEdits: 0
      reactions: []
    id: 64bb15ef9f94ea2554c4a9a8
    type: comment
  author: algorithm
  content: 'I think this is related: https://github.com/ggerganov/llama.cpp/pull/2304'
  created_at: 2023-07-21 22:34:07+00:00
  edited: false
  hidden: false
  id: 64bb15ef9f94ea2554c4a9a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639609374505-noauth.jpeg?w=200&h=200&f=face
      fullname: Vibudh Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iVibudh
      type: user
    createdAt: '2023-09-02T02:23:57.000Z'
    data:
      edited: false
      editors:
      - iVibudh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4135032892227173
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639609374505-noauth.jpeg?w=200&h=200&f=face
          fullname: Vibudh Singh
          isHf: false
          isPro: false
          name: iVibudh
          type: user
        html: "<p>def convert_to_llama_format(messages):<br>''' Function to convert\
          \ a message or an array of user and System prompts to llama format.'''<br>\
          \    if isinstance(messages, list) == False:<br>        messages = f'[INST]\
          \ &lt;&gt; &lt;&gt;\\n\\n {messages} [/INST]'<br>        return(messages)</p>\n\
          <pre><code>formatted_messages = \"[INST] &lt;&lt;SYS&gt;&gt; &lt;&lt;/SYS&gt;&gt;\"\
          \nfor message in messages:\n    role = message[\"role\"]\n    content =\
          \ message[\"content\"]\n    \n    if role == \"system\":\n        formatted_messages\
          \ = f\"[INST]&lt;&lt;SYS&gt;&gt;\\n{content}\\n&lt;&lt;/SYS&gt;&gt;\"\n\
          \    if role == \"user\":\n        formatted_messages = formatted_messages\
          \ + f\"\\n\\n{content} [/INST]\"\n    if role == \"assistant\":\n      \
          \  formatted_messages = formatted_messages + f\"\\n\\n{content} [INST]\"\
          \n\nreturn(formatted_messages)\n</code></pre>\n<p>Input -&gt;<br>messages=[<br>\
          \        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"\
          },<br>        {\"role\": \"user\", \"content\": \"Knock knock.\"},<br> \
          \       {\"role\": \"assistant\", \"content\": \"Who's there?\"},<br>  \
          \      {\"role\": \"user\", \"content\": \"Orange.\"},<br>    ]</p>\n<p>Output\
          \ -&gt;<br>[INST]&lt;&gt;<br>You are a helpful assistant.<br>&lt;&gt;</p>\n\
          <p>Knock knock. [/INST]</p>\n<p>Who's there? [INST]</p>\n<p>Orange. [/INST]</p>\n"
        raw: "def convert_to_llama_format(messages):\n''' Function to convert a message\
          \ or an array of user and System prompts to llama format.'''\n    if isinstance(messages,\
          \ list) == False:\n        messages = f'[INST] <<SYS>> <</SYS>>\\n\\n {messages}\
          \ [/INST]'\n        return(messages)\n    \n    \n    formatted_messages\
          \ = \"[INST] <<SYS>> <</SYS>>\"\n    for message in messages:\n        role\
          \ = message[\"role\"]\n        content = message[\"content\"]\n        \n\
          \        if role == \"system\":\n            formatted_messages = f\"[INST]<<SYS>>\\\
          n{content}\\n<</SYS>>\"\n        if role == \"user\":\n            formatted_messages\
          \ = formatted_messages + f\"\\n\\n{content} [/INST]\"\n        if role ==\
          \ \"assistant\":\n            formatted_messages = formatted_messages +\
          \ f\"\\n\\n{content} [INST]\"\n\n    return(formatted_messages)\n\nInput\
          \ -> \nmessages=[\n        {\"role\": \"system\", \"content\": \"You are\
          \ a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"\
          Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's\
          \ there?\"},\n        {\"role\": \"user\", \"content\": \"Orange.\"},\n\
          \    ]\n\nOutput -> \n[INST]<<SYS>>\nYou are a helpful assistant.\n<</SYS>>\n\
          \nKnock knock. [/INST]\n\nWho's there? [INST]\n\nOrange. [/INST]"
        updatedAt: '2023-09-02T02:23:57.688Z'
      numEdits: 0
      reactions: []
    id: 64f29cbdad08fa22955e7ca8
    type: comment
  author: iVibudh
  content: "def convert_to_llama_format(messages):\n''' Function to convert a message\
    \ or an array of user and System prompts to llama format.'''\n    if isinstance(messages,\
    \ list) == False:\n        messages = f'[INST] <<SYS>> <</SYS>>\\n\\n {messages}\
    \ [/INST]'\n        return(messages)\n    \n    \n    formatted_messages = \"\
    [INST] <<SYS>> <</SYS>>\"\n    for message in messages:\n        role = message[\"\
    role\"]\n        content = message[\"content\"]\n        \n        if role ==\
    \ \"system\":\n            formatted_messages = f\"[INST]<<SYS>>\\n{content}\\\
    n<</SYS>>\"\n        if role == \"user\":\n            formatted_messages = formatted_messages\
    \ + f\"\\n\\n{content} [/INST]\"\n        if role == \"assistant\":\n        \
    \    formatted_messages = formatted_messages + f\"\\n\\n{content} [INST]\"\n\n\
    \    return(formatted_messages)\n\nInput -> \nmessages=[\n        {\"role\": \"\
    system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\":\
    \ \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\"\
    , \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"\
    Orange.\"},\n    ]\n\nOutput -> \n[INST]<<SYS>>\nYou are a helpful assistant.\n\
    <</SYS>>\n\nKnock knock. [/INST]\n\nWho's there? [INST]\n\nOrange. [/INST]"
  created_at: 2023-09-02 01:23:57+00:00
  edited: false
  hidden: false
  id: 64f29cbdad08fa22955e7ca8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4e03fd5025350426dae942c475010f.svg
      fullname: Mohammed Hannan Baig
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baig214
      type: user
    createdAt: '2023-12-11T10:58:09.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/9e4e03fd5025350426dae942c475010f.svg
          fullname: Mohammed Hannan Baig
          isHf: false
          isPro: false
          name: baig214
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-11T10:58:25.025Z'
      numEdits: 0
      reactions: []
    id: 6576eb41a802b4d18419af51
    type: comment
  author: baig214
  content: This comment has been hidden
  created_at: 2023-12-11 10:58:09+00:00
  edited: true
  hidden: true
  id: 6576eb41a802b4d18419af51
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: closed
target_branch: null
title: Chat prompt format?
