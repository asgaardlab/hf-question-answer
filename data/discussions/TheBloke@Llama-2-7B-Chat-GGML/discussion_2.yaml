!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aiapprentice101
conflicting_files: null
created_at: 2023-07-18 21:23:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b029235e2480a9f37d11435fdbd18940.svg
      fullname: AI Apprentice
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aiapprentice101
      type: user
    createdAt: '2023-07-18T22:23:43.000Z'
    data:
      edited: false
      editors:
      - aiapprentice101
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8974292278289795
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b029235e2480a9f37d11435fdbd18940.svg
          fullname: AI Apprentice
          isHf: false
          isPro: false
          name: aiapprentice101
          type: user
        html: '<p>Hi,</p>

          <p>Awesome work! Do you happen to have any benchmark of different versions
          of the GGML model? It will be great to see how much performance deterioration
          we get from these quantization techniques.</p>

          '
        raw: "Hi,\r\n\r\nAwesome work! Do you happen to have any benchmark of different\
          \ versions of the GGML model? It will be great to see how much performance\
          \ deterioration we get from these quantization techniques."
        updatedAt: '2023-07-18T22:23:43.495Z'
      numEdits: 0
      reactions: []
    id: 64b710efbdf3789730726fec
    type: comment
  author: aiapprentice101
  content: "Hi,\r\n\r\nAwesome work! Do you happen to have any benchmark of different\
    \ versions of the GGML model? It will be great to see how much performance deterioration\
    \ we get from these quantization techniques."
  created_at: 2023-07-18 21:23:43+00:00
  edited: false
  hidden: false
  id: 64b710efbdf3789730726fec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-20T14:02:17.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8290165066719055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aiapprentice101&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aiapprentice101\"\
          >@<span class=\"underline\">aiapprentice101</span></a></span>\n\n\t</span></span>\
          \ Did you see <a rel=\"nofollow\" href=\"https://oobabooga.github.io/blog/posts/perplexities/\"\
          >https://oobabooga.github.io/blog/posts/perplexities/</a> ?  Its not about\
          \ this model specifically but covers the relative performance of different\
          \ GGML and GPTQ quants.</p>\n"
        raw: '@aiapprentice101 Did you see https://oobabooga.github.io/blog/posts/perplexities/
          ?  Its not about this model specifically but covers the relative performance
          of different GGML and GPTQ quants.'
        updatedAt: '2023-07-20T14:02:17.328Z'
      numEdits: 0
      reactions: []
    id: 64b93e69008ff25caf4a4875
    type: comment
  author: mike-ravkine
  content: '@aiapprentice101 Did you see https://oobabooga.github.io/blog/posts/perplexities/
    ?  Its not about this model specifically but covers the relative performance of
    different GGML and GPTQ quants.'
  created_at: 2023-07-20 13:02:17+00:00
  edited: false
  hidden: false
  id: 64b93e69008ff25caf4a4875
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b029235e2480a9f37d11435fdbd18940.svg
      fullname: AI Apprentice
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aiapprentice101
      type: user
    createdAt: '2023-07-23T22:11:38.000Z'
    data:
      edited: false
      editors:
      - aiapprentice101
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9548458456993103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b029235e2480a9f37d11435fdbd18940.svg
          fullname: AI Apprentice
          isHf: false
          isPro: false
          name: aiapprentice101
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;mike-ravkine&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mike-ravkine\"\
          >@<span class=\"underline\">mike-ravkine</span></a></span>\n\n\t</span></span>\
          \ . The post seems to claim GPTQ is the best in terms of quality. However,\
          \ when I test out the GPTQ version of Llama-2 (also from TheBloke), I get\
          \ very bad performance. The model doesn't capture the one-shot instructions,\
          \ and generates random stuffs. </p>\n"
        raw: 'Thank you @mike-ravkine . The post seems to claim GPTQ is the best in
          terms of quality. However, when I test out the GPTQ version of Llama-2 (also
          from TheBloke), I get very bad performance. The model doesn''t capture the
          one-shot instructions, and generates random stuffs. '
        updatedAt: '2023-07-23T22:11:38.114Z'
      numEdits: 0
      reactions: []
    id: 64bda59a4b4ff0d509560714
    type: comment
  author: aiapprentice101
  content: 'Thank you @mike-ravkine . The post seems to claim GPTQ is the best in
    terms of quality. However, when I test out the GPTQ version of Llama-2 (also from
    TheBloke), I get very bad performance. The model doesn''t capture the one-shot
    instructions, and generates random stuffs. '
  created_at: 2023-07-23 21:11:38+00:00
  edited: false
  hidden: false
  id: 64bda59a4b4ff0d509560714
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: Benchmark of different GGML version
