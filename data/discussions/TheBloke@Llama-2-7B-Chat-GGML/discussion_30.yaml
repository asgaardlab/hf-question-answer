!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AAAkuan
conflicting_files: null
created_at: 2023-10-16 03:19:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/vGyp5uPnR7O_KWO9mmXx8.jpeg?w=200&h=200&f=face
      fullname: akuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AAAkuan
      type: user
    createdAt: '2023-10-16T04:19:00.000Z'
    data:
      edited: true
      editors:
      - AAAkuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37509599328041077
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/vGyp5uPnR7O_KWO9mmXx8.jpeg?w=200&h=200&f=face
          fullname: akuan
          isHf: false
          isPro: false
          name: AAAkuan
          type: user
        html: '<p>gguf_init_from_file: invalid magic number 67676a74<br>error loading
          model: llama_model_loader: failed to load model from models\llama-2-7b-chat.ggmlv3.q2_K.bin</p>

          <p>llama_load_model_from_file: failed to load model<br>2023-10-16 12:10:46
          ERROR:Failed to load the model.<br>Traceback (most recent call last):<br>  File
          "E:\oobabooga_windows\text-generation-webui\modules\ui_model_menu.py", line
          201, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "E:\oobabooga_windows\text-generation-webui\modules\models.py",
          line 79, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "E:\oobabooga_windows\text-generation-webui\modules\models.py", line 225,
          in llamacpp_loader<br>    model, tokenizer = LlamaCppModel.from_pretrained(model_file)<br>  File
          "E:\oobabooga_windows\text-generation-webui\modules\llamacpp_model.py",
          line 91, in from_pretrained<br>    result.model = Llama(**params)<br>  File
          "E:\oobabooga_windows\installer_files\env\lib\site-packages\llama_cpp_cuda\llama.py",
          line 365, in <strong>init</strong><br>    assert self.model is not None<br>AssertionError</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x0000024EFFB1D870&gt;<br>Traceback
          (most recent call last):<br>  File "E:\oobabooga_windows\text-generation-webui\modules\llamacpp_model.py",
          line 49, in <strong>del</strong><br>    self.model.<strong>del</strong>()<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''</p>

          <p>CPU:I7-6700K<br>GPU:RTX 3060TI 8G<br>RAM:32G<br>OS:WIN11</p>

          '
        raw: "gguf_init_from_file: invalid magic number 67676a74\nerror loading model:\
          \ llama_model_loader: failed to load model from models\\llama-2-7b-chat.ggmlv3.q2_K.bin\n\
          \nllama_load_model_from_file: failed to load model\n2023-10-16 12:10:46\
          \ ERROR:Failed to load the model.\nTraceback (most recent call last):\n\
          \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\ui_model_menu.py\"\
          , line 201, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\n  File \"E:\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 79, in load_model\n  \
          \  output = load_func_map[loader](model_name)\n  File \"E:\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 225, in llamacpp_loader\n\
          \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n  File\
          \ \"E:\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 91, in from_pretrained\n    result.model = Llama(**params)\n  File\
          \ \"E:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 365, in __init__\n    assert self.model is not None\nAssertionError\n\
          \nException ignored in: <function LlamaCppModel.__del__ at 0x0000024EFFB1D870>\n\
          Traceback (most recent call last):\n  File \"E:\\oobabooga_windows\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 49, in __del__\n    self.model.__del__()\n\
          AttributeError: 'LlamaCppModel' object has no attribute 'model'\n\nCPU:I7-6700K\n\
          GPU:RTX 3060TI 8G\nRAM:32G\nOS:WIN11"
        updatedAt: '2023-10-16T04:19:54.355Z'
      numEdits: 1
      reactions: []
    id: 652cb9b41ef9983c6d41cbd6
    type: comment
  author: AAAkuan
  content: "gguf_init_from_file: invalid magic number 67676a74\nerror loading model:\
    \ llama_model_loader: failed to load model from models\\llama-2-7b-chat.ggmlv3.q2_K.bin\n\
    \nllama_load_model_from_file: failed to load model\n2023-10-16 12:10:46 ERROR:Failed\
    \ to load the model.\nTraceback (most recent call last):\n  File \"E:\\oobabooga_windows\\\
    text-generation-webui\\modules\\ui_model_menu.py\", line 201, in load_model_wrapper\n\
    \    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\n\
    \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\",\
    \ line 79, in load_model\n    output = load_func_map[loader](model_name)\n  File\
    \ \"E:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 225,\
    \ in llamacpp_loader\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\
    \  File \"E:\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
    , line 91, in from_pretrained\n    result.model = Llama(**params)\n  File \"E:\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\llama_cpp_cuda\\\
    llama.py\", line 365, in __init__\n    assert self.model is not None\nAssertionError\n\
    \nException ignored in: <function LlamaCppModel.__del__ at 0x0000024EFFB1D870>\n\
    Traceback (most recent call last):\n  File \"E:\\oobabooga_windows\\text-generation-webui\\\
    modules\\llamacpp_model.py\", line 49, in __del__\n    self.model.__del__()\n\
    AttributeError: 'LlamaCppModel' object has no attribute 'model'\n\nCPU:I7-6700K\n\
    GPU:RTX 3060TI 8G\nRAM:32G\nOS:WIN11"
  created_at: 2023-10-16 03:19:00+00:00
  edited: true
  hidden: false
  id: 652cb9b41ef9983c6d41cbd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33e92e634cafaa9efe9bbc6f7b20180a.svg
      fullname: reza karimzadeh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rezak
      type: user
    createdAt: '2023-10-18T14:17:36.000Z'
    data:
      edited: false
      editors:
      - rezak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4027402102947235
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33e92e634cafaa9efe9bbc6f7b20180a.svg
          fullname: reza karimzadeh
          isHf: false
          isPro: false
          name: rezak
          type: user
        html: '<p>Same problem ....</p>

          '
        raw: 'Same problem ....

          '
        updatedAt: '2023-10-18T14:17:36.518Z'
      numEdits: 0
      reactions: []
    id: 652fe9009c9f959739891367
    type: comment
  author: rezak
  content: 'Same problem ....

    '
  created_at: 2023-10-18 13:17:36+00:00
  edited: false
  hidden: false
  id: 652fe9009c9f959739891367
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: Traceback about ui_model_menu.py
