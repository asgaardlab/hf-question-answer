!!python/object:huggingface_hub.community.DiscussionWithDetails
author: viniciusarruda
conflicting_files: null
created_at: 2023-07-20 16:53:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
      fullname: "Vinicius Ferra\xE7o Arruda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusarruda
      type: user
    createdAt: '2023-07-20T17:53:55.000Z'
    data:
      edited: false
      editors:
      - viniciusarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9049856662750244
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
          fullname: "Vinicius Ferra\xE7o Arruda"
          isHf: false
          isPro: false
          name: viniciusarruda
          type: user
        html: '<p>I''m having a issue while decoding/encoding.<br>This is also related
          to the chat completion format already mentioned previously in other discussions.<br>You
          can see the issue in details and also replicate it <a rel="nofollow" href="https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/#issues">here</a>.
          I''m comparing Meta original tokenizer with this model using <a rel="nofollow"
          href="https://github.com/abetlen/llama-cpp-python">llama-cpp-python</a>.<br>In
          summary, the tokens <code>518</code> and <code>29961</code> are being decoded/encoded
          differently.</p>

          '
        raw: "I'm having a issue while decoding/encoding.\r\nThis is also related\
          \ to the chat completion format already mentioned previously in other discussions.\r\
          \nYou can see the issue in details and also replicate it [here](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/#issues).\
          \ I'm comparing Meta original tokenizer with this model using [llama-cpp-python](https://github.com/abetlen/llama-cpp-python).\r\
          \nIn summary, the tokens `518` and `29961` are being decoded/encoded differently.\r\
          \n\r\n"
        updatedAt: '2023-07-20T17:53:55.511Z'
      numEdits: 0
      reactions: []
    id: 64b974b3c41bf217c119c1d2
    type: comment
  author: viniciusarruda
  content: "I'm having a issue while decoding/encoding.\r\nThis is also related to\
    \ the chat completion format already mentioned previously in other discussions.\r\
    \nYou can see the issue in details and also replicate it [here](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/#issues).\
    \ I'm comparing Meta original tokenizer with this model using [llama-cpp-python](https://github.com/abetlen/llama-cpp-python).\r\
    \nIn summary, the tokens `518` and `29961` are being decoded/encoded differently.\r\
    \n\r\n"
  created_at: 2023-07-20 16:53:55+00:00
  edited: false
  hidden: false
  id: 64b974b3c41bf217c119c1d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T09:34:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9798725247383118
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>As I think we discussed on my Discord, there''s nothing I can do
          about this as I used the correct tokenizer.model and that is the output
          that was produced.  Have you discussed it on the llama.cpp Github?</p>

          '
        raw: As I think we discussed on my Discord, there's nothing I can do about
          this as I used the correct tokenizer.model and that is the output that was
          produced.  Have you discussed it on the llama.cpp Github?
        updatedAt: '2023-07-21T09:34:37.818Z'
      numEdits: 0
      reactions: []
    id: 64ba512d42d3266394017bec
    type: comment
  author: TheBloke
  content: As I think we discussed on my Discord, there's nothing I can do about this
    as I used the correct tokenizer.model and that is the output that was produced.  Have
    you discussed it on the llama.cpp Github?
  created_at: 2023-07-21 08:34:37+00:00
  edited: false
  hidden: false
  id: 64ba512d42d3266394017bec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
      fullname: "Vinicius Ferra\xE7o Arruda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusarruda
      type: user
    createdAt: '2023-07-21T13:23:55.000Z'
    data:
      edited: false
      editors:
      - viniciusarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9104019403457642
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
          fullname: "Vinicius Ferra\xE7o Arruda"
          isHf: false
          isPro: false
          name: viniciusarruda
          type: user
        html: '<p>Yes, I''m trying to get this conversation on the llama.cpp repo.
          Thank you very much!</p>

          '
        raw: Yes, I'm trying to get this conversation on the llama.cpp repo. Thank
          you very much!
        updatedAt: '2023-07-21T13:23:55.686Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64ba86ebe8762a2751b0ea40
    id: 64ba86ebe8762a2751b0ea3f
    type: comment
  author: viniciusarruda
  content: Yes, I'm trying to get this conversation on the llama.cpp repo. Thank you
    very much!
  created_at: 2023-07-21 12:23:55+00:00
  edited: false
  hidden: false
  id: 64ba86ebe8762a2751b0ea3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
      fullname: "Vinicius Ferra\xE7o Arruda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusarruda
      type: user
    createdAt: '2023-07-21T13:23:55.000Z'
    data:
      status: closed
    id: 64ba86ebe8762a2751b0ea40
    type: status-change
  author: viniciusarruda
  created_at: 2023-07-21 12:23:55+00:00
  id: 64ba86ebe8762a2751b0ea40
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
      fullname: "Vinicius Ferra\xE7o Arruda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusarruda
      type: user
    createdAt: '2023-07-21T13:24:21.000Z'
    data:
      status: open
    id: 64ba8705d6a95cc4a1be4776
    type: status-change
  author: viniciusarruda
  created_at: 2023-07-21 12:24:21+00:00
  id: 64ba8705d6a95cc4a1be4776
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Llama-2-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: Tokenizer behaving differently than Meta's original.
