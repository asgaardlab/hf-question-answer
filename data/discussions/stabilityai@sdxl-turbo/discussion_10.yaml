!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tintwotin
conflicting_files: null
created_at: 2023-11-29 15:26:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1732106613b89ea94b3f189bb73349d0.svg
      fullname: tin tin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tintwotin
      type: user
    createdAt: '2023-11-29T15:26:34.000Z'
    data:
      edited: false
      editors:
      - tintwotin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36997687816619873
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1732106613b89ea94b3f189bb73349d0.svg
          fullname: tin tin
          isHf: false
          isPro: false
          name: tintwotin
          type: user
        html: "<p>After adding \"import torch\" to the img2img example I get this\
          \ error:</p>\n<p>The config attributes {'feature_extractor': [None, None],\
          \ 'image_encoder': [None, None]} were passed to StableDiffusionXLImg2ImgPipeline,\
          \ but are not expected and will be ignored. Please verify your model_index.json\
          \ configuration file.<br>Keyword arguments {'feature_extractor': [None,\
          \ None], 'image_encoder': [None, None]} are not expected by StableDiffusionXLImg2ImgPipeline\
          \ and will be ignored.<br>Loading pipeline components...: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 7/7 [00:02&lt;00:00,  2.57it/s]<br>Error: Python: Traceback\
          \ (most recent call last):<br>  File \"C:\\Users\\45239\\AppData\\Roaming\\\
          Python\\Python310\\site-packages\\torch\\utils_contextlib.py\", line 115,\
          \ in decorate_context<br>    return func(*args, **kwargs)<br>  File \"C:\\\
          Program Files\\Blender Foundation\\Blender 4.0\\4.0\\python\\lib\\site-packages\\\
          diffusers\\pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl_img2img.py\"\
          , line 1074, in <strong>call</strong><br>    ) = self.encode_prompt(<br>\
          \  File \"C:\\Program Files\\Blender Foundation\\Blender 4.0\\4.0\\python\\\
          lib\\site-packages\\diffusers\\pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl_img2img.py\"\
          , line 360, in encode_prompt<br>    prompt_embeds = text_encoder(text_input_ids.to(device),\
          \ output_hidden_states=True)<br>  File \"C:\\Users\\45239\\AppData\\Roaming\\\
          Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line\
          \ 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>\
          \  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1527, in _call_impl<br>    return\
          \ forward_call(*args, **kwargs)<br>  File \"C:\\Users\\45239\\AppData\\\
          Roaming\\Python\\Python310\\site-packages\\transformers\\models\\clip\\\
          modeling_clip.py\", line 798, in forward<br>    return self.text_model(<br>\
          \  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl<br>  \
          \  return self._call_impl(*args, **kwargs)<br>  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1527, in _call_impl<br>    return forward_call(*args,\
          \ **kwargs)<br>  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 703,\
          \ in forward<br>    encoder_outputs = self.encoder(<br>  File \"C:\\Users\\\
          45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl<br>\
          \    return forward_call(*args, **kwargs)<br>  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\\
          clip\\modeling_clip.py\", line 630, in forward<br>    layer_outputs = encoder_layer(<br>\
          \  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl<br>  \
          \  return self._call_impl(*args, **kwargs)<br>  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1527, in _call_impl<br>    return forward_call(*args,\
          \ **kwargs)<br>  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 371,\
          \ in forward<br>    hidden_states = self.layer_norm1(hidden_states)<br>\
          \  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl<br>  \
          \  return self._call_impl(*args, **kwargs)<br>  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1527, in _call_impl<br>    return forward_call(*args,\
          \ **kwargs)<br>  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\torch\\nn\\modules\\normalization.py\", line 196, in forward<br>\
          \    return F.layer_norm(<br>  File \"C:\\Users\\45239\\AppData\\Roaming\\\
          Python\\Python310\\site-packages\\torch\\nn\\functional.py\", line 2543,\
          \ in layer_norm<br>    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)<br>RuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'</p>\n"
        raw: "After adding \"import torch\" to the img2img example I get this error:\r\
          \n\r\nThe config attributes {'feature_extractor': [None, None], 'image_encoder':\
          \ [None, None]} were passed to StableDiffusionXLImg2ImgPipeline, but are\
          \ not expected and will be ignored. Please verify your model_index.json\
          \ configuration file.\r\nKeyword arguments {'feature_extractor': [None,\
          \ None], 'image_encoder': [None, None]} are not expected by StableDiffusionXLImg2ImgPipeline\
          \ and will be ignored.\r\nLoading pipeline components...: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 7/7 [00:02<00:00,  2.57it/s]\r\nError: Python: Traceback (most\
          \ recent call last):\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\\
          Python310\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\
          \n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Blender\
          \ Foundation\\Blender 4.0\\4.0\\python\\lib\\site-packages\\diffusers\\\
          pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl_img2img.py\"\
          , line 1074, in __call__\r\n    ) = self.encode_prompt(\r\n  File \"C:\\\
          Program Files\\Blender Foundation\\Blender 4.0\\4.0\\python\\lib\\site-packages\\\
          diffusers\\pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl_img2img.py\"\
          , line 360, in encode_prompt\r\n    prompt_embeds = text_encoder(text_input_ids.to(device),\
          \ output_hidden_states=True)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\\
          Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line\
          \ 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
          \n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\\
          Roaming\\Python\\Python310\\site-packages\\transformers\\models\\clip\\\
          modeling_clip.py\", line 798, in forward\r\n    return self.text_model(\r\
          \n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n  \
          \  return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 703,\
          \ in forward\r\n    encoder_outputs = self.encoder(\r\n  File \"C:\\Users\\\
          45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\
          \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\\
          clip\\modeling_clip.py\", line 630, in forward\r\n    layer_outputs = encoder_layer(\r\
          \n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n  \
          \  return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 371,\
          \ in forward\r\n    hidden_states = self.layer_norm1(hidden_states)\r\n\
          \  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n  \
          \  return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\45239\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
          site-packages\\torch\\nn\\modules\\normalization.py\", line 196, in forward\r\
          \n    return F.layer_norm(\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\\
          Python\\Python310\\site-packages\\torch\\nn\\functional.py\", line 2543,\
          \ in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'"
        updatedAt: '2023-11-29T15:26:34.366Z'
      numEdits: 0
      reactions: []
    id: 6567582add4a892a144e1408
    type: comment
  author: tintwotin
  content: "After adding \"import torch\" to the img2img example I get this error:\r\
    \n\r\nThe config attributes {'feature_extractor': [None, None], 'image_encoder':\
    \ [None, None]} were passed to StableDiffusionXLImg2ImgPipeline, but are not expected\
    \ and will be ignored. Please verify your model_index.json configuration file.\r\
    \nKeyword arguments {'feature_extractor': [None, None], 'image_encoder': [None,\
    \ None]} are not expected by StableDiffusionXLImg2ImgPipeline and will be ignored.\r\
    \nLoading pipeline components...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  2.57it/s]\r\nError: Python:\
    \ Traceback (most recent call last):\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\\
    Python\\Python310\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in\
    \ decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program\
    \ Files\\Blender Foundation\\Blender 4.0\\4.0\\python\\lib\\site-packages\\diffusers\\\
    pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl_img2img.py\", line\
    \ 1074, in __call__\r\n    ) = self.encode_prompt(\r\n  File \"C:\\Program Files\\\
    Blender Foundation\\Blender 4.0\\4.0\\python\\lib\\site-packages\\diffusers\\\
    pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl_img2img.py\", line\
    \ 360, in encode_prompt\r\n    prompt_embeds = text_encoder(text_input_ids.to(device),\
    \ output_hidden_states=True)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\\
    Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line 1518,\
    \ in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File\
    \ \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 798, in forward\r\
    \n    return self.text_model(\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\\
    Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line 1518,\
    \ in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File\
    \ \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 703, in forward\r\
    \n    encoder_outputs = self.encoder(\r\n  File \"C:\\Users\\45239\\AppData\\\
    Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 630, in forward\r\
    \n    layer_outputs = encoder_layer(\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\\
    Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line 1518,\
    \ in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File\
    \ \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 371, in forward\r\
    \n    hidden_states = self.layer_norm1(hidden_states)\r\n  File \"C:\\Users\\\
    45239\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n \
    \   return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\45239\\AppData\\\
    Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\normalization.py\"\
    , line 196, in forward\r\n    return F.layer_norm(\r\n  File \"C:\\Users\\45239\\\
    AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py\"\
    , line 2543, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
    \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\"\
    \ not implemented for 'Half'"
  created_at: 2023-11-29 15:26:34+00:00
  edited: false
  hidden: false
  id: 6567582add4a892a144e1408
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-11-29T17:12:47.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6668374538421631
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: '<p>Can you make sure to run your model on GPU <strong>or</strong> remove
          <code>torch_dtype=torch.float16</code>?</p>

          '
        raw: Can you make sure to run your model on GPU **or** remove `torch_dtype=torch.float16`?
        updatedAt: '2023-11-29T17:12:47.465Z'
      numEdits: 0
      reactions: []
    id: 6567710f05fb89fb7e1373ec
    type: comment
  author: patrickvonplaten
  content: Can you make sure to run your model on GPU **or** remove `torch_dtype=torch.float16`?
  created_at: 2023-11-29 17:12:47+00:00
  edited: false
  hidden: false
  id: 6567710f05fb89fb7e1373ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1732106613b89ea94b3f189bb73349d0.svg
      fullname: tin tin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tintwotin
      type: user
    createdAt: '2023-11-29T19:53:02.000Z'
    data:
      edited: false
      editors:
      - tintwotin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9832866191864014
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1732106613b89ea94b3f189bb73349d0.svg
          fullname: tin tin
          isHf: false
          isPro: false
          name: tintwotin
          type: user
        html: '<p>Ah, yes, it was just pipe.to("cuda") missing from that example.
          </p>

          '
        raw: 'Ah, yes, it was just pipe.to("cuda") missing from that example. '
        updatedAt: '2023-11-29T19:53:02.269Z'
      numEdits: 0
      reactions: []
    id: 6567969e5025f8e01b101f57
    type: comment
  author: tintwotin
  content: 'Ah, yes, it was just pipe.to("cuda") missing from that example. '
  created_at: 2023-11-29 19:53:02+00:00
  edited: false
  hidden: false
  id: 6567969e5025f8e01b101f57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1732106613b89ea94b3f189bb73349d0.svg
      fullname: tin tin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tintwotin
      type: user
    createdAt: '2023-11-29T19:53:06.000Z'
    data:
      status: closed
    id: 656796a28e2b7c744f98b176
    type: status-change
  author: tintwotin
  created_at: 2023-11-29 19:53:06+00:00
  id: 656796a28e2b7c744f98b176
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: stabilityai/sdxl-turbo
repo_type: model
status: closed
target_branch: null
title: img2img example error
