!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ThijsL202
conflicting_files: null
created_at: 2023-09-23 13:02:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f2fa05e66ecaa2c502cfb892309b1b3.svg
      fullname: Thijs Lumeij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThijsL202
      type: user
    createdAt: '2023-09-23T14:02:24.000Z'
    data:
      edited: false
      editors:
      - ThijsL202
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3684442937374115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f2fa05e66ecaa2c502cfb892309b1b3.svg
          fullname: Thijs Lumeij
          isHf: false
          isPro: false
          name: ThijsL202
          type: user
        html: "<p>I've never reported something like this so pardon me.</p>\n<p>autogptq</p>\n\
          <p>raceback (most recent call last):</p>\n<p>File \u201CH:\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\ui_model_menu.py\u201D, line 194, in load_model_wrapper</p>\n\
          <p>shared.model, shared.tokenizer = load_model(shared.model_name, loader)<br>File\
          \ \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 76, in load_model</p>\n<p>output = load_func_map<a rel=\"nofollow\"\
          \ href=\"model_name\">loader</a><br>File \u201CH:\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 302, in AutoGPTQ_loader</p>\n\
          <p>return modules.AutoGPTQ_loader.load_quantized(model_name)<br>File \u201C\
          H:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
          , line 57, in load_quantized</p>\n<p>model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)<br>File \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 108, in from_quantized</p>\n\
          <p>return quant_func(<br>File \u201CH:\\AI\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling_base.py\u201D, line 875, in\
          \ from_quantized</p>\n<p>accelerate.utils.modeling.load_checkpoint_in_model(<br>File\
          \ \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\utils\\modeling.py\u201D, line 1414, in load_checkpoint_in_model</p>\n\
          <p>set_module_tensor_to_device(<br>File \u201CH:\\AI\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\utils\\modeling.py\u201D\
          , line 285, in set_module_tensor_to_device</p>\n<p>raise ValueError(<br>ValueError:\
          \ Trying to set a tensor of shape torch.Size([32000, 5120]) in \u201Cweight\u201D\
          \ (which has shape torch.Size([32000, 4096])), this look incorrect.</p>\n\
          <hr>\n<p>exllama</p>\n<p>Traceback (most recent call last):</p>\n<p>File\
          \ \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\ui_model_menu.py\u201D\
          , line 194, in load_model_wrapper</p>\n<p>shared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)<br>File \u201CH:\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 76, in load_model</p>\n\
          <p>output = load_func_map<a rel=\"nofollow\" href=\"model_name\">loader</a><br>File\
          \ \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 308, in ExLlama_loader</p>\n<p>model, tokenizer = ExllamaModel.from_pretrained(model_name)<br>File\
          \ \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\exllama.py\u201D\
          , line 75, in from_pretrained</p>\n<p>model = ExLlama(config)<br>File \u201C\
          H:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllama\\\
          model.py\u201D, line 814, in init</p>\n<p>device = self.config.device_map.map(key)<br>File\
          \ \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          exllama\\model.py\u201D, line 666, in map</p>\n<p>return self.layers[num]<br>IndexError:\
          \ list index out of range</p>\n"
        raw: "I've never reported something like this so pardon me.\r\n\r\nautogptq\r\
          \n\r\nraceback (most recent call last):\r\n\r\nFile \u201CH:\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\ui_model_menu.py\u201D, line 194, in load_model_wrapper\r\
          \n\r\nshared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 76, in load_model\r\n\r\noutput = load_func_map[loader](model_name)\r\
          \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 302, in AutoGPTQ_loader\r\n\r\nreturn modules.AutoGPTQ_loader.load_quantized(model_name)\r\
          \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          AutoGPTQ_loader.py\u201D, line 57, in load_quantized\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)\r\nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 108, in from_quantized\r\
          \n\r\nreturn quant_func(\r\nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling_base.py\u201D, line 875, in\
          \ from_quantized\r\n\r\naccelerate.utils.modeling.load_checkpoint_in_model(\r\
          \nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\utils\\modeling.py\u201D, line 1414, in load_checkpoint_in_model\r\
          \n\r\nset_module_tensor_to_device(\r\nFile \u201CH:\\AI\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\utils\\modeling.py\u201D\
          , line 285, in set_module_tensor_to_device\r\n\r\nraise ValueError(\r\n\
          ValueError: Trying to set a tensor of shape torch.Size([32000, 5120]) in\
          \ \u201Cweight\u201D (which has shape torch.Size([32000, 4096])), this look\
          \ incorrect.\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\
          \nexllama\r\n\r\nTraceback (most recent call last):\r\n\r\nFile \u201CH:\\\
          AI\\oobabooga_windows\\text-generation-webui\\modules\\ui_model_menu.py\u201D\
          , line 194, in load_model_wrapper\r\n\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\r\nFile \u201CH:\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 76, in load_model\r\
          \n\r\noutput = load_func_map[loader](model_name)\r\nFile \u201CH:\\AI\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 308, in ExLlama_loader\r\n\r\nmodel, tokenizer = ExllamaModel.from_pretrained(model_name)\r\
          \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          exllama.py\u201D, line 75, in from_pretrained\r\n\r\nmodel = ExLlama(config)\r\
          \nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          exllama\\model.py\u201D, line 814, in init\r\n\r\ndevice = self.config.device_map.map(key)\r\
          \nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          exllama\\model.py\u201D, line 666, in map\r\n\r\nreturn self.layers[num]\r\
          \nIndexError: list index out of range"
        updatedAt: '2023-09-23T14:02:24.962Z'
      numEdits: 0
      reactions: []
    id: 650eeff0076b6734578e5eaa
    type: comment
  author: ThijsL202
  content: "I've never reported something like this so pardon me.\r\n\r\nautogptq\r\
    \n\r\nraceback (most recent call last):\r\n\r\nFile \u201CH:\\AI\\oobabooga_windows\\\
    text-generation-webui\\modules\\ui_model_menu.py\u201D, line 194, in load_model_wrapper\r\
    \n\r\nshared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
    \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 76, in load_model\r\n\r\noutput = load_func_map[loader](model_name)\r\n\
    File \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 302, in AutoGPTQ_loader\r\n\r\nreturn modules.AutoGPTQ_loader.load_quantized(model_name)\r\
    \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
    , line 57, in load_quantized\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params)\r\nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 108, in from_quantized\r\
    \n\r\nreturn quant_func(\r\nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling_base.py\u201D, line 875, in from_quantized\r\
    \n\r\naccelerate.utils.modeling.load_checkpoint_in_model(\r\nFile \u201CH:\\AI\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\utils\\\
    modeling.py\u201D, line 1414, in load_checkpoint_in_model\r\n\r\nset_module_tensor_to_device(\r\
    \nFile \u201CH:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    accelerate\\utils\\modeling.py\u201D, line 285, in set_module_tensor_to_device\r\
    \n\r\nraise ValueError(\r\nValueError: Trying to set a tensor of shape torch.Size([32000,\
    \ 5120]) in \u201Cweight\u201D (which has shape torch.Size([32000, 4096])), this\
    \ look incorrect.\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\
    \nexllama\r\n\r\nTraceback (most recent call last):\r\n\r\nFile \u201CH:\\AI\\\
    oobabooga_windows\\text-generation-webui\\modules\\ui_model_menu.py\u201D, line\
    \ 194, in load_model_wrapper\r\n\r\nshared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
    models.py\u201D, line 76, in load_model\r\n\r\noutput = load_func_map[loader](model_name)\r\
    \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 308, in ExLlama_loader\r\n\r\nmodel, tokenizer = ExllamaModel.from_pretrained(model_name)\r\
    \nFile \u201CH:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\exllama.py\u201D\
    , line 75, in from_pretrained\r\n\r\nmodel = ExLlama(config)\r\nFile \u201CH:\\\
    AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllama\\model.py\u201D\
    , line 814, in init\r\n\r\ndevice = self.config.device_map.map(key)\r\nFile \u201C\
    H:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllama\\\
    model.py\u201D, line 666, in map\r\n\r\nreturn self.layers[num]\r\nIndexError:\
    \ list index out of range"
  created_at: 2023-09-23 13:02:24+00:00
  edited: false
  hidden: false
  id: 650eeff0076b6734578e5eaa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Synthia-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Failing to load in oobabooga exllama and autogptq
