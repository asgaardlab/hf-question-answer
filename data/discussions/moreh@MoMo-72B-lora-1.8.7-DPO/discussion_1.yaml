!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MaziyarPanahi
conflicting_files: null
created_at: 2024-01-22 15:56:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-22T15:56:29.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8356725573539734
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<p>Congratulations! This model is now ranked <a href="/moreh/MoMo-72B-lora-1.8.7-DPO/discussions/1">#1</a>!!!
          </p>

          <p>I went ahead and made a GPTQ quantization for this and happy to share
          it: <a href="https://huggingface.co/MaziyarPanahi/MoMo-72B-lora-1.8.7-DPO-GPTQ">MoMo-72B-lora-1.8.7-DPO-GPTQ</a></p>

          '
        raw: "Congratulations! This model is now ranked #1!!! \r\n\r\nI went ahead\
          \ and made a GPTQ quantization for this and happy to share it: [MoMo-72B-lora-1.8.7-DPO-GPTQ](https://huggingface.co/MaziyarPanahi/MoMo-72B-lora-1.8.7-DPO-GPTQ)"
        updatedAt: '2024-01-22T15:56:29.774Z'
      numEdits: 0
      reactions: []
    id: 65ae902da560bc932984b7ba
    type: comment
  author: MaziyarPanahi
  content: "Congratulations! This model is now ranked #1!!! \r\n\r\nI went ahead and\
    \ made a GPTQ quantization for this and happy to share it: [MoMo-72B-lora-1.8.7-DPO-GPTQ](https://huggingface.co/MaziyarPanahi/MoMo-72B-lora-1.8.7-DPO-GPTQ)"
  created_at: 2024-01-22 15:56:29+00:00
  edited: false
  hidden: false
  id: 65ae902da560bc932984b7ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2024-01-22T17:30:43.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9802109599113464
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I was making my private tests for understanding and reasoning and
          common sense of that llm and seems like I talk with finetuned very old llama
          65b ... poor results.<br>For instance mistral instruct 0.2 seems to be much
          more advanced in understanding, reasoning and common sense . I not even
          mentioned mixtral 8x7b which is like on totally different level... leaps
          ahead.</p>

          <p>I suspect this model is contaminated and that is why so high on the leaderboard.</p>

          '
        raw: '

          I was making my private tests for understanding and reasoning and common
          sense of that llm and seems like I talk with finetuned very old llama 65b
          ... poor results.

          For instance mistral instruct 0.2 seems to be much more advanced in understanding,
          reasoning and common sense . I not even mentioned mixtral 8x7b which is
          like on totally different level... leaps ahead.


          I suspect this model is contaminated and that is why so high on the leaderboard.'
        updatedAt: '2024-01-22T17:30:43.162Z'
      numEdits: 0
      reactions: []
    id: 65aea64369cd2991ef1df518
    type: comment
  author: mirek190
  content: '

    I was making my private tests for understanding and reasoning and common sense
    of that llm and seems like I talk with finetuned very old llama 65b ... poor results.

    For instance mistral instruct 0.2 seems to be much more advanced in understanding,
    reasoning and common sense . I not even mentioned mixtral 8x7b which is like on
    totally different level... leaps ahead.


    I suspect this model is contaminated and that is why so high on the leaderboard.'
  created_at: 2024-01-22 17:30:43+00:00
  edited: false
  hidden: false
  id: 65aea64369cd2991ef1df518
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-22T18:31:17.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.943901002407074
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mirek190\">@<span class=\"\
          underline\">mirek190</span></a></span>\n\n\t</span></span> the HF staff\
          \ and the community are very active to flag contaminated models specially\
          \ top 20. Do you happen to have some examples where mixtral does better?\
          \ </p>\n"
        raw: '@mirek190 the HF staff and the community are very active to flag contaminated
          models specially top 20. Do you happen to have some examples where mixtral
          does better? '
        updatedAt: '2024-01-22T18:31:17.966Z'
      numEdits: 0
      reactions: []
    id: 65aeb475b64e1c2389bfffdf
    type: comment
  author: MaziyarPanahi
  content: '@mirek190 the HF staff and the community are very active to flag contaminated
    models specially top 20. Do you happen to have some examples where mixtral does
    better? '
  created_at: 2024-01-22 18:31:17+00:00
  edited: false
  hidden: false
  id: 65aeb475b64e1c2389bfffdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2024-01-22T18:51:26.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9524099230766296
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>In every my test mixtral is better including coding.<br>For instance
          try :</p>

          <p>Create 10 sentences that ends with a word "apple".<br>Or<br>Provide complete
          working code for realistic looking tree in python using turtle library and
          recursive algorithm.</p>

          <p>The MoMo model fail those and most other my 20 questions where mixtral
          8x7b instruct is able to answer all my questions properly and mistral 7b
          0.2 instruct more than a half of them ( ~15).</p>

          <p>That''s why I claim that model is contaminated.<br>I do not believe model
          with such hight score is so stupid with lack of reasoning and common sense
          .<br>Answers are very similar to old llama 65 models  what I was testing
          7 month ago .</p>

          <p>Actually any open source  model right now is better than general knowledge
          LLM mixtral 8x7b instruct. I''m testing most of the then and didn''t find
          any model is able to answer even those 2 questions properly expect mixtral
          ...<br>Of course coding models will answer question about that python tree
          but only newer ones like a wizard coder 34b 0.2 . </p>

          <p>You can find my nick mirek190 with tests of those models on huggingface.
          I did it because only those models were worth after tests to say something
          more in the internet.</p>

          '
        raw: "In every my test mixtral is better including coding.\nFor instance try\
          \ :\n\nCreate 10 sentences that ends with a word \"apple\".\nOr\nProvide\
          \ complete working code for realistic looking tree in python using turtle\
          \ library and recursive algorithm.\n\nThe MoMo model fail those and most\
          \ other my 20 questions where mixtral 8x7b instruct is able to answer all\
          \ my questions properly and mistral 7b 0.2 instruct more than a half of\
          \ them ( ~15).\n\nThat's why I claim that model is contaminated.\nI do not\
          \ believe model with such hight score is so stupid with lack of reasoning\
          \ and common sense .\nAnswers are very similar to old llama 65 models  what\
          \ I was testing 7 month ago .\n\n\n\nActually any open source  model right\
          \ now is better than general knowledge LLM mixtral 8x7b instruct. I'm testing\
          \ most of the then and didn't find any model is able to answer even those\
          \ 2 questions properly expect mixtral ...\nOf course coding models will\
          \ answer question about that python tree but only newer ones like a wizard\
          \ coder 34b 0.2 . \n\nYou can find my nick mirek190 with tests of those\
          \ models on huggingface. I did it because only those models were worth after\
          \ tests to say something more in the internet."
        updatedAt: '2024-01-22T19:02:33.301Z'
      numEdits: 3
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - stolsvik
        - sethuiyer
        - Yhyu13
    id: 65aeb92ede38fbe922654230
    type: comment
  author: mirek190
  content: "In every my test mixtral is better including coding.\nFor instance try\
    \ :\n\nCreate 10 sentences that ends with a word \"apple\".\nOr\nProvide complete\
    \ working code for realistic looking tree in python using turtle library and recursive\
    \ algorithm.\n\nThe MoMo model fail those and most other my 20 questions where\
    \ mixtral 8x7b instruct is able to answer all my questions properly and mistral\
    \ 7b 0.2 instruct more than a half of them ( ~15).\n\nThat's why I claim that\
    \ model is contaminated.\nI do not believe model with such hight score is so stupid\
    \ with lack of reasoning and common sense .\nAnswers are very similar to old llama\
    \ 65 models  what I was testing 7 month ago .\n\n\n\nActually any open source\
    \  model right now is better than general knowledge LLM mixtral 8x7b instruct.\
    \ I'm testing most of the then and didn't find any model is able to answer even\
    \ those 2 questions properly expect mixtral ...\nOf course coding models will\
    \ answer question about that python tree but only newer ones like a wizard coder\
    \ 34b 0.2 . \n\nYou can find my nick mirek190 with tests of those models on huggingface.\
    \ I did it because only those models were worth after tests to say something more\
    \ in the internet."
  created_at: 2024-01-22 18:51:26+00:00
  edited: true
  hidden: false
  id: 65aeb92ede38fbe922654230
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg
      fullname: kibong choi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bongchoi
      type: user
    createdAt: '2024-01-23T01:05:24.000Z'
    data:
      edited: false
      editors:
      - bongchoi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.806759238243103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg
          fullname: kibong choi
          isHf: false
          isPro: false
          name: bongchoi
          type: user
        html: '<p>Hi, we haven''t trained our model on any datasets other than the
          three mentioned in our model card</p>

          <ol>

          <li>Open-Orca/SlimOrca</li>

          <li>jondurbin/truthy-dpo-v0.1</li>

          <li>Intel/orca_dpo_pairs</li>

          </ol>

          <p>and to the best of our knowledge, these three are not contaminated data.</p>

          <p>+  we have tested contamination refer to [<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/472%5D">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/472]</a><br>gsm8k:
          result &lt; 0.1, %: 0.47<br>truthfulqa: result &lt; 0.1, %: 0.44</p>

          <p>contamination test results for other tasks will be updated soon</p>

          '
        raw: 'Hi, we haven''t trained our model on any datasets other than the three
          mentioned in our model card

          1. Open-Orca/SlimOrca

          2. jondurbin/truthy-dpo-v0.1

          3. Intel/orca_dpo_pairs


          and to the best of our knowledge, these three are not contaminated data.


          \+  we have tested contamination refer to [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/472]

          gsm8k: result < 0.1, %: 0.47

          truthfulqa: result < 0.1, %: 0.44


          contamination test results for other tasks will be updated soon


          '
        updatedAt: '2024-01-23T01:05:24.963Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
    id: 65af10d452e1b2aae4fb300b
    type: comment
  author: bongchoi
  content: 'Hi, we haven''t trained our model on any datasets other than the three
    mentioned in our model card

    1. Open-Orca/SlimOrca

    2. jondurbin/truthy-dpo-v0.1

    3. Intel/orca_dpo_pairs


    and to the best of our knowledge, these three are not contaminated data.


    \+  we have tested contamination refer to [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/472]

    gsm8k: result < 0.1, %: 0.47

    truthfulqa: result < 0.1, %: 0.44


    contamination test results for other tasks will be updated soon


    '
  created_at: 2024-01-23 01:05:24+00:00
  edited: false
  hidden: false
  id: 65af10d452e1b2aae4fb300b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-23T11:32:32.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.920535683631897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<blockquote>

          <p>In every my test mixtral is better including coding.<br>For instance
          try :</p>

          <p>Create 10 sentences that ends with a word "apple".<br>Or<br>Provide complete
          working code for realistic looking tree in python using turtle library and
          recursive algorithm.</p>

          <p>The MoMo model fail those and most other my 20 questions where mixtral
          8x7b instruct is able to answer all my questions properly and mistral 7b
          0.2 instruct more than a half of them ( ~15).</p>

          <p>That''s why I claim that model is contaminated.<br>I do not believe model
          with such hight score is so stupid with lack of reasoning and common sense
          .<br>Answers are very similar to old llama 65 models  what I was testing
          7 month ago .</p>

          <p>Actually any open source  model right now is better than general knowledge
          LLM mixtral 8x7b instruct. I''m testing most of the then and didn''t find
          any model is able to answer even those 2 questions properly expect mixtral
          ...<br>Of course coding models will answer question about that python tree
          but only newer ones like a wizard coder 34b 0.2 . </p>

          <p>You can find my nick mirek190 with tests of those models on huggingface.
          I did it because only those models were worth after tests to say something
          more in the internet.</p>

          </blockquote>

          <p>Thanks for providing some examples. Just out of curiosity, do you have
          a specific System Prompt or chat template? In HuggingChat Mixtral 8x7B Instruct
          is not able to answer the first question correctly:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png"></a></p>

          '
        raw: "> In every my test mixtral is better including coding.\n> For instance\
          \ try :\n> \n> Create 10 sentences that ends with a word \"apple\".\n> Or\n\
          > Provide complete working code for realistic looking tree in python using\
          \ turtle library and recursive algorithm.\n> \n> The MoMo model fail those\
          \ and most other my 20 questions where mixtral 8x7b instruct is able to\
          \ answer all my questions properly and mistral 7b 0.2 instruct more than\
          \ a half of them ( ~15).\n> \n> That's why I claim that model is contaminated.\n\
          > I do not believe model with such hight score is so stupid with lack of\
          \ reasoning and common sense .\n> Answers are very similar to old llama\
          \ 65 models  what I was testing 7 month ago .\n> \n> \n> \n> Actually any\
          \ open source  model right now is better than general knowledge LLM mixtral\
          \ 8x7b instruct. I'm testing most of the then and didn't find any model\
          \ is able to answer even those 2 questions properly expect mixtral ...\n\
          > Of course coding models will answer question about that python tree but\
          \ only newer ones like a wizard coder 34b 0.2 . \n> \n> You can find my\
          \ nick mirek190 with tests of those models on huggingface. I did it because\
          \ only those models were worth after tests to say something more in the\
          \ internet.\n\nThanks for providing some examples. Just out of curiosity,\
          \ do you have a specific System Prompt or chat template? In HuggingChat\
          \ Mixtral 8x7B Instruct is not able to answer the first question correctly:\n\
          \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png)\n"
        updatedAt: '2024-01-23T11:32:32.246Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - bongchoi
        - afonsofroes
        - leejunhyeok
    id: 65afa3d03db2280ece95aa8c
    type: comment
  author: MaziyarPanahi
  content: "> In every my test mixtral is better including coding.\n> For instance\
    \ try :\n> \n> Create 10 sentences that ends with a word \"apple\".\n> Or\n> Provide\
    \ complete working code for realistic looking tree in python using turtle library\
    \ and recursive algorithm.\n> \n> The MoMo model fail those and most other my\
    \ 20 questions where mixtral 8x7b instruct is able to answer all my questions\
    \ properly and mistral 7b 0.2 instruct more than a half of them ( ~15).\n> \n\
    > That's why I claim that model is contaminated.\n> I do not believe model with\
    \ such hight score is so stupid with lack of reasoning and common sense .\n> Answers\
    \ are very similar to old llama 65 models  what I was testing 7 month ago .\n\
    > \n> \n> \n> Actually any open source  model right now is better than general\
    \ knowledge LLM mixtral 8x7b instruct. I'm testing most of the then and didn't\
    \ find any model is able to answer even those 2 questions properly expect mixtral\
    \ ...\n> Of course coding models will answer question about that python tree but\
    \ only newer ones like a wizard coder 34b 0.2 . \n> \n> You can find my nick mirek190\
    \ with tests of those models on huggingface. I did it because only those models\
    \ were worth after tests to say something more in the internet.\n\nThanks for\
    \ providing some examples. Just out of curiosity, do you have a specific System\
    \ Prompt or chat template? In HuggingChat Mixtral 8x7B Instruct is not able to\
    \ answer the first question correctly:\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png)\n"
  created_at: 2024-01-23 11:32:32+00:00
  edited: false
  hidden: false
  id: 65afa3d03db2280ece95aa8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2024-01-24T18:13:02.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9021083116531372
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<blockquote>

          <blockquote>

          <p>In every my test mixtral is better including coding.<br>For instance
          try :</p>

          <p>Create 10 sentences that ends with a word "apple".<br>Or<br>Provide complete
          working code for realistic looking tree in python using turtle library and
          recursive algorithm.</p>

          <p>The MoMo model fail those and most other my 20 questions where mixtral
          8x7b instruct is able to answer all my questions properly and mistral 7b
          0.2 instruct more than a half of them ( ~15).</p>

          <p>That''s why I claim that model is contaminated.<br>I do not believe model
          with such hight score is so stupid with lack of reasoning and common sense
          .<br>Answers are very similar to old llama 65 models  what I was testing
          7 month ago .</p>

          <p>Actually any open source  model right now is better than general knowledge
          LLM mixtral 8x7b instruct. I''m testing most of the then and didn''t find
          any model is able to answer even those 2 questions properly expect mixtral
          ...<br>Of course coding models will answer question about that python tree
          but only newer ones like a wizard coder 34b 0.2 . </p>

          <p>You can find my nick mirek190 with tests of those models on huggingface.
          I did it because only those models were worth after tests to say something
          more in the internet.</p>

          </blockquote>

          <p>Thanks for providing some examples. Just out of curiosity, do you have
          a specific System Prompt or chat template? In HuggingChat Mixtral 8x7B Instruct
          is not able to answer the first question correctly:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png"></a></p>

          </blockquote>

          <p>Check here </p>

          <p><a href="https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/discussions/1">https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/discussions/1</a></p>

          <p>Try those questions and you find out that answers are worse with MoMo
          in spite of much higher on leader board.<br>If model s not contaminated
          then the testing procedure is very unreliable right now. </p>

          <p>Or you can just talk about everything with model and is clear after a
          few minutes of conversation how smart and  intelligent model is... MoMo
          as I said before has reasoning and intelligence finetuned of first llama
          65b for me.  </p>

          '
        raw: "> > In every my test mixtral is better including coding.\n> > For instance\
          \ try :\n> > \n> > Create 10 sentences that ends with a word \"apple\".\n\
          > > Or\n> > Provide complete working code for realistic looking tree in\
          \ python using turtle library and recursive algorithm.\n> > \n> > The MoMo\
          \ model fail those and most other my 20 questions where mixtral 8x7b instruct\
          \ is able to answer all my questions properly and mistral 7b 0.2 instruct\
          \ more than a half of them ( ~15).\n> > \n> > That's why I claim that model\
          \ is contaminated.\n> > I do not believe model with such hight score is\
          \ so stupid with lack of reasoning and common sense .\n> > Answers are very\
          \ similar to old llama 65 models  what I was testing 7 month ago .\n> >\
          \ \n> > \n> > \n> > Actually any open source  model right now is better\
          \ than general knowledge LLM mixtral 8x7b instruct. I'm testing most of\
          \ the then and didn't find any model is able to answer even those 2 questions\
          \ properly expect mixtral ...\n> > Of course coding models will answer question\
          \ about that python tree but only newer ones like a wizard coder 34b 0.2\
          \ . \n> > \n> > You can find my nick mirek190 with tests of those models\
          \ on huggingface. I did it because only those models were worth after tests\
          \ to say something more in the internet.\n> \n> Thanks for providing some\
          \ examples. Just out of curiosity, do you have a specific System Prompt\
          \ or chat template? In HuggingChat Mixtral 8x7B Instruct is not able to\
          \ answer the first question correctly:\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png)\n\
          \nCheck here \n\nhttps://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/discussions/1\n\
          \nTry those questions and you find out that answers are worse with MoMo\
          \ in spite of much higher on leader board.\nIf model s not contaminated\
          \ then the testing procedure is very unreliable right now. \n\nOr you can\
          \ just talk about everything with model and is clear after a few minutes\
          \ of conversation how smart and  intelligent model is... MoMo as I said\
          \ before has reasoning and intelligence finetuned of first llama 65b for\
          \ me.  "
        updatedAt: '2024-01-24T18:21:38.400Z'
      numEdits: 2
      reactions: []
    id: 65b1532eb4225e3e420fba43
    type: comment
  author: mirek190
  content: "> > In every my test mixtral is better including coding.\n> > For instance\
    \ try :\n> > \n> > Create 10 sentences that ends with a word \"apple\".\n> > Or\n\
    > > Provide complete working code for realistic looking tree in python using turtle\
    \ library and recursive algorithm.\n> > \n> > The MoMo model fail those and most\
    \ other my 20 questions where mixtral 8x7b instruct is able to answer all my questions\
    \ properly and mistral 7b 0.2 instruct more than a half of them ( ~15).\n> > \n\
    > > That's why I claim that model is contaminated.\n> > I do not believe model\
    \ with such hight score is so stupid with lack of reasoning and common sense .\n\
    > > Answers are very similar to old llama 65 models  what I was testing 7 month\
    \ ago .\n> > \n> > \n> > \n> > Actually any open source  model right now is better\
    \ than general knowledge LLM mixtral 8x7b instruct. I'm testing most of the then\
    \ and didn't find any model is able to answer even those 2 questions properly\
    \ expect mixtral ...\n> > Of course coding models will answer question about that\
    \ python tree but only newer ones like a wizard coder 34b 0.2 . \n> > \n> > You\
    \ can find my nick mirek190 with tests of those models on huggingface. I did it\
    \ because only those models were worth after tests to say something more in the\
    \ internet.\n> \n> Thanks for providing some examples. Just out of curiosity,\
    \ do you have a specific System Prompt or chat template? In HuggingChat Mixtral\
    \ 8x7B Instruct is not able to answer the first question correctly:\n> \n> \n\
    > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/EVFNVCGWfqavgzomw_bmg.png)\n\
    \nCheck here \n\nhttps://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/discussions/1\n\
    \nTry those questions and you find out that answers are worse with MoMo in spite\
    \ of much higher on leader board.\nIf model s not contaminated then the testing\
    \ procedure is very unreliable right now. \n\nOr you can just talk about everything\
    \ with model and is clear after a few minutes of conversation how smart and  intelligent\
    \ model is... MoMo as I said before has reasoning and intelligence finetuned of\
    \ first llama 65b for me.  "
  created_at: 2024-01-24 18:13:02+00:00
  edited: true
  hidden: false
  id: 65b1532eb4225e3e420fba43
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: moreh/MoMo-72B-lora-1.8.7-DPO
repo_type: model
status: open
target_branch: null
title: GPTQ model is available
