!!python/object:huggingface_hub.community.DiscussionWithDetails
author: james92
conflicting_files: null
created_at: 2022-10-11 06:03:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/223d964e993126fac680a8dfb71cc291.svg
      fullname: James K J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: james92
      type: user
    createdAt: '2022-10-11T07:03:48.000Z'
    data:
      edited: false
      editors:
      - james92
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/223d964e993126fac680a8dfb71cc291.svg
          fullname: James K J
          isHf: false
          isPro: false
          name: james92
          type: user
        html: '<p>Hi, Thanks for the model. Correct me if I am wrong please. I have
          picked both the versions ie. code under <code>zero-shot classification pipeline</code>
          and the code under <code>manual pytorch</code> versions and run against
          the labels <code>[''Positive'',''Neutral'',''Negative'']</code> for the
          sequence <code>one day I will see the world</code>. Below are the results.</p>

          <p>Results (from <code>zero-shot classification pipeline</code>)<br><code>{''sequence'':
          ''one day I will see the world'', ''labels'': [''Positive'', ''Negative'',
          ''Neutral''], ''scores'': [0.48784172534942627, 0.26007547974586487, 0.25208279490470886]}</code></p>

          <p>Results (from <code>Manual Pytorch Version</code>; For the label ''Positive''}<br><code>tensor([0.2946],
          grad_fn=&lt;SelectBackward0&gt;)</code></p>

          <p>If you notice from the both the results for the label <code>positive</code>,
          there is a huge variation. I ran the exact same code given in model page
          in order to test it.   I am doing anything wrong ?. Please help me. Thank
          you.</p>

          <p>Extra Information<br>The logit values from Method <code>Manual Pytorch</code>
          after applying softmax<br><code>tensor([[0.0874, 0.8761, 0.0365]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></p>

          '
        raw: "Hi, Thanks for the model. Correct me if I am wrong please. I have picked\
          \ both the versions ie. code under `zero-shot classification pipeline` and\
          \ the code under `manual pytorch` versions and run against the labels `['Positive','Neutral','Negative']`\
          \ for the sequence `one day I will see the world`. Below are the results.\r\
          \n\r\nResults (from `zero-shot classification pipeline`)  \r\n`{'sequence':\
          \ 'one day I will see the world', 'labels': ['Positive', 'Negative', 'Neutral'],\
          \ 'scores': [0.48784172534942627, 0.26007547974586487, 0.25208279490470886]}`\r\
          \n\r\nResults (from `Manual Pytorch Version`; For the label 'Positive'}\r\
          \n`tensor([0.2946], grad_fn=<SelectBackward0>)`\r\n\r\nIf you notice from\
          \ the both the results for the label `positive`, there is a huge variation.\
          \ I ran the exact same code given in model page in order to test it.   I\
          \ am doing anything wrong ?. Please help me. Thank you.\r\n\r\n\r\nExtra\
          \ Information \r\nThe logit values from Method `Manual Pytorch` after applying\
          \ softmax\r\n`tensor([[0.0874, 0.8761, 0.0365]], grad_fn=<SoftmaxBackward0>)`\r\
          \n\r\n"
        updatedAt: '2022-10-11T07:03:48.742Z'
      numEdits: 0
      reactions: []
    id: 63451554a05b51f7ded1ff53
    type: comment
  author: james92
  content: "Hi, Thanks for the model. Correct me if I am wrong please. I have picked\
    \ both the versions ie. code under `zero-shot classification pipeline` and the\
    \ code under `manual pytorch` versions and run against the labels `['Positive','Neutral','Negative']`\
    \ for the sequence `one day I will see the world`. Below are the results.\r\n\r\
    \nResults (from `zero-shot classification pipeline`)  \r\n`{'sequence': 'one day\
    \ I will see the world', 'labels': ['Positive', 'Negative', 'Neutral'], 'scores':\
    \ [0.48784172534942627, 0.26007547974586487, 0.25208279490470886]}`\r\n\r\nResults\
    \ (from `Manual Pytorch Version`; For the label 'Positive'}\r\n`tensor([0.2946],\
    \ grad_fn=<SelectBackward0>)`\r\n\r\nIf you notice from the both the results for\
    \ the label `positive`, there is a huge variation. I ran the exact same code given\
    \ in model page in order to test it.   I am doing anything wrong ?. Please help\
    \ me. Thank you.\r\n\r\n\r\nExtra Information \r\nThe logit values from Method\
    \ `Manual Pytorch` after applying softmax\r\n`tensor([[0.0874, 0.8761, 0.0365]],\
    \ grad_fn=<SoftmaxBackward0>)`\r\n\r\n"
  created_at: 2022-10-11 06:03:48+00:00
  edited: false
  hidden: false
  id: 63451554a05b51f7ded1ff53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/18ab58760691b6c34cda85f340841dc2.svg
      fullname: Karthik Revanuru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Karthikrevanuru
      type: user
    createdAt: '2022-11-24T04:20:11.000Z'
    data:
      edited: false
      editors:
      - Karthikrevanuru
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/18ab58760691b6c34cda85f340841dc2.svg
          fullname: Karthik Revanuru
          isHf: false
          isPro: false
          name: Karthikrevanuru
          type: user
        html: "<p>Same, <span data-props=\"{&quot;user&quot;:&quot;james92&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/james92\"\
          >@<span class=\"underline\">james92</span></a></span>\n\n\t</span></span>\
          \  were you able to solve it ?</p>\n"
        raw: Same, @james92  were you able to solve it ?
        updatedAt: '2022-11-24T04:20:11.580Z'
      numEdits: 0
      reactions: []
    id: 637ef0fb0fa26740e5517448
    type: comment
  author: Karthikrevanuru
  content: Same, @james92  were you able to solve it ?
  created_at: 2022-11-24 04:20:11+00:00
  edited: false
  hidden: false
  id: 637ef0fb0fa26740e5517448
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/223d964e993126fac680a8dfb71cc291.svg
      fullname: James K J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: james92
      type: user
    createdAt: '2023-03-07T05:39:38.000Z'
    data:
      edited: false
      editors:
      - james92
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/223d964e993126fac680a8dfb71cc291.svg
          fullname: James K J
          isHf: false
          isPro: false
          name: james92
          type: user
        html: '<p>Sorry.No I couldn''t. How about you ?</p>

          '
        raw: Sorry.No I couldn't. How about you ?
        updatedAt: '2023-03-07T05:39:38.638Z'
      numEdits: 0
      reactions: []
    id: 6406ce1a0a16a5dca3309018
    type: comment
  author: james92
  content: Sorry.No I couldn't. How about you ?
  created_at: 2023-03-07 05:39:38+00:00
  edited: false
  hidden: false
  id: 6406ce1a0a16a5dca3309018
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679313805603-640b396091a7c53f34896887.jpeg?w=200&h=200&f=face
      fullname: Adriel Martins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Martins6
      type: user
    createdAt: '2023-03-20T12:23:21.000Z'
    data:
      edited: false
      editors:
      - Martins6
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679313805603-640b396091a7c53f34896887.jpeg?w=200&h=200&f=face
          fullname: Adriel Martins
          isHf: false
          isPro: false
          name: Martins6
          type: user
        html: "<p>Would you be able to share your code, please, <span data-props=\"\
          {&quot;user&quot;:&quot;james92&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/james92\">@<span class=\"underline\">james92</span></a></span>\n\
          \n\t</span></span> ? It seems hard to debug without it. Thanks! :)</p>\n"
        raw: Would you be able to share your code, please, @james92 ? It seems hard
          to debug without it. Thanks! :)
        updatedAt: '2023-03-20T12:23:21.712Z'
      numEdits: 0
      reactions: []
    id: 64185039be72e3e4781e21ed
    type: comment
  author: Martins6
  content: Would you be able to share your code, please, @james92 ? It seems hard
    to debug without it. Thanks! :)
  created_at: 2023-03-20 11:23:21+00:00
  edited: false
  hidden: false
  id: 64185039be72e3e4781e21ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662281006880-noauth.png?w=200&h=200&f=face
      fullname: yuehpo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: y10ab1
      type: user
    createdAt: '2023-04-09T13:39:12.000Z'
    data:
      edited: false
      editors:
      - y10ab1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662281006880-noauth.png?w=200&h=200&f=face
          fullname: yuehpo
          isHf: false
          isPro: false
          name: y10ab1
          type: user
        html: '<p>Hi, I think I found sth from the config of model. Just print model.config
          and you will see following:</p>

          <p>model config: BartConfig {<br>  "_name_or_path": "facebook/bart-large-mnli",<br>  "_num_labels":
          3,<br>  "activation_dropout": 0.0,<br>  "activation_function": "gelu",<br>  "add_final_layer_norm":
          false,<br>  "architectures": [<br>    "BartForSequenceClassification"<br>  ],<br>  "attention_dropout":
          0.0,<br>  "bos_token_id": 0,<br>  "classif_dropout": 0.0,<br>  "classifier_dropout":
          0.0,<br>  "d_model": 1024,<br>  "decoder_attention_heads": 16,<br>  "decoder_ffn_dim":
          4096,<br>  "decoder_layerdrop": 0.0,<br>  "decoder_layers": 12,<br>  "decoder_start_token_id":
          2,<br>  "dropout": 0.1,<br>  "encoder_attention_heads": 16,<br>  "encoder_ffn_dim":
          4096,<br>  "encoder_layerdrop": 0.0,<br>  "encoder_layers": 12,<br>  "eos_token_id":
          2,<br>  "forced_eos_token_id": 2,<br>  "gradient_checkpointing": false,<br>  "id2label":
          {<br>    "0": "contradiction",<br>    "1": "neutral",<br>    "2": "entailment"<br>  },<br>  "init_std":
          0.02,<br>  "is_encoder_decoder": true,<br>  "label2id": {<br>    "contradiction":
          0,<br>    "entailment": 2,<br>    "neutral": 1<br>  },<br>  "max_position_embeddings":
          1024,<br>  "model_type": "bart",<br>  "normalize_before": false,<br>  "num_hidden_layers":
          12,<br>  "output_past": false,<br>  "pad_token_id": 1,<br>  "scale_embedding":
          false,<br>  "transformers_version": "4.27.3",<br>  "use_cache": true,<br>  "vocab_size":
          50265<br>}</p>

          <p>As you can see, the label default is,<br>    "0": "contradiction",<br>    "1":
          "neutral",<br>    "2": "entailment"</p>

          <p>which means the highest probability in your case is actually "neutral".</p>

          <p>Try to change the hypothesis to  ''This example is positive.''<br>You
          may find that the prob of entailment will be the highest one.</p>

          '
        raw: "Hi, I think I found sth from the config of model. Just print model.config\
          \ and you will see following:\n\nmodel config: BartConfig {\n  \"_name_or_path\"\
          : \"facebook/bart-large-mnli\",\n  \"_num_labels\": 3,\n  \"activation_dropout\"\
          : 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\"\
          : false,\n  \"architectures\": [\n    \"BartForSequenceClassification\"\n\
          \  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\"\
          : 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\"\
          : 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"\
          decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\":\
          \ 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n\
          \  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\"\
          : 2,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n\
          \  \"id2label\": {\n    \"0\": \"contradiction\",\n    \"1\": \"neutral\"\
          ,\n    \"2\": \"entailment\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\"\
          : true,\n  \"label2id\": {\n    \"contradiction\": 0,\n    \"entailment\"\
          : 2,\n    \"neutral\": 1\n  },\n  \"max_position_embeddings\": 1024,\n \
          \ \"model_type\": \"bart\",\n  \"normalize_before\": false,\n  \"num_hidden_layers\"\
          : 12,\n  \"output_past\": false,\n  \"pad_token_id\": 1,\n  \"scale_embedding\"\
          : false,\n  \"transformers_version\": \"4.27.3\",\n  \"use_cache\": true,\n\
          \  \"vocab_size\": 50265\n}\n\nAs you can see, the label default is,\n \
          \   \"0\": \"contradiction\",\n    \"1\": \"neutral\",\n    \"2\": \"entailment\"\
          \n\nwhich means the highest probability in your case is actually \"neutral\"\
          .\n\nTry to change the hypothesis to  'This example is positive.' \nYou\
          \ may find that the prob of entailment will be the highest one."
        updatedAt: '2023-04-09T13:39:12.888Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Martins6
    id: 6432c00037d643c2690e4dbb
    type: comment
  author: y10ab1
  content: "Hi, I think I found sth from the config of model. Just print model.config\
    \ and you will see following:\n\nmodel config: BartConfig {\n  \"_name_or_path\"\
    : \"facebook/bart-large-mnli\",\n  \"_num_labels\": 3,\n  \"activation_dropout\"\
    : 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n\
    \  \"architectures\": [\n    \"BartForSequenceClassification\"\n  ],\n  \"attention_dropout\"\
    : 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\"\
    : 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\"\
    : 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\"\
    : 2,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\"\
    : 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\"\
    : 2,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"\
    id2label\": {\n    \"0\": \"contradiction\",\n    \"1\": \"neutral\",\n    \"\
    2\": \"entailment\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n\
    \  \"label2id\": {\n    \"contradiction\": 0,\n    \"entailment\": 2,\n    \"\
    neutral\": 1\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"\
    bart\",\n  \"normalize_before\": false,\n  \"num_hidden_layers\": 12,\n  \"output_past\"\
    : false,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"transformers_version\"\
    : \"4.27.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nAs you can\
    \ see, the label default is,\n    \"0\": \"contradiction\",\n    \"1\": \"neutral\"\
    ,\n    \"2\": \"entailment\"\n\nwhich means the highest probability in your case\
    \ is actually \"neutral\".\n\nTry to change the hypothesis to  'This example is\
    \ positive.' \nYou may find that the prob of entailment will be the highest one."
  created_at: 2023-04-09 12:39:12+00:00
  edited: false
  hidden: false
  id: 6432c00037d643c2690e4dbb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: facebook/bart-large-mnli
repo_type: model
status: open
target_branch: null
title: zero-shot classification pipeline and manual pytorch versions are different
  results
