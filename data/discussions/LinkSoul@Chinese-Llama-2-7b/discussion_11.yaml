!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aldennX
conflicting_files: null
created_at: 2023-08-06 09:29:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T10:29:53.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.25368407368659973
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>ERROR:Failed to load the model.<br>Traceback (most recent call last):<br>  File
          "C:\Users\Administrator\text-generation-webui\server.py", line 68, in load_model_wrapper<br>    shared.model,
          shared.tokenizer = load_model(shared.model_name, loader)<br>  File "C:\Users\Administrator\text-generation-webui\modules\models.py",
          line 78, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "C:\Users\Administrator\text-generation-webui\modules\models.py", line 232,
          in llamacpp_loader<br>    from modules.llamacpp_model import LlamaCppModel<br>  File
          "C:\Users\Administrator\text-generation-webui\modules\llamacpp_model.py",
          line 11, in <br>    import llama_cpp<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp_<em>init</em>_.py",
          line 1, in <br>    from .llama_cpp import *<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 1292, in <br>    llama_backend_init(c_bool(False))<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 403, in llama_backend_init<br>    return _lib.llama_backend_init(numa)<br>OSError:
          [WinError -1073741795] Windows Error 0xc000001d</p>

          '
        raw: "ERROR:Failed to load the model.\r\nTraceback (most recent call last):\r\
          \n  File \"C:\\Users\\Administrator\\text-generation-webui\\server.py\"\
          , line 68, in load_model_wrapper\r\n    shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\r\n  File \"C:\\Users\\Administrator\\\
          text-generation-webui\\modules\\models.py\", line 78, in load_model\r\n\
          \    output = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\\
          Administrator\\text-generation-webui\\modules\\models.py\", line 232, in\
          \ llamacpp_loader\r\n    from modules.llamacpp_model import LlamaCppModel\r\
          \n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 11, in <module>\r\n    import llama_cpp\r\n  File \"C:\\Users\\Administrator\\\
          .conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\__init__.py\", line\
          \ 1, in <module>\r\n    from .llama_cpp import *\r\n  File \"C:\\Users\\\
          Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\"\
          , line 1292, in <module>\r\n    llama_backend_init(c_bool(False))\r\n  File\
          \ \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp\\llama_cpp.py\", line 403, in llama_backend_init\r\n    return\
          \ _lib.llama_backend_init(numa)\r\nOSError: [WinError -1073741795] Windows\
          \ Error 0xc000001d"
        updatedAt: '2023-08-06T10:29:53.953Z'
      numEdits: 0
      reactions: []
    id: 64cf76215c86caf951e0ae23
    type: comment
  author: aldennX
  content: "ERROR:Failed to load the model.\r\nTraceback (most recent call last):\r\
    \n  File \"C:\\Users\\Administrator\\text-generation-webui\\server.py\", line\
    \ 68, in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\\
    models.py\", line 78, in load_model\r\n    output = load_func_map[loader](model_name)\r\
    \n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\models.py\"\
    , line 232, in llamacpp_loader\r\n    from modules.llamacpp_model import LlamaCppModel\r\
    \n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\llamacpp_model.py\"\
    , line 11, in <module>\r\n    import llama_cpp\r\n  File \"C:\\Users\\Administrator\\\
    .conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\__init__.py\", line 1, in\
    \ <module>\r\n    from .llama_cpp import *\r\n  File \"C:\\Users\\Administrator\\\
    .conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\", line 1292,\
    \ in <module>\r\n    llama_backend_init(c_bool(False))\r\n  File \"C:\\Users\\\
    Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\"\
    , line 403, in llama_backend_init\r\n    return _lib.llama_backend_init(numa)\r\
    \nOSError: [WinError -1073741795] Windows Error 0xc000001d"
  created_at: 2023-08-06 09:29:53+00:00
  edited: false
  hidden: false
  id: 64cf76215c86caf951e0ae23
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: LinkSoul/Chinese-Llama-2-7b
repo_type: model
status: open
target_branch: null
title: "oobabooga-windows \u51FA\u9519  !   \u9009\u7684\u662Fllmam.cpp"
