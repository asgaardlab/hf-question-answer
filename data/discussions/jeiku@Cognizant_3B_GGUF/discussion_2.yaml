!!python/object:huggingface_hub.community.DiscussionWithDetails
author: matrixportalx
conflicting_files: null
created_at: 2024-01-09 14:38:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd3cdd19c847193f97dadf8ef7d6565b.svg
      fullname: Matrix Portal X
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matrixportalx
      type: user
    createdAt: '2024-01-09T14:38:26.000Z'
    data:
      edited: false
      editors:
      - matrixportalx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9714594483375549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd3cdd19c847193f97dadf8ef7d6565b.svg
          fullname: Matrix Portal X
          isHf: false
          isPro: false
          name: matrixportalx
          type: user
        html: '<p>I just discovered your projects. I have an almost 20 years old laptop.
          I wanted to try AI models until I get a new one. I tried many gguf format
          models such as Llama 2 7B, Vicuna, Llava, etc. Even if it works on my device,
          I couldn''t get much efficiency because it was too slow. Then I wanted to
          give 3D models a chance. I started with your Rosa v1 model. I also tried
          Rosa v3 (Rosa v1 is better). Finally I discovered the Cognizant 3D model
          Q6 gguf format which is really a great model. Thank you very much for sharing
          this project with us. It''s great to be able to run our own offline AI model
          on an old laptop with an old CPU and get responses almost close to the responses
          of the Llama 2 7B, 13B and even 70B models. I wish you success and continuation
          of your projects my friend.</p>

          '
        raw: I just discovered your projects. I have an almost 20 years old laptop.
          I wanted to try AI models until I get a new one. I tried many gguf format
          models such as Llama 2 7B, Vicuna, Llava, etc. Even if it works on my device,
          I couldn't get much efficiency because it was too slow. Then I wanted to
          give 3D models a chance. I started with your Rosa v1 model. I also tried
          Rosa v3 (Rosa v1 is better). Finally I discovered the Cognizant 3D model
          Q6 gguf format which is really a great model. Thank you very much for sharing
          this project with us. It's great to be able to run our own offline AI model
          on an old laptop with an old CPU and get responses almost close to the responses
          of the Llama 2 7B, 13B and even 70B models. I wish you success and continuation
          of your projects my friend.
        updatedAt: '2024-01-09T14:38:26.939Z'
      numEdits: 0
      reactions: []
    id: 659d5a62fbe15cfc784703b9
    type: comment
  author: matrixportalx
  content: I just discovered your projects. I have an almost 20 years old laptop.
    I wanted to try AI models until I get a new one. I tried many gguf format models
    such as Llama 2 7B, Vicuna, Llava, etc. Even if it works on my device, I couldn't
    get much efficiency because it was too slow. Then I wanted to give 3D models a
    chance. I started with your Rosa v1 model. I also tried Rosa v3 (Rosa v1 is better).
    Finally I discovered the Cognizant 3D model Q6 gguf format which is really a great
    model. Thank you very much for sharing this project with us. It's great to be
    able to run our own offline AI model on an old laptop with an old CPU and get
    responses almost close to the responses of the Llama 2 7B, 13B and even 70B models.
    I wish you success and continuation of your projects my friend.
  created_at: 2024-01-09 14:38:26+00:00
  edited: false
  hidden: false
  id: 659d5a62fbe15cfc784703b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/a4AlmzpXRwLW_Z93Velts.jpeg?w=200&h=200&f=face
      fullname: jeiku
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jeiku
      type: user
    createdAt: '2024-01-09T21:49:38.000Z'
    data:
      edited: false
      editors:
      - jeiku
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9711380004882812
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/a4AlmzpXRwLW_Z93Velts.jpeg?w=200&h=200&f=face
          fullname: jeiku
          isHf: false
          isPro: false
          name: jeiku
          type: user
        html: '<p>Thank you very much! I also believe Rosa v1 is better and have begun
          using it as the base of all my finetuning projects. Personally, I have the
          hardware to run 7-13B models, but I am very much drawn to 3B as they can
          be run so much more efficiently or in parallel with other software like
          multimodal captioning models or image generation models.</p>

          <p>Thank you for your kind words!</p>

          '
        raw: 'Thank you very much! I also believe Rosa v1 is better and have begun
          using it as the base of all my finetuning projects. Personally, I have the
          hardware to run 7-13B models, but I am very much drawn to 3B as they can
          be run so much more efficiently or in parallel with other software like
          multimodal captioning models or image generation models.


          Thank you for your kind words!'
        updatedAt: '2024-01-09T21:49:38.044Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - matrixportalx
    id: 659dbf7218dc73602915ee81
    type: comment
  author: jeiku
  content: 'Thank you very much! I also believe Rosa v1 is better and have begun using
    it as the base of all my finetuning projects. Personally, I have the hardware
    to run 7-13B models, but I am very much drawn to 3B as they can be run so much
    more efficiently or in parallel with other software like multimodal captioning
    models or image generation models.


    Thank you for your kind words!'
  created_at: 2024-01-09 21:49:38+00:00
  edited: false
  hidden: false
  id: 659dbf7218dc73602915ee81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/a4AlmzpXRwLW_Z93Velts.jpeg?w=200&h=200&f=face
      fullname: jeiku
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jeiku
      type: user
    createdAt: '2024-01-10T21:29:04.000Z'
    data:
      status: closed
    id: 659f0c20b62804e6f4230798
    type: status-change
  author: jeiku
  created_at: 2024-01-10 21:29:04+00:00
  id: 659f0c20b62804e6f4230798
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jeiku/Cognizant_3B_GGUF
repo_type: model
status: closed
target_branch: null
title: Great AI model
