!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jwilkins96
conflicting_files: null
created_at: 2023-01-30 05:48:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b4e3d1b3e72fe7f5b1daa46726004be.svg
      fullname: Julia Wilkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jwilkins96
      type: user
    createdAt: '2023-01-30T05:48:46.000Z'
    data:
      edited: false
      editors:
      - jwilkins96
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b4e3d1b3e72fe7f5b1daa46726004be.svg
          fullname: Julia Wilkins
          isHf: false
          isPro: false
          name: jwilkins96
          type: user
        html: '<p>Hi all - great work here. I had one clarification on the sampling
          method used during training. The paper and card here says that "8 frames
          per video" were used. Are these frames sampled from one point in the video,
          consecutively and at a high frame rate (i.e. frames 21, 22, 23... 28), or
          taken from different points sparsely in the video (i.e. frames 1, 200, 400,
          600...). In the cited works that use sparse sampling, it seems as though
          single frames are extracted from different salient "segments" of the video,
          but in the demo notebook here: <a rel="nofollow" href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Video_text_matching_with_X_CLIP.ipynb">https://github.com/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Video_text_matching_with_X_CLIP.ipynb</a>,
          frames are extracted at a high frame rate consecutively. I just want to
          make sure that at inference time, I''m sampling frames in the same way that
          the model was trained in. Thank you!</p>

          '
        raw: 'Hi all - great work here. I had one clarification on the sampling method
          used during training. The paper and card here says that "8 frames per video"
          were used. Are these frames sampled from one point in the video, consecutively
          and at a high frame rate (i.e. frames 21, 22, 23... 28), or taken from different
          points sparsely in the video (i.e. frames 1, 200, 400, 600...). In the cited
          works that use sparse sampling, it seems as though single frames are extracted
          from different salient "segments" of the video, but in the demo notebook
          here: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Video_text_matching_with_X_CLIP.ipynb,
          frames are extracted at a high frame rate consecutively. I just want to
          make sure that at inference time, I''m sampling frames in the same way that
          the model was trained in. Thank you!'
        updatedAt: '2023-01-30T05:48:46.209Z'
      numEdits: 0
      reactions: []
    id: 63d75a3e2e397d9f8e30170b
    type: comment
  author: jwilkins96
  content: 'Hi all - great work here. I had one clarification on the sampling method
    used during training. The paper and card here says that "8 frames per video" were
    used. Are these frames sampled from one point in the video, consecutively and
    at a high frame rate (i.e. frames 21, 22, 23... 28), or taken from different points
    sparsely in the video (i.e. frames 1, 200, 400, 600...). In the cited works that
    use sparse sampling, it seems as though single frames are extracted from different
    salient "segments" of the video, but in the demo notebook here: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Video_text_matching_with_X_CLIP.ipynb,
    frames are extracted at a high frame rate consecutively. I just want to make sure
    that at inference time, I''m sampling frames in the same way that the model was
    trained in. Thank you!'
  created_at: 2023-01-30 05:48:46+00:00
  edited: false
  hidden: false
  id: 63d75a3e2e397d9f8e30170b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed2d5709242811f99727344f63ce298c.svg
      fullname: Mwni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mwni
      type: user
    createdAt: '2023-06-10T23:13:18.000Z'
    data:
      edited: false
      editors:
      - Mwni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8140224814414978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed2d5709242811f99727344f63ce298c.svg
          fullname: Mwni
          isHf: false
          isPro: false
          name: Mwni
          type: user
        html: '<p>A very good question that remains unanswered. The only thing to
          be found about this is the default value of a reference implementation''s
          training args: <a rel="nofollow" href="https://github.com/xuguohai/X-CLIP/blob/6b5344f44537d758acb82d115b8484f7430f9fb0/main_xclip.py#L47">https://github.com/xuguohai/X-CLIP/blob/6b5344f44537d758acb82d115b8484f7430f9fb0/main_xclip.py#L47</a><br>Did
          Microsoft use the default value when they trained the model? Nobody knows.</p>

          '
        raw: 'A very good question that remains unanswered. The only thing to be found
          about this is the default value of a reference implementation''s training
          args: https://github.com/xuguohai/X-CLIP/blob/6b5344f44537d758acb82d115b8484f7430f9fb0/main_xclip.py#L47

          Did Microsoft use the default value when they trained the model? Nobody
          knows.'
        updatedAt: '2023-06-10T23:13:18.095Z'
      numEdits: 0
      reactions: []
    id: 6485038e6bc1cfb5dc244bc1
    type: comment
  author: Mwni
  content: 'A very good question that remains unanswered. The only thing to be found
    about this is the default value of a reference implementation''s training args:
    https://github.com/xuguohai/X-CLIP/blob/6b5344f44537d758acb82d115b8484f7430f9fb0/main_xclip.py#L47

    Did Microsoft use the default value when they trained the model? Nobody knows.'
  created_at: 2023-06-10 22:13:18+00:00
  edited: false
  hidden: false
  id: 6485038e6bc1cfb5dc244bc1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: microsoft/xclip-base-patch32
repo_type: model
status: open
target_branch: null
title: Sparse sampling question
