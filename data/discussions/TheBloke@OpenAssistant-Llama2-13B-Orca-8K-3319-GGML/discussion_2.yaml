!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YAKOVNUKJHJ
conflicting_files: null
created_at: 2023-07-26 09:16:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
      fullname: YAKOV BRL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YAKOVNUKJHJ
      type: user
    createdAt: '2023-07-26T10:16:46.000Z'
    data:
      edited: false
      editors:
      - YAKOVNUKJHJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8594100475311279
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
          fullname: YAKOV BRL
          isHf: false
          isPro: false
          name: YAKOVNUKJHJ
          type: user
        html: '<p>I tried several times, and it replies with blank text, producing
          a long loop of blank text, without any characters</p>

          '
        raw: I tried several times, and it replies with blank text, producing a long
          loop of blank text, without any characters
        updatedAt: '2023-07-26T10:16:46.750Z'
      numEdits: 0
      reactions: []
    id: 64c0f28e200a51559693d6b5
    type: comment
  author: YAKOVNUKJHJ
  content: I tried several times, and it replies with blank text, producing a long
    loop of blank text, without any characters
  created_at: 2023-07-26 09:16:46+00:00
  edited: false
  hidden: false
  id: 64c0f28e200a51559693d6b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
      fullname: YAKOV BRL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YAKOVNUKJHJ
      type: user
    createdAt: '2023-07-26T10:17:20.000Z'
    data:
      edited: false
      editors:
      - YAKOVNUKJHJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9893847107887268
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
          fullname: YAKOV BRL
          isHf: false
          isPro: false
          name: YAKOVNUKJHJ
          type: user
        html: '<p>On short questions, he was fine, once it was an answer that should
          be long, he just printed over and over, spaces, and empty paragraph breaks</p>

          '
        raw: On short questions, he was fine, once it was an answer that should be
          long, he just printed over and over, spaces, and empty paragraph breaks
        updatedAt: '2023-07-26T10:17:20.500Z'
      numEdits: 0
      reactions: []
    id: 64c0f2b00cfdb492ce95570b
    type: comment
  author: YAKOVNUKJHJ
  content: On short questions, he was fine, once it was an answer that should be long,
    he just printed over and over, spaces, and empty paragraph breaks
  created_at: 2023-07-26 09:17:20+00:00
  edited: false
  hidden: false
  id: 64c0f2b00cfdb492ce95570b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-07-26T12:06:30.000Z'
    data:
      edited: false
      editors:
      - dzupin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7199533581733704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
          fullname: Robert Dzupin
          isHf: false
          isPro: false
          name: dzupin
          type: user
        html: '<p>I had the same problem. Longer answers became garbled for me. Solution
          is provided by comment made by TheBloke in second thread of discussion for
          this model. </p>

          <p>In short: Current instructions in README for this model in how to use
          this model is faulty</p>

          <p>This is 8K model therefore following line from README is wrong:   </p>

          <p>./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin
          --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "&lt;|system|&gt;You
          are a story writing assistant&lt;|prompter|&gt;write a story about llamas&lt;|assistant|&gt;"</p>

          <p>Instead your should use (info provided by TheBloke) and update it to
          :<br>./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin
          --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7
          --repeat_penalty 1.1 -n -1 -p "&lt;|system|&gt;You are a story writing assistant&lt;|prompter|&gt;write
          a story about llamas&lt;|assistant|&gt;"</p>

          '
        raw: "I had the same problem. Longer answers became garbled for me. Solution\
          \ is provided by comment made by TheBloke in second thread of discussion\
          \ for this model. \n\nIn short: Current instructions in README for this\
          \ model in how to use this model is faulty\n\nThis is 8K model therefore\
          \ following line from README is wrong:   \n\n./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
          \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|system|>You\
          \ are a story writing assistant</s><|prompter|>write a story about llamas</s><|assistant|>\"\
          \n\n\nInstead your should use (info provided by TheBloke) and update it\
          \ to : \n./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
          \ --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7\
          \ --repeat_penalty 1.1 -n -1 -p \"<|system|>You are a story writing assistant</s><|prompter|>write\
          \ a story about llamas</s><|assistant|>\"\n"
        updatedAt: '2023-07-26T12:06:30.038Z'
      numEdits: 0
      reactions: []
    id: 64c10c46f20fc9b9ea77eacd
    type: comment
  author: dzupin
  content: "I had the same problem. Longer answers became garbled for me. Solution\
    \ is provided by comment made by TheBloke in second thread of discussion for this\
    \ model. \n\nIn short: Current instructions in README for this model in how to\
    \ use this model is faulty\n\nThis is 8K model therefore following line from README\
    \ is wrong:   \n\n./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
    \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|system|>You are\
    \ a story writing assistant</s><|prompter|>write a story about llamas</s><|assistant|>\"\
    \n\n\nInstead your should use (info provided by TheBloke) and update it to : \n\
    ./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
    \ --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7 --repeat_penalty\
    \ 1.1 -n -1 -p \"<|system|>You are a story writing assistant</s><|prompter|>write\
    \ a story about llamas</s><|assistant|>\"\n"
  created_at: 2023-07-26 11:06:30+00:00
  edited: false
  hidden: false
  id: 64c10c46f20fc9b9ea77eacd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T12:53:22.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9324620366096497
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Does that fix the problem <span data-props=\"{&quot;user&quot;:&quot;dzupin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dzupin\"\
          >@<span class=\"underline\">dzupin</span></a></span>\n\n\t</span></span>\
          \ ?  You are right those params should be in there for 8K usage.  But I\
          \ did not think they were needed also for &lt;4K usage.</p>\n<p>I just tried\
          \ <code>./main -t 10 -ngl 50 -m /workspace/openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
          \ --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7\
          \ --repeat_penalty 1.1 -n -1 -p</code> with a long prompt (summarise an\
          \ existing story) and it still returned nothing, like others are reporting.</p>\n\
          <p>It could be a problem with the custom tokens of this model in GGML -\
          \ I have had reports of that before.  I don't think there's anything I can\
          \ do about that until GGML changes/improves its tokenisation.  But there\
          \ might be some llama.cpp options that would improve it; let me know if\
          \ anyone finds anything.</p>\n"
        raw: 'Does that fix the problem @dzupin ?  You are right those params should
          be in there for 8K usage.  But I did not think they were needed also for
          <4K usage.


          I just tried `./main -t 10 -ngl 50 -m /workspace/openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin
          --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7
          --repeat_penalty 1.1 -n -1 -p` with a long prompt (summarise an existing
          story) and it still returned nothing, like others are reporting.


          It could be a problem with the custom tokens of this model in GGML - I have
          had reports of that before.  I don''t think there''s anything I can do about
          that until GGML changes/improves its tokenisation.  But there might be some
          llama.cpp options that would improve it; let me know if anyone finds anything.'
        updatedAt: '2023-07-26T12:53:36.316Z'
      numEdits: 1
      reactions: []
    id: 64c117424f1deeecbbc7af72
    type: comment
  author: TheBloke
  content: 'Does that fix the problem @dzupin ?  You are right those params should
    be in there for 8K usage.  But I did not think they were needed also for <4K usage.


    I just tried `./main -t 10 -ngl 50 -m /workspace/openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin
    --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7 --repeat_penalty
    1.1 -n -1 -p` with a long prompt (summarise an existing story) and it still returned
    nothing, like others are reporting.


    It could be a problem with the custom tokens of this model in GGML - I have had
    reports of that before.  I don''t think there''s anything I can do about that
    until GGML changes/improves its tokenisation.  But there might be some llama.cpp
    options that would improve it; let me know if anyone finds anything.'
  created_at: 2023-07-26 11:53:22+00:00
  edited: true
  hidden: false
  id: 64c117424f1deeecbbc7af72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
      fullname: YAKOV BRL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YAKOVNUKJHJ
      type: user
    createdAt: '2023-07-26T13:18:58.000Z'
    data:
      edited: false
      editors:
      - YAKOVNUKJHJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9364079833030701
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
          fullname: YAKOV BRL
          isHf: false
          isPro: false
          name: YAKOVNUKJHJ
          type: user
        html: "<blockquote>\n<p>Does that fix the problem <span data-props=\"{&quot;user&quot;:&quot;dzupin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dzupin\"\
          >@<span class=\"underline\">dzupin</span></a></span>\n\n\t</span></span>\
          \ ?  You are right those params should be in there for 8K usage.  But I\
          \ did not think they were needed also for &lt;4K usage.</p>\n<p>I just tried\
          \ <code>./main -t 10 -ngl 50 -m /workspace/openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
          \ --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7\
          \ --repeat_penalty 1.1 -n -1 -p</code> with a long prompt (summarise an\
          \ existing story) and it still returned nothing, like others are reporting.</p>\n\
          <p>It could be a problem with the custom tokens of this model in GGML -\
          \ I have had reports of that before.  I don't think there's anything I can\
          \ do about that until GGML changes/improves its tokenisation.  But there\
          \ might be some llama.cpp options that would improve it; let me know if\
          \ anyone finds anything.</p>\n</blockquote>\n<p>Well, thanks at least for\
          \ all your effort, I download every day at least one new model to play with,\
          \ never boring</p>\n"
        raw: "> Does that fix the problem @dzupin ?  You are right those params should\
          \ be in there for 8K usage.  But I did not think they were needed also for\
          \ <4K usage.\n> \n> I just tried `./main -t 10 -ngl 50 -m /workspace/openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
          \ --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7\
          \ --repeat_penalty 1.1 -n -1 -p` with a long prompt (summarise an existing\
          \ story) and it still returned nothing, like others are reporting.\n> \n\
          > It could be a problem with the custom tokens of this model in GGML - I\
          \ have had reports of that before.  I don't think there's anything I can\
          \ do about that until GGML changes/improves its tokenisation.  But there\
          \ might be some llama.cpp options that would improve it; let me know if\
          \ anyone finds anything.\n\nWell, thanks at least for all your effort, I\
          \ download every day at least one new model to play with, never boring"
        updatedAt: '2023-07-26T13:18:58.806Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c11d42a20269b8bd948379
    id: 64c11d42a20269b8bd948376
    type: comment
  author: YAKOVNUKJHJ
  content: "> Does that fix the problem @dzupin ?  You are right those params should\
    \ be in there for 8K usage.  But I did not think they were needed also for <4K\
    \ usage.\n> \n> I just tried `./main -t 10 -ngl 50 -m /workspace/openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin\
    \ --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7 --repeat_penalty\
    \ 1.1 -n -1 -p` with a long prompt (summarise an existing story) and it still\
    \ returned nothing, like others are reporting.\n> \n> It could be a problem with\
    \ the custom tokens of this model in GGML - I have had reports of that before.\
    \  I don't think there's anything I can do about that until GGML changes/improves\
    \ its tokenisation.  But there might be some llama.cpp options that would improve\
    \ it; let me know if anyone finds anything.\n\nWell, thanks at least for all your\
    \ effort, I download every day at least one new model to play with, never boring"
  created_at: 2023-07-26 12:18:58+00:00
  edited: false
  hidden: false
  id: 64c11d42a20269b8bd948376
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
      fullname: YAKOV BRL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YAKOVNUKJHJ
      type: user
    createdAt: '2023-07-26T13:18:58.000Z'
    data:
      status: closed
    id: 64c11d42a20269b8bd948379
    type: status-change
  author: YAKOVNUKJHJ
  created_at: 2023-07-26 12:18:58+00:00
  id: 64c11d42a20269b8bd948379
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-07-26T15:50:42.000Z'
    data:
      edited: false
      editors:
      - dzupin
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.2921864986419678
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
          fullname: Robert Dzupin
          isHf: false
          isPro: false
          name: dzupin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span><br>using parameters:\
          \   -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5      fixed several\
          \ problems for me</p>\n<p>e.g.<br>main.exe  -t 16 -ngl 44 -m C:\\AI\\WIP\\\
          8K\\openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_M.bin -s 7 --color\
          \ -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7 --repeat_penalty\
          \ 1.1  -p \"### Instruction: Explain in great detail the reason for number\
          \ 42 to be an important number\\n### Response:\" -ins</p>\n<p> fixed following\
          \ problems for me:</p>\n<ol>\n<li><p>Same prompt with same seed produces\
          \ longer more insightful/interesting  output with 8k         -c 8192 --rope-freq-base\
          \ 10000 --rope-freq-scale 0.5   </p>\n</li>\n<li><p>I can use instruction\
          \ mode ( -ins)   to enter follow up requests in my prompt and model produce\
          \ replies without any issues if I use parameters   -c 8192 --rope-freq-base\
          \ 10000 --rope-freq-scale 0.5<br>But if I use only  -c 4096 then only first\
          \ reply is generated in -ins mode and after that only infinite loop of empty\
          \ lines is generated by model.  (-c 4096 is therefore unusable for me when\
          \ parameter -ins is specified ) </p>\n</li>\n<li><p>Occasionally/randomly\
          \  even prompt with only around 500 tokes, after initially correct output\
          \ generates something like:<br>previuoiououiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui^C<br>I\
          \ have difficulty to reliable reproduce third issue. Most prompt works fine\
          \ with -c 4096 or even with -c 2048, until they don't</p>\n</li>\n</ol>\n\
          <p>When using  -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   I\
          \ never ran into garbled text problem  </p>\n"
        raw: "@TheBloke  \nusing parameters:   -c 8192 --rope-freq-base 10000 --rope-freq-scale\
          \ 0.5      fixed several problems for me\n\ne.g.   \nmain.exe  -t 16 -ngl\
          \ 44 -m C:\\AI\\WIP\\8K\\openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_M.bin\
          \ -s 7 --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp\
          \ 0.7 --repeat_penalty 1.1  -p \"### Instruction: Explain in great detail\
          \ the reason for number 42 to be an important number\\n### Response:\" -ins\n\
          \n fixed following problems for me:\n1. Same prompt with same seed produces\
          \ longer more insightful/interesting  output with 8k         -c 8192 --rope-freq-base\
          \ 10000 --rope-freq-scale 0.5   \n\n2. I can use instruction mode ( -ins)\
          \   to enter follow up requests in my prompt and model produce replies without\
          \ any issues if I use parameters   -c 8192 --rope-freq-base 10000 --rope-freq-scale\
          \ 0.5   \n   But if I use only  -c 4096 then only first reply is generated\
          \ in -ins mode and after that only infinite loop of empty lines is generated\
          \ by model.  (-c 4096 is therefore unusable for me when parameter -ins is\
          \ specified ) \n\n3. Occasionally/randomly  even prompt with only around\
          \ 500 tokes, after initially correct output generates something like:\n\
          previuoiououiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui^C\n\
          I have difficulty to reliable reproduce third issue. Most prompt works fine\
          \ with -c 4096 or even with -c 2048, until they don't \n\nWhen using  -c\
          \ 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   I never ran into garbled\
          \ text problem  \n\n\n\n\n\n\n\n"
        updatedAt: '2023-07-26T15:50:42.380Z'
      numEdits: 0
      reactions: []
    id: 64c140d25258457cc51def12
    type: comment
  author: dzupin
  content: "@TheBloke  \nusing parameters:   -c 8192 --rope-freq-base 10000 --rope-freq-scale\
    \ 0.5      fixed several problems for me\n\ne.g.   \nmain.exe  -t 16 -ngl 44 -m\
    \ C:\\AI\\WIP\\8K\\openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_M.bin -s\
    \ 7 --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7 --repeat_penalty\
    \ 1.1  -p \"### Instruction: Explain in great detail the reason for number 42\
    \ to be an important number\\n### Response:\" -ins\n\n fixed following problems\
    \ for me:\n1. Same prompt with same seed produces longer more insightful/interesting\
    \  output with 8k         -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5\
    \   \n\n2. I can use instruction mode ( -ins)   to enter follow up requests in\
    \ my prompt and model produce replies without any issues if I use parameters \
    \  -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   \n   But if I use only\
    \  -c 4096 then only first reply is generated in -ins mode and after that only\
    \ infinite loop of empty lines is generated by model.  (-c 4096 is therefore unusable\
    \ for me when parameter -ins is specified ) \n\n3. Occasionally/randomly  even\
    \ prompt with only around 500 tokes, after initially correct output generates\
    \ something like:\npreviuoiououiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui^C\n\
    I have difficulty to reliable reproduce third issue. Most prompt works fine with\
    \ -c 4096 or even with -c 2048, until they don't \n\nWhen using  -c 8192 --rope-freq-base\
    \ 10000 --rope-freq-scale 0.5   I never ran into garbled text problem  \n\n\n\n\
    \n\n\n\n"
  created_at: 2023-07-26 14:50:42+00:00
  edited: false
  hidden: false
  id: 64c140d25258457cc51def12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
      fullname: YAKOV BRL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YAKOVNUKJHJ
      type: user
    createdAt: '2023-07-26T15:52:14.000Z'
    data:
      edited: false
      editors:
      - YAKOVNUKJHJ
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.27485260367393494
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
          fullname: YAKOV BRL
          isHf: false
          isPro: false
          name: YAKOVNUKJHJ
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span><br>using\
          \ parameters:   -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   \
          \   fixed several problems for me</p>\n<p>e.g.<br>main.exe  -t 16 -ngl 44\
          \ -m C:\\AI\\WIP\\8K\\openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_M.bin\
          \ -s 7 --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp\
          \ 0.7 --repeat_penalty 1.1  -p \"### Instruction: Explain in great detail\
          \ the reason for number 42 to be an important number\\n### Response:\" -ins</p>\n\
          <p> fixed following problems for me:</p>\n<ol>\n<li><p>Same prompt with\
          \ same seed produces longer more insightful/interesting  output with 8k\
          \         -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   </p>\n\
          </li>\n<li><p>I can use instruction mode ( -ins)   to enter follow up requests\
          \ in my prompt and model produce replies without any issues if I use parameters\
          \   -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5<br>But if I use\
          \ only  -c 4096 then only first reply is generated in -ins mode and after\
          \ that only infinite loop of empty lines is generated by model.  (-c 4096\
          \ is therefore unusable for me when parameter -ins is specified ) </p>\n\
          </li>\n<li><p>Occasionally/randomly  even prompt with only around 500 tokes,\
          \ after initially correct output generates something like:<br>previuoiououiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui^C<br>I\
          \ have difficulty to reliable reproduce third issue. Most prompt works fine\
          \ with -c 4096 or even with -c 2048, until they don't</p>\n</li>\n</ol>\n\
          <p>When using  -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   I\
          \ never ran into garbled text problem</p>\n</blockquote>\n<p>I work in LLamaSharp\
          \ in C#, so none of this helps me... but thanks for the advice</p>\n"
        raw: "> @TheBloke  \n> using parameters:   -c 8192 --rope-freq-base 10000\
          \ --rope-freq-scale 0.5      fixed several problems for me\n> \n> e.g. \
          \  \n> main.exe  -t 16 -ngl 44 -m C:\\AI\\WIP\\8K\\openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_M.bin\
          \ -s 7 --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp\
          \ 0.7 --repeat_penalty 1.1  -p \"### Instruction: Explain in great detail\
          \ the reason for number 42 to be an important number\\n### Response:\" -ins\n\
          > \n>  fixed following problems for me:\n> 1. Same prompt with same seed\
          \ produces longer more insightful/interesting  output with 8k         -c\
          \ 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   \n> \n> 2. I can use\
          \ instruction mode ( -ins)   to enter follow up requests in my prompt and\
          \ model produce replies without any issues if I use parameters   -c 8192\
          \ --rope-freq-base 10000 --rope-freq-scale 0.5   \n>    But if I use only\
          \  -c 4096 then only first reply is generated in -ins mode and after that\
          \ only infinite loop of empty lines is generated by model.  (-c 4096 is\
          \ therefore unusable for me when parameter -ins is specified ) \n> \n> 3.\
          \ Occasionally/randomly  even prompt with only around 500 tokes, after initially\
          \ correct output generates something like:\n> previuoiououiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui^C\n\
          > I have difficulty to reliable reproduce third issue. Most prompt works\
          \ fine with -c 4096 or even with -c 2048, until they don't \n> \n> When\
          \ using  -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5   I never\
          \ ran into garbled text problem\n\nI work in LLamaSharp in C#, so none of\
          \ this helps me... but thanks for the advice"
        updatedAt: '2023-07-26T15:52:14.771Z'
      numEdits: 0
      reactions: []
    id: 64c1412ea865fbe56ab916f9
    type: comment
  author: YAKOVNUKJHJ
  content: "> @TheBloke  \n> using parameters:   -c 8192 --rope-freq-base 10000 --rope-freq-scale\
    \ 0.5      fixed several problems for me\n> \n> e.g.   \n> main.exe  -t 16 -ngl\
    \ 44 -m C:\\AI\\WIP\\8K\\openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_M.bin\
    \ -s 7 --color -c 8192 --rope-freq-base 10000 --rope-freq-scale 0.5 --temp 0.7\
    \ --repeat_penalty 1.1  -p \"### Instruction: Explain in great detail the reason\
    \ for number 42 to be an important number\\n### Response:\" -ins\n> \n>  fixed\
    \ following problems for me:\n> 1. Same prompt with same seed produces longer\
    \ more insightful/interesting  output with 8k         -c 8192 --rope-freq-base\
    \ 10000 --rope-freq-scale 0.5   \n> \n> 2. I can use instruction mode ( -ins)\
    \   to enter follow up requests in my prompt and model produce replies without\
    \ any issues if I use parameters   -c 8192 --rope-freq-base 10000 --rope-freq-scale\
    \ 0.5   \n>    But if I use only  -c 4096 then only first reply is generated in\
    \ -ins mode and after that only infinite loop of empty lines is generated by model.\
    \  (-c 4096 is therefore unusable for me when parameter -ins is specified ) \n\
    > \n> 3. Occasionally/randomly  even prompt with only around 500 tokes, after\
    \ initially correct output generates something like:\n> previuoiououiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui^C\n\
    > I have difficulty to reliable reproduce third issue. Most prompt works fine\
    \ with -c 4096 or even with -c 2048, until they don't \n> \n> When using  -c 8192\
    \ --rope-freq-base 10000 --rope-freq-scale 0.5   I never ran into garbled text\
    \ problem\n\nI work in LLamaSharp in C#, so none of this helps me... but thanks\
    \ for the advice"
  created_at: 2023-07-26 14:52:14+00:00
  edited: false
  hidden: false
  id: 64c1412ea865fbe56ab916f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T16:27:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9613551497459412
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dzupin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dzupin\">@<span class=\"\
          underline\">dzupin</span></a></span>\n\n\t</span></span> OK great, thanks\
          \ for the details. I will update the README.</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;YAKOVNUKJHJ&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/YAKOVNUKJHJ\">@<span class=\"underline\"\
          >YAKOVNUKJHJ</span></a></span>\n\n\t</span></span> I had a quick look at\
          \ LlamaSharp and yes it seems they have not added RoPE yet. </p>\n"
        raw: '@dzupin OK great, thanks for the details. I will update the README.


          @YAKOVNUKJHJ I had a quick look at LlamaSharp and yes it seems they have
          not added RoPE yet. '
        updatedAt: '2023-07-26T16:27:16.742Z'
      numEdits: 0
      reactions: []
    id: 64c1496425be3567c97074f6
    type: comment
  author: TheBloke
  content: '@dzupin OK great, thanks for the details. I will update the README.


    @YAKOVNUKJHJ I had a quick look at LlamaSharp and yes it seems they have not added
    RoPE yet. '
  created_at: 2023-07-26 15:27:16+00:00
  edited: false
  hidden: false
  id: 64c1496425be3567c97074f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T16:28:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6035711169242859
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>By the way <span data-props=\"{&quot;user&quot;:&quot;dzupin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dzupin\"\
          >@<span class=\"underline\">dzupin</span></a></span>\n\n\t</span></span>\
          \ I see you're using the wrong prompt template. At least, not the prompt\
          \ template OpenAssistant say you should use.  But I guess Alpaca still works,\
          \ if that's working for you!</p>\n<p>Have you tried the correct prompt template,\
          \ <code>&lt;|system|&gt;{system_message}&lt;/s&gt;&lt;|prompter|&gt;{prompt}&lt;/s&gt;&lt;|assistant|&gt;</code>\
          \ ?</p>\n"
        raw: 'By the way @dzupin I see you''re using the wrong prompt template. At
          least, not the prompt template OpenAssistant say you should use.  But I
          guess Alpaca still works, if that''s working for you!


          Have you tried the correct prompt template, `<|system|>{system_message}</s><|prompter|>{prompt}</s><|assistant|>`
          ?'
        updatedAt: '2023-07-26T16:28:33.087Z'
      numEdits: 0
      reactions: []
    id: 64c149b13b1626314e3cd953
    type: comment
  author: TheBloke
  content: 'By the way @dzupin I see you''re using the wrong prompt template. At least,
    not the prompt template OpenAssistant say you should use.  But I guess Alpaca
    still works, if that''s working for you!


    Have you tried the correct prompt template, `<|system|>{system_message}</s><|prompter|>{prompt}</s><|assistant|>`
    ?'
  created_at: 2023-07-26 15:28:33+00:00
  edited: false
  hidden: false
  id: 64c149b13b1626314e3cd953
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
      fullname: YAKOV BRL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YAKOVNUKJHJ
      type: user
    createdAt: '2023-07-26T16:54:24.000Z'
    data:
      edited: false
      editors:
      - YAKOVNUKJHJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769600033760071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6f1579ade6ce37610f3a07aaab83e4b.svg
          fullname: YAKOV BRL
          isHf: false
          isPro: false
          name: YAKOVNUKJHJ
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;dzupin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dzupin\"\
          >@<span class=\"underline\">dzupin</span></a></span>\n\n\t</span></span>\
          \ OK great, thanks for the details. I will update the README.</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;YAKOVNUKJHJ&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/YAKOVNUKJHJ\">@<span\
          \ class=\"underline\">YAKOVNUKJHJ</span></a></span>\n\n\t</span></span>\
          \ I had a quick look at LlamaSharp and yes it seems they have not added\
          \ RoPE yet.</p>\n</blockquote>\n<p>Well thank you very much, I didn't know\
          \ about that at all.<br>Thank you for all your efforts, I have no idea how\
          \ you manage so much in a day</p>\n"
        raw: "> @dzupin OK great, thanks for the details. I will update the README.\n\
          > \n> @YAKOVNUKJHJ I had a quick look at LlamaSharp and yes it seems they\
          \ have not added RoPE yet.\n\nWell thank you very much, I didn't know about\
          \ that at all.\nThank you for all your efforts, I have no idea how you manage\
          \ so much in a day"
        updatedAt: '2023-07-26T16:54:24.337Z'
      numEdits: 0
      reactions: []
    id: 64c14fc04cbd12e168f3d234
    type: comment
  author: YAKOVNUKJHJ
  content: "> @dzupin OK great, thanks for the details. I will update the README.\n\
    > \n> @YAKOVNUKJHJ I had a quick look at LlamaSharp and yes it seems they have\
    \ not added RoPE yet.\n\nWell thank you very much, I didn't know about that at\
    \ all.\nThank you for all your efforts, I have no idea how you manage so much\
    \ in a day"
  created_at: 2023-07-26 15:54:24+00:00
  edited: false
  hidden: false
  id: 64c14fc04cbd12e168f3d234
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GGML
repo_type: model
status: closed
target_branch: null
title: The model is broken
