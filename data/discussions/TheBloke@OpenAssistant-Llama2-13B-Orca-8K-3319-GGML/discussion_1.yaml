!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mirek190
conflicting_files: null
created_at: 2023-07-25 11:50:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-07-25T12:50:55.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5074445009231567
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Hi bloke</p>

          <p>Command for llamacpp is correct for this model ?</p>

          <p>./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin
          --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "### Instruction:
          Write a story about llamas\n### Response:"</p>

          <p>It is model is 8k so should be -c 8192 or at least like for llama2 models
          -c 4096 ? </p>

          '
        raw: 'Hi bloke


          Command for llamacpp is correct for this model ?



          ./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin
          --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "### Instruction:
          Write a story about llamas\n### Response:"


          It is model is 8k so should be -c 8192 or at least like for llama2 models
          -c 4096 ? '
        updatedAt: '2023-07-25T12:55:03.003Z'
      numEdits: 1
      reactions: []
    id: 64bfc52f87d8cb4bb36eed30
    type: comment
  author: mirek190
  content: 'Hi bloke


    Command for llamacpp is correct for this model ?



    ./main -t 10 -ngl 32 -m openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin
    --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "### Instruction: Write
    a story about llamas\n### Response:"


    It is model is 8k so should be -c 8192 or at least like for llama2 models -c 4096
    ? '
  created_at: 2023-07-25 11:50:55+00:00
  edited: true
  hidden: false
  id: 64bfc52f87d8cb4bb36eed30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T12:52:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7905940413475037
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It should be -c 4096 as a base, and then if you want 8192 you need
          to use the RoPE parameters  which I think are:<br><code>-c &lt;contextsize&gt;
          --rope-freq-base 10000 --rope-freq-scale 0.5"</code></p>

          <p>I will update the README shortly</p>

          <p>Also that prompt template is wrong. My README creation code doesn''t
          handle putting the right prompt template in the llama.cpp example code yet,
          I need to fix that</p>

          '
        raw: 'It should be -c 4096 as a base, and then if you want 8192 you need to
          use the RoPE parameters  which I think are:

          `-c <contextsize> --rope-freq-base 10000 --rope-freq-scale 0.5"`


          I will update the README shortly


          Also that prompt template is wrong. My README creation code doesn''t handle
          putting the right prompt template in the llama.cpp example code yet, I need
          to fix that'
        updatedAt: '2023-07-25T12:52:59.689Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - dzupin
        - mufeed
        - edwios
        - streetyogi
    id: 64bfc5ab7b4c422525ed63df
    type: comment
  author: TheBloke
  content: 'It should be -c 4096 as a base, and then if you want 8192 you need to
    use the RoPE parameters  which I think are:

    `-c <contextsize> --rope-freq-base 10000 --rope-freq-scale 0.5"`


    I will update the README shortly


    Also that prompt template is wrong. My README creation code doesn''t handle putting
    the right prompt template in the llama.cpp example code yet, I need to fix that'
  created_at: 2023-07-25 11:52:59+00:00
  edited: false
  hidden: false
  id: 64bfc5ab7b4c422525ed63df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-07-25T12:54:11.000Z'
    data:
      from: Command for it is correct?
      to: llamacpp command for it is correct?
    id: 64bfc5f3e045ce0c4d93b256
    type: title-change
  author: mirek190
  created_at: 2023-07-25 11:54:11+00:00
  id: 64bfc5f3e045ce0c4d93b256
  new_title: llamacpp command for it is correct?
  old_title: Command for it is correct?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-07-25T12:54:48.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.833315372467041
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Ok<br>Thanks </p>

          '
        raw: "Ok \nThanks "
        updatedAt: '2023-07-25T12:54:48.023Z'
      numEdits: 0
      reactions: []
    id: 64bfc618969957691bf3a6b4
    type: comment
  author: mirek190
  content: "Ok \nThanks "
  created_at: 2023-07-25 11:54:48+00:00
  edited: false
  hidden: false
  id: 64bfc618969957691bf3a6b4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GGML
repo_type: model
status: open
target_branch: null
title: llamacpp command for it is correct?
