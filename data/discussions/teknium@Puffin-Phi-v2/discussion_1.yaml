!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rraulison
conflicting_files: null
created_at: 2023-09-12 21:57:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb7cc64f55dbca3022e7b8f50d88641c.svg
      fullname: remo raulison de oliveira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rraulison
      type: user
    createdAt: '2023-09-12T22:57:19.000Z'
    data:
      edited: false
      editors:
      - rraulison
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6738787889480591
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb7cc64f55dbca3022e7b8f50d88641c.svg
          fullname: remo raulison de oliveira
          isHf: false
          isPro: false
          name: rraulison
          type: user
        html: "<p>chat gpt gives me this code to run correctly on google colab</p>\n\
          <pre><code>import torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\n# Verifique se a GPU est\xE1 dispon\xEDvel e configure-a\
          \ como dispositivo padr\xE3o se estiver dispon\xEDvel\ndevice = \"cuda\"\
          \ if torch.cuda.is_available() else \"cpu\"\n\nsysprompt = \"The assistant\
          \ gives helpful, detailed, and polite answers to the user's questions.\\\
          n\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\"teknium/Puffin-Phi-v2\"\
          , trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True,\
          \ torch_dtype=torch.bfloat16)\n\ninput_text = f\"{sysprompt}USER: Write\
          \ a negative review for the website Twitter.\\nASSISTANT:\"\ninput_ids =\
          \ tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\noutputs\
          \ = model.generate(input_ids, max_length=128, do_sample=True, temperature=0.2,\
          \ top_p=0.9, use_cache=True, repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\n\
          text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text)\n\
          </code></pre>\n<p>that gives the answer: </p>\n<pre><code>The attention\
          \ mask and the pad token id were not set. As a consequence, you may observe\
          \ unexpected behavior. Please pass your input's `attention_mask` to obtain\
          \ reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for\
          \ open-end generation.\n\nThe assistant gives helpful, detailed, and polite\
          \ answers to the user's questions.\nUSER: Write a negative review for the\
          \ website Twitter.\nASSISTANT: Here is a negative review for your website\
          \ Twitter:\n\n\"\U0001F525 Terrible User Experience \U0001F525\n\nFirst\
          \ off, let's talk about the design. The layout of your site feels cluttered\
          \ and hard to navigate, making it nearly impossible to find what you're\
          \ looking for. Additionally, the color scheme is garish and unappealing,\
          \ which only serves to further discourage users from exploring more.\n\n\
          Next, I have serious concerns about the performance\n</code></pre>\n"
        raw: "chat gpt gives me this code to run correctly on google colab\r\n\r\n\
          ```\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \n\r\n# Verifique se a GPU est\xE1 dispon\xEDvel e configure-a como dispositivo\
          \ padr\xE3o se estiver dispon\xEDvel\r\ndevice = \"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\"\r\n\r\nsysprompt = \"The assistant gives helpful, detailed,\
          \ and polite answers to the user's questions.\\n\"\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True,\
          \ torch_dtype=torch.bfloat16)\r\n\r\ninput_text = f\"{sysprompt}USER: Write\
          \ a negative review for the website Twitter.\\nASSISTANT:\"\r\ninput_ids\
          \ = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\r\n\r\
          \noutputs = model.generate(input_ids, max_length=128, do_sample=True, temperature=0.2,\
          \ top_p=0.9, use_cache=True, repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\r\
          \ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\r\nprint(text)\r\
          \n```\r\nthat gives the answer: \r\n```\r\nThe attention mask and the pad\
          \ token id were not set. As a consequence, you may observe unexpected behavior.\
          \ Please pass your input's `attention_mask` to obtain reliable results.\r\
          \nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\
          \n\r\nThe assistant gives helpful, detailed, and polite answers to the user's\
          \ questions.\r\nUSER: Write a negative review for the website Twitter.\r\
          \nASSISTANT: Here is a negative review for your website Twitter:\r\n\r\n\
          \"\U0001F525 Terrible User Experience \U0001F525\r\n\r\nFirst off, let's\
          \ talk about the design. The layout of your site feels cluttered and hard\
          \ to navigate, making it nearly impossible to find what you're looking for.\
          \ Additionally, the color scheme is garish and unappealing, which only serves\
          \ to further discourage users from exploring more.\r\n\r\nNext, I have serious\
          \ concerns about the performance\r\n```"
        updatedAt: '2023-09-12T22:57:19.245Z'
      numEdits: 0
      reactions: []
    id: 6500eccff322f9156682dcfc
    type: comment
  author: rraulison
  content: "chat gpt gives me this code to run correctly on google colab\r\n\r\n```\r\
    \nimport torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \n\r\n# Verifique se a GPU est\xE1 dispon\xEDvel e configure-a como dispositivo\
    \ padr\xE3o se estiver dispon\xEDvel\r\ndevice = \"cuda\" if torch.cuda.is_available()\
    \ else \"cpu\"\r\n\r\nsysprompt = \"The assistant gives helpful, detailed, and\
    \ polite answers to the user's questions.\\n\"\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True,\
    \ torch_dtype=torch.bfloat16)\r\n\r\ninput_text = f\"{sysprompt}USER: Write a\
    \ negative review for the website Twitter.\\nASSISTANT:\"\r\ninput_ids = tokenizer.encode(input_text,\
    \ return_tensors=\"pt\").to(device)\r\n\r\noutputs = model.generate(input_ids,\
    \ max_length=128, do_sample=True, temperature=0.2, top_p=0.9, use_cache=True,\
    \ repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\r\ntext = tokenizer.decode(outputs[0],\
    \ skip_special_tokens=True)\r\nprint(text)\r\n```\r\nthat gives the answer: \r\
    \n```\r\nThe attention mask and the pad token id were not set. As a consequence,\
    \ you may observe unexpected behavior. Please pass your input's `attention_mask`\
    \ to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:50256\
    \ for open-end generation.\r\n\r\nThe assistant gives helpful, detailed, and polite\
    \ answers to the user's questions.\r\nUSER: Write a negative review for the website\
    \ Twitter.\r\nASSISTANT: Here is a negative review for your website Twitter:\r\
    \n\r\n\"\U0001F525 Terrible User Experience \U0001F525\r\n\r\nFirst off, let's\
    \ talk about the design. The layout of your site feels cluttered and hard to navigate,\
    \ making it nearly impossible to find what you're looking for. Additionally, the\
    \ color scheme is garish and unappealing, which only serves to further discourage\
    \ users from exploring more.\r\n\r\nNext, I have serious concerns about the performance\r\
    \n```"
  created_at: 2023-09-12 21:57:19+00:00
  edited: false
  hidden: false
  id: 6500eccff322f9156682dcfc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-09-12T23:45:32.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5723655819892883
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Change the max token length variable to a higher number</p>

          '
        raw: Change the max token length variable to a higher number
        updatedAt: '2023-09-12T23:45:32.931Z'
      numEdits: 0
      reactions: []
    id: 6500f81c868ac1994a3379c4
    type: comment
  author: teknium
  content: Change the max token length variable to a higher number
  created_at: 2023-09-12 22:45:32+00:00
  edited: false
  hidden: false
  id: 6500f81c868ac1994a3379c4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: teknium/Puffin-Phi-v2
repo_type: model
status: open
target_branch: null
title: 'code to run on colab '
