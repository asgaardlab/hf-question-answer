!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Diavator
conflicting_files: null
created_at: 2023-12-09 14:58:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65731f4888805b3ba199f1fc/42aw4W5FX5-HVmpMcvR5V.jpeg?w=200&h=200&f=face
      fullname: Allerias
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Diavator
      type: user
    createdAt: '2023-12-09T14:58:41.000Z'
    data:
      edited: false
      editors:
      - Diavator
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9606407880783081
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65731f4888805b3ba199f1fc/42aw4W5FX5-HVmpMcvR5V.jpeg?w=200&h=200&f=face
          fullname: Allerias
          isHf: false
          isPro: false
          name: Diavator
          type: user
        html: '<p>Good afternoon. Have you considered creating a 20B model with a
          16k or 32k context? The problem is that there are no such models. All models
          with extended context are mostly 7b,16B, 33B and higher.</p>

          '
        raw: Good afternoon. Have you considered creating a 20B model with a 16k or
          32k context? The problem is that there are no such models. All models with
          extended context are mostly 7b,16B, 33B and higher.
        updatedAt: '2023-12-09T14:58:41.671Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 657480a1fca370282a4b6c58
    type: comment
  author: Diavator
  content: Good afternoon. Have you considered creating a 20B model with a 16k or
    32k context? The problem is that there are no such models. All models with extended
    context are mostly 7b,16B, 33B and higher.
  created_at: 2023-12-09 14:58:41+00:00
  edited: false
  hidden: false
  id: 657480a1fca370282a4b6c58
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
      fullname: Raven
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: athirdpath
      type: user
    createdAt: '2023-12-09T18:09:14.000Z'
    data:
      edited: true
      editors:
      - athirdpath
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.946377694606781
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
          fullname: Raven
          isHf: false
          isPro: false
          name: athirdpath
          type: user
        html: '<p>Unfortunately for all of us, there is a non-linear relationship
          between trained context length and needed VRAM and time.</p>

          <p>For each epoch (full training pass), this 20b model @ 4096 context needs
          98% of the VRAM (78GB) and 100% of the GPU in a 80gb A100 for a batch size
          of 1, and it takes 5 hours. (~$10 cost).</p>

          <p>Every ~512 tokens in new context roughly doubles the VRAM and also the
          processing requirements (for training). There are ~24 (forced to round up
          by VRAM) 512 token increases between 4k and 16k context, and ANOTHER 32
          from 16k to 32k. So @ 16k context, it would take hundreds of A100s for five
          hours (many thousands of dollars in cost), and I don''t think there are
          enough GPUs for rent on the planet to train a 20b 32k model.</p>

          <p>OpenAI and Google and such can do it because they have massive server
          farms and trade secret software. Mistral can do it due to SWA, something
          L2 does not support. At this size, I think 4k is the best we''ll see until
          Llama 3.</p>

          '
        raw: 'Unfortunately for all of us, there is a non-linear relationship between
          trained context length and needed VRAM and time.


          For each epoch (full training pass), this 20b model @ 4096 context needs
          98% of the VRAM (78GB) and 100% of the GPU in a 80gb A100 for a batch size
          of 1, and it takes 5 hours. (~$10 cost).


          Every ~512 tokens in new context roughly doubles the VRAM and also the processing
          requirements (for training). There are ~24 (forced to round up by VRAM)
          512 token increases between 4k and 16k context, and ANOTHER 32 from 16k
          to 32k. So @ 16k context, it would take hundreds of A100s for five hours
          (many thousands of dollars in cost), and I don''t think there are enough
          GPUs for rent on the planet to train a 20b 32k model.


          OpenAI and Google and such can do it because they have massive server farms
          and trade secret software. Mistral can do it due to SWA, something L2 does
          not support. At this size, I think 4k is the best we''ll see until Llama
          3.'
        updatedAt: '2023-12-09T18:09:59.458Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Diavator
    id: 6574ad4a1488186315a43a35
    type: comment
  author: athirdpath
  content: 'Unfortunately for all of us, there is a non-linear relationship between
    trained context length and needed VRAM and time.


    For each epoch (full training pass), this 20b model @ 4096 context needs 98% of
    the VRAM (78GB) and 100% of the GPU in a 80gb A100 for a batch size of 1, and
    it takes 5 hours. (~$10 cost).


    Every ~512 tokens in new context roughly doubles the VRAM and also the processing
    requirements (for training). There are ~24 (forced to round up by VRAM) 512 token
    increases between 4k and 16k context, and ANOTHER 32 from 16k to 32k. So @ 16k
    context, it would take hundreds of A100s for five hours (many thousands of dollars
    in cost), and I don''t think there are enough GPUs for rent on the planet to train
    a 20b 32k model.


    OpenAI and Google and such can do it because they have massive server farms and
    trade secret software. Mistral can do it due to SWA, something L2 does not support.
    At this size, I think 4k is the best we''ll see until Llama 3.'
  created_at: 2023-12-09 18:09:14+00:00
  edited: true
  hidden: false
  id: 6574ad4a1488186315a43a35
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: athirdpath/Iambe-RP-cDPO-20b-ALT-GGUF
repo_type: model
status: open
target_branch: null
title: increased context
