!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hashb
conflicting_files: null
created_at: 2022-07-26 19:30:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67f67da6d93674e10e4f2efce46ea8db.svg
      fullname: Harshad Bhandwaldar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hashb
      type: user
    createdAt: '2022-07-26T20:30:38.000Z'
    data:
      edited: false
      editors:
      - hashb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67f67da6d93674e10e4f2efce46ea8db.svg
          fullname: Harshad Bhandwaldar
          isHf: false
          isPro: false
          name: hashb
          type: user
        html: '<p>I downloaded the model files using git lfs and pointed AutoConfig
          to the model file and tokenizer to JSON.<br>However, when I run the code
          I get the below error -</p>

          <p>  File "/home/harshad/.cache/pypoetry/virtualenvs/tgeb2c-1dT1tncl-py3.10/lib/python3.10/site-packages/transformers/configuration_utils.py",
          line 652, in _get_config_dict<br>    raise EnvironmentError(<br>OSError:
          It looks like the config file at ''roberta/pytorch_model.bin'' is not a
          valid JSON file.</p>

          <p>Directory:<br>-project-app-folder<br>           -main.py<br>            -roberta<br>                      -<em>.bin<br>                      -</em>.json</p>

          <p> Code:</p>

          <p>from transformers import AutoModelForQuestionAnswering, AutoTokenizer,
          pipeline</p>

          <h1 id="load-model--tokenizer">Load model &amp; tokenizer</h1>

          <p>roberta_model = AutoModelForQuestionAnswering.from_pretrained(''roberta/pytorch_model.bin'')<br>roberta_tokenizer
          = AutoTokenizer.from_pretrained(''roberta/tokenizer_config.json'')</p>

          <p>nlp = pipeline("question-answering", model=roberta_model, tokenizer=roberta_tokenizer)</p>

          <pre><code>QA_input = {"question": question, "context": context}

          res = nlp(QA_input)

          </code></pre>

          '
        raw: "I downloaded the model files using git lfs and pointed AutoConfig to\
          \ the model file and tokenizer to JSON.\r\nHowever, when I run the code\
          \ I get the below error -\r\n\r\n  File \"/home/harshad/.cache/pypoetry/virtualenvs/tgeb2c-1dT1tncl-py3.10/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
          , line 652, in _get_config_dict\r\n    raise EnvironmentError(\r\nOSError:\
          \ It looks like the config file at 'roberta/pytorch_model.bin' is not a\
          \ valid JSON file.\r\n\r\n\r\nDirectory:\r\n-project-app-folder\r\n    \
          \       -main.py\r\n            -roberta\r\n                      -*.bin\r\
          \n                      -*.json\r\n\r\n Code:\r\n\r\nfrom transformers import\
          \ AutoModelForQuestionAnswering, AutoTokenizer, pipeline\r\n\r\n# Load model\
          \ & tokenizer\r\nroberta_model = AutoModelForQuestionAnswering.from_pretrained('roberta/pytorch_model.bin')\r\
          \nroberta_tokenizer = AutoTokenizer.from_pretrained('roberta/tokenizer_config.json')\r\
          \n\r\n\r\nnlp = pipeline(\"question-answering\", model=roberta_model, tokenizer=roberta_tokenizer)\r\
          \n\r\n    QA_input = {\"question\": question, \"context\": context}\r\n\
          \    res = nlp(QA_input)\r\n"
        updatedAt: '2022-07-26T20:30:38.833Z'
      numEdits: 0
      reactions: []
    id: 62e04eee1b0ece20b8aa3dee
    type: comment
  author: hashb
  content: "I downloaded the model files using git lfs and pointed AutoConfig to the\
    \ model file and tokenizer to JSON.\r\nHowever, when I run the code I get the\
    \ below error -\r\n\r\n  File \"/home/harshad/.cache/pypoetry/virtualenvs/tgeb2c-1dT1tncl-py3.10/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
    , line 652, in _get_config_dict\r\n    raise EnvironmentError(\r\nOSError: It\
    \ looks like the config file at 'roberta/pytorch_model.bin' is not a valid JSON\
    \ file.\r\n\r\n\r\nDirectory:\r\n-project-app-folder\r\n           -main.py\r\n\
    \            -roberta\r\n                      -*.bin\r\n                    \
    \  -*.json\r\n\r\n Code:\r\n\r\nfrom transformers import AutoModelForQuestionAnswering,\
    \ AutoTokenizer, pipeline\r\n\r\n# Load model & tokenizer\r\nroberta_model = AutoModelForQuestionAnswering.from_pretrained('roberta/pytorch_model.bin')\r\
    \nroberta_tokenizer = AutoTokenizer.from_pretrained('roberta/tokenizer_config.json')\r\
    \n\r\n\r\nnlp = pipeline(\"question-answering\", model=roberta_model, tokenizer=roberta_tokenizer)\r\
    \n\r\n    QA_input = {\"question\": question, \"context\": context}\r\n    res\
    \ = nlp(QA_input)\r\n"
  created_at: 2022-07-26 19:30:38+00:00
  edited: false
  hidden: false
  id: 62e04eee1b0ece20b8aa3dee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c28b8b669f6c2bc4126354/vMFQ2BK8mCEFuNSC4kr5P.png?w=200&h=200&f=face
      fullname: Sebastian Husch Lee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sjrhuschlee
      type: user
    createdAt: '2022-07-27T07:35:35.000Z'
    data:
      edited: false
      editors:
      - sjrhuschlee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c28b8b669f6c2bc4126354/vMFQ2BK8mCEFuNSC4kr5P.png?w=200&h=200&f=face
          fullname: Sebastian Husch Lee
          isHf: false
          isPro: false
          name: sjrhuschlee
          type: user
        html: '<p>Hi @hb0313 thanks for your interest in downloading our model! I
          think you need to make a slight modification to your loading code. I believe
          <code>AutoModelForQuestionAnswering.from_pretrained()</code> and <code>AutoTokenizer.from_pretrained()</code>
          expect the directory containing the model and not the <code>.bin</code>
          and <code>.json</code> files. </p>

          <p>So your code might look something like this:</p>

          <pre><code class="language-python">roberta_model = AutoModelForQuestionAnswering.from_pretrained(<span
          class="hljs-string">''directory_containing_model/''</span>)

          roberta_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">''directory_containing_model/''</span>)

          </code></pre>

          <p>You can find more information on how <code>AutoModelForQuestionAnswering.from_pretrained()</code>
          works in HF''s docs: <a href="https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForQuestionAnswering.from_pretrained">https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForQuestionAnswering.from_pretrained</a></p>

          '
        raw: "Hi @hb0313 thanks for your interest in downloading our model! I think\
          \ you need to make a slight modification to your loading code. I believe\
          \ `AutoModelForQuestionAnswering.from_pretrained()` and `AutoTokenizer.from_pretrained()`\
          \ expect the directory containing the model and not the `.bin` and `.json`\
          \ files. \n\nSo your code might look something like this:\n```python\nroberta_model\
          \ = AutoModelForQuestionAnswering.from_pretrained('directory_containing_model/')\n\
          roberta_tokenizer = AutoTokenizer.from_pretrained('directory_containing_model/')\n\
          ```\n\nYou can find more information on how `AutoModelForQuestionAnswering.from_pretrained()`\
          \ works in HF's docs: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForQuestionAnswering.from_pretrained"
        updatedAt: '2022-07-27T07:35:35.026Z'
      numEdits: 0
      reactions: []
    id: 62e0eac702a6c13e46738f7f
    type: comment
  author: sjrhuschlee
  content: "Hi @hb0313 thanks for your interest in downloading our model! I think\
    \ you need to make a slight modification to your loading code. I believe `AutoModelForQuestionAnswering.from_pretrained()`\
    \ and `AutoTokenizer.from_pretrained()` expect the directory containing the model\
    \ and not the `.bin` and `.json` files. \n\nSo your code might look something\
    \ like this:\n```python\nroberta_model = AutoModelForQuestionAnswering.from_pretrained('directory_containing_model/')\n\
    roberta_tokenizer = AutoTokenizer.from_pretrained('directory_containing_model/')\n\
    ```\n\nYou can find more information on how `AutoModelForQuestionAnswering.from_pretrained()`\
    \ works in HF's docs: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModelForQuestionAnswering.from_pretrained"
  created_at: 2022-07-27 06:35:35+00:00
  edited: false
  hidden: false
  id: 62e0eac702a6c13e46738f7f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: deepset/tinyroberta-squad2
repo_type: model
status: open
target_branch: null
title: Unable to load model offline
