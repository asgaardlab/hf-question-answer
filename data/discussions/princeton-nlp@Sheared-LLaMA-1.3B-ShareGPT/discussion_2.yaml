!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fwtan
conflicting_files: null
created_at: 2023-11-30 10:47:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04267f4e834c57b679663b735b59e624.svg
      fullname: Fuwen Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fwtan
      type: user
    createdAt: '2023-11-30T10:47:56.000Z'
    data:
      edited: true
      editors:
      - fwtan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9263619184494019
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04267f4e834c57b679663b735b59e624.svg
          fullname: Fuwen Tan
          isHf: false
          isPro: false
          name: fwtan
          type: user
        html: '<p>In config.json, it seems n_kv_heads was set to be the default value
          (32), which does not align with the actual setting (i.e. 24). The same case
          for other Sheared Llama models.<br>This may lead to an error when using
          transformers with a version &gt; 4.28.</p>

          '
        raw: 'In config.json, it seems n_kv_heads was set to be the default value
          (32), which does not align with the actual setting (i.e. 24). The same case
          for other Sheared Llama models.

          This may lead to an error when using transformers with a version > 4.28.'
        updatedAt: '2023-11-30T10:55:44.776Z'
      numEdits: 1
      reactions: []
    id: 6568685cea4744e8ef9158af
    type: comment
  author: fwtan
  content: 'In config.json, it seems n_kv_heads was set to be the default value (32),
    which does not align with the actual setting (i.e. 24). The same case for other
    Sheared Llama models.

    This may lead to an error when using transformers with a version > 4.28.'
  created_at: 2023-11-30 10:47:56+00:00
  edited: true
  hidden: false
  id: 6568685cea4744e8ef9158af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63dcff68a8877129a1574f33/O-8C_Wy8nr_zo8TudBF1k.jpeg?w=200&h=200&f=face
      fullname: Sinan Akkoyun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SinanAkkoyun
      type: user
    createdAt: '2023-12-04T04:00:12.000Z'
    data:
      edited: false
      editors:
      - SinanAkkoyun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772594571113586
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63dcff68a8877129a1574f33/O-8C_Wy8nr_zo8TudBF1k.jpeg?w=200&h=200&f=face
          fullname: Sinan Akkoyun
          isHf: false
          isPro: false
          name: SinanAkkoyun
          type: user
        html: '<p>I just saw, shouldn''t it be 16 like in the base shear?</p>

          '
        raw: I just saw, shouldn't it be 16 like in the base shear?
        updatedAt: '2023-12-04T04:00:12.724Z'
      numEdits: 0
      reactions: []
    id: 656d4eccf39b7b6a5de9a096
    type: comment
  author: SinanAkkoyun
  content: I just saw, shouldn't it be 16 like in the base shear?
  created_at: 2023-12-04 04:00:12+00:00
  edited: false
  hidden: false
  id: 656d4eccf39b7b6a5de9a096
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04267f4e834c57b679663b735b59e624.svg
      fullname: Fuwen Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fwtan
      type: user
    createdAt: '2023-12-07T13:53:42.000Z'
    data:
      edited: false
      editors:
      - fwtan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9353426098823547
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04267f4e834c57b679663b735b59e624.svg
          fullname: Fuwen Tan
          isHf: false
          isPro: false
          name: fwtan
          type: user
        html: '<blockquote>

          <p>I just saw, shouldn''t it be 16 like in the base shear?</p>

          </blockquote>

          <p>Yes, you''re correct! It should be 16 for this model. Transformers &lt;=
          4.28 uses n_kv_heads == n_heads by default.</p>

          '
        raw: '> I just saw, shouldn''t it be 16 like in the base shear?


          Yes, you''re correct! It should be 16 for this model. Transformers <= 4.28
          uses n_kv_heads == n_heads by default.'
        updatedAt: '2023-12-07T13:53:42.302Z'
      numEdits: 0
      reactions: []
    id: 6571ce6693ea01156da40fad
    type: comment
  author: fwtan
  content: '> I just saw, shouldn''t it be 16 like in the base shear?


    Yes, you''re correct! It should be 16 for this model. Transformers <= 4.28 uses
    n_kv_heads == n_heads by default.'
  created_at: 2023-12-07 13:53:42+00:00
  edited: false
  hidden: false
  id: 6571ce6693ea01156da40fad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT
repo_type: model
status: open
target_branch: null
title: updating n_kv_heads in config.json
