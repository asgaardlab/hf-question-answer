!!python/object:huggingface_hub.community.DiscussionWithDetails
author: phi0112358
conflicting_files: null
created_at: 2023-08-19 22:50:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661340992329-noauth.png?w=200&h=200&f=face
      fullname: Yazan Agha-Schrader
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phi0112358
      type: user
    createdAt: '2023-08-19T23:50:11.000Z'
    data:
      edited: true
      editors:
      - phi0112358
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9359108209609985
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661340992329-noauth.png?w=200&h=200&f=face
          fullname: Yazan Agha-Schrader
          isHf: false
          isPro: false
          name: phi0112358
          type: user
        html: '<p>Hi guys, first of all thank you a lot for your amazing work and
          the contributions to the opensource community.</p>

          <p>I would like to know how you guys are creating these formats (sharded,
          q4f16 or q3f16). I am asking, because I wanted to try the llama2 uncensored
          on my iphone and on ipad, but whenever i try to start a conversation, the
          app crashes. I noticed, that these crashes only occur, if I try to use q4
          models. The q3 models do work fine, but unfortunately none of the q4 models.</p>

          <p>So it would be pretty cool if you could provide a llama-2 uncensored
          q3 model too, or if you could tell how i can create such a model myself.</p>

          <p>Greetings<br>Y</p>

          '
        raw: 'Hi guys, first of all thank you a lot for your amazing work and the
          contributions to the opensource community.


          I would like to know how you guys are creating these formats (sharded, q4f16
          or q3f16). I am asking, because I wanted to try the llama2 uncensored on
          my iphone and on ipad, but whenever i try to start a conversation, the app
          crashes. I noticed, that these crashes only occur, if I try to use q4 models.
          The q3 models do work fine, but unfortunately none of the q4 models.


          So it would be pretty cool if you could provide a llama-2 uncensored q3
          model too, or if you could tell how i can create such a model myself.


          Greetings

          Y'
        updatedAt: '2023-08-31T04:18:57.259Z'
      numEdits: 1
      reactions: []
    id: 64e1553302fa032de410af39
    type: comment
  author: phi0112358
  content: 'Hi guys, first of all thank you a lot for your amazing work and the contributions
    to the opensource community.


    I would like to know how you guys are creating these formats (sharded, q4f16 or
    q3f16). I am asking, because I wanted to try the llama2 uncensored on my iphone
    and on ipad, but whenever i try to start a conversation, the app crashes. I noticed,
    that these crashes only occur, if I try to use q4 models. The q3 models do work
    fine, but unfortunately none of the q4 models.


    So it would be pretty cool if you could provide a llama-2 uncensored q3 model
    too, or if you could tell how i can create such a model myself.


    Greetings

    Y'
  created_at: 2023-08-19 22:50:11+00:00
  edited: true
  hidden: false
  id: 64e1553302fa032de410af39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
      fullname: Om
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asach
      type: user
    createdAt: '2023-08-20T06:24:07.000Z'
    data:
      edited: false
      editors:
      - asach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9512712955474854
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
          fullname: Om
          isHf: false
          isPro: false
          name: asach
          type: user
        html: '<p>Yeah I am also searching for this ! </p>

          '
        raw: 'Yeah I am also searching for this ! '
        updatedAt: '2023-08-20T06:24:07.742Z'
      numEdits: 0
      reactions: []
    id: 64e1b1878e2084e1d7ebe6dd
    type: comment
  author: asach
  content: 'Yeah I am also searching for this ! '
  created_at: 2023-08-20 05:24:07+00:00
  edited: false
  hidden: false
  id: 64e1b1878e2084e1d7ebe6dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
      fullname: Charlie Ruan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CharlieFRuan
      type: user
    createdAt: '2023-08-29T22:41:35.000Z'
    data:
      edited: false
      editors:
      - CharlieFRuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8517673015594482
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
          fullname: Charlie Ruan
          isHf: false
          isPro: false
          name: CharlieFRuan
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;phi0112358&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/phi0112358\"\
          >@<span class=\"underline\">phi0112358</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;asach&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/asach\">@<span class=\"\
          underline\">asach</span></a></span>\n\n\t</span></span>, thanks for the\
          \ question! You could refer to <a rel=\"nofollow\" href=\"https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb\"\
          >this tutorial</a>, which is runnable on google Colab, on compiling any\
          \ huggingface model (as long as the model architecture is supported).</p>\n\
          <p>This is the general documentation of the mlc-llm project: <a rel=\"nofollow\"\
          \ href=\"https://mlc.ai/mlc-llm/docs/\">https://mlc.ai/mlc-llm/docs/</a><br>And\
          \ in general posting questions on the github repo would get more attention:\
          \ <a rel=\"nofollow\" href=\"https://github.com/mlc-ai/mlc-llm\">https://github.com/mlc-ai/mlc-llm</a></p>\n"
        raw: 'Hi @phi0112358 @asach, thanks for the question! You could refer to [this
          tutorial](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb),
          which is runnable on google Colab, on compiling any huggingface model (as
          long as the model architecture is supported).


          This is the general documentation of the mlc-llm project: https://mlc.ai/mlc-llm/docs/

          And in general posting questions on the github repo would get more attention:
          https://github.com/mlc-ai/mlc-llm'
        updatedAt: '2023-08-29T22:41:35.278Z'
      numEdits: 0
      reactions: []
    id: 64ee741ffe7ebfcec4b58a3f
    type: comment
  author: CharlieFRuan
  content: 'Hi @phi0112358 @asach, thanks for the question! You could refer to [this
    tutorial](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb),
    which is runnable on google Colab, on compiling any huggingface model (as long
    as the model architecture is supported).


    This is the general documentation of the mlc-llm project: https://mlc.ai/mlc-llm/docs/

    And in general posting questions on the github repo would get more attention:
    https://github.com/mlc-ai/mlc-llm'
  created_at: 2023-08-29 21:41:35+00:00
  edited: false
  hidden: false
  id: 64ee741ffe7ebfcec4b58a3f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: mlc-ai/mlc-chat-georgesung-llama2-7b-chat-uncensored-q4f16_1
repo_type: model
status: open
target_branch: null
title: How to convert into your format?
