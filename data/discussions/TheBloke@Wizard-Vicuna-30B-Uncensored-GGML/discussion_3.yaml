!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-06-08 06:59:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-08T07:59:55.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8453983068466187
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Well actually I can''t get GPU acceleration to work with any model
          but I''ve only tried this model and only the 5_1 version of it.<br>I''m
          using Windows 10 </p>

          <p>I followed the steps here. At least I think I did.<br><a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md</a></p>

          <p>I tried<br>pip uninstall -y llama-cpp-python<br>set CMAKE_ARGS="-DLLAMA_CUBLAS=on"<br>set
          FORCE_CMAKE=1<br>pip install llama-cpp-python --no-cache-dir</p>

          <p>I don''t think I noticed any errors while performing these steps. Tried
          it a few times. I updated Ooba as well. </p>

          <p>Afterwards I used these settings. But I tried all sorts of different
          settings. Tried only Pre_Layer or only N-GPU-Layers. Saving and reloading
          etc.<br>But my VRAM does not get used at all. I even tried turning on gptq-for-llama
          but I get errors. I don''t know what that even if though.<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/6303d5aba362e7e8b51c13a3/2CE8-Z-HPg1WybSu0KTdM.png"><img
          alt="Capture.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6303d5aba362e7e8b51c13a3/2CE8-Z-HPg1WybSu0KTdM.png"></a></p>

          <p>If anyone has any ideas or can confirm if this model supports or does
          not support GPU Acceleration let me know.<br>Thank you.</p>

          '
        raw: "Well actually I can't get GPU acceleration to work with any model but\
          \ I've only tried this model and only the 5_1 version of it. \nI'm using\
          \ Windows 10 \n\nI followed the steps here. At least I think I did. \nhttps://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md\n\
          \nI tried\npip uninstall -y llama-cpp-python\nset CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\
          \nset FORCE_CMAKE=1\npip install llama-cpp-python --no-cache-dir\n\nI don't\
          \ think I noticed any errors while performing these steps. Tried it a few\
          \ times. I updated Ooba as well. \n\n\nAfterwards I used these settings.\
          \ But I tried all sorts of different settings. Tried only Pre_Layer or only\
          \ N-GPU-Layers. Saving and reloading etc. \nBut my VRAM does not get used\
          \ at all. I even tried turning on gptq-for-llama but I get errors. I don't\
          \ know what that even if though. \n![Capture.PNG](https://cdn-uploads.huggingface.co/production/uploads/6303d5aba362e7e8b51c13a3/2CE8-Z-HPg1WybSu0KTdM.png)\n\
          \nIf anyone has any ideas or can confirm if this model supports or does\
          \ not support GPU Acceleration let me know. \nThank you."
        updatedAt: '2023-06-08T08:01:55.424Z'
      numEdits: 2
      reactions: []
    id: 64818a7b8af4675862f04649
    type: comment
  author: Goldenblood56
  content: "Well actually I can't get GPU acceleration to work with any model but\
    \ I've only tried this model and only the 5_1 version of it. \nI'm using Windows\
    \ 10 \n\nI followed the steps here. At least I think I did. \nhttps://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md\n\
    \nI tried\npip uninstall -y llama-cpp-python\nset CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\
    \nset FORCE_CMAKE=1\npip install llama-cpp-python --no-cache-dir\n\nI don't think\
    \ I noticed any errors while performing these steps. Tried it a few times. I updated\
    \ Ooba as well. \n\n\nAfterwards I used these settings. But I tried all sorts\
    \ of different settings. Tried only Pre_Layer or only N-GPU-Layers. Saving and\
    \ reloading etc. \nBut my VRAM does not get used at all. I even tried turning\
    \ on gptq-for-llama but I get errors. I don't know what that even if though. \n\
    ![Capture.PNG](https://cdn-uploads.huggingface.co/production/uploads/6303d5aba362e7e8b51c13a3/2CE8-Z-HPg1WybSu0KTdM.png)\n\
    \nIf anyone has any ideas or can confirm if this model supports or does not support\
    \ GPU Acceleration let me know. \nThank you."
  created_at: 2023-06-08 06:59:55+00:00
  edited: true
  hidden: false
  id: 64818a7b8af4675862f04649
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T08:17:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8962951898574829
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve tested text-generation-webui and it definitely does work with
          GGML models with CUDA acceleration. And this model does support that - all
          GGML models do; there aren''t "models with GPU" and "models without".</p>

          <p>As you''re on Windows, it may be harder to get it working. Do you have
          the CUDA toolkit installed? That''s a requirement for compiling llama-cpp-python
          with CUDA support.</p>

          <p>On Linux I can install llama-cpp-python like so:</p>

          <pre><code>pip uninstall -y llama-cpp-python

          CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python
          --no-cache-dir

          </code></pre>

          <p>And it then works with GPU offload, including in text-generation-webui</p>

          <p>I''m afraid I''m unable to test anything on Windows, so I''m not sure
          what to suggest.  I can help with getting it working in WSL2, if you feel
          like installing that.</p>

          '
        raw: 'I''ve tested text-generation-webui and it definitely does work with
          GGML models with CUDA acceleration. And this model does support that - all
          GGML models do; there aren''t "models with GPU" and "models without".


          As you''re on Windows, it may be harder to get it working. Do you have the
          CUDA toolkit installed? That''s a requirement for compiling llama-cpp-python
          with CUDA support.


          On Linux I can install llama-cpp-python like so:

          ```

          pip uninstall -y llama-cpp-python

          CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python
          --no-cache-dir

          ```

          And it then works with GPU offload, including in text-generation-webui


          I''m afraid I''m unable to test anything on Windows, so I''m not sure what
          to suggest.  I can help with getting it working in WSL2, if you feel like
          installing that.'
        updatedAt: '2023-06-08T08:17:08.581Z'
      numEdits: 0
      reactions: []
    id: 64818e8417f2fba00087bee4
    type: comment
  author: TheBloke
  content: 'I''ve tested text-generation-webui and it definitely does work with GGML
    models with CUDA acceleration. And this model does support that - all GGML models
    do; there aren''t "models with GPU" and "models without".


    As you''re on Windows, it may be harder to get it working. Do you have the CUDA
    toolkit installed? That''s a requirement for compiling llama-cpp-python with CUDA
    support.


    On Linux I can install llama-cpp-python like so:

    ```

    pip uninstall -y llama-cpp-python

    CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir

    ```

    And it then works with GPU offload, including in text-generation-webui


    I''m afraid I''m unable to test anything on Windows, so I''m not sure what to
    suggest.  I can help with getting it working in WSL2, if you feel like installing
    that.'
  created_at: 2023-06-08 07:17:08+00:00
  edited: false
  hidden: false
  id: 64818e8417f2fba00087bee4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-08T08:50:50.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9502216577529907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks TheBloke. Your a hard worker so I''m surprised and appreciate
          you even answering this and this quickly as well. I am starting to figure
          out all of this but I''m still very lacking. I don''t do any sort of non-windows
          OS stuff. I don''t know what WSL2 is. I do not know if I have CUDA toolkit.
          If it''s not needed for Llama.cpp, SD, or Ooba only for GPU accleration
          then that may be my issue. But it''s not like I got an error when I input...
          </p>

          <p>pip uninstall -y llama-cpp-python<br>set CMAKE_ARGS="-DLLAMA_CUBLAS=on"<br>set
          FORCE_CMAKE=1<br>pip install llama-cpp-python --no-cache-dir</p>

          <p>However at least I have something to look into. I read others having
          my issue too. At least the good news is that you answered the most important
          question that GGML all supports GPU acceleration so now I only need to figure
          out my issue I don''t have to download other models to troubleshoot. If
          I find the solution I will likely post it here. Thanks. I may look into
          things like WSL2 etc. However it takes me a lot to pickup on all of this.</p>

          '
        raw: "Thanks TheBloke. Your a hard worker so I'm surprised and appreciate\
          \ you even answering this and this quickly as well. I am starting to figure\
          \ out all of this but I'm still very lacking. I don't do any sort of non-windows\
          \ OS stuff. I don't know what WSL2 is. I do not know if I have CUDA toolkit.\
          \ If it's not needed for Llama.cpp, SD, or Ooba only for GPU accleration\
          \ then that may be my issue. But it's not like I got an error when I input...\
          \ \n\npip uninstall -y llama-cpp-python\nset CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\
          \nset FORCE_CMAKE=1\npip install llama-cpp-python --no-cache-dir\n\nHowever\
          \ at least I have something to look into. I read others having my issue\
          \ too. At least the good news is that you answered the most important question\
          \ that GGML all supports GPU acceleration so now I only need to figure out\
          \ my issue I don't have to download other models to troubleshoot. If I find\
          \ the solution I will likely post it here. Thanks. I may look into things\
          \ like WSL2 etc. However it takes me a lot to pickup on all of this."
        updatedAt: '2023-06-08T08:51:42.427Z'
      numEdits: 1
      reactions: []
    id: 6481966a722e358ebf8476f3
    type: comment
  author: Goldenblood56
  content: "Thanks TheBloke. Your a hard worker so I'm surprised and appreciate you\
    \ even answering this and this quickly as well. I am starting to figure out all\
    \ of this but I'm still very lacking. I don't do any sort of non-windows OS stuff.\
    \ I don't know what WSL2 is. I do not know if I have CUDA toolkit. If it's not\
    \ needed for Llama.cpp, SD, or Ooba only for GPU accleration then that may be\
    \ my issue. But it's not like I got an error when I input... \n\npip uninstall\
    \ -y llama-cpp-python\nset CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\nset FORCE_CMAKE=1\n\
    pip install llama-cpp-python --no-cache-dir\n\nHowever at least I have something\
    \ to look into. I read others having my issue too. At least the good news is that\
    \ you answered the most important question that GGML all supports GPU acceleration\
    \ so now I only need to figure out my issue I don't have to download other models\
    \ to troubleshoot. If I find the solution I will likely post it here. Thanks.\
    \ I may look into things like WSL2 etc. However it takes me a lot to pickup on\
    \ all of this."
  created_at: 2023-06-08 07:50:50+00:00
  edited: true
  hidden: false
  id: 6481966a722e358ebf8476f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T08:55:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9303920865058899
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, good luck!</p>

          '
        raw: OK, good luck!
        updatedAt: '2023-06-08T08:55:25.083Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Goldenblood56
    id: 6481977d15c5dc529064488f
    type: comment
  author: TheBloke
  content: OK, good luck!
  created_at: 2023-06-08 07:55:25+00:00
  edited: false
  hidden: false
  id: 6481977d15c5dc529064488f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-12T05:28:45.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9290242195129395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<blockquote>

          <p>I can help with getting it working in WSL2, if you feel like installing
          that.</p>

          </blockquote>

          <p>I ended up getting it working. Several issues. Hard to explain.<br>I
          was going to put in WSL2 but I got it working in windows 10.</p>

          '
        raw: "> I can help with getting it working in WSL2, if you feel like installing\
          \ that.\n\nI ended up getting it working. Several issues. Hard to explain.\
          \ \nI was going to put in WSL2 but I got it working in windows 10."
        updatedAt: '2023-06-13T12:55:04.263Z'
      numEdits: 3
      reactions: []
    id: 6486ad0dc89500f674522a7d
    type: comment
  author: Goldenblood56
  content: "> I can help with getting it working in WSL2, if you feel like installing\
    \ that.\n\nI ended up getting it working. Several issues. Hard to explain. \n\
    I was going to put in WSL2 but I got it working in windows 10."
  created_at: 2023-06-12 04:28:45+00:00
  edited: true
  hidden: false
  id: 6486ad0dc89500f674522a7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cd4b6d1c8a5d1d7d76a778/FAnsBh9h5-wG9Vb252ZNt.png?w=200&h=200&f=face
      fullname: Concedo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: concedo
      type: user
    createdAt: '2023-06-15T04:00:29.000Z'
    data:
      edited: false
      editors:
      - concedo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9279780387878418
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cd4b6d1c8a5d1d7d76a778/FAnsBh9h5-wG9Vb252ZNt.png?w=200&h=200&f=face
          fullname: Concedo
          isHf: false
          isPro: false
          name: concedo
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Goldenblood56&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Goldenblood56\"\
          >@<span class=\"underline\">Goldenblood56</span></a></span>\n\n\t</span></span>\
          \ another alternative you might consider for windows is KoboldCpp, there\
          \ are ready-to-use exes which come with GPU support and no installation\
          \ required.</p>\n"
        raw: '@Goldenblood56 another alternative you might consider for windows is
          KoboldCpp, there are ready-to-use exes which come with GPU support and no
          installation required.'
        updatedAt: '2023-06-15T04:00:29.442Z'
      numEdits: 0
      reactions: []
    id: 648a8cdd5b4d1594af39ef8d
    type: comment
  author: concedo
  content: '@Goldenblood56 another alternative you might consider for windows is KoboldCpp,
    there are ready-to-use exes which come with GPU support and no installation required.'
  created_at: 2023-06-15 03:00:29+00:00
  edited: false
  hidden: false
  id: 648a8cdd5b4d1594af39ef8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-15T06:27:29.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992455005645752
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks but I finally got it all working on Ooba and really well.  I
          can run up to 65B models with alright but slower speeds. I can run 30B models
          very well. With quite the speed.  GGML CPU with GPU offloading. </p>

          '
        raw: 'Thanks but I finally got it all working on Ooba and really well.  I
          can run up to 65B models with alright but slower speeds. I can run 30B models
          very well. With quite the speed.  GGML CPU with GPU offloading. '
        updatedAt: '2023-06-15T06:29:18.856Z'
      numEdits: 3
      reactions: []
    id: 648aaf51173958e8d244c76f
    type: comment
  author: Goldenblood56
  content: 'Thanks but I finally got it all working on Ooba and really well.  I can
    run up to 65B models with alright but slower speeds. I can run 30B models very
    well. With quite the speed.  GGML CPU with GPU offloading. '
  created_at: 2023-06-15 05:27:29+00:00
  edited: true
  hidden: false
  id: 648aaf51173958e8d244c76f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fa476f4380ab0cb953a450/mwM-Xl70QAvYwKrvzyw_C.png?w=200&h=200&f=face
      fullname: hermes t
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: psyberm
      type: user
    createdAt: '2023-06-19T18:12:54.000Z'
    data:
      edited: false
      editors:
      - psyberm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9775916337966919
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fa476f4380ab0cb953a450/mwM-Xl70QAvYwKrvzyw_C.png?w=200&h=200&f=face
          fullname: hermes t
          isHf: false
          isPro: false
          name: psyberm
          type: user
        html: '<blockquote>

          <p>Thanks but I finally got it all working on Ooba and really well.  I can
          run up to 65B models with alright but slower speeds. I can run 30B models
          very well. With quite the speed.  GGML CPU with GPU offloading.</p>

          </blockquote>

          <p>it is always helpful when you share the fix</p>

          '
        raw: '> Thanks but I finally got it all working on Ooba and really well.  I
          can run up to 65B models with alright but slower speeds. I can run 30B models
          very well. With quite the speed.  GGML CPU with GPU offloading.


          it is always helpful when you share the fix'
        updatedAt: '2023-06-19T18:12:54.997Z'
      numEdits: 0
      reactions: []
    id: 64909aa6717173d11635ab5b
    type: comment
  author: psyberm
  content: '> Thanks but I finally got it all working on Ooba and really well.  I
    can run up to 65B models with alright but slower speeds. I can run 30B models
    very well. With quite the speed.  GGML CPU with GPU offloading.


    it is always helpful when you share the fix'
  created_at: 2023-06-19 17:12:54+00:00
  edited: false
  hidden: false
  id: 64909aa6717173d11635ab5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-19T23:00:35.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9869548082351685
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Thanks but I finally got it all working on Ooba and really well.  I can
          run up to 65B models with alright but slower speeds. I can run 30B models
          very well. With quite the speed.  GGML CPU with GPU offloading.</p>

          </blockquote>

          <p>it is always helpful when you share the fix</p>

          </blockquote>

          <p>I did so much that I can''t name all possible solutions. I just found
          a guide of someone else who did it. I re-installed everything but first
          I uninstalled it all to do a clean install. So remove VS and Cuda and then
          install Visual Studios 2022, Cudatool kit 12.1, but I think one of the major
          issues was that I was not activating the right conda environment. So when
          I was install llama-cpp with GPU I was not really installing it. </p>

          '
        raw: "> > Thanks but I finally got it all working on Ooba and really well.\
          \  I can run up to 65B models with alright but slower speeds. I can run\
          \ 30B models very well. With quite the speed.  GGML CPU with GPU offloading.\n\
          > \n> it is always helpful when you share the fix\n\nI did so much that\
          \ I can't name all possible solutions. I just found a guide of someone else\
          \ who did it. I re-installed everything but first I uninstalled it all to\
          \ do a clean install. So remove VS and Cuda and then install Visual Studios\
          \ 2022, Cudatool kit 12.1, but I think one of the major issues was that\
          \ I was not activating the right conda environment. So when I was install\
          \ llama-cpp with GPU I was not really installing it. "
        updatedAt: '2023-06-19T23:01:43.731Z'
      numEdits: 1
      reactions: []
    id: 6490de138a73901838b81063
    type: comment
  author: Goldenblood56
  content: "> > Thanks but I finally got it all working on Ooba and really well. \
    \ I can run up to 65B models with alright but slower speeds. I can run 30B models\
    \ very well. With quite the speed.  GGML CPU with GPU offloading.\n> \n> it is\
    \ always helpful when you share the fix\n\nI did so much that I can't name all\
    \ possible solutions. I just found a guide of someone else who did it. I re-installed\
    \ everything but first I uninstalled it all to do a clean install. So remove VS\
    \ and Cuda and then install Visual Studios 2022, Cudatool kit 12.1, but I think\
    \ one of the major issues was that I was not activating the right conda environment.\
    \ So when I was install llama-cpp with GPU I was not really installing it. "
  created_at: 2023-06-19 22:00:35+00:00
  edited: true
  hidden: false
  id: 6490de138a73901838b81063
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fa476f4380ab0cb953a450/mwM-Xl70QAvYwKrvzyw_C.png?w=200&h=200&f=face
      fullname: hermes t
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: psyberm
      type: user
    createdAt: '2023-06-20T04:16:42.000Z'
    data:
      edited: false
      editors:
      - psyberm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658364057540894
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fa476f4380ab0cb953a450/mwM-Xl70QAvYwKrvzyw_C.png?w=200&h=200&f=face
          fullname: hermes t
          isHf: false
          isPro: false
          name: psyberm
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>Thanks but I finally got it all working on Ooba and really well.  I can
          run up to 65B models with alright but slower speeds. I can run 30B models
          very well. With quite the speed.  GGML CPU with GPU offloading.</p>

          </blockquote>

          <p>it is always helpful when you share the fix</p>

          </blockquote>

          <p>I did so much that I can''t name all possible solutions. I just found
          a guide of someone else who did it. I re-installed everything but first
          I uninstalled it all to do a clean install. So remove VS and Cuda and then
          install Visual Studios 2022, Cudatool kit 12.1, but I think one of the major
          issues was that I was not activating the right conda environment. So when
          I was install llama-cpp with GPU I was not really installing it.</p>

          </blockquote>

          <p>Thank you I updated to CUDA 12.1 and ran the following from ooba''s conda
          environment and was able to get it working </p>

          <p>set FORCE_CMAKE=1<br>set "CMAKE_ARGS=-DLLAMA_CUBLAS=on"<br>set "CUDAFLAGS=-arch=all
          -lcublas"<br>python -m pip install git+<a rel="nofollow" href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a></p>

          '
        raw: "> > > Thanks but I finally got it all working on Ooba and really well.\
          \  I can run up to 65B models with alright but slower speeds. I can run\
          \ 30B models very well. With quite the speed.  GGML CPU with GPU offloading.\n\
          > > \n> > it is always helpful when you share the fix\n> \n> I did so much\
          \ that I can't name all possible solutions. I just found a guide of someone\
          \ else who did it. I re-installed everything but first I uninstalled it\
          \ all to do a clean install. So remove VS and Cuda and then install Visual\
          \ Studios 2022, Cudatool kit 12.1, but I think one of the major issues was\
          \ that I was not activating the right conda environment. So when I was install\
          \ llama-cpp with GPU I was not really installing it.\n\nThank you I updated\
          \ to CUDA 12.1 and ran the following from ooba's conda environment and was\
          \ able to get it working \n\nset FORCE_CMAKE=1\nset \"CMAKE_ARGS=-DLLAMA_CUBLAS=on\"\
          \nset \"CUDAFLAGS=-arch=all -lcublas\"\npython -m pip install git+https://github.com/abetlen/llama-cpp-python"
        updatedAt: '2023-06-20T04:16:42.449Z'
      numEdits: 0
      reactions: []
    id: 6491282a9b15594829c724cf
    type: comment
  author: psyberm
  content: "> > > Thanks but I finally got it all working on Ooba and really well.\
    \  I can run up to 65B models with alright but slower speeds. I can run 30B models\
    \ very well. With quite the speed.  GGML CPU with GPU offloading.\n> > \n> > it\
    \ is always helpful when you share the fix\n> \n> I did so much that I can't name\
    \ all possible solutions. I just found a guide of someone else who did it. I re-installed\
    \ everything but first I uninstalled it all to do a clean install. So remove VS\
    \ and Cuda and then install Visual Studios 2022, Cudatool kit 12.1, but I think\
    \ one of the major issues was that I was not activating the right conda environment.\
    \ So when I was install llama-cpp with GPU I was not really installing it.\n\n\
    Thank you I updated to CUDA 12.1 and ran the following from ooba's conda environment\
    \ and was able to get it working \n\nset FORCE_CMAKE=1\nset \"CMAKE_ARGS=-DLLAMA_CUBLAS=on\"\
    \nset \"CUDAFLAGS=-arch=all -lcublas\"\npython -m pip install git+https://github.com/abetlen/llama-cpp-python"
  created_at: 2023-06-20 03:16:42+00:00
  edited: false
  hidden: false
  id: 6491282a9b15594829c724cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-20T04:31:05.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9942227005958557
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Your welcome. Glad it worked. I also think those commands changed
          since I did it. This stuff all evolves to quickly! lol </p>

          '
        raw: 'Your welcome. Glad it worked. I also think those commands changed since
          I did it. This stuff all evolves to quickly! lol '
        updatedAt: '2023-06-20T04:31:05.262Z'
      numEdits: 0
      reactions: []
    id: 64912b896764aef550f15ca4
    type: comment
  author: Goldenblood56
  content: 'Your welcome. Glad it worked. I also think those commands changed since
    I did it. This stuff all evolves to quickly! lol '
  created_at: 2023-06-20 03:31:05+00:00
  edited: false
  hidden: false
  id: 64912b896764aef550f15ca4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: 'I can''t get GGML GPU accelleration to work with Wizard-Vicuna-30B 5_1? '
