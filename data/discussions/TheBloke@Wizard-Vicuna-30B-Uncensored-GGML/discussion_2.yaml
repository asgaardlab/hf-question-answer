!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CeeGee
conflicting_files: null
created_at: 2023-06-06 21:28:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631ecedf124782a19ef65d88/I4yjJ1j5HdnIoLdbAZuNq.png?w=200&h=200&f=face
      fullname: OldTimeRadioNWN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CeeGee
      type: user
    createdAt: '2023-06-06T22:28:58.000Z'
    data:
      edited: false
      editors:
      - CeeGee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.981220543384552
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631ecedf124782a19ef65d88/I4yjJ1j5HdnIoLdbAZuNq.png?w=200&h=200&f=face
          fullname: OldTimeRadioNWN
          isHf: false
          isPro: false
          name: CeeGee
          type: user
        html: '<p>I assume it would land here- the other quants you already released
          haven''t changed, have they?</p>

          <p>Thanks again for all that you do!</p>

          '
        raw: "I assume it would land here- the other quants you already released haven't\
          \ changed, have they?\r\n\r\nThanks again for all that you do!"
        updatedAt: '2023-06-06T22:28:58.238Z'
      numEdits: 0
      reactions: []
    id: 647fb32a73f7b4386b3ccd51
    type: comment
  author: CeeGee
  content: "I assume it would land here- the other quants you already released haven't\
    \ changed, have they?\r\n\r\nThanks again for all that you do!"
  created_at: 2023-06-06 21:28:58+00:00
  edited: false
  hidden: false
  id: 647fb32a73f7b4386b3ccd51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-06T22:31:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.903484046459198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Correct they''ve not changed. Well, to be exact, old format quants
          created with latest llama.cpp would not be compatible with older versions.
          Which is a bit annoying.  But fortunately the latest llama.cpp code is still
          compatible with files made with the previous versions.</p>

          <p>Therefore from now on I will be making q4_0, q4_1, q5_0, q5_1 and q8_0
          using llama.cpp from a couple of days ago, so they will be compatible with
          both the latest and older llama.cpp code.  Plus I will be adding the newer
          quants that only work with latest llama.cpp.</p>

          <p>And yes I will be adding the new quant types to most or all of my already
          released models.  That process will be starting shortly.</p>

          '
        raw: 'Correct they''ve not changed. Well, to be exact, old format quants created
          with latest llama.cpp would not be compatible with older versions. Which
          is a bit annoying.  But fortunately the latest llama.cpp code is still compatible
          with files made with the previous versions.


          Therefore from now on I will be making q4_0, q4_1, q5_0, q5_1 and q8_0 using
          llama.cpp from a couple of days ago, so they will be compatible with both
          the latest and older llama.cpp code.  Plus I will be adding the newer quants
          that only work with latest llama.cpp.


          And yes I will be adding the new quant types to most or all of my already
          released models.  That process will be starting shortly.'
        updatedAt: '2023-06-06T22:31:19.633Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - CeeGee
        - mikeee
    id: 647fb3b7cbb8294ed80a99a4
    type: comment
  author: TheBloke
  content: 'Correct they''ve not changed. Well, to be exact, old format quants created
    with latest llama.cpp would not be compatible with older versions. Which is a
    bit annoying.  But fortunately the latest llama.cpp code is still compatible with
    files made with the previous versions.


    Therefore from now on I will be making q4_0, q4_1, q5_0, q5_1 and q8_0 using llama.cpp
    from a couple of days ago, so they will be compatible with both the latest and
    older llama.cpp code.  Plus I will be adding the newer quants that only work with
    latest llama.cpp.


    And yes I will be adding the new quant types to most or all of my already released
    models.  That process will be starting shortly.'
  created_at: 2023-06-06 21:31:19+00:00
  edited: false
  hidden: false
  id: 647fb3b7cbb8294ed80a99a4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: Any chance of getting this in Q2_K?
