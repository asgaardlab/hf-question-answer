!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bonswouar
conflicting_files: null
created_at: 2023-07-23 08:54:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b97c6080da1f3fe2c44bf2/Aw0ld5oFxCbzv7cHA0K1w.jpeg?w=200&h=200&f=face
      fullname: Bonswouar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bonswouar
      type: user
    createdAt: '2023-07-23T09:54:55.000Z'
    data:
      edited: false
      editors:
      - bonswouar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6381925940513611
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b97c6080da1f3fe2c44bf2/Aw0ld5oFxCbzv7cHA0K1w.jpeg?w=200&h=200&f=face
          fullname: Bonswouar
          isHf: false
          isPro: false
          name: bonswouar
          type: user
        html: "<p>When I try to load the model I get:<br><code>[...] env\\lib\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py\u201D, line 699, in from_pretrained\
          \ raise ValueError( ValueError: Tokenizer class CodeGen25Tokenizer does\
          \ not exist or is not currently imported.</code><br>Did I miss something?</p>\n"
        raw: "When I try to load the model I get:\r\n`[...] env\\lib\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py\u201D, line 699, in from_pretrained\
          \ raise ValueError( ValueError: Tokenizer class CodeGen25Tokenizer does\
          \ not exist or is not currently imported.`\r\nDid I miss something?"
        updatedAt: '2023-07-23T09:54:55.917Z'
      numEdits: 0
      reactions: []
    id: 64bcf8ef86e7fb5b8a5a3ab1
    type: comment
  author: bonswouar
  content: "When I try to load the model I get:\r\n`[...] env\\lib\\site-packages\\\
    transformers\\models\\auto\\tokenization_auto.py\u201D, line 699, in from_pretrained\
    \ raise ValueError( ValueError: Tokenizer class CodeGen25Tokenizer does not exist\
    \ or is not currently imported.`\r\nDid I miss something?"
  created_at: 2023-07-23 08:54:55+00:00
  edited: false
  hidden: false
  id: 64bcf8ef86e7fb5b8a5a3ab1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78736524404496a1373b731893299915.svg
      fullname: Andrew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DewEfresh
      type: user
    createdAt: '2023-07-24T16:46:00.000Z'
    data:
      edited: false
      editors:
      - DewEfresh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5759165287017822
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78736524404496a1373b731893299915.svg
          fullname: Andrew
          isHf: false
          isPro: false
          name: DewEfresh
          type: user
        html: '<p>I get the same error</p>

          '
        raw: I get the same error
        updatedAt: '2023-07-24T16:46:00.620Z'
      numEdits: 0
      reactions: []
    id: 64beaac84561d0aca2c4df8d
    type: comment
  author: DewEfresh
  content: I get the same error
  created_at: 2023-07-24 15:46:00+00:00
  edited: false
  hidden: false
  id: 64beaac84561d0aca2c4df8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T16:52:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9400680661201477
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to install <code>tiktoken</code> - I included the install
          command in the README.  If you''re using the text-generation-webui one-click-installer,
          tiktoken will need to be installed in the conda environment created for
          text-generation-webui.</p>

          <p>You also need Trust Remote Code = ticked, which I forgot to mention in
          the README.  I''ll add that now.</p>

          <p>I''ve never tested this in text-generation-webui as it''s a code generation
          model which isn''t really suited for UI use, but I do believe it works if
          the above are done.</p>

          '
        raw: 'You need to install `tiktoken` - I included the install command in the
          README.  If you''re using the text-generation-webui one-click-installer,
          tiktoken will need to be installed in the conda environment created for
          text-generation-webui.


          You also need Trust Remote Code = ticked, which I forgot to mention in the
          README.  I''ll add that now.


          I''ve never tested this in text-generation-webui as it''s a code generation
          model which isn''t really suited for UI use, but I do believe it works if
          the above are done.'
        updatedAt: '2023-07-24T16:52:51.553Z'
      numEdits: 0
      reactions: []
    id: 64beac6312afb2f11961eae5
    type: comment
  author: TheBloke
  content: 'You need to install `tiktoken` - I included the install command in the
    README.  If you''re using the text-generation-webui one-click-installer, tiktoken
    will need to be installed in the conda environment created for text-generation-webui.


    You also need Trust Remote Code = ticked, which I forgot to mention in the README.  I''ll
    add that now.


    I''ve never tested this in text-generation-webui as it''s a code generation model
    which isn''t really suited for UI use, but I do believe it works if the above
    are done.'
  created_at: 2023-07-24 15:52:51+00:00
  edited: false
  hidden: false
  id: 64beac6312afb2f11961eae5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T16:57:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8022158145904541
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve updated the README to make the instructions clearer</p>

          '
        raw: I've updated the README to make the instructions clearer
        updatedAt: '2023-07-24T16:57:47.483Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - bonswouar
    id: 64bead8b8496ee0fb622f9a5
    type: comment
  author: TheBloke
  content: I've updated the README to make the instructions clearer
  created_at: 2023-07-24 15:57:47+00:00
  edited: false
  hidden: false
  id: 64bead8b8496ee0fb622f9a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1687e20278bcc4d2afabde1bedb1ca4a.svg
      fullname: Valentino Leonardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fusseldieb
      type: user
    createdAt: '2023-07-24T20:56:03.000Z'
    data:
      edited: true
      editors:
      - Fusseldieb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5925371646881104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1687e20278bcc4d2afabde1bedb1ca4a.svg
          fullname: Valentino Leonardo
          isHf: false
          isPro: false
          name: Fusseldieb
          type: user
        html: '<p>Trust Remote Code = ticked solved it for me. However, I got some
          warnings:</p>

          <pre><code>2023-07-24 17:54:50 INFO:Loading TheBloke_Codegen25-7B-mono-GPTQ...

          2023-07-24 17:54:50 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit-128g'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          True, ''max_memory'': {0: ''7390MiB'', ''cpu'': ''99GiB''}, ''quantize_config'':
          None, ''use_cuda_fp16'': True}

          2023-07-24 17:54:52 WARNING:The safetensors archive passed at models\TheBloke_Codegen25-7B-mono-GPTQ\gptq_model-4bit-128g.safetensors
          does not contain metadata. Make sure to save your model with the `save_pretrained`
          method. Defaulting to ''pt'' metadata.

          2023-07-24 17:54:56 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel
          not support integrate without triton yet.

          Using unk_token, but it is not set yet.

          Using unk_token, but it is not set yet.

          2023-07-24 17:54:56 INFO:Loaded the model in 5.81 seconds.

          </code></pre>

          '
        raw: 'Trust Remote Code = ticked solved it for me. However, I got some warnings:


          ```

          2023-07-24 17:54:50 INFO:Loading TheBloke_Codegen25-7B-mono-GPTQ...

          2023-07-24 17:54:50 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit-128g'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          True, ''max_memory'': {0: ''7390MiB'', ''cpu'': ''99GiB''}, ''quantize_config'':
          None, ''use_cuda_fp16'': True}

          2023-07-24 17:54:52 WARNING:The safetensors archive passed at models\TheBloke_Codegen25-7B-mono-GPTQ\gptq_model-4bit-128g.safetensors
          does not contain metadata. Make sure to save your model with the `save_pretrained`
          method. Defaulting to ''pt'' metadata.

          2023-07-24 17:54:56 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel
          not support integrate without triton yet.

          Using unk_token, but it is not set yet.

          Using unk_token, but it is not set yet.

          2023-07-24 17:54:56 INFO:Loaded the model in 5.81 seconds.

          ```'
        updatedAt: '2023-07-24T20:56:28.394Z'
      numEdits: 1
      reactions: []
    id: 64bee5631a62149c5e9f820b
    type: comment
  author: Fusseldieb
  content: 'Trust Remote Code = ticked solved it for me. However, I got some warnings:


    ```

    2023-07-24 17:54:50 INFO:Loading TheBloke_Codegen25-7B-mono-GPTQ...

    2023-07-24 17:54:50 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit-128g'',
    ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'': True,
    ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
    True, ''max_memory'': {0: ''7390MiB'', ''cpu'': ''99GiB''}, ''quantize_config'':
    None, ''use_cuda_fp16'': True}

    2023-07-24 17:54:52 WARNING:The safetensors archive passed at models\TheBloke_Codegen25-7B-mono-GPTQ\gptq_model-4bit-128g.safetensors
    does not contain metadata. Make sure to save your model with the `save_pretrained`
    method. Defaulting to ''pt'' metadata.

    2023-07-24 17:54:56 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel
    not support integrate without triton yet.

    Using unk_token, but it is not set yet.

    Using unk_token, but it is not set yet.

    2023-07-24 17:54:56 INFO:Loaded the model in 5.81 seconds.

    ```'
  created_at: 2023-07-24 19:56:03+00:00
  edited: true
  hidden: false
  id: 64bee5631a62149c5e9f820b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Codegen25-7B-mono-GPTQ
repo_type: model
status: open
target_branch: null
title: text-generation-webui issue
