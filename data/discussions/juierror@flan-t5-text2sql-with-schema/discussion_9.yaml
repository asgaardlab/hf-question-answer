!!python/object:huggingface_hub.community.DiscussionWithDetails
author: karrr0n
conflicting_files: null
created_at: 2023-07-23 11:01:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/116a0c2faf0ed49bee166d2d77d19226.svg
      fullname: karrr0n
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: karrr0n
      type: user
    createdAt: '2023-07-23T12:01:21.000Z'
    data:
      edited: false
      editors:
      - karrr0n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.822377622127533
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/116a0c2faf0ed49bee166d2d77d19226.svg
          fullname: karrr0n
          isHf: false
          isPro: false
          name: karrr0n
          type: user
        html: '<p>Hello,<br>I exported this model to onnx (decoder.onnx, encoder.onnx,
          ..). Now I would like to load it into some C# code and use it.<br>I tried
          with Tutorials like <a rel="nofollow" href="https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html">https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html</a>,
          but I run into Error:  "Tensor element data type discovered: Int64 metadata
          expected: Float".</p>

          <p>Can someone help?</p>

          <p>Best Regards</p>

          '
        raw: "Hello,\r\nI exported this model to onnx (decoder.onnx, encoder.onnx,\
          \ ..). Now I would like to load it into some C# code and use it. \r\nI tried\
          \ with Tutorials like https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html,\
          \ but I run into Error:  \"Tensor element data type discovered: Int64 metadata\
          \ expected: Float\".\r\n\r\nCan someone help?\r\n\r\nBest Regards"
        updatedAt: '2023-07-23T12:01:21.285Z'
      numEdits: 0
      reactions: []
    id: 64bd1691b9296eef62cc7987
    type: comment
  author: karrr0n
  content: "Hello,\r\nI exported this model to onnx (decoder.onnx, encoder.onnx, ..).\
    \ Now I would like to load it into some C# code and use it. \r\nI tried with Tutorials\
    \ like https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html,\
    \ but I run into Error:  \"Tensor element data type discovered: Int64 metadata\
    \ expected: Float\".\r\n\r\nCan someone help?\r\n\r\nBest Regards"
  created_at: 2023-07-23 11:01:21+00:00
  edited: false
  hidden: false
  id: 64bd1691b9296eef62cc7987
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
      fullname: Siwa Boonpunmongkol
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: juierror
      type: user
    createdAt: '2023-07-24T07:54:45.000Z'
    data:
      edited: false
      editors:
      - juierror
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9486491084098816
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
          fullname: Siwa Boonpunmongkol
          isHf: false
          isPro: false
          name: juierror
          type: user
        html: '<p>@cioo </p>

          <p>Hi, to be frank, I have never export this model to onnx and use it with
          C#, so I''m not sure what happen, but from the error, maybe some of the
          first layer is missing (not sure).</p>

          <p>If I have time, I will try to reproduce it with python onnx.</p>

          '
        raw: "@cioo \n\nHi, to be frank, I have never export this model to onnx and\
          \ use it with C#, so I'm not sure what happen, but from the error, maybe\
          \ some of the first layer is missing (not sure).\n\nIf I have time, I will\
          \ try to reproduce it with python onnx."
        updatedAt: '2023-07-24T07:54:45.689Z'
      numEdits: 0
      reactions: []
    id: 64be2e45c05a0df0d2b305ac
    type: comment
  author: juierror
  content: "@cioo \n\nHi, to be frank, I have never export this model to onnx and\
    \ use it with C#, so I'm not sure what happen, but from the error, maybe some\
    \ of the first layer is missing (not sure).\n\nIf I have time, I will try to reproduce\
    \ it with python onnx."
  created_at: 2023-07-24 06:54:45+00:00
  edited: false
  hidden: false
  id: 64be2e45c05a0df0d2b305ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/116a0c2faf0ed49bee166d2d77d19226.svg
      fullname: karrr0n
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: karrr0n
      type: user
    createdAt: '2023-07-24T16:02:38.000Z'
    data:
      edited: false
      editors:
      - karrr0n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5443079471588135
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/116a0c2faf0ed49bee166d2d77d19226.svg
          fullname: karrr0n
          isHf: false
          isPro: false
          name: karrr0n
          type: user
        html: '<p>Hello,<br>thank you for your feedback!<br>I simply exported btw
          with: optimum-cli export onnx --model "juierror/text-to-sql-with-table-schema"
          model_onnx/</p>

          <p>Then I tried to load decoder_model.onnx (<a rel="nofollow" href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-automl-onnx-model-dotnet?view=azureml-api-2">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-automl-onnx-model-dotnet?view=azureml-api-2</a>).<br>I
          dont know how exactly the input-data (encoder_attention_mask, input_ids,
          encoder_hidden_states) should be initialized.<br>For input_ids I used the
          output of: prepare_input(question=myquestion, table=mytable) - in python.<br>encoder_attention_mask
          = 1 and encoder_hidden_states = 1, but get the error "encoder_hidden_states
          Got: 2 Expected: 3 Please fix either the inputs or the model.".</p>

          <p>I''m curious about your experiences. Best Regards</p>

          '
        raw: "Hello, \nthank you for your feedback!\nI simply exported btw with: optimum-cli\
          \ export onnx --model \"juierror/text-to-sql-with-table-schema\" model_onnx/\n\
          \nThen I tried to load decoder_model.onnx (https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-automl-onnx-model-dotnet?view=azureml-api-2).\
          \ \nI dont know how exactly the input-data (encoder_attention_mask, input_ids,\
          \ encoder_hidden_states) should be initialized.\nFor input_ids I used the\
          \ output of: prepare_input(question=myquestion, table=mytable) - in python.\n\
          encoder_attention_mask = 1 and encoder_hidden_states = 1, but get the error\
          \ \"encoder_hidden_states Got: 2 Expected: 3 Please fix either the inputs\
          \ or the model.\".\n\nI'm curious about your experiences. Best Regards"
        updatedAt: '2023-07-24T16:02:38.837Z'
      numEdits: 0
      reactions: []
    id: 64bea09ef8f28a19b0f080b1
    type: comment
  author: karrr0n
  content: "Hello, \nthank you for your feedback!\nI simply exported btw with: optimum-cli\
    \ export onnx --model \"juierror/text-to-sql-with-table-schema\" model_onnx/\n\
    \nThen I tried to load decoder_model.onnx (https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-automl-onnx-model-dotnet?view=azureml-api-2).\
    \ \nI dont know how exactly the input-data (encoder_attention_mask, input_ids,\
    \ encoder_hidden_states) should be initialized.\nFor input_ids I used the output\
    \ of: prepare_input(question=myquestion, table=mytable) - in python.\nencoder_attention_mask\
    \ = 1 and encoder_hidden_states = 1, but get the error \"encoder_hidden_states\
    \ Got: 2 Expected: 3 Please fix either the inputs or the model.\".\n\nI'm curious\
    \ about your experiences. Best Regards"
  created_at: 2023-07-24 15:02:38+00:00
  edited: false
  hidden: false
  id: 64bea09ef8f28a19b0f080b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
      fullname: Siwa Boonpunmongkol
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: juierror
      type: user
    createdAt: '2023-07-25T15:54:45.000Z'
    data:
      edited: false
      editors:
      - juierror
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6477736830711365
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
          fullname: Siwa Boonpunmongkol
          isHf: false
          isPro: false
          name: juierror
          type: user
        html: '<p>Hi,</p>

          <p>I already try to export and inference the onnx model. The T5 model is
          a encoder-decoder model, so it different from the tutorial you are provide
          which is encoder only model <a rel="nofollow" href="https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html">https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html</a>.</p>

          <p>When I run <code>optimum-cli export onnx --model "juierror/text-to-sql-with-table-schema"
          model_onnx/</code>, I get 4 onnx models inside <code>model_onnx/</code>,
          but I use only <code>encoder_model.onnx</code> and <code>decoder_model.onnx</code>.</p>

          <p>From my understanding, we have to run encoder model first and then use
          the encoder output as one of input to the decoder model.</p>

          <p>I can get the result with this method, but the output is incorrect, so
          I''m not sure if my step is wrong or not.</p>

          <p>this is input<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/HC6ZfZiR96u0Ek96XmNC5.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/HC6ZfZiR96u0Ek96XmNC5.png"></a></p>

          <p>this is output<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/DjKFazVmlUfgGPcPWwK9R.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/DjKFazVmlUfgGPcPWwK9R.png"></a></p>

          <p>this is the code <a rel="nofollow" href="https://github.com/juierror/experiment_code/blob/master/test_onnx_clean.ipynb">https://github.com/juierror/experiment_code/blob/master/test_onnx_clean.ipynb</a></p>

          <p>I also see optimum has a way to run the model<br> <a href="https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-cli">https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-cli</a><br><a
          href="https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModel">https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModel</a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/Mm--Awa-Irp00DdUjwTdo.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/Mm--Awa-Irp00DdUjwTdo.png"></a></p>

          <p>Thus, maybe you can check how optimum inference and try it with C#.</p>

          '
        raw: "Hi,\n\nI already try to export and inference the onnx model. The T5\
          \ model is a encoder-decoder model, so it different from the tutorial you\
          \ are provide which is encoder only model [https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html](https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html).\n\
          \nWhen I run `optimum-cli export onnx --model \"juierror/text-to-sql-with-table-schema\"\
          \ model_onnx/`, I get 4 onnx models inside `model_onnx/`, but I use only\
          \ `encoder_model.onnx` and `decoder_model.onnx`.\n\nFrom my understanding,\
          \ we have to run encoder model first and then use the encoder output as\
          \ one of input to the decoder model.\n\nI can get the result with this method,\
          \ but the output is incorrect, so I'm not sure if my step is wrong or not.\n\
          \nthis is input\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/HC6ZfZiR96u0Ek96XmNC5.png)\n\
          \nthis is output\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/DjKFazVmlUfgGPcPWwK9R.png)\n\
          \nthis is the code [https://github.com/juierror/experiment_code/blob/master/test_onnx_clean.ipynb](https://github.com/juierror/experiment_code/blob/master/test_onnx_clean.ipynb)\n\
          \nI also see optimum has a way to run the model\n [https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-cli](https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-cli)\n\
          [https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModel](https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModel)\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/Mm--Awa-Irp00DdUjwTdo.png)\n\
          \nThus, maybe you can check how optimum inference and try it with C#."
        updatedAt: '2023-07-25T15:54:45.249Z'
      numEdits: 0
      reactions: []
    id: 64bff0456c643f404162e206
    type: comment
  author: juierror
  content: "Hi,\n\nI already try to export and inference the onnx model. The T5 model\
    \ is a encoder-decoder model, so it different from the tutorial you are provide\
    \ which is encoder only model [https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html](https://onnxruntime.ai/docs/tutorials/csharp/bert-nlp-csharp-console-app.html).\n\
    \nWhen I run `optimum-cli export onnx --model \"juierror/text-to-sql-with-table-schema\"\
    \ model_onnx/`, I get 4 onnx models inside `model_onnx/`, but I use only `encoder_model.onnx`\
    \ and `decoder_model.onnx`.\n\nFrom my understanding, we have to run encoder model\
    \ first and then use the encoder output as one of input to the decoder model.\n\
    \nI can get the result with this method, but the output is incorrect, so I'm not\
    \ sure if my step is wrong or not.\n\nthis is input\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/HC6ZfZiR96u0Ek96XmNC5.png)\n\
    \nthis is output\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/DjKFazVmlUfgGPcPWwK9R.png)\n\
    \nthis is the code [https://github.com/juierror/experiment_code/blob/master/test_onnx_clean.ipynb](https://github.com/juierror/experiment_code/blob/master/test_onnx_clean.ipynb)\n\
    \nI also see optimum has a way to run the model\n [https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-cli](https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-cli)\n\
    [https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModel](https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModel)\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/61d13ad6db453feb805ba8df/Mm--Awa-Irp00DdUjwTdo.png)\n\
    \nThus, maybe you can check how optimum inference and try it with C#."
  created_at: 2023-07-25 14:54:45+00:00
  edited: false
  hidden: false
  id: 64bff0456c643f404162e206
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/116a0c2faf0ed49bee166d2d77d19226.svg
      fullname: karrr0n
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: karrr0n
      type: user
    createdAt: '2023-08-14T17:33:23.000Z'
    data:
      edited: false
      editors:
      - karrr0n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8534501791000366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/116a0c2faf0ed49bee166d2d77d19226.svg
          fullname: karrr0n
          isHf: false
          isPro: false
          name: karrr0n
          type: user
        html: '<p>Hi,</p>

          <p>I ran encoder model first and then used the encoder output as one of
          input to the decoder model, like this: </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64aaa0a2baf671fbebd8d8cd/PmbWFOz2P0JZKt0iYuuFP.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64aaa0a2baf671fbebd8d8cd/PmbWFOz2P0JZKt0iYuuFP.png"></a></p>

          <p>I also can get the decoder output. The decoder models output is "logits",
          a float array.<br>Do I need to use "google/flan-t5-base"-Tokenizer to get
          the words from logits? Because in .Net there are only a few Tokenizer, e.g.
          BertUncasedLargeTokenizer. </p>

          <p>I also tried your training code with "google/flan-t5-base" model and
          BertTokenizer, but result was:<br>question: Who is the director of the episode
          that corresponds to the total episodes number 14?<br>table: [''Total#'',
          ''Series#'', ''Title'', ''Writer'', ''Director'', ''Original air date'']<br>model
          result: [unused19] [unused19] [unused19] [unused19] [unused19] [unused19]
          [unused19] [unused19] [unused19] [unused1]<br>real result: SELECT Director
          FROM table WHERE Total# = 14</p>

          <p>Is it important that the model was trained with the same tokenizer as
          it will be called? If that''s the case, I would need a model trained with
          the BertTokenizer. Do you have any idea why simply swapping the tokenizers
          in your training code doesn''t work?</p>

          <p>Best regards!</p>

          '
        raw: "Hi,\n\nI ran encoder model first and then used the encoder output as\
          \ one of input to the decoder model, like this: \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64aaa0a2baf671fbebd8d8cd/PmbWFOz2P0JZKt0iYuuFP.png)\n\
          \nI also can get the decoder output. The decoder models output is \"logits\"\
          , a float array.\nDo I need to use \"google/flan-t5-base\"-Tokenizer to\
          \ get the words from logits? Because in .Net there are only a few Tokenizer,\
          \ e.g. BertUncasedLargeTokenizer. \n\nI also tried your training code with\
          \ \"google/flan-t5-base\" model and BertTokenizer, but result was: \nquestion:\
          \ Who is the director of the episode that corresponds to the total episodes\
          \ number 14? \ntable: ['Total#', 'Series#', 'Title', 'Writer', 'Director',\
          \ 'Original air date']\nmodel result: [unused19] [unused19] [unused19] [unused19]\
          \ [unused19] [unused19] [unused19] [unused19] [unused19] [unused1]\nreal\
          \ result: SELECT Director FROM table WHERE Total# = 14\n\nIs it important\
          \ that the model was trained with the same tokenizer as it will be called?\
          \ If that's the case, I would need a model trained with the BertTokenizer.\
          \ Do you have any idea why simply swapping the tokenizers in your training\
          \ code doesn't work?\n\nBest regards!"
        updatedAt: '2023-08-14T17:33:23.955Z'
      numEdits: 0
      reactions: []
    id: 64da6563b5d625e0e96ae95d
    type: comment
  author: karrr0n
  content: "Hi,\n\nI ran encoder model first and then used the encoder output as one\
    \ of input to the decoder model, like this: \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64aaa0a2baf671fbebd8d8cd/PmbWFOz2P0JZKt0iYuuFP.png)\n\
    \nI also can get the decoder output. The decoder models output is \"logits\",\
    \ a float array.\nDo I need to use \"google/flan-t5-base\"-Tokenizer to get the\
    \ words from logits? Because in .Net there are only a few Tokenizer, e.g. BertUncasedLargeTokenizer.\
    \ \n\nI also tried your training code with \"google/flan-t5-base\" model and BertTokenizer,\
    \ but result was: \nquestion: Who is the director of the episode that corresponds\
    \ to the total episodes number 14? \ntable: ['Total#', 'Series#', 'Title', 'Writer',\
    \ 'Director', 'Original air date']\nmodel result: [unused19] [unused19] [unused19]\
    \ [unused19] [unused19] [unused19] [unused19] [unused19] [unused19] [unused1]\n\
    real result: SELECT Director FROM table WHERE Total# = 14\n\nIs it important that\
    \ the model was trained with the same tokenizer as it will be called? If that's\
    \ the case, I would need a model trained with the BertTokenizer. Do you have any\
    \ idea why simply swapping the tokenizers in your training code doesn't work?\n\
    \nBest regards!"
  created_at: 2023-08-14 16:33:23+00:00
  edited: false
  hidden: false
  id: 64da6563b5d625e0e96ae95d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
      fullname: Siwa Boonpunmongkol
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: juierror
      type: user
    createdAt: '2023-08-15T00:21:58.000Z'
    data:
      edited: false
      editors:
      - juierror
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9141295552253723
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
          fullname: Siwa Boonpunmongkol
          isHf: false
          isPro: false
          name: juierror
          type: user
        html: '<p>Hi @cioo</p>

          <p>About the first question, after you get logits, you should use the same
          tokenizer as training phase, because each logits value represent the token
          which is different for each tokenizer.</p>

          <p>For the second question, from my understanding, the T5 model is pre-trained
          on T5 tokenizer, so it is recognize which logit is which token. If you change
          the tokenizer when finetune, the model might confuse.</p>

          '
        raw: 'Hi @cioo


          About the first question, after you get logits, you should use the same
          tokenizer as training phase, because each logits value represent the token
          which is different for each tokenizer.


          For the second question, from my understanding, the T5 model is pre-trained
          on T5 tokenizer, so it is recognize which logit is which token. If you change
          the tokenizer when finetune, the model might confuse.'
        updatedAt: '2023-08-15T00:21:58.287Z'
      numEdits: 0
      reactions: []
    id: 64dac5261d19239f50534710
    type: comment
  author: juierror
  content: 'Hi @cioo


    About the first question, after you get logits, you should use the same tokenizer
    as training phase, because each logits value represent the token which is different
    for each tokenizer.


    For the second question, from my understanding, the T5 model is pre-trained on
    T5 tokenizer, so it is recognize which logit is which token. If you change the
    tokenizer when finetune, the model might confuse.'
  created_at: 2023-08-14 23:21:58+00:00
  edited: false
  hidden: false
  id: 64dac5261d19239f50534710
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d0979c1ca701a336a7cfc7445dff92bf.svg
      fullname: Mohammad Baasit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muhammadbaasit
      type: user
    createdAt: '2023-09-11T18:38:36.000Z'
    data:
      edited: false
      editors:
      - muhammadbaasit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7852903008460999
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d0979c1ca701a336a7cfc7445dff92bf.svg
          fullname: Mohammad Baasit
          isHf: false
          isPro: false
          name: muhammadbaasit
          type: user
        html: "<p>Hello @cioo <span data-props=\"{&quot;user&quot;:&quot;juierror&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juierror\"\
          >@<span class=\"underline\">juierror</span></a></span>\n\n\t</span></span>\
          \ ,<br>I it possible to create single onnx file instead of creating 2 files\
          \ that is encoder.onnx and decoder.onnx . same thing i am trying to convert\
          \ a model to tflite getting same issue. please le me know is there any way\
          \ to do this.</p>\n"
        raw: 'Hello @cioo @juierror ,

          I it possible to create single onnx file instead of creating 2 files that
          is encoder.onnx and decoder.onnx . same thing i am trying to convert a model
          to tflite getting same issue. please le me know is there any way to do this.'
        updatedAt: '2023-09-11T18:38:36.685Z'
      numEdits: 0
      reactions: []
    id: 64ff5eac3587d3ebfd729c46
    type: comment
  author: muhammadbaasit
  content: 'Hello @cioo @juierror ,

    I it possible to create single onnx file instead of creating 2 files that is encoder.onnx
    and decoder.onnx . same thing i am trying to convert a model to tflite getting
    same issue. please le me know is there any way to do this.'
  created_at: 2023-09-11 17:38:36+00:00
  edited: false
  hidden: false
  id: 64ff5eac3587d3ebfd729c46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
      fullname: Siwa Boonpunmongkol
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: juierror
      type: user
    createdAt: '2023-09-17T05:57:52.000Z'
    data:
      edited: false
      editors:
      - juierror
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8243818879127502
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c90043d021dfce39448979dee01350eb.svg
          fullname: Siwa Boonpunmongkol
          isHf: false
          isPro: false
          name: juierror
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;muhammadbaasit&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/muhammadbaasit\"\
          >@<span class=\"underline\">muhammadbaasit</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Hi, sorry for late reply, to be frank, I am not not sure how\
          \ to merge encoder.onnx and decoder.onnx into 1 file or export it in single\
          \ file.</p>\n"
        raw: "@muhammadbaasit \n\nHi, sorry for late reply, to be frank, I am not\
          \ not sure how to merge encoder.onnx and decoder.onnx into 1 file or export\
          \ it in single file."
        updatedAt: '2023-09-17T05:57:52.021Z'
      numEdits: 0
      reactions: []
    id: 650695600c873319478deb00
    type: comment
  author: juierror
  content: "@muhammadbaasit \n\nHi, sorry for late reply, to be frank, I am not not\
    \ sure how to merge encoder.onnx and decoder.onnx into 1 file or export it in\
    \ single file."
  created_at: 2023-09-17 04:57:52+00:00
  edited: false
  hidden: false
  id: 650695600c873319478deb00
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: juierror/flan-t5-text2sql-with-schema
repo_type: model
status: open
target_branch: null
title: ONNX Inference in C#
