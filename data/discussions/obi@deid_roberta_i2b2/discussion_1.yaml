!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pyang
conflicting_files: null
created_at: 2022-08-05 13:34:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
      fullname: Pengwei Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pyang
      type: user
    createdAt: '2022-08-05T14:34:04.000Z'
    data:
      edited: false
      editors:
      - pyang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
          fullname: Pengwei Yang
          isHf: false
          isPro: false
          name: pyang
          type: user
        html: '<p>HI: have been testing HF azure endpoint for this model (obi/deid_roberta_i2b2).
          Model is able to accept list of sentences as input, and noticed that HTTP
          errors occur when input list sizes are large. Wondering what limit of tokens
          for each sequence and limit of the length of list. Its HF model description
          page doesn''t provide much details, any further documentation about model
          would be appreciated. Also for its endpoint, any request rate limit and
          if concurrent requests are allowed. Thanks. </p>

          '
        raw: 'HI: have been testing HF azure endpoint for this model (obi/deid_roberta_i2b2).
          Model is able to accept list of sentences as input, and noticed that HTTP
          errors occur when input list sizes are large. Wondering what limit of tokens
          for each sequence and limit of the length of list. Its HF model description
          page doesn''t provide much details, any further documentation about model
          would be appreciated. Also for its endpoint, any request rate limit and
          if concurrent requests are allowed. Thanks. '
        updatedAt: '2022-08-05T14:34:04.932Z'
      numEdits: 0
      reactions: []
    id: 62ed2a5c0effdbb9137d48c9
    type: comment
  author: pyang
  content: 'HI: have been testing HF azure endpoint for this model (obi/deid_roberta_i2b2).
    Model is able to accept list of sentences as input, and noticed that HTTP errors
    occur when input list sizes are large. Wondering what limit of tokens for each
    sequence and limit of the length of list. Its HF model description page doesn''t
    provide much details, any further documentation about model would be appreciated.
    Also for its endpoint, any request rate limit and if concurrent requests are allowed.
    Thanks. '
  created_at: 2022-08-05 13:34:04+00:00
  edited: false
  hidden: false
  id: 62ed2a5c0effdbb9137d48c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
      fullname: Prajwal Kailas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: prajwal967
      type: user
    createdAt: '2022-08-05T17:53:12.000Z'
    data:
      edited: false
      editors:
      - prajwal967
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
          fullname: Prajwal Kailas
          isHf: false
          isPro: false
          name: prajwal967
          type: user
        html: '<p>Hi, the transformer model expects 512 tokens. The text input to
          the model should be such that, the resulting subword tokens should be less
          than 512. In our project we used a limit of 128 spacy tokens. The 128 spacy
          tokens on our dataset always resulted in less than 512 subword tokens. But
          you can modify your sequences so that the subword tokenization always results
          in less than 512 tokens - or you can use the truncate function in the subword
          tokenizer. </p>

          <p>We believe the limit on the length of the input list will be dependent
          on your machine and available resources. </p>

          <p>I hope this answered a few of your questions, let us know if there is
          anything else you want to clarify. We''re working on improving the model
          and documentation!</p>

          '
        raw: "Hi, the transformer model expects 512 tokens. The text input to the\
          \ model should be such that, the resulting subword tokens should be less\
          \ than 512. In our project we used a limit of 128 spacy tokens. The 128\
          \ spacy tokens on our dataset always resulted in less than 512 subword tokens.\
          \ But you can modify your sequences so that the subword tokenization always\
          \ results in less than 512 tokens - or you can use the truncate function\
          \ in the subword tokenizer. \n\nWe believe the limit on the length of the\
          \ input list will be dependent on your machine and available resources.\
          \ \n\nI hope this answered a few of your questions, let us know if there\
          \ is anything else you want to clarify. We're working on improving the model\
          \ and documentation!"
        updatedAt: '2022-08-05T17:53:12.487Z'
      numEdits: 0
      reactions: []
    id: 62ed590859db46cc832e2d10
    type: comment
  author: prajwal967
  content: "Hi, the transformer model expects 512 tokens. The text input to the model\
    \ should be such that, the resulting subword tokens should be less than 512. In\
    \ our project we used a limit of 128 spacy tokens. The 128 spacy tokens on our\
    \ dataset always resulted in less than 512 subword tokens. But you can modify\
    \ your sequences so that the subword tokenization always results in less than\
    \ 512 tokens - or you can use the truncate function in the subword tokenizer.\
    \ \n\nWe believe the limit on the length of the input list will be dependent on\
    \ your machine and available resources. \n\nI hope this answered a few of your\
    \ questions, let us know if there is anything else you want to clarify. We're\
    \ working on improving the model and documentation!"
  created_at: 2022-08-05 16:53:12+00:00
  edited: false
  hidden: false
  id: 62ed590859db46cc832e2d10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
      fullname: Pengwei Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pyang
      type: user
    createdAt: '2022-08-05T21:51:14.000Z'
    data:
      edited: true
      editors:
      - pyang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
          fullname: Pengwei Yang
          isHf: false
          isPro: false
          name: pyang
          type: user
        html: "<p>HI <span data-props=\"{&quot;user&quot;:&quot;prajwal967&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/prajwal967\"\
          >@<span class=\"underline\">prajwal967</span></a></span>\n\n\t</span></span>\
          \ , thanks very much for the input! Does 128-token include 32 added-on tokens?\
          \ We use scispacy \"en_core_sci_lg\" to sentencise text for endpoint inputs,\
          \ in this case what token limit would you suggest to avoid exceeding 512\
          \ subword token limit? what parameters can be passed via HF azure endpoint\
          \ to config this model? thanks!</p>\n"
        raw: HI @prajwal967 , thanks very much for the input! Does 128-token include
          32 added-on tokens? We use scispacy "en_core_sci_lg" to sentencise text
          for endpoint inputs, in this case what token limit would you suggest to
          avoid exceeding 512 subword token limit? what parameters can be passed via
          HF azure endpoint to config this model? thanks!
        updatedAt: '2022-08-05T22:14:36.971Z'
      numEdits: 3
      reactions: []
    id: 62ed90d290cfc05207f438c1
    type: comment
  author: pyang
  content: HI @prajwal967 , thanks very much for the input! Does 128-token include
    32 added-on tokens? We use scispacy "en_core_sci_lg" to sentencise text for endpoint
    inputs, in this case what token limit would you suggest to avoid exceeding 512
    subword token limit? what parameters can be passed via HF azure endpoint to config
    this model? thanks!
  created_at: 2022-08-05 20:51:14+00:00
  edited: true
  hidden: false
  id: 62ed90d290cfc05207f438c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
      fullname: Prajwal Kailas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: prajwal967
      type: user
    createdAt: '2022-08-08T14:53:45.000Z'
    data:
      edited: false
      editors:
      - prajwal967
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
          fullname: Prajwal Kailas
          isHf: false
          isPro: false
          name: prajwal967
          type: user
        html: '<p>Yes, the 128 tokens include the 32 added on tokens. Which means
          there will be 32 added on tokens on either side (hence 64 added on tokens
          in total) of the current sentence/chunk. If 128 tokens are exceeding the
          tokens limit, you can try setting the token limits to smaller values. For
          example:<br>max_tokens = 64<br>max_prev_sentence_token = 16<br>max_next_sentence_token
          = 16<br>default_chunk_size = 16</p>

          <p>By HF azure endpoint, are you referring to the AutoModel from Hugging
          Face? If yes, then the parameters that can be passed are available on the
          Hugging Face documentation pages.</p>

          <p>Let us know if you still have trouble getting this to work! Thanks!</p>

          '
        raw: 'Yes, the 128 tokens include the 32 added on tokens. Which means there
          will be 32 added on tokens on either side (hence 64 added on tokens in total)
          of the current sentence/chunk. If 128 tokens are exceeding the tokens limit,
          you can try setting the token limits to smaller values. For example:

          max_tokens = 64

          max_prev_sentence_token = 16

          max_next_sentence_token = 16

          default_chunk_size = 16


          By HF azure endpoint, are you referring to the AutoModel from Hugging Face?
          If yes, then the parameters that can be passed are available on the Hugging
          Face documentation pages.


          Let us know if you still have trouble getting this to work! Thanks!'
        updatedAt: '2022-08-08T14:53:45.722Z'
      numEdits: 0
      reactions: []
    id: 62f12379335ce25164bcd11e
    type: comment
  author: prajwal967
  content: 'Yes, the 128 tokens include the 32 added on tokens. Which means there
    will be 32 added on tokens on either side (hence 64 added on tokens in total)
    of the current sentence/chunk. If 128 tokens are exceeding the tokens limit, you
    can try setting the token limits to smaller values. For example:

    max_tokens = 64

    max_prev_sentence_token = 16

    max_next_sentence_token = 16

    default_chunk_size = 16


    By HF azure endpoint, are you referring to the AutoModel from Hugging Face? If
    yes, then the parameters that can be passed are available on the Hugging Face
    documentation pages.


    Let us know if you still have trouble getting this to work! Thanks!'
  created_at: 2022-08-08 13:53:45+00:00
  edited: false
  hidden: false
  id: 62f12379335ce25164bcd11e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
      fullname: Pengwei Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pyang
      type: user
    createdAt: '2022-08-08T17:11:52.000Z'
    data:
      edited: false
      editors:
      - pyang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
          fullname: Pengwei Yang
          isHf: false
          isPro: false
          name: pyang
          type: user
        html: "<p>thanks very much, <span data-props=\"{&quot;user&quot;:&quot;prajwal967&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/prajwal967\"\
          >@<span class=\"underline\">prajwal967</span></a></span>\n\n\t</span></span>\
          \ ! what compute instance specs in terms of vCPUs/memory would you suggest\
          \ for model deployment for inference? thanks.</p>\n"
        raw: thanks very much, @prajwal967 ! what compute instance specs in terms
          of vCPUs/memory would you suggest for model deployment for inference? thanks.
        updatedAt: '2022-08-08T17:11:52.292Z'
      numEdits: 0
      reactions: []
    id: 62f143d827cc18b5fc36c33a
    type: comment
  author: pyang
  content: thanks very much, @prajwal967 ! what compute instance specs in terms of
    vCPUs/memory would you suggest for model deployment for inference? thanks.
  created_at: 2022-08-08 16:11:52+00:00
  edited: false
  hidden: false
  id: 62f143d827cc18b5fc36c33a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
      fullname: Prajwal Kailas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: prajwal967
      type: user
    createdAt: '2022-08-09T15:24:17.000Z'
    data:
      edited: false
      editors:
      - prajwal967
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
          fullname: Prajwal Kailas
          isHf: false
          isPro: false
          name: prajwal967
          type: user
        html: '<p>That would depend on a few parameter (for example batch size). But
          I think with a batch size of 1 or 2, you should be able to run it on CPU
          machine with about 16GB of RAM. You can go higher with the batch size if
          you have more compute resources/memory available.</p>

          '
        raw: That would depend on a few parameter (for example batch size). But I
          think with a batch size of 1 or 2, you should be able to run it on CPU machine
          with about 16GB of RAM. You can go higher with the batch size if you have
          more compute resources/memory available.
        updatedAt: '2022-08-09T15:24:17.633Z'
      numEdits: 0
      reactions: []
    id: 62f27c21b6b837caa6081948
    type: comment
  author: prajwal967
  content: That would depend on a few parameter (for example batch size). But I think
    with a batch size of 1 or 2, you should be able to run it on CPU machine with
    about 16GB of RAM. You can go higher with the batch size if you have more compute
    resources/memory available.
  created_at: 2022-08-09 14:24:17+00:00
  edited: false
  hidden: false
  id: 62f27c21b6b837caa6081948
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
      fullname: Pengwei Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pyang
      type: user
    createdAt: '2022-08-09T17:48:41.000Z'
    data:
      edited: false
      editors:
      - pyang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56190d82862dc627cf3d354d8b368fac.svg
          fullname: Pengwei Yang
          isHf: false
          isPro: false
          name: pyang
          type: user
        html: '<p>noticed that in training 32 batch_size was used, but model batch_size
          not listed in config file here, wondering what default batch size is and
          if batch_size config would impact the limit on the length of the input list.
          thanks.</p>

          '
        raw: noticed that in training 32 batch_size was used, but model batch_size
          not listed in config file here, wondering what default batch size is and
          if batch_size config would impact the limit on the length of the input list.
          thanks.
        updatedAt: '2022-08-09T17:48:41.899Z'
      numEdits: 0
      reactions: []
    id: 62f29df979132a7591ad3c78
    type: comment
  author: pyang
  content: noticed that in training 32 batch_size was used, but model batch_size not
    listed in config file here, wondering what default batch size is and if batch_size
    config would impact the limit on the length of the input list. thanks.
  created_at: 2022-08-09 16:48:41+00:00
  edited: false
  hidden: false
  id: 62f29df979132a7591ad3c78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
      fullname: Prajwal Kailas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: prajwal967
      type: user
    createdAt: '2022-08-09T18:51:21.000Z'
    data:
      edited: false
      editors:
      - prajwal967
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
          fullname: Prajwal Kailas
          isHf: false
          isPro: false
          name: prajwal967
          type: user
        html: '<p>The batch size will not impact the length of the input. The batch
          size is a HuggingFace parameter. For the forward pass, you can find the
          parameter "per_device_eval_batch_size" listed in the config file.<br>Hope
          that helps!</p>

          '
        raw: 'The batch size will not impact the length of the input. The batch size
          is a HuggingFace parameter. For the forward pass, you can find the parameter
          "per_device_eval_batch_size" listed in the config file.

          Hope that helps!'
        updatedAt: '2022-08-09T18:51:21.871Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pyang
    id: 62f2aca9cc711d96ce7190b0
    type: comment
  author: prajwal967
  content: 'The batch size will not impact the length of the input. The batch size
    is a HuggingFace parameter. For the forward pass, you can find the parameter "per_device_eval_batch_size"
    listed in the config file.

    Hope that helps!'
  created_at: 2022-08-09 17:51:21+00:00
  edited: false
  hidden: false
  id: 62f2aca9cc711d96ce7190b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
      fullname: Prajwal Kailas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: prajwal967
      type: user
    createdAt: '2022-08-15T19:17:16.000Z'
    data:
      status: closed
    id: 62fa9bbc8cd542e895ba1918
    type: status-change
  author: prajwal967
  created_at: 2022-08-15 18:17:16+00:00
  id: 62fa9bbc8cd542e895ba1918
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: obi/deid_roberta_i2b2
repo_type: model
status: closed
target_branch: null
title: input limit of model and rate limit of endpoint
