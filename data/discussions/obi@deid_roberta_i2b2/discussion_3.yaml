!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ntwm
conflicting_files: null
created_at: 2023-10-09 10:15:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11ea2d4b0e2f626aacd2f18dc93fc583.svg
      fullname: NATHAN DEBELLIS AMARAL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ntwm
      type: user
    createdAt: '2023-10-09T11:15:42.000Z'
    data:
      edited: false
      editors:
      - ntwm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5271743535995483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11ea2d4b0e2f626aacd2f18dc93fc583.svg
          fullname: NATHAN DEBELLIS AMARAL
          isHf: false
          isPro: false
          name: ntwm
          type: user
        html: '<p>Hello, I found this model to be very interesting. However, I am
          struggling to make it run locally when leveraging huggingface pipelines.
          I''m probably doing something wrong, but I feel like a hit a wall.</p>

          <p>Here is my code: </p>

          <p>from transformers import AutoTokenizer, AutoModelForMaskedLM<br>from
          transformers import pipeline</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("obi/deid_roberta_i2b2")<br>model
          = AutoModelForMaskedLM.from_pretrained("obi/deid_roberta_i2b2")</p>

          <p>nlp = pipeline(model=model, tokenizer=tokenizer)</p>

          <p>example = "My name is Wolfgang and I live in Berlin"</p>

          <p>ner_results = nlp(example)<br>print(ner_results)</p>

          <p>And here is the error stack I get: </p>

          <hr>

          <p>RuntimeError                              Traceback (most recent call
          last)<br>Cell In[22], line 2<br>      1 from transformers import pipeline<br>----&gt;
          2 nlp = pipeline(model=model, tokenizer=tokenizer)<br>      4 example =
          "My name is Wolfgang and I live in Berlin"<br>      6 ner_results = nlp(example)</p>

          <p>File ~/hug/lib/python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py:768,
          in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,
          framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code,
          model_kwargs, pipeline_class, **kwargs)<br>    766 if task is None and model
          is not None:<br>    767     if not isinstance(model, str):<br>--&gt; 768         raise
          RuntimeError(<br>    769             "Inferring the task automatically requires
          to check the hub with a model_id defined as a <code>str</code>."<br>    770             f"{model}
          is not a valid model_id."<br>    771         )<br>    772     task = get_task(model,
          use_auth_token)<br>    774 # Retrieve the task</p>

          <p>RuntimeError: Inferring the task automatically requires to check the
          hub with a model_id defined as a <code>str</code>.RobertaForMaskedLM(<br>  (roberta):
          RobertaModel(<br>    (embeddings): RobertaEmbeddings(<br>      (word_embeddings):
          Embedding(50265, 1024, padding_idx=1)<br>      (position_embeddings): Embedding(514,
          1024, padding_idx=1)<br>      (token_type_embeddings): Embedding(1, 1024)<br>      (LayerNorm):
          LayerNorm((1024,), eps=1e-05, elementwise_affine=True)<br>      (dropout):
          Dropout(p=0.1, inplace=False)<br>    )<br>    (encoder): RobertaEncoder(<br>      (layer):
          ModuleList(<br>        (0-23): 24 x RobertaLayer(<br>          (attention):
          RobertaAttention(<br>            (self): RobertaSelfAttention(<br>              (query):
          Linear(in_features=1024, out_features=1024, bias=True)<br>              (key):
          Linear(in_features=1024, out_features=1024, bias=True)<br>              (value):
          Linear(in_features=1024, out_features=1024, bias=True)<br>              (dropout):
          Dropout(p=0.1, inplace=False)<br>            )<br>            (output):
          RobertaSelfOutput(<br>              (dense): Linear(in_features=1024, out_features=1024,
          bias=True)<br>              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)<br>              (dropout):
          Dropout(p=0.1, inplace=False)<br>            )<br>          )<br>          (intermediate):
          RobertaIntermediate(<br>            (dense): Linear(in_features=1024, out_features=4096,
          bias=True)<br>            (intermediate_act_fn): GELUActivation()<br>          )<br>          (output):
          RobertaOutput(<br>            (dense): Linear(in_features=4096, out_features=1024,
          bias=True)<br>            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)<br>            (dropout):
          Dropout(p=0.1, inplace=False)<br>          )<br>        )<br>      )<br>    )<br>  )<br>  (lm_head):
          RobertaLMHead(<br>    (dense): Linear(in_features=1024, out_features=1024,
          bias=True)<br>    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)<br>    (decoder):
          Linear(in_features=1024, out_features=50265, bias=True)<br>  )<br>) is not
          a valid model_id.</p>

          '
        raw: "Hello, I found this model to be very interesting. However, I am struggling\
          \ to make it run locally when leveraging huggingface pipelines. I'm probably\
          \ doing something wrong, but I feel like a hit a wall.\r\n\r\nHere is my\
          \ code: \r\n\r\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\r\
          \nfrom transformers import pipeline\r\n  \r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          obi/deid_roberta_i2b2\")\r\nmodel = AutoModelForMaskedLM.from_pretrained(\"\
          obi/deid_roberta_i2b2\")\r\n\r\nnlp = pipeline(model=model, tokenizer=tokenizer)\r\
          \n\r\nexample = \"My name is Wolfgang and I live in Berlin\"\r\n\r\nner_results\
          \ = nlp(example)\r\nprint(ner_results)\r\n\r\nAnd here is the error stack\
          \ I get: \r\n\r\n---------------------------------------------------------------------------\r\
          \nRuntimeError                              Traceback (most recent call\
          \ last)\r\nCell In[22], line 2\r\n      1 from transformers import pipeline\r\
          \n----> 2 nlp = pipeline(model=model, tokenizer=tokenizer)\r\n      4 example\
          \ = \"My name is Wolfgang and I live in Berlin\"\r\n      6 ner_results\
          \ = nlp(example)\r\n\r\nFile ~/hug/lib/python3.10/site-packages/transformers/pipelines/__init__.py:768,\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
          \ framework, revision, use_fast, token, device, device_map, torch_dtype,\
          \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    766\
          \ if task is None and model is not None:\r\n    767     if not isinstance(model,\
          \ str):\r\n--> 768         raise RuntimeError(\r\n    769             \"\
          Inferring the task automatically requires to check the hub with a model_id\
          \ defined as a `str`.\"\r\n    770             f\"{model} is not a valid\
          \ model_id.\"\r\n    771         )\r\n    772     task = get_task(model,\
          \ use_auth_token)\r\n    774 # Retrieve the task\r\n\r\nRuntimeError: Inferring\
          \ the task automatically requires to check the hub with a model_id defined\
          \ as a `str`.RobertaForMaskedLM(\r\n  (roberta): RobertaModel(\r\n    (embeddings):\
          \ RobertaEmbeddings(\r\n      (word_embeddings): Embedding(50265, 1024,\
          \ padding_idx=1)\r\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\r\
          \n      (token_type_embeddings): Embedding(1, 1024)\r\n      (LayerNorm):\
          \ LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      (dropout):\
          \ Dropout(p=0.1, inplace=False)\r\n    )\r\n    (encoder): RobertaEncoder(\r\
          \n      (layer): ModuleList(\r\n        (0-23): 24 x RobertaLayer(\r\n \
          \         (attention): RobertaAttention(\r\n            (self): RobertaSelfAttention(\r\
          \n              (query): Linear(in_features=1024, out_features=1024, bias=True)\r\
          \n              (key): Linear(in_features=1024, out_features=1024, bias=True)\r\
          \n              (value): Linear(in_features=1024, out_features=1024, bias=True)\r\
          \n              (dropout): Dropout(p=0.1, inplace=False)\r\n           \
          \ )\r\n            (output): RobertaSelfOutput(\r\n              (dense):\
          \ Linear(in_features=1024, out_features=1024, bias=True)\r\n           \
          \   (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\
          \n              (dropout): Dropout(p=0.1, inplace=False)\r\n           \
          \ )\r\n          )\r\n          (intermediate): RobertaIntermediate(\r\n\
          \            (dense): Linear(in_features=1024, out_features=4096, bias=True)\r\
          \n            (intermediate_act_fn): GELUActivation()\r\n          )\r\n\
          \          (output): RobertaOutput(\r\n            (dense): Linear(in_features=4096,\
          \ out_features=1024, bias=True)\r\n            (LayerNorm): LayerNorm((1024,),\
          \ eps=1e-05, elementwise_affine=True)\r\n            (dropout): Dropout(p=0.1,\
          \ inplace=False)\r\n          )\r\n        )\r\n      )\r\n    )\r\n  )\r\
          \n  (lm_head): RobertaLMHead(\r\n    (dense): Linear(in_features=1024, out_features=1024,\
          \ bias=True)\r\n    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\
          \n    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\r\
          \n  )\r\n) is not a valid model_id."
        updatedAt: '2023-10-09T11:15:42.371Z'
      numEdits: 0
      reactions: []
    id: 6523e0de0b84de275c12bd71
    type: comment
  author: ntwm
  content: "Hello, I found this model to be very interesting. However, I am struggling\
    \ to make it run locally when leveraging huggingface pipelines. I'm probably doing\
    \ something wrong, but I feel like a hit a wall.\r\n\r\nHere is my code: \r\n\r\
    \nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\r\nfrom transformers\
    \ import pipeline\r\n  \r\ntokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\"\
    )\r\nmodel = AutoModelForMaskedLM.from_pretrained(\"obi/deid_roberta_i2b2\")\r\
    \n\r\nnlp = pipeline(model=model, tokenizer=tokenizer)\r\n\r\nexample = \"My name\
    \ is Wolfgang and I live in Berlin\"\r\n\r\nner_results = nlp(example)\r\nprint(ner_results)\r\
    \n\r\nAnd here is the error stack I get: \r\n\r\n---------------------------------------------------------------------------\r\
    \nRuntimeError                              Traceback (most recent call last)\r\
    \nCell In[22], line 2\r\n      1 from transformers import pipeline\r\n----> 2\
    \ nlp = pipeline(model=model, tokenizer=tokenizer)\r\n      4 example = \"My name\
    \ is Wolfgang and I live in Berlin\"\r\n      6 ner_results = nlp(example)\r\n\
    \r\nFile ~/hug/lib/python3.10/site-packages/transformers/pipelines/__init__.py:768,\
    \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
    \ framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code,\
    \ model_kwargs, pipeline_class, **kwargs)\r\n    766 if task is None and model\
    \ is not None:\r\n    767     if not isinstance(model, str):\r\n--> 768      \
    \   raise RuntimeError(\r\n    769             \"Inferring the task automatically\
    \ requires to check the hub with a model_id defined as a `str`.\"\r\n    770 \
    \            f\"{model} is not a valid model_id.\"\r\n    771         )\r\n  \
    \  772     task = get_task(model, use_auth_token)\r\n    774 # Retrieve the task\r\
    \n\r\nRuntimeError: Inferring the task automatically requires to check the hub\
    \ with a model_id defined as a `str`.RobertaForMaskedLM(\r\n  (roberta): RobertaModel(\r\
    \n    (embeddings): RobertaEmbeddings(\r\n      (word_embeddings): Embedding(50265,\
    \ 1024, padding_idx=1)\r\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\r\
    \n      (token_type_embeddings): Embedding(1, 1024)\r\n      (LayerNorm): LayerNorm((1024,),\
    \ eps=1e-05, elementwise_affine=True)\r\n      (dropout): Dropout(p=0.1, inplace=False)\r\
    \n    )\r\n    (encoder): RobertaEncoder(\r\n      (layer): ModuleList(\r\n  \
    \      (0-23): 24 x RobertaLayer(\r\n          (attention): RobertaAttention(\r\
    \n            (self): RobertaSelfAttention(\r\n              (query): Linear(in_features=1024,\
    \ out_features=1024, bias=True)\r\n              (key): Linear(in_features=1024,\
    \ out_features=1024, bias=True)\r\n              (value): Linear(in_features=1024,\
    \ out_features=1024, bias=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\
    \n            )\r\n            (output): RobertaSelfOutput(\r\n              (dense):\
    \ Linear(in_features=1024, out_features=1024, bias=True)\r\n              (LayerNorm):\
    \ LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n              (dropout):\
    \ Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (intermediate):\
    \ RobertaIntermediate(\r\n            (dense): Linear(in_features=1024, out_features=4096,\
    \ bias=True)\r\n            (intermediate_act_fn): GELUActivation()\r\n      \
    \    )\r\n          (output): RobertaOutput(\r\n            (dense): Linear(in_features=4096,\
    \ out_features=1024, bias=True)\r\n            (LayerNorm): LayerNorm((1024,),\
    \ eps=1e-05, elementwise_affine=True)\r\n            (dropout): Dropout(p=0.1,\
    \ inplace=False)\r\n          )\r\n        )\r\n      )\r\n    )\r\n  )\r\n  (lm_head):\
    \ RobertaLMHead(\r\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\r\
    \n    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n\
    \    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\r\n  )\r\
    \n) is not a valid model_id."
  created_at: 2023-10-09 10:15:42+00:00
  edited: false
  hidden: false
  id: 6523e0de0b84de275c12bd71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
      fullname: Prajwal Kailas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: prajwal967
      type: user
    createdAt: '2023-11-14T15:44:39.000Z'
    data:
      edited: false
      editors:
      - prajwal967
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8738287091255188
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
          fullname: Prajwal Kailas
          isHf: false
          isPro: false
          name: prajwal967
          type: user
        html: '<p>Could you try adding task=''token-classification'' or task=''ner''
          to the pipeline call and see if that works?</p>

          '
        raw: Could you try adding task='token-classification' or task='ner' to the
          pipeline call and see if that works?
        updatedAt: '2023-11-14T15:44:39.063Z'
      numEdits: 0
      reactions: []
    id: 655395e7f60d0d9678f0ff80
    type: comment
  author: prajwal967
  content: Could you try adding task='token-classification' or task='ner' to the pipeline
    call and see if that works?
  created_at: 2023-11-14 15:44:39+00:00
  edited: false
  hidden: false
  id: 655395e7f60d0d9678f0ff80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/706b857e8c0e60606ade26b144401c00.svg
      fullname: Chandra Sekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chandras002
      type: user
    createdAt: '2023-12-09T10:39:13.000Z'
    data:
      edited: true
      editors:
      - chandras002
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44773006439208984
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/706b857e8c0e60606ade26b144401c00.svg
          fullname: Chandra Sekar
          isHf: false
          isPro: false
          name: chandras002
          type: user
        html: '<p>This code works !</p>

          <p>from transformers import AutoTokenizer, AutoModelForMaskedLM<br>from
          transformers import pipeline<br>from presidio_analyzer import AnalyzerEngine<br>from
          presidio_anonymizer import AnonymizerEngine</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("obi/deid_roberta_i2b2")<br>model
          = AutoModelForMaskedLM.from_pretrained("obi/deid_roberta_i2b2")</p>

          <p>#nlp = pipeline(model=model,tokenizer=ner)</p>

          <p>#nlp = pipeline(task="ner", model=model,tokenizer=tokenizer)<br>nlp=pipeline(task="ner",
          model="obi/deid_roberta_i2b2")</p>

          <p>#nlp = pipeline(model=model)<br>#nlp = pipeline(model=model, tokenizer=tokenizer)</p>

          <p>example = "mobile number 9450 6413"</p>

          <p>ner_results = nlp(example)<br>print(ner_results)</p>

          '
        raw: "This code works !\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\
          from transformers import pipeline\nfrom presidio_analyzer import AnalyzerEngine\n\
          from presidio_anonymizer import AnonymizerEngine\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          obi/deid_roberta_i2b2\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"\
          obi/deid_roberta_i2b2\")\n\n\n#nlp = pipeline(model=model,tokenizer=ner)\n\
          \n#nlp = pipeline(task=\"ner\", model=model,tokenizer=tokenizer) \nnlp=pipeline(task=\"\
          ner\", model=\"obi/deid_roberta_i2b2\")\n \n#nlp = pipeline(model=model)\n\
          #nlp = pipeline(model=model, tokenizer=tokenizer)\n\nexample = \"mobile\
          \ number 9450 6413\"\n\nner_results = nlp(example)\nprint(ner_results)"
        updatedAt: '2023-12-09T15:48:18.564Z'
      numEdits: 1
      reactions: []
    id: 657443d1fc88f842e3ca83a5
    type: comment
  author: chandras002
  content: "This code works !\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\
    from transformers import pipeline\nfrom presidio_analyzer import AnalyzerEngine\n\
    from presidio_anonymizer import AnonymizerEngine\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    obi/deid_roberta_i2b2\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"obi/deid_roberta_i2b2\"\
    )\n\n\n#nlp = pipeline(model=model,tokenizer=ner)\n\n#nlp = pipeline(task=\"ner\"\
    , model=model,tokenizer=tokenizer) \nnlp=pipeline(task=\"ner\", model=\"obi/deid_roberta_i2b2\"\
    )\n \n#nlp = pipeline(model=model)\n#nlp = pipeline(model=model, tokenizer=tokenizer)\n\
    \nexample = \"mobile number 9450 6413\"\n\nner_results = nlp(example)\nprint(ner_results)"
  created_at: 2023-12-09 10:39:13+00:00
  edited: true
  hidden: false
  id: 657443d1fc88f842e3ca83a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/706b857e8c0e60606ade26b144401c00.svg
      fullname: Chandra Sekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chandras002
      type: user
    createdAt: '2023-12-09T10:40:36.000Z'
    data:
      edited: true
      editors:
      - chandras002
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8935651183128357
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/706b857e8c0e60606ade26b144401c00.svg
          fullname: Chandra Sekar
          isHf: false
          isPro: false
          name: chandras002
          type: user
        html: "<p>I am new to this domain.  The output for the above code is<br>[{'entity':\
          \ 'B-ID', 'score': 0.9996006, 'index': 3, 'word': '\u012094', 'start': 14,\
          \ 'end': 16}, {'entity': 'B-ID', 'score': 0.6739617, 'index': 4, 'word':\
          \ '50', 'start': 16, 'end': 18}, {'entity': 'L-ID', 'score': 0.9968354,\
          \ 'index': 5, 'word': '\u012064', 'start': 19, 'end': 21}, {'entity': 'L-ID',\
          \ 'score': 0.5859114, 'index': 6, 'word': '13', 'start': 21, 'end': 23}]</p>\n\
          <p>But i want to detect the mobile number as like the following ----&gt;\
          \   My mobile  phone number <br>PLEASE advise...how to do any thoughts/Questions\
          \ useful. Thanks!</p>\n"
        raw: "I am new to this domain.  The output for the above code is \n[{'entity':\
          \ 'B-ID', 'score': 0.9996006, 'index': 3, 'word': '\u012094', 'start': 14,\
          \ 'end': 16}, {'entity': 'B-ID', 'score': 0.6739617, 'index': 4, 'word':\
          \ '50', 'start': 16, 'end': 18}, {'entity': 'L-ID', 'score': 0.9968354,\
          \ 'index': 5, 'word': '\u012064', 'start': 19, 'end': 21}, {'entity': 'L-ID',\
          \ 'score': 0.5859114, 'index': 6, 'word': '13', 'start': 21, 'end': 23}]\n\
          \nBut i want to detect the mobile number as like the following ---->   My\
          \ mobile  phone number <ID><ID>\nPLEASE advise...how to do any thoughts/Questions\
          \ useful. Thanks!"
        updatedAt: '2023-12-09T15:51:15.390Z'
      numEdits: 2
      reactions: []
    id: 657444245df6f1fec18a6acb
    type: comment
  author: chandras002
  content: "I am new to this domain.  The output for the above code is \n[{'entity':\
    \ 'B-ID', 'score': 0.9996006, 'index': 3, 'word': '\u012094', 'start': 14, 'end':\
    \ 16}, {'entity': 'B-ID', 'score': 0.6739617, 'index': 4, 'word': '50', 'start':\
    \ 16, 'end': 18}, {'entity': 'L-ID', 'score': 0.9968354, 'index': 5, 'word': '\u0120\
    64', 'start': 19, 'end': 21}, {'entity': 'L-ID', 'score': 0.5859114, 'index':\
    \ 6, 'word': '13', 'start': 21, 'end': 23}]\n\nBut i want to detect the mobile\
    \ number as like the following ---->   My mobile  phone number <ID><ID>\nPLEASE\
    \ advise...how to do any thoughts/Questions useful. Thanks!"
  created_at: 2023-12-09 10:40:36+00:00
  edited: true
  hidden: false
  id: 657444245df6f1fec18a6acb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
      fullname: Prajwal Kailas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: prajwal967
      type: user
    createdAt: '2023-12-20T10:08:28.000Z'
    data:
      edited: false
      editors:
      - prajwal967
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9192052483558655
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a08074e7b05d933c65c5404b3ee3c2a.svg
          fullname: Prajwal Kailas
          isHf: false
          isPro: false
          name: prajwal967
          type: user
        html: '<p>We have a custom tokenization process - that doesn''t get used when
          youo use pipelines. - it''s currently seeing the phone number and predicting
          it as ID entity.<br>You can try referring to the github page and follow
          the steps here to run the forward pass: <a rel="nofollow" href="https://github.com/obi-ml-public/ehr_deidentification/blob/main/steps/forward_pass/Forward%20Pass.ipynb">https://github.com/obi-ml-public/ehr_deidentification/blob/main/steps/forward_pass/Forward%20Pass.ipynb</a><br>This
          might be a little more complicated than using pipelines but you can try
          and see if this works for your use case.</p>

          '
        raw: 'We have a custom tokenization process - that doesn''t get used when
          youo use pipelines. - it''s currently seeing the phone number and predicting
          it as ID entity.

          You can try referring to the github page and follow the steps here to run
          the forward pass: https://github.com/obi-ml-public/ehr_deidentification/blob/main/steps/forward_pass/Forward%20Pass.ipynb

          This might be a little more complicated than using pipelines but you can
          try and see if this works for your use case.'
        updatedAt: '2023-12-20T10:08:28.941Z'
      numEdits: 0
      reactions: []
    id: 6582bd1c3fc227c16d785fd3
    type: comment
  author: prajwal967
  content: 'We have a custom tokenization process - that doesn''t get used when youo
    use pipelines. - it''s currently seeing the phone number and predicting it as
    ID entity.

    You can try referring to the github page and follow the steps here to run the
    forward pass: https://github.com/obi-ml-public/ehr_deidentification/blob/main/steps/forward_pass/Forward%20Pass.ipynb

    This might be a little more complicated than using pipelines but you can try and
    see if this works for your use case.'
  created_at: 2023-12-20 10:08:28+00:00
  edited: false
  hidden: false
  id: 6582bd1c3fc227c16d785fd3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: obi/deid_roberta_i2b2
repo_type: model
status: open
target_branch: null
title: Cannot run model with pipelines
