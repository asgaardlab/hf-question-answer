!!python/object:huggingface_hub.community.DiscussionWithDetails
author: szbigcat
conflicting_files: null
created_at: 2023-08-06 17:06:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6463539fc615cbc12446b65d/At-weRzpELN7LCGgvFXXK.png?w=200&h=200&f=face
      fullname: Chris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: szbigcat
      type: user
    createdAt: '2023-08-06T18:06:19.000Z'
    data:
      edited: false
      editors:
      - szbigcat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9367809295654297
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6463539fc615cbc12446b65d/At-weRzpELN7LCGgvFXXK.png?w=200&h=200&f=face
          fullname: Chris
          isHf: false
          isPro: false
          name: szbigcat
          type: user
        html: '<p>Hi, I have learned how to convert the original format to hf format
          from your document and converted the 13b model successfully. But I can not
          load the hf model into GPU since it takes over than 39G VRAM. So I wonder
          quantise the model may solve my problem but I dont know how to do it. </p>

          '
        raw: 'Hi, I have learned how to convert the original format to hf format from
          your document and converted the 13b model successfully. But I can not load
          the hf model into GPU since it takes over than 39G VRAM. So I wonder quantise
          the model may solve my problem but I dont know how to do it. '
        updatedAt: '2023-08-06T18:06:19.809Z'
      numEdits: 0
      reactions: []
    id: 64cfe11bd2a781d3f00ed4e6
    type: comment
  author: szbigcat
  content: 'Hi, I have learned how to convert the original format to hf format from
    your document and converted the 13b model successfully. But I can not load the
    hf model into GPU since it takes over than 39G VRAM. So I wonder quantise the
    model may solve my problem but I dont know how to do it. '
  created_at: 2023-08-06 17:06:19+00:00
  edited: false
  hidden: false
  id: 64cfe11bd2a781d3f00ed4e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg
      fullname: Junlin Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jlzhou
      type: user
    createdAt: '2023-08-07T02:28:30.000Z'
    data:
      edited: false
      editors:
      - jlzhou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8351348638534546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg
          fullname: Junlin Zhou
          isHf: false
          isPro: false
          name: jlzhou
          type: user
        html: '<p>Performing quantization also requires loading the model into VRAM,
          instead you could try some already-quantized model.</p>

          <p>For example <a href="https://huggingface.co/TheBloke/Llama-2-70B-GPTQ">this
          gptq one</a></p>

          '
        raw: 'Performing quantization also requires loading the model into VRAM, instead
          you could try some already-quantized model.


          For example [this gptq one](https://huggingface.co/TheBloke/Llama-2-70B-GPTQ)

          '
        updatedAt: '2023-08-07T02:28:30.869Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - szbigcat
    id: 64d056ce749587dbe033aee1
    type: comment
  author: jlzhou
  content: 'Performing quantization also requires loading the model into VRAM, instead
    you could try some already-quantized model.


    For example [this gptq one](https://huggingface.co/TheBloke/Llama-2-70B-GPTQ)

    '
  created_at: 2023-08-07 01:28:30+00:00
  edited: false
  hidden: false
  id: 64d056ce749587dbe033aee1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6463539fc615cbc12446b65d/At-weRzpELN7LCGgvFXXK.png?w=200&h=200&f=face
      fullname: Chris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: szbigcat
      type: user
    createdAt: '2023-08-07T04:19:39.000Z'
    data:
      edited: true
      editors:
      - szbigcat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9429157376289368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6463539fc615cbc12446b65d/At-weRzpELN7LCGgvFXXK.png?w=200&h=200&f=face
          fullname: Chris
          isHf: false
          isPro: false
          name: szbigcat
          type: user
        html: "<blockquote>\n<p>Performing quantization also requires loading the\
          \ model into VRAM, instead you could try some already-quantized model.</p>\n\
          <p>For example <a href=\"https://huggingface.co/TheBloke/Llama-2-70B-GPTQ\"\
          >this gptq one</a></p>\n</blockquote>\n<p>Thank you. I realize I have to\
          \ give up doing it on my own PC now. But I still want to know how to quantize\
          \ a model, maybe I will try it on a smaller model like the LLama-2 7B model\
          \ later.</p>\n<p>Is that the code below is the answer to my question?</p>\n\
          <pre><code class=\"language-pytorch\"># Apply quantization\nquantized_model\
          \ = torch.quantization.quantize_dynamic(\n    model_fp32, {torch.nn.Linear},\
          \ dtype=torch.qint8, inplace=False\n)\n\n# Convert the quantized model to\
          \ FP16 (half-precision) 4-bit\nquantized_model = quantized_model.to(torch.float16)\n\
          \n# Save the quantized model\ntorch.save(quantized_model.state_dict(), \"\
          fp16_4bit_model.pth\")\n</code></pre>\n"
        raw: "> Performing quantization also requires loading the model into VRAM,\
          \ instead you could try some already-quantized model.\n> \n> For example\
          \ [this gptq one](https://huggingface.co/TheBloke/Llama-2-70B-GPTQ)\n\n\
          Thank you. I realize I have to give up doing it on my own PC now. But I\
          \ still want to know how to quantize a model, maybe I will try it on a smaller\
          \ model like the LLama-2 7B model later.\n\nIs that the code below is the\
          \ answer to my question?\n\n```pytorch\n# Apply quantization\nquantized_model\
          \ = torch.quantization.quantize_dynamic(\n    model_fp32, {torch.nn.Linear},\
          \ dtype=torch.qint8, inplace=False\n)\n\n# Convert the quantized model to\
          \ FP16 (half-precision) 4-bit\nquantized_model = quantized_model.to(torch.float16)\n\
          \n# Save the quantized model\ntorch.save(quantized_model.state_dict(), \"\
          fp16_4bit_model.pth\")\n```"
        updatedAt: '2023-08-07T06:26:38.194Z'
      numEdits: 1
      reactions: []
    id: 64d070db0b71aea8be7a2834
    type: comment
  author: szbigcat
  content: "> Performing quantization also requires loading the model into VRAM, instead\
    \ you could try some already-quantized model.\n> \n> For example [this gptq one](https://huggingface.co/TheBloke/Llama-2-70B-GPTQ)\n\
    \nThank you. I realize I have to give up doing it on my own PC now. But I still\
    \ want to know how to quantize a model, maybe I will try it on a smaller model\
    \ like the LLama-2 7B model later.\n\nIs that the code below is the answer to\
    \ my question?\n\n```pytorch\n# Apply quantization\nquantized_model = torch.quantization.quantize_dynamic(\n\
    \    model_fp32, {torch.nn.Linear}, dtype=torch.qint8, inplace=False\n)\n\n# Convert\
    \ the quantized model to FP16 (half-precision) 4-bit\nquantized_model = quantized_model.to(torch.float16)\n\
    \n# Save the quantized model\ntorch.save(quantized_model.state_dict(), \"fp16_4bit_model.pth\"\
    )\n```"
  created_at: 2023-08-07 03:19:39+00:00
  edited: true
  hidden: false
  id: 64d070db0b71aea8be7a2834
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-70B-fp16
repo_type: model
status: open
target_branch: null
title: How to quantise the model?
