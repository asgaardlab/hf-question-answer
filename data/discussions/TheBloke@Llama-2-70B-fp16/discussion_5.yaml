!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zuhashaik
conflicting_files: null
created_at: 2023-10-30 17:50:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
      fullname: zuhair hasan shaik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuhashaik
      type: user
    createdAt: '2023-10-30T18:50:49.000Z'
    data:
      edited: false
      editors:
      - zuhashaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.927299976348877
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
          fullname: zuhair hasan shaik
          isHf: false
          isPro: false
          name: zuhashaik
          type: user
        html: '<p>As the open-source Llama-2-70b model gains popularity within the
          community, questions arise about its performance on longer token sequences,
          potentially exceeding 2500 tokens. In my case, it seems to struggle after
          500 tokens. Specifically, I''m referring to the Llama-2-70b model.</p>

          <p>May be after finetuning with the data (simulated with gpt-3.5 or 4).what
          does the community thinks about working on 2500 tokens and higher or any
          suggetions on some other models? </p>

          '
        raw: "As the open-source Llama-2-70b model gains popularity within the community,\
          \ questions arise about its performance on longer token sequences, potentially\
          \ exceeding 2500 tokens. In my case, it seems to struggle after 500 tokens.\
          \ Specifically, I'm referring to the Llama-2-70b model.\r\n\r\nMay be after\
          \ finetuning with the data (simulated with gpt-3.5 or 4).what does the community\
          \ thinks about working on 2500 tokens and higher or any suggetions on some\
          \ other models? "
        updatedAt: '2023-10-30T18:50:49.276Z'
      numEdits: 0
      reactions: []
    id: 653ffb09cdc9c22e359d0909
    type: comment
  author: zuhashaik
  content: "As the open-source Llama-2-70b model gains popularity within the community,\
    \ questions arise about its performance on longer token sequences, potentially\
    \ exceeding 2500 tokens. In my case, it seems to struggle after 500 tokens. Specifically,\
    \ I'm referring to the Llama-2-70b model.\r\n\r\nMay be after finetuning with\
    \ the data (simulated with gpt-3.5 or 4).what does the community thinks about\
    \ working on 2500 tokens and higher or any suggetions on some other models? "
  created_at: 2023-10-30 17:50:49+00:00
  edited: false
  hidden: false
  id: 653ffb09cdc9c22e359d0909
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Llama-2-70B-fp16
repo_type: model
status: open
target_branch: null
title: 'Llama-2-70b Model: Challenges with Long Token Sequences'
