!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eramax
conflicting_files: null
created_at: 2023-12-16 10:05:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-16T10:05:34.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8662570714950562
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>I tried to run it on colab T4 and it seems there is something wrong\
          \ in the model outputs</p>\n<pre><code>!CUDA_VISIBLE_DEVICES=0 python interactive_gen.py\
          \ --hf_path /content/model --no_use_flash_attn\n</code></pre>\n<pre><code>I1216\
          \ 09:57:57.827290 5316 utils.py:160] NumExpr defaulting to 2 threads.\n\
          I1216 09:58:09.321581 5316 modeling.py:799] We will use 90% of the memory\
          \ on device 0 for storing the model, and 10% for the buffer to avoid OOM.\
          \ You can set `max_memory` in to a higher value to use more memory (at your\
          \ own risk).\nSpecial tokens have been added in the vocabulary, make sure\
          \ the associated word embeddings are fine-tuned or trained.\n\nPlease enter\
          \ your prompt or 'quit' (without quotes) to quit: explain quick sort\nSetting\
          \ `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\nModel Output:\
          \  explain quick sort\nexplain quick sort - 6.0 out of 5 based on 10 reviews\n\
          \nPlease enter your prompt or 'quit' (without quotes) to quit: what is huggingface\n\
          Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\n\
          Model Output:  what is huggingface\nHugging Face is a nonprofit 501(c)(3)\
          \ corporation. We have no employees, and all work is done by volunteers.\
          \ Our mission is to help children who are victims of war, poverty, natural\
          \ disasters,\n\nPlease enter your prompt or 'quit' (without quotes) to quit:\
          \ give me python quick sort example\nSetting `pad_token_id` to `eos_token_id`:2\
          \ for open-end generation.\n\nModel Output:  give me python quick sort example\n\
          Asked by Anonymous a on January 15, 2018 Verified by bdehara\n\nPlease enter\
          \ your prompt or 'quit' (without quotes) to quit: Write a python script\
          \ to output numbers 1 to 53 with step = 3 and then the should run the script\n\
          Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\n\
          Model Output:  Write a python script to output numbers 1 to 53 with step\
          \ = 3 and then the should run the script.\nThis is a very simple problem.\
          \ I'm sure there are a lot of ways to solve it. Here's one way.\n# Write\
          \ a python script to output numbers\n\nPlease enter your prompt or 'quit'\
          \ (without quotes) to quit: \n</code></pre>\n"
        raw: "I tried to run it on colab T4 and it seems there is something wrong\
          \ in the model outputs\r\n```\r\n!CUDA_VISIBLE_DEVICES=0 python interactive_gen.py\
          \ --hf_path /content/model --no_use_flash_attn\r\n```\r\n```\r\nI1216 09:57:57.827290\
          \ 5316 utils.py:160] NumExpr defaulting to 2 threads.\r\nI1216 09:58:09.321581\
          \ 5316 modeling.py:799] We will use 90% of the memory on device 0 for storing\
          \ the model, and 10% for the buffer to avoid OOM. You can set `max_memory`\
          \ in to a higher value to use more memory (at your own risk).\r\nSpecial\
          \ tokens have been added in the vocabulary, make sure the associated word\
          \ embeddings are fine-tuned or trained.\r\n\r\nPlease enter your prompt\
          \ or 'quit' (without quotes) to quit: explain quick sort\r\nSetting `pad_token_id`\
          \ to `eos_token_id`:2 for open-end generation.\r\n\r\nModel Output:  explain\
          \ quick sort\r\nexplain quick sort - 6.0 out of 5 based on 10 reviews\r\n\
          \r\nPlease enter your prompt or 'quit' (without quotes) to quit: what is\
          \ huggingface\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end\
          \ generation.\r\n\r\nModel Output:  what is huggingface\r\nHugging Face\
          \ is a nonprofit 501(c)(3) corporation. We have no employees, and all work\
          \ is done by volunteers. Our mission is to help children who are victims\
          \ of war, poverty, natural disasters,\r\n\r\nPlease enter your prompt or\
          \ 'quit' (without quotes) to quit: give me python quick sort example\r\n\
          Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\n\r\
          \nModel Output:  give me python quick sort example\r\nAsked by Anonymous\
          \ a on January 15, 2018 Verified by bdehara\r\n\r\nPlease enter your prompt\
          \ or 'quit' (without quotes) to quit: Write a python script to output numbers\
          \ 1 to 53 with step = 3 and then the should run the script\r\nSetting `pad_token_id`\
          \ to `eos_token_id`:2 for open-end generation.\r\n\r\nModel Output:  Write\
          \ a python script to output numbers 1 to 53 with step = 3 and then the should\
          \ run the script.\r\nThis is a very simple problem. I'm sure there are a\
          \ lot of ways to solve it. Here's one way.\r\n# Write a python script to\
          \ output numbers\r\n\r\nPlease enter your prompt or 'quit' (without quotes)\
          \ to quit: \r\n\r\n```"
        updatedAt: '2023-12-16T10:05:34.710Z'
      numEdits: 0
      reactions: []
    id: 657d766e1e208703241531f3
    type: comment
  author: eramax
  content: "I tried to run it on colab T4 and it seems there is something wrong in\
    \ the model outputs\r\n```\r\n!CUDA_VISIBLE_DEVICES=0 python interactive_gen.py\
    \ --hf_path /content/model --no_use_flash_attn\r\n```\r\n```\r\nI1216 09:57:57.827290\
    \ 5316 utils.py:160] NumExpr defaulting to 2 threads.\r\nI1216 09:58:09.321581\
    \ 5316 modeling.py:799] We will use 90% of the memory on device 0 for storing\
    \ the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in\
    \ to a higher value to use more memory (at your own risk).\r\nSpecial tokens have\
    \ been added in the vocabulary, make sure the associated word embeddings are fine-tuned\
    \ or trained.\r\n\r\nPlease enter your prompt or 'quit' (without quotes) to quit:\
    \ explain quick sort\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end\
    \ generation.\r\n\r\nModel Output:  explain quick sort\r\nexplain quick sort -\
    \ 6.0 out of 5 based on 10 reviews\r\n\r\nPlease enter your prompt or 'quit' (without\
    \ quotes) to quit: what is huggingface\r\nSetting `pad_token_id` to `eos_token_id`:2\
    \ for open-end generation.\r\n\r\nModel Output:  what is huggingface\r\nHugging\
    \ Face is a nonprofit 501(c)(3) corporation. We have no employees, and all work\
    \ is done by volunteers. Our mission is to help children who are victims of war,\
    \ poverty, natural disasters,\r\n\r\nPlease enter your prompt or 'quit' (without\
    \ quotes) to quit: give me python quick sort example\r\nSetting `pad_token_id`\
    \ to `eos_token_id`:2 for open-end generation.\r\n\r\nModel Output:  give me python\
    \ quick sort example\r\nAsked by Anonymous a on January 15, 2018 Verified by bdehara\r\
    \n\r\nPlease enter your prompt or 'quit' (without quotes) to quit: Write a python\
    \ script to output numbers 1 to 53 with step = 3 and then the should run the script\r\
    \nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\n\r\n\
    Model Output:  Write a python script to output numbers 1 to 53 with step = 3 and\
    \ then the should run the script.\r\nThis is a very simple problem. I'm sure there\
    \ are a lot of ways to solve it. Here's one way.\r\n# Write a python script to\
    \ output numbers\r\n\r\nPlease enter your prompt or 'quit' (without quotes) to\
    \ quit: \r\n\r\n```"
  created_at: 2023-12-16 10:05:34+00:00
  edited: false
  hidden: false
  id: 657d766e1e208703241531f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
      fullname: Albert
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: at676
      type: user
    createdAt: '2023-12-18T04:42:12.000Z'
    data:
      edited: false
      editors:
      - at676
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9747907519340515
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
          fullname: Albert
          isHf: false
          isPro: false
          name: at676
          type: user
        html: '<p>What seems wrong about the output? You''re running the non instruction
          tuned version, which isn''t very good at following instruction prompts.
          There is also a max_length flag in <code>interactive_gen.py</code> which
          should let you get more output from the models.</p>

          '
        raw: What seems wrong about the output? You're running the non instruction
          tuned version, which isn't very good at following instruction prompts. There
          is also a max_length flag in `interactive_gen.py` which should let you get
          more output from the models.
        updatedAt: '2023-12-18T04:42:12.564Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eramax
    id: 657fcda4f010d76b6e6e3dcc
    type: comment
  author: at676
  content: What seems wrong about the output? You're running the non instruction tuned
    version, which isn't very good at following instruction prompts. There is also
    a max_length flag in `interactive_gen.py` which should let you get more output
    from the models.
  created_at: 2023-12-18 04:42:12+00:00
  edited: false
  hidden: false
  id: 657fcda4f010d76b6e6e3dcc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/558b1b08d70fa897b25fe1b9dd719a01.svg
      fullname: jerry chee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jc3464
      type: user
    createdAt: '2023-12-18T16:21:52.000Z'
    data:
      edited: false
      editors:
      - jc3464
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.910133957862854
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/558b1b08d70fa897b25fe1b9dd719a01.svg
          fullname: jerry chee
          isHf: false
          isPro: false
          name: jc3464
          type: user
        html: '<p>Just curious, can you share the prompts that you typically use to
          evaluate a model? </p>

          '
        raw: 'Just curious, can you share the prompts that you typically use to evaluate
          a model? '
        updatedAt: '2023-12-18T16:21:52.584Z'
      numEdits: 0
      reactions: []
    id: 658071a00d738799eca3b322
    type: comment
  author: jc3464
  content: 'Just curious, can you share the prompts that you typically use to evaluate
    a model? '
  created_at: 2023-12-18 16:21:52+00:00
  edited: false
  hidden: false
  id: 658071a00d738799eca3b322
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-18T22:28:48.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9546438455581665
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: '<p>Thank you everyone, and kindly review my attempt to use quip to
          run a different model: <a rel="nofollow" href="https://gist.github.com/eramax/64d59644d600b3db3fcb6acf0133721b">https://gist.github.com/eramax/64d59644d600b3db3fcb6acf0133721b</a><br>I
          haven''t used a prompt template and I''m not sure how to set one for the
          script <code>interactive_gen.py</code>.</p>

          <p>My experience with the <code>relaxml/Llama-2-13b-chat-E8P-2Bit} instruction-tuned
          model wasn''t great; perhaps because I hadn''t applied a prompt template,
          the model didn''t respond well to my questions. Since there are many packages
          in the </code>requirements.txt` file, I discovered that I don''t need to
          install them all. I simply installed the ones that were stated, and I was
          able to function.</p>

          <p>Best regards,</p>

          '
        raw: 'Thank you everyone, and kindly review my attempt to use quip to run
          a different model: https://gist.github.com/eramax/64d59644d600b3db3fcb6acf0133721b

          I haven''t used a prompt template and I''m not sure how to set one for the
          script `interactive_gen.py`.


          My experience with the `relaxml/Llama-2-13b-chat-E8P-2Bit} instruction-tuned
          model wasn''t great; perhaps because I hadn''t applied a prompt template,
          the model didn''t respond well to my questions.

          Since there are many packages in the `requirements.txt` file, I discovered
          that I don''t need to install them all. I simply installed the ones that
          were stated, and I was able to function.


          Best regards,'
        updatedAt: '2023-12-18T22:28:48.102Z'
      numEdits: 0
      reactions: []
    id: 6580c7a07b6157482d99033c
    type: comment
  author: eramax
  content: 'Thank you everyone, and kindly review my attempt to use quip to run a
    different model: https://gist.github.com/eramax/64d59644d600b3db3fcb6acf0133721b

    I haven''t used a prompt template and I''m not sure how to set one for the script
    `interactive_gen.py`.


    My experience with the `relaxml/Llama-2-13b-chat-E8P-2Bit} instruction-tuned model
    wasn''t great; perhaps because I hadn''t applied a prompt template, the model
    didn''t respond well to my questions.

    Since there are many packages in the `requirements.txt` file, I discovered that
    I don''t need to install them all. I simply installed the ones that were stated,
    and I was able to function.


    Best regards,'
  created_at: 2023-12-18 22:28:48+00:00
  edited: false
  hidden: false
  id: 6580c7a07b6157482d99033c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
      fullname: Albert
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: at676
      type: user
    createdAt: '2023-12-19T21:03:36.000Z'
    data:
      edited: false
      editors:
      - at676
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960064709186554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
          fullname: Albert
          isHf: false
          isPro: false
          name: at676
          type: user
        html: '<p>interactive_gen.py is just a simple script that calls hf.generate()
          internally. If you want to set up prompt templates, you''ll have to work
          that in somehow. I personally have not used a prompt template before so
          I can''t really help with that. I think the quality of output you''re seeing
          is in line with what you should expect when entering those prompts. There
          is still some degradation in text generation quality when going from fp16
          to 2 bit (remember, this model is 8x smaller than the original model!).
          Openhermes seems to be a better "chat" model and is smaller. You can also
          try using a 4 bit model if you want higher quality generation.</p>

          '
        raw: interactive_gen.py is just a simple script that calls hf.generate() internally.
          If you want to set up prompt templates, you'll have to work that in somehow.
          I personally have not used a prompt template before so I can't really help
          with that. I think the quality of output you're seeing is in line with what
          you should expect when entering those prompts. There is still some degradation
          in text generation quality when going from fp16 to 2 bit (remember, this
          model is 8x smaller than the original model!). Openhermes seems to be a
          better "chat" model and is smaller. You can also try using a 4 bit model
          if you want higher quality generation.
        updatedAt: '2023-12-19T21:03:36.420Z'
      numEdits: 0
      reactions: []
    id: 6582052818be848c0cce0f9d
    type: comment
  author: at676
  content: interactive_gen.py is just a simple script that calls hf.generate() internally.
    If you want to set up prompt templates, you'll have to work that in somehow. I
    personally have not used a prompt template before so I can't really help with
    that. I think the quality of output you're seeing is in line with what you should
    expect when entering those prompts. There is still some degradation in text generation
    quality when going from fp16 to 2 bit (remember, this model is 8x smaller than
    the original model!). Openhermes seems to be a better "chat" model and is smaller.
    You can also try using a 4 bit model if you want higher quality generation.
  created_at: 2023-12-19 21:03:36+00:00
  edited: false
  hidden: false
  id: 6582052818be848c0cce0f9d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: relaxml/Llama-1-30b-E8P-2Bit
repo_type: model
status: open
target_branch: null
title: outputs are wrong
