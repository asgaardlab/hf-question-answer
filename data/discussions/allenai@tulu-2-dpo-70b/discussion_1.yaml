!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Panchovix
conflicting_files: null
created_at: 2023-11-21 03:52:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-11-21T03:52:51.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.828803300857544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>Hi there, noticed on the config.json</p>

          <p><code>"max_position_embeddings": 8192</code></p>

          <p>But when trying to do inference at context above 4K, I get gibberish.
          Are we supposed to use a rope value different than 1?</p>

          <p>Thanks!</p>

          '
        raw: "Hi there, noticed on the config.json\r\n\r\n`\"max_position_embeddings\"\
          : 8192`\r\n\r\nBut when trying to do inference at context above 4K, I get\
          \ gibberish. Are we supposed to use a rope value different than 1?\r\n\r\
          \nThanks!"
        updatedAt: '2023-11-21T03:52:51.864Z'
      numEdits: 0
      reactions: []
    id: 655c29930c27b3fb70830ef8
    type: comment
  author: Panchovix
  content: "Hi there, noticed on the config.json\r\n\r\n`\"max_position_embeddings\"\
    : 8192`\r\n\r\nBut when trying to do inference at context above 4K, I get gibberish.\
    \ Are we supposed to use a rope value different than 1?\r\n\r\nThanks!"
  created_at: 2023-11-21 03:52:51+00:00
  edited: false
  hidden: false
  id: 655c29930c27b3fb70830ef8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png?w=200&h=200&f=face
      fullname: Hamish Ivison
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hamishivi
      type: user
    createdAt: '2023-11-21T06:30:09.000Z'
    data:
      edited: false
      editors:
      - hamishivi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.986215353012085
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png?w=200&h=200&f=face
          fullname: Hamish Ivison
          isHf: false
          isPro: false
          name: hamishivi
          type: user
        html: '<p>Hi, we definitely trained with a length of 8192, although I think
          most of the data was &lt;= 4k in terms of length, so this might be what''s
          going on. Additionally, the DPO training data didn''t go over 6k tokens.
          So probably some extra work is required to get these models to work well
          with such long contexts - we didn''t really test long context settings much.</p>

          '
        raw: Hi, we definitely trained with a length of 8192, although I think most
          of the data was <= 4k in terms of length, so this might be what's going
          on. Additionally, the DPO training data didn't go over 6k tokens. So probably
          some extra work is required to get these models to work well with such long
          contexts - we didn't really test long context settings much.
        updatedAt: '2023-11-21T06:30:09.110Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - Panchovix
        - radm
        - PrimeD
        - julien-c
    id: 655c4e71cfe086853e4086dd
    type: comment
  author: hamishivi
  content: Hi, we definitely trained with a length of 8192, although I think most
    of the data was <= 4k in terms of length, so this might be what's going on. Additionally,
    the DPO training data didn't go over 6k tokens. So probably some extra work is
    required to get these models to work well with such long contexts - we didn't
    really test long context settings much.
  created_at: 2023-11-21 06:30:09+00:00
  edited: false
  hidden: false
  id: 655c4e71cfe086853e4086dd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: allenai/tulu-2-dpo-70b
repo_type: model
status: open
target_branch: null
title: 'Nice work! The config.json seems to have "max_position_embeddings": 8192,
  but it doesn''t work fine after 4096'
