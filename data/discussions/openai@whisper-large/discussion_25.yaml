!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eashanchawla
conflicting_files: null
created_at: 2023-02-03 21:54:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07639710a80fdbee1897ec279666e53f.svg
      fullname: Eashan Chawla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eashanchawla
      type: user
    createdAt: '2023-02-03T21:54:40.000Z'
    data:
      edited: false
      editors:
      - eashanchawla
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07639710a80fdbee1897ec279666e53f.svg
          fullname: Eashan Chawla
          isHf: false
          isPro: false
          name: eashanchawla
          type: user
        html: '<p>I am downloading the whisper-large model and caching it on the root
          level of my directory by doing:<br>               model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large")<br>               processor
          = WhisperProcessor.from_pretrained("openai/whisper-large")</p>

          <p>My question is, how do I figure out what URL this downloads the model
          from? I see that there''s the original openai model links in convert_openai_to_hf.py
          (transformers/models/whisper in the github repo for transformers). Is the
          model being downloaded from there or is it being downloaded from hugging
          face hub (because I also see a convert function to save a pytorch model
          object at a specific location)? </p>

          <p>This is needed so I can add this URL to an accept list, allowing my flask
          app to access and download from it. </p>

          '
        raw: "I am downloading the whisper-large model and caching it on the root\
          \ level of my directory by doing:\r\n               model = WhisperForConditionalGeneration.from_pretrained(\"\
          openai/whisper-large\")\r\n               processor = WhisperProcessor.from_pretrained(\"\
          openai/whisper-large\")\r\n\r\nMy question is, how do I figure out what\
          \ URL this downloads the model from? I see that there's the original openai\
          \ model links in convert_openai_to_hf.py (transformers/models/whisper in\
          \ the github repo for transformers). Is the model being downloaded from\
          \ there or is it being downloaded from hugging face hub (because I also\
          \ see a convert function to save a pytorch model object at a specific location)?\
          \ \r\n\r\nThis is needed so I can add this URL to an accept list, allowing\
          \ my flask app to access and download from it. \r\n\r\n"
        updatedAt: '2023-02-03T21:54:40.261Z'
      numEdits: 0
      reactions: []
    id: 63dd82a00f6d2d6c3efbb508
    type: comment
  author: eashanchawla
  content: "I am downloading the whisper-large model and caching it on the root level\
    \ of my directory by doing:\r\n               model = WhisperForConditionalGeneration.from_pretrained(\"\
    openai/whisper-large\")\r\n               processor = WhisperProcessor.from_pretrained(\"\
    openai/whisper-large\")\r\n\r\nMy question is, how do I figure out what URL this\
    \ downloads the model from? I see that there's the original openai model links\
    \ in convert_openai_to_hf.py (transformers/models/whisper in the github repo for\
    \ transformers). Is the model being downloaded from there or is it being downloaded\
    \ from hugging face hub (because I also see a convert function to save a pytorch\
    \ model object at a specific location)? \r\n\r\nThis is needed so I can add this\
    \ URL to an accept list, allowing my flask app to access and download from it.\
    \ \r\n\r\n"
  created_at: 2023-02-03 21:54:40+00:00
  edited: false
  hidden: false
  id: 63dd82a00f6d2d6c3efbb508
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648966381588-6064e095abd8d3692e3e2ed6.jpeg?w=200&h=200&f=face
      fullname: "Radam\xE9s Ajna"
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radames
      type: user
    createdAt: '2023-02-03T22:28:05.000Z'
    data:
      edited: false
      editors:
      - radames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648966381588-6064e095abd8d3692e3e2ed6.jpeg?w=200&h=200&f=face
          fullname: "Radam\xE9s Ajna"
          isHf: true
          isPro: false
          name: radames
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;eashanchawla&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/eashanchawla\"\
          >@<span class=\"underline\">eashanchawla</span></a></span>\n\n\t</span></span>\
          \ , all the data for this model is on our hub. When you use <code>from_pretrained(\"\
          openai/whisper-large)</code> the data is fetch the repo from <code>https://huggingface.co/openai/whisper-large/tree/main</code>\
          \ for instance the pytorch model file is here <a href=\"https://huggingface.co/openai/whisper-large/blob/main/pytorch_model.bin\"\
          >https://huggingface.co/openai/whisper-large/blob/main/pytorch_model.bin</a>\
          \ and via resolve <a href=\"https://huggingface.co/openai/whisper-large/resolve/main/pytorch_model.bin\"\
          >https://huggingface.co/openai/whisper-large/resolve/main/pytorch_model.bin</a></p>\n"
        raw: hi @eashanchawla , all the data for this model is on our hub. When you
          use `from_pretrained("openai/whisper-large)` the data is fetch the repo
          from `https://huggingface.co/openai/whisper-large/tree/main` for instance
          the pytorch model file is here https://huggingface.co/openai/whisper-large/blob/main/pytorch_model.bin
          and via resolve https://huggingface.co/openai/whisper-large/resolve/main/pytorch_model.bin
        updatedAt: '2023-02-03T22:28:05.490Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - sanchit-gandhi
    id: 63dd8a75ad14107542d89fb2
    type: comment
  author: radames
  content: hi @eashanchawla , all the data for this model is on our hub. When you
    use `from_pretrained("openai/whisper-large)` the data is fetch the repo from `https://huggingface.co/openai/whisper-large/tree/main`
    for instance the pytorch model file is here https://huggingface.co/openai/whisper-large/blob/main/pytorch_model.bin
    and via resolve https://huggingface.co/openai/whisper-large/resolve/main/pytorch_model.bin
  created_at: 2023-02-03 22:28:05+00:00
  edited: false
  hidden: false
  id: 63dd8a75ad14107542d89fb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07639710a80fdbee1897ec279666e53f.svg
      fullname: Eashan Chawla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eashanchawla
      type: user
    createdAt: '2023-02-07T14:58:17.000Z'
    data:
      edited: true
      editors:
      - eashanchawla
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07639710a80fdbee1897ec279666e53f.svg
          fullname: Eashan Chawla
          isHf: false
          isPro: false
          name: eashanchawla
          type: user
        html: "<p>Thank you for the quick response <span data-props=\"{&quot;user&quot;:&quot;radames&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/radames\"\
          >@<span class=\"underline\">radames</span></a></span>\n\n\t</span></span>!\
          \ I do have a follow up question though:<br>I defined a proxy dict as follows:<br>PROXY_DICT\
          \ = {<br>    'http:': 'proxy.ec.com:8000',<br>    'https:': 'proxy.ec.com:8000'<br>}<br>self.processor\
          \ = WhisperProcessor.from_pretrained(<br>            pretrained_model_name_or_path\
          \ =\"openai/whisper-large\",<br>            cache_dir=TEMP_DIR,<br>    \
          \        force_download=True,<br>            revision='e5aba7b7d827c01bf4db9b90d9ea7d670295b212',<br>\
          \            proxies=PROXY_DICT<br>            )</p>\n<p>I get the following\
          \ error:<br>ValueError: We have no connection or you passed local_files_only,\
          \ so force_download is not an accepted option.</p>\n<p>Is there something\
          \ I am missing out with how I am defining the proxy? Because when I try\
          \ to run this command on the powershell: Invoke-restmethod -Uri \"<a href=\"\
          https://huggingface.co/openai/whisper-large/resolve/main/preprocessor_config.json&quot;\"\
          >https://huggingface.co/openai/whisper-large/resolve/main/preprocessor_config.json\"\
          </a> -Proxy \"<a rel=\"nofollow\" href=\"http://proxy.ec.com:8000&quot;\"\
          >http://proxy.ec.com:8000\"</a>, I see it pulling config info. </p>\n<p>Is\
          \ it not downloading but loading from the path somehow? What can I try additionally\
          \ to make it work?</p>\n"
        raw: "Thank you for the quick response @radames! I do have a follow up question\
          \ though:\nI defined a proxy dict as follows:\nPROXY_DICT = {\n    'http:':\
          \ 'proxy.ec.com:8000',\n    'https:': 'proxy.ec.com:8000'\n}\nself.processor\
          \ = WhisperProcessor.from_pretrained(\n            pretrained_model_name_or_path\
          \ =\"openai/whisper-large\",\n            cache_dir=TEMP_DIR,\n        \
          \    force_download=True,\n            revision='e5aba7b7d827c01bf4db9b90d9ea7d670295b212',\n\
          \            proxies=PROXY_DICT\n            )\n\nI get the following error:\
          \ \nValueError: We have no connection or you passed local_files_only, so\
          \ force_download is not an accepted option.\n\nIs there something I am missing\
          \ out with how I am defining the proxy? Because when I try to run this command\
          \ on the powershell: Invoke-restmethod -Uri \"https://huggingface.co/openai/whisper-large/resolve/main/preprocessor_config.json\"\
          \ -Proxy \"http://proxy.ec.com:8000\", I see it pulling config info. \n\n\
          Is it not downloading but loading from the path somehow? What can I try\
          \ additionally to make it work?"
        updatedAt: '2023-02-07T14:58:59.685Z'
      numEdits: 1
      reactions: []
    id: 63e2670984566d7b44be1dcb
    type: comment
  author: eashanchawla
  content: "Thank you for the quick response @radames! I do have a follow up question\
    \ though:\nI defined a proxy dict as follows:\nPROXY_DICT = {\n    'http:': 'proxy.ec.com:8000',\n\
    \    'https:': 'proxy.ec.com:8000'\n}\nself.processor = WhisperProcessor.from_pretrained(\n\
    \            pretrained_model_name_or_path =\"openai/whisper-large\",\n      \
    \      cache_dir=TEMP_DIR,\n            force_download=True,\n            revision='e5aba7b7d827c01bf4db9b90d9ea7d670295b212',\n\
    \            proxies=PROXY_DICT\n            )\n\nI get the following error: \n\
    ValueError: We have no connection or you passed local_files_only, so force_download\
    \ is not an accepted option.\n\nIs there something I am missing out with how I\
    \ am defining the proxy? Because when I try to run this command on the powershell:\
    \ Invoke-restmethod -Uri \"https://huggingface.co/openai/whisper-large/resolve/main/preprocessor_config.json\"\
    \ -Proxy \"http://proxy.ec.com:8000\", I see it pulling config info. \n\nIs it\
    \ not downloading but loading from the path somehow? What can I try additionally\
    \ to make it work?"
  created_at: 2023-02-07 14:58:17+00:00
  edited: true
  hidden: false
  id: 63e2670984566d7b44be1dcb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07639710a80fdbee1897ec279666e53f.svg
      fullname: Eashan Chawla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eashanchawla
      type: user
    createdAt: '2023-02-07T15:34:26.000Z'
    data:
      edited: true
      editors:
      - eashanchawla
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07639710a80fdbee1897ec279666e53f.svg
          fullname: Eashan Chawla
          isHf: false
          isPro: false
          name: eashanchawla
          type: user
        html: '<p>If I don''t set force_download=True, I see this error</p>

          <p>(OSError: We couldn''t connect to ''<a rel="nofollow" href="https://huggingface.co''">https://huggingface.co''</a>
          to load this file, couldn''t find it in the cached files and it looks like
          openai/whisper-large is not the path to a directory containing a file named
          preprocessor_config.json.<br>Checkout your internet connection or see how
          to run the library in offline mode at ''<a href="https://huggingface.co/docs/transformers/installation#offline-mode''">https://huggingface.co/docs/transformers/installation#offline-mode''</a>.)</p>

          '
        raw: 'If I don''t set force_download=True, I see this error


          (OSError: We couldn''t connect to ''https://huggingface.co'' to load this
          file, couldn''t find it in the cached files and it looks like openai/whisper-large
          is not the path to a directory containing a file named preprocessor_config.json.

          Checkout your internet connection or see how to run the library in offline
          mode at ''https://huggingface.co/docs/transformers/installation#offline-mode''.)'
        updatedAt: '2023-02-07T15:37:30.759Z'
      numEdits: 1
      reactions: []
    id: 63e26f8284566d7b44beecdb
    type: comment
  author: eashanchawla
  content: 'If I don''t set force_download=True, I see this error


    (OSError: We couldn''t connect to ''https://huggingface.co'' to load this file,
    couldn''t find it in the cached files and it looks like openai/whisper-large is
    not the path to a directory containing a file named preprocessor_config.json.

    Checkout your internet connection or see how to run the library in offline mode
    at ''https://huggingface.co/docs/transformers/installation#offline-mode''.)'
  created_at: 2023-02-07 15:34:26+00:00
  edited: true
  hidden: false
  id: 63e26f8284566d7b44beecdb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: openai/whisper-large
repo_type: model
status: open
target_branch: null
title: Link of model download
