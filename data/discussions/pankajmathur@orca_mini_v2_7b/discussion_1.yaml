!!python/object:huggingface_hub.community.DiscussionWithDetails
author: migtissera
conflicting_files: null
created_at: 2023-07-10 02:16:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-07-10T03:16:43.000Z'
    data:
      edited: true
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.823444128036499
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: "<p>Can confirm that this is scoring 0.39405 in my tests using MMLU.</p>\n\
          <p>Just to write this out in case people don't know, the HuggingFace leaderboard's\
          \ LLaMA score for MMLU was questioned by many people a couple of months\
          \ ago, and HuggingFace wrote a blog post about it here: <a href=\"https://huggingface.co/blog/evaluating-mmlu-leaderboard\"\
          >https://huggingface.co/blog/evaluating-mmlu-leaderboard</a></p>\n<p>The\
          \ outcome was that ElutherAI's Eval Harness repo had not been using the\
          \ original MMLU approach proposed by the initial authors, but LLaMA had.\
          \ ElutherAI is now in the process of updating their library, but the commit\
          \ (5ba0c5f9020bdb980b998f934682f1805f9437f0) after PR #497 merge brings\
          \ the original MMLU approach to the repo: <a rel=\"nofollow\" href=\"https://github.com/EleutherAI/lm-evaluation-harness/pull/497\"\
          >https://github.com/EleutherAI/lm-evaluation-harness/pull/497</a></p>\n\
          <p>I used the following to evaluate this model, using the original approach\
          \ proposed by MMLU's authors: </p>\n<p><code>python main.py --model=hf-causal\
          \ --model_args=\"pretrained=&lt;MODEL_PATH&gt;,trust_remote_code=True,dtype=bfloat16,load_in_8bit=False\u201D\
          \ --tasks=hendrycksTest-* --num_fewshot=5  --device=cuda --output_path=&lt;OUTPUT_JSON_PATH&gt;\
          \ --batch_size=2 --no_cache</code></p>\n<p>and I had to replace the tokenizer\
          \ in <code>/lm_eval/models/gpt2.py</code> with the following:</p>\n<p><code>self.tokenizer\
          \ = transformers.LlamaTokenizer.from_pretrained(             \"/path/to/orca_mini_v2_7b\"\
          , trust_remote_code=True         )</code></p>\n"
        raw: "Can confirm that this is scoring 0.39405 in my tests using MMLU.\n\n\
          Just to write this out in case people don't know, the HuggingFace leaderboard's\
          \ LLaMA score for MMLU was questioned by many people a couple of months\
          \ ago, and HuggingFace wrote a blog post about it here: https://huggingface.co/blog/evaluating-mmlu-leaderboard\n\
          \nThe outcome was that ElutherAI's Eval Harness repo had not been using\
          \ the original MMLU approach proposed by the initial authors, but LLaMA\
          \ had. ElutherAI is now in the process of updating their library, but the\
          \ commit (5ba0c5f9020bdb980b998f934682f1805f9437f0) after PR #497 merge\
          \ brings the original MMLU approach to the repo: https://github.com/EleutherAI/lm-evaluation-harness/pull/497\n\
          \nI used the following to evaluate this model, using the original approach\
          \ proposed by MMLU's authors: \n\n```python main.py --model=hf-causal --model_args=\"\
          pretrained=<MODEL_PATH>,trust_remote_code=True,dtype=bfloat16,load_in_8bit=False\u201D\
          \ --tasks=hendrycksTest-* --num_fewshot=5  --device=cuda --output_path=<OUTPUT_JSON_PATH>\
          \ --batch_size=2 --no_cache```\n\nand I had to replace the tokenizer in\
          \ `/lm_eval/models/gpt2.py` with the following:\n\n```self.tokenizer = transformers.LlamaTokenizer.from_pretrained(\n\
          \            \"/path/to/orca_mini_v2_7b\", trust_remote_code=True\n    \
          \    )```"
        updatedAt: '2023-07-10T03:21:10.079Z'
      numEdits: 7
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - pankajmathur
    id: 64ab781b8cac7b871a7f9040
    type: comment
  author: migtissera
  content: "Can confirm that this is scoring 0.39405 in my tests using MMLU.\n\nJust\
    \ to write this out in case people don't know, the HuggingFace leaderboard's LLaMA\
    \ score for MMLU was questioned by many people a couple of months ago, and HuggingFace\
    \ wrote a blog post about it here: https://huggingface.co/blog/evaluating-mmlu-leaderboard\n\
    \nThe outcome was that ElutherAI's Eval Harness repo had not been using the original\
    \ MMLU approach proposed by the initial authors, but LLaMA had. ElutherAI is now\
    \ in the process of updating their library, but the commit (5ba0c5f9020bdb980b998f934682f1805f9437f0)\
    \ after PR #497 merge brings the original MMLU approach to the repo: https://github.com/EleutherAI/lm-evaluation-harness/pull/497\n\
    \nI used the following to evaluate this model, using the original approach proposed\
    \ by MMLU's authors: \n\n```python main.py --model=hf-causal --model_args=\"pretrained=<MODEL_PATH>,trust_remote_code=True,dtype=bfloat16,load_in_8bit=False\u201D\
    \ --tasks=hendrycksTest-* --num_fewshot=5  --device=cuda --output_path=<OUTPUT_JSON_PATH>\
    \ --batch_size=2 --no_cache```\n\nand I had to replace the tokenizer in `/lm_eval/models/gpt2.py`\
    \ with the following:\n\n```self.tokenizer = transformers.LlamaTokenizer.from_pretrained(\n\
    \            \"/path/to/orca_mini_v2_7b\", trust_remote_code=True\n        )```"
  created_at: 2023-07-10 02:16:43+00:00
  edited: true
  hidden: false
  id: 64ab781b8cac7b871a7f9040
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-07-10T04:48:50.000Z'
    data:
      edited: true
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.857422411441803
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Here is also a good resource that contain MMLU scores using the
          original method, for 7B models: <a rel="nofollow" href="https://blog.salesforceairesearch.com/xgen/#results-on-standard-benchmarks">https://blog.salesforceairesearch.com/xgen/#results-on-standard-benchmarks</a></p>

          '
        raw: 'Here is also a good resource that contain MMLU scores using the original
          method, for 7B models: https://blog.salesforceairesearch.com/xgen/#results-on-standard-benchmarks

          '
        updatedAt: '2023-07-10T04:49:29.119Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - pankajmathur
        - mike-ravkine
    id: 64ab8db273790912c7ac5e49
    type: comment
  author: migtissera
  content: 'Here is also a good resource that contain MMLU scores using the original
    method, for 7B models: https://blog.salesforceairesearch.com/xgen/#results-on-standard-benchmarks

    '
  created_at: 2023-07-10 03:48:50+00:00
  edited: true
  hidden: false
  id: 64ab8db273790912c7ac5e49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
      fullname: Pankaj Mathur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: pankajmathur
      type: user
    createdAt: '2023-07-10T06:23:35.000Z'
    data:
      edited: false
      editors:
      - pankajmathur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9004801511764526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
          fullname: Pankaj Mathur
          isHf: false
          isPro: true
          name: pankajmathur
          type: user
        html: '<p>This is amazing work thanks Migel for sharing detail posts about
          instructions for users to conduct their own eval using LM Eval Harness repo.<br>Quick
          question 0.39405 is acc or acc_norm or average of both.  Also if you can
          please share the version # too for your test so other can replicate the
          eval, if they need.<br>Thanks again :)</p>

          '
        raw: "This is amazing work thanks Migel for sharing detail posts about instructions\
          \ for users to conduct their own eval using LM Eval Harness repo. \nQuick\
          \ question 0.39405 is acc or acc_norm or average of both.  Also if you can\
          \ please share the version # too for your test so other can replicate the\
          \ eval, if they need. \nThanks again :)"
        updatedAt: '2023-07-10T06:23:35.578Z'
      numEdits: 0
      reactions: []
    id: 64aba3e7cc73827ce6b43ea4
    type: comment
  author: pankajmathur
  content: "This is amazing work thanks Migel for sharing detail posts about instructions\
    \ for users to conduct their own eval using LM Eval Harness repo. \nQuick question\
    \ 0.39405 is acc or acc_norm or average of both.  Also if you can please share\
    \ the version # too for your test so other can replicate the eval, if they need.\
    \ \nThanks again :)"
  created_at: 2023-07-10 05:23:35+00:00
  edited: false
  hidden: false
  id: 64aba3e7cc73827ce6b43ea4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-07-10T15:45:07.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3309825360774994
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: "<p>Hey Pankaj,</p>\n<p>0.39405 is the average of all 57 tests, on the\
          \ \"acc\". Version # of the tests is 1.</p>\n<p>Here's the full test:</p>\n\
          <pre><code>{\n  \"results\": {\n    \"hendrycksTest-abstract_algebra\":\
          \ {\n      \"acc\": 0.31,\n      \"acc_stderr\": 0.04648231987117316,\n\
          \      \"acc_norm\": 0.31,\n      \"acc_norm_stderr\": 0.04648231987117316\n\
          \    },\n    \"hendrycksTest-anatomy\": {\n      \"acc\": 0.4222222222222222,\n\
          \      \"acc_stderr\": 0.04266763404099582,\n      \"acc_norm\": 0.4222222222222222,\n\
          \      \"acc_norm_stderr\": 0.04266763404099582\n    },\n    \"hendrycksTest-astronomy\"\
          : {\n      \"acc\": 0.34210526315789475,\n      \"acc_stderr\": 0.03860731599316092,\n\
          \      \"acc_norm\": 0.34210526315789475,\n      \"acc_norm_stderr\": 0.03860731599316092\n\
          \    },\n    \"hendrycksTest-business_ethics\": {\n      \"acc\": 0.43,\n\
          \      \"acc_stderr\": 0.049756985195624284,\n      \"acc_norm\": 0.43,\n\
          \      \"acc_norm_stderr\": 0.049756985195624284\n    },\n    \"hendrycksTest-clinical_knowledge\"\
          : {\n      \"acc\": 0.4528301886792453,\n      \"acc_stderr\": 0.030635627957961827,\n\
          \      \"acc_norm\": 0.4528301886792453,\n      \"acc_norm_stderr\": 0.030635627957961827\n\
          \    },\n    \"hendrycksTest-college_biology\": {\n      \"acc\": 0.3611111111111111,\n\
          \      \"acc_stderr\": 0.04016660030451233,\n      \"acc_norm\": 0.3611111111111111,\n\
          \      \"acc_norm_stderr\": 0.04016660030451233\n    },\n    \"hendrycksTest-college_chemistry\"\
          : {\n      \"acc\": 0.25,\n      \"acc_stderr\": 0.04351941398892446,\n\
          \      \"acc_norm\": 0.25,\n      \"acc_norm_stderr\": 0.04351941398892446\n\
          \    },\n    \"hendrycksTest-college_computer_science\": {\n      \"acc\"\
          : 0.37,\n      \"acc_stderr\": 0.04852365870939099,\n      \"acc_norm\"\
          : 0.37,\n      \"acc_norm_stderr\": 0.04852365870939099\n    },\n    \"\
          hendrycksTest-college_mathematics\": {\n      \"acc\": 0.3,\n      \"acc_stderr\"\
          : 0.046056618647183814,\n      \"acc_norm\": 0.3,\n      \"acc_norm_stderr\"\
          : 0.046056618647183814\n    },\n    \"hendrycksTest-college_medicine\":\
          \ {\n      \"acc\": 0.2774566473988439,\n      \"acc_stderr\": 0.03414014007044037,\n\
          \      \"acc_norm\": 0.2774566473988439,\n      \"acc_norm_stderr\": 0.03414014007044037\n\
          \    },\n    \"hendrycksTest-college_physics\": {\n      \"acc\": 0.16666666666666666,\n\
          \      \"acc_stderr\": 0.03708284662416545,\n      \"acc_norm\": 0.16666666666666666,\n\
          \      \"acc_norm_stderr\": 0.03708284662416545\n    },\n    \"hendrycksTest-computer_security\"\
          : {\n      \"acc\": 0.52,\n      \"acc_stderr\": 0.05021167315686779,\n\
          \      \"acc_norm\": 0.52,\n      \"acc_norm_stderr\": 0.05021167315686779\n\
          \    },\n    \"hendrycksTest-conceptual_physics\": {\n      \"acc\": 0.3702127659574468,\n\
          \      \"acc_stderr\": 0.03156564682236785,\n      \"acc_norm\": 0.3702127659574468,\n\
          \      \"acc_norm_stderr\": 0.03156564682236785\n    },\n    \"hendrycksTest-econometrics\"\
          : {\n      \"acc\": 0.23684210526315788,\n      \"acc_stderr\": 0.03999423879281336,\n\
          \      \"acc_norm\": 0.23684210526315788,\n      \"acc_norm_stderr\": 0.03999423879281336\n\
          \    },\n    \"hendrycksTest-electrical_engineering\": {\n      \"acc\"\
          : 0.38620689655172413,\n      \"acc_stderr\": 0.04057324734419036,\n   \
          \   \"acc_norm\": 0.38620689655172413,\n      \"acc_norm_stderr\": 0.04057324734419036\n\
          \    },\n    \"hendrycksTest-elementary_mathematics\": {\n      \"acc\"\
          : 0.26455026455026454,\n      \"acc_stderr\": 0.02271746789770861,\n   \
          \   \"acc_norm\": 0.26455026455026454,\n      \"acc_norm_stderr\": 0.02271746789770861\n\
          \    },\n    \"hendrycksTest-formal_logic\": {\n      \"acc\": 0.25396825396825395,\n\
          \      \"acc_stderr\": 0.03893259610604673,\n      \"acc_norm\": 0.25396825396825395,\n\
          \      \"acc_norm_stderr\": 0.03893259610604673\n    },\n    \"hendrycksTest-global_facts\"\
          : {\n      \"acc\": 0.36,\n      \"acc_stderr\": 0.04824181513244218,\n\
          \      \"acc_norm\": 0.36,\n      \"acc_norm_stderr\": 0.04824181513244218\n\
          \    },\n    \"hendrycksTest-high_school_biology\": {\n      \"acc\": 0.3967741935483871,\n\
          \      \"acc_stderr\": 0.027831231605767948,\n      \"acc_norm\": 0.3967741935483871,\n\
          \      \"acc_norm_stderr\": 0.027831231605767948\n    },\n    \"hendrycksTest-high_school_chemistry\"\
          : {\n      \"acc\": 0.26108374384236455,\n      \"acc_stderr\": 0.030903796952114475,\n\
          \      \"acc_norm\": 0.26108374384236455,\n      \"acc_norm_stderr\": 0.030903796952114475\n\
          \    },\n    \"hendrycksTest-high_school_computer_science\": {\n      \"\
          acc\": 0.45,\n      \"acc_stderr\": 0.049999999999999996,\n      \"acc_norm\"\
          : 0.45,\n      \"acc_norm_stderr\": 0.049999999999999996\n    },\n    \"\
          hendrycksTest-high_school_european_history\": {\n      \"acc\": 0.503030303030303,\n\
          \      \"acc_stderr\": 0.03904272341431857,\n      \"acc_norm\": 0.503030303030303,\n\
          \      \"acc_norm_stderr\": 0.03904272341431857\n    },\n    \"hendrycksTest-high_school_geography\"\
          : {\n      \"acc\": 0.4898989898989899,\n      \"acc_stderr\": 0.035616254886737454,\n\
          \      \"acc_norm\": 0.4898989898989899,\n      \"acc_norm_stderr\": 0.035616254886737454\n\
          \    },\n    \"hendrycksTest-high_school_government_and_politics\": {\n\
          \      \"acc\": 0.5544041450777202,\n      \"acc_stderr\": 0.03587014986075658,\n\
          \      \"acc_norm\": 0.5544041450777202,\n      \"acc_norm_stderr\": 0.03587014986075658\n\
          \    },\n    \"hendrycksTest-high_school_macroeconomics\": {\n      \"acc\"\
          : 0.3717948717948718,\n      \"acc_stderr\": 0.024503472557110936,\n   \
          \   \"acc_norm\": 0.3717948717948718,\n      \"acc_norm_stderr\": 0.024503472557110936\n\
          \    },\n    \"hendrycksTest-high_school_mathematics\": {\n      \"acc\"\
          : 0.22962962962962963,\n      \"acc_stderr\": 0.02564410863926764,\n   \
          \   \"acc_norm\": 0.22962962962962963,\n      \"acc_norm_stderr\": 0.02564410863926764\n\
          \    },\n    \"hendrycksTest-high_school_microeconomics\": {\n      \"acc\"\
          : 0.3235294117647059,\n      \"acc_stderr\": 0.030388353551886845,\n   \
          \   \"acc_norm\": 0.3235294117647059,\n      \"acc_norm_stderr\": 0.030388353551886845\n\
          \    },\n    \"hendrycksTest-high_school_physics\": {\n      \"acc\": 0.24503311258278146,\n\
          \      \"acc_stderr\": 0.03511807571804724,\n      \"acc_norm\": 0.24503311258278146,\n\
          \      \"acc_norm_stderr\": 0.03511807571804724\n    },\n    \"hendrycksTest-high_school_psychology\"\
          : {\n      \"acc\": 0.46972477064220186,\n      \"acc_stderr\": 0.021397988604936965,\n\
          \      \"acc_norm\": 0.46972477064220186,\n      \"acc_norm_stderr\": 0.021397988604936965\n\
          \    },\n    \"hendrycksTest-high_school_statistics\": {\n      \"acc\"\
          : 0.2824074074074074,\n      \"acc_stderr\": 0.030701372111510934,\n   \
          \   \"acc_norm\": 0.2824074074074074,\n      \"acc_norm_stderr\": 0.030701372111510934\n\
          \    },\n    \"hendrycksTest-high_school_us_history\": {\n      \"acc\"\
          : 0.4803921568627451,\n      \"acc_stderr\": 0.03506612560524866,\n    \
          \  \"acc_norm\": 0.4803921568627451,\n      \"acc_norm_stderr\": 0.03506612560524866\n\
          \    },\n    \"hendrycksTest-high_school_world_history\": {\n      \"acc\"\
          : 0.4978902953586498,\n      \"acc_stderr\": 0.032546938018020076,\n   \
          \   \"acc_norm\": 0.4978902953586498,\n      \"acc_norm_stderr\": 0.032546938018020076\n\
          \    },\n    \"hendrycksTest-human_aging\": {\n      \"acc\": 0.4977578475336323,\n\
          \      \"acc_stderr\": 0.033557465352232634,\n      \"acc_norm\": 0.4977578475336323,\n\
          \      \"acc_norm_stderr\": 0.033557465352232634\n    },\n    \"hendrycksTest-human_sexuality\"\
          : {\n      \"acc\": 0.45038167938931295,\n      \"acc_stderr\": 0.04363643698524779,\n\
          \      \"acc_norm\": 0.45038167938931295,\n      \"acc_norm_stderr\": 0.04363643698524779\n\
          \    },\n    \"hendrycksTest-international_law\": {\n      \"acc\": 0.6363636363636364,\n\
          \      \"acc_stderr\": 0.043913262867240704,\n      \"acc_norm\": 0.6363636363636364,\n\
          \      \"acc_norm_stderr\": 0.043913262867240704\n    },\n    \"hendrycksTest-jurisprudence\"\
          : {\n      \"acc\": 0.46296296296296297,\n      \"acc_stderr\": 0.04820403072760626,\n\
          \      \"acc_norm\": 0.46296296296296297,\n      \"acc_norm_stderr\": 0.04820403072760626\n\
          \    },\n    \"hendrycksTest-logical_fallacies\": {\n      \"acc\": 0.43558282208588955,\n\
          \      \"acc_stderr\": 0.038956324641389366,\n      \"acc_norm\": 0.43558282208588955,\n\
          \      \"acc_norm_stderr\": 0.038956324641389366\n    },\n    \"hendrycksTest-machine_learning\"\
          : {\n      \"acc\": 0.29464285714285715,\n      \"acc_stderr\": 0.043270409325787296,\n\
          \      \"acc_norm\": 0.29464285714285715,\n      \"acc_norm_stderr\": 0.043270409325787296\n\
          \    },\n    \"hendrycksTest-management\": {\n      \"acc\": 0.3883495145631068,\n\
          \      \"acc_stderr\": 0.048257293373563895,\n      \"acc_norm\": 0.3883495145631068,\n\
          \      \"acc_norm_stderr\": 0.048257293373563895\n    },\n    \"hendrycksTest-marketing\"\
          : {\n      \"acc\": 0.5811965811965812,\n      \"acc_stderr\": 0.03232128912157792,\n\
          \      \"acc_norm\": 0.5811965811965812,\n      \"acc_norm_stderr\": 0.03232128912157792\n\
          \    },\n    \"hendrycksTest-medical_genetics\": {\n      \"acc\": 0.44,\n\
          \      \"acc_stderr\": 0.049888765156985884,\n      \"acc_norm\": 0.44,\n\
          \      \"acc_norm_stderr\": 0.049888765156985884\n    },\n    \"hendrycksTest-miscellaneous\"\
          : {\n      \"acc\": 0.5376756066411239,\n      \"acc_stderr\": 0.017829131764287184,\n\
          \      \"acc_norm\": 0.5376756066411239,\n      \"acc_norm_stderr\": 0.017829131764287184\n\
          \    },\n    \"hendrycksTest-moral_disputes\": {\n      \"acc\": 0.4393063583815029,\n\
          \      \"acc_stderr\": 0.026720034380514998,\n      \"acc_norm\": 0.4393063583815029,\n\
          \      \"acc_norm_stderr\": 0.026720034380514998\n    },\n    \"hendrycksTest-moral_scenarios\"\
          : {\n      \"acc\": 0.23016759776536314,\n      \"acc_stderr\": 0.014078339253425826,\n\
          \      \"acc_norm\": 0.23016759776536314,\n      \"acc_norm_stderr\": 0.014078339253425826\n\
          \    },\n    \"hendrycksTest-nutrition\": {\n      \"acc\": 0.37254901960784315,\n\
          \      \"acc_stderr\": 0.027684181883302898,\n      \"acc_norm\": 0.37254901960784315,\n\
          \      \"acc_norm_stderr\": 0.027684181883302898\n    },\n    \"hendrycksTest-philosophy\"\
          : {\n      \"acc\": 0.42765273311897106,\n      \"acc_stderr\": 0.02809924077580957,\n\
          \      \"acc_norm\": 0.42765273311897106,\n      \"acc_norm_stderr\": 0.02809924077580957\n\
          \    },\n    \"hendrycksTest-prehistory\": {\n      \"acc\": 0.4506172839506173,\n\
          \      \"acc_stderr\": 0.02768472141565619,\n      \"acc_norm\": 0.4506172839506173,\n\
          \      \"acc_norm_stderr\": 0.02768472141565619\n    },\n    \"hendrycksTest-professional_accounting\"\
          : {\n      \"acc\": 0.31560283687943264,\n      \"acc_stderr\": 0.027724989449509307,\n\
          \      \"acc_norm\": 0.31560283687943264,\n      \"acc_norm_stderr\": 0.027724989449509307\n\
          \    },\n    \"hendrycksTest-professional_law\": {\n      \"acc\": 0.3109517601043025,\n\
          \      \"acc_stderr\": 0.011822252917799207,\n      \"acc_norm\": 0.3109517601043025,\n\
          \      \"acc_norm_stderr\": 0.011822252917799207\n    },\n    \"hendrycksTest-professional_medicine\"\
          : {\n      \"acc\": 0.40808823529411764,\n      \"acc_stderr\": 0.029855261393483924,\n\
          \      \"acc_norm\": 0.40808823529411764,\n      \"acc_norm_stderr\": 0.029855261393483924\n\
          \    },\n    \"hendrycksTest-professional_psychology\": {\n      \"acc\"\
          : 0.4035947712418301,\n      \"acc_stderr\": 0.019848280168401154,\n   \
          \   \"acc_norm\": 0.4035947712418301,\n      \"acc_norm_stderr\": 0.019848280168401154\n\
          \    },\n    \"hendrycksTest-public_relations\": {\n      \"acc\": 0.4090909090909091,\n\
          \      \"acc_stderr\": 0.04709306978661896,\n      \"acc_norm\": 0.4090909090909091,\n\
          \      \"acc_norm_stderr\": 0.04709306978661896\n    },\n    \"hendrycksTest-security_studies\"\
          : {\n      \"acc\": 0.3510204081632653,\n      \"acc_stderr\": 0.030555316755573637,\n\
          \      \"acc_norm\": 0.3510204081632653,\n      \"acc_norm_stderr\": 0.030555316755573637\n\
          \    },\n    \"hendrycksTest-sociology\": {\n      \"acc\": 0.4925373134328358,\n\
          \      \"acc_stderr\": 0.03535140084276719,\n      \"acc_norm\": 0.4925373134328358,\n\
          \      \"acc_norm_stderr\": 0.03535140084276719\n    },\n    \"hendrycksTest-us_foreign_policy\"\
          : {\n      \"acc\": 0.59,\n      \"acc_stderr\": 0.04943110704237101,\n\
          \      \"acc_norm\": 0.59,\n      \"acc_norm_stderr\": 0.04943110704237101\n\
          \    },\n    \"hendrycksTest-virology\": {\n      \"acc\": 0.3614457831325301,\n\
          \      \"acc_stderr\": 0.03740059382029321,\n      \"acc_norm\": 0.3614457831325301,\n\
          \      \"acc_norm_stderr\": 0.03740059382029321\n    },\n    \"hendrycksTest-world_religions\"\
          : {\n      \"acc\": 0.543859649122807,\n      \"acc_stderr\": 0.038200425866029654,\n\
          \      \"acc_norm\": 0.543859649122807,\n      \"acc_norm_stderr\": 0.038200425866029654\n\
          \    }\n  },\n  \"versions\": {\n    \"hendrycksTest-abstract_algebra\"\
          : 1,\n    \"hendrycksTest-anatomy\": 1,\n    \"hendrycksTest-astronomy\"\
          : 1,\n    \"hendrycksTest-business_ethics\": 1,\n    \"hendrycksTest-clinical_knowledge\"\
          : 1,\n    \"hendrycksTest-college_biology\": 1,\n    \"hendrycksTest-college_chemistry\"\
          : 1,\n    \"hendrycksTest-college_computer_science\": 1,\n    \"hendrycksTest-college_mathematics\"\
          : 1,\n    \"hendrycksTest-college_medicine\": 1,\n    \"hendrycksTest-college_physics\"\
          : 1,\n    \"hendrycksTest-computer_security\": 1,\n    \"hendrycksTest-conceptual_physics\"\
          : 1,\n    \"hendrycksTest-econometrics\": 1,\n    \"hendrycksTest-electrical_engineering\"\
          : 1,\n    \"hendrycksTest-elementary_mathematics\": 1,\n    \"hendrycksTest-formal_logic\"\
          : 1,\n    \"hendrycksTest-global_facts\": 1,\n    \"hendrycksTest-high_school_biology\"\
          : 1,\n    \"hendrycksTest-high_school_chemistry\": 1,\n    \"hendrycksTest-high_school_computer_science\"\
          : 1,\n    \"hendrycksTest-high_school_european_history\": 1,\n    \"hendrycksTest-high_school_geography\"\
          : 1,\n    \"hendrycksTest-high_school_government_and_politics\": 1,\n  \
          \  \"hendrycksTest-high_school_macroeconomics\": 1,\n    \"hendrycksTest-high_school_mathematics\"\
          : 1,\n    \"hendrycksTest-high_school_microeconomics\": 1,\n    \"hendrycksTest-high_school_physics\"\
          : 1,\n    \"hendrycksTest-high_school_psychology\": 1,\n    \"hendrycksTest-high_school_statistics\"\
          : 1,\n    \"hendrycksTest-high_school_us_history\": 1,\n    \"hendrycksTest-high_school_world_history\"\
          : 1,\n    \"hendrycksTest-human_aging\": 1,\n    \"hendrycksTest-human_sexuality\"\
          : 1,\n    \"hendrycksTest-international_law\": 1,\n    \"hendrycksTest-jurisprudence\"\
          : 1,\n    \"hendrycksTest-logical_fallacies\": 1,\n    \"hendrycksTest-machine_learning\"\
          : 1,\n    \"hendrycksTest-management\": 1,\n    \"hendrycksTest-marketing\"\
          : 1,\n    \"hendrycksTest-medical_genetics\": 1,\n    \"hendrycksTest-miscellaneous\"\
          : 1,\n    \"hendrycksTest-moral_disputes\": 1,\n    \"hendrycksTest-moral_scenarios\"\
          : 1,\n    \"hendrycksTest-nutrition\": 1,\n    \"hendrycksTest-philosophy\"\
          : 1,\n    \"hendrycksTest-prehistory\": 1,\n    \"hendrycksTest-professional_accounting\"\
          : 1,\n    \"hendrycksTest-professional_law\": 1,\n    \"hendrycksTest-professional_medicine\"\
          : 1,\n    \"hendrycksTest-professional_psychology\": 1,\n    \"hendrycksTest-public_relations\"\
          : 1,\n    \"hendrycksTest-security_studies\": 1,\n    \"hendrycksTest-sociology\"\
          : 1,\n    \"hendrycksTest-us_foreign_policy\": 1,\n    \"hendrycksTest-virology\"\
          : 1,\n    \"hendrycksTest-world_religions\": 1\n  },\n  \"config\": {\n\
          \    \"model\": \"hf-causal\",\n    \"model_args\": \"pretrained=/home/models/orca_mini_v2_7b,trust_remote_code=True,dtype=bfloat16,load_in_8bit=False\"\
          ,\n    \"num_fewshot\": 5,\n    \"batch_size\": \"2\",\n    \"batch_sizes\"\
          : [],\n    \"device\": \"cuda\",\n    \"no_cache\": true,\n    \"limit\"\
          : null,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\": {}\n\
          \  }\n}\n</code></pre>\n"
        raw: "Hey Pankaj,\n\n0.39405 is the average of all 57 tests, on the \"acc\"\
          . Version # of the tests is 1.\n\n\nHere's the full test:\n\n```\n{\n  \"\
          results\": {\n    \"hendrycksTest-abstract_algebra\": {\n      \"acc\":\
          \ 0.31,\n      \"acc_stderr\": 0.04648231987117316,\n      \"acc_norm\"\
          : 0.31,\n      \"acc_norm_stderr\": 0.04648231987117316\n    },\n    \"\
          hendrycksTest-anatomy\": {\n      \"acc\": 0.4222222222222222,\n      \"\
          acc_stderr\": 0.04266763404099582,\n      \"acc_norm\": 0.4222222222222222,\n\
          \      \"acc_norm_stderr\": 0.04266763404099582\n    },\n    \"hendrycksTest-astronomy\"\
          : {\n      \"acc\": 0.34210526315789475,\n      \"acc_stderr\": 0.03860731599316092,\n\
          \      \"acc_norm\": 0.34210526315789475,\n      \"acc_norm_stderr\": 0.03860731599316092\n\
          \    },\n    \"hendrycksTest-business_ethics\": {\n      \"acc\": 0.43,\n\
          \      \"acc_stderr\": 0.049756985195624284,\n      \"acc_norm\": 0.43,\n\
          \      \"acc_norm_stderr\": 0.049756985195624284\n    },\n    \"hendrycksTest-clinical_knowledge\"\
          : {\n      \"acc\": 0.4528301886792453,\n      \"acc_stderr\": 0.030635627957961827,\n\
          \      \"acc_norm\": 0.4528301886792453,\n      \"acc_norm_stderr\": 0.030635627957961827\n\
          \    },\n    \"hendrycksTest-college_biology\": {\n      \"acc\": 0.3611111111111111,\n\
          \      \"acc_stderr\": 0.04016660030451233,\n      \"acc_norm\": 0.3611111111111111,\n\
          \      \"acc_norm_stderr\": 0.04016660030451233\n    },\n    \"hendrycksTest-college_chemistry\"\
          : {\n      \"acc\": 0.25,\n      \"acc_stderr\": 0.04351941398892446,\n\
          \      \"acc_norm\": 0.25,\n      \"acc_norm_stderr\": 0.04351941398892446\n\
          \    },\n    \"hendrycksTest-college_computer_science\": {\n      \"acc\"\
          : 0.37,\n      \"acc_stderr\": 0.04852365870939099,\n      \"acc_norm\"\
          : 0.37,\n      \"acc_norm_stderr\": 0.04852365870939099\n    },\n    \"\
          hendrycksTest-college_mathematics\": {\n      \"acc\": 0.3,\n      \"acc_stderr\"\
          : 0.046056618647183814,\n      \"acc_norm\": 0.3,\n      \"acc_norm_stderr\"\
          : 0.046056618647183814\n    },\n    \"hendrycksTest-college_medicine\":\
          \ {\n      \"acc\": 0.2774566473988439,\n      \"acc_stderr\": 0.03414014007044037,\n\
          \      \"acc_norm\": 0.2774566473988439,\n      \"acc_norm_stderr\": 0.03414014007044037\n\
          \    },\n    \"hendrycksTest-college_physics\": {\n      \"acc\": 0.16666666666666666,\n\
          \      \"acc_stderr\": 0.03708284662416545,\n      \"acc_norm\": 0.16666666666666666,\n\
          \      \"acc_norm_stderr\": 0.03708284662416545\n    },\n    \"hendrycksTest-computer_security\"\
          : {\n      \"acc\": 0.52,\n      \"acc_stderr\": 0.05021167315686779,\n\
          \      \"acc_norm\": 0.52,\n      \"acc_norm_stderr\": 0.05021167315686779\n\
          \    },\n    \"hendrycksTest-conceptual_physics\": {\n      \"acc\": 0.3702127659574468,\n\
          \      \"acc_stderr\": 0.03156564682236785,\n      \"acc_norm\": 0.3702127659574468,\n\
          \      \"acc_norm_stderr\": 0.03156564682236785\n    },\n    \"hendrycksTest-econometrics\"\
          : {\n      \"acc\": 0.23684210526315788,\n      \"acc_stderr\": 0.03999423879281336,\n\
          \      \"acc_norm\": 0.23684210526315788,\n      \"acc_norm_stderr\": 0.03999423879281336\n\
          \    },\n    \"hendrycksTest-electrical_engineering\": {\n      \"acc\"\
          : 0.38620689655172413,\n      \"acc_stderr\": 0.04057324734419036,\n   \
          \   \"acc_norm\": 0.38620689655172413,\n      \"acc_norm_stderr\": 0.04057324734419036\n\
          \    },\n    \"hendrycksTest-elementary_mathematics\": {\n      \"acc\"\
          : 0.26455026455026454,\n      \"acc_stderr\": 0.02271746789770861,\n   \
          \   \"acc_norm\": 0.26455026455026454,\n      \"acc_norm_stderr\": 0.02271746789770861\n\
          \    },\n    \"hendrycksTest-formal_logic\": {\n      \"acc\": 0.25396825396825395,\n\
          \      \"acc_stderr\": 0.03893259610604673,\n      \"acc_norm\": 0.25396825396825395,\n\
          \      \"acc_norm_stderr\": 0.03893259610604673\n    },\n    \"hendrycksTest-global_facts\"\
          : {\n      \"acc\": 0.36,\n      \"acc_stderr\": 0.04824181513244218,\n\
          \      \"acc_norm\": 0.36,\n      \"acc_norm_stderr\": 0.04824181513244218\n\
          \    },\n    \"hendrycksTest-high_school_biology\": {\n      \"acc\": 0.3967741935483871,\n\
          \      \"acc_stderr\": 0.027831231605767948,\n      \"acc_norm\": 0.3967741935483871,\n\
          \      \"acc_norm_stderr\": 0.027831231605767948\n    },\n    \"hendrycksTest-high_school_chemistry\"\
          : {\n      \"acc\": 0.26108374384236455,\n      \"acc_stderr\": 0.030903796952114475,\n\
          \      \"acc_norm\": 0.26108374384236455,\n      \"acc_norm_stderr\": 0.030903796952114475\n\
          \    },\n    \"hendrycksTest-high_school_computer_science\": {\n      \"\
          acc\": 0.45,\n      \"acc_stderr\": 0.049999999999999996,\n      \"acc_norm\"\
          : 0.45,\n      \"acc_norm_stderr\": 0.049999999999999996\n    },\n    \"\
          hendrycksTest-high_school_european_history\": {\n      \"acc\": 0.503030303030303,\n\
          \      \"acc_stderr\": 0.03904272341431857,\n      \"acc_norm\": 0.503030303030303,\n\
          \      \"acc_norm_stderr\": 0.03904272341431857\n    },\n    \"hendrycksTest-high_school_geography\"\
          : {\n      \"acc\": 0.4898989898989899,\n      \"acc_stderr\": 0.035616254886737454,\n\
          \      \"acc_norm\": 0.4898989898989899,\n      \"acc_norm_stderr\": 0.035616254886737454\n\
          \    },\n    \"hendrycksTest-high_school_government_and_politics\": {\n\
          \      \"acc\": 0.5544041450777202,\n      \"acc_stderr\": 0.03587014986075658,\n\
          \      \"acc_norm\": 0.5544041450777202,\n      \"acc_norm_stderr\": 0.03587014986075658\n\
          \    },\n    \"hendrycksTest-high_school_macroeconomics\": {\n      \"acc\"\
          : 0.3717948717948718,\n      \"acc_stderr\": 0.024503472557110936,\n   \
          \   \"acc_norm\": 0.3717948717948718,\n      \"acc_norm_stderr\": 0.024503472557110936\n\
          \    },\n    \"hendrycksTest-high_school_mathematics\": {\n      \"acc\"\
          : 0.22962962962962963,\n      \"acc_stderr\": 0.02564410863926764,\n   \
          \   \"acc_norm\": 0.22962962962962963,\n      \"acc_norm_stderr\": 0.02564410863926764\n\
          \    },\n    \"hendrycksTest-high_school_microeconomics\": {\n      \"acc\"\
          : 0.3235294117647059,\n      \"acc_stderr\": 0.030388353551886845,\n   \
          \   \"acc_norm\": 0.3235294117647059,\n      \"acc_norm_stderr\": 0.030388353551886845\n\
          \    },\n    \"hendrycksTest-high_school_physics\": {\n      \"acc\": 0.24503311258278146,\n\
          \      \"acc_stderr\": 0.03511807571804724,\n      \"acc_norm\": 0.24503311258278146,\n\
          \      \"acc_norm_stderr\": 0.03511807571804724\n    },\n    \"hendrycksTest-high_school_psychology\"\
          : {\n      \"acc\": 0.46972477064220186,\n      \"acc_stderr\": 0.021397988604936965,\n\
          \      \"acc_norm\": 0.46972477064220186,\n      \"acc_norm_stderr\": 0.021397988604936965\n\
          \    },\n    \"hendrycksTest-high_school_statistics\": {\n      \"acc\"\
          : 0.2824074074074074,\n      \"acc_stderr\": 0.030701372111510934,\n   \
          \   \"acc_norm\": 0.2824074074074074,\n      \"acc_norm_stderr\": 0.030701372111510934\n\
          \    },\n    \"hendrycksTest-high_school_us_history\": {\n      \"acc\"\
          : 0.4803921568627451,\n      \"acc_stderr\": 0.03506612560524866,\n    \
          \  \"acc_norm\": 0.4803921568627451,\n      \"acc_norm_stderr\": 0.03506612560524866\n\
          \    },\n    \"hendrycksTest-high_school_world_history\": {\n      \"acc\"\
          : 0.4978902953586498,\n      \"acc_stderr\": 0.032546938018020076,\n   \
          \   \"acc_norm\": 0.4978902953586498,\n      \"acc_norm_stderr\": 0.032546938018020076\n\
          \    },\n    \"hendrycksTest-human_aging\": {\n      \"acc\": 0.4977578475336323,\n\
          \      \"acc_stderr\": 0.033557465352232634,\n      \"acc_norm\": 0.4977578475336323,\n\
          \      \"acc_norm_stderr\": 0.033557465352232634\n    },\n    \"hendrycksTest-human_sexuality\"\
          : {\n      \"acc\": 0.45038167938931295,\n      \"acc_stderr\": 0.04363643698524779,\n\
          \      \"acc_norm\": 0.45038167938931295,\n      \"acc_norm_stderr\": 0.04363643698524779\n\
          \    },\n    \"hendrycksTest-international_law\": {\n      \"acc\": 0.6363636363636364,\n\
          \      \"acc_stderr\": 0.043913262867240704,\n      \"acc_norm\": 0.6363636363636364,\n\
          \      \"acc_norm_stderr\": 0.043913262867240704\n    },\n    \"hendrycksTest-jurisprudence\"\
          : {\n      \"acc\": 0.46296296296296297,\n      \"acc_stderr\": 0.04820403072760626,\n\
          \      \"acc_norm\": 0.46296296296296297,\n      \"acc_norm_stderr\": 0.04820403072760626\n\
          \    },\n    \"hendrycksTest-logical_fallacies\": {\n      \"acc\": 0.43558282208588955,\n\
          \      \"acc_stderr\": 0.038956324641389366,\n      \"acc_norm\": 0.43558282208588955,\n\
          \      \"acc_norm_stderr\": 0.038956324641389366\n    },\n    \"hendrycksTest-machine_learning\"\
          : {\n      \"acc\": 0.29464285714285715,\n      \"acc_stderr\": 0.043270409325787296,\n\
          \      \"acc_norm\": 0.29464285714285715,\n      \"acc_norm_stderr\": 0.043270409325787296\n\
          \    },\n    \"hendrycksTest-management\": {\n      \"acc\": 0.3883495145631068,\n\
          \      \"acc_stderr\": 0.048257293373563895,\n      \"acc_norm\": 0.3883495145631068,\n\
          \      \"acc_norm_stderr\": 0.048257293373563895\n    },\n    \"hendrycksTest-marketing\"\
          : {\n      \"acc\": 0.5811965811965812,\n      \"acc_stderr\": 0.03232128912157792,\n\
          \      \"acc_norm\": 0.5811965811965812,\n      \"acc_norm_stderr\": 0.03232128912157792\n\
          \    },\n    \"hendrycksTest-medical_genetics\": {\n      \"acc\": 0.44,\n\
          \      \"acc_stderr\": 0.049888765156985884,\n      \"acc_norm\": 0.44,\n\
          \      \"acc_norm_stderr\": 0.049888765156985884\n    },\n    \"hendrycksTest-miscellaneous\"\
          : {\n      \"acc\": 0.5376756066411239,\n      \"acc_stderr\": 0.017829131764287184,\n\
          \      \"acc_norm\": 0.5376756066411239,\n      \"acc_norm_stderr\": 0.017829131764287184\n\
          \    },\n    \"hendrycksTest-moral_disputes\": {\n      \"acc\": 0.4393063583815029,\n\
          \      \"acc_stderr\": 0.026720034380514998,\n      \"acc_norm\": 0.4393063583815029,\n\
          \      \"acc_norm_stderr\": 0.026720034380514998\n    },\n    \"hendrycksTest-moral_scenarios\"\
          : {\n      \"acc\": 0.23016759776536314,\n      \"acc_stderr\": 0.014078339253425826,\n\
          \      \"acc_norm\": 0.23016759776536314,\n      \"acc_norm_stderr\": 0.014078339253425826\n\
          \    },\n    \"hendrycksTest-nutrition\": {\n      \"acc\": 0.37254901960784315,\n\
          \      \"acc_stderr\": 0.027684181883302898,\n      \"acc_norm\": 0.37254901960784315,\n\
          \      \"acc_norm_stderr\": 0.027684181883302898\n    },\n    \"hendrycksTest-philosophy\"\
          : {\n      \"acc\": 0.42765273311897106,\n      \"acc_stderr\": 0.02809924077580957,\n\
          \      \"acc_norm\": 0.42765273311897106,\n      \"acc_norm_stderr\": 0.02809924077580957\n\
          \    },\n    \"hendrycksTest-prehistory\": {\n      \"acc\": 0.4506172839506173,\n\
          \      \"acc_stderr\": 0.02768472141565619,\n      \"acc_norm\": 0.4506172839506173,\n\
          \      \"acc_norm_stderr\": 0.02768472141565619\n    },\n    \"hendrycksTest-professional_accounting\"\
          : {\n      \"acc\": 0.31560283687943264,\n      \"acc_stderr\": 0.027724989449509307,\n\
          \      \"acc_norm\": 0.31560283687943264,\n      \"acc_norm_stderr\": 0.027724989449509307\n\
          \    },\n    \"hendrycksTest-professional_law\": {\n      \"acc\": 0.3109517601043025,\n\
          \      \"acc_stderr\": 0.011822252917799207,\n      \"acc_norm\": 0.3109517601043025,\n\
          \      \"acc_norm_stderr\": 0.011822252917799207\n    },\n    \"hendrycksTest-professional_medicine\"\
          : {\n      \"acc\": 0.40808823529411764,\n      \"acc_stderr\": 0.029855261393483924,\n\
          \      \"acc_norm\": 0.40808823529411764,\n      \"acc_norm_stderr\": 0.029855261393483924\n\
          \    },\n    \"hendrycksTest-professional_psychology\": {\n      \"acc\"\
          : 0.4035947712418301,\n      \"acc_stderr\": 0.019848280168401154,\n   \
          \   \"acc_norm\": 0.4035947712418301,\n      \"acc_norm_stderr\": 0.019848280168401154\n\
          \    },\n    \"hendrycksTest-public_relations\": {\n      \"acc\": 0.4090909090909091,\n\
          \      \"acc_stderr\": 0.04709306978661896,\n      \"acc_norm\": 0.4090909090909091,\n\
          \      \"acc_norm_stderr\": 0.04709306978661896\n    },\n    \"hendrycksTest-security_studies\"\
          : {\n      \"acc\": 0.3510204081632653,\n      \"acc_stderr\": 0.030555316755573637,\n\
          \      \"acc_norm\": 0.3510204081632653,\n      \"acc_norm_stderr\": 0.030555316755573637\n\
          \    },\n    \"hendrycksTest-sociology\": {\n      \"acc\": 0.4925373134328358,\n\
          \      \"acc_stderr\": 0.03535140084276719,\n      \"acc_norm\": 0.4925373134328358,\n\
          \      \"acc_norm_stderr\": 0.03535140084276719\n    },\n    \"hendrycksTest-us_foreign_policy\"\
          : {\n      \"acc\": 0.59,\n      \"acc_stderr\": 0.04943110704237101,\n\
          \      \"acc_norm\": 0.59,\n      \"acc_norm_stderr\": 0.04943110704237101\n\
          \    },\n    \"hendrycksTest-virology\": {\n      \"acc\": 0.3614457831325301,\n\
          \      \"acc_stderr\": 0.03740059382029321,\n      \"acc_norm\": 0.3614457831325301,\n\
          \      \"acc_norm_stderr\": 0.03740059382029321\n    },\n    \"hendrycksTest-world_religions\"\
          : {\n      \"acc\": 0.543859649122807,\n      \"acc_stderr\": 0.038200425866029654,\n\
          \      \"acc_norm\": 0.543859649122807,\n      \"acc_norm_stderr\": 0.038200425866029654\n\
          \    }\n  },\n  \"versions\": {\n    \"hendrycksTest-abstract_algebra\"\
          : 1,\n    \"hendrycksTest-anatomy\": 1,\n    \"hendrycksTest-astronomy\"\
          : 1,\n    \"hendrycksTest-business_ethics\": 1,\n    \"hendrycksTest-clinical_knowledge\"\
          : 1,\n    \"hendrycksTest-college_biology\": 1,\n    \"hendrycksTest-college_chemistry\"\
          : 1,\n    \"hendrycksTest-college_computer_science\": 1,\n    \"hendrycksTest-college_mathematics\"\
          : 1,\n    \"hendrycksTest-college_medicine\": 1,\n    \"hendrycksTest-college_physics\"\
          : 1,\n    \"hendrycksTest-computer_security\": 1,\n    \"hendrycksTest-conceptual_physics\"\
          : 1,\n    \"hendrycksTest-econometrics\": 1,\n    \"hendrycksTest-electrical_engineering\"\
          : 1,\n    \"hendrycksTest-elementary_mathematics\": 1,\n    \"hendrycksTest-formal_logic\"\
          : 1,\n    \"hendrycksTest-global_facts\": 1,\n    \"hendrycksTest-high_school_biology\"\
          : 1,\n    \"hendrycksTest-high_school_chemistry\": 1,\n    \"hendrycksTest-high_school_computer_science\"\
          : 1,\n    \"hendrycksTest-high_school_european_history\": 1,\n    \"hendrycksTest-high_school_geography\"\
          : 1,\n    \"hendrycksTest-high_school_government_and_politics\": 1,\n  \
          \  \"hendrycksTest-high_school_macroeconomics\": 1,\n    \"hendrycksTest-high_school_mathematics\"\
          : 1,\n    \"hendrycksTest-high_school_microeconomics\": 1,\n    \"hendrycksTest-high_school_physics\"\
          : 1,\n    \"hendrycksTest-high_school_psychology\": 1,\n    \"hendrycksTest-high_school_statistics\"\
          : 1,\n    \"hendrycksTest-high_school_us_history\": 1,\n    \"hendrycksTest-high_school_world_history\"\
          : 1,\n    \"hendrycksTest-human_aging\": 1,\n    \"hendrycksTest-human_sexuality\"\
          : 1,\n    \"hendrycksTest-international_law\": 1,\n    \"hendrycksTest-jurisprudence\"\
          : 1,\n    \"hendrycksTest-logical_fallacies\": 1,\n    \"hendrycksTest-machine_learning\"\
          : 1,\n    \"hendrycksTest-management\": 1,\n    \"hendrycksTest-marketing\"\
          : 1,\n    \"hendrycksTest-medical_genetics\": 1,\n    \"hendrycksTest-miscellaneous\"\
          : 1,\n    \"hendrycksTest-moral_disputes\": 1,\n    \"hendrycksTest-moral_scenarios\"\
          : 1,\n    \"hendrycksTest-nutrition\": 1,\n    \"hendrycksTest-philosophy\"\
          : 1,\n    \"hendrycksTest-prehistory\": 1,\n    \"hendrycksTest-professional_accounting\"\
          : 1,\n    \"hendrycksTest-professional_law\": 1,\n    \"hendrycksTest-professional_medicine\"\
          : 1,\n    \"hendrycksTest-professional_psychology\": 1,\n    \"hendrycksTest-public_relations\"\
          : 1,\n    \"hendrycksTest-security_studies\": 1,\n    \"hendrycksTest-sociology\"\
          : 1,\n    \"hendrycksTest-us_foreign_policy\": 1,\n    \"hendrycksTest-virology\"\
          : 1,\n    \"hendrycksTest-world_religions\": 1\n  },\n  \"config\": {\n\
          \    \"model\": \"hf-causal\",\n    \"model_args\": \"pretrained=/home/models/orca_mini_v2_7b,trust_remote_code=True,dtype=bfloat16,load_in_8bit=False\"\
          ,\n    \"num_fewshot\": 5,\n    \"batch_size\": \"2\",\n    \"batch_sizes\"\
          : [],\n    \"device\": \"cuda\",\n    \"no_cache\": true,\n    \"limit\"\
          : null,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\": {}\n\
          \  }\n}\n\n```"
        updatedAt: '2023-07-10T15:45:07.056Z'
      numEdits: 0
      reactions: []
    id: 64ac2783ad9d232d9f740d27
    type: comment
  author: migtissera
  content: "Hey Pankaj,\n\n0.39405 is the average of all 57 tests, on the \"acc\"\
    . Version # of the tests is 1.\n\n\nHere's the full test:\n\n```\n{\n  \"results\"\
    : {\n    \"hendrycksTest-abstract_algebra\": {\n      \"acc\": 0.31,\n      \"\
    acc_stderr\": 0.04648231987117316,\n      \"acc_norm\": 0.31,\n      \"acc_norm_stderr\"\
    : 0.04648231987117316\n    },\n    \"hendrycksTest-anatomy\": {\n      \"acc\"\
    : 0.4222222222222222,\n      \"acc_stderr\": 0.04266763404099582,\n      \"acc_norm\"\
    : 0.4222222222222222,\n      \"acc_norm_stderr\": 0.04266763404099582\n    },\n\
    \    \"hendrycksTest-astronomy\": {\n      \"acc\": 0.34210526315789475,\n   \
    \   \"acc_stderr\": 0.03860731599316092,\n      \"acc_norm\": 0.34210526315789475,\n\
    \      \"acc_norm_stderr\": 0.03860731599316092\n    },\n    \"hendrycksTest-business_ethics\"\
    : {\n      \"acc\": 0.43,\n      \"acc_stderr\": 0.049756985195624284,\n     \
    \ \"acc_norm\": 0.43,\n      \"acc_norm_stderr\": 0.049756985195624284\n    },\n\
    \    \"hendrycksTest-clinical_knowledge\": {\n      \"acc\": 0.4528301886792453,\n\
    \      \"acc_stderr\": 0.030635627957961827,\n      \"acc_norm\": 0.4528301886792453,\n\
    \      \"acc_norm_stderr\": 0.030635627957961827\n    },\n    \"hendrycksTest-college_biology\"\
    : {\n      \"acc\": 0.3611111111111111,\n      \"acc_stderr\": 0.04016660030451233,\n\
    \      \"acc_norm\": 0.3611111111111111,\n      \"acc_norm_stderr\": 0.04016660030451233\n\
    \    },\n    \"hendrycksTest-college_chemistry\": {\n      \"acc\": 0.25,\n  \
    \    \"acc_stderr\": 0.04351941398892446,\n      \"acc_norm\": 0.25,\n      \"\
    acc_norm_stderr\": 0.04351941398892446\n    },\n    \"hendrycksTest-college_computer_science\"\
    : {\n      \"acc\": 0.37,\n      \"acc_stderr\": 0.04852365870939099,\n      \"\
    acc_norm\": 0.37,\n      \"acc_norm_stderr\": 0.04852365870939099\n    },\n  \
    \  \"hendrycksTest-college_mathematics\": {\n      \"acc\": 0.3,\n      \"acc_stderr\"\
    : 0.046056618647183814,\n      \"acc_norm\": 0.3,\n      \"acc_norm_stderr\":\
    \ 0.046056618647183814\n    },\n    \"hendrycksTest-college_medicine\": {\n  \
    \    \"acc\": 0.2774566473988439,\n      \"acc_stderr\": 0.03414014007044037,\n\
    \      \"acc_norm\": 0.2774566473988439,\n      \"acc_norm_stderr\": 0.03414014007044037\n\
    \    },\n    \"hendrycksTest-college_physics\": {\n      \"acc\": 0.16666666666666666,\n\
    \      \"acc_stderr\": 0.03708284662416545,\n      \"acc_norm\": 0.16666666666666666,\n\
    \      \"acc_norm_stderr\": 0.03708284662416545\n    },\n    \"hendrycksTest-computer_security\"\
    : {\n      \"acc\": 0.52,\n      \"acc_stderr\": 0.05021167315686779,\n      \"\
    acc_norm\": 0.52,\n      \"acc_norm_stderr\": 0.05021167315686779\n    },\n  \
    \  \"hendrycksTest-conceptual_physics\": {\n      \"acc\": 0.3702127659574468,\n\
    \      \"acc_stderr\": 0.03156564682236785,\n      \"acc_norm\": 0.3702127659574468,\n\
    \      \"acc_norm_stderr\": 0.03156564682236785\n    },\n    \"hendrycksTest-econometrics\"\
    : {\n      \"acc\": 0.23684210526315788,\n      \"acc_stderr\": 0.03999423879281336,\n\
    \      \"acc_norm\": 0.23684210526315788,\n      \"acc_norm_stderr\": 0.03999423879281336\n\
    \    },\n    \"hendrycksTest-electrical_engineering\": {\n      \"acc\": 0.38620689655172413,\n\
    \      \"acc_stderr\": 0.04057324734419036,\n      \"acc_norm\": 0.38620689655172413,\n\
    \      \"acc_norm_stderr\": 0.04057324734419036\n    },\n    \"hendrycksTest-elementary_mathematics\"\
    : {\n      \"acc\": 0.26455026455026454,\n      \"acc_stderr\": 0.02271746789770861,\n\
    \      \"acc_norm\": 0.26455026455026454,\n      \"acc_norm_stderr\": 0.02271746789770861\n\
    \    },\n    \"hendrycksTest-formal_logic\": {\n      \"acc\": 0.25396825396825395,\n\
    \      \"acc_stderr\": 0.03893259610604673,\n      \"acc_norm\": 0.25396825396825395,\n\
    \      \"acc_norm_stderr\": 0.03893259610604673\n    },\n    \"hendrycksTest-global_facts\"\
    : {\n      \"acc\": 0.36,\n      \"acc_stderr\": 0.04824181513244218,\n      \"\
    acc_norm\": 0.36,\n      \"acc_norm_stderr\": 0.04824181513244218\n    },\n  \
    \  \"hendrycksTest-high_school_biology\": {\n      \"acc\": 0.3967741935483871,\n\
    \      \"acc_stderr\": 0.027831231605767948,\n      \"acc_norm\": 0.3967741935483871,\n\
    \      \"acc_norm_stderr\": 0.027831231605767948\n    },\n    \"hendrycksTest-high_school_chemistry\"\
    : {\n      \"acc\": 0.26108374384236455,\n      \"acc_stderr\": 0.030903796952114475,\n\
    \      \"acc_norm\": 0.26108374384236455,\n      \"acc_norm_stderr\": 0.030903796952114475\n\
    \    },\n    \"hendrycksTest-high_school_computer_science\": {\n      \"acc\"\
    : 0.45,\n      \"acc_stderr\": 0.049999999999999996,\n      \"acc_norm\": 0.45,\n\
    \      \"acc_norm_stderr\": 0.049999999999999996\n    },\n    \"hendrycksTest-high_school_european_history\"\
    : {\n      \"acc\": 0.503030303030303,\n      \"acc_stderr\": 0.03904272341431857,\n\
    \      \"acc_norm\": 0.503030303030303,\n      \"acc_norm_stderr\": 0.03904272341431857\n\
    \    },\n    \"hendrycksTest-high_school_geography\": {\n      \"acc\": 0.4898989898989899,\n\
    \      \"acc_stderr\": 0.035616254886737454,\n      \"acc_norm\": 0.4898989898989899,\n\
    \      \"acc_norm_stderr\": 0.035616254886737454\n    },\n    \"hendrycksTest-high_school_government_and_politics\"\
    : {\n      \"acc\": 0.5544041450777202,\n      \"acc_stderr\": 0.03587014986075658,\n\
    \      \"acc_norm\": 0.5544041450777202,\n      \"acc_norm_stderr\": 0.03587014986075658\n\
    \    },\n    \"hendrycksTest-high_school_macroeconomics\": {\n      \"acc\": 0.3717948717948718,\n\
    \      \"acc_stderr\": 0.024503472557110936,\n      \"acc_norm\": 0.3717948717948718,\n\
    \      \"acc_norm_stderr\": 0.024503472557110936\n    },\n    \"hendrycksTest-high_school_mathematics\"\
    : {\n      \"acc\": 0.22962962962962963,\n      \"acc_stderr\": 0.02564410863926764,\n\
    \      \"acc_norm\": 0.22962962962962963,\n      \"acc_norm_stderr\": 0.02564410863926764\n\
    \    },\n    \"hendrycksTest-high_school_microeconomics\": {\n      \"acc\": 0.3235294117647059,\n\
    \      \"acc_stderr\": 0.030388353551886845,\n      \"acc_norm\": 0.3235294117647059,\n\
    \      \"acc_norm_stderr\": 0.030388353551886845\n    },\n    \"hendrycksTest-high_school_physics\"\
    : {\n      \"acc\": 0.24503311258278146,\n      \"acc_stderr\": 0.03511807571804724,\n\
    \      \"acc_norm\": 0.24503311258278146,\n      \"acc_norm_stderr\": 0.03511807571804724\n\
    \    },\n    \"hendrycksTest-high_school_psychology\": {\n      \"acc\": 0.46972477064220186,\n\
    \      \"acc_stderr\": 0.021397988604936965,\n      \"acc_norm\": 0.46972477064220186,\n\
    \      \"acc_norm_stderr\": 0.021397988604936965\n    },\n    \"hendrycksTest-high_school_statistics\"\
    : {\n      \"acc\": 0.2824074074074074,\n      \"acc_stderr\": 0.030701372111510934,\n\
    \      \"acc_norm\": 0.2824074074074074,\n      \"acc_norm_stderr\": 0.030701372111510934\n\
    \    },\n    \"hendrycksTest-high_school_us_history\": {\n      \"acc\": 0.4803921568627451,\n\
    \      \"acc_stderr\": 0.03506612560524866,\n      \"acc_norm\": 0.4803921568627451,\n\
    \      \"acc_norm_stderr\": 0.03506612560524866\n    },\n    \"hendrycksTest-high_school_world_history\"\
    : {\n      \"acc\": 0.4978902953586498,\n      \"acc_stderr\": 0.032546938018020076,\n\
    \      \"acc_norm\": 0.4978902953586498,\n      \"acc_norm_stderr\": 0.032546938018020076\n\
    \    },\n    \"hendrycksTest-human_aging\": {\n      \"acc\": 0.4977578475336323,\n\
    \      \"acc_stderr\": 0.033557465352232634,\n      \"acc_norm\": 0.4977578475336323,\n\
    \      \"acc_norm_stderr\": 0.033557465352232634\n    },\n    \"hendrycksTest-human_sexuality\"\
    : {\n      \"acc\": 0.45038167938931295,\n      \"acc_stderr\": 0.04363643698524779,\n\
    \      \"acc_norm\": 0.45038167938931295,\n      \"acc_norm_stderr\": 0.04363643698524779\n\
    \    },\n    \"hendrycksTest-international_law\": {\n      \"acc\": 0.6363636363636364,\n\
    \      \"acc_stderr\": 0.043913262867240704,\n      \"acc_norm\": 0.6363636363636364,\n\
    \      \"acc_norm_stderr\": 0.043913262867240704\n    },\n    \"hendrycksTest-jurisprudence\"\
    : {\n      \"acc\": 0.46296296296296297,\n      \"acc_stderr\": 0.04820403072760626,\n\
    \      \"acc_norm\": 0.46296296296296297,\n      \"acc_norm_stderr\": 0.04820403072760626\n\
    \    },\n    \"hendrycksTest-logical_fallacies\": {\n      \"acc\": 0.43558282208588955,\n\
    \      \"acc_stderr\": 0.038956324641389366,\n      \"acc_norm\": 0.43558282208588955,\n\
    \      \"acc_norm_stderr\": 0.038956324641389366\n    },\n    \"hendrycksTest-machine_learning\"\
    : {\n      \"acc\": 0.29464285714285715,\n      \"acc_stderr\": 0.043270409325787296,\n\
    \      \"acc_norm\": 0.29464285714285715,\n      \"acc_norm_stderr\": 0.043270409325787296\n\
    \    },\n    \"hendrycksTest-management\": {\n      \"acc\": 0.3883495145631068,\n\
    \      \"acc_stderr\": 0.048257293373563895,\n      \"acc_norm\": 0.3883495145631068,\n\
    \      \"acc_norm_stderr\": 0.048257293373563895\n    },\n    \"hendrycksTest-marketing\"\
    : {\n      \"acc\": 0.5811965811965812,\n      \"acc_stderr\": 0.03232128912157792,\n\
    \      \"acc_norm\": 0.5811965811965812,\n      \"acc_norm_stderr\": 0.03232128912157792\n\
    \    },\n    \"hendrycksTest-medical_genetics\": {\n      \"acc\": 0.44,\n   \
    \   \"acc_stderr\": 0.049888765156985884,\n      \"acc_norm\": 0.44,\n      \"\
    acc_norm_stderr\": 0.049888765156985884\n    },\n    \"hendrycksTest-miscellaneous\"\
    : {\n      \"acc\": 0.5376756066411239,\n      \"acc_stderr\": 0.017829131764287184,\n\
    \      \"acc_norm\": 0.5376756066411239,\n      \"acc_norm_stderr\": 0.017829131764287184\n\
    \    },\n    \"hendrycksTest-moral_disputes\": {\n      \"acc\": 0.4393063583815029,\n\
    \      \"acc_stderr\": 0.026720034380514998,\n      \"acc_norm\": 0.4393063583815029,\n\
    \      \"acc_norm_stderr\": 0.026720034380514998\n    },\n    \"hendrycksTest-moral_scenarios\"\
    : {\n      \"acc\": 0.23016759776536314,\n      \"acc_stderr\": 0.014078339253425826,\n\
    \      \"acc_norm\": 0.23016759776536314,\n      \"acc_norm_stderr\": 0.014078339253425826\n\
    \    },\n    \"hendrycksTest-nutrition\": {\n      \"acc\": 0.37254901960784315,\n\
    \      \"acc_stderr\": 0.027684181883302898,\n      \"acc_norm\": 0.37254901960784315,\n\
    \      \"acc_norm_stderr\": 0.027684181883302898\n    },\n    \"hendrycksTest-philosophy\"\
    : {\n      \"acc\": 0.42765273311897106,\n      \"acc_stderr\": 0.02809924077580957,\n\
    \      \"acc_norm\": 0.42765273311897106,\n      \"acc_norm_stderr\": 0.02809924077580957\n\
    \    },\n    \"hendrycksTest-prehistory\": {\n      \"acc\": 0.4506172839506173,\n\
    \      \"acc_stderr\": 0.02768472141565619,\n      \"acc_norm\": 0.4506172839506173,\n\
    \      \"acc_norm_stderr\": 0.02768472141565619\n    },\n    \"hendrycksTest-professional_accounting\"\
    : {\n      \"acc\": 0.31560283687943264,\n      \"acc_stderr\": 0.027724989449509307,\n\
    \      \"acc_norm\": 0.31560283687943264,\n      \"acc_norm_stderr\": 0.027724989449509307\n\
    \    },\n    \"hendrycksTest-professional_law\": {\n      \"acc\": 0.3109517601043025,\n\
    \      \"acc_stderr\": 0.011822252917799207,\n      \"acc_norm\": 0.3109517601043025,\n\
    \      \"acc_norm_stderr\": 0.011822252917799207\n    },\n    \"hendrycksTest-professional_medicine\"\
    : {\n      \"acc\": 0.40808823529411764,\n      \"acc_stderr\": 0.029855261393483924,\n\
    \      \"acc_norm\": 0.40808823529411764,\n      \"acc_norm_stderr\": 0.029855261393483924\n\
    \    },\n    \"hendrycksTest-professional_psychology\": {\n      \"acc\": 0.4035947712418301,\n\
    \      \"acc_stderr\": 0.019848280168401154,\n      \"acc_norm\": 0.4035947712418301,\n\
    \      \"acc_norm_stderr\": 0.019848280168401154\n    },\n    \"hendrycksTest-public_relations\"\
    : {\n      \"acc\": 0.4090909090909091,\n      \"acc_stderr\": 0.04709306978661896,\n\
    \      \"acc_norm\": 0.4090909090909091,\n      \"acc_norm_stderr\": 0.04709306978661896\n\
    \    },\n    \"hendrycksTest-security_studies\": {\n      \"acc\": 0.3510204081632653,\n\
    \      \"acc_stderr\": 0.030555316755573637,\n      \"acc_norm\": 0.3510204081632653,\n\
    \      \"acc_norm_stderr\": 0.030555316755573637\n    },\n    \"hendrycksTest-sociology\"\
    : {\n      \"acc\": 0.4925373134328358,\n      \"acc_stderr\": 0.03535140084276719,\n\
    \      \"acc_norm\": 0.4925373134328358,\n      \"acc_norm_stderr\": 0.03535140084276719\n\
    \    },\n    \"hendrycksTest-us_foreign_policy\": {\n      \"acc\": 0.59,\n  \
    \    \"acc_stderr\": 0.04943110704237101,\n      \"acc_norm\": 0.59,\n      \"\
    acc_norm_stderr\": 0.04943110704237101\n    },\n    \"hendrycksTest-virology\"\
    : {\n      \"acc\": 0.3614457831325301,\n      \"acc_stderr\": 0.03740059382029321,\n\
    \      \"acc_norm\": 0.3614457831325301,\n      \"acc_norm_stderr\": 0.03740059382029321\n\
    \    },\n    \"hendrycksTest-world_religions\": {\n      \"acc\": 0.543859649122807,\n\
    \      \"acc_stderr\": 0.038200425866029654,\n      \"acc_norm\": 0.543859649122807,\n\
    \      \"acc_norm_stderr\": 0.038200425866029654\n    }\n  },\n  \"versions\"\
    : {\n    \"hendrycksTest-abstract_algebra\": 1,\n    \"hendrycksTest-anatomy\"\
    : 1,\n    \"hendrycksTest-astronomy\": 1,\n    \"hendrycksTest-business_ethics\"\
    : 1,\n    \"hendrycksTest-clinical_knowledge\": 1,\n    \"hendrycksTest-college_biology\"\
    : 1,\n    \"hendrycksTest-college_chemistry\": 1,\n    \"hendrycksTest-college_computer_science\"\
    : 1,\n    \"hendrycksTest-college_mathematics\": 1,\n    \"hendrycksTest-college_medicine\"\
    : 1,\n    \"hendrycksTest-college_physics\": 1,\n    \"hendrycksTest-computer_security\"\
    : 1,\n    \"hendrycksTest-conceptual_physics\": 1,\n    \"hendrycksTest-econometrics\"\
    : 1,\n    \"hendrycksTest-electrical_engineering\": 1,\n    \"hendrycksTest-elementary_mathematics\"\
    : 1,\n    \"hendrycksTest-formal_logic\": 1,\n    \"hendrycksTest-global_facts\"\
    : 1,\n    \"hendrycksTest-high_school_biology\": 1,\n    \"hendrycksTest-high_school_chemistry\"\
    : 1,\n    \"hendrycksTest-high_school_computer_science\": 1,\n    \"hendrycksTest-high_school_european_history\"\
    : 1,\n    \"hendrycksTest-high_school_geography\": 1,\n    \"hendrycksTest-high_school_government_and_politics\"\
    : 1,\n    \"hendrycksTest-high_school_macroeconomics\": 1,\n    \"hendrycksTest-high_school_mathematics\"\
    : 1,\n    \"hendrycksTest-high_school_microeconomics\": 1,\n    \"hendrycksTest-high_school_physics\"\
    : 1,\n    \"hendrycksTest-high_school_psychology\": 1,\n    \"hendrycksTest-high_school_statistics\"\
    : 1,\n    \"hendrycksTest-high_school_us_history\": 1,\n    \"hendrycksTest-high_school_world_history\"\
    : 1,\n    \"hendrycksTest-human_aging\": 1,\n    \"hendrycksTest-human_sexuality\"\
    : 1,\n    \"hendrycksTest-international_law\": 1,\n    \"hendrycksTest-jurisprudence\"\
    : 1,\n    \"hendrycksTest-logical_fallacies\": 1,\n    \"hendrycksTest-machine_learning\"\
    : 1,\n    \"hendrycksTest-management\": 1,\n    \"hendrycksTest-marketing\": 1,\n\
    \    \"hendrycksTest-medical_genetics\": 1,\n    \"hendrycksTest-miscellaneous\"\
    : 1,\n    \"hendrycksTest-moral_disputes\": 1,\n    \"hendrycksTest-moral_scenarios\"\
    : 1,\n    \"hendrycksTest-nutrition\": 1,\n    \"hendrycksTest-philosophy\": 1,\n\
    \    \"hendrycksTest-prehistory\": 1,\n    \"hendrycksTest-professional_accounting\"\
    : 1,\n    \"hendrycksTest-professional_law\": 1,\n    \"hendrycksTest-professional_medicine\"\
    : 1,\n    \"hendrycksTest-professional_psychology\": 1,\n    \"hendrycksTest-public_relations\"\
    : 1,\n    \"hendrycksTest-security_studies\": 1,\n    \"hendrycksTest-sociology\"\
    : 1,\n    \"hendrycksTest-us_foreign_policy\": 1,\n    \"hendrycksTest-virology\"\
    : 1,\n    \"hendrycksTest-world_religions\": 1\n  },\n  \"config\": {\n    \"\
    model\": \"hf-causal\",\n    \"model_args\": \"pretrained=/home/models/orca_mini_v2_7b,trust_remote_code=True,dtype=bfloat16,load_in_8bit=False\"\
    ,\n    \"num_fewshot\": 5,\n    \"batch_size\": \"2\",\n    \"batch_sizes\": [],\n\
    \    \"device\": \"cuda\",\n    \"no_cache\": true,\n    \"limit\": null,\n  \
    \  \"bootstrap_iters\": 100000,\n    \"description_dict\": {}\n  }\n}\n\n```"
  created_at: 2023-07-10 14:45:07+00:00
  edited: false
  hidden: false
  id: 64ac2783ad9d232d9f740d27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
      fullname: Pankaj Mathur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: pankajmathur
      type: user
    createdAt: '2023-07-10T18:28:11.000Z'
    data:
      edited: false
      editors:
      - pankajmathur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3098853528499603
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
          fullname: Pankaj Mathur
          isHf: false
          isPro: true
          name: pankajmathur
          type: user
        html: '<p>Awesome, Here is output which I got, when I ran the test:</p>

          <p>{<br>  "results": {<br>    "hendrycksTest-abstract_algebra": {<br>      "acc":
          0.33,<br>      "acc_stderr": 0.047258156262526045,<br>      "acc_norm":
          0.33,<br>      "acc_norm_stderr": 0.047258156262526045<br>    },<br>    "hendrycksTest-anatomy":
          {<br>      "acc": 0.4148148148148148,<br>      "acc_stderr": 0.04256193767901408,<br>      "acc_norm":
          0.4148148148148148,<br>      "acc_norm_stderr": 0.04256193767901408<br>    },<br>    "hendrycksTest-astronomy":
          {<br>      "acc": 0.34210526315789475,<br>      "acc_stderr": 0.03860731599316092,<br>      "acc_norm":
          0.34210526315789475,<br>      "acc_norm_stderr": 0.03860731599316092<br>    },<br>    "hendrycksTest-business_ethics":
          {<br>      "acc": 0.44,<br>      "acc_stderr": 0.04988876515698589,<br>      "acc_norm":
          0.44,<br>      "acc_norm_stderr": 0.04988876515698589<br>    },<br>    "hendrycksTest-clinical_knowledge":
          {<br>      "acc": 0.45660377358490567,<br>      "acc_stderr": 0.03065674869673943,<br>      "acc_norm":
          0.45660377358490567,<br>      "acc_norm_stderr": 0.03065674869673943<br>    },<br>    "hendrycksTest-college_biology":
          {<br>      "acc": 0.3541666666666667,<br>      "acc_stderr": 0.039994111357535424,<br>      "acc_norm":
          0.3541666666666667,<br>      "acc_norm_stderr": 0.039994111357535424<br>    },<br>    "hendrycksTest-college_chemistry":
          {<br>      "acc": 0.26,<br>      "acc_stderr": 0.04408440022768077,<br>      "acc_norm":
          0.26,<br>      "acc_norm_stderr": 0.04408440022768077<br>    },<br>    "hendrycksTest-college_computer_science":
          {<br>      "acc": 0.37,<br>      "acc_stderr": 0.04852365870939099,<br>      "acc_norm":
          0.37,<br>      "acc_norm_stderr": 0.04852365870939099<br>    },<br>    "hendrycksTest-college_mathematics":
          {<br>      "acc": 0.3,<br>      "acc_stderr": 0.046056618647183814,<br>      "acc_norm":
          0.3,<br>      "acc_norm_stderr": 0.046056618647183814<br>    },<br>    "hendrycksTest-college_medicine":
          {<br>      "acc": 0.2774566473988439,<br>      "acc_stderr": 0.03414014007044037,<br>      "acc_norm":
          0.2774566473988439,<br>      "acc_norm_stderr": 0.03414014007044037<br>    },<br>    "hendrycksTest-college_physics":
          {<br>      "acc": 0.13725490196078433,<br>      "acc_stderr": 0.03424084669891523,<br>      "acc_norm":
          0.13725490196078433,<br>      "acc_norm_stderr": 0.03424084669891523<br>    },<br>    "hendrycksTest-computer_security":
          {<br>      "acc": 0.53,<br>      "acc_stderr": 0.05016135580465919,<br>      "acc_norm":
          0.53,<br>      "acc_norm_stderr": 0.05016135580465919<br>    },<br>    "hendrycksTest-conceptual_physics":
          {<br>      "acc": 0.35319148936170214,<br>      "acc_stderr": 0.031245325202761926,<br>      "acc_norm":
          0.35319148936170214,<br>      "acc_norm_stderr": 0.031245325202761926<br>    },<br>    "hendrycksTest-econometrics":
          {<br>      "acc": 0.21929824561403508,<br>      "acc_stderr": 0.03892431106518753,<br>      "acc_norm":
          0.21929824561403508,<br>      "acc_norm_stderr": 0.03892431106518753<br>    },<br>    "hendrycksTest-electrical_engineering":
          {<br>      "acc": 0.38620689655172413,<br>      "acc_stderr": 0.04057324734419036,<br>      "acc_norm":
          0.38620689655172413,<br>      "acc_norm_stderr": 0.04057324734419036<br>    },<br>    "hendrycksTest-elementary_mathematics":
          {<br>      "acc": 0.2619047619047619,<br>      "acc_stderr": 0.02264421261552521,<br>      "acc_norm":
          0.2619047619047619,<br>      "acc_norm_stderr": 0.02264421261552521<br>    },<br>    "hendrycksTest-formal_logic":
          {<br>      "acc": 0.24603174603174602,<br>      "acc_stderr": 0.03852273364924316,<br>      "acc_norm":
          0.24603174603174602,<br>      "acc_norm_stderr": 0.03852273364924316<br>    },<br>    "hendrycksTest-global_facts":
          {<br>      "acc": 0.37,<br>      "acc_stderr": 0.048523658709391,<br>      "acc_norm":
          0.37,<br>      "acc_norm_stderr": 0.048523658709391<br>    },<br>    "hendrycksTest-high_school_biology":
          {<br>      "acc": 0.4096774193548387,<br>      "acc_stderr": 0.02797605491534736,<br>      "acc_norm":
          0.4096774193548387,<br>      "acc_norm_stderr": 0.02797605491534736<br>    },<br>    "hendrycksTest-high_school_chemistry":
          {<br>      "acc": 0.270935960591133,<br>      "acc_stderr": 0.03127090713297698,<br>      "acc_norm":
          0.270935960591133,<br>      "acc_norm_stderr": 0.03127090713297698<br>    },<br>    "hendrycksTest-high_school_computer_science":
          {<br>      "acc": 0.46,<br>      "acc_stderr": 0.05009082659620332,<br>      "acc_norm":
          0.46,<br>      "acc_norm_stderr": 0.05009082659620332<br>    },<br>    "hendrycksTest-high_school_european_history":
          {<br>      "acc": 0.4909090909090909,<br>      "acc_stderr": 0.0390369864774844,<br>      "acc_norm":
          0.4909090909090909,<br>      "acc_norm_stderr": 0.0390369864774844<br>    },<br>    "hendrycksTest-high_school_geography":
          {<br>      "acc": 0.4898989898989899,<br>      "acc_stderr": 0.035616254886737454,<br>      "acc_norm":
          0.4898989898989899,<br>      "acc_norm_stderr": 0.035616254886737454<br>    },<br>    "hendrycksTest-high_school_government_and_politics":
          {<br>      "acc": 0.5699481865284974,<br>      "acc_stderr": 0.035729543331448094,<br>      "acc_norm":
          0.5699481865284974,<br>      "acc_norm_stderr": 0.035729543331448094<br>    },<br>    "hendrycksTest-high_school_macroeconomics":
          {<br>      "acc": 0.36666666666666664,<br>      "acc_stderr": 0.02443301646605246,<br>      "acc_norm":
          0.36666666666666664,<br>      "acc_norm_stderr": 0.02443301646605246<br>    },<br>    "hendrycksTest-high_school_mathematics":
          {<br>      "acc": 0.23333333333333334,<br>      "acc_stderr": 0.025787874220959302,<br>      "acc_norm":
          0.23333333333333334,<br>      "acc_norm_stderr": 0.025787874220959302<br>    },<br>    "hendrycksTest-high_school_microeconomics":
          {<br>      "acc": 0.3403361344537815,<br>      "acc_stderr": 0.030778057422931673,<br>      "acc_norm":
          0.3403361344537815,<br>      "acc_norm_stderr": 0.030778057422931673<br>    },<br>    "hendrycksTest-high_school_physics":
          {<br>      "acc": 0.26490066225165565,<br>      "acc_stderr": 0.03603038545360385,<br>      "acc_norm":
          0.26490066225165565,<br>      "acc_norm_stderr": 0.03603038545360385<br>    },<br>    "hendrycksTest-high_school_psychology":
          {<br>      "acc": 0.48440366972477067,<br>      "acc_stderr": 0.02142689153920805,<br>      "acc_norm":
          0.48440366972477067,<br>      "acc_norm_stderr": 0.02142689153920805<br>    },<br>    "hendrycksTest-high_school_statistics":
          {<br>      "acc": 0.30092592592592593,<br>      "acc_stderr": 0.03128039084329882,<br>      "acc_norm":
          0.30092592592592593,<br>      "acc_norm_stderr": 0.03128039084329882<br>    },<br>    "hendrycksTest-high_school_us_history":
          {<br>      "acc": 0.4852941176470588,<br>      "acc_stderr": 0.03507793834791324,<br>      "acc_norm":
          0.4852941176470588,<br>      "acc_norm_stderr": 0.03507793834791324<br>    },<br>    "hendrycksTest-high_school_world_history":
          {<br>      "acc": 0.4936708860759494,<br>      "acc_stderr": 0.03254462010767859,<br>      "acc_norm":
          0.4936708860759494,<br>      "acc_norm_stderr": 0.03254462010767859<br>    },<br>    "hendrycksTest-human_aging":
          {<br>      "acc": 0.4977578475336323,<br>      "acc_stderr": 0.033557465352232634,<br>      "acc_norm":
          0.4977578475336323,<br>      "acc_norm_stderr": 0.033557465352232634<br>    },<br>    "hendrycksTest-human_sexuality":
          {<br>      "acc": 0.4580152671755725,<br>      "acc_stderr": 0.04369802690578756,<br>      "acc_norm":
          0.4580152671755725,<br>      "acc_norm_stderr": 0.04369802690578756<br>    },<br>    "hendrycksTest-international_law":
          {<br>      "acc": 0.6198347107438017,<br>      "acc_stderr": 0.04431324501968431,<br>      "acc_norm":
          0.6198347107438017,<br>      "acc_norm_stderr": 0.04431324501968431<br>    },<br>    "hendrycksTest-jurisprudence":
          {<br>      "acc": 0.48148148148148145,<br>      "acc_stderr": 0.04830366024635331,<br>      "acc_norm":
          0.48148148148148145,<br>      "acc_norm_stderr": 0.04830366024635331<br>    },<br>    "hendrycksTest-logical_fallacies":
          {<br>      "acc": 0.44171779141104295,<br>      "acc_stderr": 0.03901591825836184,<br>      "acc_norm":
          0.44171779141104295,<br>      "acc_norm_stderr": 0.03901591825836184<br>    },<br>    "hendrycksTest-machine_learning":
          {<br>      "acc": 0.29464285714285715,<br>      "acc_stderr": 0.04327040932578729,<br>      "acc_norm":
          0.29464285714285715,<br>      "acc_norm_stderr": 0.04327040932578729<br>    },<br>    "hendrycksTest-management":
          {<br>      "acc": 0.4077669902912621,<br>      "acc_stderr": 0.048657775704107675,<br>      "acc_norm":
          0.4077669902912621,<br>      "acc_norm_stderr": 0.048657775704107675<br>    },<br>    "hendrycksTest-marketing":
          {<br>      "acc": 0.5683760683760684,<br>      "acc_stderr": 0.0324483553531149,<br>      "acc_norm":
          0.5683760683760684,<br>      "acc_norm_stderr": 0.0324483553531149<br>    },<br>    "hendrycksTest-medical_genetics":
          {<br>      "acc": 0.45,<br>      "acc_stderr": 0.05,<br>      "acc_norm":
          0.45,<br>      "acc_norm_stderr": 0.05<br>    },<br>    "hendrycksTest-miscellaneous":
          {<br>      "acc": 0.5376756066411239,<br>      "acc_stderr": 0.017829131764287184,<br>      "acc_norm":
          0.5376756066411239,<br>      "acc_norm_stderr": 0.017829131764287184<br>    },<br>    "hendrycksTest-moral_disputes":
          {<br>      "acc": 0.4479768786127168,<br>      "acc_stderr": 0.02677299065336182,<br>      "acc_norm":
          0.4479768786127168,<br>      "acc_norm_stderr": 0.02677299065336182<br>    },<br>    "hendrycksTest-moral_scenarios":
          {<br>      "acc": 0.23910614525139665,<br>      "acc_stderr": 0.01426555419233116,<br>      "acc_norm":
          0.23910614525139665,<br>      "acc_norm_stderr": 0.01426555419233116<br>    },<br>    "hendrycksTest-nutrition":
          {<br>      "acc": 0.3790849673202614,<br>      "acc_stderr": 0.02778014120702334,<br>      "acc_norm":
          0.3790849673202614,<br>      "acc_norm_stderr": 0.02778014120702334<br>    },<br>    "hendrycksTest-philosophy":
          {<br>      "acc": 0.42443729903536975,<br>      "acc_stderr": 0.028071928247946205,<br>      "acc_norm":
          0.42443729903536975,<br>      "acc_norm_stderr": 0.028071928247946205<br>    },<br>    "hendrycksTest-prehistory":
          {<br>      "acc": 0.4537037037037037,<br>      "acc_stderr": 0.027701228468542602,<br>      "acc_norm":
          0.4537037037037037,<br>      "acc_norm_stderr": 0.027701228468542602<br>    },<br>    "hendrycksTest-professional_accounting":
          {<br>      "acc": 0.3120567375886525,<br>      "acc_stderr": 0.02764012054516993,<br>      "acc_norm":
          0.3120567375886525,<br>      "acc_norm_stderr": 0.02764012054516993<br>    },<br>    "hendrycksTest-professional_law":
          {<br>      "acc": 0.3109517601043025,<br>      "acc_stderr": 0.011822252917799205,<br>      "acc_norm":
          0.3109517601043025,<br>      "acc_norm_stderr": 0.011822252917799205<br>    },<br>    "hendrycksTest-professional_medicine":
          {<br>      "acc": 0.40808823529411764,<br>      "acc_stderr": 0.029855261393483924,<br>      "acc_norm":
          0.40808823529411764,<br>      "acc_norm_stderr": 0.029855261393483924<br>    },<br>    "hendrycksTest-professional_psychology":
          {<br>      "acc": 0.4068627450980392,<br>      "acc_stderr": 0.019873802005061173,<br>      "acc_norm":
          0.4068627450980392,<br>      "acc_norm_stderr": 0.019873802005061173<br>    },<br>    "hendrycksTest-public_relations":
          {<br>      "acc": 0.4,<br>      "acc_stderr": 0.0469237132203465,<br>      "acc_norm":
          0.4,<br>      "acc_norm_stderr": 0.0469237132203465<br>    },<br>    "hendrycksTest-security_studies":
          {<br>      "acc": 0.3510204081632653,<br>      "acc_stderr": 0.030555316755573637,<br>      "acc_norm":
          0.3510204081632653,<br>      "acc_norm_stderr": 0.030555316755573637<br>    },<br>    "hendrycksTest-sociology":
          {<br>      "acc": 0.4975124378109453,<br>      "acc_stderr": 0.03535490150137289,<br>      "acc_norm":
          0.4975124378109453,<br>      "acc_norm_stderr": 0.03535490150137289<br>    },<br>    "hendrycksTest-us_foreign_policy":
          {<br>      "acc": 0.57,<br>      "acc_stderr": 0.04975698519562428,<br>      "acc_norm":
          0.57,<br>      "acc_norm_stderr": 0.04975698519562428<br>    },<br>    "hendrycksTest-virology":
          {<br>      "acc": 0.3313253012048193,<br>      "acc_stderr": 0.036643147772880844,<br>      "acc_norm":
          0.3313253012048193,<br>      "acc_norm_stderr": 0.036643147772880844<br>    },<br>    "hendrycksTest-world_religions":
          {<br>      "acc": 0.5497076023391813,<br>      "acc_stderr": 0.03815827365913237,<br>      "acc_norm":
          0.5497076023391813,<br>      "acc_norm_stderr": 0.03815827365913237<br>    }<br>  },<br>  "versions":
          {<br>    "hendrycksTest-abstract_algebra": 1,<br>    "hendrycksTest-anatomy":
          1,<br>    "hendrycksTest-astronomy": 1,<br>    "hendrycksTest-business_ethics":
          1,<br>    "hendrycksTest-clinical_knowledge": 1,<br>    "hendrycksTest-college_biology":
          1,<br>    "hendrycksTest-college_chemistry": 1,<br>    "hendrycksTest-college_computer_science":
          1,<br>    "hendrycksTest-college_mathematics": 1,<br>    "hendrycksTest-college_medicine":
          1,<br>    "hendrycksTest-college_physics": 1,<br>    "hendrycksTest-computer_security":
          1,<br>    "hendrycksTest-conceptual_physics": 1,<br>    "hendrycksTest-econometrics":
          1,<br>    "hendrycksTest-electrical_engineering": 1,<br>    "hendrycksTest-elementary_mathematics":
          1,<br>    "hendrycksTest-formal_logic": 1,<br>    "hendrycksTest-global_facts":
          1,<br>    "hendrycksTest-high_school_biology": 1,<br>    "hendrycksTest-high_school_chemistry":
          1,<br>    "hendrycksTest-high_school_computer_science": 1,<br>    "hendrycksTest-high_school_european_history":
          1,<br>    "hendrycksTest-high_school_geography": 1,<br>    "hendrycksTest-high_school_government_and_politics":
          1,<br>    "hendrycksTest-high_school_macroeconomics": 1,<br>    "hendrycksTest-high_school_mathematics":
          1,<br>    "hendrycksTest-high_school_microeconomics": 1,<br>    "hendrycksTest-high_school_physics":
          1,<br>    "hendrycksTest-high_school_psychology": 1,<br>    "hendrycksTest-high_school_statistics":
          1,<br>    "hendrycksTest-high_school_us_history": 1,<br>    "hendrycksTest-high_school_world_history":
          1,<br>    "hendrycksTest-human_aging": 1,<br>    "hendrycksTest-human_sexuality":
          1,<br>    "hendrycksTest-international_law": 1,<br>    "hendrycksTest-jurisprudence":
          1,<br>    "hendrycksTest-logical_fallacies": 1,<br>    "hendrycksTest-machine_learning":
          1,<br>    "hendrycksTest-management": 1,<br>    "hendrycksTest-marketing":
          1,<br>    "hendrycksTest-medical_genetics": 1,<br>    "hendrycksTest-miscellaneous":
          1,<br>    "hendrycksTest-moral_disputes": 1,<br>    "hendrycksTest-moral_scenarios":
          1,<br>    "hendrycksTest-nutrition": 1,<br>    "hendrycksTest-philosophy":
          1,<br>    "hendrycksTest-prehistory": 1,<br>    "hendrycksTest-professional_accounting":
          1,<br>    "hendrycksTest-professional_law": 1,<br>    "hendrycksTest-professional_medicine":
          1,<br>    "hendrycksTest-professional_psychology": 1,<br>    "hendrycksTest-public_relations":
          1,<br>    "hendrycksTest-security_studies": 1,<br>    "hendrycksTest-sociology":
          1,<br>    "hendrycksTest-us_foreign_policy": 1,<br>    "hendrycksTest-virology":
          1,<br>    "hendrycksTest-world_religions": 1<br>  },<br>  "config": {<br>    "model":
          "hf-causal",<br>    "model_args": "pretrained=psmathur/orca_mini_v2_7b",<br>    "num_fewshot":
          5,<br>    "batch_size": "12",<br>    "batch_sizes": [],<br>    "device":
          "cuda:0",<br>    "no_cache": false,<br>    "limit": null,<br>    "bootstrap_iters":
          100000,<br>    "description_dict": {}<br>  }<br>}</p>

          '
        raw: "Awesome, Here is output which I got, when I ran the test:\n\n{\n  \"\
          results\": {\n    \"hendrycksTest-abstract_algebra\": {\n      \"acc\":\
          \ 0.33,\n      \"acc_stderr\": 0.047258156262526045,\n      \"acc_norm\"\
          : 0.33,\n      \"acc_norm_stderr\": 0.047258156262526045\n    },\n    \"\
          hendrycksTest-anatomy\": {\n      \"acc\": 0.4148148148148148,\n      \"\
          acc_stderr\": 0.04256193767901408,\n      \"acc_norm\": 0.4148148148148148,\n\
          \      \"acc_norm_stderr\": 0.04256193767901408\n    },\n    \"hendrycksTest-astronomy\"\
          : {\n      \"acc\": 0.34210526315789475,\n      \"acc_stderr\": 0.03860731599316092,\n\
          \      \"acc_norm\": 0.34210526315789475,\n      \"acc_norm_stderr\": 0.03860731599316092\n\
          \    },\n    \"hendrycksTest-business_ethics\": {\n      \"acc\": 0.44,\n\
          \      \"acc_stderr\": 0.04988876515698589,\n      \"acc_norm\": 0.44,\n\
          \      \"acc_norm_stderr\": 0.04988876515698589\n    },\n    \"hendrycksTest-clinical_knowledge\"\
          : {\n      \"acc\": 0.45660377358490567,\n      \"acc_stderr\": 0.03065674869673943,\n\
          \      \"acc_norm\": 0.45660377358490567,\n      \"acc_norm_stderr\": 0.03065674869673943\n\
          \    },\n    \"hendrycksTest-college_biology\": {\n      \"acc\": 0.3541666666666667,\n\
          \      \"acc_stderr\": 0.039994111357535424,\n      \"acc_norm\": 0.3541666666666667,\n\
          \      \"acc_norm_stderr\": 0.039994111357535424\n    },\n    \"hendrycksTest-college_chemistry\"\
          : {\n      \"acc\": 0.26,\n      \"acc_stderr\": 0.04408440022768077,\n\
          \      \"acc_norm\": 0.26,\n      \"acc_norm_stderr\": 0.04408440022768077\n\
          \    },\n    \"hendrycksTest-college_computer_science\": {\n      \"acc\"\
          : 0.37,\n      \"acc_stderr\": 0.04852365870939099,\n      \"acc_norm\"\
          : 0.37,\n      \"acc_norm_stderr\": 0.04852365870939099\n    },\n    \"\
          hendrycksTest-college_mathematics\": {\n      \"acc\": 0.3,\n      \"acc_stderr\"\
          : 0.046056618647183814,\n      \"acc_norm\": 0.3,\n      \"acc_norm_stderr\"\
          : 0.046056618647183814\n    },\n    \"hendrycksTest-college_medicine\":\
          \ {\n      \"acc\": 0.2774566473988439,\n      \"acc_stderr\": 0.03414014007044037,\n\
          \      \"acc_norm\": 0.2774566473988439,\n      \"acc_norm_stderr\": 0.03414014007044037\n\
          \    },\n    \"hendrycksTest-college_physics\": {\n      \"acc\": 0.13725490196078433,\n\
          \      \"acc_stderr\": 0.03424084669891523,\n      \"acc_norm\": 0.13725490196078433,\n\
          \      \"acc_norm_stderr\": 0.03424084669891523\n    },\n    \"hendrycksTest-computer_security\"\
          : {\n      \"acc\": 0.53,\n      \"acc_stderr\": 0.05016135580465919,\n\
          \      \"acc_norm\": 0.53,\n      \"acc_norm_stderr\": 0.05016135580465919\n\
          \    },\n    \"hendrycksTest-conceptual_physics\": {\n      \"acc\": 0.35319148936170214,\n\
          \      \"acc_stderr\": 0.031245325202761926,\n      \"acc_norm\": 0.35319148936170214,\n\
          \      \"acc_norm_stderr\": 0.031245325202761926\n    },\n    \"hendrycksTest-econometrics\"\
          : {\n      \"acc\": 0.21929824561403508,\n      \"acc_stderr\": 0.03892431106518753,\n\
          \      \"acc_norm\": 0.21929824561403508,\n      \"acc_norm_stderr\": 0.03892431106518753\n\
          \    },\n    \"hendrycksTest-electrical_engineering\": {\n      \"acc\"\
          : 0.38620689655172413,\n      \"acc_stderr\": 0.04057324734419036,\n   \
          \   \"acc_norm\": 0.38620689655172413,\n      \"acc_norm_stderr\": 0.04057324734419036\n\
          \    },\n    \"hendrycksTest-elementary_mathematics\": {\n      \"acc\"\
          : 0.2619047619047619,\n      \"acc_stderr\": 0.02264421261552521,\n    \
          \  \"acc_norm\": 0.2619047619047619,\n      \"acc_norm_stderr\": 0.02264421261552521\n\
          \    },\n    \"hendrycksTest-formal_logic\": {\n      \"acc\": 0.24603174603174602,\n\
          \      \"acc_stderr\": 0.03852273364924316,\n      \"acc_norm\": 0.24603174603174602,\n\
          \      \"acc_norm_stderr\": 0.03852273364924316\n    },\n    \"hendrycksTest-global_facts\"\
          : {\n      \"acc\": 0.37,\n      \"acc_stderr\": 0.048523658709391,\n  \
          \    \"acc_norm\": 0.37,\n      \"acc_norm_stderr\": 0.048523658709391\n\
          \    },\n    \"hendrycksTest-high_school_biology\": {\n      \"acc\": 0.4096774193548387,\n\
          \      \"acc_stderr\": 0.02797605491534736,\n      \"acc_norm\": 0.4096774193548387,\n\
          \      \"acc_norm_stderr\": 0.02797605491534736\n    },\n    \"hendrycksTest-high_school_chemistry\"\
          : {\n      \"acc\": 0.270935960591133,\n      \"acc_stderr\": 0.03127090713297698,\n\
          \      \"acc_norm\": 0.270935960591133,\n      \"acc_norm_stderr\": 0.03127090713297698\n\
          \    },\n    \"hendrycksTest-high_school_computer_science\": {\n      \"\
          acc\": 0.46,\n      \"acc_stderr\": 0.05009082659620332,\n      \"acc_norm\"\
          : 0.46,\n      \"acc_norm_stderr\": 0.05009082659620332\n    },\n    \"\
          hendrycksTest-high_school_european_history\": {\n      \"acc\": 0.4909090909090909,\n\
          \      \"acc_stderr\": 0.0390369864774844,\n      \"acc_norm\": 0.4909090909090909,\n\
          \      \"acc_norm_stderr\": 0.0390369864774844\n    },\n    \"hendrycksTest-high_school_geography\"\
          : {\n      \"acc\": 0.4898989898989899,\n      \"acc_stderr\": 0.035616254886737454,\n\
          \      \"acc_norm\": 0.4898989898989899,\n      \"acc_norm_stderr\": 0.035616254886737454\n\
          \    },\n    \"hendrycksTest-high_school_government_and_politics\": {\n\
          \      \"acc\": 0.5699481865284974,\n      \"acc_stderr\": 0.035729543331448094,\n\
          \      \"acc_norm\": 0.5699481865284974,\n      \"acc_norm_stderr\": 0.035729543331448094\n\
          \    },\n    \"hendrycksTest-high_school_macroeconomics\": {\n      \"acc\"\
          : 0.36666666666666664,\n      \"acc_stderr\": 0.02443301646605246,\n   \
          \   \"acc_norm\": 0.36666666666666664,\n      \"acc_norm_stderr\": 0.02443301646605246\n\
          \    },\n    \"hendrycksTest-high_school_mathematics\": {\n      \"acc\"\
          : 0.23333333333333334,\n      \"acc_stderr\": 0.025787874220959302,\n  \
          \    \"acc_norm\": 0.23333333333333334,\n      \"acc_norm_stderr\": 0.025787874220959302\n\
          \    },\n    \"hendrycksTest-high_school_microeconomics\": {\n      \"acc\"\
          : 0.3403361344537815,\n      \"acc_stderr\": 0.030778057422931673,\n   \
          \   \"acc_norm\": 0.3403361344537815,\n      \"acc_norm_stderr\": 0.030778057422931673\n\
          \    },\n    \"hendrycksTest-high_school_physics\": {\n      \"acc\": 0.26490066225165565,\n\
          \      \"acc_stderr\": 0.03603038545360385,\n      \"acc_norm\": 0.26490066225165565,\n\
          \      \"acc_norm_stderr\": 0.03603038545360385\n    },\n    \"hendrycksTest-high_school_psychology\"\
          : {\n      \"acc\": 0.48440366972477067,\n      \"acc_stderr\": 0.02142689153920805,\n\
          \      \"acc_norm\": 0.48440366972477067,\n      \"acc_norm_stderr\": 0.02142689153920805\n\
          \    },\n    \"hendrycksTest-high_school_statistics\": {\n      \"acc\"\
          : 0.30092592592592593,\n      \"acc_stderr\": 0.03128039084329882,\n   \
          \   \"acc_norm\": 0.30092592592592593,\n      \"acc_norm_stderr\": 0.03128039084329882\n\
          \    },\n    \"hendrycksTest-high_school_us_history\": {\n      \"acc\"\
          : 0.4852941176470588,\n      \"acc_stderr\": 0.03507793834791324,\n    \
          \  \"acc_norm\": 0.4852941176470588,\n      \"acc_norm_stderr\": 0.03507793834791324\n\
          \    },\n    \"hendrycksTest-high_school_world_history\": {\n      \"acc\"\
          : 0.4936708860759494,\n      \"acc_stderr\": 0.03254462010767859,\n    \
          \  \"acc_norm\": 0.4936708860759494,\n      \"acc_norm_stderr\": 0.03254462010767859\n\
          \    },\n    \"hendrycksTest-human_aging\": {\n      \"acc\": 0.4977578475336323,\n\
          \      \"acc_stderr\": 0.033557465352232634,\n      \"acc_norm\": 0.4977578475336323,\n\
          \      \"acc_norm_stderr\": 0.033557465352232634\n    },\n    \"hendrycksTest-human_sexuality\"\
          : {\n      \"acc\": 0.4580152671755725,\n      \"acc_stderr\": 0.04369802690578756,\n\
          \      \"acc_norm\": 0.4580152671755725,\n      \"acc_norm_stderr\": 0.04369802690578756\n\
          \    },\n    \"hendrycksTest-international_law\": {\n      \"acc\": 0.6198347107438017,\n\
          \      \"acc_stderr\": 0.04431324501968431,\n      \"acc_norm\": 0.6198347107438017,\n\
          \      \"acc_norm_stderr\": 0.04431324501968431\n    },\n    \"hendrycksTest-jurisprudence\"\
          : {\n      \"acc\": 0.48148148148148145,\n      \"acc_stderr\": 0.04830366024635331,\n\
          \      \"acc_norm\": 0.48148148148148145,\n      \"acc_norm_stderr\": 0.04830366024635331\n\
          \    },\n    \"hendrycksTest-logical_fallacies\": {\n      \"acc\": 0.44171779141104295,\n\
          \      \"acc_stderr\": 0.03901591825836184,\n      \"acc_norm\": 0.44171779141104295,\n\
          \      \"acc_norm_stderr\": 0.03901591825836184\n    },\n    \"hendrycksTest-machine_learning\"\
          : {\n      \"acc\": 0.29464285714285715,\n      \"acc_stderr\": 0.04327040932578729,\n\
          \      \"acc_norm\": 0.29464285714285715,\n      \"acc_norm_stderr\": 0.04327040932578729\n\
          \    },\n    \"hendrycksTest-management\": {\n      \"acc\": 0.4077669902912621,\n\
          \      \"acc_stderr\": 0.048657775704107675,\n      \"acc_norm\": 0.4077669902912621,\n\
          \      \"acc_norm_stderr\": 0.048657775704107675\n    },\n    \"hendrycksTest-marketing\"\
          : {\n      \"acc\": 0.5683760683760684,\n      \"acc_stderr\": 0.0324483553531149,\n\
          \      \"acc_norm\": 0.5683760683760684,\n      \"acc_norm_stderr\": 0.0324483553531149\n\
          \    },\n    \"hendrycksTest-medical_genetics\": {\n      \"acc\": 0.45,\n\
          \      \"acc_stderr\": 0.05,\n      \"acc_norm\": 0.45,\n      \"acc_norm_stderr\"\
          : 0.05\n    },\n    \"hendrycksTest-miscellaneous\": {\n      \"acc\": 0.5376756066411239,\n\
          \      \"acc_stderr\": 0.017829131764287184,\n      \"acc_norm\": 0.5376756066411239,\n\
          \      \"acc_norm_stderr\": 0.017829131764287184\n    },\n    \"hendrycksTest-moral_disputes\"\
          : {\n      \"acc\": 0.4479768786127168,\n      \"acc_stderr\": 0.02677299065336182,\n\
          \      \"acc_norm\": 0.4479768786127168,\n      \"acc_norm_stderr\": 0.02677299065336182\n\
          \    },\n    \"hendrycksTest-moral_scenarios\": {\n      \"acc\": 0.23910614525139665,\n\
          \      \"acc_stderr\": 0.01426555419233116,\n      \"acc_norm\": 0.23910614525139665,\n\
          \      \"acc_norm_stderr\": 0.01426555419233116\n    },\n    \"hendrycksTest-nutrition\"\
          : {\n      \"acc\": 0.3790849673202614,\n      \"acc_stderr\": 0.02778014120702334,\n\
          \      \"acc_norm\": 0.3790849673202614,\n      \"acc_norm_stderr\": 0.02778014120702334\n\
          \    },\n    \"hendrycksTest-philosophy\": {\n      \"acc\": 0.42443729903536975,\n\
          \      \"acc_stderr\": 0.028071928247946205,\n      \"acc_norm\": 0.42443729903536975,\n\
          \      \"acc_norm_stderr\": 0.028071928247946205\n    },\n    \"hendrycksTest-prehistory\"\
          : {\n      \"acc\": 0.4537037037037037,\n      \"acc_stderr\": 0.027701228468542602,\n\
          \      \"acc_norm\": 0.4537037037037037,\n      \"acc_norm_stderr\": 0.027701228468542602\n\
          \    },\n    \"hendrycksTest-professional_accounting\": {\n      \"acc\"\
          : 0.3120567375886525,\n      \"acc_stderr\": 0.02764012054516993,\n    \
          \  \"acc_norm\": 0.3120567375886525,\n      \"acc_norm_stderr\": 0.02764012054516993\n\
          \    },\n    \"hendrycksTest-professional_law\": {\n      \"acc\": 0.3109517601043025,\n\
          \      \"acc_stderr\": 0.011822252917799205,\n      \"acc_norm\": 0.3109517601043025,\n\
          \      \"acc_norm_stderr\": 0.011822252917799205\n    },\n    \"hendrycksTest-professional_medicine\"\
          : {\n      \"acc\": 0.40808823529411764,\n      \"acc_stderr\": 0.029855261393483924,\n\
          \      \"acc_norm\": 0.40808823529411764,\n      \"acc_norm_stderr\": 0.029855261393483924\n\
          \    },\n    \"hendrycksTest-professional_psychology\": {\n      \"acc\"\
          : 0.4068627450980392,\n      \"acc_stderr\": 0.019873802005061173,\n   \
          \   \"acc_norm\": 0.4068627450980392,\n      \"acc_norm_stderr\": 0.019873802005061173\n\
          \    },\n    \"hendrycksTest-public_relations\": {\n      \"acc\": 0.4,\n\
          \      \"acc_stderr\": 0.0469237132203465,\n      \"acc_norm\": 0.4,\n \
          \     \"acc_norm_stderr\": 0.0469237132203465\n    },\n    \"hendrycksTest-security_studies\"\
          : {\n      \"acc\": 0.3510204081632653,\n      \"acc_stderr\": 0.030555316755573637,\n\
          \      \"acc_norm\": 0.3510204081632653,\n      \"acc_norm_stderr\": 0.030555316755573637\n\
          \    },\n    \"hendrycksTest-sociology\": {\n      \"acc\": 0.4975124378109453,\n\
          \      \"acc_stderr\": 0.03535490150137289,\n      \"acc_norm\": 0.4975124378109453,\n\
          \      \"acc_norm_stderr\": 0.03535490150137289\n    },\n    \"hendrycksTest-us_foreign_policy\"\
          : {\n      \"acc\": 0.57,\n      \"acc_stderr\": 0.04975698519562428,\n\
          \      \"acc_norm\": 0.57,\n      \"acc_norm_stderr\": 0.04975698519562428\n\
          \    },\n    \"hendrycksTest-virology\": {\n      \"acc\": 0.3313253012048193,\n\
          \      \"acc_stderr\": 0.036643147772880844,\n      \"acc_norm\": 0.3313253012048193,\n\
          \      \"acc_norm_stderr\": 0.036643147772880844\n    },\n    \"hendrycksTest-world_religions\"\
          : {\n      \"acc\": 0.5497076023391813,\n      \"acc_stderr\": 0.03815827365913237,\n\
          \      \"acc_norm\": 0.5497076023391813,\n      \"acc_norm_stderr\": 0.03815827365913237\n\
          \    }\n  },\n  \"versions\": {\n    \"hendrycksTest-abstract_algebra\"\
          : 1,\n    \"hendrycksTest-anatomy\": 1,\n    \"hendrycksTest-astronomy\"\
          : 1,\n    \"hendrycksTest-business_ethics\": 1,\n    \"hendrycksTest-clinical_knowledge\"\
          : 1,\n    \"hendrycksTest-college_biology\": 1,\n    \"hendrycksTest-college_chemistry\"\
          : 1,\n    \"hendrycksTest-college_computer_science\": 1,\n    \"hendrycksTest-college_mathematics\"\
          : 1,\n    \"hendrycksTest-college_medicine\": 1,\n    \"hendrycksTest-college_physics\"\
          : 1,\n    \"hendrycksTest-computer_security\": 1,\n    \"hendrycksTest-conceptual_physics\"\
          : 1,\n    \"hendrycksTest-econometrics\": 1,\n    \"hendrycksTest-electrical_engineering\"\
          : 1,\n    \"hendrycksTest-elementary_mathematics\": 1,\n    \"hendrycksTest-formal_logic\"\
          : 1,\n    \"hendrycksTest-global_facts\": 1,\n    \"hendrycksTest-high_school_biology\"\
          : 1,\n    \"hendrycksTest-high_school_chemistry\": 1,\n    \"hendrycksTest-high_school_computer_science\"\
          : 1,\n    \"hendrycksTest-high_school_european_history\": 1,\n    \"hendrycksTest-high_school_geography\"\
          : 1,\n    \"hendrycksTest-high_school_government_and_politics\": 1,\n  \
          \  \"hendrycksTest-high_school_macroeconomics\": 1,\n    \"hendrycksTest-high_school_mathematics\"\
          : 1,\n    \"hendrycksTest-high_school_microeconomics\": 1,\n    \"hendrycksTest-high_school_physics\"\
          : 1,\n    \"hendrycksTest-high_school_psychology\": 1,\n    \"hendrycksTest-high_school_statistics\"\
          : 1,\n    \"hendrycksTest-high_school_us_history\": 1,\n    \"hendrycksTest-high_school_world_history\"\
          : 1,\n    \"hendrycksTest-human_aging\": 1,\n    \"hendrycksTest-human_sexuality\"\
          : 1,\n    \"hendrycksTest-international_law\": 1,\n    \"hendrycksTest-jurisprudence\"\
          : 1,\n    \"hendrycksTest-logical_fallacies\": 1,\n    \"hendrycksTest-machine_learning\"\
          : 1,\n    \"hendrycksTest-management\": 1,\n    \"hendrycksTest-marketing\"\
          : 1,\n    \"hendrycksTest-medical_genetics\": 1,\n    \"hendrycksTest-miscellaneous\"\
          : 1,\n    \"hendrycksTest-moral_disputes\": 1,\n    \"hendrycksTest-moral_scenarios\"\
          : 1,\n    \"hendrycksTest-nutrition\": 1,\n    \"hendrycksTest-philosophy\"\
          : 1,\n    \"hendrycksTest-prehistory\": 1,\n    \"hendrycksTest-professional_accounting\"\
          : 1,\n    \"hendrycksTest-professional_law\": 1,\n    \"hendrycksTest-professional_medicine\"\
          : 1,\n    \"hendrycksTest-professional_psychology\": 1,\n    \"hendrycksTest-public_relations\"\
          : 1,\n    \"hendrycksTest-security_studies\": 1,\n    \"hendrycksTest-sociology\"\
          : 1,\n    \"hendrycksTest-us_foreign_policy\": 1,\n    \"hendrycksTest-virology\"\
          : 1,\n    \"hendrycksTest-world_religions\": 1\n  },\n  \"config\": {\n\
          \    \"model\": \"hf-causal\",\n    \"model_args\": \"pretrained=psmathur/orca_mini_v2_7b\"\
          ,\n    \"num_fewshot\": 5,\n    \"batch_size\": \"12\",\n    \"batch_sizes\"\
          : [],\n    \"device\": \"cuda:0\",\n    \"no_cache\": false,\n    \"limit\"\
          : null,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\": {}\n\
          \  }\n}"
        updatedAt: '2023-07-10T18:28:11.417Z'
      numEdits: 0
      reactions: []
    id: 64ac4dbb6e9a4384cf6ce2eb
    type: comment
  author: pankajmathur
  content: "Awesome, Here is output which I got, when I ran the test:\n\n{\n  \"results\"\
    : {\n    \"hendrycksTest-abstract_algebra\": {\n      \"acc\": 0.33,\n      \"\
    acc_stderr\": 0.047258156262526045,\n      \"acc_norm\": 0.33,\n      \"acc_norm_stderr\"\
    : 0.047258156262526045\n    },\n    \"hendrycksTest-anatomy\": {\n      \"acc\"\
    : 0.4148148148148148,\n      \"acc_stderr\": 0.04256193767901408,\n      \"acc_norm\"\
    : 0.4148148148148148,\n      \"acc_norm_stderr\": 0.04256193767901408\n    },\n\
    \    \"hendrycksTest-astronomy\": {\n      \"acc\": 0.34210526315789475,\n   \
    \   \"acc_stderr\": 0.03860731599316092,\n      \"acc_norm\": 0.34210526315789475,\n\
    \      \"acc_norm_stderr\": 0.03860731599316092\n    },\n    \"hendrycksTest-business_ethics\"\
    : {\n      \"acc\": 0.44,\n      \"acc_stderr\": 0.04988876515698589,\n      \"\
    acc_norm\": 0.44,\n      \"acc_norm_stderr\": 0.04988876515698589\n    },\n  \
    \  \"hendrycksTest-clinical_knowledge\": {\n      \"acc\": 0.45660377358490567,\n\
    \      \"acc_stderr\": 0.03065674869673943,\n      \"acc_norm\": 0.45660377358490567,\n\
    \      \"acc_norm_stderr\": 0.03065674869673943\n    },\n    \"hendrycksTest-college_biology\"\
    : {\n      \"acc\": 0.3541666666666667,\n      \"acc_stderr\": 0.039994111357535424,\n\
    \      \"acc_norm\": 0.3541666666666667,\n      \"acc_norm_stderr\": 0.039994111357535424\n\
    \    },\n    \"hendrycksTest-college_chemistry\": {\n      \"acc\": 0.26,\n  \
    \    \"acc_stderr\": 0.04408440022768077,\n      \"acc_norm\": 0.26,\n      \"\
    acc_norm_stderr\": 0.04408440022768077\n    },\n    \"hendrycksTest-college_computer_science\"\
    : {\n      \"acc\": 0.37,\n      \"acc_stderr\": 0.04852365870939099,\n      \"\
    acc_norm\": 0.37,\n      \"acc_norm_stderr\": 0.04852365870939099\n    },\n  \
    \  \"hendrycksTest-college_mathematics\": {\n      \"acc\": 0.3,\n      \"acc_stderr\"\
    : 0.046056618647183814,\n      \"acc_norm\": 0.3,\n      \"acc_norm_stderr\":\
    \ 0.046056618647183814\n    },\n    \"hendrycksTest-college_medicine\": {\n  \
    \    \"acc\": 0.2774566473988439,\n      \"acc_stderr\": 0.03414014007044037,\n\
    \      \"acc_norm\": 0.2774566473988439,\n      \"acc_norm_stderr\": 0.03414014007044037\n\
    \    },\n    \"hendrycksTest-college_physics\": {\n      \"acc\": 0.13725490196078433,\n\
    \      \"acc_stderr\": 0.03424084669891523,\n      \"acc_norm\": 0.13725490196078433,\n\
    \      \"acc_norm_stderr\": 0.03424084669891523\n    },\n    \"hendrycksTest-computer_security\"\
    : {\n      \"acc\": 0.53,\n      \"acc_stderr\": 0.05016135580465919,\n      \"\
    acc_norm\": 0.53,\n      \"acc_norm_stderr\": 0.05016135580465919\n    },\n  \
    \  \"hendrycksTest-conceptual_physics\": {\n      \"acc\": 0.35319148936170214,\n\
    \      \"acc_stderr\": 0.031245325202761926,\n      \"acc_norm\": 0.35319148936170214,\n\
    \      \"acc_norm_stderr\": 0.031245325202761926\n    },\n    \"hendrycksTest-econometrics\"\
    : {\n      \"acc\": 0.21929824561403508,\n      \"acc_stderr\": 0.03892431106518753,\n\
    \      \"acc_norm\": 0.21929824561403508,\n      \"acc_norm_stderr\": 0.03892431106518753\n\
    \    },\n    \"hendrycksTest-electrical_engineering\": {\n      \"acc\": 0.38620689655172413,\n\
    \      \"acc_stderr\": 0.04057324734419036,\n      \"acc_norm\": 0.38620689655172413,\n\
    \      \"acc_norm_stderr\": 0.04057324734419036\n    },\n    \"hendrycksTest-elementary_mathematics\"\
    : {\n      \"acc\": 0.2619047619047619,\n      \"acc_stderr\": 0.02264421261552521,\n\
    \      \"acc_norm\": 0.2619047619047619,\n      \"acc_norm_stderr\": 0.02264421261552521\n\
    \    },\n    \"hendrycksTest-formal_logic\": {\n      \"acc\": 0.24603174603174602,\n\
    \      \"acc_stderr\": 0.03852273364924316,\n      \"acc_norm\": 0.24603174603174602,\n\
    \      \"acc_norm_stderr\": 0.03852273364924316\n    },\n    \"hendrycksTest-global_facts\"\
    : {\n      \"acc\": 0.37,\n      \"acc_stderr\": 0.048523658709391,\n      \"\
    acc_norm\": 0.37,\n      \"acc_norm_stderr\": 0.048523658709391\n    },\n    \"\
    hendrycksTest-high_school_biology\": {\n      \"acc\": 0.4096774193548387,\n \
    \     \"acc_stderr\": 0.02797605491534736,\n      \"acc_norm\": 0.4096774193548387,\n\
    \      \"acc_norm_stderr\": 0.02797605491534736\n    },\n    \"hendrycksTest-high_school_chemistry\"\
    : {\n      \"acc\": 0.270935960591133,\n      \"acc_stderr\": 0.03127090713297698,\n\
    \      \"acc_norm\": 0.270935960591133,\n      \"acc_norm_stderr\": 0.03127090713297698\n\
    \    },\n    \"hendrycksTest-high_school_computer_science\": {\n      \"acc\"\
    : 0.46,\n      \"acc_stderr\": 0.05009082659620332,\n      \"acc_norm\": 0.46,\n\
    \      \"acc_norm_stderr\": 0.05009082659620332\n    },\n    \"hendrycksTest-high_school_european_history\"\
    : {\n      \"acc\": 0.4909090909090909,\n      \"acc_stderr\": 0.0390369864774844,\n\
    \      \"acc_norm\": 0.4909090909090909,\n      \"acc_norm_stderr\": 0.0390369864774844\n\
    \    },\n    \"hendrycksTest-high_school_geography\": {\n      \"acc\": 0.4898989898989899,\n\
    \      \"acc_stderr\": 0.035616254886737454,\n      \"acc_norm\": 0.4898989898989899,\n\
    \      \"acc_norm_stderr\": 0.035616254886737454\n    },\n    \"hendrycksTest-high_school_government_and_politics\"\
    : {\n      \"acc\": 0.5699481865284974,\n      \"acc_stderr\": 0.035729543331448094,\n\
    \      \"acc_norm\": 0.5699481865284974,\n      \"acc_norm_stderr\": 0.035729543331448094\n\
    \    },\n    \"hendrycksTest-high_school_macroeconomics\": {\n      \"acc\": 0.36666666666666664,\n\
    \      \"acc_stderr\": 0.02443301646605246,\n      \"acc_norm\": 0.36666666666666664,\n\
    \      \"acc_norm_stderr\": 0.02443301646605246\n    },\n    \"hendrycksTest-high_school_mathematics\"\
    : {\n      \"acc\": 0.23333333333333334,\n      \"acc_stderr\": 0.025787874220959302,\n\
    \      \"acc_norm\": 0.23333333333333334,\n      \"acc_norm_stderr\": 0.025787874220959302\n\
    \    },\n    \"hendrycksTest-high_school_microeconomics\": {\n      \"acc\": 0.3403361344537815,\n\
    \      \"acc_stderr\": 0.030778057422931673,\n      \"acc_norm\": 0.3403361344537815,\n\
    \      \"acc_norm_stderr\": 0.030778057422931673\n    },\n    \"hendrycksTest-high_school_physics\"\
    : {\n      \"acc\": 0.26490066225165565,\n      \"acc_stderr\": 0.03603038545360385,\n\
    \      \"acc_norm\": 0.26490066225165565,\n      \"acc_norm_stderr\": 0.03603038545360385\n\
    \    },\n    \"hendrycksTest-high_school_psychology\": {\n      \"acc\": 0.48440366972477067,\n\
    \      \"acc_stderr\": 0.02142689153920805,\n      \"acc_norm\": 0.48440366972477067,\n\
    \      \"acc_norm_stderr\": 0.02142689153920805\n    },\n    \"hendrycksTest-high_school_statistics\"\
    : {\n      \"acc\": 0.30092592592592593,\n      \"acc_stderr\": 0.03128039084329882,\n\
    \      \"acc_norm\": 0.30092592592592593,\n      \"acc_norm_stderr\": 0.03128039084329882\n\
    \    },\n    \"hendrycksTest-high_school_us_history\": {\n      \"acc\": 0.4852941176470588,\n\
    \      \"acc_stderr\": 0.03507793834791324,\n      \"acc_norm\": 0.4852941176470588,\n\
    \      \"acc_norm_stderr\": 0.03507793834791324\n    },\n    \"hendrycksTest-high_school_world_history\"\
    : {\n      \"acc\": 0.4936708860759494,\n      \"acc_stderr\": 0.03254462010767859,\n\
    \      \"acc_norm\": 0.4936708860759494,\n      \"acc_norm_stderr\": 0.03254462010767859\n\
    \    },\n    \"hendrycksTest-human_aging\": {\n      \"acc\": 0.4977578475336323,\n\
    \      \"acc_stderr\": 0.033557465352232634,\n      \"acc_norm\": 0.4977578475336323,\n\
    \      \"acc_norm_stderr\": 0.033557465352232634\n    },\n    \"hendrycksTest-human_sexuality\"\
    : {\n      \"acc\": 0.4580152671755725,\n      \"acc_stderr\": 0.04369802690578756,\n\
    \      \"acc_norm\": 0.4580152671755725,\n      \"acc_norm_stderr\": 0.04369802690578756\n\
    \    },\n    \"hendrycksTest-international_law\": {\n      \"acc\": 0.6198347107438017,\n\
    \      \"acc_stderr\": 0.04431324501968431,\n      \"acc_norm\": 0.6198347107438017,\n\
    \      \"acc_norm_stderr\": 0.04431324501968431\n    },\n    \"hendrycksTest-jurisprudence\"\
    : {\n      \"acc\": 0.48148148148148145,\n      \"acc_stderr\": 0.04830366024635331,\n\
    \      \"acc_norm\": 0.48148148148148145,\n      \"acc_norm_stderr\": 0.04830366024635331\n\
    \    },\n    \"hendrycksTest-logical_fallacies\": {\n      \"acc\": 0.44171779141104295,\n\
    \      \"acc_stderr\": 0.03901591825836184,\n      \"acc_norm\": 0.44171779141104295,\n\
    \      \"acc_norm_stderr\": 0.03901591825836184\n    },\n    \"hendrycksTest-machine_learning\"\
    : {\n      \"acc\": 0.29464285714285715,\n      \"acc_stderr\": 0.04327040932578729,\n\
    \      \"acc_norm\": 0.29464285714285715,\n      \"acc_norm_stderr\": 0.04327040932578729\n\
    \    },\n    \"hendrycksTest-management\": {\n      \"acc\": 0.4077669902912621,\n\
    \      \"acc_stderr\": 0.048657775704107675,\n      \"acc_norm\": 0.4077669902912621,\n\
    \      \"acc_norm_stderr\": 0.048657775704107675\n    },\n    \"hendrycksTest-marketing\"\
    : {\n      \"acc\": 0.5683760683760684,\n      \"acc_stderr\": 0.0324483553531149,\n\
    \      \"acc_norm\": 0.5683760683760684,\n      \"acc_norm_stderr\": 0.0324483553531149\n\
    \    },\n    \"hendrycksTest-medical_genetics\": {\n      \"acc\": 0.45,\n   \
    \   \"acc_stderr\": 0.05,\n      \"acc_norm\": 0.45,\n      \"acc_norm_stderr\"\
    : 0.05\n    },\n    \"hendrycksTest-miscellaneous\": {\n      \"acc\": 0.5376756066411239,\n\
    \      \"acc_stderr\": 0.017829131764287184,\n      \"acc_norm\": 0.5376756066411239,\n\
    \      \"acc_norm_stderr\": 0.017829131764287184\n    },\n    \"hendrycksTest-moral_disputes\"\
    : {\n      \"acc\": 0.4479768786127168,\n      \"acc_stderr\": 0.02677299065336182,\n\
    \      \"acc_norm\": 0.4479768786127168,\n      \"acc_norm_stderr\": 0.02677299065336182\n\
    \    },\n    \"hendrycksTest-moral_scenarios\": {\n      \"acc\": 0.23910614525139665,\n\
    \      \"acc_stderr\": 0.01426555419233116,\n      \"acc_norm\": 0.23910614525139665,\n\
    \      \"acc_norm_stderr\": 0.01426555419233116\n    },\n    \"hendrycksTest-nutrition\"\
    : {\n      \"acc\": 0.3790849673202614,\n      \"acc_stderr\": 0.02778014120702334,\n\
    \      \"acc_norm\": 0.3790849673202614,\n      \"acc_norm_stderr\": 0.02778014120702334\n\
    \    },\n    \"hendrycksTest-philosophy\": {\n      \"acc\": 0.42443729903536975,\n\
    \      \"acc_stderr\": 0.028071928247946205,\n      \"acc_norm\": 0.42443729903536975,\n\
    \      \"acc_norm_stderr\": 0.028071928247946205\n    },\n    \"hendrycksTest-prehistory\"\
    : {\n      \"acc\": 0.4537037037037037,\n      \"acc_stderr\": 0.027701228468542602,\n\
    \      \"acc_norm\": 0.4537037037037037,\n      \"acc_norm_stderr\": 0.027701228468542602\n\
    \    },\n    \"hendrycksTest-professional_accounting\": {\n      \"acc\": 0.3120567375886525,\n\
    \      \"acc_stderr\": 0.02764012054516993,\n      \"acc_norm\": 0.3120567375886525,\n\
    \      \"acc_norm_stderr\": 0.02764012054516993\n    },\n    \"hendrycksTest-professional_law\"\
    : {\n      \"acc\": 0.3109517601043025,\n      \"acc_stderr\": 0.011822252917799205,\n\
    \      \"acc_norm\": 0.3109517601043025,\n      \"acc_norm_stderr\": 0.011822252917799205\n\
    \    },\n    \"hendrycksTest-professional_medicine\": {\n      \"acc\": 0.40808823529411764,\n\
    \      \"acc_stderr\": 0.029855261393483924,\n      \"acc_norm\": 0.40808823529411764,\n\
    \      \"acc_norm_stderr\": 0.029855261393483924\n    },\n    \"hendrycksTest-professional_psychology\"\
    : {\n      \"acc\": 0.4068627450980392,\n      \"acc_stderr\": 0.019873802005061173,\n\
    \      \"acc_norm\": 0.4068627450980392,\n      \"acc_norm_stderr\": 0.019873802005061173\n\
    \    },\n    \"hendrycksTest-public_relations\": {\n      \"acc\": 0.4,\n    \
    \  \"acc_stderr\": 0.0469237132203465,\n      \"acc_norm\": 0.4,\n      \"acc_norm_stderr\"\
    : 0.0469237132203465\n    },\n    \"hendrycksTest-security_studies\": {\n    \
    \  \"acc\": 0.3510204081632653,\n      \"acc_stderr\": 0.030555316755573637,\n\
    \      \"acc_norm\": 0.3510204081632653,\n      \"acc_norm_stderr\": 0.030555316755573637\n\
    \    },\n    \"hendrycksTest-sociology\": {\n      \"acc\": 0.4975124378109453,\n\
    \      \"acc_stderr\": 0.03535490150137289,\n      \"acc_norm\": 0.4975124378109453,\n\
    \      \"acc_norm_stderr\": 0.03535490150137289\n    },\n    \"hendrycksTest-us_foreign_policy\"\
    : {\n      \"acc\": 0.57,\n      \"acc_stderr\": 0.04975698519562428,\n      \"\
    acc_norm\": 0.57,\n      \"acc_norm_stderr\": 0.04975698519562428\n    },\n  \
    \  \"hendrycksTest-virology\": {\n      \"acc\": 0.3313253012048193,\n      \"\
    acc_stderr\": 0.036643147772880844,\n      \"acc_norm\": 0.3313253012048193,\n\
    \      \"acc_norm_stderr\": 0.036643147772880844\n    },\n    \"hendrycksTest-world_religions\"\
    : {\n      \"acc\": 0.5497076023391813,\n      \"acc_stderr\": 0.03815827365913237,\n\
    \      \"acc_norm\": 0.5497076023391813,\n      \"acc_norm_stderr\": 0.03815827365913237\n\
    \    }\n  },\n  \"versions\": {\n    \"hendrycksTest-abstract_algebra\": 1,\n\
    \    \"hendrycksTest-anatomy\": 1,\n    \"hendrycksTest-astronomy\": 1,\n    \"\
    hendrycksTest-business_ethics\": 1,\n    \"hendrycksTest-clinical_knowledge\"\
    : 1,\n    \"hendrycksTest-college_biology\": 1,\n    \"hendrycksTest-college_chemistry\"\
    : 1,\n    \"hendrycksTest-college_computer_science\": 1,\n    \"hendrycksTest-college_mathematics\"\
    : 1,\n    \"hendrycksTest-college_medicine\": 1,\n    \"hendrycksTest-college_physics\"\
    : 1,\n    \"hendrycksTest-computer_security\": 1,\n    \"hendrycksTest-conceptual_physics\"\
    : 1,\n    \"hendrycksTest-econometrics\": 1,\n    \"hendrycksTest-electrical_engineering\"\
    : 1,\n    \"hendrycksTest-elementary_mathematics\": 1,\n    \"hendrycksTest-formal_logic\"\
    : 1,\n    \"hendrycksTest-global_facts\": 1,\n    \"hendrycksTest-high_school_biology\"\
    : 1,\n    \"hendrycksTest-high_school_chemistry\": 1,\n    \"hendrycksTest-high_school_computer_science\"\
    : 1,\n    \"hendrycksTest-high_school_european_history\": 1,\n    \"hendrycksTest-high_school_geography\"\
    : 1,\n    \"hendrycksTest-high_school_government_and_politics\": 1,\n    \"hendrycksTest-high_school_macroeconomics\"\
    : 1,\n    \"hendrycksTest-high_school_mathematics\": 1,\n    \"hendrycksTest-high_school_microeconomics\"\
    : 1,\n    \"hendrycksTest-high_school_physics\": 1,\n    \"hendrycksTest-high_school_psychology\"\
    : 1,\n    \"hendrycksTest-high_school_statistics\": 1,\n    \"hendrycksTest-high_school_us_history\"\
    : 1,\n    \"hendrycksTest-high_school_world_history\": 1,\n    \"hendrycksTest-human_aging\"\
    : 1,\n    \"hendrycksTest-human_sexuality\": 1,\n    \"hendrycksTest-international_law\"\
    : 1,\n    \"hendrycksTest-jurisprudence\": 1,\n    \"hendrycksTest-logical_fallacies\"\
    : 1,\n    \"hendrycksTest-machine_learning\": 1,\n    \"hendrycksTest-management\"\
    : 1,\n    \"hendrycksTest-marketing\": 1,\n    \"hendrycksTest-medical_genetics\"\
    : 1,\n    \"hendrycksTest-miscellaneous\": 1,\n    \"hendrycksTest-moral_disputes\"\
    : 1,\n    \"hendrycksTest-moral_scenarios\": 1,\n    \"hendrycksTest-nutrition\"\
    : 1,\n    \"hendrycksTest-philosophy\": 1,\n    \"hendrycksTest-prehistory\":\
    \ 1,\n    \"hendrycksTest-professional_accounting\": 1,\n    \"hendrycksTest-professional_law\"\
    : 1,\n    \"hendrycksTest-professional_medicine\": 1,\n    \"hendrycksTest-professional_psychology\"\
    : 1,\n    \"hendrycksTest-public_relations\": 1,\n    \"hendrycksTest-security_studies\"\
    : 1,\n    \"hendrycksTest-sociology\": 1,\n    \"hendrycksTest-us_foreign_policy\"\
    : 1,\n    \"hendrycksTest-virology\": 1,\n    \"hendrycksTest-world_religions\"\
    : 1\n  },\n  \"config\": {\n    \"model\": \"hf-causal\",\n    \"model_args\"\
    : \"pretrained=psmathur/orca_mini_v2_7b\",\n    \"num_fewshot\": 5,\n    \"batch_size\"\
    : \"12\",\n    \"batch_sizes\": [],\n    \"device\": \"cuda:0\",\n    \"no_cache\"\
    : false,\n    \"limit\": null,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\"\
    : {}\n  }\n}"
  created_at: 2023-07-10 17:28:11+00:00
  edited: false
  hidden: false
  id: 64ac4dbb6e9a4384cf6ce2eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
      fullname: Pankaj Mathur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: pankajmathur
      type: user
    createdAt: '2023-07-10T18:30:29.000Z'
    data:
      edited: false
      editors:
      - pankajmathur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9331890344619751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
          fullname: Pankaj Mathur
          isHf: false
          isPro: true
          name: pankajmathur
          type: user
        html: '<p>so minute differences in metrics between your results and my results
          could be just "dtype=bfloat16,load_in_8bit=False" but overall they both
          look similar  range 0.3955 vs 0.39405</p>

          '
        raw: so minute differences in metrics between your results and my results
          could be just "dtype=bfloat16,load_in_8bit=False" but overall they both
          look similar  range 0.3955 vs 0.39405
        updatedAt: '2023-07-10T18:30:29.948Z'
      numEdits: 0
      reactions: []
    id: 64ac4e45ca0e2e433bdcbb31
    type: comment
  author: pankajmathur
  content: so minute differences in metrics between your results and my results could
    be just "dtype=bfloat16,load_in_8bit=False" but overall they both look similar  range
    0.3955 vs 0.39405
  created_at: 2023-07-10 17:30:29+00:00
  edited: false
  hidden: false
  id: 64ac4e45ca0e2e433bdcbb31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-07-10T19:00:12.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9731535911560059
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>It''s most likely the batch size. But there''s not too big of a
          difference anyway!</p>

          '
        raw: It's most likely the batch size. But there's not too big of a difference
          anyway!
        updatedAt: '2023-07-10T19:00:12.312Z'
      numEdits: 0
      reactions: []
    id: 64ac553c714bad736d2e304e
    type: comment
  author: migtissera
  content: It's most likely the batch size. But there's not too big of a difference
    anyway!
  created_at: 2023-07-10 18:00:12+00:00
  edited: false
  hidden: false
  id: 64ac553c714bad736d2e304e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-07-10T19:18:35.000Z'
    data:
      status: closed
    id: 64ac598b0d0826bdd8814cef
    type: status-change
  author: migtissera
  created_at: 2023-07-10 18:18:35+00:00
  id: 64ac598b0d0826bdd8814cef
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: pankajmathur/orca_mini_v2_7b
repo_type: model
status: closed
target_branch: null
title: Confirming the MMLU Score
