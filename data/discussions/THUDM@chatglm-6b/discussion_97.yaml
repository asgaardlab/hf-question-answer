!!python/object:huggingface_hub.community.DiscussionWithDetails
author: someone652314
conflicting_files: null
created_at: 2023-09-09 14:49:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b87e4341628b8eb431aa8212d49690f.svg
      fullname: someone652314
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: someone652314
      type: user
    createdAt: '2023-09-09T15:49:01.000Z'
    data:
      edited: false
      editors:
      - someone652314
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22441357374191284
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b87e4341628b8eb431aa8212d49690f.svg
          fullname: someone652314
          isHf: false
          isPro: false
          name: someone652314
          type: user
        html: "<h1 id=\"chatglmpretrainedmodel\u7C7B\u5BF9\u5E94\u7684\u51FD\u6570\
          \">ChatGLMPreTrainedModel\u7C7B\u5BF9\u5E94\u7684\u51FD\u6570</h1>\n<p>\
          \ def get_masks(self, input_ids, past_key_values, padding_mask=None):  #\
          \ padding_mask=\u4F20\u5165\u7684attention_mask\uFF0C\u7EF4\u5EA6=[batch_size,\
          \ seq_len]<br>        batch_size, seq_length = input_ids.shape<br>     \
          \   full_attention_mask = torch.ones(batch_size, seq_length, seq_length,\
          \ device=input_ids.device)<br>        full_attention_mask.tril_()<br>  \
          \      past_length = 0<br>        if past_key_values:<br>            past_length\
          \ = past_key_values[0][0].shape[0]<br>        if past_length:  # full_attention_mask\u7EF4\
          \u5EA6 = [batch_size, seq_length, past_length+seq_length]<br>          \
          \  full_attention_mask = torch.cat((torch.ones(batch_size, seq_length, past_length,<br>\
          \                                                        device=input_ids.device),\
          \ full_attention_mask), dim=-1)<br>        if padding_mask is not None:<br>\
          \            full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)\
          \  # padding_mask.unsqueeze(1) = [batch_size, 1, seq_len]<br>        if\
          \ not past_length and padding_mask is not None:<br>            full_attention_mask\
          \ -= padding_mask.unsqueeze(-1) - 1<br>        full_attention_mask = (full_attention_mask\
          \ &lt; 0.5).bool()<br>        full_attention_mask.unsqueeze_(1)<br>    \
          \    return full_attention_mask</p>\n<p>\u4ECE\u8FD9\u4E2A\u51FD\u6570\u6765\
          \u770B\uFF0C\u662F\u5426\u53EA\u6709prefix_encoder\u751F\u6210\u7684prompt\u5BF9\
          \u5E94\u7684attention\u662F\u53CC\u5411\u7684\uFF0C\u5176\u4F59\u6A21\u578B\
          \u8F93\u5165\u7684\u6587\u672C\u90FD\u662F\u5355\u5411\u7684attention\uFF1F\
          </p>\n"
        raw: "# ChatGLMPreTrainedModel\u7C7B\u5BF9\u5E94\u7684\u51FD\u6570   \r\n\
          \ def get_masks(self, input_ids, past_key_values, padding_mask=None):  #\
          \ padding_mask=\u4F20\u5165\u7684attention_mask\uFF0C\u7EF4\u5EA6=[batch_size,\
          \ seq_len]\r\n        batch_size, seq_length = input_ids.shape\r\n     \
          \   full_attention_mask = torch.ones(batch_size, seq_length, seq_length,\
          \ device=input_ids.device)\r\n        full_attention_mask.tril_()\r\n  \
          \      past_length = 0\r\n        if past_key_values:\r\n            past_length\
          \ = past_key_values[0][0].shape[0]\r\n        if past_length:  # full_attention_mask\u7EF4\
          \u5EA6 = [batch_size, seq_length, past_length+seq_length]\r\n          \
          \  full_attention_mask = torch.cat((torch.ones(batch_size, seq_length, past_length,\r\
          \n                                                        device=input_ids.device),\
          \ full_attention_mask), dim=-1)\r\n        if padding_mask is not None:\r\
          \n            full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)\
          \  # padding_mask.unsqueeze(1) = [batch_size, 1, seq_len]\r\n        if\
          \ not past_length and padding_mask is not None:\r\n            full_attention_mask\
          \ -= padding_mask.unsqueeze(-1) - 1\r\n        full_attention_mask = (full_attention_mask\
          \ < 0.5).bool()\r\n        full_attention_mask.unsqueeze_(1)\r\n       \
          \ return full_attention_mask\r\n\r\n\r\n\u4ECE\u8FD9\u4E2A\u51FD\u6570\u6765\
          \u770B\uFF0C\u662F\u5426\u53EA\u6709prefix_encoder\u751F\u6210\u7684prompt\u5BF9\
          \u5E94\u7684attention\u662F\u53CC\u5411\u7684\uFF0C\u5176\u4F59\u6A21\u578B\
          \u8F93\u5165\u7684\u6587\u672C\u90FD\u662F\u5355\u5411\u7684attention\uFF1F"
        updatedAt: '2023-09-09T15:49:01.877Z'
      numEdits: 0
      reactions: []
    id: 64fc93ed74574268a578aa86
    type: comment
  author: someone652314
  content: "# ChatGLMPreTrainedModel\u7C7B\u5BF9\u5E94\u7684\u51FD\u6570   \r\n def\
    \ get_masks(self, input_ids, past_key_values, padding_mask=None):  # padding_mask=\u4F20\
    \u5165\u7684attention_mask\uFF0C\u7EF4\u5EA6=[batch_size, seq_len]\r\n       \
    \ batch_size, seq_length = input_ids.shape\r\n        full_attention_mask = torch.ones(batch_size,\
    \ seq_length, seq_length, device=input_ids.device)\r\n        full_attention_mask.tril_()\r\
    \n        past_length = 0\r\n        if past_key_values:\r\n            past_length\
    \ = past_key_values[0][0].shape[0]\r\n        if past_length:  # full_attention_mask\u7EF4\
    \u5EA6 = [batch_size, seq_length, past_length+seq_length]\r\n            full_attention_mask\
    \ = torch.cat((torch.ones(batch_size, seq_length, past_length,\r\n           \
    \                                             device=input_ids.device), full_attention_mask),\
    \ dim=-1)\r\n        if padding_mask is not None:\r\n            full_attention_mask\
    \ = full_attention_mask * padding_mask.unsqueeze(1)  # padding_mask.unsqueeze(1)\
    \ = [batch_size, 1, seq_len]\r\n        if not past_length and padding_mask is\
    \ not None:\r\n            full_attention_mask -= padding_mask.unsqueeze(-1) -\
    \ 1\r\n        full_attention_mask = (full_attention_mask < 0.5).bool()\r\n  \
    \      full_attention_mask.unsqueeze_(1)\r\n        return full_attention_mask\r\
    \n\r\n\r\n\u4ECE\u8FD9\u4E2A\u51FD\u6570\u6765\u770B\uFF0C\u662F\u5426\u53EA\u6709\
    prefix_encoder\u751F\u6210\u7684prompt\u5BF9\u5E94\u7684attention\u662F\u53CC\u5411\
    \u7684\uFF0C\u5176\u4F59\u6A21\u578B\u8F93\u5165\u7684\u6587\u672C\u90FD\u662F\
    \u5355\u5411\u7684attention\uFF1F"
  created_at: 2023-09-09 14:49:01+00:00
  edited: false
  hidden: false
  id: 64fc93ed74574268a578aa86
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 97
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: "chatglm2-6b\u5FAE\u8C03\u8FC7\u7A0B\u4E2D\u7684attention_mask\u5177\u4F53\u662F\
  \u5982\u4F55\u5B9E\u73B0\u7684\uFF1F\u54EA\u4E00\u90E8\u5206\u662F\u53CC\u5411\uFF0C\
  \u54EA\u4E00\u90E8\u5206\u662F\u5355\u5411\uFF1F"
