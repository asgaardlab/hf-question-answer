!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FenixInDarkSolo
conflicting_files: null
created_at: 2023-03-15 09:33:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-03-15T10:33:40.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>I try to load the program in CPU mode by remove cuda() on AutoModel.FromPretrained().
          But it failed. I don''t care the inference speed. Do we have anyway to setup
          the device parameter on model object? Thank you.</p>

          '
        raw: I try to load the program in CPU mode by remove cuda() on AutoModel.FromPretrained().
          But it failed. I don't care the inference speed. Do we have anyway to setup
          the device parameter on model object? Thank you.
        updatedAt: '2023-03-15T10:33:40.297Z'
      numEdits: 0
      reactions: []
    id: 64119f04ca1bb1e9253b600e
    type: comment
  author: FenixInDarkSolo
  content: I try to load the program in CPU mode by remove cuda() on AutoModel.FromPretrained().
    But it failed. I don't care the inference speed. Do we have anyway to setup the
    device parameter on model object? Thank you.
  created_at: 2023-03-15 09:33:40+00:00
  edited: false
  hidden: false
  id: 64119f04ca1bb1e9253b600e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a4d10b08614484a5f0da89c7624597c.svg
      fullname: xxxxxxx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThreeFlower
      type: user
    createdAt: '2023-03-15T14:25:09.000Z'
    data:
      edited: false
      editors:
      - ThreeFlower
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a4d10b08614484a5f0da89c7624597c.svg
          fullname: xxxxxxx
          isHf: false
          isPro: false
          name: ThreeFlower
          type: user
        html: "<p>in README.md<br>CPU\u90E8\u7F72( use  CPU  )<br>model = AutoModel.from_pretrained(\"\
          THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()<br>change to<br>model\
          \ = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).float()</p>\n"
        raw: "in README.md \nCPU\u90E8\u7F72( use  CPU  )\nmodel = AutoModel.from_pretrained(\"\
          THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\nchange to \n\
          model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).float()"
        updatedAt: '2023-03-15T14:25:09.910Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F614"
        users:
        - FenixInDarkSolo
        - tonytony2023
    id: 6411d54534ef7a70c4f243a2
    type: comment
  author: ThreeFlower
  content: "in README.md \nCPU\u90E8\u7F72( use  CPU  )\nmodel = AutoModel.from_pretrained(\"\
    THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\nchange to \nmodel =\
    \ AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).float()"
  created_at: 2023-03-15 13:25:09+00:00
  edited: false
  hidden: false
  id: 6411d54534ef7a70c4f243a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-03-22T10:37:26.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>No it didn''t work. The problem is coming from cpm_kernels library.<br>It
          will load the cuda setting. But I didn''t found anyway to avoid it.</p>

          '
        raw: "No it didn't work. The problem is coming from cpm_kernels library. \n\
          It will load the cuda setting. But I didn't found anyway to avoid it."
        updatedAt: '2023-03-22T10:37:26.920Z'
      numEdits: 0
      reactions: []
    id: 641ada665d3929663011ff0d
    type: comment
  author: FenixInDarkSolo
  content: "No it didn't work. The problem is coming from cpm_kernels library. \n\
    It will load the cuda setting. But I didn't found anyway to avoid it."
  created_at: 2023-03-22 09:37:26+00:00
  edited: false
  hidden: false
  id: 641ada665d3929663011ff0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f985d0ae179a9902eef49527d10b8da9.svg
      fullname: Jesse  Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jaleelchen
      type: user
    createdAt: '2023-03-22T13:52:23.000Z'
    data:
      edited: false
      editors:
      - jaleelchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f985d0ae179a9902eef49527d10b8da9.svg
          fullname: Jesse  Chen
          isHf: false
          isPro: false
          name: jaleelchen
          type: user
        html: '<p>It worked for me. I am able to deploy the model on a 48gb ram and
          2vcpu, without gpu. It took at least 2-3 minutes for a simple question(less
          than 10 tokens) though.</p>

          '
        raw: It worked for me. I am able to deploy the model on a 48gb ram and 2vcpu,
          without gpu. It took at least 2-3 minutes for a simple question(less than
          10 tokens) though.
        updatedAt: '2023-03-22T13:52:23.136Z'
      numEdits: 0
      reactions: []
    id: 641b08174723a2b0aa49ab0b
    type: comment
  author: jaleelchen
  content: It worked for me. I am able to deploy the model on a 48gb ram and 2vcpu,
    without gpu. It took at least 2-3 minutes for a simple question(less than 10 tokens)
    though.
  created_at: 2023-03-22 12:52:23+00:00
  edited: false
  hidden: false
  id: 641b08174723a2b0aa49ab0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679311624745-63f36b504745321de3510823.jpeg?w=200&h=200&f=face
      fullname: Song XiXuan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DrSong
      type: user
    createdAt: '2023-03-23T09:07:28.000Z'
    data:
      edited: false
      editors:
      - DrSong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679311624745-63f36b504745321de3510823.jpeg?w=200&h=200&f=face
          fullname: Song XiXuan
          isHf: false
          isPro: false
          name: DrSong
          type: user
        html: '<p>Code in ''dev'' branch might be what you are looking for, won''t
          load cpm_kernels if don''t have one.<br>Or you can try "THUDM/chatglm-6b-int4",
          the new feature has been merged into ''main'' already:<br>model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4",
          trust_remote_code=True).cpu().float()</p>

          '
        raw: "Code in 'dev' branch might be what you are looking for, won't load cpm_kernels\
          \ if don't have one.\nOr you can try \"THUDM/chatglm-6b-int4\", the new\
          \ feature has been merged into 'main' already: \nmodel = AutoModel.from_pretrained(\"\
          THUDM/chatglm-6b-int4\", trust_remote_code=True).cpu().float()"
        updatedAt: '2023-03-23T09:07:28.645Z'
      numEdits: 0
      reactions: []
    id: 641c16d0f38d419f611d97e5
    type: comment
  author: DrSong
  content: "Code in 'dev' branch might be what you are looking for, won't load cpm_kernels\
    \ if don't have one.\nOr you can try \"THUDM/chatglm-6b-int4\", the new feature\
    \ has been merged into 'main' already: \nmodel = AutoModel.from_pretrained(\"\
    THUDM/chatglm-6b-int4\", trust_remote_code=True).cpu().float()"
  created_at: 2023-03-23 08:07:28+00:00
  edited: false
  hidden: false
  id: 641c16d0f38d419f611d97e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4542ddc8e97ea095e94ba9fbe051c2f.svg
      fullname: kim wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: happykim
      type: user
    createdAt: '2023-07-17T07:08:32.000Z'
    data:
      edited: false
      editors:
      - happykim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48270097374916077
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4542ddc8e97ea095e94ba9fbe051c2f.svg
          fullname: kim wei
          isHf: false
          isPro: false
          name: happykim
          type: user
        html: '<p>I''m seeing a similar issue (trying to run model on CPU from Google
          colab), issue seems to be from the <code>cpm_kernels</code> package</p>

          <p>Explicitly passing a <code>revision</code> is encouraged when loading
          a model with custom code to ensure no malicious code has been contributed
          in a newer revision.<br>Explicitly passing a <code>revision</code> is encouraged
          when loading a configuration with custom code to ensure no malicious code
          has been contributed in a newer revision.<br>Explicitly passing a <code>revision</code>
          is encouraged when loading a model with custom code to ensure no malicious
          code has been contributed in a newer revision.</p>

          <hr>

          <p>OSError                                   Traceback (most recent call
          last)<br> in &lt;cell line: 8&gt;()<br>      6 # CPU model<br>      7 #
          model = AutoModel.from_pretrained("THUDM/chatglm2-6b-int4",trust_remote_code=True).cpu().float()<br>----&gt;
          8 model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).cpu().float()<br>      9<br>     10
          model = model.eval()</p>

          <p>17 frames<br>/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py
          in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)<br>    460             class_ref
          = config.auto_map[cls.<strong>name</strong>]<br>    461             module_file,
          class_name = class_ref.split(".")<br>--&gt; 462             model_class
          = get_class_from_dynamic_module(<br>    463                 pretrained_model_name_or_path,
          module_file + ".py", class_name, **hub_kwargs, **kwargs<br>    464             )</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py
          in get_class_from_dynamic_module(pretrained_model_name_or_path, module_file,
          class_name, cache_dir, force_download, resume_download, proxies, use_auth_token,
          revision, local_files_only, **kwargs)<br>    386     ```"""<br>    387     #
          And lastly we get the class inside our newly created module<br>--&gt; 388     final_module
          = get_cached_module_file(<br>    389         pretrained_model_name_or_path,<br>    390         module_file,</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py
          in get_cached_module_file(pretrained_model_name_or_path, module_file, cache_dir,
          force_download, resume_download, proxies, use_auth_token, revision, local_files_only)<br>    297         for
          module_needed in modules_needed:<br>    298             if not (submodule_path
          / module_needed).exists():<br>--&gt; 299                 get_cached_module_file(<br>    300                     pretrained_model_name_or_path,<br>    301                     f"{module_needed}.py",</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py
          in get_cached_module_file(pretrained_model_name_or_path, module_file, cache_dir,
          force_download, resume_download, proxies, use_auth_token, revision, local_files_only)<br>    267<br>    268     #
          Check we have all the requirements in our environment<br>--&gt; 269     modules_needed
          = check_imports(resolved_module_file)<br>    270<br>    271     # Now we
          move the module inside our cached dynamic modules.</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py
          in check_imports(filename)<br>    132     for imp in imports:<br>    133         try:<br>--&gt;
          134             importlib.import_module(imp)<br>    135         except ImportError:<br>    136             missing_packages.append(imp)</p>

          <p>/usr/lib/python3.10/importlib/<strong>init</strong>.py in import_module(name,
          package)<br>    124                 break<br>    125             level +=
          1<br>--&gt; 126     return _bootstrap._gcd_import(name[level:], package,
          level)<br>    127<br>    128 </p>

          <p>/usr/lib/python3.10/importlib/_bootstrap.py in _gcd_import(name, package,
          level)</p>

          <p>/usr/lib/python3.10/importlib/_bootstrap.py in <em>find_and_load(name,
          import</em>)</p>

          <p>/usr/lib/python3.10/importlib/_bootstrap.py in <em>find_and_load_unlocked(name,
          import</em>)</p>

          <p>/usr/lib/python3.10/importlib/_bootstrap.py in _load_unlocked(spec)</p>

          <p>/usr/lib/python3.10/importlib/_bootstrap_external.py in exec_module(self,
          module)</p>

          <p>/usr/lib/python3.10/importlib/_bootstrap.py in _call_with_frames_removed(f,
          *args, **kwds)</p>

          <p>/usr/local/lib/python3.10/dist-packages/cpm_kernels/<strong>init</strong>.py
          in <br>----&gt; 1 from . import library<br>      2 from .kernels import
          *</p>

          <p>/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/<strong>init</strong>.py
          in <br>      1 from . import nvrtc<br>----&gt; 2 from . import cuda<br>      3
          from . import cudart<br>      4 from . import cublaslt</p>

          <p>/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/cuda.py in
          <br>      7     cuda = Lib.from_lib("cuda", ctypes.WinDLL("nvcuda.dll"))<br>      8
          else:<br>----&gt; 9     cuda = Lib("cuda")<br>     10 CUresult = ctypes.c_int<br>     11
          </p>

          <p>/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/base.py in
          <strong>init</strong>(self, name)<br>     53             self.__lib_path
          = lib_path<br>     54             if lib_path is not None:<br>---&gt; 55                 self.__lib
          = ctypes.cdll.LoadLibrary(lib_path)<br>     56             else:<br>     57                 self.__lib
          = None</p>

          <p>/usr/lib/python3.10/ctypes/<strong>init</strong>.py in LoadLibrary(self,
          name)<br>    450<br>    451     def LoadLibrary(self, name):<br>--&gt; 452         return
          self._dlltype(name)<br>    453<br>    454     <strong>class_getitem</strong>
          = classmethod(_types.GenericAlias)</p>

          <p>/usr/lib/python3.10/ctypes/<strong>init</strong>.py in <strong>init</strong>(self,
          name, mode, handle, use_errno, use_last_error, winmode)<br>    372<br>    373         if
          handle is None:<br>--&gt; 374             self._handle = _dlopen(self._name,
          mode)<br>    375         else:<br>    376             self._handle = handle</p>

          <p>OSError: libcuda.so.1: cannot open shared object file: No such file or
          directory</p>

          '
        raw: "I'm seeing a similar issue (trying to run model on CPU from Google colab),\
          \ issue seems to be from the `cpm_kernels` package\n\nExplicitly passing\
          \ a `revision` is encouraged when loading a model with custom code to ensure\
          \ no malicious code has been contributed in a newer revision.\nExplicitly\
          \ passing a `revision` is encouraged when loading a configuration with custom\
          \ code to ensure no malicious code has been contributed in a newer revision.\n\
          Explicitly passing a `revision` is encouraged when loading a model with\
          \ custom code to ensure no malicious code has been contributed in a newer\
          \ revision.\n---------------------------------------------------------------------------\n\
          OSError                                   Traceback (most recent call last)\n\
          <ipython-input-10-d75c8d7eddc0> in <cell line: 8>()\n      6 # CPU model\n\
          \      7 # model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b-int4\"\
          ,trust_remote_code=True).cpu().float()\n----> 8 model = AutoModel.from_pretrained(\"\
          THUDM/chatglm-6b-int4\", trust_remote_code=True).cpu().float()\n      9\
          \ \n     10 model = model.eval()\n\n17 frames\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
          \    460             class_ref = config.auto_map[cls.__name__]\n    461\
          \             module_file, class_name = class_ref.split(\".\")\n--> 462\
          \             model_class = get_class_from_dynamic_module(\n    463    \
          \             pretrained_model_name_or_path, module_file + \".py\", class_name,\
          \ **hub_kwargs, **kwargs\n    464             )\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
          \ in get_class_from_dynamic_module(pretrained_model_name_or_path, module_file,\
          \ class_name, cache_dir, force_download, resume_download, proxies, use_auth_token,\
          \ revision, local_files_only, **kwargs)\n    386     ```\"\"\"\n    387\
          \     # And lastly we get the class inside our newly created module\n-->\
          \ 388     final_module = get_cached_module_file(\n    389         pretrained_model_name_or_path,\n\
          \    390         module_file,\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
          \ in get_cached_module_file(pretrained_model_name_or_path, module_file,\
          \ cache_dir, force_download, resume_download, proxies, use_auth_token, revision,\
          \ local_files_only)\n    297         for module_needed in modules_needed:\n\
          \    298             if not (submodule_path / module_needed).exists():\n\
          --> 299                 get_cached_module_file(\n    300               \
          \      pretrained_model_name_or_path,\n    301                     f\"{module_needed}.py\"\
          ,\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
          \ in get_cached_module_file(pretrained_model_name_or_path, module_file,\
          \ cache_dir, force_download, resume_download, proxies, use_auth_token, revision,\
          \ local_files_only)\n    267 \n    268     # Check we have all the requirements\
          \ in our environment\n--> 269     modules_needed = check_imports(resolved_module_file)\n\
          \    270 \n    271     # Now we move the module inside our cached dynamic\
          \ modules.\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
          \ in check_imports(filename)\n    132     for imp in imports:\n    133 \
          \        try:\n--> 134             importlib.import_module(imp)\n    135\
          \         except ImportError:\n    136             missing_packages.append(imp)\n\
          \n/usr/lib/python3.10/importlib/__init__.py in import_module(name, package)\n\
          \    124                 break\n    125             level += 1\n--> 126\
          \     return _bootstrap._gcd_import(name[level:], package, level)\n    127\
          \ \n    128 \n\n/usr/lib/python3.10/importlib/_bootstrap.py in _gcd_import(name,\
          \ package, level)\n\n/usr/lib/python3.10/importlib/_bootstrap.py in _find_and_load(name,\
          \ import_)\n\n/usr/lib/python3.10/importlib/_bootstrap.py in _find_and_load_unlocked(name,\
          \ import_)\n\n/usr/lib/python3.10/importlib/_bootstrap.py in _load_unlocked(spec)\n\
          \n/usr/lib/python3.10/importlib/_bootstrap_external.py in exec_module(self,\
          \ module)\n\n/usr/lib/python3.10/importlib/_bootstrap.py in _call_with_frames_removed(f,\
          \ *args, **kwds)\n\n/usr/local/lib/python3.10/dist-packages/cpm_kernels/__init__.py\
          \ in <module>\n----> 1 from . import library\n      2 from .kernels import\
          \ *\n\n/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/__init__.py\
          \ in <module>\n      1 from . import nvrtc\n----> 2 from . import cuda\n\
          \      3 from . import cudart\n      4 from . import cublaslt\n\n/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/cuda.py\
          \ in <module>\n      7     cuda = Lib.from_lib(\"cuda\", ctypes.WinDLL(\"\
          nvcuda.dll\"))\n      8 else:\n----> 9     cuda = Lib(\"cuda\")\n     10\
          \ CUresult = ctypes.c_int\n     11 \n\n/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/base.py\
          \ in __init__(self, name)\n     53             self.__lib_path = lib_path\n\
          \     54             if lib_path is not None:\n---> 55                 self.__lib\
          \ = ctypes.cdll.LoadLibrary(lib_path)\n     56             else:\n     57\
          \                 self.__lib = None\n\n/usr/lib/python3.10/ctypes/__init__.py\
          \ in LoadLibrary(self, name)\n    450 \n    451     def LoadLibrary(self,\
          \ name):\n--> 452         return self._dlltype(name)\n    453 \n    454\
          \     __class_getitem__ = classmethod(_types.GenericAlias)\n\n/usr/lib/python3.10/ctypes/__init__.py\
          \ in __init__(self, name, mode, handle, use_errno, use_last_error, winmode)\n\
          \    372 \n    373         if handle is None:\n--> 374             self._handle\
          \ = _dlopen(self._name, mode)\n    375         else:\n    376          \
          \   self._handle = handle\n\nOSError: libcuda.so.1: cannot open shared object\
          \ file: No such file or directory"
        updatedAt: '2023-07-17T07:08:32.562Z'
      numEdits: 0
      reactions: []
    id: 64b4e8f0aa03b65208403cec
    type: comment
  author: happykim
  content: "I'm seeing a similar issue (trying to run model on CPU from Google colab),\
    \ issue seems to be from the `cpm_kernels` package\n\nExplicitly passing a `revision`\
    \ is encouraged when loading a model with custom code to ensure no malicious code\
    \ has been contributed in a newer revision.\nExplicitly passing a `revision` is\
    \ encouraged when loading a configuration with custom code to ensure no malicious\
    \ code has been contributed in a newer revision.\nExplicitly passing a `revision`\
    \ is encouraged when loading a model with custom code to ensure no malicious code\
    \ has been contributed in a newer revision.\n---------------------------------------------------------------------------\n\
    OSError                                   Traceback (most recent call last)\n\
    <ipython-input-10-d75c8d7eddc0> in <cell line: 8>()\n      6 # CPU model\n   \
    \   7 # model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b-int4\",trust_remote_code=True).cpu().float()\n\
    ----> 8 model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).cpu().float()\n\
    \      9 \n     10 model = model.eval()\n\n17 frames\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
    \    460             class_ref = config.auto_map[cls.__name__]\n    461      \
    \       module_file, class_name = class_ref.split(\".\")\n--> 462            \
    \ model_class = get_class_from_dynamic_module(\n    463                 pretrained_model_name_or_path,\
    \ module_file + \".py\", class_name, **hub_kwargs, **kwargs\n    464         \
    \    )\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
    \ in get_class_from_dynamic_module(pretrained_model_name_or_path, module_file,\
    \ class_name, cache_dir, force_download, resume_download, proxies, use_auth_token,\
    \ revision, local_files_only, **kwargs)\n    386     ```\"\"\"\n    387     #\
    \ And lastly we get the class inside our newly created module\n--> 388     final_module\
    \ = get_cached_module_file(\n    389         pretrained_model_name_or_path,\n\
    \    390         module_file,\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
    \ in get_cached_module_file(pretrained_model_name_or_path, module_file, cache_dir,\
    \ force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\n\
    \    297         for module_needed in modules_needed:\n    298             if\
    \ not (submodule_path / module_needed).exists():\n--> 299                 get_cached_module_file(\n\
    \    300                     pretrained_model_name_or_path,\n    301         \
    \            f\"{module_needed}.py\",\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
    \ in get_cached_module_file(pretrained_model_name_or_path, module_file, cache_dir,\
    \ force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\n\
    \    267 \n    268     # Check we have all the requirements in our environment\n\
    --> 269     modules_needed = check_imports(resolved_module_file)\n    270 \n \
    \   271     # Now we move the module inside our cached dynamic modules.\n\n/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\
    \ in check_imports(filename)\n    132     for imp in imports:\n    133       \
    \  try:\n--> 134             importlib.import_module(imp)\n    135         except\
    \ ImportError:\n    136             missing_packages.append(imp)\n\n/usr/lib/python3.10/importlib/__init__.py\
    \ in import_module(name, package)\n    124                 break\n    125    \
    \         level += 1\n--> 126     return _bootstrap._gcd_import(name[level:],\
    \ package, level)\n    127 \n    128 \n\n/usr/lib/python3.10/importlib/_bootstrap.py\
    \ in _gcd_import(name, package, level)\n\n/usr/lib/python3.10/importlib/_bootstrap.py\
    \ in _find_and_load(name, import_)\n\n/usr/lib/python3.10/importlib/_bootstrap.py\
    \ in _find_and_load_unlocked(name, import_)\n\n/usr/lib/python3.10/importlib/_bootstrap.py\
    \ in _load_unlocked(spec)\n\n/usr/lib/python3.10/importlib/_bootstrap_external.py\
    \ in exec_module(self, module)\n\n/usr/lib/python3.10/importlib/_bootstrap.py\
    \ in _call_with_frames_removed(f, *args, **kwds)\n\n/usr/local/lib/python3.10/dist-packages/cpm_kernels/__init__.py\
    \ in <module>\n----> 1 from . import library\n      2 from .kernels import *\n\
    \n/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/__init__.py in <module>\n\
    \      1 from . import nvrtc\n----> 2 from . import cuda\n      3 from . import\
    \ cudart\n      4 from . import cublaslt\n\n/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/cuda.py\
    \ in <module>\n      7     cuda = Lib.from_lib(\"cuda\", ctypes.WinDLL(\"nvcuda.dll\"\
    ))\n      8 else:\n----> 9     cuda = Lib(\"cuda\")\n     10 CUresult = ctypes.c_int\n\
    \     11 \n\n/usr/local/lib/python3.10/dist-packages/cpm_kernels/library/base.py\
    \ in __init__(self, name)\n     53             self.__lib_path = lib_path\n  \
    \   54             if lib_path is not None:\n---> 55                 self.__lib\
    \ = ctypes.cdll.LoadLibrary(lib_path)\n     56             else:\n     57    \
    \             self.__lib = None\n\n/usr/lib/python3.10/ctypes/__init__.py in LoadLibrary(self,\
    \ name)\n    450 \n    451     def LoadLibrary(self, name):\n--> 452         return\
    \ self._dlltype(name)\n    453 \n    454     __class_getitem__ = classmethod(_types.GenericAlias)\n\
    \n/usr/lib/python3.10/ctypes/__init__.py in __init__(self, name, mode, handle,\
    \ use_errno, use_last_error, winmode)\n    372 \n    373         if handle is\
    \ None:\n--> 374             self._handle = _dlopen(self._name, mode)\n    375\
    \         else:\n    376             self._handle = handle\n\nOSError: libcuda.so.1:\
    \ cannot open shared object file: No such file or directory"
  created_at: 2023-07-17 06:08:32+00:00
  edited: false
  hidden: false
  id: 64b4e8f0aa03b65208403cec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d319f7628609a2074c4281329777e06.svg
      fullname: Rifat Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0xrk
      type: user
    createdAt: '2023-09-14T11:57:43.000Z'
    data:
      edited: false
      editors:
      - 0xrk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5197321176528931
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d319f7628609a2074c4281329777e06.svg
          fullname: Rifat Khan
          isHf: false
          isPro: false
          name: 0xrk
          type: user
        html: '<p>Got :     return F.layer_norm(<br>  File "E:\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\functional.py",
          line 2515, in layer_norm<br>    return torch.layer_norm(input, normalized_shape,
          weight, bias, eps, torch.backends.cudnn.enabled)<br>RuntimeError: mixed
          dtype (CPU): expect input to have scalar type of BFloat16</p>

          '
        raw: "Got :     return F.layer_norm(\n  File \"E:\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\functional.py\", line\
          \ 2515, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: mixed\
          \ dtype (CPU): expect input to have scalar type of BFloat16"
        updatedAt: '2023-09-14T11:57:43.174Z'
      numEdits: 0
      reactions: []
    id: 6502f5375b67fa2846e95f38
    type: comment
  author: 0xrk
  content: "Got :     return F.layer_norm(\n  File \"E:\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\functional.py\", line 2515,\
    \ in layer_norm\n    return torch.layer_norm(input, normalized_shape, weight,\
    \ bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: mixed dtype (CPU): expect\
    \ input to have scalar type of BFloat16"
  created_at: 2023-09-14 10:57:43+00:00
  edited: false
  hidden: false
  id: 6502f5375b67fa2846e95f38
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: Can it load on CPU mode?
