!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kebo
conflicting_files: null
created_at: 2023-03-21 01:31:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c830c0f2af432c0e4ee343e78410e62.svg
      fullname: kebo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kebo
      type: user
    createdAt: '2023-03-21T02:31:04.000Z'
    data:
      edited: false
      editors:
      - kebo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c830c0f2af432c0e4ee343e78410e62.svg
          fullname: kebo
          isHf: false
          isPro: false
          name: kebo
          type: user
        html: "<pre><code class=\"language-python\">    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">get_position_ids</span>(<span\
          \ class=\"hljs-params\">self, seq, mask_position, device, gmask=<span class=\"\
          hljs-literal\">False</span></span>):\n        context_length = seq.index(self.config.bos_token_id)\
          \ + <span class=\"hljs-number\">1</span>\n        <span class=\"hljs-keyword\"\
          >if</span> self.position_encoding_2d:\n            seq_length = seq.index(self.config.bos_token_id)\n\
          \            position_ids = torch.arange(context_length, dtype=torch.long,\
          \ device=device)\n            <span class=\"hljs-keyword\">if</span> <span\
          \ class=\"hljs-keyword\">not</span> gmask:\n                position_ids[seq_length:]\
          \ = mask_position\n            block_position_ids = torch.cat((\n      \
          \          torch.zeros(seq_length, dtype=torch.long, device=device),\n \
          \               torch.arange(context_length - seq_length, dtype=torch.long,\
          \ device=device) + <span class=\"hljs-number\">1</span>\n            ))\n\
          \            position_ids = torch.stack((position_ids, block_position_ids),\
          \ dim=<span class=\"hljs-number\">0</span>)\n        <span class=\"hljs-keyword\"\
          >else</span>:\n            position_ids = torch.arange(context_length, dtype=torch.long,\
          \ device=device)\n            <span class=\"hljs-keyword\">if</span> <span\
          \ class=\"hljs-keyword\">not</span> gmask:\n                position_ids[context_length\
          \ - <span class=\"hljs-number\">1</span>:] = mask_position\n\n        position_ids\
          \ = position_ids.unsqueeze(<span class=\"hljs-number\">0</span>)\n\n   \
          \     <span class=\"hljs-keyword\">return</span> position_ids\n</code></pre>\n\
          <p> <code>context_length = seq.index(self.config.bos_token_id) + 1</code><br><code>seq_length\
          \ = seq.index(self.config.bos_token_id)</code> ?   context_length - seq_length\
          \ == 1 ?  why?</p>\n"
        raw: "```python\r\n    def get_position_ids(self, seq, mask_position, device,\
          \ gmask=False):\r\n        context_length = seq.index(self.config.bos_token_id)\
          \ + 1\r\n        if self.position_encoding_2d:\r\n            seq_length\
          \ = seq.index(self.config.bos_token_id)\r\n            position_ids = torch.arange(context_length,\
          \ dtype=torch.long, device=device)\r\n            if not gmask:\r\n    \
          \            position_ids[seq_length:] = mask_position\r\n            block_position_ids\
          \ = torch.cat((\r\n                torch.zeros(seq_length, dtype=torch.long,\
          \ device=device),\r\n                torch.arange(context_length - seq_length,\
          \ dtype=torch.long, device=device) + 1\r\n            ))\r\n           \
          \ position_ids = torch.stack((position_ids, block_position_ids), dim=0)\r\
          \n        else:\r\n            position_ids = torch.arange(context_length,\
          \ dtype=torch.long, device=device)\r\n            if not gmask:\r\n    \
          \            position_ids[context_length - 1:] = mask_position\r\n\r\n \
          \       position_ids = position_ids.unsqueeze(0)\r\n\r\n        return position_ids\r\
          \n```\r\n\r\n `context_length = seq.index(self.config.bos_token_id) + 1`\
          \ \r\n`seq_length = seq.index(self.config.bos_token_id)` ?   context_length\
          \ - seq_length == 1 ?  why?"
        updatedAt: '2023-03-21T02:31:04.298Z'
      numEdits: 0
      reactions: []
    id: 641916e822270b3ccf16e712
    type: comment
  author: kebo
  content: "```python\r\n    def get_position_ids(self, seq, mask_position, device,\
    \ gmask=False):\r\n        context_length = seq.index(self.config.bos_token_id)\
    \ + 1\r\n        if self.position_encoding_2d:\r\n            seq_length = seq.index(self.config.bos_token_id)\r\
    \n            position_ids = torch.arange(context_length, dtype=torch.long, device=device)\r\
    \n            if not gmask:\r\n                position_ids[seq_length:] = mask_position\r\
    \n            block_position_ids = torch.cat((\r\n                torch.zeros(seq_length,\
    \ dtype=torch.long, device=device),\r\n                torch.arange(context_length\
    \ - seq_length, dtype=torch.long, device=device) + 1\r\n            ))\r\n   \
    \         position_ids = torch.stack((position_ids, block_position_ids), dim=0)\r\
    \n        else:\r\n            position_ids = torch.arange(context_length, dtype=torch.long,\
    \ device=device)\r\n            if not gmask:\r\n                position_ids[context_length\
    \ - 1:] = mask_position\r\n\r\n        position_ids = position_ids.unsqueeze(0)\r\
    \n\r\n        return position_ids\r\n```\r\n\r\n `context_length = seq.index(self.config.bos_token_id)\
    \ + 1` \r\n`seq_length = seq.index(self.config.bos_token_id)` ?   context_length\
    \ - seq_length == 1 ?  why?"
  created_at: 2023-03-21 01:31:04+00:00
  edited: false
  hidden: false
  id: 641916e822270b3ccf16e712
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
      fullname: Zhengxiao Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zxdu20
      type: user
    createdAt: '2023-04-15T11:01:46.000Z'
    data:
      status: closed
    id: 643a841a9f4e4abaf85086b5
    type: status-change
  author: zxdu20
  created_at: 2023-04-15 10:01:46+00:00
  id: 643a841a9f4e4abaf85086b5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: THUDM/chatglm-6b
repo_type: model
status: closed
target_branch: null
title: '[bug] get_position_ids'
