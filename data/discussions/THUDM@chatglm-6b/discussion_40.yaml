!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hiyouga
conflicting_files: null
created_at: 2023-04-12 10:14:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-04-12T11:14:26.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>Hi,</p>

          <p>Thanks for your wonderful work.</p>

          <p>I found the attention mask of ChatGLM uses <code>1</code> to indicate
          the indices to be masked and <code>0</code> to indicate the indices not
          to be masked, which differs from Huggingface''s implementation (see [1]),
          which use <code>1</code> for tokens that are not masked. Although it depends
          on different implementations, the attention mask of ChatGLM may cause unexpected
          problems. For example, it is incompatible with the Prompt-Tuning and P-Tuning
          methods provided by the Huggingface''s PEFT library (see [2]). I wonder
          is there a plan to fix this?</p>

          <p>Looking forward to your reply.</p>

          <p>Sincerely.</p>

          <p>[1] <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/5a71977b8b95d39834f07a1f739305e354bc05d0/src/transformers/models/bert/modeling_bert.py#L828">https://github.com/huggingface/transformers/blob/5a71977b8b95d39834f07a1f739305e354bc05d0/src/transformers/models/bert/modeling_bert.py#L828</a><br>[2]
          <a rel="nofollow" href="https://github.com/huggingface/peft/blob/cc82b674b5db38b9a393463d38afe66e8f48ac1c/src/peft/peft_model.py#L728">https://github.com/huggingface/peft/blob/cc82b674b5db38b9a393463d38afe66e8f48ac1c/src/peft/peft_model.py#L728</a></p>

          '
        raw: "Hi,\r\n\r\nThanks for your wonderful work.\r\n\r\nI found the attention\
          \ mask of ChatGLM uses `1` to indicate the indices to be masked and `0`\
          \ to indicate the indices not to be masked, which differs from Huggingface's\
          \ implementation (see [1]), which use `1` for tokens that are not masked.\
          \ Although it depends on different implementations, the attention mask of\
          \ ChatGLM may cause unexpected problems. For example, it is incompatible\
          \ with the Prompt-Tuning and P-Tuning methods provided by the Huggingface's\
          \ PEFT library (see [2]). I wonder is there a plan to fix this?\r\n\r\n\
          Looking forward to your reply.\r\n\r\nSincerely.\r\n\r\n[1] https://github.com/huggingface/transformers/blob/5a71977b8b95d39834f07a1f739305e354bc05d0/src/transformers/models/bert/modeling_bert.py#L828\r\
          \n[2] https://github.com/huggingface/peft/blob/cc82b674b5db38b9a393463d38afe66e8f48ac1c/src/peft/peft_model.py#L728"
        updatedAt: '2023-04-12T11:14:26.575Z'
      numEdits: 0
      reactions: []
    id: 64369292a76f2c3a1555c575
    type: comment
  author: hiyouga
  content: "Hi,\r\n\r\nThanks for your wonderful work.\r\n\r\nI found the attention\
    \ mask of ChatGLM uses `1` to indicate the indices to be masked and `0` to indicate\
    \ the indices not to be masked, which differs from Huggingface's implementation\
    \ (see [1]), which use `1` for tokens that are not masked. Although it depends\
    \ on different implementations, the attention mask of ChatGLM may cause unexpected\
    \ problems. For example, it is incompatible with the Prompt-Tuning and P-Tuning\
    \ methods provided by the Huggingface's PEFT library (see [2]). I wonder is there\
    \ a plan to fix this?\r\n\r\nLooking forward to your reply.\r\n\r\nSincerely.\r\
    \n\r\n[1] https://github.com/huggingface/transformers/blob/5a71977b8b95d39834f07a1f739305e354bc05d0/src/transformers/models/bert/modeling_bert.py#L828\r\
    \n[2] https://github.com/huggingface/peft/blob/cc82b674b5db38b9a393463d38afe66e8f48ac1c/src/peft/peft_model.py#L728"
  created_at: 2023-04-12 10:14:26+00:00
  edited: false
  hidden: false
  id: 64369292a76f2c3a1555c575
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-04-12T13:59:54.000Z'
    data:
      from: About attention_mask
      to: About the unusual attention_mask of ChatGLM
    id: 6436b95a2e14d51720feecc6
    type: title-change
  author: hiyouga
  created_at: 2023-04-12 12:59:54+00:00
  id: 6436b95a2e14d51720feecc6
  new_title: About the unusual attention_mask of ChatGLM
  old_title: About attention_mask
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IIYLstf-q7ZFWDZ3FHyiC.jpeg?w=200&h=200&f=face
      fullname: Tao Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: twang2218
      type: user
    createdAt: '2023-06-29T10:26:38.000Z'
    data:
      edited: false
      editors:
      - twang2218
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4797261357307434
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IIYLstf-q7ZFWDZ3FHyiC.jpeg?w=200&h=200&f=face
          fullname: Tao Wang
          isHf: false
          isPro: false
          name: twang2218
          type: user
        html: "<p>I also noticed the unusual <code>attention_mask</code> for <code>THUDM/chatglm-6b</code>,\
          \ here is my findings:</p>\n<pre><code class=\"language-python\">kwargs\
          \ = {\n    <span class=\"hljs-string\">'max_length'</span>: <span class=\"\
          hljs-number\">5</span>,\n    <span class=\"hljs-string\">'padding'</span>:\
          \ <span class=\"hljs-literal\">True</span>,\n    <span class=\"hljs-string\"\
          >'truncation'</span>: <span class=\"hljs-literal\">True</span>,\n    <span\
          \ class=\"hljs-string\">'add_special_tokens'</span>: <span class=\"hljs-literal\"\
          >False</span>,\n}\ntext = <span class=\"hljs-string\">'\u6C49'</span>\n\
          tokenizer(text, **kwargs)\n</code></pre>\n<ul>\n<li>ChatGLM-6B</li>\n</ul>\n\
          <pre><code class=\"language-json\"><span class=\"hljs-punctuation\">{</span>'input_ids'<span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span\
          \ class=\"hljs-number\">5</span><span class=\"hljs-punctuation\">,</span>\
          \ <span class=\"hljs-number\">64876</span><span class=\"hljs-punctuation\"\
          >]</span><span class=\"hljs-punctuation\">,</span> 'attention_mask'<span\
          \ class=\"hljs-punctuation\">:</span> array(<span class=\"hljs-punctuation\"\
          >[</span><span class=\"hljs-punctuation\">[</span><span class=\"hljs-punctuation\"\
          >[</span>False<span class=\"hljs-punctuation\">,</span> False<span class=\"\
          hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n \
          \       <span class=\"hljs-punctuation\">[</span>False<span class=\"hljs-punctuation\"\
          >,</span> False<span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\"\
          >]</span><span class=\"hljs-punctuation\">]</span>)<span class=\"hljs-punctuation\"\
          >,</span> 'position_ids'<span class=\"hljs-punctuation\">:</span> array(<span\
          \ class=\"hljs-punctuation\">[</span><span class=\"hljs-punctuation\">[</span><span\
          \ class=\"hljs-number\">0</span><span class=\"hljs-punctuation\">,</span>\
          \ <span class=\"hljs-number\">1</span><span class=\"hljs-punctuation\">]</span><span\
          \ class=\"hljs-punctuation\">,</span>\n       <span class=\"hljs-punctuation\"\
          >[</span><span class=\"hljs-number\">0</span><span class=\"hljs-punctuation\"\
          >,</span> <span class=\"hljs-number\">0</span><span class=\"hljs-punctuation\"\
          >]</span><span class=\"hljs-punctuation\">]</span>)<span class=\"hljs-punctuation\"\
          >}</span>\n</code></pre>\n<ul>\n<li>ChatGLM2-6B</li>\n</ul>\n<pre><code\
          \ class=\"language-json\"><span class=\"hljs-punctuation\">{</span>'input_ids'<span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span\
          \ class=\"hljs-number\">30910</span><span class=\"hljs-punctuation\">,</span>\
          \ <span class=\"hljs-number\">55313</span><span class=\"hljs-punctuation\"\
          >]</span><span class=\"hljs-punctuation\">,</span> 'attention_mask'<span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span\
          \ class=\"hljs-number\">1</span><span class=\"hljs-punctuation\">,</span>\
          \ <span class=\"hljs-number\">1</span><span class=\"hljs-punctuation\">]</span><span\
          \ class=\"hljs-punctuation\">,</span> 'position_ids'<span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-punctuation\">[</span><span class=\"hljs-number\"\
          >0</span><span class=\"hljs-punctuation\">,</span> <span class=\"hljs-number\"\
          >1</span><span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\"\
          >}</span>\n</code></pre>\n<ul>\n<li>bert-base-chinese</li>\n</ul>\n<pre><code\
          \ class=\"language-json\"><span class=\"hljs-punctuation\">{</span>'input_ids'<span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span\
          \ class=\"hljs-number\">3727</span><span class=\"hljs-punctuation\">]</span><span\
          \ class=\"hljs-punctuation\">,</span> 'token_type_ids'<span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-punctuation\">[</span><span class=\"hljs-number\"\
          >0</span><span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\"\
          >,</span> 'attention_mask'<span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-punctuation\">[</span><span class=\"hljs-number\">1</span><span\
          \ class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">}</span>\n\
          </code></pre>\n<p><code>False</code> is <strong>NOT masked</strong> here,\
          \ and <code>int(False)</code> is <code>0</code>, that might be where <code>0</code>\
          \ comes from.</p>\n<p>Another thing is the shape of the<code>attention_mask</code>\
          \ is unusual as well.</p>\n<p><code>(1, 2, 2)</code> which should be <code>(2,)</code></p>\n\
          <p>The code which generated those <code>attention_mask</code> is here:</p>\n\
          <ul>\n<li><a href=\"https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py#L404-L407\"\
          >https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py#L404-L407</a></li>\n\
          </ul>\n<pre><code class=\"language-python\">                attention_mask\
          \ = np.ones((<span class=\"hljs-number\">1</span>, seq_length, seq_length))\n\
          \                attention_mask = np.tril(attention_mask)\n            \
          \    attention_mask[:, :, :context_length] = <span class=\"hljs-number\"\
          >1</span>\n                attention_mask = np.bool_(attention_mask &lt;\
          \ <span class=\"hljs-number\">0.5</span>)\n</code></pre>\n<p>To convert\
          \ the <code>attention_mask</code> to the normal one, I used the following\
          \ code:</p>\n<pre><code class=\"language-python\">attention_mask = np.where([m[<span\
          \ class=\"hljs-number\">0</span>][-<span class=\"hljs-number\">1</span>]\
          \ <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\"\
          >in</span> attention_mask], <span class=\"hljs-number\">0</span>, <span\
          \ class=\"hljs-number\">1</span>)\n</code></pre>\n"
        raw: "I also noticed the unusual `attention_mask` for `THUDM/chatglm-6b`,\
          \ here is my findings:\n\n```python\nkwargs = {\n    'max_length': 5,\n\
          \    'padding': True,\n    'truncation': True,\n    'add_special_tokens':\
          \ False,\n}\ntext = '\u6C49'\ntokenizer(text, **kwargs)\n```\n\n* ChatGLM-6B\n\
          \n```json\n{'input_ids': [5, 64876], 'attention_mask': array([[[False, False],\n\
          \        [False, False]]]), 'position_ids': array([[0, 1],\n       [0, 0]])}\n\
          ```\n\n* ChatGLM2-6B\n\n```json\n{'input_ids': [30910, 55313], 'attention_mask':\
          \ [1, 1], 'position_ids': [0, 1]}\n```\n\n* bert-base-chinese\n\n```json\n\
          {'input_ids': [3727], 'token_type_ids': [0], 'attention_mask': [1]}\n```\n\
          \n`False` is **NOT masked** here, and `int(False)` is `0`, that might be\
          \ where `0` comes from.\n\nAnother thing is the shape of the`attention_mask`\
          \ is unusual as well.\n\n`(1, 2, 2)` which should be `(2,)`\n\nThe code\
          \ which generated those `attention_mask` is here:\n\n* https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py#L404-L407\n\
          \n```python\n                attention_mask = np.ones((1, seq_length, seq_length))\n\
          \                attention_mask = np.tril(attention_mask)\n            \
          \    attention_mask[:, :, :context_length] = 1\n                attention_mask\
          \ = np.bool_(attention_mask < 0.5)\n```\n\nTo convert the `attention_mask`\
          \ to the normal one, I used the following code:\n\n```python\nattention_mask\
          \ = np.where([m[0][-1] for m in attention_mask], 0, 1)\n```\n"
        updatedAt: '2023-06-29T10:26:38.465Z'
      numEdits: 0
      reactions: []
    id: 649d5c5eae6e9abae4f9d526
    type: comment
  author: twang2218
  content: "I also noticed the unusual `attention_mask` for `THUDM/chatglm-6b`, here\
    \ is my findings:\n\n```python\nkwargs = {\n    'max_length': 5,\n    'padding':\
    \ True,\n    'truncation': True,\n    'add_special_tokens': False,\n}\ntext =\
    \ '\u6C49'\ntokenizer(text, **kwargs)\n```\n\n* ChatGLM-6B\n\n```json\n{'input_ids':\
    \ [5, 64876], 'attention_mask': array([[[False, False],\n        [False, False]]]),\
    \ 'position_ids': array([[0, 1],\n       [0, 0]])}\n```\n\n* ChatGLM2-6B\n\n```json\n\
    {'input_ids': [30910, 55313], 'attention_mask': [1, 1], 'position_ids': [0, 1]}\n\
    ```\n\n* bert-base-chinese\n\n```json\n{'input_ids': [3727], 'token_type_ids':\
    \ [0], 'attention_mask': [1]}\n```\n\n`False` is **NOT masked** here, and `int(False)`\
    \ is `0`, that might be where `0` comes from.\n\nAnother thing is the shape of\
    \ the`attention_mask` is unusual as well.\n\n`(1, 2, 2)` which should be `(2,)`\n\
    \nThe code which generated those `attention_mask` is here:\n\n* https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py#L404-L407\n\
    \n```python\n                attention_mask = np.ones((1, seq_length, seq_length))\n\
    \                attention_mask = np.tril(attention_mask)\n                attention_mask[:,\
    \ :, :context_length] = 1\n                attention_mask = np.bool_(attention_mask\
    \ < 0.5)\n```\n\nTo convert the `attention_mask` to the normal one, I used the\
    \ following code:\n\n```python\nattention_mask = np.where([m[0][-1] for m in attention_mask],\
    \ 0, 1)\n```\n"
  created_at: 2023-06-29 09:26:38+00:00
  edited: false
  hidden: false
  id: 649d5c5eae6e9abae4f9d526
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 40
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: About the unusual attention_mask of ChatGLM
