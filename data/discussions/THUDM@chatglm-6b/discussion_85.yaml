!!python/object:huggingface_hub.community.DiscussionWithDetails
author: changwangss
conflicting_files: null
created_at: 2023-06-28 03:42:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e38a35ca22da4c5ba89bb8a87482bb61.svg
      fullname: Chang Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: changwangss
      type: user
    createdAt: '2023-06-28T04:42:00.000Z'
    data:
      edited: false
      editors:
      - changwangss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4051785469055176
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e38a35ca22da4c5ba89bb8a87482bb61.svg
          fullname: Chang Wang
          isHf: false
          isPro: false
          name: changwangss
          type: user
        html: "<pre><code>Traceback (most recent call last):\n  File \"test.py\",\
          \ line 5, in &lt;module&gt;\n    response, history = model.chat(tokenizer,\
          \ \"\u4F60\u597D\", history=[])\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 1285, in chat\n    outputs = self.generate(**inputs, **gen_kwargs)\n\
          \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\n    return self.sample(\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 2619, in sample\n    outputs = self(\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 1190, in forward\n    transformer_outputs = self.transformer(\n \
          \ File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 996, in forward\n    layer_ret = layer(\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 624, in forward\n    attention_input = self.input_layernorm(hidden_states)\n\
          \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/normalization.py\"\
          , line 190, in forward\n    return F.layer_norm(\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/functional.py\"\
          , line 2548, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: mixed\
          \ dtype (CPU): all inputs must share same datatype.\n</code></pre>\n"
        raw: "```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line\
          \ 5, in <module>\r\n    response, history = model.chat(tokenizer, \"\u4F60\
          \u597D\", history=[])\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 1285, in chat\r\n    outputs = self.generate(**inputs, **gen_kwargs)\r\
          \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\r\n    return self.sample(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 2619, in sample\r\n    outputs = self(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 1190, in forward\r\n    transformer_outputs = self.transformer(\r\
          \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 996, in forward\r\n    layer_ret = layer(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
          , line 624, in forward\r\n    attention_input = self.input_layernorm(hidden_states)\r\
          \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/normalization.py\"\
          , line 190, in forward\r\n    return F.layer_norm(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/functional.py\"\
          , line 2548, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: mixed\
          \ dtype (CPU): all inputs must share same datatype.\r\n\r\n```"
        updatedAt: '2023-06-28T04:42:00.427Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Aleou27
        - Jacky0824
    id: 649bba182a8d0042c30390fc
    type: comment
  author: changwangss
  content: "```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line\
    \ 5, in <module>\r\n    response, history = model.chat(tokenizer, \"\u4F60\u597D\
    \", history=[])\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
    , line 1285, in chat\r\n    outputs = self.generate(**inputs, **gen_kwargs)\r\n\
    \  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/transformers/generation/utils.py\"\
    , line 1572, in generate\r\n    return self.sample(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/transformers/generation/utils.py\"\
    , line 2619, in sample\r\n    outputs = self(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
    , line 1190, in forward\r\n    transformer_outputs = self.transformer(\r\n  File\
    \ \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
    , line 996, in forward\r\n    layer_ret = layer(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/dataset/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\"\
    , line 624, in forward\r\n    attention_input = self.input_layernorm(hidden_states)\r\
    \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/modules/normalization.py\"\
    , line 190, in forward\r\n    return F.layer_norm(\r\n  File \"/home/changwa1/anaconda3/envs/ipex_latest/lib/python3.8/site-packages/torch/nn/functional.py\"\
    , line 2548, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
    \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: mixed dtype\
    \ (CPU): all inputs must share same datatype.\r\n\r\n```"
  created_at: 2023-06-28 03:42:00+00:00
  edited: false
  hidden: false
  id: 649bba182a8d0042c30390fc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 85
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: Could you support CPU device?
