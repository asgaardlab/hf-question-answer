!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Au3C2
conflicting_files: null
created_at: 2023-05-15 02:43:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da37631d00ebfe2d0e9a764a6d4bd2d7.svg
      fullname: JoeyLin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Au3C2
      type: user
    createdAt: '2023-05-15T03:43:04.000Z'
    data:
      edited: false
      editors:
      - Au3C2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da37631d00ebfe2d0e9a764a6d4bd2d7.svg
          fullname: JoeyLin
          isHf: false
          isPro: false
          name: Au3C2
          type: user
        html: "<h3 id=\"is-there-an-existing-issue-for-this\">Is there an existing\
          \ issue for this?</h3>\n<ul>\n<li><input type=\"checkbox\" disabled=\"\"\
          \ checked=\"\"> I have searched the existing issues</li>\n</ul>\n<h3 id=\"\
          current-behavior\">Current Behavior</h3>\n<p>ice_text.model\u8BCD\u8868\u957F\
          \u5EA6130344</p>\n<pre><code class=\"language-python\"><span class=\"hljs-meta\"\
          >&gt;&gt;&gt; </span><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModel\n\
          <span class=\"hljs-meta\">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"THUDM/chatglm-6b\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n<span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span><span class=\"hljs-built_in\">len</span>(tokenizer.get_vocab())\n\
          <span class=\"hljs-number\">130344</span>\n</code></pre>\n<p>config:</p>\n\
          <pre><code class=\"language-json\"><span class=\"hljs-attr\">\"vocab_size\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >130528</span>\n</code></pre>\n<p>\u6A21\u578B\u53C2\u6570:</p>\n<pre><code\
          \ class=\"language-log\">transformer.word_embeddings.embedding_table torch.Size([130528,\
          \ 4096]) torch.float16\nlm_head.weight torch.Size([130528, 4096]) torch.float16\n\
          </code></pre>\n<p>\u8BCD\u8868\u957F\u5EA6\u4E0D\u4E00\u81F4\u5BFC\u81F4\
          \u6709\u65F6\u4F1A\u751F\u6210\u8BCD\u8868\u5916\u7684\u8BCD\uFF0C\u7136\
          \u540E\u7D22\u5F15\u8D8A\u754C\u9000\u51FA</p>\n<h3 id=\"expected-behavior\"\
          >Expected Behavior</h3>\n<p>\u8BCD\u8868\u5927\u5C0F\u4E0Econfig\u3001\u6A21\
          \u578B\u53C2\u6570\u4E00\u81F4</p>\n<h3 id=\"steps-to-reproduce\">Steps\
          \ To Reproduce</h3>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-meta\">&gt;&gt;&gt; </span><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ AutoModel\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"THUDM/chatglm-6b\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n<span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span><span class=\"hljs-built_in\">len</span>(tokenizer.get_vocab())\n\
          <span class=\"hljs-number\">130344</span>\n</code></pre>\n<h3 id=\"environment\"\
          >Environment</h3>\n<pre><code class=\"language-markdown\"><span class=\"\
          hljs-bullet\">-</span> OS:\n<span class=\"hljs-bullet\">-</span> Python:\n\
          <span class=\"hljs-bullet\">-</span> Transformers:\n<span class=\"hljs-bullet\"\
          >-</span> PyTorch:\n<span class=\"hljs-bullet\">-</span> CUDA Support (<span\
          \ class=\"hljs-code\">`python -c \"import torch; print(torch.cuda.is_available())\"\
          `</span>) :\n</code></pre>\n<h3 id=\"anything-else\">Anything else?</h3>\n\
          <p><em>No response</em></p>\n"
        raw: "### Is there an existing issue for this?\r\n\r\n- [x] I have searched\
          \ the existing issues\r\n\r\n### Current Behavior\r\n\r\nice_text.model\u8BCD\
          \u8868\u957F\u5EA6130344\r\n```python\r\n>>> from transformers import AutoTokenizer,\
          \ AutoModel\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\"\
          , trust_remote_code=True)\r\n>>> len(tokenizer.get_vocab())\r\n130344\r\n\
          ```\r\nconfig:\r\n```json\r\n\"vocab_size\": 130528\r\n```\r\n\u6A21\u578B\
          \u53C2\u6570:\r\n```log\r\ntransformer.word_embeddings.embedding_table torch.Size([130528,\
          \ 4096]) torch.float16\r\nlm_head.weight torch.Size([130528, 4096]) torch.float16\r\
          \n```\r\n\u8BCD\u8868\u957F\u5EA6\u4E0D\u4E00\u81F4\u5BFC\u81F4\u6709\u65F6\
          \u4F1A\u751F\u6210\u8BCD\u8868\u5916\u7684\u8BCD\uFF0C\u7136\u540E\u7D22\
          \u5F15\u8D8A\u754C\u9000\u51FA\r\n\r\n### Expected Behavior\r\n\r\n\u8BCD\
          \u8868\u5927\u5C0F\u4E0Econfig\u3001\u6A21\u578B\u53C2\u6570\u4E00\u81F4\
          \r\n\r\n### Steps To Reproduce\r\n\r\n```python\r\n>>> from transformers\
          \ import AutoTokenizer, AutoModel\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"\
          THUDM/chatglm-6b\", trust_remote_code=True)\r\n>>> len(tokenizer.get_vocab())\r\
          \n130344\r\n```\r\n\r\n### Environment\r\n\r\n```markdown\r\n- OS:\r\n-\
          \ Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA Support (`python -c\
          \ \"import torch; print(torch.cuda.is_available())\"`) :\r\n```\r\n\r\n\r\
          \n### Anything else?\r\n\r\n_No response_"
        updatedAt: '2023-05-15T03:43:04.589Z'
      numEdits: 0
      reactions: []
    id: 6461aa48cf638aa8f856a524
    type: comment
  author: Au3C2
  content: "### Is there an existing issue for this?\r\n\r\n- [x] I have searched\
    \ the existing issues\r\n\r\n### Current Behavior\r\n\r\nice_text.model\u8BCD\u8868\
    \u957F\u5EA6130344\r\n```python\r\n>>> from transformers import AutoTokenizer,\
    \ AutoModel\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\"\
    , trust_remote_code=True)\r\n>>> len(tokenizer.get_vocab())\r\n130344\r\n```\r\
    \nconfig:\r\n```json\r\n\"vocab_size\": 130528\r\n```\r\n\u6A21\u578B\u53C2\u6570\
    :\r\n```log\r\ntransformer.word_embeddings.embedding_table torch.Size([130528,\
    \ 4096]) torch.float16\r\nlm_head.weight torch.Size([130528, 4096]) torch.float16\r\
    \n```\r\n\u8BCD\u8868\u957F\u5EA6\u4E0D\u4E00\u81F4\u5BFC\u81F4\u6709\u65F6\u4F1A\
    \u751F\u6210\u8BCD\u8868\u5916\u7684\u8BCD\uFF0C\u7136\u540E\u7D22\u5F15\u8D8A\
    \u754C\u9000\u51FA\r\n\r\n### Expected Behavior\r\n\r\n\u8BCD\u8868\u5927\u5C0F\
    \u4E0Econfig\u3001\u6A21\u578B\u53C2\u6570\u4E00\u81F4\r\n\r\n### Steps To Reproduce\r\
    \n\r\n```python\r\n>>> from transformers import AutoTokenizer, AutoModel\r\n>>>\
    \ tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\r\
    \n>>> len(tokenizer.get_vocab())\r\n130344\r\n```\r\n\r\n### Environment\r\n\r\
    \n```markdown\r\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA\
    \ Support (`python -c \"import torch; print(torch.cuda.is_available())\"`) :\r\
    \n```\r\n\r\n\r\n### Anything else?\r\n\r\n_No response_"
  created_at: 2023-05-15 02:43:04+00:00
  edited: false
  hidden: false
  id: 6461aa48cf638aa8f856a524
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/da37631d00ebfe2d0e9a764a6d4bd2d7.svg
      fullname: JoeyLin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Au3C2
      type: user
    createdAt: '2023-05-15T03:44:03.000Z'
    data:
      from: Au3C2
      to: "[BUG/Help] ice_text.model\u8BCD\u8868\u957F\u5EA6\u4E0Econfig\u91CC\u8BBE\
        \u7F6E\u4E0D\u4E00\u81F4"
    id: 6461aa83b2ae2983b10b1046
    type: title-change
  author: Au3C2
  created_at: 2023-05-15 02:44:03+00:00
  id: 6461aa83b2ae2983b10b1046
  new_title: "[BUG/Help] ice_text.model\u8BCD\u8868\u957F\u5EA6\u4E0Econfig\u91CC\u8BBE\
    \u7F6E\u4E0D\u4E00\u81F4"
  old_title: Au3C2
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 65
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: "[BUG/Help] ice_text.model\u8BCD\u8868\u957F\u5EA6\u4E0Econfig\u91CC\u8BBE\u7F6E\
  \u4E0D\u4E00\u81F4"
