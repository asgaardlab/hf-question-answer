!!python/object:huggingface_hub.community.DiscussionWithDetails
author: qingsonglv
conflicting_files: null
created_at: 2023-04-05 09:05:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff14b64339217858927ae01c2b2fa28d.svg
      fullname: Qingsong Lv
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: qingsonglv
      type: user
    createdAt: '2023-04-05T10:05:11.000Z'
    data:
      edited: true
      editors:
      - qingsonglv
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff14b64339217858927ae01c2b2fa28d.svg
          fullname: Qingsong Lv
          isHf: false
          isPro: false
          name: qingsonglv
          type: user
        html: "<p><a href=\"https://huggingface.co/THUDM/chatglm-6b/commit/fdb7a601d8f8279806124542e11549bdd76f62f6\"\
          >https://huggingface.co/THUDM/chatglm-6b/commit/fdb7a601d8f8279806124542e11549bdd76f62f6</a></p>\n\
          <p>\u8FD9\u6BB5\u4EE3\u7801\uFF1A</p>\n<pre><code class=\"language-python\"\
          >tokens = tokenizer(text, return_tensors=<span class=\"hljs-string\">'pt'</span>)[<span\
          \ class=\"hljs-string\">'input_ids'</span>].cuda()\ngen_kwargs = {<span\
          \ class=\"hljs-string\">\"max_length\"</span>: <span class=\"hljs-number\"\
          >512</span>, <span class=\"hljs-string\">\"num_beams\"</span>: <span class=\"\
          hljs-number\">1</span>, <span class=\"hljs-string\">\"do_sample\"</span>:\
          \ <span class=\"hljs-literal\">True</span>, <span class=\"hljs-string\"\
          >\"top_p\"</span>: <span class=\"hljs-number\">0.7</span>,\n           \
          \     <span class=\"hljs-string\">\"temperature\"</span>: <span class=\"\
          hljs-number\">0.95</span>}\noutputs = model.generate(input_ids=tokens, **gen_kwargs)\n\
          preds = tokenizer.batch_decode(outputs, skip_special_tokens=<span class=\"\
          hljs-literal\">True</span>)\n</code></pre>\n<p>\u7528\u6700\u65B0\u7684\u7248\
          \u672Cdecode\u4F1A\u62A5\u8FD9\u4E2A\u9519\u8BEF\uFF1A</p>\n<pre><code>Traceback\
          \ (most recent call last):                                             \
          \                               \n  File \"inference_adgen.py\", line 28,\
          \ in &lt;module&gt;                                                    \
          \         \n    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\
          \                                         \n  File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 3437, in batch_d\necode                                         \
          \                                                                \n    return\
          \ [                                                                    \
          \                              \n  File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 3438, in &lt;listco\nmp&gt;                                     \
          \                                                                      \n\
          \    self.decode(                                                      \
          \                                        \n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/fdb7a601d8f8279806124542e11549b\n\
          dd76f62f6/tokenization_chatglm.py\", line 276, in decode               \
          \                                        \n    if self.pad_token_id in token_ids:\
          \  # remove pad                                                        \
          \  \nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\
          </code></pre>\n<p>\u65AD\u70B9\u6253\u5370\u5BF9\u5E94\u7684\u53D8\u91CF\
          \u5982\u4E0B\uFF1A</p>\n<pre><code>&gt; /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/fdb7a601d8f8279806124542e11549bdd76f6\n\
          2f6/tokenization_chatglm.py(277)decode()\n-&gt; if self.pad_token_id in\
          \ token_ids:  # remove pad\n(Pdb) p self.pad_token_id\n20003\n(Pdb) p token_ids\n\
          [tensor([ 20005,  85421,  20061,  95898,  20032,  88554,  20061,  97257,\
          \  84555,\n         20032,  85107,  20061,  86268,  20032,  85347,  20061,\
          \  91689,  20032,\n         89768,  20061, 105428,  20032,  85173,  93942,\
          \  20061,  90984,  20032,\n         85173,  90936,  20061,  84703,  85509,\
          \ 150001, 150004,  20005,  84703,\n         85509,  94681,  83829,  85388,\
          \  85023,  92871, 106418,  83825,  94707,\n         20006,  83827,  94681,\
          \  83839,  84690, 105428,  89768,  20006,  85109,\n         94705, 107834,\
          \  20006,  83887,  98530,  85425, 116447,  20006,  97284,\n         83842,\
          \ 101233, 109473, 109473,  83825, 139235,  83823,  92194,  91231,\n    \
          \     20006,  87122,  91689,  20006,  86955,  86800,  83823, 150005],\n\
          \       device='cuda:0')]\n(Pdb) \n</code></pre>\n"
        raw: "https://huggingface.co/THUDM/chatglm-6b/commit/fdb7a601d8f8279806124542e11549bdd76f62f6\n\
          \n\u8FD9\u6BB5\u4EE3\u7801\uFF1A\n\n```python\ntokens = tokenizer(text,\
          \ return_tensors='pt')['input_ids'].cuda()\ngen_kwargs = {\"max_length\"\
          : 512, \"num_beams\": 1, \"do_sample\": True, \"top_p\": 0.7,\n        \
          \        \"temperature\": 0.95}\noutputs = model.generate(input_ids=tokens,\
          \ **gen_kwargs)\npreds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\
          ```\n\n\u7528\u6700\u65B0\u7684\u7248\u672Cdecode\u4F1A\u62A5\u8FD9\u4E2A\
          \u9519\u8BEF\uFF1A\n\n```\nTraceback (most recent call last):          \
          \                                                                  \n  File\
          \ \"inference_adgen.py\", line 28, in <module>                         \
          \                                    \n    preds = tokenizer.batch_decode(outputs,\
          \ skip_special_tokens=True)                                         \n \
          \ File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 3437, in batch_d\necode                                         \
          \                                                                \n    return\
          \ [                                                                    \
          \                              \n  File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 3438, in <listco\nmp>                                           \
          \                                                                \n    self.decode(\
          \                                                                      \
          \                        \n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/fdb7a601d8f8279806124542e11549b\n\
          dd76f62f6/tokenization_chatglm.py\", line 276, in decode               \
          \                                        \n    if self.pad_token_id in token_ids:\
          \  # remove pad                                                        \
          \  \nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\
          ```\n\n\u65AD\u70B9\u6253\u5370\u5BF9\u5E94\u7684\u53D8\u91CF\u5982\u4E0B\
          \uFF1A\n\n```\n> /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/fdb7a601d8f8279806124542e11549bdd76f6\n\
          2f6/tokenization_chatglm.py(277)decode()\n-> if self.pad_token_id in token_ids:\
          \  # remove pad\n(Pdb) p self.pad_token_id\n20003\n(Pdb) p token_ids\n[tensor([\
          \ 20005,  85421,  20061,  95898,  20032,  88554,  20061,  97257,  84555,\n\
          \         20032,  85107,  20061,  86268,  20032,  85347,  20061,  91689,\
          \  20032,\n         89768,  20061, 105428,  20032,  85173,  93942,  20061,\
          \  90984,  20032,\n         85173,  90936,  20061,  84703,  85509, 150001,\
          \ 150004,  20005,  84703,\n         85509,  94681,  83829,  85388,  85023,\
          \  92871, 106418,  83825,  94707,\n         20006,  83827,  94681,  83839,\
          \  84690, 105428,  89768,  20006,  85109,\n         94705, 107834,  20006,\
          \  83887,  98530,  85425, 116447,  20006,  97284,\n         83842, 101233,\
          \ 109473, 109473,  83825, 139235,  83823,  92194,  91231,\n         20006,\
          \  87122,  91689,  20006,  86955,  86800,  83823, 150005],\n       device='cuda:0')]\n\
          (Pdb) \n```"
        updatedAt: '2023-04-05T10:23:10.219Z'
      numEdits: 1
      reactions: []
    id: 642d47d7f65714b4585fe09b
    type: comment
  author: qingsonglv
  content: "https://huggingface.co/THUDM/chatglm-6b/commit/fdb7a601d8f8279806124542e11549bdd76f62f6\n\
    \n\u8FD9\u6BB5\u4EE3\u7801\uFF1A\n\n```python\ntokens = tokenizer(text, return_tensors='pt')['input_ids'].cuda()\n\
    gen_kwargs = {\"max_length\": 512, \"num_beams\": 1, \"do_sample\": True, \"top_p\"\
    : 0.7,\n                \"temperature\": 0.95}\noutputs = model.generate(input_ids=tokens,\
    \ **gen_kwargs)\npreds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\
    ```\n\n\u7528\u6700\u65B0\u7684\u7248\u672Cdecode\u4F1A\u62A5\u8FD9\u4E2A\u9519\
    \u8BEF\uFF1A\n\n```\nTraceback (most recent call last):                      \
    \                                                      \n  File \"inference_adgen.py\"\
    , line 28, in <module>                                                       \
    \      \n    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\
    \                                         \n  File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\"\
    , line 3437, in batch_d\necode                                               \
    \                                                          \n    return [    \
    \                                                                            \
    \                  \n  File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\"\
    , line 3438, in <listco\nmp>                                                 \
    \                                                          \n    self.decode(\
    \                                                                            \
    \                  \n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/fdb7a601d8f8279806124542e11549b\n\
    dd76f62f6/tokenization_chatglm.py\", line 276, in decode                     \
    \                                  \n    if self.pad_token_id in token_ids:  #\
    \ remove pad                                                          \nRuntimeError:\
    \ Boolean value of Tensor with more than one value is ambiguous\n```\n\n\u65AD\
    \u70B9\u6253\u5370\u5BF9\u5E94\u7684\u53D8\u91CF\u5982\u4E0B\uFF1A\n\n```\n> /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/fdb7a601d8f8279806124542e11549bdd76f6\n\
    2f6/tokenization_chatglm.py(277)decode()\n-> if self.pad_token_id in token_ids:\
    \  # remove pad\n(Pdb) p self.pad_token_id\n20003\n(Pdb) p token_ids\n[tensor([\
    \ 20005,  85421,  20061,  95898,  20032,  88554,  20061,  97257,  84555,\n   \
    \      20032,  85107,  20061,  86268,  20032,  85347,  20061,  91689,  20032,\n\
    \         89768,  20061, 105428,  20032,  85173,  93942,  20061,  90984,  20032,\n\
    \         85173,  90936,  20061,  84703,  85509, 150001, 150004,  20005,  84703,\n\
    \         85509,  94681,  83829,  85388,  85023,  92871, 106418,  83825,  94707,\n\
    \         20006,  83827,  94681,  83839,  84690, 105428,  89768,  20006,  85109,\n\
    \         94705, 107834,  20006,  83887,  98530,  85425, 116447,  20006,  97284,\n\
    \         83842, 101233, 109473, 109473,  83825, 139235,  83823,  92194,  91231,\n\
    \         20006,  87122,  91689,  20006,  86955,  86800,  83823, 150005],\n  \
    \     device='cuda:0')]\n(Pdb) \n```"
  created_at: 2023-04-05 09:05:11+00:00
  edited: true
  hidden: false
  id: 642d47d7f65714b4585fe09b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
      fullname: Zhengxiao Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zxdu20
      type: user
    createdAt: '2023-04-05T11:10:25.000Z'
    data:
      edited: false
      editors:
      - zxdu20
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
          fullname: Zhengxiao Du
          isHf: false
          isPro: false
          name: zxdu20
          type: user
        html: '<p>Fixed</p>

          '
        raw: Fixed
        updatedAt: '2023-04-05T11:10:25.314Z'
      numEdits: 0
      reactions: []
      relatedEventId: 642d5721eb4f494c8c108c4c
    id: 642d5721eb4f494c8c108c4b
    type: comment
  author: zxdu20
  content: Fixed
  created_at: 2023-04-05 10:10:25+00:00
  edited: false
  hidden: false
  id: 642d5721eb4f494c8c108c4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
      fullname: Zhengxiao Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zxdu20
      type: user
    createdAt: '2023-04-05T11:10:25.000Z'
    data:
      status: closed
    id: 642d5721eb4f494c8c108c4c
    type: status-change
  author: zxdu20
  created_at: 2023-04-05 10:10:25+00:00
  id: 642d5721eb4f494c8c108c4c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: THUDM/chatglm-6b
repo_type: model
status: closed
target_branch: null
title: "\u6700\u65B0\u7684commit\u597D\u50CF\u6709bug\uFF0C\u4F46\u662F\u4E0D\u786E\
  \u5B9A\u8BE5\u600E\u4E48\u4FEE\u590D"
