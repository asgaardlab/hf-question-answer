!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HuangSong
conflicting_files: null
created_at: 2023-05-17 12:55:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36a6f721265b1834f0474898b41a4ee4.svg
      fullname: HuangSong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HuangSong
      type: user
    createdAt: '2023-05-17T13:55:09.000Z'
    data:
      edited: false
      editors:
      - HuangSong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36a6f721265b1834f0474898b41a4ee4.svg
          fullname: HuangSong
          isHf: false
          isPro: false
          name: HuangSong
          type: user
        html: "<p>\u8BF7\u95EE\u4EE5\u4E0B\u7684\u95EE\u9898\u5E94\u8BE5\u5982\u4F55\
          \u89E3\u51B3\uFF1F\u5C1D\u8BD5\u4E86\u8C03\u6574model = AutoModel.from_pretrained(\"\
          chatglm-6b\", trust_remote_code=True).half().cuda()  \u663E\u5B58\u5927\u5C0F\
          \uFF0C\u5F02\u5E38\u5E76\u65E0\u53D8\u5316\u3002</p>\n<p>\u4F7F\u7528\u7684\
          \u817E\u8BAF\u4E91\u7684GPU T4 \u670D\u52A1\u5668\uFF0C\u540C\u6837\u7684\
          chatglm-6b\u6267\u884C\u5728\u672C\u5730\u7B14\u8BB0\u672C\u80FD\u6B63\u5E38\
          \u542F\u52A8\uFF0C\u4F46\u662F\u5728\u4E91\u7AEF\u629B\u5F02\u5E38\u3002\
          \u5B89\u88C5\u6B65\u9AA4\u5982\u4E0B\uFF1A<br>1.git clone <a rel=\"nofollow\"\
          \ href=\"https://github.com/THUDM/ChatGLM-6B\">https://github.com/THUDM/ChatGLM-6B</a><br>2.\
          \ \u8FDB\u5165\u5230\u4E0B\u8F7D\u540E\u7684ChatGLM-6B\u6587\u4EF6\u5939\
          \u4E2D<br>3.\u547D\u4EE4\u884C\u4E2D\u6267\u884C\uFF1Apip install -r requirements.txt\
          \ \u5B89\u88C5\u76F8\u5173\u4F9D\u8D56<br>4.\u5B89\u88C5 Gradio\uFF1Apip\
          \ install gradio<br>5.\u4E0B\u8F7D\u6A21\u578B\uFF1A\u5728ChatGLM-6B-main\u8DEF\
          \u5F84\u4E0B\uFF0C\u547D\u4EE4\u884C\u4E2D\u6267\u884C git clone <a href=\"\
          https://huggingface.co/THUDM/chatglm-6b\">https://huggingface.co/THUDM/chatglm-6b</a>\
          \ \u8FD9\u91CC\u5728\u817E\u8BAF\u4E91\u79D2\u7EA7\u5B8C\u6210\uFF0C\u5E94\
          \u8BE5\u662F\u7528\u4E86\u955C\u50CF\u6587\u4EF6\u3002<br>6. \u66FF\u6362\
          chatGLM6B/config.json\u4E2D\u6A21\u578B\u8DEF\u5F84THUDM/chatglm-6b\u4E3A\
          \ chatglm-6b\uFF0C\u5C06web_demo.py\u4E2D\u6240\u6709\u7684THUDM/chatglm-6b\
          \ \u66FF\u6362\u6210 chatglm-6b<br>7. \u6267\u884Cpython web_demo.py\uFF0C\
          \u5F02\u5E38\u5806\u6808\u5982\u4E0B\uFF1A</p>\n<p>Explicitly passing a\
          \ <code>revision</code> is encouraged when loading a model with custom code\
          \ to ensure no malicious code has been contributed in a newer revision.<br>Traceback\
          \ (most recent call last):<br>  File \"/home/ubuntu/ChatGLM-6B/web_demo.py\"\
          , line 5, in <br>    tokenizer = AutoTokenizer.from_pretrained(\"chatglm-6b\"\
          , trust_remote_code=True)<br>  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 679, in from_pretrained<br>    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)<br>  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1804, in from_pretrained<br>    return cls._from_pretrained(<br>\
          \  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1958, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>\
          \  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
          , line 221, in <strong>init</strong><br>    self.sp_tokenizer = SPTokenizer(vocab_file,\
          \ num_image_tokens=num_image_tokens)<br>  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
          , line 64, in <strong>init</strong><br>    self.text_tokenizer = TextTokenizer(vocab_file)<br>\
          \  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
          , line 22, in <strong>init</strong><br>    self.sp.Load(model_path)<br>\
          \  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/sentencepiece/<strong>init</strong>.py\"\
          , line 905, in Load<br>    return self.LoadFromFile(model_file)<br>  File\
          \ \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/sentencepiece/<strong>init</strong>.py\"\
          , line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)<br>RuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())] </p>\n"
        raw: "\u8BF7\u95EE\u4EE5\u4E0B\u7684\u95EE\u9898\u5E94\u8BE5\u5982\u4F55\u89E3\
          \u51B3\uFF1F\u5C1D\u8BD5\u4E86\u8C03\u6574model = AutoModel.from_pretrained(\"\
          chatglm-6b\", trust_remote_code=True).half().cuda()  \u663E\u5B58\u5927\u5C0F\
          \uFF0C\u5F02\u5E38\u5E76\u65E0\u53D8\u5316\u3002\r\n\r\n\u4F7F\u7528\u7684\
          \u817E\u8BAF\u4E91\u7684GPU T4 \u670D\u52A1\u5668\uFF0C\u540C\u6837\u7684\
          chatglm-6b\u6267\u884C\u5728\u672C\u5730\u7B14\u8BB0\u672C\u80FD\u6B63\u5E38\
          \u542F\u52A8\uFF0C\u4F46\u662F\u5728\u4E91\u7AEF\u629B\u5F02\u5E38\u3002\
          \u5B89\u88C5\u6B65\u9AA4\u5982\u4E0B\uFF1A\r\n1.git clone https://github.com/THUDM/ChatGLM-6B\r\
          \n2. \u8FDB\u5165\u5230\u4E0B\u8F7D\u540E\u7684ChatGLM-6B\u6587\u4EF6\u5939\
          \u4E2D\r\n3.\u547D\u4EE4\u884C\u4E2D\u6267\u884C\uFF1Apip install -r requirements.txt\
          \ \u5B89\u88C5\u76F8\u5173\u4F9D\u8D56\r\n4.\u5B89\u88C5 Gradio\uFF1Apip\
          \ install gradio\r\n5.\u4E0B\u8F7D\u6A21\u578B\uFF1A\u5728ChatGLM-6B-main\u8DEF\
          \u5F84\u4E0B\uFF0C\u547D\u4EE4\u884C\u4E2D\u6267\u884C git clone https://huggingface.co/THUDM/chatglm-6b\
          \ \u8FD9\u91CC\u5728\u817E\u8BAF\u4E91\u79D2\u7EA7\u5B8C\u6210\uFF0C\u5E94\
          \u8BE5\u662F\u7528\u4E86\u955C\u50CF\u6587\u4EF6\u3002\r\n6. \u66FF\u6362\
          chatGLM6B/config.json\u4E2D\u6A21\u578B\u8DEF\u5F84THUDM/chatglm-6b\u4E3A\
          \ chatglm-6b\uFF0C\u5C06web_demo.py\u4E2D\u6240\u6709\u7684THUDM/chatglm-6b\
          \ \u66FF\u6362\u6210 chatglm-6b\r\n7. \u6267\u884Cpython web_demo.py\uFF0C\
          \u5F02\u5E38\u5806\u6808\u5982\u4E0B\uFF1A\r\n\r\nExplicitly passing a `revision`\
          \ is encouraged when loading a model with custom code to ensure no malicious\
          \ code has been contributed in a newer revision.\r\nTraceback (most recent\
          \ call last):\r\n  File \"/home/ubuntu/ChatGLM-6B/web_demo.py\", line 5,\
          \ in <module>\r\n    tokenizer = AutoTokenizer.from_pretrained(\"chatglm-6b\"\
          , trust_remote_code=True)\r\n  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 679, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1804, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1958, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
          , line 221, in __init__\r\n    self.sp_tokenizer = SPTokenizer(vocab_file,\
          \ num_image_tokens=num_image_tokens)\r\n  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
          , line 64, in __init__\r\n    self.text_tokenizer = TextTokenizer(vocab_file)\r\
          \n  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
          , line 22, in __init__\r\n    self.sp.Load(model_path)\r\n  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File\
          \ \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())] "
        updatedAt: '2023-05-17T13:55:09.935Z'
      numEdits: 0
      reactions: []
    id: 6464dcbde27766e892185219
    type: comment
  author: HuangSong
  content: "\u8BF7\u95EE\u4EE5\u4E0B\u7684\u95EE\u9898\u5E94\u8BE5\u5982\u4F55\u89E3\
    \u51B3\uFF1F\u5C1D\u8BD5\u4E86\u8C03\u6574model = AutoModel.from_pretrained(\"\
    chatglm-6b\", trust_remote_code=True).half().cuda()  \u663E\u5B58\u5927\u5C0F\uFF0C\
    \u5F02\u5E38\u5E76\u65E0\u53D8\u5316\u3002\r\n\r\n\u4F7F\u7528\u7684\u817E\u8BAF\
    \u4E91\u7684GPU T4 \u670D\u52A1\u5668\uFF0C\u540C\u6837\u7684chatglm-6b\u6267\u884C\
    \u5728\u672C\u5730\u7B14\u8BB0\u672C\u80FD\u6B63\u5E38\u542F\u52A8\uFF0C\u4F46\
    \u662F\u5728\u4E91\u7AEF\u629B\u5F02\u5E38\u3002\u5B89\u88C5\u6B65\u9AA4\u5982\
    \u4E0B\uFF1A\r\n1.git clone https://github.com/THUDM/ChatGLM-6B\r\n2. \u8FDB\u5165\
    \u5230\u4E0B\u8F7D\u540E\u7684ChatGLM-6B\u6587\u4EF6\u5939\u4E2D\r\n3.\u547D\u4EE4\
    \u884C\u4E2D\u6267\u884C\uFF1Apip install -r requirements.txt \u5B89\u88C5\u76F8\
    \u5173\u4F9D\u8D56\r\n4.\u5B89\u88C5 Gradio\uFF1Apip install gradio\r\n5.\u4E0B\
    \u8F7D\u6A21\u578B\uFF1A\u5728ChatGLM-6B-main\u8DEF\u5F84\u4E0B\uFF0C\u547D\u4EE4\
    \u884C\u4E2D\u6267\u884C git clone https://huggingface.co/THUDM/chatglm-6b \u8FD9\
    \u91CC\u5728\u817E\u8BAF\u4E91\u79D2\u7EA7\u5B8C\u6210\uFF0C\u5E94\u8BE5\u662F\
    \u7528\u4E86\u955C\u50CF\u6587\u4EF6\u3002\r\n6. \u66FF\u6362chatGLM6B/config.json\u4E2D\
    \u6A21\u578B\u8DEF\u5F84THUDM/chatglm-6b\u4E3A chatglm-6b\uFF0C\u5C06web_demo.py\u4E2D\
    \u6240\u6709\u7684THUDM/chatglm-6b \u66FF\u6362\u6210 chatglm-6b\r\n7. \u6267\u884C\
    python web_demo.py\uFF0C\u5F02\u5E38\u5806\u6808\u5982\u4E0B\uFF1A\r\n\r\nExplicitly\
    \ passing a `revision` is encouraged when loading a model with custom code to\
    \ ensure no malicious code has been contributed in a newer revision.\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/ubuntu/ChatGLM-6B/web_demo.py\",\
    \ line 5, in <module>\r\n    tokenizer = AutoTokenizer.from_pretrained(\"chatglm-6b\"\
    , trust_remote_code=True)\r\n  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 679, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1804, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1958, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
    , line 221, in __init__\r\n    self.sp_tokenizer = SPTokenizer(vocab_file, num_image_tokens=num_image_tokens)\r\
    \n  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
    , line 64, in __init__\r\n    self.text_tokenizer = TextTokenizer(vocab_file)\r\
    \n  File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm-6b/tokenization_chatglm.py\"\
    , line 22, in __init__\r\n    self.sp.Load(model_path)\r\n  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"/home/ubuntu/anaconda3/envs/chatGLM/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())] "
  created_at: 2023-05-17 12:55:09+00:00
  edited: false
  hidden: false
  id: 6464dcbde27766e892185219
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 68
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: "\u6C42\u52A9\uFF1A\u8DD1webDemo\u629B\u5F02\u5E38"
