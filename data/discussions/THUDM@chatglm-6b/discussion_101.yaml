!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PiaoYang
conflicting_files: null
created_at: 2024-01-08 09:07:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5f7da303092dc391aeda64d7812cc7c.svg
      fullname: PiaoYang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PiaoYang
      type: user
    createdAt: '2024-01-08T09:07:38.000Z'
    data:
      edited: false
      editors:
      - PiaoYang
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.19625620543956757
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5f7da303092dc391aeda64d7812cc7c.svg
          fullname: PiaoYang
          isHf: false
          isPro: false
          name: PiaoYang
          type: user
        html: "<p>\u4F7F\u7528\u6700\u65B0\u7248Transformers\u5BF9Tokenizer\u8FDB\u884C\
          \u52A0\u8F7D\u7684\u65F6\u5019\u4F1A\u62A5\u9519</p>\n<pre><code>AttributeError:\
          \ 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'. Did you mean:\
          \ '_tokenize'?\n    return self.sp_tokenizer.num_tokens\n</code></pre>\n\
          <p>\u89E3\u51B3\u529E\u6CD5\uFF1A\u4F7F\u7528\u4FEE\u590D\u7248\u7684Tokenizer</p>\n\
          <p>\u5C06<code>THUDM/chatglm-6b</code>\u66FF\u6362\u4E3A<code>PiaoYang/chatglm-6b</code>\u5373\
          \u53EF\u3002\u540C\u65F6\u5982\u679C\u6709<code>revision</code>\u53C2\u6570\
          \uFF0C\u9700\u8981\u53BB\u6389\u3002</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(\"\
          PiaoYang/chatglm-6b\", trust_remote_code=True)\n</code></pre>\n"
        raw: "\u4F7F\u7528\u6700\u65B0\u7248Transformers\u5BF9Tokenizer\u8FDB\u884C\
          \u52A0\u8F7D\u7684\u65F6\u5019\u4F1A\u62A5\u9519\r\n```\r\nAttributeError:\
          \ 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'. Did you mean:\
          \ '_tokenize'?\r\n    return self.sp_tokenizer.num_tokens\r\n```\r\n\r\n\
          \u89E3\u51B3\u529E\u6CD5\uFF1A\u4F7F\u7528\u4FEE\u590D\u7248\u7684Tokenizer\r\
          \n\r\n\u5C06`THUDM/chatglm-6b`\u66FF\u6362\u4E3A`PiaoYang/chatglm-6b`\u5373\
          \u53EF\u3002\u540C\u65F6\u5982\u679C\u6709`revision`\u53C2\u6570\uFF0C\u9700\
          \u8981\u53BB\u6389\u3002\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          PiaoYang/chatglm-6b\", trust_remote_code=True)\r\n```"
        updatedAt: '2024-01-08T09:07:38.053Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PiaoYang
      - count: 1
        reaction: "\U0001F917"
        users:
        - PiaoYang
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - PiaoYang
    id: 659bbb5ab2ec894c51407610
    type: comment
  author: PiaoYang
  content: "\u4F7F\u7528\u6700\u65B0\u7248Transformers\u5BF9Tokenizer\u8FDB\u884C\u52A0\
    \u8F7D\u7684\u65F6\u5019\u4F1A\u62A5\u9519\r\n```\r\nAttributeError: 'ChatGLMTokenizer'\
    \ object has no attribute 'sp_tokenizer'. Did you mean: '_tokenize'?\r\n    return\
    \ self.sp_tokenizer.num_tokens\r\n```\r\n\r\n\u89E3\u51B3\u529E\u6CD5\uFF1A\u4F7F\
    \u7528\u4FEE\u590D\u7248\u7684Tokenizer\r\n\r\n\u5C06`THUDM/chatglm-6b`\u66FF\u6362\
    \u4E3A`PiaoYang/chatglm-6b`\u5373\u53EF\u3002\u540C\u65F6\u5982\u679C\u6709`revision`\u53C2\
    \u6570\uFF0C\u9700\u8981\u53BB\u6389\u3002\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    PiaoYang/chatglm-6b\", trust_remote_code=True)\r\n```"
  created_at: 2024-01-08 09:07:38+00:00
  edited: false
  hidden: false
  id: 659bbb5ab2ec894c51407610
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 101
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: "\u7528\u6700\u65B0\u7248Transformers\u62A5\u9519\u7684\u89E3\u51B3\u529E\u6CD5\
  \ AttributeError: 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'. Did\
  \ you mean: '_tokenize'?     return self.sp_tokenizer.num_tokens"
