!!python/object:huggingface_hub.community.DiscussionWithDetails
author: huoyuan
conflicting_files: null
created_at: 2023-03-14 09:46:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f43f18d39584a007315863411526344e.svg
      fullname: dong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huoyuan
      type: user
    createdAt: '2023-03-14T10:46:59.000Z'
    data:
      edited: false
      editors:
      - huoyuan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f43f18d39584a007315863411526344e.svg
          fullname: dong
          isHf: false
          isPro: false
          name: huoyuan
          type: user
        html: '<p>does anyone train this model on local datasets? which kind of GPU
          we need to finetune this model?</p>

          '
        raw: does anyone train this model on local datasets? which kind of GPU we
          need to finetune this model?
        updatedAt: '2023-03-14T10:46:59.318Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - wanglettes
        - zacharyzhao
        - haipeng
        - Tianyi12138
        - joytafty
        - CeroShrijver
        - BabyRon
        - alex950
        - scy99
      - count: 6
        reaction: "\U0001F91D"
        users:
        - warmilk
        - Tianyi12138
        - leran2098
        - wuzq
        - alex950
        - scy99
    id: 641050a3b27543634e38bdab
    type: comment
  author: huoyuan
  content: does anyone train this model on local datasets? which kind of GPU we need
    to finetune this model?
  created_at: 2023-03-14 09:46:59+00:00
  edited: false
  hidden: false
  id: 641050a3b27543634e38bdab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e021e8b90a739014837f807b4448c5f0.svg
      fullname: asdqwea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asdqwea
      type: user
    createdAt: '2023-03-14T10:59:59.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/e021e8b90a739014837f807b4448c5f0.svg
          fullname: asdqwea
          isHf: false
          isPro: false
          name: asdqwea
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-03-14T11:00:59.133Z'
      numEdits: 0
      reactions: []
    id: 641053afef6be374e0d43c8d
    type: comment
  author: asdqwea
  content: This comment has been hidden
  created_at: 2023-03-14 09:59:59+00:00
  edited: true
  hidden: true
  id: 641053afef6be374e0d43c8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/15431454aaf143aba158b77f98e176b8.svg
      fullname: spencer-yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spencer-yuan
      type: user
    createdAt: '2023-03-14T14:37:48.000Z'
    data:
      edited: false
      editors:
      - spencer-yuan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/15431454aaf143aba158b77f98e176b8.svg
          fullname: spencer-yuan
          isHf: false
          isPro: false
          name: spencer-yuan
          type: user
        html: '<p>I have the same question</p>

          '
        raw: I have the same question
        updatedAt: '2023-03-14T14:37:48.116Z'
      numEdits: 0
      reactions: []
    id: 641086bc2a593afb553e76fc
    type: comment
  author: spencer-yuan
  content: I have the same question
  created_at: 2023-03-14 13:37:48+00:00
  edited: false
  hidden: false
  id: 641086bc2a593afb553e76fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69a18382c0198d5faf1c4569aac94ae3.svg
      fullname: HuangTao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: htao36
      type: user
    createdAt: '2023-03-15T02:24:33.000Z'
    data:
      edited: false
      editors:
      - htao36
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69a18382c0198d5faf1c4569aac94ae3.svg
          fullname: HuangTao
          isHf: false
          isPro: false
          name: htao36
          type: user
        html: '<p>I have the same question</p>

          '
        raw: I have the same question
        updatedAt: '2023-03-15T02:24:33.939Z'
      numEdits: 0
      reactions: []
    id: 64112c61fe063dadcd4784d9
    type: comment
  author: htao36
  content: I have the same question
  created_at: 2023-03-15 01:24:33+00:00
  edited: false
  hidden: false
  id: 64112c61fe063dadcd4784d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672842546205-62d78fbb9266b7aa5309ee99.jpeg?w=200&h=200&f=face
      fullname: wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wdkwdkwdk
      type: user
    createdAt: '2023-03-15T02:28:31.000Z'
    data:
      edited: false
      editors:
      - wdkwdkwdk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672842546205-62d78fbb9266b7aa5309ee99.jpeg?w=200&h=200&f=face
          fullname: wang
          isHf: false
          isPro: false
          name: wdkwdkwdk
          type: user
        html: '<p>I have the same question</p>

          '
        raw: I have the same question
        updatedAt: '2023-03-15T02:28:31.288Z'
      numEdits: 0
      reactions: []
    id: 64112d4fc100524d3ce05f1e
    type: comment
  author: wdkwdkwdk
  content: I have the same question
  created_at: 2023-03-15 01:28:31+00:00
  edited: false
  hidden: false
  id: 64112d4fc100524d3ce05f1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678848503821-64112e2cc406d88b91e5474d.jpeg?w=200&h=200&f=face
      fullname: FatFatCat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: warmilk
      type: user
    createdAt: '2023-03-15T02:33:31.000Z'
    data:
      edited: false
      editors:
      - warmilk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678848503821-64112e2cc406d88b91e5474d.jpeg?w=200&h=200&f=face
          fullname: FatFatCat
          isHf: false
          isPro: false
          name: warmilk
          type: user
        html: '<p>I have the same question, plase share the training dataset</p>

          '
        raw: I have the same question, plase share the training dataset
        updatedAt: '2023-03-15T02:33:31.809Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - warmilk
      - count: 1
        reaction: "\U0001F44D"
        users:
        - warmilk
    id: 64112e7b3da6bf65e69f5793
    type: comment
  author: warmilk
  content: I have the same question, plase share the training dataset
  created_at: 2023-03-15 01:33:31+00:00
  edited: false
  hidden: false
  id: 64112e7b3da6bf65e69f5793
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
      fullname: sadfsaddf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fasoasdf
      type: user
    createdAt: '2023-03-15T02:58:51.000Z'
    data:
      edited: false
      editors:
      - fasoasdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
          fullname: sadfsaddf
          isHf: false
          isPro: false
          name: fasoasdf
          type: user
        html: '<p>I''m trying to use lora from peft to fine-tune this model, and it
          seems to be working.</p>

          '
        raw: I'm trying to use lora from peft to fine-tune this model, and it seems
          to be working.
        updatedAt: '2023-03-15T02:58:51.168Z'
      numEdits: 0
      reactions:
      - count: 12
        reaction: "\U0001F44D"
        users:
        - Xiaomaxiang
        - leran2098
        - htao36
        - evanho
        - GMFTBY
        - Minami-su
        - maschen
        - ZYFDroid
        - zqzq71
        - raohai
        - eternalgogi
        - ycjcl868
      - count: 1
        reaction: "\U0001F92F"
        users:
        - warmilk
    id: 6411346b0403b5c00c65298b
    type: comment
  author: fasoasdf
  content: I'm trying to use lora from peft to fine-tune this model, and it seems
    to be working.
  created_at: 2023-03-15 01:58:51+00:00
  edited: false
  hidden: false
  id: 6411346b0403b5c00c65298b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b86f3b2d0ba0328b11ed24cb46f6f2fb.svg
      fullname: RanLe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leran2098
      type: user
    createdAt: '2023-03-15T12:23:47.000Z'
    data:
      edited: false
      editors:
      - leran2098
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b86f3b2d0ba0328b11ed24cb46f6f2fb.svg
          fullname: RanLe
          isHf: false
          isPro: false
          name: leran2098
          type: user
        html: '<p>I have the same question</p>

          '
        raw: I have the same question
        updatedAt: '2023-03-15T12:23:47.258Z'
      numEdits: 0
      reactions: []
    id: 6411b8d3d7792bd687fac9b5
    type: comment
  author: leran2098
  content: I have the same question
  created_at: 2023-03-15 11:23:47+00:00
  edited: false
  hidden: false
  id: 6411b8d3d7792bd687fac9b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/404869a87480ef526c1d5fccb583bfa8.svg
      fullname: Xinyao Niu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sirius-ctrl
      type: user
    createdAt: '2023-03-16T07:59:34.000Z'
    data:
      edited: false
      editors:
      - sirius-ctrl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/404869a87480ef526c1d5fccb583bfa8.svg
          fullname: Xinyao Niu
          isHf: false
          isPro: false
          name: sirius-ctrl
          type: user
        html: '<blockquote>

          <p>I''m trying to use lora from peft to fine-tune this model, and it seems
          to be working.</p>

          </blockquote>

          <p>Could you share the code?</p>

          '
        raw: '> I''m trying to use lora from peft to fine-tune this model, and it
          seems to be working.


          Could you share the code?'
        updatedAt: '2023-03-16T07:59:34.345Z'
      numEdits: 0
      reactions: []
    id: 6412cc66ea7d1a726ac95fa6
    type: comment
  author: sirius-ctrl
  content: '> I''m trying to use lora from peft to fine-tune this model, and it seems
    to be working.


    Could you share the code?'
  created_at: 2023-03-16 06:59:34+00:00
  edited: false
  hidden: false
  id: 6412cc66ea7d1a726ac95fa6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
      fullname: xinnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xxxxnn
      type: user
    createdAt: '2023-03-16T08:26:40.000Z'
    data:
      edited: false
      editors:
      - xxxxnn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
          fullname: xinnan
          isHf: false
          isPro: false
          name: xxxxnn
          type: user
        html: '<p>i try to use lora too, but the result seems weird.<br>could you
          please share your settings? especially how to process the input text and
          target text. i think something is wrong with my code.</p>

          '
        raw: "i try to use lora too, but the result seems weird. \ncould you please\
          \ share your settings? especially how to process the input text and target\
          \ text. i think something is wrong with my code."
        updatedAt: '2023-03-16T08:26:40.144Z'
      numEdits: 0
      reactions: []
    id: 6412d2c0b45fa3cee6757d51
    type: comment
  author: xxxxnn
  content: "i try to use lora too, but the result seems weird. \ncould you please\
    \ share your settings? especially how to process the input text and target text.\
    \ i think something is wrong with my code."
  created_at: 2023-03-16 07:26:40+00:00
  edited: false
  hidden: false
  id: 6412d2c0b45fa3cee6757d51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
      fullname: sadfsaddf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fasoasdf
      type: user
    createdAt: '2023-03-16T09:17:20.000Z'
    data:
      edited: false
      editors:
      - fasoasdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
          fullname: sadfsaddf
          isHf: false
          isPro: false
          name: fasoasdf
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I''m trying to use lora from peft to fine-tune this model, and it seems
          to be working.</p>

          </blockquote>

          <p>Could you share the code?</p>

          </blockquote>

          <blockquote>

          <p>i try to use lora too, but the result seems weird.<br>could you please
          share your settings? especially how to process the input text and target
          text. i think something is wrong with my code.</p>

          </blockquote>

          <p>Sure! I put the code here: <a rel="nofollow" href="https://github.com/thaumstrial/FinetuneGLMWithPeft">https://github.com/thaumstrial/FinetuneGLMWithPeft</a><br>You
          can share your ideas and suggestions.</p>

          <p>However, I changed the original code to train ClueAI/ChatYuan-large-v1.
          The current code is rewritten according to my impression, and there may
          be some missing places</p>

          '
        raw: "> > I'm trying to use lora from peft to fine-tune this model, and it\
          \ seems to be working.\n> \n> Could you share the code?\n\n> i try to use\
          \ lora too, but the result seems weird. \n> could you please share your\
          \ settings? especially how to process the input text and target text. i\
          \ think something is wrong with my code.\n\nSure! I put the code here: https://github.com/thaumstrial/FinetuneGLMWithPeft\n\
          You can share your ideas and suggestions.\n\nHowever, I changed the original\
          \ code to train ClueAI/ChatYuan-large-v1. The current code is rewritten\
          \ according to my impression, and there may be some missing places"
        updatedAt: '2023-03-16T09:17:20.654Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - tjmm
        - zqzq71
        - raohai
        - starg
    id: 6412dea0c2c81db1e7a49299
    type: comment
  author: fasoasdf
  content: "> > I'm trying to use lora from peft to fine-tune this model, and it seems\
    \ to be working.\n> \n> Could you share the code?\n\n> i try to use lora too,\
    \ but the result seems weird. \n> could you please share your settings? especially\
    \ how to process the input text and target text. i think something is wrong with\
    \ my code.\n\nSure! I put the code here: https://github.com/thaumstrial/FinetuneGLMWithPeft\n\
    You can share your ideas and suggestions.\n\nHowever, I changed the original code\
    \ to train ClueAI/ChatYuan-large-v1. The current code is rewritten according to\
    \ my impression, and there may be some missing places"
  created_at: 2023-03-16 08:17:20+00:00
  edited: false
  hidden: false
  id: 6412dea0c2c81db1e7a49299
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
      fullname: xinnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xxxxnn
      type: user
    createdAt: '2023-03-16T10:31:56.000Z'
    data:
      edited: false
      editors:
      - xxxxnn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
          fullname: xinnan
          isHf: false
          isPro: false
          name: xxxxnn
          type: user
        html: '<p>thanks for sharing!<br>i have a question here: why are you use TaskType.CAUSAL_LM
          in lora config, instead of TaskType.SEQ_2_SEQ_LM? i think chatglm is a seq2seq
          model(because of the transformers class: AutoModelForSeq2SeqLM)</p>

          '
        raw: 'thanks for sharing!

          i have a question here: why are you use TaskType.CAUSAL_LM in lora config,
          instead of TaskType.SEQ_2_SEQ_LM? i think chatglm is a seq2seq model(because
          of the transformers class: AutoModelForSeq2SeqLM)'
        updatedAt: '2023-03-16T10:31:56.866Z'
      numEdits: 0
      reactions: []
    id: 6412f01c42b32b6552783e37
    type: comment
  author: xxxxnn
  content: 'thanks for sharing!

    i have a question here: why are you use TaskType.CAUSAL_LM in lora config, instead
    of TaskType.SEQ_2_SEQ_LM? i think chatglm is a seq2seq model(because of the transformers
    class: AutoModelForSeq2SeqLM)'
  created_at: 2023-03-16 09:31:56+00:00
  edited: false
  hidden: false
  id: 6412f01c42b32b6552783e37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
      fullname: sadfsaddf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fasoasdf
      type: user
    createdAt: '2023-03-16T10:35:22.000Z'
    data:
      edited: false
      editors:
      - fasoasdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
          fullname: sadfsaddf
          isHf: false
          isPro: false
          name: fasoasdf
          type: user
        html: '<blockquote>

          <p>thanks for sharing!<br>i have a question here: why are you use TaskType.CAUSAL_LM
          in lora config, instead of TaskType.SEQ_2_SEQ_LM? i think chatglm is a seq2seq
          model(because of the transformers class: AutoModelForSeq2SeqLM)</p>

          </blockquote>

          <p>That''s what got me confused. I also thought chatglm was a seq2seq model,
          but if you use TaskType.SEQ_2_SEQ_LM, chatglm''s forward function is missing
          some parameters.</p>

          '
        raw: '> thanks for sharing!

          > i have a question here: why are you use TaskType.CAUSAL_LM in lora config,
          instead of TaskType.SEQ_2_SEQ_LM? i think chatglm is a seq2seq model(because
          of the transformers class: AutoModelForSeq2SeqLM)


          That''s what got me confused. I also thought chatglm was a seq2seq model,
          but if you use TaskType.SEQ_2_SEQ_LM, chatglm''s forward function is missing
          some parameters.'
        updatedAt: '2023-03-16T10:35:22.051Z'
      numEdits: 0
      reactions: []
    id: 6412f0ea33b71a584b0f6fc9
    type: comment
  author: fasoasdf
  content: '> thanks for sharing!

    > i have a question here: why are you use TaskType.CAUSAL_LM in lora config, instead
    of TaskType.SEQ_2_SEQ_LM? i think chatglm is a seq2seq model(because of the transformers
    class: AutoModelForSeq2SeqLM)


    That''s what got me confused. I also thought chatglm was a seq2seq model, but
    if you use TaskType.SEQ_2_SEQ_LM, chatglm''s forward function is missing some
    parameters.'
  created_at: 2023-03-16 09:35:22+00:00
  edited: false
  hidden: false
  id: 6412f0ea33b71a584b0f6fc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
      fullname: sadfsaddf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fasoasdf
      type: user
    createdAt: '2023-03-16T10:42:59.000Z'
    data:
      edited: true
      editors:
      - fasoasdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
          fullname: sadfsaddf
          isHf: false
          isPro: false
          name: fasoasdf
          type: user
        html: '<p>I don''t know if chatglm has changed their code yet, but I did start
          with TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(<a href="https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py">https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py</a>).
          Does TaskType.SEQ_2_SEQ_LM work when you try it now?</p>

          <p>You can see that the forward function in modeling_chatglm.py is missing
          some parameters about encoder.</p>

          '
        raw: 'I don''t know if chatglm has changed their code yet, but I did start
          with TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).
          Does TaskType.SEQ_2_SEQ_LM work when you try it now?


          You can see that the forward function in modeling_chatglm.py is missing
          some parameters about encoder.'
        updatedAt: '2023-03-16T10:46:42.693Z'
      numEdits: 2
      reactions: []
    id: 6412f2b3bad67915a2928b0b
    type: comment
  author: fasoasdf
  content: 'I don''t know if chatglm has changed their code yet, but I did start with
    TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).
    Does TaskType.SEQ_2_SEQ_LM work when you try it now?


    You can see that the forward function in modeling_chatglm.py is missing some parameters
    about encoder.'
  created_at: 2023-03-16 09:42:59+00:00
  edited: true
  hidden: false
  id: 6412f2b3bad67915a2928b0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
      fullname: xinnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xxxxnn
      type: user
    createdAt: '2023-03-16T17:03:07.000Z'
    data:
      edited: true
      editors:
      - xxxxnn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
          fullname: xinnan
          isHf: false
          isPro: false
          name: xxxxnn
          type: user
        html: '<blockquote>

          <p>I don''t know if chatglm has changed their code yet, but I did start
          with TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(<a href="https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py">https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py</a>).
          Does TaskType.SEQ_2_SEQ_LM work when you try it now?</p>

          <p>You can see that the forward function in modeling_chatglm.py is missing
          some parameters about encoder.</p>

          </blockquote>

          <p>I think I had the same situation with TaskType.SEQ_2_SEQ_LM before. I
          just edited the source code of chatglm where the function missing parameters.
          I did some debugging and added a **kwargs and it worked.</p>

          <p>I tried your code a few hours ago and it still didn''t work. the fine-tuning
          makes the model worse. I guess the data is not handled in the right way.</p>

          '
        raw: "> I don't know if chatglm has changed their code yet, but I did start\
          \ with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
          \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> \n> You can see\
          \ that the forward function in modeling_chatglm.py is missing some parameters\
          \ about encoder.\n\nI think I had the same situation with TaskType.SEQ_2_SEQ_LM\
          \ before. I just edited the source code of chatglm where the function missing\
          \ parameters. I did some debugging and added a **kwargs and it worked.\n\
          \nI tried your code a few hours ago and it still didn't work. the fine-tuning\
          \ makes the model worse. I guess the data is not handled in the right way."
        updatedAt: '2023-03-16T17:07:43.451Z'
      numEdits: 1
      reactions: []
    id: 64134bcb4c23691ebe1bdfee
    type: comment
  author: xxxxnn
  content: "> I don't know if chatglm has changed their code yet, but I did start\
    \ with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
    \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> \n> You can see that\
    \ the forward function in modeling_chatglm.py is missing some parameters about\
    \ encoder.\n\nI think I had the same situation with TaskType.SEQ_2_SEQ_LM before.\
    \ I just edited the source code of chatglm where the function missing parameters.\
    \ I did some debugging and added a **kwargs and it worked.\n\nI tried your code\
    \ a few hours ago and it still didn't work. the fine-tuning makes the model worse.\
    \ I guess the data is not handled in the right way."
  created_at: 2023-03-16 16:03:07+00:00
  edited: true
  hidden: false
  id: 64134bcb4c23691ebe1bdfee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
      fullname: sadfsaddf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fasoasdf
      type: user
    createdAt: '2023-03-17T00:56:23.000Z'
    data:
      edited: false
      editors:
      - fasoasdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
          fullname: sadfsaddf
          isHf: false
          isPro: false
          name: fasoasdf
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I don''t know if chatglm has changed their code yet, but I did start
          with TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(<a href="https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py">https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py</a>).
          Does TaskType.SEQ_2_SEQ_LM work when you try it now?</p>

          <p>You can see that the forward function in modeling_chatglm.py is missing
          some parameters about encoder.</p>

          </blockquote>

          <p>I think I had the same situation with TaskType.SEQ_2_SEQ_LM before. I
          just edited the source code of chatglm where the function missing parameters.
          I did some debugging and added a **kwargs and it worked.</p>

          <p>I tried your code a few hours ago and it still didn''t work. the fine-tuning
          makes the model worse. I guess the data is not handled in the right way.<br>Can
          you share some of your training hyperparameters? In my test, I only try
          a few data to convince chatglm that itself wasn''t a robot, but I set lr
          and batch_num very high, 1e-2 to 1e-3, batch_num around 10 and no warmup.</p>

          </blockquote>

          '
        raw: "> > I don't know if chatglm has changed their code yet, but I did start\
          \ with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
          \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > \n> > You can\
          \ see that the forward function in modeling_chatglm.py is missing some parameters\
          \ about encoder.\n> \n> I think I had the same situation with TaskType.SEQ_2_SEQ_LM\
          \ before. I just edited the source code of chatglm where the function missing\
          \ parameters. I did some debugging and added a **kwargs and it worked.\n\
          > \n> I tried your code a few hours ago and it still didn't work. the fine-tuning\
          \ makes the model worse. I guess the data is not handled in the right way.\n\
          Can you share some of your training hyperparameters? In my test, I only\
          \ try a few data to convince chatglm that itself wasn't a robot, but I set\
          \ lr and batch_num very high, 1e-2 to 1e-3, batch_num around 10 and no warmup."
        updatedAt: '2023-03-17T00:56:23.562Z'
      numEdits: 0
      reactions: []
    id: 6413bab73f228ab8c04fc976
    type: comment
  author: fasoasdf
  content: "> > I don't know if chatglm has changed their code yet, but I did start\
    \ with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
    \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > \n> > You can see\
    \ that the forward function in modeling_chatglm.py is missing some parameters\
    \ about encoder.\n> \n> I think I had the same situation with TaskType.SEQ_2_SEQ_LM\
    \ before. I just edited the source code of chatglm where the function missing\
    \ parameters. I did some debugging and added a **kwargs and it worked.\n> \n>\
    \ I tried your code a few hours ago and it still didn't work. the fine-tuning\
    \ makes the model worse. I guess the data is not handled in the right way.\nCan\
    \ you share some of your training hyperparameters? In my test, I only try a few\
    \ data to convince chatglm that itself wasn't a robot, but I set lr and batch_num\
    \ very high, 1e-2 to 1e-3, batch_num around 10 and no warmup."
  created_at: 2023-03-16 23:56:23+00:00
  edited: false
  hidden: false
  id: 6413bab73f228ab8c04fc976
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
      fullname: xinnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xxxxnn
      type: user
    createdAt: '2023-03-17T03:49:56.000Z'
    data:
      edited: true
      editors:
      - xxxxnn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
          fullname: xinnan
          isHf: false
          isPro: false
          name: xxxxnn
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>I don''t know if chatglm has changed their code yet, but I did start
          with TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(<a href="https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py">https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py</a>).
          Does TaskType.SEQ_2_SEQ_LM work when you try it now?</p>

          <p>You can see that the forward function in modeling_chatglm.py is missing
          some parameters about encoder.</p>

          </blockquote>

          <p>I think I had the same situation with TaskType.SEQ_2_SEQ_LM before. I
          just edited the source code of chatglm where the function missing parameters.
          I did some debugging and added a **kwargs and it worked.</p>

          <p>I tried your code a few hours ago and it still didn''t work. the fine-tuning
          makes the model worse. I guess the data is not handled in the right way.<br>Can
          you share some of your training hyperparameters? In my test, I only try
          a few data to convince chatglm that itself wasn''t a robot, but I set lr
          and batch_num very high, 1e-2 to 1e-3, batch_num around 10 and no warmup.</p>

          </blockquote>

          </blockquote>

          <p>num batches: 16(sum of all gpus)<br>warmup: None<br>lr: 3e-3<br>lora
          config:</p>

          <ul>

          <li>target module: ["query_key_value"]</li>

          <li>r: 8</li>

          <li>lora_alpha: 32</li>

          <li>lora_dropout: 0.1</li>

          </ul>

          <p>I finetuned the model on few data  too(about 700). The same setting on
          bloomz-7b1-mt or gpt-neox-20b works well.</p>

          '
        raw: "> > > I don't know if chatglm has changed their code yet, but I did\
          \ start with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
          \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > > \n> > > You\
          \ can see that the forward function in modeling_chatglm.py is missing some\
          \ parameters about encoder.\n> > \n> > I think I had the same situation\
          \ with TaskType.SEQ_2_SEQ_LM before. I just edited the source code of chatglm\
          \ where the function missing parameters. I did some debugging and added\
          \ a **kwargs and it worked.\n> > \n> > I tried your code a few hours ago\
          \ and it still didn't work. the fine-tuning makes the model worse. I guess\
          \ the data is not handled in the right way.\n> Can you share some of your\
          \ training hyperparameters? In my test, I only try a few data to convince\
          \ chatglm that itself wasn't a robot, but I set lr and batch_num very high,\
          \ 1e-2 to 1e-3, batch_num around 10 and no warmup.\n\nnum batches: 16(sum\
          \ of all gpus)\nwarmup: None\nlr: 3e-3\nlora config:\n- target module: [\"\
          query_key_value\"]\n- r: 8\n- lora_alpha: 32\n- lora_dropout: 0.1\n\nI finetuned\
          \ the model on few data  too(about 700). The same setting on bloomz-7b1-mt\
          \ or gpt-neox-20b works well."
        updatedAt: '2023-03-17T03:52:32.672Z'
      numEdits: 1
      reactions: []
    id: 6413e36447b37232c80dcc0e
    type: comment
  author: xxxxnn
  content: "> > > I don't know if chatglm has changed their code yet, but I did start\
    \ with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
    \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > > \n> > > You can\
    \ see that the forward function in modeling_chatglm.py is missing some parameters\
    \ about encoder.\n> > \n> > I think I had the same situation with TaskType.SEQ_2_SEQ_LM\
    \ before. I just edited the source code of chatglm where the function missing\
    \ parameters. I did some debugging and added a **kwargs and it worked.\n> > \n\
    > > I tried your code a few hours ago and it still didn't work. the fine-tuning\
    \ makes the model worse. I guess the data is not handled in the right way.\n>\
    \ Can you share some of your training hyperparameters? In my test, I only try\
    \ a few data to convince chatglm that itself wasn't a robot, but I set lr and\
    \ batch_num very high, 1e-2 to 1e-3, batch_num around 10 and no warmup.\n\nnum\
    \ batches: 16(sum of all gpus)\nwarmup: None\nlr: 3e-3\nlora config:\n- target\
    \ module: [\"query_key_value\"]\n- r: 8\n- lora_alpha: 32\n- lora_dropout: 0.1\n\
    \nI finetuned the model on few data  too(about 700). The same setting on bloomz-7b1-mt\
    \ or gpt-neox-20b works well."
  created_at: 2023-03-17 02:49:56+00:00
  edited: true
  hidden: false
  id: 6413e36447b37232c80dcc0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
      fullname: sadfsaddf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fasoasdf
      type: user
    createdAt: '2023-03-17T04:08:17.000Z'
    data:
      edited: false
      editors:
      - fasoasdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/494426208cb45378092268e92fc582c6.svg
          fullname: sadfsaddf
          isHf: false
          isPro: false
          name: fasoasdf
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>I don''t know if chatglm has changed their code yet, but I did start
          with TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(<a href="https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py">https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py</a>).
          Does TaskType.SEQ_2_SEQ_LM work when you try it now?</p>

          <p>You can see that the forward function in modeling_chatglm.py is missing
          some parameters about encoder.</p>

          </blockquote>

          <p>I think I had the same situation with TaskType.SEQ_2_SEQ_LM before. I
          just edited the source code of chatglm where the function missing parameters.
          I did some debugging and added a **kwargs and it worked.</p>

          <p>I tried your code a few hours ago and it still didn''t work. the fine-tuning
          makes the model worse. I guess the data is not handled in the right way.<br>Can
          you share some of your training hyperparameters? In my test, I only try
          a few data to convince chatglm that itself wasn''t a robot, but I set lr
          and batch_num very high, 1e-2 to 1e-3, batch_num around 10 and no warmup.</p>

          </blockquote>

          </blockquote>

          <p>num batches: 16(sum of all gpus)<br>warmup: None<br>lr: 3e-3<br>lora
          config:</p>

          <ul>

          <li>target module: ["query_key_value"]</li>

          <li>r: 8</li>

          <li>lora_alpha: 32</li>

          <li>lora_dropout: 0.1</li>

          </ul>

          <p>I finetuned the model on few data  too(about 700). The same setting on
          bloomz-7b1-mt or gpt-neox-20b works well.<br>Could you please share your
          code? What special treatment did you do to the sentence?</p>

          </blockquote>

          '
        raw: "> > > > I don't know if chatglm has changed their code yet, but I did\
          \ start with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
          \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > > > \n> > >\
          \ > You can see that the forward function in modeling_chatglm.py is missing\
          \ some parameters about encoder.\n> > > \n> > > I think I had the same situation\
          \ with TaskType.SEQ_2_SEQ_LM before. I just edited the source code of chatglm\
          \ where the function missing parameters. I did some debugging and added\
          \ a **kwargs and it worked.\n> > > \n> > > I tried your code a few hours\
          \ ago and it still didn't work. the fine-tuning makes the model worse. I\
          \ guess the data is not handled in the right way.\n> > Can you share some\
          \ of your training hyperparameters? In my test, I only try a few data to\
          \ convince chatglm that itself wasn't a robot, but I set lr and batch_num\
          \ very high, 1e-2 to 1e-3, batch_num around 10 and no warmup.\n> \n> num\
          \ batches: 16(sum of all gpus)\n> warmup: None\n> lr: 3e-3\n> lora config:\n\
          > - target module: [\"query_key_value\"]\n> - r: 8\n> - lora_alpha: 32\n\
          > - lora_dropout: 0.1\n> \n> I finetuned the model on few data  too(about\
          \ 700). The same setting on bloomz-7b1-mt or gpt-neox-20b works well.\n\
          Could you please share your code? What special treatment did you do to the\
          \ sentence?"
        updatedAt: '2023-03-17T04:08:17.075Z'
      numEdits: 0
      reactions: []
    id: 6413e7b1964f3409c4779c93
    type: comment
  author: fasoasdf
  content: "> > > > I don't know if chatglm has changed their code yet, but I did\
    \ start with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
    \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > > > \n> > > > You\
    \ can see that the forward function in modeling_chatglm.py is missing some parameters\
    \ about encoder.\n> > > \n> > > I think I had the same situation with TaskType.SEQ_2_SEQ_LM\
    \ before. I just edited the source code of chatglm where the function missing\
    \ parameters. I did some debugging and added a **kwargs and it worked.\n> > >\
    \ \n> > > I tried your code a few hours ago and it still didn't work. the fine-tuning\
    \ makes the model worse. I guess the data is not handled in the right way.\n>\
    \ > Can you share some of your training hyperparameters? In my test, I only try\
    \ a few data to convince chatglm that itself wasn't a robot, but I set lr and\
    \ batch_num very high, 1e-2 to 1e-3, batch_num around 10 and no warmup.\n> \n\
    > num batches: 16(sum of all gpus)\n> warmup: None\n> lr: 3e-3\n> lora config:\n\
    > - target module: [\"query_key_value\"]\n> - r: 8\n> - lora_alpha: 32\n> - lora_dropout:\
    \ 0.1\n> \n> I finetuned the model on few data  too(about 700). The same setting\
    \ on bloomz-7b1-mt or gpt-neox-20b works well.\nCould you please share your code?\
    \ What special treatment did you do to the sentence?"
  created_at: 2023-03-17 03:08:17+00:00
  edited: false
  hidden: false
  id: 6413e7b1964f3409c4779c93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
      fullname: xinnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xxxxnn
      type: user
    createdAt: '2023-03-18T15:49:54.000Z'
    data:
      edited: true
      editors:
      - xxxxnn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b941da5f57069c66393816e411ff913.svg
          fullname: xinnan
          isHf: false
          isPro: false
          name: xxxxnn
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>I don''t know if chatglm has changed their code yet, but I did start
          with TaskType.SEQ_2_SEQ_LM and changed chatglm''s code(<a href="https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py">https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py</a>).
          Does TaskType.SEQ_2_SEQ_LM work when you try it now?</p>

          <p>You can see that the forward function in modeling_chatglm.py is missing
          some parameters about encoder.</p>

          </blockquote>

          <p>I think I had the same situation with TaskType.SEQ_2_SEQ_LM before. I
          just edited the source code of chatglm where the function missing parameters.
          I did some debugging and added a **kwargs and it worked.</p>

          <p>I tried your code a few hours ago and it still didn''t work. the fine-tuning
          makes the model worse. I guess the data is not handled in the right way.<br>Can
          you share some of your training hyperparameters? In my test, I only try
          a few data to convince chatglm that itself wasn''t a robot, but I set lr
          and batch_num very high, 1e-2 to 1e-3, batch_num around 10 and no warmup.</p>

          </blockquote>

          </blockquote>

          <p>num batches: 16(sum of all gpus)<br>warmup: None<br>lr: 3e-3<br>lora
          config:</p>

          <ul>

          <li>target module: ["query_key_value"]</li>

          <li>r: 8</li>

          <li>lora_alpha: 32</li>

          <li>lora_dropout: 0.1</li>

          </ul>

          <p>I finetuned the model on few data  too(about 700). The same setting on
          bloomz-7b1-mt or gpt-neox-20b works well.<br>Could you please share your
          code? What special treatment did you do to the sentence?</p>

          </blockquote>

          </blockquote>

          <p>Sorry for the late reply. My code is integrate with some other codes
          in my company, it is troublesome to abstract it. I guess you are Chinese?
          you can follow this issue <a rel="nofollow" href="https://github.com/mymusise/ChatGLM-Tuning/issues/11#issuecomment-1474880311">https://github.com/mymusise/ChatGLM-Tuning/issues/11#issuecomment-1474880311</a>
          to see what did I do with the dataset. I think your code has similar problem
          like that.</p>

          '
        raw: "> > > > > I don't know if chatglm has changed their code yet, but I\
          \ did start with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
          \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > > > > \n> >\
          \ > > > You can see that the forward function in modeling_chatglm.py is\
          \ missing some parameters about encoder.\n> > > > \n> > > > I think I had\
          \ the same situation with TaskType.SEQ_2_SEQ_LM before. I just edited the\
          \ source code of chatglm where the function missing parameters. I did some\
          \ debugging and added a **kwargs and it worked.\n> > > > \n> > > > I tried\
          \ your code a few hours ago and it still didn't work. the fine-tuning makes\
          \ the model worse. I guess the data is not handled in the right way.\n>\
          \ > > Can you share some of your training hyperparameters? In my test, I\
          \ only try a few data to convince chatglm that itself wasn't a robot, but\
          \ I set lr and batch_num very high, 1e-2 to 1e-3, batch_num around 10 and\
          \ no warmup.\n> > \n> > num batches: 16(sum of all gpus)\n> > warmup: None\n\
          > > lr: 3e-3\n> > lora config:\n> > - target module: [\"query_key_value\"\
          ]\n> > - r: 8\n> > - lora_alpha: 32\n> > - lora_dropout: 0.1\n> > \n> >\
          \ I finetuned the model on few data  too(about 700). The same setting on\
          \ bloomz-7b1-mt or gpt-neox-20b works well.\n> Could you please share your\
          \ code? What special treatment did you do to the sentence?\n\nSorry for\
          \ the late reply. My code is integrate with some other codes in my company,\
          \ it is troublesome to abstract it. I guess you are Chinese? you can follow\
          \ this issue https://github.com/mymusise/ChatGLM-Tuning/issues/11#issuecomment-1474880311\
          \ to see what did I do with the dataset. I think your code has similar problem\
          \ like that."
        updatedAt: '2023-03-18T15:52:43.810Z'
      numEdits: 1
      reactions: []
    id: 6415dda287030915d5ae9523
    type: comment
  author: xxxxnn
  content: "> > > > > I don't know if chatglm has changed their code yet, but I did\
    \ start with TaskType.SEQ_2_SEQ_LM and changed chatglm's code(https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py).\
    \ Does TaskType.SEQ_2_SEQ_LM work when you try it now?\n> > > > > \n> > > > >\
    \ You can see that the forward function in modeling_chatglm.py is missing some\
    \ parameters about encoder.\n> > > > \n> > > > I think I had the same situation\
    \ with TaskType.SEQ_2_SEQ_LM before. I just edited the source code of chatglm\
    \ where the function missing parameters. I did some debugging and added a **kwargs\
    \ and it worked.\n> > > > \n> > > > I tried your code a few hours ago and it still\
    \ didn't work. the fine-tuning makes the model worse. I guess the data is not\
    \ handled in the right way.\n> > > Can you share some of your training hyperparameters?\
    \ In my test, I only try a few data to convince chatglm that itself wasn't a robot,\
    \ but I set lr and batch_num very high, 1e-2 to 1e-3, batch_num around 10 and\
    \ no warmup.\n> > \n> > num batches: 16(sum of all gpus)\n> > warmup: None\n>\
    \ > lr: 3e-3\n> > lora config:\n> > - target module: [\"query_key_value\"]\n>\
    \ > - r: 8\n> > - lora_alpha: 32\n> > - lora_dropout: 0.1\n> > \n> > I finetuned\
    \ the model on few data  too(about 700). The same setting on bloomz-7b1-mt or\
    \ gpt-neox-20b works well.\n> Could you please share your code? What special treatment\
    \ did you do to the sentence?\n\nSorry for the late reply. My code is integrate\
    \ with some other codes in my company, it is troublesome to abstract it. I guess\
    \ you are Chinese? you can follow this issue https://github.com/mymusise/ChatGLM-Tuning/issues/11#issuecomment-1474880311\
    \ to see what did I do with the dataset. I think your code has similar problem\
    \ like that."
  created_at: 2023-03-18 14:49:54+00:00
  edited: true
  hidden: false
  id: 6415dda287030915d5ae9523
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a34077766f47129e66d8a27f631c0b60.svg
      fullname: AegisGPT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AegisGPT
      type: user
    createdAt: '2023-03-22T10:32:46.000Z'
    data:
      edited: false
      editors:
      - AegisGPT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a34077766f47129e66d8a27f631c0b60.svg
          fullname: AegisGPT
          isHf: false
          isPro: false
          name: AegisGPT
          type: user
        html: '<p>Tried running the training code and it went out of memory for Google
          Colab Pro+, exceeding 40GB of GPU RAM. Anyone faced similar issue?</p>

          '
        raw: Tried running the training code and it went out of memory for Google
          Colab Pro+, exceeding 40GB of GPU RAM. Anyone faced similar issue?
        updatedAt: '2023-03-22T10:32:46.277Z'
      numEdits: 0
      reactions: []
    id: 641ad94e5d0c7772c604c912
    type: comment
  author: AegisGPT
  content: Tried running the training code and it went out of memory for Google Colab
    Pro+, exceeding 40GB of GPU RAM. Anyone faced similar issue?
  created_at: 2023-03-22 09:32:46+00:00
  edited: false
  hidden: false
  id: 641ad94e5d0c7772c604c912
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679368067117-64100c65ef5c6dcac8b59214.png?w=200&h=200&f=face
      fullname: Zhang Peiyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aizpy
      type: user
    createdAt: '2023-03-31T09:24:55.000Z'
    data:
      edited: false
      editors:
      - aizpy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679368067117-64100c65ef5c6dcac8b59214.png?w=200&h=200&f=face
          fullname: Zhang Peiyang
          isHf: false
          isPro: false
          name: aizpy
          type: user
        html: "<p>Check out my blog here if you understand Chinese <a rel=\"nofollow\"\
          \ href=\"https://aizpy.com/2023/03/30/chatglm-6b-lora/\">\u300A\u5BF9 ChatGLM-6B\
          \ \u505A LoRA Fine-tuning\u300B</a></p>\n"
        raw: "Check out my blog here if you understand Chinese [\u300A\u5BF9 ChatGLM-6B\
          \ \u505A LoRA Fine-tuning\u300B](https://aizpy.com/2023/03/30/chatglm-6b-lora/)"
        updatedAt: '2023-03-31T09:24:55.699Z'
      numEdits: 0
      reactions: []
    id: 6426a6e7be9aa99aa807c090
    type: comment
  author: aizpy
  content: "Check out my blog here if you understand Chinese [\u300A\u5BF9 ChatGLM-6B\
    \ \u505A LoRA Fine-tuning\u300B](https://aizpy.com/2023/03/30/chatglm-6b-lora/)"
  created_at: 2023-03-31 08:24:55+00:00
  edited: false
  hidden: false
  id: 6426a6e7be9aa99aa807c090
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a34077766f47129e66d8a27f631c0b60.svg
      fullname: AegisGPT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AegisGPT
      type: user
    createdAt: '2023-03-31T14:48:16.000Z'
    data:
      edited: true
      editors:
      - AegisGPT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a34077766f47129e66d8a27f631c0b60.svg
          fullname: AegisGPT
          isHf: false
          isPro: false
          name: AegisGPT
          type: user
        html: "<blockquote>\n<p>Check out my blog here if you understand Chinese <a\
          \ rel=\"nofollow\" href=\"https://aizpy.com/2023/03/30/chatglm-6b-lora/\"\
          >\u300A\u5BF9 ChatGLM-6B \u505A LoRA Fine-tuning\u300B</a><br>Thank you!\
          \ I will definitely try it out. I previously used another sample code for\
          \ LoRA Fine-tuning of GLM and the memory usage went above 40GB.</p>\n</blockquote>\n"
        raw: "> Check out my blog here if you understand Chinese [\u300A\u5BF9 ChatGLM-6B\
          \ \u505A LoRA Fine-tuning\u300B](https://aizpy.com/2023/03/30/chatglm-6b-lora/)\n\
          Thank you! I will definitely try it out. I previously used another sample\
          \ code for LoRA Fine-tuning of GLM and the memory usage went above 40GB."
        updatedAt: '2023-03-31T14:48:36.009Z'
      numEdits: 1
      reactions: []
    id: 6426f2b02db5108b10b6d145
    type: comment
  author: AegisGPT
  content: "> Check out my blog here if you understand Chinese [\u300A\u5BF9 ChatGLM-6B\
    \ \u505A LoRA Fine-tuning\u300B](https://aizpy.com/2023/03/30/chatglm-6b-lora/)\n\
    Thank you! I will definitely try it out. I previously used another sample code\
    \ for LoRA Fine-tuning of GLM and the memory usage went above 40GB."
  created_at: 2023-03-31 13:48:16+00:00
  edited: true
  hidden: false
  id: 6426f2b02db5108b10b6d145
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a34077766f47129e66d8a27f631c0b60.svg
      fullname: AegisGPT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AegisGPT
      type: user
    createdAt: '2023-03-31T14:52:25.000Z'
    data:
      edited: true
      editors:
      - AegisGPT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a34077766f47129e66d8a27f631c0b60.svg
          fullname: AegisGPT
          isHf: false
          isPro: false
          name: AegisGPT
          type: user
        html: "<p>I have a question though <span data-props=\"{&quot;user&quot;:&quot;aizpy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aizpy\"\
          >@<span class=\"underline\">aizpy</span></a></span>\n\n\t</span></span>.\
          \ Based on your blog, am I right to say that after fine-tuning, you saved\
          \ all the LoRA of k, q and v?</p>\n"
        raw: I have a question though @aizpy. Based on your blog, am I right to say
          that after fine-tuning, you saved all the LoRA of k, q and v?
        updatedAt: '2023-03-31T15:15:25.053Z'
      numEdits: 2
      reactions: []
    id: 6426f3a9fe8a66429bb9c8f7
    type: comment
  author: AegisGPT
  content: I have a question though @aizpy. Based on your blog, am I right to say
    that after fine-tuning, you saved all the LoRA of k, q and v?
  created_at: 2023-03-31 13:52:25+00:00
  edited: true
  hidden: false
  id: 6426f3a9fe8a66429bb9c8f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679368067117-64100c65ef5c6dcac8b59214.png?w=200&h=200&f=face
      fullname: Zhang Peiyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aizpy
      type: user
    createdAt: '2023-04-01T06:03:02.000Z'
    data:
      edited: false
      editors:
      - aizpy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679368067117-64100c65ef5c6dcac8b59214.png?w=200&h=200&f=face
          fullname: Zhang Peiyang
          isHf: false
          isPro: false
          name: aizpy
          type: user
        html: "<blockquote>\n<p>I have a question though <span data-props=\"{&quot;user&quot;:&quot;aizpy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aizpy\"\
          >@<span class=\"underline\">aizpy</span></a></span>\n\n\t</span></span>.\
          \ Based on your blog, am I right to say that after fine-tuning, you saved\
          \ all the LoRA of k, q and v?</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;AegisGPT&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/AegisGPT\">@<span class=\"underline\">AegisGPT</span></a></span>\n\
          \n\t</span></span> Yes, all the parameters that need to do gradient descent\
          \ calculations are saved. And these are the very parameters that LoRA has\
          \ processed.</p>\n"
        raw: '> I have a question though @aizpy. Based on your blog, am I right to
          say that after fine-tuning, you saved all the LoRA of k, q and v?


          @AegisGPT Yes, all the parameters that need to do gradient descent calculations
          are saved. And these are the very parameters that LoRA has processed.'
        updatedAt: '2023-04-01T06:03:02.904Z'
      numEdits: 0
      reactions: []
    id: 6427c916a760fe0bf372cdaf
    type: comment
  author: aizpy
  content: '> I have a question though @aizpy. Based on your blog, am I right to say
    that after fine-tuning, you saved all the LoRA of k, q and v?


    @AegisGPT Yes, all the parameters that need to do gradient descent calculations
    are saved. And these are the very parameters that LoRA has processed.'
  created_at: 2023-04-01 05:03:02+00:00
  edited: false
  hidden: false
  id: 6427c916a760fe0bf372cdaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679311624745-63f36b504745321de3510823.jpeg?w=200&h=200&f=face
      fullname: Song XiXuan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DrSong
      type: user
    createdAt: '2023-04-01T13:37:07.000Z'
    data:
      edited: false
      editors:
      - DrSong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679311624745-63f36b504745321de3510823.jpeg?w=200&h=200&f=face
          fullname: Song XiXuan
          isHf: false
          isPro: false
          name: DrSong
          type: user
        html: '<p>Why don''t try p-tuning? Only 7G gpu ram is needed: <a rel="nofollow"
          href="https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md">https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md</a></p>

          '
        raw: 'Why don''t try p-tuning? Only 7G gpu ram is needed: https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md'
        updatedAt: '2023-04-01T13:37:07.853Z'
      numEdits: 0
      reactions: []
    id: 642833837b2cfe4e499b7f54
    type: comment
  author: DrSong
  content: 'Why don''t try p-tuning? Only 7G gpu ram is needed: https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md'
  created_at: 2023-04-01 12:37:07+00:00
  edited: false
  hidden: false
  id: 642833837b2cfe4e499b7f54
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: How to finetune this model
