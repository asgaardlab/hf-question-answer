!!python/object:huggingface_hub.community.DiscussionWithDetails
author: liuhantao
conflicting_files: null
created_at: 2023-04-14 03:15:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bf0a4eb263e8f1733ae1c9d54cad120.svg
      fullname: liuhantao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liuhantao
      type: user
    createdAt: '2023-04-14T04:15:36.000Z'
    data:
      edited: false
      editors:
      - liuhantao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bf0a4eb263e8f1733ae1c9d54cad120.svg
          fullname: liuhantao
          isHf: false
          isPro: false
          name: liuhantao
          type: user
        html: "<p>transformers  \u4ECE4.26.1 \u5347\u7EA7\u81F34.27.1 \u540E\u62A5\
          \u9519 ModuleNotFoundError: No module named 'transformers_modules.THUDM/chatglm-6b'</p>\n\
          <p>\u5F02\u5E38\u65E5\u5FD7\u5982\u4E0B\uFF0C\u8BF7\u6C42\u6307\u5BFC\u6211\
          \u89E3\u51B3</p>\n<p>Explicitly passing a <code>revision</code> is encouraged\
          \ when loading a model with custom code to ensure no malicious code has\
          \ been contributed in a newer revision.<br>2023-04-14 11:56:23.342 Uncaught\
          \ app exception<br>Traceback (most recent call last):<br>  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\"\
          , line 245, in _get_or_create_cached_value<br>    cached_result = cache.read_result(value_key)<br>\
          \  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
          caching\\cache_resource_api.py\", line 447, in read_result<br>    raise\
          \ CacheKeyNotFoundError()<br>streamlit.runtime.caching.cache_errors.CacheKeyNotFoundError</p>\n\
          <p>During handling of the above exception, another exception occurred:</p>\n\
          <p>Traceback (most recent call last):<br>  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\"\
          , line 293, in _handle_cache_miss<br>    cached_result = cache.read_result(value_key)<br>\
          \  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
          caching\\cache_resource_api.py\", line 447, in read_result<br>    raise\
          \ CacheKeyNotFoundError()<br>streamlit.runtime.caching.cache_errors.CacheKeyNotFoundError</p>\n\
          <p>During handling of the above exception, another exception occurred:</p>\n\
          <p>Traceback (most recent call last):<br>  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\streamlit\\runtime\\scriptrunner\\script_runner.py\"\
          , line 565, in _run_script<br>    exec(code, module.<strong>dict</strong>)<br>\
          \  File \"D:\\ChatGLM-6B-main\\web_demo2.py\", line 69, in <br>    st.session_state[\"\
          state\"] = predict(prompt_text, max_length, top_p, temperature, st.session_state[\"\
          state\"])<br>  File \"D:\\ChatGLM-6B-main\\web_demo2.py\", line 25, in predict<br>\
          \    tokenizer, model = get_model()<br>  File \"D:\\ChatGLM-6B-main\\MyENV\\\
          lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\", line\
          \ 194, in wrapper<br>    return cached_func(*args, **kwargs)<br>  File \"\
          D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\caching\\\
          cache_utils.py\", line 223, in <strong>call</strong><br>    return self._get_or_create_cached_value(args,\
          \ kwargs)<br>  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\\
          runtime\\caching\\cache_utils.py\", line 248, in _get_or_create_cached_value<br>\
          \    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)<br>\
          \  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
          caching\\cache_utils.py\", line 302, in _handle_cache_miss<br>    computed_value\
          \ = self.<em>info.func(*func_args, **func_kwargs)<br>  File \"D:\\ChatGLM-6B-main\\\
          web_demo2.py\", line 14, in get_model<br>    tokenizer = AutoTokenizer.from_pretrained(\"\
          THUDM/chatglm-6b\", trust_remote_code=True)<br>  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\"\
          , line 663, in from_pretrained<br>    tokenizer_class = get_class_from_dynamic_module(<br>\
          \  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\transformers\\\
          dynamic_module_utils.py\", line 399, in get_class_from_dynamic_module<br>\
          \    return get_class_in_module(class_name, final_module.replace(\".py\"\
          , \"\"))<br>  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\transformers\\\
          dynamic_module_utils.py\", line 177, in get_class_in_module<br>    module\
          \ = importlib.import_module(module_path)<br>  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\importlib_<em>init</em></em>.py\", line 126, in import_module<br>\
          \    return _bootstrap._gcd_import(name[level:], package, level)<br>  File\
          \ \"\", line 1050, in _gcd_import<br>  File \"\", line 1027, in _find_and_load<br>\
          \  File \"\", line 992, in _find_and_load_unlocked<br>  File \"\", line\
          \ 241, in _call_with_frames_removed<br>  File \"\", line 1050, in _gcd_import<br>\
          \  File \"\", line 1027, in _find_and_load<br>  File \"\", line 1004, in\
          \ _find_and_load_unlocked<br>ModuleNotFoundError: No module named 'transformers_modules.THUDM/chatglm-6b'</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6438d1fe1efe72ba48091231/-6dZhzA04lUfyDPrApyFz.jpeg\"\
          ><img alt=\"transformers_modules.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6438d1fe1efe72ba48091231/-6dZhzA04lUfyDPrApyFz.jpeg\"\
          ></a></p>\n"
        raw: "transformers  \u4ECE4.26.1 \u5347\u7EA7\u81F34.27.1 \u540E\u62A5\u9519\
          \ ModuleNotFoundError: No module named 'transformers_modules.THUDM/chatglm-6b'\r\
          \n\r\n\r\n\u5F02\u5E38\u65E5\u5FD7\u5982\u4E0B\uFF0C\u8BF7\u6C42\u6307\u5BFC\
          \u6211\u89E3\u51B3\r\n\r\nExplicitly passing a `revision` is encouraged\
          \ when loading a model with custom code to ensure no malicious code has\
          \ been contributed in a newer revision.\r\n2023-04-14 11:56:23.342 Uncaught\
          \ app exception\r\nTraceback (most recent call last):\r\n  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\"\
          , line 245, in _get_or_create_cached_value\r\n    cached_result = cache.read_result(value_key)\r\
          \n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
          caching\\cache_resource_api.py\", line 447, in read_result\r\n    raise\
          \ CacheKeyNotFoundError()\r\nstreamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\r\
          \n\r\nDuring handling of the above exception, another exception occurred:\r\
          \n\r\nTraceback (most recent call last):\r\n  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\"\
          , line 293, in _handle_cache_miss\r\n    cached_result = cache.read_result(value_key)\r\
          \n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
          caching\\cache_resource_api.py\", line 447, in read_result\r\n    raise\
          \ CacheKeyNotFoundError()\r\nstreamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\r\
          \n\r\nDuring handling of the above exception, another exception occurred:\r\
          \n\r\nTraceback (most recent call last):\r\n  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\streamlit\\runtime\\scriptrunner\\script_runner.py\"\
          , line 565, in _run_script\r\n    exec(code, module.__dict__)\r\n  File\
          \ \"D:\\ChatGLM-6B-main\\web_demo2.py\", line 69, in <module>\r\n    st.session_state[\"\
          state\"] = predict(prompt_text, max_length, top_p, temperature, st.session_state[\"\
          state\"])\r\n  File \"D:\\ChatGLM-6B-main\\web_demo2.py\", line 25, in predict\r\
          \n    tokenizer, model = get_model()\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\\
          lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\", line\
          \ 194, in wrapper\r\n    return cached_func(*args, **kwargs)\r\n  File \"\
          D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\caching\\\
          cache_utils.py\", line 223, in __call__\r\n    return self._get_or_create_cached_value(args,\
          \ kwargs)\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\\
          runtime\\caching\\cache_utils.py\", line 248, in _get_or_create_cached_value\r\
          \n    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\r\
          \n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
          caching\\cache_utils.py\", line 302, in _handle_cache_miss\r\n    computed_value\
          \ = self._info.func(*func_args, **func_kwargs)\r\n  File \"D:\\ChatGLM-6B-main\\\
          web_demo2.py\", line 14, in get_model\r\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          THUDM/chatglm-6b\", trust_remote_code=True)\r\n  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\"\
          , line 663, in from_pretrained\r\n    tokenizer_class = get_class_from_dynamic_module(\r\
          \n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\transformers\\\
          dynamic_module_utils.py\", line 399, in get_class_from_dynamic_module\r\n\
          \    return get_class_in_module(class_name, final_module.replace(\".py\"\
          , \"\"))\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\transformers\\\
          dynamic_module_utils.py\", line 177, in get_class_in_module\r\n    module\
          \ = importlib.import_module(module_path)\r\n  File \"D:\\ChatGLM-6B-main\\\
          MyENV\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n   \
          \ return _bootstrap._gcd_import(name[level:], package, level)\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\
          \n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n\
          \  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\
          \n  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\r\
          \nModuleNotFoundError: No module named 'transformers_modules.THUDM/chatglm-6b'\r\
          \n\r\n\r\n\r\n![transformers_modules.jpg](https://cdn-uploads.huggingface.co/production/uploads/6438d1fe1efe72ba48091231/-6dZhzA04lUfyDPrApyFz.jpeg)\r\
          \n"
        updatedAt: '2023-04-14T04:15:36.752Z'
      numEdits: 0
      reactions: []
    id: 6438d3683b46237de3d08699
    type: comment
  author: liuhantao
  content: "transformers  \u4ECE4.26.1 \u5347\u7EA7\u81F34.27.1 \u540E\u62A5\u9519\
    \ ModuleNotFoundError: No module named 'transformers_modules.THUDM/chatglm-6b'\r\
    \n\r\n\r\n\u5F02\u5E38\u65E5\u5FD7\u5982\u4E0B\uFF0C\u8BF7\u6C42\u6307\u5BFC\u6211\
    \u89E3\u51B3\r\n\r\nExplicitly passing a `revision` is encouraged when loading\
    \ a model with custom code to ensure no malicious code has been contributed in\
    \ a newer revision.\r\n2023-04-14 11:56:23.342 Uncaught app exception\r\nTraceback\
    \ (most recent call last):\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\\
    streamlit\\runtime\\caching\\cache_utils.py\", line 245, in _get_or_create_cached_value\r\
    \n    cached_result = cache.read_result(value_key)\r\n  File \"D:\\ChatGLM-6B-main\\\
    MyENV\\lib\\site-packages\\streamlit\\runtime\\caching\\cache_resource_api.py\"\
    , line 447, in read_result\r\n    raise CacheKeyNotFoundError()\r\nstreamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nTraceback (most recent call last):\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\\
    lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\", line 293, in\
    \ _handle_cache_miss\r\n    cached_result = cache.read_result(value_key)\r\n \
    \ File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
    caching\\cache_resource_api.py\", line 447, in read_result\r\n    raise CacheKeyNotFoundError()\r\
    \nstreamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\r\n\r\nDuring handling\
    \ of the above exception, another exception occurred:\r\n\r\nTraceback (most recent\
    \ call last):\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\\
    runtime\\scriptrunner\\script_runner.py\", line 565, in _run_script\r\n    exec(code,\
    \ module.__dict__)\r\n  File \"D:\\ChatGLM-6B-main\\web_demo2.py\", line 69, in\
    \ <module>\r\n    st.session_state[\"state\"] = predict(prompt_text, max_length,\
    \ top_p, temperature, st.session_state[\"state\"])\r\n  File \"D:\\ChatGLM-6B-main\\\
    web_demo2.py\", line 25, in predict\r\n    tokenizer, model = get_model()\r\n\
    \  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
    caching\\cache_utils.py\", line 194, in wrapper\r\n    return cached_func(*args,\
    \ **kwargs)\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\\
    runtime\\caching\\cache_utils.py\", line 223, in __call__\r\n    return self._get_or_create_cached_value(args,\
    \ kwargs)\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\\
    runtime\\caching\\cache_utils.py\", line 248, in _get_or_create_cached_value\r\
    \n    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\r\
    \n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\streamlit\\runtime\\\
    caching\\cache_utils.py\", line 302, in _handle_cache_miss\r\n    computed_value\
    \ = self._info.func(*func_args, **func_kwargs)\r\n  File \"D:\\ChatGLM-6B-main\\\
    web_demo2.py\", line 14, in get_model\r\n    tokenizer = AutoTokenizer.from_pretrained(\"\
    THUDM/chatglm-6b\", trust_remote_code=True)\r\n  File \"D:\\ChatGLM-6B-main\\\
    MyENV\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\"\
    , line 663, in from_pretrained\r\n    tokenizer_class = get_class_from_dynamic_module(\r\
    \n  File \"D:\\ChatGLM-6B-main\\MyENV\\lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
    , line 399, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\r\n  File \"D:\\ChatGLM-6B-main\\MyENV\\\
    lib\\site-packages\\transformers\\dynamic_module_utils.py\", line 177, in get_class_in_module\r\
    \n    module = importlib.import_module(module_path)\r\n  File \"D:\\ChatGLM-6B-main\\\
    MyENV\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return\
    \ _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line\
    \ 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 992,\
    \ in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line\
    \ 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line\
    \ 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1004,\
    \ in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'transformers_modules.THUDM/chatglm-6b'\r\
    \n\r\n\r\n\r\n![transformers_modules.jpg](https://cdn-uploads.huggingface.co/production/uploads/6438d1fe1efe72ba48091231/-6dZhzA04lUfyDPrApyFz.jpeg)\r\
    \n"
  created_at: 2023-04-14 03:15:36+00:00
  edited: false
  hidden: false
  id: 6438d3683b46237de3d08699
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bf0a4eb263e8f1733ae1c9d54cad120.svg
      fullname: liuhantao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liuhantao
      type: user
    createdAt: '2023-04-17T06:43:35.000Z'
    data:
      edited: false
      editors:
      - liuhantao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bf0a4eb263e8f1733ae1c9d54cad120.svg
          fullname: liuhantao
          isHf: false
          isPro: false
          name: liuhantao
          type: user
        html: "<p>\u7ECF\u53CD\u590D\u5C1D\u8BD5\uFF0C\u5347\u7EA74.27.1\u4EE5\u53CA\
          \u4EE5\u4E0A\u7248\u672C\u540E\uFF0Cweb_demo2.py\u3001web_demo.py \u6587\
          \u4EF6\u4E2D\u8BC6\u522B\u76EE\u5F55\u7531\u201CTHUDM/chatglm-6b\u201D\uFF0C\
          \u9700\u8981\u66F4\u6539\u4E3A\"THUDM\\chatglm-6b\"\uFF0C\u66F4\u6539\u540E\
          \u95EE\u9898\u89E3\u51B3\u3002</p>\n"
        raw: "\u7ECF\u53CD\u590D\u5C1D\u8BD5\uFF0C\u5347\u7EA74.27.1\u4EE5\u53CA\u4EE5\
          \u4E0A\u7248\u672C\u540E\uFF0Cweb_demo2.py\u3001web_demo.py \u6587\u4EF6\
          \u4E2D\u8BC6\u522B\u76EE\u5F55\u7531\u201CTHUDM/chatglm-6b\u201D\uFF0C\u9700\
          \u8981\u66F4\u6539\u4E3A\"THUDM\\chatglm-6b\"\uFF0C\u66F4\u6539\u540E\u95EE\
          \u9898\u89E3\u51B3\u3002"
        updatedAt: '2023-04-17T06:43:35.159Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mayzyo
        - Akakki
    id: 643cea971c29e1386bb65bd3
    type: comment
  author: liuhantao
  content: "\u7ECF\u53CD\u590D\u5C1D\u8BD5\uFF0C\u5347\u7EA74.27.1\u4EE5\u53CA\u4EE5\
    \u4E0A\u7248\u672C\u540E\uFF0Cweb_demo2.py\u3001web_demo.py \u6587\u4EF6\u4E2D\
    \u8BC6\u522B\u76EE\u5F55\u7531\u201CTHUDM/chatglm-6b\u201D\uFF0C\u9700\u8981\u66F4\
    \u6539\u4E3A\"THUDM\\chatglm-6b\"\uFF0C\u66F4\u6539\u540E\u95EE\u9898\u89E3\u51B3\
    \u3002"
  created_at: 2023-04-17 05:43:35+00:00
  edited: false
  hidden: false
  id: 643cea971c29e1386bb65bd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d8a1af9f8f22e18315bcd50f590caf7.svg
      fullname: zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: redauzhang
      type: user
    createdAt: '2023-04-19T06:23:07.000Z'
    data:
      edited: false
      editors:
      - redauzhang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d8a1af9f8f22e18315bcd50f590caf7.svg
          fullname: zhang
          isHf: false
          isPro: false
          name: redauzhang
          type: user
        html: "<p>\u6709\u6761\u4EF6\uFF0C\u6211\u4EEC\u90FD\u662F\u4E0B\u8F7D\u79BB\
          \u7EBF\u7248\u672C\u7684\uFF0C\u90FD\u6CA1\u6709\u8FD9\u4E2A\u56F0\u6270\
          \u4E86\u3002\u3002</p>\n"
        raw: "\u6709\u6761\u4EF6\uFF0C\u6211\u4EEC\u90FD\u662F\u4E0B\u8F7D\u79BB\u7EBF\
          \u7248\u672C\u7684\uFF0C\u90FD\u6CA1\u6709\u8FD9\u4E2A\u56F0\u6270\u4E86\
          \u3002\u3002"
        updatedAt: '2023-04-19T06:23:07.054Z'
      numEdits: 0
      reactions: []
    id: 643f88cbba7506b57e370835
    type: comment
  author: redauzhang
  content: "\u6709\u6761\u4EF6\uFF0C\u6211\u4EEC\u90FD\u662F\u4E0B\u8F7D\u79BB\u7EBF\
    \u7248\u672C\u7684\uFF0C\u90FD\u6CA1\u6709\u8FD9\u4E2A\u56F0\u6270\u4E86\u3002\
    \u3002"
  created_at: 2023-04-19 05:23:07+00:00
  edited: false
  hidden: false
  id: 643f88cbba7506b57e370835
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: THUDM/chatglm-6b
repo_type: model
status: open
target_branch: null
title: "transformers  \u4ECE4.26.1 \u5347\u7EA7\u81F34.27.1 \u540E\u62A5\u9519 ModuleNotFoundError:\
  \ No module named 'transformers_modules.THUDM/chatglm-6b'"
