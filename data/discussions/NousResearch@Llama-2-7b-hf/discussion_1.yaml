!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hambaobao
conflicting_files: null
created_at: 2023-07-20 15:18:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50dbeefeb3e5e830a91e412bf8368396.svg
      fullname: James
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hambaobao
      type: user
    createdAt: '2023-07-20T16:18:47.000Z'
    data:
      edited: false
      editors:
      - Hambaobao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9351035356521606
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50dbeefeb3e5e830a91e412bf8368396.svg
          fullname: James
          isHf: false
          isPro: false
          name: Hambaobao
          type: user
        html: '<p>max_model_length in tokenizer_config.json seems wrong. Why is it
          so big, I see max_model_length in other reps is only 2048.</p>

          '
        raw: max_model_length in tokenizer_config.json seems wrong. Why is it so big,
          I see max_model_length in other reps is only 2048.
        updatedAt: '2023-07-20T16:18:47.563Z'
      numEdits: 0
      reactions: []
    id: 64b95e67acfb31c3f7bc6729
    type: comment
  author: Hambaobao
  content: max_model_length in tokenizer_config.json seems wrong. Why is it so big,
    I see max_model_length in other reps is only 2048.
  created_at: 2023-07-20 15:18:47+00:00
  edited: false
  hidden: false
  id: 64b95e67acfb31c3f7bc6729
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/18f50817eebbf4acddb0f61170b9a2bd.svg
      fullname: cnut1648
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cnut1648
      type: user
    createdAt: '2023-07-21T15:15:42.000Z'
    data:
      edited: false
      editors:
      - cnut1648
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9287405610084534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/18f50817eebbf4acddb0f61170b9a2bd.svg
          fullname: cnut1648
          isHf: false
          isPro: false
          name: cnut1648
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Hambaobao&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Hambaobao\">@<span class=\"\
          underline\">Hambaobao</span></a></span>\n\n\t</span></span> yeah I had this\
          \ issue too when I wrote a script to pad my datasets in <code>max_model_length</code>,\
          \ boy that was insane to have <code>1000000000000000019884624838656</code>\
          \ length. I made a pull request in <a href=\"https://huggingface.co/NousResearch/Llama-2-7b-hf/discussions/2/files\"\
          >https://huggingface.co/NousResearch/Llama-2-7b-hf/discussions/2/files</a>\
          \ to make it 4k</p>\n"
        raw: '@Hambaobao yeah I had this issue too when I wrote a script to pad my
          datasets in `max_model_length`, boy that was insane to have `1000000000000000019884624838656`
          length. I made a pull request in https://huggingface.co/NousResearch/Llama-2-7b-hf/discussions/2/files
          to make it 4k'
        updatedAt: '2023-07-21T15:15:42.992Z'
      numEdits: 0
      reactions: []
    id: 64baa11e06087679b82e8cfc
    type: comment
  author: cnut1648
  content: '@Hambaobao yeah I had this issue too when I wrote a script to pad my datasets
    in `max_model_length`, boy that was insane to have `1000000000000000019884624838656`
    length. I made a pull request in https://huggingface.co/NousResearch/Llama-2-7b-hf/discussions/2/files
    to make it 4k'
  created_at: 2023-07-21 14:15:42+00:00
  edited: false
  hidden: false
  id: 64baa11e06087679b82e8cfc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-07-21T17:22:48.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9908255934715271
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>It doesn''t actually matter so long as its 4096 or greater, but
          it is confusing. Updating</p>

          '
        raw: It doesn't actually matter so long as its 4096 or greater, but it is
          confusing. Updating
        updatedAt: '2023-07-21T17:22:48.946Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64babee878b89c4aa4b44c8b
    id: 64babee878b89c4aa4b44c87
    type: comment
  author: teknium
  content: It doesn't actually matter so long as its 4096 or greater, but it is confusing.
    Updating
  created_at: 2023-07-21 16:22:48+00:00
  edited: false
  hidden: false
  id: 64babee878b89c4aa4b44c87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-07-21T17:22:48.000Z'
    data:
      status: closed
    id: 64babee878b89c4aa4b44c8b
    type: status-change
  author: teknium
  created_at: 2023-07-21 16:22:48+00:00
  id: 64babee878b89c4aa4b44c8b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NousResearch/Llama-2-7b-hf
repo_type: model
status: closed
target_branch: null
title: Too BIG! max_model_length in tokenizer_config.json
