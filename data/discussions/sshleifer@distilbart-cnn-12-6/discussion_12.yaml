!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aschmidt99
conflicting_files: null
created_at: 2023-02-05 11:26:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7562f6995d0fac77d135a0ca0d0325c.svg
      fullname: Anders McIlquham-Schmidt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aschmidt99
      type: user
    createdAt: '2023-02-05T11:26:58.000Z'
    data:
      edited: false
      editors:
      - aschmidt99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7562f6995d0fac77d135a0ca0d0325c.svg
          fullname: Anders McIlquham-Schmidt
          isHf: false
          isPro: false
          name: aschmidt99
          type: user
        html: "<p>Hi all</p>\n<p>I am trying to summarize scientific articles using\
          \ distilbart-cnn-12-6<br>I tried it with a normal fictional book (Dorian\
          \ Gray) to start with and it worked fine but as soon as I wanted to summarize\
          \ a scientific article it came up with several error messages after a couple\
          \ of minutes.<br>I don\u2019t know what they mean and how to rectify them.<br>Does\
          \ anyone have any suggestions?</p>\n<p>Kind regards</p>\n<p>Anders</p>\n\
          <p>IndexError                           Traceback (most recent call last)<br>\
          \ in <br>      1 for input in inputs:<br>----&gt; 2   output = model.generate(**input)<br>\
          \      3   print(tokenizer.decode(*output, skip_special_tokens=True))</p>\n\
          <hr>\n<p>8 frames</p>\n<hr>\n<p>/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\
          \ in decorate_context(*args, **kwargs)<br>     25         def decorate_context(*args,\
          \ **kwargs):<br>     26             with self.clone():<br>---&gt; 27   \
          \              return func(*args, **kwargs)<br>     28         return cast(F,\
          \ decorate_context)<br>     29 </p>\n<p>/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\
          \ in generate(self, inputs, generation_config, logits_processor, stopping_criteria,\
          \ prefix_allowed_tokens_fn, synced_gpus, **kwargs)<br>   1250          \
          \   # if model is encoder decoder encoder_outputs are created<br>   1251\
          \             # and added to <code>model_kwargs</code><br>-&gt; 1252   \
          \          model_kwargs = self.<em>prepare_encoder_decoder</em> kwargs_for_generation(<br>\
          \   1253                 inputs_tensor, model_kwargs, model_input_name<br>\
          \   1254             )</p>\n<p>/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\
          \ in _prepare_encoder_decoder_kwargs_for_generation(self, inputs_tensor,\
          \ model_kwargs, model_input_name)<br>    615         encoder_kwargs[\"return_dict\"\
          ] = True<br>    616         encoder_kwargs[model_input_name] = inputs_tensor<br>--&gt;\
          \ 617         model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)<br>\
          \    618<br>    619         return model_kwargs</p>\n<p>/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *input, **kwargs)<br>   1192         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks<br>\
          \   1193                 or _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt;\
          \ 1194             return forward_call(*input, **kwargs)<br>   1195    \
          \     # Do not call functions when jit is used<br>   1196         full_backward_hooks,\
          \ non_full_backward_hooks = [], []</p>\n<p>/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\
          \ in forward(self, input_ids, attention_mask, head_mask, inputs_embeds,\
          \ output_attentions, output_hidden_states, return_dict)<br>    808     \
          \        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale<br>\
          \    809<br>--&gt; 810         embed_pos = self.embed_positions(input)<br>\
          \    811         embed_pos = embed_pos.to(inputs_embeds.device)<br>    812\
          \ </p>\n<p>/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *input, **kwargs)<br>   1192         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks<br>\
          \   1193                 or _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt;\
          \ 1194             return forward_call(*input, **kwargs)<br>   1195    \
          \     # Do not call functions when jit is used<br>   1196         full_backward_hooks,\
          \ non_full_backward_hooks = [], []</p>\n<p>/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\
          \ in forward(self, input_ids, past_key_values_length)<br>    136       \
          \  ).expand(bsz, -1)<br>    137<br>--&gt; 138         return super().forward(positions\
          \ + self.offset)<br>    139<br>    140 </p>\n<p>/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\
          \ in forward(self, input)<br>    158<br>    159     def forward(self, input:\
          \ Tensor) -&gt; Tensor:<br>--&gt; 160         return F.embedding(<br>  \
          \  161             input, self.weight, self.padding_idx, self.max_norm,<br>\
          \    162             self.norm_type, self.scale_grad_by_freq, self.sparse)</p>\n\
          <p>/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py in embedding(input,\
          \ weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)<br>\
          \   2208         # remove once script supports set_grad_enabled<br>   2209\
          \         <em>no_grad_embedding_renorm</em>(weight, input, max_norm, norm_type)<br>-&gt;\
          \ 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
          \ sparse)<br>   2211<br>   2212 </p>\n<p>IndexError: index out of range\
          \ in self</p>\n"
        raw: "Hi all\r\n\r\nI am trying to summarize scientific articles using distilbart-cnn-12-6\
          \ \r\nI tried it with a normal fictional book (Dorian Gray) to start with\
          \ and it worked fine but as soon as I wanted to summarize a scientific article\
          \ it came up with several error messages after a couple of minutes. \r\n\
          I don\u2019t know what they mean and how to rectify them. \r\nDoes anyone\
          \ have any suggestions?\r\n\r\nKind regards\r\n\r\nAnders\r\n\r\nIndexError\
          \                           Traceback (most recent call last)\r\n<ipython-input-18-2f5276bec215>\
          \ in <module>\r\n      1 for input in inputs:\r\n----> 2   output = model.generate(**input)\r\
          \n      3   print(tokenizer.decode(*output, skip_special_tokens=True))\r\
          \n\r\n________________________________________\r\n8 frames\r\n________________________________________\r\
          \n/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py in\
          \ decorate_context(*args, **kwargs)\r\n     25         def decorate_context(*args,\
          \ **kwargs):\r\n     26             with self.clone():\r\n---> 27      \
          \           return func(*args, **kwargs)\r\n     28         return cast(F,\
          \ decorate_context)\r\n     29 \r\n\r\n/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\
          \ in generate(self, inputs, generation_config, logits_processor, stopping_criteria,\
          \ prefix_allowed_tokens_fn, synced_gpus, **kwargs)\r\n   1250          \
          \   # if model is encoder decoder encoder_outputs are created\r\n   1251\
          \             # and added to `model_kwargs`\r\n-> 1252             model_kwargs\
          \ = self._prepare_encoder_decoder_ kwargs_for_generation(\r\n   1253   \
          \              inputs_tensor, model_kwargs, model_input_name\r\n   1254\
          \             )\r\n\r\n/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\
          \ in _prepare_encoder_decoder_kwargs_for_generation(self, inputs_tensor,\
          \ model_kwargs, model_input_name)\r\n    615         encoder_kwargs[\"return_dict\"\
          ] = True\r\n    616         encoder_kwargs[model_input_name] = inputs_tensor\r\
          \n--> 617         model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\r\
          \n    618 \r\n    619         return model_kwargs\r\n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *input, **kwargs)\r\n   1192         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
          \n   1193                 or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1194             return forward_call(*input, **kwargs)\r\n   1195 \
          \        # Do not call functions when jit is used\r\n   1196         full_backward_hooks,\
          \ non_full_backward_hooks = [], []\r\n\r\n/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\
          \ in forward(self, input_ids, attention_mask, head_mask, inputs_embeds,\
          \ output_attentions, output_hidden_states, return_dict)\r\n    808     \
          \        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\r\
          \n    809 \r\n--> 810         embed_pos = self.embed_positions(input)\r\n\
          \    811         embed_pos = embed_pos.to(inputs_embeds.device)\r\n    812\
          \ \r\n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *input, **kwargs)\r\n   1192         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
          \n   1193                 or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1194             return forward_call(*input, **kwargs)\r\n   1195 \
          \        # Do not call functions when jit is used\r\n   1196         full_backward_hooks,\
          \ non_full_backward_hooks = [], []\r\n\r\n/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\
          \ in forward(self, input_ids, past_key_values_length)\r\n    136       \
          \  ).expand(bsz, -1)\r\n    137 \r\n--> 138         return super().forward(positions\
          \ + self.offset)\r\n    139 \r\n    140 \r\n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\
          \ in forward(self, input)\r\n    158 \r\n    159     def forward(self, input:\
          \ Tensor) -> Tensor:\r\n--> 160         return F.embedding(\r\n    161 \
          \            input, self.weight, self.padding_idx, self.max_norm,\r\n  \
          \  162             self.norm_type, self.scale_grad_by_freq, self.sparse)\r\
          \n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py in embedding(input,\
          \ weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\r\
          \n   2208         # remove once script supports set_grad_enabled\r\n   2209\
          \         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\
          \n-> 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
          \ sparse)\r\n   2211 \r\n   2212 \r\n\r\nIndexError: index out of range\
          \ in self\r\n\r\n\r\n\r\n"
        updatedAt: '2023-02-05T11:26:58.563Z'
      numEdits: 0
      reactions: []
    id: 63df928242591dda0b921e96
    type: comment
  author: aschmidt99
  content: "Hi all\r\n\r\nI am trying to summarize scientific articles using distilbart-cnn-12-6\
    \ \r\nI tried it with a normal fictional book (Dorian Gray) to start with and\
    \ it worked fine but as soon as I wanted to summarize a scientific article it\
    \ came up with several error messages after a couple of minutes. \r\nI don\u2019\
    t know what they mean and how to rectify them. \r\nDoes anyone have any suggestions?\r\
    \n\r\nKind regards\r\n\r\nAnders\r\n\r\nIndexError                           Traceback\
    \ (most recent call last)\r\n<ipython-input-18-2f5276bec215> in <module>\r\n \
    \     1 for input in inputs:\r\n----> 2   output = model.generate(**input)\r\n\
    \      3   print(tokenizer.decode(*output, skip_special_tokens=True))\r\n\r\n\
    ________________________________________\r\n8 frames\r\n________________________________________\r\
    \n/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args,\
    \ **kwargs)\r\n     25         def decorate_context(*args, **kwargs):\r\n    \
    \ 26             with self.clone():\r\n---> 27                 return func(*args,\
    \ **kwargs)\r\n     28         return cast(F, decorate_context)\r\n     29 \r\n\
    \r\n/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py in\
    \ generate(self, inputs, generation_config, logits_processor, stopping_criteria,\
    \ prefix_allowed_tokens_fn, synced_gpus, **kwargs)\r\n   1250             # if\
    \ model is encoder decoder encoder_outputs are created\r\n   1251            \
    \ # and added to `model_kwargs`\r\n-> 1252             model_kwargs = self._prepare_encoder_decoder_\
    \ kwargs_for_generation(\r\n   1253                 inputs_tensor, model_kwargs,\
    \ model_input_name\r\n   1254             )\r\n\r\n/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\
    \ in _prepare_encoder_decoder_kwargs_for_generation(self, inputs_tensor, model_kwargs,\
    \ model_input_name)\r\n    615         encoder_kwargs[\"return_dict\"] = True\r\
    \n    616         encoder_kwargs[model_input_name] = inputs_tensor\r\n--> 617\
    \         model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\r\
    \n    618 \r\n    619         return model_kwargs\r\n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *input, **kwargs)\r\n   1192         if not (self._backward_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
    \n   1193                 or _global_forward_hooks or _global_forward_pre_hooks):\r\
    \n-> 1194             return forward_call(*input, **kwargs)\r\n   1195       \
    \  # Do not call functions when jit is used\r\n   1196         full_backward_hooks,\
    \ non_full_backward_hooks = [], []\r\n\r\n/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\
    \ in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions,\
    \ output_hidden_states, return_dict)\r\n    808             inputs_embeds = self.embed_tokens(input_ids)\
    \ * self.embed_scale\r\n    809 \r\n--> 810         embed_pos = self.embed_positions(input)\r\
    \n    811         embed_pos = embed_pos.to(inputs_embeds.device)\r\n    812 \r\
    \n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py in _call_impl(self,\
    \ *input, **kwargs)\r\n   1192         if not (self._backward_hooks or self._forward_hooks\
    \ or self._forward_pre_hooks or _global_backward_hooks\r\n   1193            \
    \     or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1194     \
    \        return forward_call(*input, **kwargs)\r\n   1195         # Do not call\
    \ functions when jit is used\r\n   1196         full_backward_hooks, non_full_backward_hooks\
    \ = [], []\r\n\r\n/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\
    \ in forward(self, input_ids, past_key_values_length)\r\n    136         ).expand(bsz,\
    \ -1)\r\n    137 \r\n--> 138         return super().forward(positions + self.offset)\r\
    \n    139 \r\n    140 \r\n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\
    \ in forward(self, input)\r\n    158 \r\n    159     def forward(self, input:\
    \ Tensor) -> Tensor:\r\n--> 160         return F.embedding(\r\n    161       \
    \      input, self.weight, self.padding_idx, self.max_norm,\r\n    162       \
    \      self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n\r\n/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\
    \ in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq,\
    \ sparse)\r\n   2208         # remove once script supports set_grad_enabled\r\n\
    \   2209         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\
    \n-> 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
    \ sparse)\r\n   2211 \r\n   2212 \r\n\r\nIndexError: index out of range in self\r\
    \n\r\n\r\n\r\n"
  created_at: 2023-02-05 11:26:58+00:00
  edited: false
  hidden: false
  id: 63df928242591dda0b921e96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0f41f2b9f838135f28cc3825af908b5.svg
      fullname: Norman Baatz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: normanbaatz
      type: user
    createdAt: '2023-08-08T14:35:32.000Z'
    data:
      edited: false
      editors:
      - normanbaatz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9482645392417908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0f41f2b9f838135f28cc3825af908b5.svg
          fullname: Norman Baatz
          isHf: false
          isPro: false
          name: normanbaatz
          type: user
        html: '<p>I see this error when I try to process more text than the limit
          of the model. Maybe try it with a few paragraphs first?</p>

          '
        raw: I see this error when I try to process more text than the limit of the
          model. Maybe try it with a few paragraphs first?
        updatedAt: '2023-08-08T14:35:32.839Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aschmidt99
    id: 64d252b4f1fb6b41aef64377
    type: comment
  author: normanbaatz
  content: I see this error when I try to process more text than the limit of the
    model. Maybe try it with a few paragraphs first?
  created_at: 2023-08-08 13:35:32+00:00
  edited: false
  hidden: false
  id: 64d252b4f1fb6b41aef64377
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7562f6995d0fac77d135a0ca0d0325c.svg
      fullname: Anders McIlquham-Schmidt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aschmidt99
      type: user
    createdAt: '2023-08-09T13:50:49.000Z'
    data:
      edited: false
      editors:
      - aschmidt99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9141826629638672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7562f6995d0fac77d135a0ca0d0325c.svg
          fullname: Anders McIlquham-Schmidt
          isHf: false
          isPro: false
          name: aschmidt99
          type: user
        html: '<p>Thank you for the input. I will try that</p>

          '
        raw: Thank you for the input. I will try that
        updatedAt: '2023-08-09T13:50:49.487Z'
      numEdits: 0
      reactions: []
    id: 64d399b91074f37ed54bdc77
    type: comment
  author: aschmidt99
  content: Thank you for the input. I will try that
  created_at: 2023-08-09 12:50:49+00:00
  edited: false
  hidden: false
  id: 64d399b91074f37ed54bdc77
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: sshleifer/distilbart-cnn-12-6
repo_type: model
status: open
target_branch: null
title: Summarization of scientific literature with destilbart-cnn-12-6
