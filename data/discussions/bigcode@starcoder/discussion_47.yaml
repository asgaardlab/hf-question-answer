!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ayaan-k1
conflicting_files: null
created_at: 2023-06-02 06:19:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b28522a1b2b59e3dee9b1d7e4e19150.svg
      fullname: Ayaan Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ayaan-k1
      type: user
    createdAt: '2023-06-02T07:19:31.000Z'
    data:
      edited: false
      editors:
      - ayaan-k1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b28522a1b2b59e3dee9b1d7e4e19150.svg
          fullname: Ayaan Khan
          isHf: false
          isPro: false
          name: ayaan-k1
          type: user
        html: '<p>I have loaded the model in 8 bit so as to save memory:</p>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoder",force_download=True)</p>

          <h1 id="for-fp16-replace-with--load_in_8bittrue-with---torch_dtypetorchfloat16">for
          fp16 replace with  <code>load_in_8bit=True</code> with   <code>torch_dtype=torch.float16</code></h1>

          <p>model = AutoModelForCausalLM.from_pretrained("bigcode/starcoder", load_in_8bit=True,device_map={"":
          ''cuda''})<br>print(f"Memory footprint: {model.get_memory_footprint() /
          1e6:.2f} MB")</p>

          <p>Any solutions?</p>

          '
        raw: "I have loaded the model in 8 bit so as to save memory:\r\n\r\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          bigcode/starcoder\",force_download=True)\r\n# for fp16 replace with  `load_in_8bit=True`\
          \ with   `torch_dtype=torch.float16`\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          bigcode/starcoder\", load_in_8bit=True,device_map={\"\": 'cuda'})\r\nprint(f\"\
          Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\r\n\r\n\
          Any solutions?"
        updatedAt: '2023-06-02T07:19:31.233Z'
      numEdits: 0
      reactions: []
    id: 647998037ca7c879b843111f
    type: comment
  author: ayaan-k1
  content: "I have loaded the model in 8 bit so as to save memory:\r\n\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    bigcode/starcoder\",force_download=True)\r\n# for fp16 replace with  `load_in_8bit=True`\
    \ with   `torch_dtype=torch.float16`\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    bigcode/starcoder\", load_in_8bit=True,device_map={\"\": 'cuda'})\r\nprint(f\"\
    Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\r\n\r\nAny solutions?"
  created_at: 2023-06-02 06:19:31+00:00
  edited: false
  hidden: false
  id: 647998037ca7c879b843111f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018dd34e29a13c4fbe9273f33ee3d3d.svg
      fullname: AI Safety Research
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AISafety
      type: user
    createdAt: '2023-06-02T10:24:39.000Z'
    data:
      edited: false
      editors:
      - AISafety
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018dd34e29a13c4fbe9273f33ee3d3d.svg
          fullname: AI Safety Research
          isHf: false
          isPro: false
          name: AISafety
          type: user
        html: '<p>Would be better if you also include what happened with the code
          run. The problem would likely become clearer.</p>

          '
        raw: Would be better if you also include what happened with the code run.
          The problem would likely become clearer.
        updatedAt: '2023-06-02T10:24:39.020Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ybelkada
    id: 6479c367f518a860fbc4f629
    type: comment
  author: AISafety
  content: Would be better if you also include what happened with the code run. The
    problem would likely become clearer.
  created_at: 2023-06-02 09:24:39+00:00
  edited: false
  hidden: false
  id: 6479c367f518a860fbc4f629
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-10-05T09:26:10.000Z'
    data:
      status: closed
    id: 651e813268662e0479dbbd55
    type: status-change
  author: loubnabnl
  created_at: 2023-10-05 08:26:10+00:00
  id: 651e813268662e0479dbbd55
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 47
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: ' The command outputs = model.generate(inputs) is throwing error "RuntimeError:
  "LayerNormKernelImpl" not implemented for ''Half''"'
