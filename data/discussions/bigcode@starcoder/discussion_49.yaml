!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MukeshSharma
conflicting_files: null
created_at: 2023-06-05 08:45:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-06-05T09:45:12.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9138779640197754
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: '<p>knowing max_length is kept 300 , but answer is getting ended in
          150 , so how to stop the model so that it dont give further prediction .<br>Any
          suggestion can help , since I aint sure whats the max length for different
          prompts , so setting it to a static , some time gives unwanted prediction
          after the actual prediction is already done. </p>

          '
        raw: "knowing max_length is kept 300 , but answer is getting ended in 150\
          \ , so how to stop the model so that it dont give further prediction . \r\
          \nAny suggestion can help , since I aint sure whats the max length for different\
          \ prompts , so setting it to a static , some time gives unwanted prediction\
          \ after the actual prediction is already done. "
        updatedAt: '2023-06-05T09:45:12.633Z'
      numEdits: 0
      reactions: []
    id: 647daea8becb41a2729157e8
    type: comment
  author: MukeshSharma
  content: "knowing max_length is kept 300 , but answer is getting ended in 150 ,\
    \ so how to stop the model so that it dont give further prediction . \r\nAny suggestion\
    \ can help , since I aint sure whats the max length for different prompts , so\
    \ setting it to a static , some time gives unwanted prediction after the actual\
    \ prediction is already done. "
  created_at: 2023-06-05 08:45:12+00:00
  edited: false
  hidden: false
  id: 647daea8becb41a2729157e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c32ba412508219efb5685dd412059fa.svg
      fullname: Cal Mitchell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dataPlumber
      type: user
    createdAt: '2023-06-17T12:51:34.000Z'
    data:
      edited: false
      editors:
      - dataPlumber
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.9861560463905334
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c32ba412508219efb5685dd412059fa.svg
          fullname: Cal Mitchell
          isHf: false
          isPro: false
          name: dataPlumber
          type: user
        html: '<p>+1</p>

          '
        raw: '+1'
        updatedAt: '2023-06-17T12:51:34.622Z'
      numEdits: 0
      reactions: []
    id: 648dac5680bf2c95993601e4
    type: comment
  author: dataPlumber
  content: '+1'
  created_at: 2023-06-17 11:51:34+00:00
  edited: false
  hidden: false
  id: 648dac5680bf2c95993601e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d8aeec180e4bcdc527aa54141a53fb8.svg
      fullname: Gurpreet Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: doraexp
      type: user
    createdAt: '2023-06-19T18:55:33.000Z'
    data:
      edited: false
      editors:
      - doraexp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38468965888023376
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d8aeec180e4bcdc527aa54141a53fb8.svg
          fullname: Gurpreet Singh
          isHf: false
          isPro: false
          name: doraexp
          type: user
        html: "<p>use this: </p>\n<pre><code>import time\nimport torch\nfrom transformers\
          \ import pipeline\n\nstart = time.time()\n'''loading the local checkpoint\
          \ here, device_map = \"auto\" decide where to put each layer, \neither on\
          \ the GPU or the CPU'''\npipe = pipeline(\"text-generation\", model=\"/home/ec2-user/starCoderCheckpointLocal\"\
          ,\n                torch_dtype=torch.bfloat16, device_map= \"auto\",load_in_8bit=True)\n\
          \ntext = input(\"Enter query &gt;&gt;\")\n\nprompt_template = \"&lt;|system|&gt;\\\
          n&lt;|end|&gt;\\n&lt;|user|&gt;\\n{query}&lt;|end|&gt;\\n&lt;|assistant|&gt;\"\
          \nprompt = prompt_template.format(query=text)\noutputs = pipe(prompt, max_new_tokens=512,\
          \ stop_sequence='&lt;|end|&gt;', do_sample=True,\n               temperature=0.2,\
          \ top_k=50, top_p= 0.95, eos_token_id= 49155)\n\n# print(outputs)\n# print(\
          \ outputs[0]['generated_text'])\ngenerated = outputs[0]['generated_text'].split('&lt;|assistant|&gt;')[-1]\n\
          print(generated)\n\nend = seconds = time.time()\ntime = end - start\nprint(\"\
          Time taken: \", str(int(time//60))+\"minutes\",str(round(time%60))+\"seconds\"\
          )\n</code></pre>\n"
        raw: "use this: \n\n```\nimport time\nimport torch\nfrom transformers import\
          \ pipeline\n\nstart = time.time()\n'''loading the local checkpoint here,\
          \ device_map = \"auto\" decide where to put each layer, \neither on the\
          \ GPU or the CPU'''\npipe = pipeline(\"text-generation\", model=\"/home/ec2-user/starCoderCheckpointLocal\"\
          ,\n                torch_dtype=torch.bfloat16, device_map= \"auto\",load_in_8bit=True)\n\
          \ntext = input(\"Enter query >>\")\n\nprompt_template = \"<|system|>\\n<|end|>\\\
          n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\nprompt = prompt_template.format(query=text)\n\
          outputs = pipe(prompt, max_new_tokens=512, stop_sequence='<|end|>', do_sample=True,\n\
          \               temperature=0.2, top_k=50, top_p= 0.95, eos_token_id= 49155)\n\
          \n# print(outputs)\n# print( outputs[0]['generated_text'])\ngenerated =\
          \ outputs[0]['generated_text'].split('<|assistant|>')[-1]\nprint(generated)\n\
          \nend = seconds = time.time()\ntime = end - start\nprint(\"Time taken: \"\
          , str(int(time//60))+\"minutes\",str(round(time%60))+\"seconds\")\n```"
        updatedAt: '2023-06-19T18:55:33.199Z'
      numEdits: 0
      reactions: []
    id: 6490a4a530d3f10da2bbcfb5
    type: comment
  author: doraexp
  content: "use this: \n\n```\nimport time\nimport torch\nfrom transformers import\
    \ pipeline\n\nstart = time.time()\n'''loading the local checkpoint here, device_map\
    \ = \"auto\" decide where to put each layer, \neither on the GPU or the CPU'''\n\
    pipe = pipeline(\"text-generation\", model=\"/home/ec2-user/starCoderCheckpointLocal\"\
    ,\n                torch_dtype=torch.bfloat16, device_map= \"auto\",load_in_8bit=True)\n\
    \ntext = input(\"Enter query >>\")\n\nprompt_template = \"<|system|>\\n<|end|>\\\
    n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\nprompt = prompt_template.format(query=text)\n\
    outputs = pipe(prompt, max_new_tokens=512, stop_sequence='<|end|>', do_sample=True,\n\
    \               temperature=0.2, top_k=50, top_p= 0.95, eos_token_id= 49155)\n\
    \n# print(outputs)\n# print( outputs[0]['generated_text'])\ngenerated = outputs[0]['generated_text'].split('<|assistant|>')[-1]\n\
    print(generated)\n\nend = seconds = time.time()\ntime = end - start\nprint(\"\
    Time taken: \", str(int(time//60))+\"minutes\",str(round(time%60))+\"seconds\"\
    )\n```"
  created_at: 2023-06-19 17:55:33+00:00
  edited: false
  hidden: false
  id: 6490a4a530d3f10da2bbcfb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-06-20T05:25:22.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.688359797000885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;doraexp&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/doraexp\"\
          >@<span class=\"underline\">doraexp</span></a></span>\n\n\t</span></span>\
          \  i got the value error.<br>ValueError: The following <code>model_kwargs</code>\
          \ are not used by the model: ['stop_sequence'] (note: typos in the generate\
          \ arguments will also show up in this list)<br>        output = model.generate(<br>\
          \            input_ids,<br>            do_sample=True,<br>            min_length=min_length,<br>\
          \            max_length=max_length,<br>            temperature=temperature,<br>\
          \            early_stopping=True,<br>            stop_sequence='&lt;|end|&gt;',<br>\
          \            top_k=50,<br>            top_p= 0.95,<br>            eos_token_id=\
          \ 49155,</p>\n<pre><code>    )\n</code></pre>\n<p>I am using stracoder model\
          \ , any further suggestion to resolve this. Or any alternative ? do suggest<br>thanks\
          \ </p>\n"
        raw: "Hey @doraexp  i got the value error.\nValueError: The following `model_kwargs`\
          \ are not used by the model: ['stop_sequence'] (note: typos in the generate\
          \ arguments will also show up in this list)\n        output = model.generate(\n\
          \            input_ids,\n            do_sample=True,\n            min_length=min_length,\n\
          \            max_length=max_length,\n            temperature=temperature,\n\
          \            early_stopping=True,\n            stop_sequence='<|end|>',\n\
          \            top_k=50,\n            top_p= 0.95,\n            eos_token_id=\
          \ 49155,\n            \n        )\n\n\nI am using stracoder model , any\
          \ further suggestion to resolve this. Or any alternative ? do suggest \n\
          thanks "
        updatedAt: '2023-06-20T05:25:22.537Z'
      numEdits: 0
      reactions: []
    id: 649138422549fd68a7758ad0
    type: comment
  author: MukeshSharma
  content: "Hey @doraexp  i got the value error.\nValueError: The following `model_kwargs`\
    \ are not used by the model: ['stop_sequence'] (note: typos in the generate arguments\
    \ will also show up in this list)\n        output = model.generate(\n        \
    \    input_ids,\n            do_sample=True,\n            min_length=min_length,\n\
    \            max_length=max_length,\n            temperature=temperature,\n  \
    \          early_stopping=True,\n            stop_sequence='<|end|>',\n      \
    \      top_k=50,\n            top_p= 0.95,\n            eos_token_id= 49155,\n\
    \            \n        )\n\n\nI am using stracoder model , any further suggestion\
    \ to resolve this. Or any alternative ? do suggest \nthanks "
  created_at: 2023-06-20 04:25:22+00:00
  edited: false
  hidden: false
  id: 649138422549fd68a7758ad0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-06-22T10:56:49.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8923560976982117
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;doraexp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/doraexp\">@<span class=\"\
          underline\">doraexp</span></a></span>\n\n\t</span></span> can u please help\
          \ on this ? really looking forward for ur help . </p>\n"
        raw: '@doraexp can u please help on this ? really looking forward for ur help
          . '
        updatedAt: '2023-06-22T10:56:49.714Z'
      numEdits: 0
      reactions: []
    id: 649428f151d4ea34849de480
    type: comment
  author: MukeshSharma
  content: '@doraexp can u please help on this ? really looking forward for ur help
    . '
  created_at: 2023-06-22 09:56:49+00:00
  edited: false
  hidden: false
  id: 649428f151d4ea34849de480
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d8aeec180e4bcdc527aa54141a53fb8.svg
      fullname: Gurpreet Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: doraexp
      type: user
    createdAt: '2023-06-22T17:11:14.000Z'
    data:
      edited: true
      editors:
      - doraexp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9768026471138
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d8aeec180e4bcdc527aa54141a53fb8.svg
          fullname: Gurpreet Singh
          isHf: false
          isPro: false
          name: doraexp
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;MukeshSharma&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MukeshSharma\"\
          >@<span class=\"underline\">MukeshSharma</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Could you please provide me with the code snippet that you using and\
          \ the checkpoint that you are trying to load, and the whole error would\
          \ be really helpful too. :))</p>\n"
        raw: 'Hi @MukeshSharma,


          Could you please provide me with the code snippet that you using and the
          checkpoint that you are trying to load, and the whole error would be really
          helpful too. :))

          '
        updatedAt: '2023-06-22T17:11:34.082Z'
      numEdits: 1
      reactions: []
    id: 649480b28eba8ebc53fd6b43
    type: comment
  author: doraexp
  content: 'Hi @MukeshSharma,


    Could you please provide me with the code snippet that you using and the checkpoint
    that you are trying to load, and the whole error would be really helpful too.
    :))

    '
  created_at: 2023-06-22 16:11:14+00:00
  edited: true
  hidden: false
  id: 649480b28eba8ebc53fd6b43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-06-22T17:18:43.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5846466422080994
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: '<p>I am loading the same checkpoint<br>from transformers import AutoTokenizer,
          AutoModelForCausalLM</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoder")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("bigcode/starcoder")</p>

          <p>No etc changes<br>During output i am using<br>output = model.generate(<br>input_ids,<br>do_sample=True,<br>min_length=min_length,<br>max_length=max_length,<br>temperature=temperature,<br>early_stopping=True,<br>stop_sequence=''&lt;|end|&gt;'',<br>top_k=50,<br>top_p=
          0.95,<br>eos_token_id= 49155,</p>

          <pre><code>)

          </code></pre>

          <p>So nothing etc. I am trying but still this error<br>i got the value error.<br>ValueError:
          The following model_kwargs are not used by the model: [''stop_sequence'']
          (note: typos in the generate arguments will also show up in this list)</p>

          '
        raw: "I am loading the same checkpoint\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder\"\
          )\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder\"\
          )\n\n\nNo etc changes\nDuring output i am using \noutput = model.generate(\n\
          input_ids,\ndo_sample=True,\nmin_length=min_length,\nmax_length=max_length,\n\
          temperature=temperature,\nearly_stopping=True,\nstop_sequence='<|end|>',\n\
          top_k=50,\ntop_p= 0.95,\neos_token_id= 49155,\n\n    )\n\nSo nothing etc.\
          \ I am trying but still this error\ni got the value error.\nValueError:\
          \ The following model_kwargs are not used by the model: ['stop_sequence']\
          \ (note: typos in the generate arguments will also show up in this list)"
        updatedAt: '2023-06-22T17:18:43.579Z'
      numEdits: 0
      reactions: []
    id: 64948273d9a2c408f16ff28c
    type: comment
  author: MukeshSharma
  content: "I am loading the same checkpoint\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder\"\
    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder\")\n\n\n\
    No etc changes\nDuring output i am using \noutput = model.generate(\ninput_ids,\n\
    do_sample=True,\nmin_length=min_length,\nmax_length=max_length,\ntemperature=temperature,\n\
    early_stopping=True,\nstop_sequence='<|end|>',\ntop_k=50,\ntop_p= 0.95,\neos_token_id=\
    \ 49155,\n\n    )\n\nSo nothing etc. I am trying but still this error\ni got the\
    \ value error.\nValueError: The following model_kwargs are not used by the model:\
    \ ['stop_sequence'] (note: typos in the generate arguments will also show up in\
    \ this list)"
  created_at: 2023-06-22 16:18:43+00:00
  edited: false
  hidden: false
  id: 64948273d9a2c408f16ff28c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-06-22T17:19:27.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6071879863739014
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;doraexp&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/doraexp\"\
          >@<span class=\"underline\">doraexp</span></a></span>\n\n\t</span></span>\
          \ plse review once </p>\n"
        raw: 'Hey @doraexp plse review once '
        updatedAt: '2023-06-22T17:19:27.100Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - doraexp
    id: 6494829fdf2cfa917fc1bebb
    type: comment
  author: MukeshSharma
  content: 'Hey @doraexp plse review once '
  created_at: 2023-06-22 16:19:27+00:00
  edited: false
  hidden: false
  id: 6494829fdf2cfa917fc1bebb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-06-26T05:13:00.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8799420595169067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;doraexp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/doraexp\">@<span class=\"\
          underline\">doraexp</span></a></span>\n\n\t</span></span> Any help on this\
          \ , I am still in use of it .<br>Thanks </p>\n"
        raw: "@doraexp Any help on this , I am still in use of it . \nThanks "
        updatedAt: '2023-06-26T05:13:00.734Z'
      numEdits: 0
      reactions: []
    id: 64991e5c0aeeb1f81ad015e1
    type: comment
  author: MukeshSharma
  content: "@doraexp Any help on this , I am still in use of it . \nThanks "
  created_at: 2023-06-26 04:13:00+00:00
  edited: false
  hidden: false
  id: 64991e5c0aeeb1f81ad015e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d8aeec180e4bcdc527aa54141a53fb8.svg
      fullname: Gurpreet Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: doraexp
      type: user
    createdAt: '2023-07-11T22:13:04.000Z'
    data:
      edited: false
      editors:
      - doraexp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.63209068775177
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d8aeec180e4bcdc527aa54141a53fb8.svg
          fullname: Gurpreet Singh
          isHf: false
          isPro: false
          name: doraexp
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;MukeshSharma&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MukeshSharma\"\
          >@<span class=\"underline\">MukeshSharma</span></a></span>\n\n\t</span></span>,\
          \ sorry I got little busy with some-other stuff and couldn't reply before.\
          \ Also, I am not sure why you are getting this error.</p>\n<p>However, I\
          \ am downloading the model on my local and then running it. Follow the below\
          \ step and see if they works for u</p>\n<p>Run this python pgrm:</p>\n<pre><code\
          \ class=\"language-from\">\ntokenizer = AutoToklsenizer.from_pretrained(\"\
          HuggingFaceH4/starchat-alpha\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          HuggingFaceH4/starchat-alpha\")\n\n#mention the directory where you want\
          \ to save the checkpoint\ntokenizer.save_pretrained(\"/home/ec2-user/starCoderCheckpointLocal\"\
          )\nmodel.save_pretrained(\"/home/ec2-user/starCoderCheckpointLocal\")\n\n\
          #these commands check if the model is working offline using the local directory\n\
          tokenizer = AutoTokenizer.from_pretrained(\"/home/ec2-user/starCoderCheckpointLocal\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"/home/ec2-user/starCoderCheckpointLocal\"\
          )\n</code></pre>\n<p>Now just run this:</p>\n<pre><code>from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\nimport torch\n#checkpoint\
          \ = \"HuggingFaceH4/starchat-alpha\"\ncheckpoint= \"/home/ec2-user/starCoderCheckpointLocal\"\
          \n\ndevice = \"cuda\" *# for GPU usage or \"cpu\" for CPU usage*\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint)\n\n# to save memory consider\
          \ using fp16 or bf16 by specifying torch_dtype=torch.float16 for example\n\
          model = AutoModelForCausalLM.from_pretrained(checkpoint,torch_dtype=torch.float16).to(device)\n\
          \ninputs = tokenizer.encode(\"Create a typescript function that calculates\
          \ factorial of a number.\", return_tensors=\"pt\").to(device)\noutputs =\
          \ model.generate(inputs,max_length=500)\nprint(tokenizer.decode(outputs[0]))\n\
          </code></pre>\n<p>I hope this helps :)</p>\n"
        raw: 'Hi @MukeshSharma, sorry I got little busy with some-other stuff and
          couldn''t reply before. Also, I am not sure why you are getting this error.


          However, I am downloading the model on my local and then running it. Follow
          the below step and see if they works for u



          Run this python pgrm:


          ```from transformers import AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoToklsenizer.from_pretrained("HuggingFaceH4/starchat-alpha")

          model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/starchat-alpha")


          #mention the directory where you want to save the checkpoint

          tokenizer.save_pretrained("/home/ec2-user/starCoderCheckpointLocal")

          model.save_pretrained("/home/ec2-user/starCoderCheckpointLocal")


          #these commands check if the model is working offline using the local directory

          tokenizer = AutoTokenizer.from_pretrained("/home/ec2-user/starCoderCheckpointLocal")

          model = AutoModelForCausalLM.from_pretrained("/home/ec2-user/starCoderCheckpointLocal")

          ```



          Now just run this:


          ```

          from transformers import AutoModelForCausalLM, AutoTokenizer


          import torch

          #checkpoint = "HuggingFaceH4/starchat-alpha"

          checkpoint= "/home/ec2-user/starCoderCheckpointLocal"


          device = "cuda" *# for GPU usage or "cpu" for CPU usage*


          tokenizer = AutoTokenizer.from_pretrained(checkpoint)


          # to save memory consider using fp16 or bf16 by specifying torch_dtype=torch.float16
          for example

          model = AutoModelForCausalLM.from_pretrained(checkpoint,torch_dtype=torch.float16).to(device)


          inputs = tokenizer.encode("Create a typescript function that calculates
          factorial of a number.", return_tensors="pt").to(device)

          outputs = model.generate(inputs,max_length=500)

          print(tokenizer.decode(outputs[0]))

          ```


          I hope this helps :)




          '
        updatedAt: '2023-07-11T22:13:04.443Z'
      numEdits: 0
      reactions: []
    id: 64add3f02a530cbdee9081d2
    type: comment
  author: doraexp
  content: 'Hi @MukeshSharma, sorry I got little busy with some-other stuff and couldn''t
    reply before. Also, I am not sure why you are getting this error.


    However, I am downloading the model on my local and then running it. Follow the
    below step and see if they works for u



    Run this python pgrm:


    ```from transformers import AutoTokenizer, AutoModelForCausalLM


    tokenizer = AutoToklsenizer.from_pretrained("HuggingFaceH4/starchat-alpha")

    model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/starchat-alpha")


    #mention the directory where you want to save the checkpoint

    tokenizer.save_pretrained("/home/ec2-user/starCoderCheckpointLocal")

    model.save_pretrained("/home/ec2-user/starCoderCheckpointLocal")


    #these commands check if the model is working offline using the local directory

    tokenizer = AutoTokenizer.from_pretrained("/home/ec2-user/starCoderCheckpointLocal")

    model = AutoModelForCausalLM.from_pretrained("/home/ec2-user/starCoderCheckpointLocal")

    ```



    Now just run this:


    ```

    from transformers import AutoModelForCausalLM, AutoTokenizer


    import torch

    #checkpoint = "HuggingFaceH4/starchat-alpha"

    checkpoint= "/home/ec2-user/starCoderCheckpointLocal"


    device = "cuda" *# for GPU usage or "cpu" for CPU usage*


    tokenizer = AutoTokenizer.from_pretrained(checkpoint)


    # to save memory consider using fp16 or bf16 by specifying torch_dtype=torch.float16
    for example

    model = AutoModelForCausalLM.from_pretrained(checkpoint,torch_dtype=torch.float16).to(device)


    inputs = tokenizer.encode("Create a typescript function that calculates factorial
    of a number.", return_tensors="pt").to(device)

    outputs = model.generate(inputs,max_length=500)

    print(tokenizer.decode(outputs[0]))

    ```


    I hope this helps :)




    '
  created_at: 2023-07-11 21:13:04+00:00
  edited: false
  hidden: false
  id: 64add3f02a530cbdee9081d2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 49
repo_id: bigcode/starcoder
repo_type: model
status: open
target_branch: null
title: How to stop the prediction once the model is generated a sufficient solution
  for the asked prompt ?
