!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cubby9059
conflicting_files: null
created_at: 2023-10-02 04:54:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/371cf30dd7ed9d3d5683a3bea3b68feb.svg
      fullname: A.B.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cubby9059
      type: user
    createdAt: '2023-10-02T05:54:40.000Z'
    data:
      edited: false
      editors:
      - Cubby9059
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.956315279006958
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/371cf30dd7ed9d3d5683a3bea3b68feb.svg
          fullname: A.B.
          isHf: false
          isPro: false
          name: Cubby9059
          type: user
        html: '<p>Hello,<br>      I am trying to deploy starcoder as an internal coding
          assistant for a team of 100 people. However, the model is taking too long
          for predictions especially when parallel requests are being made. I am using
          an Nvidia A100 40GB. Any suggestions on how to make faster inference?<br>Thank
          you.</p>

          '
        raw: "Hello,\r\n      I am trying to deploy starcoder as an internal coding\
          \ assistant for a team of 100 people. However, the model is taking too long\
          \ for predictions especially when parallel requests are being made. I am\
          \ using an Nvidia A100 40GB. Any suggestions on how to make faster inference?\r\
          \nThank you."
        updatedAt: '2023-10-02T05:54:40.906Z'
      numEdits: 0
      reactions: []
    id: 651a5b2020b18e99b4abe256
    type: comment
  author: Cubby9059
  content: "Hello,\r\n      I am trying to deploy starcoder as an internal coding\
    \ assistant for a team of 100 people. However, the model is taking too long for\
    \ predictions especially when parallel requests are being made. I am using an\
    \ Nvidia A100 40GB. Any suggestions on how to make faster inference?\r\nThank\
    \ you."
  created_at: 2023-10-02 04:54:40+00:00
  edited: false
  hidden: false
  id: 651a5b2020b18e99b4abe256
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-10-03T12:54:41.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8063027858734131
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>you can try deploying the model with <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">Text
          Generation Inference</a> library that we use for the inference endpoints</p>

          '
        raw: you can try deploying the model with [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
          library that we use for the inference endpoints
        updatedAt: '2023-10-03T12:54:41.848Z'
      numEdits: 0
      reactions: []
    id: 651c0f11164539754da8216c
    type: comment
  author: loubnabnl
  content: you can try deploying the model with [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
    library that we use for the inference endpoints
  created_at: 2023-10-03 11:54:41+00:00
  edited: false
  hidden: false
  id: 651c0f11164539754da8216c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 93
repo_id: bigcode/starcoder
repo_type: model
status: open
target_branch: null
title: How to parallelize starcoder inference?
