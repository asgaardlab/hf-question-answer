!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JoaoLages
conflicting_files: null
created_at: 2023-05-16 12:35:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00a48762ed17bd95905ee6b56882072b.svg
      fullname: "Jo\xE3o Lages"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoaoLages
      type: user
    createdAt: '2023-05-16T13:35:02.000Z'
    data:
      edited: false
      editors:
      - JoaoLages
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00a48762ed17bd95905ee6b56882072b.svg
          fullname: "Jo\xE3o Lages"
          isHf: false
          isPro: false
          name: JoaoLages
          type: user
        html: '<p>SantaCoder used a special encoding/decoding code for FIM, does StarCoder
          also need this?<br><a href="https://huggingface.co/spaces/bigcode/santacoder-demo/blob/main/app.py#L24">https://huggingface.co/spaces/bigcode/santacoder-demo/blob/main/app.py#L24</a></p>

          '
        raw: "SantaCoder used a special encoding/decoding code for FIM, does StarCoder\
          \ also need this?\r\nhttps://huggingface.co/spaces/bigcode/santacoder-demo/blob/main/app.py#L24"
        updatedAt: '2023-05-16T13:35:02.413Z'
      numEdits: 0
      reactions: []
    id: 64638686efb4e8550485f0d3
    type: comment
  author: JoaoLages
  content: "SantaCoder used a special encoding/decoding code for FIM, does StarCoder\
    \ also need this?\r\nhttps://huggingface.co/spaces/bigcode/santacoder-demo/blob/main/app.py#L24"
  created_at: 2023-05-16 12:35:02+00:00
  edited: false
  hidden: false
  id: 64638686efb4e8550485f0d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-05-17T12:34:44.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>Yes, it''s actually documented in The <a href="https://huggingface.co/bigcode/starcoder#fill-in-the-middle">README</a></p>

          '
        raw: Yes, it's actually documented in The [README](https://huggingface.co/bigcode/starcoder#fill-in-the-middle)
        updatedAt: '2023-05-17T12:34:44.259Z'
      numEdits: 0
      reactions: []
    id: 6464c9e44855e06b9504dd46
    type: comment
  author: loubnabnl
  content: Yes, it's actually documented in The [README](https://huggingface.co/bigcode/starcoder#fill-in-the-middle)
  created_at: 2023-05-17 11:34:44+00:00
  edited: false
  hidden: false
  id: 6464c9e44855e06b9504dd46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00a48762ed17bd95905ee6b56882072b.svg
      fullname: "Jo\xE3o Lages"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoaoLages
      type: user
    createdAt: '2023-05-17T13:51:51.000Z'
    data:
      edited: false
      editors:
      - JoaoLages
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00a48762ed17bd95905ee6b56882072b.svg
          fullname: "Jo\xE3o Lages"
          isHf: false
          isPro: false
          name: JoaoLages
          type: user
        html: '<p>No, that''s not what I meant! SantaCoder did a lot of custom preprocessing:</p>

          <ul>

          <li>In SantaCoder we had to initialize the tokenizer with padding_side="left"
          - this is no longer needed in StarCoder, right?</li>

          <li>We also had to tokenize the inputs with return_token_type_ids=False
          -  this is no longer needed in StarCoder, or is it?</li>

          <li>We also had to include a pad_token_id=tokenizer.pad_token_id in model.generate
          - is this needed?</li>

          </ul>

          '
        raw: 'No, that''s not what I meant! SantaCoder did a lot of custom preprocessing:

          - In SantaCoder we had to initialize the tokenizer with padding_side="left"
          - this is no longer needed in StarCoder, right?

          - We also had to tokenize the inputs with return_token_type_ids=False -  this
          is no longer needed in StarCoder, or is it?

          - We also had to include a pad_token_id=tokenizer.pad_token_id in model.generate
          - is this needed?'
        updatedAt: '2023-05-17T13:51:51.621Z'
      numEdits: 0
      reactions: []
    id: 6464dbf786e668ad22e0fed1
    type: comment
  author: JoaoLages
  content: 'No, that''s not what I meant! SantaCoder did a lot of custom preprocessing:

    - In SantaCoder we had to initialize the tokenizer with padding_side="left" -
    this is no longer needed in StarCoder, right?

    - We also had to tokenize the inputs with return_token_type_ids=False -  this
    is no longer needed in StarCoder, or is it?

    - We also had to include a pad_token_id=tokenizer.pad_token_id in model.generate
    - is this needed?'
  created_at: 2023-05-17 12:51:51+00:00
  edited: false
  hidden: false
  id: 6464dbf786e668ad22e0fed1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1cd8aeb0688b25edbeb1a7c1faa8d5f.svg
      fullname: Fernando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nandovallec
      type: user
    createdAt: '2023-05-23T12:39:44.000Z'
    data:
      edited: false
      editors:
      - nandovallec
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1cd8aeb0688b25edbeb1a7c1faa8d5f.svg
          fullname: Fernando
          isHf: false
          isPro: false
          name: nandovallec
          type: user
        html: '<p>I would also be very interested in the configuration used.<br>For
          SantaCoder, the demo showed all the hyperparameters chosen for the tokenizer
          and the generation. On the other hand, StarCoder uses the endpoint for which
          I cannot replicate the results locally.</p>

          '
        raw: 'I would also be very interested in the configuration used.

          For SantaCoder, the demo showed all the hyperparameters chosen for the tokenizer
          and the generation. On the other hand, StarCoder uses the endpoint for which
          I cannot replicate the results locally.'
        updatedAt: '2023-05-23T12:39:44.151Z'
      numEdits: 0
      reactions: []
    id: 646cb41010f66cc3c7666be7
    type: comment
  author: nandovallec
  content: 'I would also be very interested in the configuration used.

    For SantaCoder, the demo showed all the hyperparameters chosen for the tokenizer
    and the generation. On the other hand, StarCoder uses the endpoint for which I
    cannot replicate the results locally.'
  created_at: 2023-05-23 11:39:44+00:00
  edited: false
  hidden: false
  id: 646cb41010f66cc3c7666be7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-05-25T17:48:45.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nandovallec&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nandovallec\"\
          >@<span class=\"underline\">nandovallec</span></a></span>\n\n\t</span></span>\
          \ you can run FIM using the following code, nothing special is needed except\
          \ for specifying FIM tokens:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"bigcode/starcoder\"</span>, truncation_side=<span\
          \ class=\"hljs-string\">\"left\"</span>)\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"bigcode/starcoder\"</span>, torch_dtype=torch.bfloat16).cuda()\n\
          \ninput_text = <span class=\"hljs-string\">\"&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;\
          \    else:\\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\"\
          </span>\ninputs = tokenizer(input_text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>).to(<span class=\"hljs-string\">\"cuda\"</span>)\noutputs\
          \ = model.generate(**inputs, max_new_tokens=<span class=\"hljs-number\"\
          >25</span>)\ngeneration = [tokenizer_fim.decode(tensor, skip_special_tokens=<span\
          \ class=\"hljs-literal\">False</span>) <span class=\"hljs-keyword\">for</span>\
          \ tensor <span class=\"hljs-keyword\">in</span> outputs]\n<span class=\"\
          hljs-built_in\">print</span>(generation[<span class=\"hljs-number\">0</span>])\n\
          </code></pre>\n<pre><code>&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;\
          \    else:\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\n \
          \   if n &lt; 2:\n        return n\n&lt;|endoftext|&gt;\n</code></pre>\n"
        raw: "@nandovallec you can run FIM using the following code, nothing special\
          \ is needed except for specifying FIM tokens:\n```python\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          bigcode/starcoder\", truncation_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          bigcode/starcoder\", torch_dtype=torch.bfloat16).cuda()\n\ninput_text =\
          \ \"<fim_prefix>def fib(n):<fim_suffix>    else:\\n        return fib(n\
          \ - 2) + fib(n - 1)<fim_middle>\"\ninputs = tokenizer(input_text, return_tensors=\"\
          pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=25)\n\
          generation = [tokenizer_fim.decode(tensor, skip_special_tokens=False) for\
          \ tensor in outputs]\nprint(generation[0])\n```\n```\n<fim_prefix>def fib(n):<fim_suffix>\
          \    else:\n        return fib(n - 2) + fib(n - 1)<fim_middle>\n    if n\
          \ < 2:\n        return n\n<|endoftext|>\n```"
        updatedAt: '2023-05-25T17:48:45.390Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - JoaoLages
        - nandovallec
        - pathfinder996
        - tbetton
        - haotang
        - Thinkcru
    id: 646f9f7dd1f1b73079ea8da0
    type: comment
  author: loubnabnl
  content: "@nandovallec you can run FIM using the following code, nothing special\
    \ is needed except for specifying FIM tokens:\n```python\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    bigcode/starcoder\", truncation_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    bigcode/starcoder\", torch_dtype=torch.bfloat16).cuda()\n\ninput_text = \"<fim_prefix>def\
    \ fib(n):<fim_suffix>    else:\\n        return fib(n - 2) + fib(n - 1)<fim_middle>\"\
    \ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs\
    \ = model.generate(**inputs, max_new_tokens=25)\ngeneration = [tokenizer_fim.decode(tensor,\
    \ skip_special_tokens=False) for tensor in outputs]\nprint(generation[0])\n```\n\
    ```\n<fim_prefix>def fib(n):<fim_suffix>    else:\n        return fib(n - 2) +\
    \ fib(n - 1)<fim_middle>\n    if n < 2:\n        return n\n<|endoftext|>\n```"
  created_at: 2023-05-25 16:48:45+00:00
  edited: false
  hidden: false
  id: 646f9f7dd1f1b73079ea8da0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-06T22:05:14.000Z'
    data:
      status: closed
    id: 647fad9aebbd5b7972a20ec1
    type: status-change
  author: loubnabnl
  created_at: 2023-06-06 21:05:14+00:00
  id: 647fad9aebbd5b7972a20ec1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 31
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: 'Missing documentation for FIM? '
