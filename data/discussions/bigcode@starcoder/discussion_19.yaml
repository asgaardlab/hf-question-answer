!!python/object:huggingface_hub.community.DiscussionWithDetails
author: intelligencegear
conflicting_files: null
created_at: 2023-05-09 09:48:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
      fullname: Sun Kai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: intelligencegear
      type: user
    createdAt: '2023-05-09T10:48:30.000Z'
    data:
      edited: false
      editors:
      - intelligencegear
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
          fullname: Sun Kai
          isHf: false
          isPro: false
          name: intelligencegear
          type: user
        html: '<p>Thank you for creating the StarCoder model. However, it is estimated
          that only GPUs like the A100 will be able to perform inference with this
          model. May I ask if there are plans to provide 8-bit or 4-bit quantized
          versions?</p>

          '
        raw: "Thank you for creating the StarCoder model. However, it is estimated\
          \ that only GPUs like the A100 will be able to perform inference with this\
          \ model. May I ask if there are plans to provide 8-bit or 4-bit quantized\
          \ versions?\r\n\r\n"
        updatedAt: '2023-05-09T10:48:30.331Z'
      numEdits: 0
      reactions: []
    id: 645a24fee9512e4740418dc8
    type: comment
  author: intelligencegear
  content: "Thank you for creating the StarCoder model. However, it is estimated that\
    \ only GPUs like the A100 will be able to perform inference with this model. May\
    \ I ask if there are plans to provide 8-bit or 4-bit quantized versions?\r\n\r\
    \n"
  created_at: 2023-05-09 09:48:30+00:00
  edited: false
  hidden: false
  id: 645a24fee9512e4740418dc8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-05-09T13:40:06.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;intelligencegear&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/intelligencegear\"\
          >@<span class=\"underline\">intelligencegear</span></a></span>\n\n\t</span></span>,<br>You\
          \ can already use the 8bit model out of the box, by installing <code>bitsandbytes</code>\
          \ and <code>accelerate</code>. Just run the following:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM\n\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\">\"\
          bigcode/starcoder\"</span>, load_in_8bit=<span class=\"hljs-literal\">True</span>,\
          \ device_map=<span class=\"hljs-string\">\"auto\"</span>)\n<span class=\"\
          hljs-meta\">... </span>\n</code></pre>\n<p>You can have a deeper look of\
          \ what it is using under the hood <a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\"\
          >here</a></p>\n"
        raw: "Hi @intelligencegear,\nYou can already use the 8bit model out of the\
          \ box, by installing `bitsandbytes` and `accelerate`. Just run the following:\n\
          ```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          bigcode/starcoder\", load_in_8bit=True, device_map=\"auto\")\n... \n```\n\
          You can have a deeper look of what it is using under the hood [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
        updatedAt: '2023-05-09T13:40:06.672Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - lozhnikov
        - qftie
    id: 645a4d363760da5b7fa5767a
    type: comment
  author: ybelkada
  content: "Hi @intelligencegear,\nYou can already use the 8bit model out of the box,\
    \ by installing `bitsandbytes` and `accelerate`. Just run the following:\n```python\n\
    from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    bigcode/starcoder\", load_in_8bit=True, device_map=\"auto\")\n... \n```\nYou can\
    \ have a deeper look of what it is using under the hood [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
  created_at: 2023-05-09 12:40:06+00:00
  edited: false
  hidden: false
  id: 645a4d363760da5b7fa5767a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2c4b8943676230408670850a32d634d.svg
      fullname: it
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syntaxing
      type: user
    createdAt: '2023-05-10T01:06:15.000Z'
    data:
      edited: false
      editors:
      - syntaxing
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2c4b8943676230408670850a32d634d.svg
          fullname: it
          isHf: false
          isPro: false
          name: syntaxing
          type: user
        html: '<p>load_in_8bit != GPTQ quantized. The performance is vastly difference
          with GPTQ being superior.</p>

          '
        raw: load_in_8bit != GPTQ quantized. The performance is vastly difference
          with GPTQ being superior.
        updatedAt: '2023-05-10T01:06:15.277Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - intelligencegear
        - qftie
    id: 645aee07dbf60d37336613b9
    type: comment
  author: syntaxing
  content: load_in_8bit != GPTQ quantized. The performance is vastly difference with
    GPTQ being superior.
  created_at: 2023-05-10 00:06:15+00:00
  edited: false
  hidden: false
  id: 645aee07dbf60d37336613b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
      fullname: Arjun Guha
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: arjunguha
      type: user
    createdAt: '2023-05-10T01:24:52.000Z'
    data:
      edited: false
      editors:
      - arjunguha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
          fullname: Arjun Guha
          isHf: false
          isPro: false
          name: arjunguha
          type: user
        html: '<p>These are still a work in progress, but you can get early versions
          here: <a href="https://huggingface.co/mayank31398">https://huggingface.co/mayank31398</a></p>

          <p>I believe running time is not as good as we''d like, but quality of generated
          code seems to be good. A full evaluation is pending.</p>

          '
        raw: 'These are still a work in progress, but you can get early versions here:
          https://huggingface.co/mayank31398


          I believe running time is not as good as we''d like, but quality of generated
          code seems to be good. A full evaluation is pending.'
        updatedAt: '2023-05-10T01:24:52.916Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - syntaxing
        - intelligencegear
        - EvgeniyZh
        - merlinarer
    id: 645af264dbf60d37336658ec
    type: comment
  author: arjunguha
  content: 'These are still a work in progress, but you can get early versions here:
    https://huggingface.co/mayank31398


    I believe running time is not as good as we''d like, but quality of generated
    code seems to be good. A full evaluation is pending.'
  created_at: 2023-05-10 00:24:52+00:00
  edited: false
  hidden: false
  id: 645af264dbf60d37336658ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
      fullname: Sun Kai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: intelligencegear
      type: user
    createdAt: '2023-05-10T05:55:07.000Z'
    data:
      edited: false
      editors:
      - intelligencegear
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
          fullname: Sun Kai
          isHf: false
          isPro: false
          name: intelligencegear
          type: user
        html: '<blockquote>

          <p>These are still a work in progress, but you can get early versions here:
          <a href="https://huggingface.co/mayank31398">https://huggingface.co/mayank31398</a></p>

          <p>I believe running time is not as good as we''d like, but quality of generated
          code seems to be good. A full evaluation is pending.</p>

          </blockquote>

          <p>The job is awesome, and I have tested it myself. However, there may not
          be a directly usable checkpoint available due to licensing reasons. Instead,
          it needs to be converted on the machine memory. In other words, a machine
          with possibly 64GB of memory is required, and the hardware requirement is
          relatively high.</p>

          '
        raw: "> These are still a work in progress, but you can get early versions\
          \ here: https://huggingface.co/mayank31398\n> \n> I believe running time\
          \ is not as good as we'd like, but quality of generated code seems to be\
          \ good. A full evaluation is pending.\n\nThe job is awesome, and I have\
          \ tested it myself. However, there may not be a directly usable checkpoint\
          \ available due to licensing reasons. Instead, it needs to be converted\
          \ on the machine memory. In other words, a machine with possibly 64GB of\
          \ memory is required, and the hardware requirement is relatively high."
        updatedAt: '2023-05-10T05:55:07.717Z'
      numEdits: 0
      reactions: []
    id: 645b31bb340133cc1c2237e1
    type: comment
  author: intelligencegear
  content: "> These are still a work in progress, but you can get early versions here:\
    \ https://huggingface.co/mayank31398\n> \n> I believe running time is not as good\
    \ as we'd like, but quality of generated code seems to be good. A full evaluation\
    \ is pending.\n\nThe job is awesome, and I have tested it myself. However, there\
    \ may not be a directly usable checkpoint available due to licensing reasons.\
    \ Instead, it needs to be converted on the machine memory. In other words, a machine\
    \ with possibly 64GB of memory is required, and the hardware requirement is relatively\
    \ high."
  created_at: 2023-05-10 04:55:07+00:00
  edited: false
  hidden: false
  id: 645b31bb340133cc1c2237e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
      fullname: Sun Kai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: intelligencegear
      type: user
    createdAt: '2023-05-10T05:56:04.000Z'
    data:
      edited: true
      editors:
      - intelligencegear
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
          fullname: Sun Kai
          isHf: false
          isPro: false
          name: intelligencegear
          type: user
        html: '<blockquote>

          <p>load_in_8bit != GPTQ quantized. The performance is vastly difference
          with GPTQ being superior.</p>

          </blockquote>

          <p>Yes, that''s exactly what I wanted to say.</p>

          '
        raw: '> load_in_8bit != GPTQ quantized. The performance is vastly difference
          with GPTQ being superior.


          Yes, that''s exactly what I wanted to say.'
        updatedAt: '2023-05-10T05:57:29.401Z'
      numEdits: 1
      reactions: []
      relatedEventId: 645b31f4340133cc1c223ab9
    id: 645b31f4340133cc1c223ab8
    type: comment
  author: intelligencegear
  content: '> load_in_8bit != GPTQ quantized. The performance is vastly difference
    with GPTQ being superior.


    Yes, that''s exactly what I wanted to say.'
  created_at: 2023-05-10 04:56:04+00:00
  edited: true
  hidden: false
  id: 645b31f4340133cc1c223ab8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
      fullname: Sun Kai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: intelligencegear
      type: user
    createdAt: '2023-05-10T05:56:04.000Z'
    data:
      status: closed
    id: 645b31f4340133cc1c223ab9
    type: status-change
  author: intelligencegear
  created_at: 2023-05-10 04:56:04+00:00
  id: 645b31f4340133cc1c223ab9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e9dfa475d259846eafe5035489e1c00a.svg
      fullname: Sun Kai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: intelligencegear
      type: user
    createdAt: '2023-05-10T05:56:26.000Z'
    data:
      status: open
    id: 645b320a06c24937a1c7caa9
    type: status-change
  author: intelligencegear
  created_at: 2023-05-10 04:56:26+00:00
  id: 645b320a06c24937a1c7caa9
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-10T07:56:10.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>yes, LLM.int8() works quite differently to GPTQ.<br>LLM.int8() which
          is achieved by <code>load_in_8bit=True</code> does quantization at load
          time.<br>However, GPTQ uses Optimal Brain Quantization for GPT-like models
          and this requires a samples for quantization.</p>

          '
        raw: 'yes, LLM.int8() works quite differently to GPTQ.

          LLM.int8() which is achieved by `load_in_8bit=True` does quantization at
          load time.

          However, GPTQ uses Optimal Brain Quantization for GPT-like models and this
          requires a samples for quantization.'
        updatedAt: '2023-05-10T07:56:10.503Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - intelligencegear
    id: 645b4e1aba5b7032de7fbca0
    type: comment
  author: mayank31398
  content: 'yes, LLM.int8() works quite differently to GPTQ.

    LLM.int8() which is achieved by `load_in_8bit=True` does quantization at load
    time.

    However, GPTQ uses Optimal Brain Quantization for GPT-like models and this requires
    a samples for quantization.'
  created_at: 2023-05-10 06:56:10+00:00
  edited: false
  hidden: false
  id: 645b4e1aba5b7032de7fbca0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2c4b8943676230408670850a32d634d.svg
      fullname: it
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syntaxing
      type: user
    createdAt: '2023-05-10T18:06:52.000Z'
    data:
      edited: false
      editors:
      - syntaxing
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2c4b8943676230408670850a32d634d.svg
          fullname: it
          isHf: false
          isPro: false
          name: syntaxing
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mayank31398&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mayank31398\"\
          >@<span class=\"underline\">mayank31398</span></a></span>\n\n\t</span></span>\
          \ Do you think there will be a front end or interactive mode that works\
          \ with your repo? Santa coder is great but without a chat like interface\
          \ that can maintain context, Starcoder pretty much becomes unusable except\
          \ for very specific situations. Thanks!</p>\n"
        raw: '@mayank31398 Do you think there will be a front end or interactive mode
          that works with your repo? Santa coder is great but without a chat like
          interface that can maintain context, Starcoder pretty much becomes unusable
          except for very specific situations. Thanks!'
        updatedAt: '2023-05-10T18:06:52.287Z'
      numEdits: 0
      reactions: []
    id: 645bdd3c8bbb8592d919de8f
    type: comment
  author: syntaxing
  content: '@mayank31398 Do you think there will be a front end or interactive mode
    that works with your repo? Santa coder is great but without a chat like interface
    that can maintain context, Starcoder pretty much becomes unusable except for very
    specific situations. Thanks!'
  created_at: 2023-05-10 17:06:52+00:00
  edited: false
  hidden: false
  id: 645bdd3c8bbb8592d919de8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-11T11:33:38.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: "<p>hey <span data-props=\"{&quot;user&quot;:&quot;syntaxing&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/syntaxing\"\
          >@<span class=\"underline\">syntaxing</span></a></span>\n\n\t</span></span><br>there\
          \ is already a model called starchat. Demo here: <a href=\"https://huggingface.co/spaces/HuggingFaceH4/starchat-playground\"\
          >https://huggingface.co/spaces/HuggingFaceH4/starchat-playground</a></p>\n"
        raw: "hey @syntaxing \nthere is already a model called starchat. Demo here:\
          \ https://huggingface.co/spaces/HuggingFaceH4/starchat-playground"
        updatedAt: '2023-05-11T11:33:38.057Z'
      numEdits: 0
      reactions: []
    id: 645cd292f1e3b219cb08dfb2
    type: comment
  author: mayank31398
  content: "hey @syntaxing \nthere is already a model called starchat. Demo here:\
    \ https://huggingface.co/spaces/HuggingFaceH4/starchat-playground"
  created_at: 2023-05-11 10:33:38+00:00
  edited: false
  hidden: false
  id: 645cd292f1e3b219cb08dfb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653032312063-noauth.jpeg?w=200&h=200&f=face
      fullname: Joao Pinto Correia de Moura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoaoMoura
      type: user
    createdAt: '2023-05-18T16:30:26.000Z'
    data:
      edited: true
      editors:
      - JoaoMoura
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653032312063-noauth.jpeg?w=200&h=200&f=face
          fullname: Joao Pinto Correia de Moura
          isHf: false
          isPro: false
          name: JoaoMoura
          type: user
        html: "<blockquote>\n<p>yes, LLM.int8() works quite differently to GPTQ.<br>LLM.int8()\
          \ which is achieved by <code>load_in_8bit=True</code> does quantization\
          \ at load time.<br>However, GPTQ uses Optimal Brain Quantization for GPT-like\
          \ models and this requires a samples for quantization.</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;mayank31398&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mayank31398\">@<span\
          \ class=\"underline\">mayank31398</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;syntaxing&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/syntaxing\">@<span class=\"\
          underline\">syntaxing</span></a></span>\n\n\t</span></span>  it is indeed\
          \ a different process, but wasn't it shown that <code>LLM.int8()</code>\
          \ does not result in statistically significant performance degradation in\
          \ <a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\">this\
          \ blog post</a>? In that case, what's the benefit of going with a process\
          \ that requires data samples and GPU hours (talking about the 8bit case,\
          \ not the 4bit case)? </p>\n<p>Mainly asking to validate if I'm leaving\
          \ performance on the table by using bitsandbytes.</p>\n"
        raw: "> yes, LLM.int8() works quite differently to GPTQ.\n> LLM.int8() which\
          \ is achieved by `load_in_8bit=True` does quantization at load time.\n>\
          \ However, GPTQ uses Optimal Brain Quantization for GPT-like models and\
          \ this requires a samples for quantization.\n\n@mayank31398 @syntaxing \
          \ it is indeed a different process, but wasn't it shown that `LLM.int8()`\
          \ does not result in statistically significant performance degradation in\
          \ [this blog post](https://huggingface.co/blog/hf-bitsandbytes-integration)?\
          \ In that case, what's the benefit of going with a process that requires\
          \ data samples and GPU hours (talking about the 8bit case, not the 4bit\
          \ case)? \n\nMainly asking to validate if I'm leaving performance on the\
          \ table by using bitsandbytes."
        updatedAt: '2023-05-18T16:32:00.520Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Brendan
    id: 646652a23b99ed9970fc924b
    type: comment
  author: JoaoMoura
  content: "> yes, LLM.int8() works quite differently to GPTQ.\n> LLM.int8() which\
    \ is achieved by `load_in_8bit=True` does quantization at load time.\n> However,\
    \ GPTQ uses Optimal Brain Quantization for GPT-like models and this requires a\
    \ samples for quantization.\n\n@mayank31398 @syntaxing  it is indeed a different\
    \ process, but wasn't it shown that `LLM.int8()` does not result in statistically\
    \ significant performance degradation in [this blog post](https://huggingface.co/blog/hf-bitsandbytes-integration)?\
    \ In that case, what's the benefit of going with a process that requires data\
    \ samples and GPU hours (talking about the 8bit case, not the 4bit case)? \n\n\
    Mainly asking to validate if I'm leaving performance on the table by using bitsandbytes."
  created_at: 2023-05-18 15:30:26+00:00
  edited: true
  hidden: false
  id: 646652a23b99ed9970fc924b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-10-14T20:14:41.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9672397375106812
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;JacopoBandoni&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JacopoBandoni\"\
          >@<span class=\"underline\">JacopoBandoni</span></a></span>\n\n\t</span></span>\
          \ sorry for the late reply.<br>GPTQ and LLM.int8() are completely different\
          \ quantization algorithms.<br>Please refer to their papers for the same.</p>\n"
        raw: '@JacopoBandoni sorry for the late reply.

          GPTQ and LLM.int8() are completely different quantization algorithms.

          Please refer to their papers for the same.'
        updatedAt: '2023-10-14T20:14:41.464Z'
      numEdits: 0
      reactions: []
    id: 652af6b1ff2202020e9ada7c
    type: comment
  author: mayank31398
  content: '@JacopoBandoni sorry for the late reply.

    GPTQ and LLM.int8() are completely different quantization algorithms.

    Please refer to their papers for the same.'
  created_at: 2023-10-14 19:14:41+00:00
  edited: false
  hidden: false
  id: 652af6b1ff2202020e9ada7c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: bigcode/starcoder
repo_type: model
status: open
target_branch: null
title: May I ask if there are plans to provide 8-bit or 4-bit quantized versions?
