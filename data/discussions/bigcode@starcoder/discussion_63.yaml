!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rachelshalom
conflicting_files: null
created_at: 2023-07-05 09:32:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
      fullname: Rachel Shalom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rachelshalom
      type: user
    createdAt: '2023-07-05T10:32:05.000Z'
    data:
      edited: true
      editors:
      - rachelshalom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9308373332023621
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
          fullname: Rachel Shalom
          isHf: false
          isPro: false
          name: rachelshalom
          type: user
        html: '<p>Hey I tool a loot at the startcoder finetuning code for instructions.
          I would like to finetune on a private code for autocompletion. right now
          I have private repos and I think that the autocompletion task is the only
          thing I can do with that. I am having a bit of a hard time understanding
          what do i need to modifiy in the finetune code of startcoder. anyone has
          tried to modify the finetuning code for autocompletion task?<br>that''s
          the code I refer to: <a rel="nofollow" href="https://github.com/bigcode-project/starcoder/blob/main/finetune/finetune.py">https://github.com/bigcode-project/starcoder/blob/main/finetune/finetune.py</a></p>

          '
        raw: 'Hey I tool a loot at the startcoder finetuning code for instructions.
          I would like to finetune on a private code for autocompletion. right now
          I have private repos and I think that the autocompletion task is the only
          thing I can do with that. I am having a bit of a hard time understanding
          what do i need to modifiy in the finetune code of startcoder. anyone has
          tried to modify the finetuning code for autocompletion task?

          that''s the code I refer to: https://github.com/bigcode-project/starcoder/blob/main/finetune/finetune.py'
        updatedAt: '2023-07-05T10:33:48.807Z'
      numEdits: 1
      reactions: []
    id: 64a546a527b4b620fa62642b
    type: comment
  author: rachelshalom
  content: 'Hey I tool a loot at the startcoder finetuning code for instructions.
    I would like to finetune on a private code for autocompletion. right now I have
    private repos and I think that the autocompletion task is the only thing I can
    do with that. I am having a bit of a hard time understanding what do i need to
    modifiy in the finetune code of startcoder. anyone has tried to modify the finetuning
    code for autocompletion task?

    that''s the code I refer to: https://github.com/bigcode-project/starcoder/blob/main/finetune/finetune.py'
  created_at: 2023-07-05 09:32:05+00:00
  edited: true
  hidden: false
  id: 64a546a527b4b620fa62642b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-07-05T12:02:03.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.611822783946991
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>Hi, you just need to change the input text, and use the content
          of your code files as is instead of the instruction format <a rel="nofollow"
          href="https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L124">here</a>.<br>You
          <a rel="nofollow" href="https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L172">buffer</a>
          should get</p>

          <pre><code class="language-python">buffer.append(<span class="hljs-built_in">next</span>(iterator)[<span
          class="hljs-string">"content"</span>])

          </code></pre>

          <p>If <code>"content"</code> is the name of the column that has the code
          you want to train on in your dataset.</p>

          '
        raw: 'Hi, you just need to change the input text, and use the content of your
          code files as is instead of the instruction format [here](https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L124).

          You [buffer](https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L172)
          should get

          ```python

          buffer.append(next(iterator)["content"])

          ```

          If `"content"` is the name of the column that has the code you want to train
          on in your dataset.'
        updatedAt: '2023-07-05T12:02:03.916Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - rachelshalom
        - Thinkcru
    id: 64a55bbb8d6a9ca65a5fdd15
    type: comment
  author: loubnabnl
  content: 'Hi, you just need to change the input text, and use the content of your
    code files as is instead of the instruction format [here](https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L124).

    You [buffer](https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L172)
    should get

    ```python

    buffer.append(next(iterator)["content"])

    ```

    If `"content"` is the name of the column that has the code you want to train on
    in your dataset.'
  created_at: 2023-07-05 11:02:03+00:00
  edited: false
  hidden: false
  id: 64a55bbb8d6a9ca65a5fdd15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
      fullname: Rachel Shalom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rachelshalom
      type: user
    createdAt: '2023-07-05T14:53:01.000Z'
    data:
      edited: false
      editors:
      - rachelshalom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9925299286842346
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
          fullname: Rachel Shalom
          isHf: false
          isPro: false
          name: rachelshalom
          type: user
        html: '<p>Thank you, I will try that. I think I was confused because of the
          fact that instruction tuning is supervised and autocomplete is not. So I
          thought that the training would be done differently, but as I understand
          for both cases causal training is done. ( but instruction-responses data
          is concatenated and that is how it is fed to the model)</p>

          '
        raw: Thank you, I will try that. I think I was confused because of the fact
          that instruction tuning is supervised and autocomplete is not. So I thought
          that the training would be done differently, but as I understand for both
          cases causal training is done. ( but instruction-responses data is concatenated
          and that is how it is fed to the model)
        updatedAt: '2023-07-05T14:53:01.811Z'
      numEdits: 0
      reactions: []
    id: 64a583cd5e1c3685b9aa6d05
    type: comment
  author: rachelshalom
  content: Thank you, I will try that. I think I was confused because of the fact
    that instruction tuning is supervised and autocomplete is not. So I thought that
    the training would be done differently, but as I understand for both cases causal
    training is done. ( but instruction-responses data is concatenated and that is
    how it is fed to the model)
  created_at: 2023-07-05 13:53:01+00:00
  edited: false
  hidden: false
  id: 64a583cd5e1c3685b9aa6d05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/361b58abb6b14866c4345324e1edda9e.svg
      fullname: Rob Leidle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spew
      type: user
    createdAt: '2023-07-05T16:13:28.000Z'
    data:
      edited: false
      editors:
      - spew
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9048366546630859
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/361b58abb6b14866c4345324e1edda9e.svg
          fullname: Rob Leidle
          isHf: false
          isPro: false
          name: spew
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;loubnabnl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/loubnabnl\"\
          >@<span class=\"underline\">loubnabnl</span></a></span>\n\n\t</span></span>\
          \ so you're saying to just feed the code straight into the model, without\
          \ matching the input / output format. Will this break FIM or does the model\
          \ somehow know how to support FIM for code files it has been trained on?</p>\n"
        raw: Hi @loubnabnl so you're saying to just feed the code straight into the
          model, without matching the input / output format. Will this break FIM or
          does the model somehow know how to support FIM for code files it has been
          trained on?
        updatedAt: '2023-07-05T16:13:28.051Z'
      numEdits: 0
      reactions: []
    id: 64a596a8c9fe05bfd977889a
    type: comment
  author: spew
  content: Hi @loubnabnl so you're saying to just feed the code straight into the
    model, without matching the input / output format. Will this break FIM or does
    the model somehow know how to support FIM for code files it has been trained on?
  created_at: 2023-07-05 15:13:28+00:00
  edited: false
  hidden: false
  id: 64a596a8c9fe05bfd977889a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-07-05T16:57:15.000Z'
    data:
      edited: true
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9084298014640808
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>In the finetuning code you pointed , while the data preparation
          is similar to the pre-training procedure, it doesn''t use FIM. The model
          might still be able to know how to perform FIM after that fine-tuning. However,
          if you want to preserve the same infilling capabilities you might want to
          include it in the training, you can check this <a rel="nofollow" href="https://github.com/loubnabnl/santacoder-finetuning/blob/main/train.py">code</a>
          which uses <a rel="nofollow" href="https://github.com/loubnabnl/santacoder-finetuning/blob/13b69333fe60d0201c35f40ca0c5a8bfce104f80/train.py#L152">fim</a>,
          it should be easy to adapt to the starcoder repo finetuning with PEFT since
          both use similar a data class.</p>

          '
        raw: In the finetuning code you pointed , while the data preparation is similar
          to the pre-training procedure, it doesn't use FIM. The model might still
          be able to know how to perform FIM after that fine-tuning. However, if you
          want to preserve the same infilling capabilities you might want to include
          it in the training, you can check this [code](https://github.com/loubnabnl/santacoder-finetuning/blob/main/train.py)
          which uses [fim](https://github.com/loubnabnl/santacoder-finetuning/blob/13b69333fe60d0201c35f40ca0c5a8bfce104f80/train.py#L152),
          it should be easy to adapt to the starcoder repo finetuning with PEFT since
          both use similar a data class.
        updatedAt: '2023-07-05T16:59:17.316Z'
      numEdits: 1
      reactions: []
    id: 64a5a0ebf8721c8967ab8986
    type: comment
  author: loubnabnl
  content: In the finetuning code you pointed , while the data preparation is similar
    to the pre-training procedure, it doesn't use FIM. The model might still be able
    to know how to perform FIM after that fine-tuning. However, if you want to preserve
    the same infilling capabilities you might want to include it in the training,
    you can check this [code](https://github.com/loubnabnl/santacoder-finetuning/blob/main/train.py)
    which uses [fim](https://github.com/loubnabnl/santacoder-finetuning/blob/13b69333fe60d0201c35f40ca0c5a8bfce104f80/train.py#L152),
    it should be easy to adapt to the starcoder repo finetuning with PEFT since both
    use similar a data class.
  created_at: 2023-07-05 15:57:15+00:00
  edited: true
  hidden: false
  id: 64a5a0ebf8721c8967ab8986
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/361b58abb6b14866c4345324e1edda9e.svg
      fullname: Rob Leidle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spew
      type: user
    createdAt: '2023-07-19T21:00:24.000Z'
    data:
      edited: false
      editors:
      - spew
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8231369256973267
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/361b58abb6b14866c4345324e1edda9e.svg
          fullname: Rob Leidle
          isHf: false
          isPro: false
          name: spew
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;loubnabnl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/loubnabnl\"\
          >@<span class=\"underline\">loubnabnl</span></a></span>\n\n\t</span></span>\
          \ for this \"traing on my private repo\" scenario, should the validation\
          \ dataset be the same as the training dataset?</p>\n"
        raw: Hi @loubnabnl for this "traing on my private repo" scenario, should the
          validation dataset be the same as the training dataset?
        updatedAt: '2023-07-19T21:00:24.866Z'
      numEdits: 0
      reactions: []
    id: 64b84ee8047fa3db9412a903
    type: comment
  author: spew
  content: Hi @loubnabnl for this "traing on my private repo" scenario, should the
    validation dataset be the same as the training dataset?
  created_at: 2023-07-19 20:00:24+00:00
  edited: false
  hidden: false
  id: 64b84ee8047fa3db9412a903
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-07-24T09:45:45.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7138267159461975
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>We usually select a small percentage of the training data as a validation
          dataset (and that we remove from the training, <a rel="nofollow" href="https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L205">see</a>)</p>

          '
        raw: We usually select a small percentage of the training data as a validation
          dataset (and that we remove from the training, [see](https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L205))
        updatedAt: '2023-07-24T09:45:45.381Z'
      numEdits: 0
      reactions: []
    id: 64be48491a62149c5e8a0b74
    type: comment
  author: loubnabnl
  content: We usually select a small percentage of the training data as a validation
    dataset (and that we remove from the training, [see](https://github.com/bigcode-project/starcoder/blob/d72c7fe3dda81d47ad9b851f9567393fb6b551b9/finetune/finetune.py#L205))
  created_at: 2023-07-24 08:45:45+00:00
  edited: false
  hidden: false
  id: 64be48491a62149c5e8a0b74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8bfcc688e16fbbfe58d4ae7141fbad9c.svg
      fullname: Shuja ahmed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shujahm
      type: user
    createdAt: '2023-08-10T08:58:24.000Z'
    data:
      edited: false
      editors:
      - shujahm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445863962173462
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8bfcc688e16fbbfe58d4ae7141fbad9c.svg
          fullname: Shuja ahmed
          isHf: false
          isPro: false
          name: shujahm
          type: user
        html: '<p>I am following up a similar path way where I need to train on a
          private codebase, but I needed some pointers on how to convert private repos
          into datasets that can be used in fine-tuning. Any help will be greatly
          appreciated</p>

          '
        raw: I am following up a similar path way where I need to train on a private
          codebase, but I needed some pointers on how to convert private repos into
          datasets that can be used in fine-tuning. Any help will be greatly appreciated
        updatedAt: '2023-08-10T08:58:24.910Z'
      numEdits: 0
      reactions: []
    id: 64d4a6b0603775d25e47b3e4
    type: comment
  author: shujahm
  content: I am following up a similar path way where I need to train on a private
    codebase, but I needed some pointers on how to convert private repos into datasets
    that can be used in fine-tuning. Any help will be greatly appreciated
  created_at: 2023-08-10 07:58:24+00:00
  edited: false
  hidden: false
  id: 64d4a6b0603775d25e47b3e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dbf2946c1c88eafee287d3691c85f7ee.svg
      fullname: Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tiruvishal
      type: user
    createdAt: '2023-08-12T17:58:28.000Z'
    data:
      edited: false
      editors:
      - tiruvishal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.967798113822937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dbf2946c1c88eafee287d3691c85f7ee.svg
          fullname: Agarwal
          isHf: false
          isPro: false
          name: tiruvishal
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;loubnabnl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/loubnabnl\"\
          >@<span class=\"underline\">loubnabnl</span></a></span>\n\n\t</span></span>\
          \ Even I am trying to use startcoder as a starting point, on which, I would\
          \ like to train that model with private codebase and deploy the new model\
          \ somewhere (HF perhaps) so that can be called from any IDE. Is there an\
          \ article that details the entire process end-to-end? Finding it difficult\
          \ to gather the necessary information. Any help would be greatly appreciated</p>\n"
        raw: Hi @loubnabnl Even I am trying to use startcoder as a starting point,
          on which, I would like to train that model with private codebase and deploy
          the new model somewhere (HF perhaps) so that can be called from any IDE.
          Is there an article that details the entire process end-to-end? Finding
          it difficult to gather the necessary information. Any help would be greatly
          appreciated
        updatedAt: '2023-08-12T17:58:28.783Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - ninja1290243
        - jiewmurphy
        - Rajath-jain
        - hi5-hi5
    id: 64d7c8449a6a7ae984312586
    type: comment
  author: tiruvishal
  content: Hi @loubnabnl Even I am trying to use startcoder as a starting point, on
    which, I would like to train that model with private codebase and deploy the new
    model somewhere (HF perhaps) so that can be called from any IDE. Is there an article
    that details the entire process end-to-end? Finding it difficult to gather the
    necessary information. Any help would be greatly appreciated
  created_at: 2023-08-12 16:58:28+00:00
  edited: false
  hidden: false
  id: 64d7c8449a6a7ae984312586
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a51a021644e0dd7943b0e90986897541.svg
      fullname: Shreya Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hi5-hi5
      type: user
    createdAt: '2023-09-30T13:06:00.000Z'
    data:
      edited: false
      editors:
      - hi5-hi5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9884169101715088
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a51a021644e0dd7943b0e90986897541.svg
          fullname: Shreya Singh
          isHf: false
          isPro: false
          name: hi5-hi5
          type: user
        html: '<p>Hi, anyone has tried this and has an estimate of how long it will
          take.<br>I have just started training on a dataset of 20k codes, how long
          it would take ?</p>

          '
        raw: 'Hi, anyone has tried this and has an estimate of how long it will take.

          I have just started training on a dataset of 20k codes, how long it would
          take ?'
        updatedAt: '2023-09-30T13:06:00.871Z'
      numEdits: 0
      reactions: []
    id: 65181d384b9852688663d3df
    type: comment
  author: hi5-hi5
  content: 'Hi, anyone has tried this and has an estimate of how long it will take.

    I have just started training on a dataset of 20k codes, how long it would take
    ?'
  created_at: 2023-09-30 12:06:00+00:00
  edited: false
  hidden: false
  id: 65181d384b9852688663d3df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49328849866095f481ee72eb5dd066e2.svg
      fullname: Adam klein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thinkcru
      type: user
    createdAt: '2023-10-07T17:59:20.000Z'
    data:
      edited: false
      editors:
      - Thinkcru
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.838368833065033
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49328849866095f481ee72eb5dd066e2.svg
          fullname: Adam klein
          isHf: false
          isPro: false
          name: Thinkcru
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;loubnabnl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/loubnabnl\"\
          >@<span class=\"underline\">loubnabnl</span></a></span>\n\n\t</span></span>\
          \ -<br>Regarding the comment \u201C this code which uses fim\u201D</p>\n\
          <p>Did anyone open a PR to add FIM support to the PEFT fine tuning script?</p>\n\
          <p>One more question, can one use this same script with CodeLlama?</p>\n"
        raw: "Hey @loubnabnl -\nRegarding the comment \u201C this code which uses\
          \ fim\u201D\n\nDid anyone open a PR to add FIM support to the PEFT fine\
          \ tuning script?\n\nOne more question, can one use this same script with\
          \ CodeLlama?"
        updatedAt: '2023-10-07T17:59:20.980Z'
      numEdits: 0
      reactions: []
    id: 65219c780f935fa8fd37d553
    type: comment
  author: Thinkcru
  content: "Hey @loubnabnl -\nRegarding the comment \u201C this code which uses fim\u201D\
    \n\nDid anyone open a PR to add FIM support to the PEFT fine tuning script?\n\n\
    One more question, can one use this same script with CodeLlama?"
  created_at: 2023-10-07 16:59:20+00:00
  edited: false
  hidden: false
  id: 65219c780f935fa8fd37d553
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 63
repo_id: bigcode/starcoder
repo_type: model
status: open
target_branch: null
title: finetuning for autocompletion?
