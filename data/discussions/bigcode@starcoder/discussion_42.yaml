!!python/object:huggingface_hub.community.DiscussionWithDetails
author: liukuo99
conflicting_files: null
created_at: 2023-05-25 20:35:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c09d5747623517150cc415130f14ca0d.svg
      fullname: Kuo Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liukuo99
      type: user
    createdAt: '2023-05-25T21:35:29.000Z'
    data:
      edited: false
      editors:
      - liukuo99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c09d5747623517150cc415130f14ca0d.svg
          fullname: Kuo Liu
          isHf: false
          isPro: false
          name: liukuo99
          type: user
        html: '<p>Is it possible to use starcoder for question and answering tasks?
          I''d like to use starcoder in a question-answering pipeline, but it seems
          it''s not supported.  I got the following error when running</p>

          <p><code>pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)
          question = "some specific question" pipe(question=question, context=contents)</code></p>

          <p><code>The model ''GPTBigCodeForCausalLM'' is not supported for question-answering.
          Supported models are [''AlbertForQuestionAnswering'', ''BartForQuestionAnswering'',
          ''BertForQuestionAnswering'', ''BigBirdForQuestionAnswering'', ''BigBirdPegasusForQuestionAnswering'',
          ''BloomForQuestionAnswering'', ''CamembertForQuestionAnswering'', ''CanineForQuestionAnswering'',
          ''ConvBertForQuestionAnswering'', ''Data2VecTextForQuestionAnswering'',
          ''DebertaForQuestionAnswering'', ''DebertaV2ForQuestionAnswering'', ''DistilBertForQuestionAnswering'',
          ''ElectraForQuestionAnswering'', ''ErnieForQuestionAnswering'', ''ErnieMForQuestionAnswering'',
          ''FlaubertForQuestionAnsweringSimple'', ''FNetForQuestionAnswering'', ''FunnelForQuestionAnswering'',
          ''GPT2ForQuestionAnswering'', ''GPTNeoForQuestionAnswering'', ''GPTNeoXForQuestionAnswering'',
          ''GPTJForQuestionAnswering'', ''IBertForQuestionAnswering'', ''LayoutLMv2ForQuestionAnswering'',
          ''LayoutLMv3ForQuestionAnswering'', ''LEDForQuestionAnswering'', ''LiltForQuestionAnswering'',
          ''LongformerForQuestionAnswering'', ''LukeForQuestionAnswering'', ''LxmertForQuestionAnswering'',
          ''MarkupLMForQuestionAnswering'', ''MBartForQuestionAnswering'', ''MegaForQuestionAnswering'',
          ''MegatronBertForQuestionAnswering'', ''MobileBertForQuestionAnswering'',
          ''MPNetForQuestionAnswering'', ''MvpForQuestionAnswering'', ''NezhaForQuestionAnswering'',
          ''NystromformerForQuestionAnswering'', ''OPTForQuestionAnswering'', ''QDQBertForQuestionAnswering'',
          ''ReformerForQuestionAnswering'', ''RemBertForQuestionAnswering'', ''RobertaForQuestionAnswering'',
          ''RobertaPreLayerNormForQuestionAnswering'', ''RoCBertForQuestionAnswering'',
          ''RoFormerForQuestionAnswering'', ''SplinterForQuestionAnswering'', ''SqueezeBertForQuestionAnswering'',
          ''XLMForQuestionAnsweringSimple'', ''XLMRobertaForQuestionAnswering'', ''XLMRobertaXLForQuestionAnswering'',
          ''XLNetForQuestionAnsweringSimple'', ''XmodForQuestionAnswering'', ''YosoForQuestionAnswering''].</code></p>

          '
        raw: "Is it possible to use starcoder for question and answering tasks? I'd\
          \ like to use starcoder in a question-answering pipeline, but it seems it's\
          \ not supported.  I got the following error when running\r\n\r\n```pipe\
          \ = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\r\
          \nquestion = \"some specific question\"\r\npipe(question=question, context=contents)```\r\
          \n\r\n```The model 'GPTBigCodeForCausalLM' is not supported for question-answering.\
          \ Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering',\
          \ 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering',\
          \ 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering',\
          \ 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering',\
          \ 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering',\
          \ 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple',\
          \ 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering',\
          \ 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering',\
          \ 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering',\
          \ 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LongformerForQuestionAnswering',\
          \ 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering',\
          \ 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering',\
          \ 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MvpForQuestionAnswering',\
          \ 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering',\
          \ 'QDQBertForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering',\
          \ 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering',\
          \ 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering',\
          \ 'SqueezeBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering',\
          \ 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple',\
          \ 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].```"
        updatedAt: '2023-05-25T21:35:29.730Z'
      numEdits: 0
      reactions: []
    id: 646fd4a1d742e9ef65210047
    type: comment
  author: liukuo99
  content: "Is it possible to use starcoder for question and answering tasks? I'd\
    \ like to use starcoder in a question-answering pipeline, but it seems it's not\
    \ supported.  I got the following error when running\r\n\r\n```pipe = pipeline(\"\
    question-answering\", model=model, tokenizer=tokenizer)\r\nquestion = \"some specific\
    \ question\"\r\npipe(question=question, context=contents)```\r\n\r\n```The model\
    \ 'GPTBigCodeForCausalLM' is not supported for question-answering. Supported models\
    \ are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering',\
    \ 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering',\
    \ 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering',\
    \ 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering',\
    \ 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering',\
    \ 'ErnieMForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering',\
    \ 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering',\
    \ 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering',\
    \ 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering',\
    \ 'LiltForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering',\
    \ 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering',\
    \ 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering',\
    \ 'MPNetForQuestionAnswering', 'MvpForQuestionAnswering', 'NezhaForQuestionAnswering',\
    \ 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering',\
    \ 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering',\
    \ 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering',\
    \ 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple',\
    \ 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple',\
    \ 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].```"
  created_at: 2023-05-25 20:35:29+00:00
  edited: false
  hidden: false
  id: 646fd4a1d742e9ef65210047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a4b7d30f0789a2a443f47620f376054.svg
      fullname: Jul Swart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swartj
      type: user
    createdAt: '2023-08-02T10:00:56.000Z'
    data:
      edited: false
      editors:
      - swartj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7867530584335327
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a4b7d30f0789a2a443f47620f376054.svg
          fullname: Jul Swart
          isHf: false
          isPro: false
          name: swartj
          type: user
        html: '<p>Its most likely:<br>pipe = pipeline("text-generation", model=model,
          tokenizer=tokenizer)</p>

          <p>On huggingface on top just below the reponame it says this model is a
          text generation, so that should be in the pipeline, not Q&amp;A</p>

          '
        raw: 'Its most likely:

          pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)


          On huggingface on top just below the reponame it says this model is a text
          generation, so that should be in the pipeline, not Q&A'
        updatedAt: '2023-08-02T10:00:56.730Z'
      numEdits: 0
      reactions: []
    id: 64ca295879d99f9e7a0ebed4
    type: comment
  author: swartj
  content: 'Its most likely:

    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)


    On huggingface on top just below the reponame it says this model is a text generation,
    so that should be in the pipeline, not Q&A'
  created_at: 2023-08-02 09:00:56+00:00
  edited: false
  hidden: false
  id: 64ca295879d99f9e7a0ebed4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: bigcode/starcoder
repo_type: model
status: open
target_branch: null
title: Using starcoder in question answering pipeline
