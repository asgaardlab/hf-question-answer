!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zkrider
conflicting_files: null
created_at: 2023-07-21 22:28:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
      fullname: Zak KRider
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkrider
      type: user
    createdAt: '2023-07-21T23:28:50.000Z'
    data:
      edited: true
      editors:
      - zkrider
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6663828492164612
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
          fullname: Zak KRider
          isHf: false
          isPro: false
          name: zkrider
          type: user
        html: "<p>Has anyone had luck in getting this to work with Sagemaker?</p>\n\
          <p>I'm getting the following errors in CloudWatch and even with the instance\
          \ type: ml.g5.8xlarge</p>\n<p>Error 1:</p>\n<pre><code>Error: ShardCannotStart\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize, trust_remote_code))\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 647, in run_until_complete\n    return future.result()\n  File \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize, trust_remote_code)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 134, in get_model\n    return santacoder_cls(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_santacoder.py\"\
          , line 62, in __init__\n    self.load_weights(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_santacoder.py\"\
          , line 96, in load_weights\n    value = value.to(device if quantize is None\
          \ else \"cpu\").to(dtype)\n</code></pre>\n<p>Error 2:</p>\n<pre><code>torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 22.20 GiB total\
          \ capacity; 19.72 GiB already allocated; 143.12 MiB free; 21.11 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\n</code></pre>\n<p>Using\
          \ the following for the deployment:</p>\n<pre><code>import json\nimport\
          \ sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel,\
          \ get_huggingface_llm_image_uri\n\ntry:\n    role = sagemaker.get_execution_role()\n\
          except ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\
          \n# Hub Model configuration. https://huggingface.co/models\nhub = {\n  \
          \  'HF_MODEL_ID':'bigcode/starcoder',\n    'SM_NUM_GPUS': json.dumps(1),\n\
          \    'HF_API_TOKEN': '&lt;TOKEN&gt;'\n}\n\n\n# create Hugging Face Model\
          \ Class\nhuggingface_model = HuggingFaceModel(\n    image_uri=get_huggingface_llm_image_uri(\"\
          huggingface\",version=\"0.8.2\"),\n    env=hub,\n    role=role, \n)\n\n\
          # deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n\
          \    initial_instance_count=1,\n    instance_type=\"ml.g5.8xlarge\",\n \
          \   container_startup_health_check_timeout=400,\n    endpoint_name=\"Starcoder\"\
          \n   )\n  \n# send request\npredictor.predict({\n    \"inputs\": \"def print_hello_world():\"\
          ,\n})\n</code></pre>\n"
        raw: "Has anyone had luck in getting this to work with Sagemaker?\n\nI'm getting\
          \ the following errors in CloudWatch and even with the instance type: ml.g5.8xlarge\n\
          \nError 1:\n```\nError: ShardCannotStart\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize, trust_remote_code))\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 647, in run_until_complete\n    return future.result()\n  File \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize, trust_remote_code)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 134, in get_model\n    return santacoder_cls(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_santacoder.py\"\
          , line 62, in __init__\n    self.load_weights(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_santacoder.py\"\
          , line 96, in load_weights\n    value = value.to(device if quantize is None\
          \ else \"cpu\").to(dtype)\n```\n\nError 2:\n\n```\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 22.20 GiB total\
          \ capacity; 19.72 GiB already allocated; 143.12 MiB free; 21.11 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\n```\n\n\nUsing the following for\
          \ the deployment:\n```\nimport json\nimport sagemaker\nimport boto3\nfrom\
          \ sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n\
          \ntry:\n\trole = sagemaker.get_execution_role()\nexcept ValueError:\n\t\
          iam = boto3.client('iam')\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\
          \n# Hub Model configuration. https://huggingface.co/models\nhub = {\n\t\
          'HF_MODEL_ID':'bigcode/starcoder',\n\t'SM_NUM_GPUS': json.dumps(1),\n\t\
          'HF_API_TOKEN': '<TOKEN>'\n}\n\n\n# create Hugging Face Model Class\nhuggingface_model\
          \ = HuggingFaceModel(\n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\"\
          ,version=\"0.8.2\"),\n\tenv=hub,\n\trole=role, \n)\n\n# deploy model to\
          \ SageMaker Inference\npredictor = huggingface_model.deploy(\n\tinitial_instance_count=1,\n\
          \tinstance_type=\"ml.g5.8xlarge\",\n\tcontainer_startup_health_check_timeout=400,\n\
          \    endpoint_name=\"Starcoder\"\n   )\n  \n# send request\npredictor.predict({\n\
          \t\"inputs\": \"def print_hello_world():\",\n})\n```"
        updatedAt: '2023-07-21T23:38:18.785Z'
      numEdits: 2
      reactions: []
    id: 64bb14b2c6e77d66f46038ea
    type: comment
  author: zkrider
  content: "Has anyone had luck in getting this to work with Sagemaker?\n\nI'm getting\
    \ the following errors in CloudWatch and even with the instance type: ml.g5.8xlarge\n\
    \nError 1:\n```\nError: ShardCannotStart\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
    \ quantize, trust_remote_code))\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 647, in run_until_complete\n    return future.result()\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
    \ quantize, trust_remote_code)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 134, in get_model\n    return santacoder_cls(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_santacoder.py\"\
    , line 62, in __init__\n    self.load_weights(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_santacoder.py\"\
    , line 96, in load_weights\n    value = value.to(device if quantize is None else\
    \ \"cpu\").to(dtype)\n```\n\nError 2:\n\n```\ntorch.cuda.OutOfMemoryError: CUDA\
    \ out of memory. Tried to allocate 288.00 MiB (GPU 0; 22.20 GiB total capacity;\
    \ 19.72 GiB already allocated; 143.12 MiB free; 21.11 GiB reserved in total by\
    \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    ```\n\n\nUsing the following for the deployment:\n```\nimport json\nimport sagemaker\n\
    import boto3\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n\
    \ntry:\n\trole = sagemaker.get_execution_role()\nexcept ValueError:\n\tiam = boto3.client('iam')\n\
    \trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\
    # Hub Model configuration. https://huggingface.co/models\nhub = {\n\t'HF_MODEL_ID':'bigcode/starcoder',\n\
    \t'SM_NUM_GPUS': json.dumps(1),\n\t'HF_API_TOKEN': '<TOKEN>'\n}\n\n\n# create\
    \ Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\timage_uri=get_huggingface_llm_image_uri(\"\
    huggingface\",version=\"0.8.2\"),\n\tenv=hub,\n\trole=role, \n)\n\n# deploy model\
    \ to SageMaker Inference\npredictor = huggingface_model.deploy(\n\tinitial_instance_count=1,\n\
    \tinstance_type=\"ml.g5.8xlarge\",\n\tcontainer_startup_health_check_timeout=400,\n\
    \    endpoint_name=\"Starcoder\"\n   )\n  \n# send request\npredictor.predict({\n\
    \t\"inputs\": \"def print_hello_world():\",\n})\n```"
  created_at: 2023-07-21 22:28:50+00:00
  edited: true
  hidden: false
  id: 64bb14b2c6e77d66f46038ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
      fullname: Zak KRider
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkrider
      type: user
    createdAt: '2023-07-31T18:56:46.000Z'
    data:
      edited: false
      editors:
      - zkrider
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8919632434844971
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
          fullname: Zak KRider
          isHf: false
          isPro: false
          name: zkrider
          type: user
        html: '<p>It worked by putting it on the AWS instance type: ml.g4dn.12xlarge
          and setting SM_NUM_GPUS: "4"</p>

          '
        raw: 'It worked by putting it on the AWS instance type: ml.g4dn.12xlarge and
          setting SM_NUM_GPUS: "4"'
        updatedAt: '2023-07-31T18:56:46.413Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c803ee1825d12864afa735
    id: 64c803ee1825d12864afa734
    type: comment
  author: zkrider
  content: 'It worked by putting it on the AWS instance type: ml.g4dn.12xlarge and
    setting SM_NUM_GPUS: "4"'
  created_at: 2023-07-31 17:56:46+00:00
  edited: false
  hidden: false
  id: 64c803ee1825d12864afa734
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
      fullname: Zak KRider
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkrider
      type: user
    createdAt: '2023-07-31T18:56:46.000Z'
    data:
      status: closed
    id: 64c803ee1825d12864afa735
    type: status-change
  author: zkrider
  created_at: 2023-07-31 17:56:46+00:00
  id: 64c803ee1825d12864afa735
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 66
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: 'Sagemaker: CUDA out of memory'
