!!python/object:huggingface_hub.community.DiscussionWithDetails
author: merlinarer
conflicting_files: null
created_at: 2023-05-10 03:47:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
      fullname: merlinchan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: merlinarer
      type: user
    createdAt: '2023-05-10T04:47:18.000Z'
    data:
      edited: false
      editors:
      - merlinarer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
          fullname: merlinchan
          isHf: false
          isPro: false
          name: merlinarer
          type: user
        html: '<p>Apart from int8, is there any plan to  speed up inferring, such
          as fastertransformer?</p>

          '
        raw: Apart from int8, is there any plan to  speed up inferring, such as fastertransformer?
        updatedAt: '2023-05-10T04:47:18.444Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - Anditty
        - gaeldelalleau
        - JoaoLages
        - merlinarer
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Anditty
        - Amulyaaa
    id: 645b21d6f837237ea118b7c0
    type: comment
  author: merlinarer
  content: Apart from int8, is there any plan to  speed up inferring, such as fastertransformer?
  created_at: 2023-05-10 03:47:18+00:00
  edited: false
  hidden: false
  id: 645b21d6f837237ea118b7c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4f1239b1577f16171b050106d3a82c7.svg
      fullname: yan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vandarkholme
      type: user
    createdAt: '2023-05-12T06:15:13.000Z'
    data:
      edited: false
      editors:
      - vandarkholme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4f1239b1577f16171b050106d3a82c7.svg
          fullname: yan
          isHf: false
          isPro: false
          name: vandarkholme
          type: user
        html: '<p>I dont think fastertransformer is an easy way... may torchscript
          and pytorch2.0 work</p>

          '
        raw: I dont think fastertransformer is an easy way... may torchscript and
          pytorch2.0 work
        updatedAt: '2023-05-12T06:15:13.044Z'
      numEdits: 0
      reactions: []
    id: 645dd97157f62deb29179750
    type: comment
  author: vandarkholme
  content: I dont think fastertransformer is an easy way... may torchscript and pytorch2.0
    work
  created_at: 2023-05-12 05:15:13+00:00
  edited: false
  hidden: false
  id: 645dd97157f62deb29179750
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
      fullname: Arjun Guha
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: arjunguha
      type: user
    createdAt: '2023-05-18T13:49:41.000Z'
    data:
      edited: false
      editors:
      - arjunguha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
          fullname: Arjun Guha
          isHf: false
          isPro: false
          name: arjunguha
          type: user
        html: '<p>The easiest way to do this may be to use the inference server:</p>

          <p><a rel="nofollow" href="https://github.com/bigcode-project/starcoder#text-generation-inference">https://github.com/bigcode-project/starcoder#text-generation-inference</a></p>

          '
        raw: 'The easiest way to do this may be to use the inference server:


          https://github.com/bigcode-project/starcoder#text-generation-inference'
        updatedAt: '2023-05-18T13:49:41.409Z'
      numEdits: 0
      reactions: []
    id: 64662cf5e0fe831b478f8a55
    type: comment
  author: arjunguha
  content: 'The easiest way to do this may be to use the inference server:


    https://github.com/bigcode-project/starcoder#text-generation-inference'
  created_at: 2023-05-18 12:49:41+00:00
  edited: false
  hidden: false
  id: 64662cf5e0fe831b478f8a55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-05-28T11:44:17.000Z'
    data:
      edited: false
      editors:
      - michaelfeil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: '<p>You could try: <a href="https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md">https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md</a></p>

          '
        raw: 'You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md'
        updatedAt: '2023-05-28T11:44:17.593Z'
      numEdits: 0
      reactions: []
    id: 64733e916cff2f867201c29f
    type: comment
  author: michaelfeil
  content: 'You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md'
  created_at: 2023-05-28 10:44:17+00:00
  edited: false
  hidden: false
  id: 64733e916cff2f867201c29f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
      fullname: merlinchan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: merlinarer
      type: user
    createdAt: '2023-05-30T12:03:28.000Z'
    data:
      edited: false
      editors:
      - merlinarer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
          fullname: merlinchan
          isHf: false
          isPro: false
          name: merlinarer
          type: user
        html: '<blockquote>

          <p>You could try: <a href="https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md">https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md</a></p>

          </blockquote>

          <p>Amazing, how much speed could ct2fast-starcoder bring compared with the
          oringinal starcoder?</p>

          '
        raw: '> You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md


          Amazing, how much speed could ct2fast-starcoder bring compared with the
          oringinal starcoder?'
        updatedAt: '2023-05-30T12:03:28.297Z'
      numEdits: 0
      reactions: []
    id: 6475e610ab17a37d0b1c7cc7
    type: comment
  author: merlinarer
  content: '> You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md


    Amazing, how much speed could ct2fast-starcoder bring compared with the oringinal
    starcoder?'
  created_at: 2023-05-30 11:03:28+00:00
  edited: false
  hidden: false
  id: 6475e610ab17a37d0b1c7cc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640162547139-61c2e44c39245e7bf62def6f.jpeg?w=200&h=200&f=face
      fullname: Bilibili
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bilibili
      type: user
    createdAt: '2023-06-19T07:48:58.000Z'
    data:
      edited: false
      editors:
      - Bilibili
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9365449547767639
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640162547139-61c2e44c39245e7bf62def6f.jpeg?w=200&h=200&f=face
          fullname: Bilibili
          isHf: false
          isPro: false
          name: Bilibili
          type: user
        html: '<p>This also seems interesting: <a rel="nofollow" href="https://github.com/bigcode-project/starcoder.cpp">https://github.com/bigcode-project/starcoder.cpp</a></p>

          '
        raw: 'This also seems interesting: https://github.com/bigcode-project/starcoder.cpp'
        updatedAt: '2023-06-19T07:48:58.360Z'
      numEdits: 0
      reactions: []
    id: 6490086a2d8c26797c210ecd
    type: comment
  author: Bilibili
  content: 'This also seems interesting: https://github.com/bigcode-project/starcoder.cpp'
  created_at: 2023-06-19 06:48:58+00:00
  edited: false
  hidden: false
  id: 6490086a2d8c26797c210ecd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-06-19T07:54:49.000Z'
    data:
      edited: true
      editors:
      - michaelfeil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7890316247940063
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: '<blockquote>

          <blockquote>

          <p>You could try: <a href="https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md">https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md</a></p>

          </blockquote>

          <p>Amazing, how much speed could ct2fast-starcoder bring compared with the
          oringinal starcoder?</p>

          </blockquote>

          <p>Did not have time to check for starcoder. For santacoder:<br>Task: "def
          hello"  -&gt; generate 30 tokens<br>-&gt; transformers pipeline in float
          16,  cuda: ~1300ms per inference<br>-&gt; ctranslate2 in int8, cuda -&gt;
          315ms per inference</p>

          <p>I assume for starcoder, weights are bigger, hence maybe 1.5-2.5x speedup.</p>

          '
        raw: "> > You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md\n\
          > \n> Amazing, how much speed could ct2fast-starcoder bring compared with\
          \ the oringinal starcoder?\n\nDid not have time to check for starcoder.\
          \ For santacoder:\nTask: \"def hello\"  -> generate 30 tokens\n-> transformers\
          \ pipeline in float 16,  cuda: ~1300ms per inference\n-> ctranslate2 in\
          \ int8, cuda -> 315ms per inference\n\nI assume for starcoder, weights are\
          \ bigger, hence maybe 1.5-2.5x speedup."
        updatedAt: '2023-06-19T07:55:03.835Z'
      numEdits: 1
      reactions: []
    id: 649009c947a7c8182993fe9a
    type: comment
  author: michaelfeil
  content: "> > You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md\n\
    > \n> Amazing, how much speed could ct2fast-starcoder bring compared with the\
    \ oringinal starcoder?\n\nDid not have time to check for starcoder. For santacoder:\n\
    Task: \"def hello\"  -> generate 30 tokens\n-> transformers pipeline in float\
    \ 16,  cuda: ~1300ms per inference\n-> ctranslate2 in int8, cuda -> 315ms per\
    \ inference\n\nI assume for starcoder, weights are bigger, hence maybe 1.5-2.5x\
    \ speedup."
  created_at: 2023-06-19 06:54:49+00:00
  edited: true
  hidden: false
  id: 649009c947a7c8182993fe9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69316ed42612e76335b677d2b8b5b527.svg
      fullname: Thanh Ng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thanhnew2001
      type: user
    createdAt: '2023-09-23T03:55:16.000Z'
    data:
      edited: false
      editors:
      - thanhnew2001
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8640419840812683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69316ed42612e76335b677d2b8b5b527.svg
          fullname: Thanh Ng
          isHf: false
          isPro: false
          name: thanhnew2001
          type: user
        html: '<blockquote>

          <p>You could try: <a href="https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md">https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md</a></p>

          </blockquote>

          <p>This works like a charm, 100 times faster than the starchat and starcoder.
          I tried with 8, 12, 16G but failed, at least 24G RAM GPU will work.</p>

          '
        raw: '> You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md


          This works like a charm, 100 times faster than the starchat and starcoder.
          I tried with 8, 12, 16G but failed, at least 24G RAM GPU will work.'
        updatedAt: '2023-09-23T03:55:16.837Z'
      numEdits: 0
      reactions: []
    id: 650e61a48f80c1165cd376d6
    type: comment
  author: thanhnew2001
  content: '> You could try: https://huggingface.co/michaelfeil/ct2fast-starcoder/blob/main/README.md


    This works like a charm, 100 times faster than the starchat and starcoder. I tried
    with 8, 12, 16G but failed, at least 24G RAM GPU will work.'
  created_at: 2023-09-23 02:55:16+00:00
  edited: false
  hidden: false
  id: 650e61a48f80c1165cd376d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: bigcode/starcoder
repo_type: model
status: open
target_branch: null
title: How to speed up inferring?
