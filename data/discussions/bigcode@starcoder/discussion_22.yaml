!!python/object:huggingface_hub.community.DiscussionWithDetails
author: miraclezst
conflicting_files: null
created_at: 2023-05-10 06:09:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34c7f3f30759bbbbee0a77d0361e08a2.svg
      fullname: zhaositong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miraclezst
      type: user
    createdAt: '2023-05-10T07:09:56.000Z'
    data:
      edited: false
      editors:
      - miraclezst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34c7f3f30759bbbbee0a77d0361e08a2.svg
          fullname: zhaositong
          isHf: false
          isPro: false
          name: miraclezst
          type: user
        html: '<p>my code as below:</p>

          <hr>

          <p>pip install -q transformers<br>from transformers import AutoModelForCausalLM,
          AutoTokenizer<br>import os</p>

          <p>checkpoint = "bigcode/starcoder"<br>device = "cuda" # for GPU usage or
          "cpu" for CPU usage</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)<br>model
          = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True,load_in_8bit=True,device_map={"":
          0})</p>

          <p>input_text = "def print_hello_world():\n \n print(''Hello world!'')"<br>inputs
          = tokenizer.encode(input_text, return_tensors="pt").to(device)<br>outputs
          = model.generate(inputs)<br>print(tokenizer.decode(outputs[0]))</p>

          <hr>

          <p>output:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/woqgxGJkXaN3ahCxT8v__.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/woqgxGJkXaN3ahCxT8v__.png"></a><br>Does
          anyone know what is the reason for this?</p>

          '
        raw: "my code as below:\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------\r\
          \npip install -q transformers\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\nimport os\r\n\r\ncheckpoint = \"bigcode/starcoder\"\r\
          \ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)\r\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True,load_in_8bit=True,device_map={\"\
          \": 0})\r\n\r\ninput_text = \"def print_hello_world():\\n \\n print('Hello\
          \ world!')\"\r\ninputs = tokenizer.encode(input_text, return_tensors=\"\
          pt\").to(device)\r\noutputs = model.generate(inputs)\r\nprint(tokenizer.decode(outputs[0]))\r\
          \n\r\n------------------------------------------------------------------------------------------------------------------------------------\r\
          \noutput:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/woqgxGJkXaN3ahCxT8v__.png)\r\
          \nDoes anyone know what is the reason for this?"
        updatedAt: '2023-05-10T07:09:56.879Z'
      numEdits: 0
      reactions: []
    id: 645b434478730bcc103dd292
    type: comment
  author: miraclezst
  content: "my code as below:\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------\r\
    \npip install -q transformers\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\r\nimport os\r\n\r\ncheckpoint = \"bigcode/starcoder\"\r\ndevice\
    \ = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True,load_in_8bit=True,device_map={\"\
    \": 0})\r\n\r\ninput_text = \"def print_hello_world():\\n \\n print('Hello world!')\"\
    \r\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\r\n\
    outputs = model.generate(inputs)\r\nprint(tokenizer.decode(outputs[0]))\r\n\r\n\
    ------------------------------------------------------------------------------------------------------------------------------------\r\
    \noutput:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/woqgxGJkXaN3ahCxT8v__.png)\r\
    \nDoes anyone know what is the reason for this?"
  created_at: 2023-05-10 06:09:56+00:00
  edited: false
  hidden: false
  id: 645b434478730bcc103dd292
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dee0e5f50500a7a1ed6933dfb1cf6600.svg
      fullname: Jacob Hansen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jahansen
      type: user
    createdAt: '2023-05-19T14:06:49.000Z'
    data:
      edited: false
      editors:
      - jahansen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dee0e5f50500a7a1ed6933dfb1cf6600.svg
          fullname: Jacob Hansen
          isHf: false
          isPro: false
          name: jahansen
          type: user
        html: '<p>I run into the same issues and have not been able to resolve it.</p>

          '
        raw: I run into the same issues and have not been able to resolve it.
        updatedAt: '2023-05-19T14:06:49.871Z'
      numEdits: 0
      reactions: []
    id: 64678279ab75d9cb3c47a591
    type: comment
  author: jahansen
  content: I run into the same issues and have not been able to resolve it.
  created_at: 2023-05-19 13:06:49+00:00
  edited: false
  hidden: false
  id: 64678279ab75d9cb3c47a591
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1cd8aeb0688b25edbeb1a7c1faa8d5f.svg
      fullname: Fernando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nandovallec
      type: user
    createdAt: '2023-05-23T13:57:32.000Z'
    data:
      edited: true
      editors:
      - nandovallec
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1cd8aeb0688b25edbeb1a7c1faa8d5f.svg
          fullname: Fernando
          isHf: false
          isPro: false
          name: nandovallec
          type: user
        html: '<p>In your case, increasing the length of the generated tokens may
          help.</p>

          '
        raw: In your case, increasing the length of the generated tokens may help.
        updatedAt: '2023-05-26T08:13:23.035Z'
      numEdits: 1
      reactions: []
    id: 646cc64cc51833760b53cabe
    type: comment
  author: nandovallec
  content: In your case, increasing the length of the generated tokens may help.
  created_at: 2023-05-23 12:57:32+00:00
  edited: true
  hidden: false
  id: 646cc64cc51833760b53cabe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-05-25T17:49:46.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: "<p>You can run FIM using the following code:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModelForCausalLM\n\
          \ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"bigcode/starcoder\"</span>, truncation_side=<span class=\"hljs-string\"\
          >\"left\"</span>)\nmodel = AutoModelForCausalLM.from_pretrained(<span class=\"\
          hljs-string\">\"bigcode/starcoder\"</span>, torch_dtype=torch.bfloat16).cuda()\n\
          \ninput_text = <span class=\"hljs-string\">\"&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;\
          \    else:\\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\"\
          </span>\ninputs = tokenizer(input_text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>).to(<span class=\"hljs-string\">\"cuda\"</span>)\noutputs\
          \ = model.generate(**inputs, max_new_tokens=<span class=\"hljs-number\"\
          >25</span>)\ngeneration = [tokenizer_fim.decode(tensor, skip_special_tokens=<span\
          \ class=\"hljs-literal\">False</span>) <span class=\"hljs-keyword\">for</span>\
          \ tensor <span class=\"hljs-keyword\">in</span> outputs]\n<span class=\"\
          hljs-built_in\">print</span>(generation[<span class=\"hljs-number\">0</span>])\n\
          </code></pre>\n<pre><code>&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;\
          \    else:\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\n \
          \   if n &lt; 2:\n        return n\n&lt;|endoftext|&gt;\n</code></pre>\n"
        raw: "You can run FIM using the following code:\n```python\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          bigcode/starcoder\", truncation_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          bigcode/starcoder\", torch_dtype=torch.bfloat16).cuda()\n\ninput_text =\
          \ \"<fim_prefix>def fib(n):<fim_suffix>    else:\\n        return fib(n\
          \ - 2) + fib(n - 1)<fim_middle>\"\ninputs = tokenizer(input_text, return_tensors=\"\
          pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=25)\n\
          generation = [tokenizer_fim.decode(tensor, skip_special_tokens=False) for\
          \ tensor in outputs]\nprint(generation[0])\n```\n```\n<fim_prefix>def fib(n):<fim_suffix>\
          \    else:\n        return fib(n - 2) + fib(n - 1)<fim_middle>\n    if n\
          \ < 2:\n        return n\n<|endoftext|>\n```"
        updatedAt: '2023-05-25T17:49:46.923Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jahansen
    id: 646f9fbabc42f4b00231d297
    type: comment
  author: loubnabnl
  content: "You can run FIM using the following code:\n```python\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    bigcode/starcoder\", truncation_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    bigcode/starcoder\", torch_dtype=torch.bfloat16).cuda()\n\ninput_text = \"<fim_prefix>def\
    \ fib(n):<fim_suffix>    else:\\n        return fib(n - 2) + fib(n - 1)<fim_middle>\"\
    \ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs\
    \ = model.generate(**inputs, max_new_tokens=25)\ngeneration = [tokenizer_fim.decode(tensor,\
    \ skip_special_tokens=False) for tensor in outputs]\nprint(generation[0])\n```\n\
    ```\n<fim_prefix>def fib(n):<fim_suffix>    else:\n        return fib(n - 2) +\
    \ fib(n - 1)<fim_middle>\n    if n < 2:\n        return n\n<|endoftext|>\n```"
  created_at: 2023-05-25 16:49:46+00:00
  edited: false
  hidden: false
  id: 646f9fbabc42f4b00231d297
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-06T22:07:37.000Z'
    data:
      status: closed
    id: 647fae29ebbd5b7972a2196c
    type: status-change
  author: loubnabnl
  created_at: 2023-06-06 21:07:37+00:00
  id: 647fae29ebbd5b7972a2196c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: 'can not generate with mode: Fill-in-the-middle'
