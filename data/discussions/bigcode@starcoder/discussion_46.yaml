!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BANGYU
conflicting_files: null
created_at: 2023-06-01 06:57:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e31117e4f49e6404d139592cfb2bff1.svg
      fullname: XIANG BANGYU
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BANGYU
      type: user
    createdAt: '2023-06-01T07:57:38.000Z'
    data:
      edited: false
      editors:
      - BANGYU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e31117e4f49e6404d139592cfb2bff1.svg
          fullname: XIANG BANGYU
          isHf: false
          isPro: false
          name: BANGYU
          type: user
        html: "<pre><code> File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/servers/http_server.py\"\
          , line 57, in do_POST\n    response_str = self.executor.exec_json(post_data)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
          , line 86, in exec_json\n    raise e\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
          , line 76, in exec_json\n    result = self.do_exec(args_json)\n  File \"\
          /home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
          , line 97, in do_exec\n    result = self.inference_worker.inference(**args)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/starcoder_inference/main.py\"\
          , line 46, in inference\n    res = self.predict(text, **kwargs)\n  File\
          \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/starcoder_inference/main.py\"\
          , line 42, in predict\n    outputs = self.model.generate(tokens, do_sample=True,\
          \ pad_token_id=self.tokenizer.eos_token_id, temperature=temperature, max_new_tokens=max_new_tokens,\
          \ min_new_tokens=min_new_tokens, top_p=top_p, repetition_penalty=repetition_penalty)\n\
          \  File \"/usr/local/lib64/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\n    return self.sample(\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\n    outputs = self(\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 808, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 673, in forward\n    outputs = block(\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 316, in forward\n    attn_outputs = self.attn(\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 230, in forward\n    query, key_value = self.c_attn(hidden_states).split((self.embed_dim,\
          \ 2 * self.kv_dim), dim=2)\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\"\
          , line 320, in forward\n    out = bnb.matmul(x, self.weight, bias=self.bias,\
          \ state=self.state)\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 500, in matmul\n    return MatMul8bitLt.apply(A, B, out, bias, state)\n\
          \  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 417, in forward\n    output += torch.matmul(subA, state.subB)\nRuntimeError:\
          \ mat1 and mat2 shapes cannot be multiplied (1x3 and 2x6400)\n</code></pre>\n\
          <p>But this problem is sporadic\uFF0CHas anyone encountered a similar error?</p>\n"
        raw: "\r\n```\r\n File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/servers/http_server.py\"\
          , line 57, in do_POST\r\n    response_str = self.executor.exec_json(post_data)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
          , line 86, in exec_json\r\n    raise e\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
          , line 76, in exec_json\r\n    result = self.do_exec(args_json)\r\n  File\
          \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
          , line 97, in do_exec\r\n    result = self.inference_worker.inference(**args)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/starcoder_inference/main.py\"\
          , line 46, in inference\r\n    res = self.predict(text, **kwargs)\r\n  File\
          \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/starcoder_inference/main.py\"\
          , line 42, in predict\r\n    outputs = self.model.generate(tokens, do_sample=True,\
          \ pad_token_id=self.tokenizer.eos_token_id, temperature=temperature, max_new_tokens=max_new_tokens,\
          \ min_new_tokens=min_new_tokens, top_p=top_p, repetition_penalty=repetition_penalty)\r\
          \n  File \"/usr/local/lib64/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n \
          \ File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\r\n    return self.sample(\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 808, in forward\r\n    transformer_outputs = self.transformer(\r\n\
          \  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 673, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 316, in forward\r\n    attn_outputs = self.attn(\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 230, in forward\r\n    query, key_value = self.c_attn(hidden_states).split((self.embed_dim,\
          \ 2 * self.kv_dim), dim=2)\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\"\
          , line 320, in forward\r\n    out = bnb.matmul(x, self.weight, bias=self.bias,\
          \ state=self.state)\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 500, in matmul\r\n    return MatMul8bitLt.apply(A, B, out, bias,\
          \ state)\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 417, in forward\r\n    output += torch.matmul(subA, state.subB)\r\
          \nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x3 and 2x6400)\r\
          \n```\r\n\r\nBut this problem is sporadic\uFF0CHas anyone encountered a\
          \ similar error?"
        updatedAt: '2023-06-01T07:57:38.437Z'
      numEdits: 0
      reactions: []
    id: 64784f72c43296134a431145
    type: comment
  author: BANGYU
  content: "\r\n```\r\n File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/servers/http_server.py\"\
    , line 57, in do_POST\r\n    response_str = self.executor.exec_json(post_data)\r\
    \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
    , line 86, in exec_json\r\n    raise e\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
    , line 76, in exec_json\r\n    result = self.do_exec(args_json)\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/py_inference/serving/module_executor.py\"\
    , line 97, in do_exec\r\n    result = self.inference_worker.inference(**args)\r\
    \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/starcoder_inference/main.py\"\
    , line 46, in inference\r\n    res = self.predict(text, **kwargs)\r\n  File \"\
    /home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/binary/starcoder_inference/main.py\"\
    , line 42, in predict\r\n    outputs = self.model.generate(tokens, do_sample=True,\
    \ pad_token_id=self.tokenizer.eos_token_id, temperature=temperature, max_new_tokens=max_new_tokens,\
    \ min_new_tokens=min_new_tokens, top_p=top_p, repetition_penalty=repetition_penalty)\r\
    \n  File \"/usr/local/lib64/python3.10/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"\
    /home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1565, in generate\r\n    return self.sample(\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2612, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
    , line 808, in forward\r\n    transformer_outputs = self.transformer(\r\n  File\
    \ \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\", line\
    \ 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"\
    /home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
    , line 673, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
    , line 316, in forward\r\n    attn_outputs = self.attn(\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
    , line 230, in forward\r\n    query, key_value = self.c_attn(hidden_states).split((self.embed_dim,\
    \ 2 * self.kv_dim), dim=2)\r\n  File \"/usr/local/lib64/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\"\
    , line 320, in forward\r\n    out = bnb.matmul(x, self.weight, bias=self.bias,\
    \ state=self.state)\r\n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
    , line 500, in matmul\r\n    return MatMul8bitLt.apply(A, B, out, bias, state)\r\
    \n  File \"/home/admin/worker/slave/suezops_c2_prod_pre_star_coder.pre_star_coder_15_29/python_worker/python_user_base/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
    , line 417, in forward\r\n    output += torch.matmul(subA, state.subB)\r\nRuntimeError:\
    \ mat1 and mat2 shapes cannot be multiplied (1x3 and 2x6400)\r\n```\r\n\r\nBut\
    \ this problem is sporadic\uFF0CHas anyone encountered a similar error?"
  created_at: 2023-06-01 06:57:38+00:00
  edited: false
  hidden: false
  id: 64784f72c43296134a431145
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-06T22:11:12.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9361399412155151
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>Can you provide the code for how you''re loading the model and using
          it for generation?</p>

          '
        raw: Can you provide the code for how you're loading the model and using it
          for generation?
        updatedAt: '2023-06-06T22:11:12.442Z'
      numEdits: 0
      reactions: []
    id: 647faf00ebbd5b7972a221b6
    type: comment
  author: loubnabnl
  content: Can you provide the code for how you're loading the model and using it
    for generation?
  created_at: 2023-06-06 21:11:12+00:00
  edited: false
  hidden: false
  id: 647faf00ebbd5b7972a221b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-10-05T09:25:58.000Z'
    data:
      status: closed
    id: 651e8126f5726796ee40d241
    type: status-change
  author: loubnabnl
  created_at: 2023-10-05 08:25:58+00:00
  id: 651e8126f5726796ee40d241
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 46
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: When the model is called, an error is encountered
