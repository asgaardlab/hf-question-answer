!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YoungEver
conflicting_files: null
created_at: 2023-07-18 01:28:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1a0c0802fcad91b2c3f2c6530f887011.svg
      fullname: Zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YoungEver
      type: user
    createdAt: '2023-07-18T02:28:49.000Z'
    data:
      edited: true
      editors:
      - YoungEver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6151313185691833
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1a0c0802fcad91b2c3f2c6530f887011.svg
          fullname: Zhao
          isHf: false
          isPro: false
          name: YoungEver
          type: user
        html: "<p>I want to use batch generation for better performance. I used the\
          \ toy code below but found the results is wrong. A lot of <code>&lt;|endoftext|&gt;</code>\
          \ in the new generated tokens. So how to use batch generation with starcoder?\
          \ Thanks~</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          \ncheckpoint = <span class=\"hljs-string\">\"/shared/models/huggingface/starcoder/\"\
          </span>\ndevice = <span class=\"hljs-string\">\"cuda\"</span> <span class=\"\
          hljs-comment\"># for GPU usage or \"cpu\" for CPU usage</span>\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer.pad_token = tokenizer.eos_token\n\
          tokenizer.padding_side = <span class=\"hljs-string\">\"left\"</span> <span\
          \ class=\"hljs-comment\"># padding side = left for a decoder-only architecture</span>\n\
          model = AutoModelForCausalLM.from_pretrained(checkpoint).half().to(device)\n\
          \n<span class=\"hljs-comment\">#prompt = \"def print_hello_world():\"</span>\n\
          prompt = [<span class=\"hljs-string\">\"write a python function that caculate\
          \ the max element in a list\"</span>,\n          <span class=\"hljs-string\"\
          >\"write a c++ function that caculate the max element in a list\"</span>,\n\
          \         ]\ninputs = tokenizer(prompt, padding=<span class=\"hljs-literal\"\
          >True</span>, return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(device)\n\
          outputs = model.generate(**inputs, max_new_tokens=<span class=\"hljs-number\"\
          >30</span>)\ngenerated_txts = tokenizer.batch_decode(outputs)\n<span class=\"\
          hljs-built_in\">print</span>(generated_txts)\n</code></pre>\n<p>The output:</p>\n\
          <pre><code>&gt;&gt;&gt; generated_txts\n['&lt;|endoftext|&gt;write a python\
          \ function that caculate the max element in a list\\n\\ndef max_element(list):\\\
          n    max = list[0]\\n    for i in list:\\n        if i &gt; max:\\n    \
          \        max', 'write a c++ function that caculate the max element in a\
          \ list\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#']\n</code></pre>\n"
        raw: "I want to use batch generation for better performance. I used the toy\
          \ code below but found the results is wrong. A lot of `<|endoftext|>` in\
          \ the new generated tokens. So how to use batch generation with starcoder?\
          \ Thanks~\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\ncheckpoint = \"/shared/models/huggingface/starcoder/\"\
          \ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer.pad_token = tokenizer.eos_token\n\
          tokenizer.padding_side = \"left\" # padding side = left for a decoder-only\
          \ architecture\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).half().to(device)\n\
          \n#prompt = \"def print_hello_world():\"\nprompt = [\"write a python function\
          \ that caculate the max element in a list\",\n          \"write a c++ function\
          \ that caculate the max element in a list\",\n         ]\ninputs = tokenizer(prompt,\
          \ padding=True, return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs,\
          \ max_new_tokens=30)\ngenerated_txts = tokenizer.batch_decode(outputs)\n\
          print(generated_txts)\n```\n\nThe output:\n```\n>>> generated_txts\n['<|endoftext|>write\
          \ a python function that caculate the max element in a list\\n\\ndef max_element(list):\\\
          n    max = list[0]\\n    for i in list:\\n        if i > max:\\n       \
          \     max', 'write a c++ function that caculate the max element in a list\\\
          n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#']\n```\n\n"
        updatedAt: '2023-07-18T02:29:53.334Z'
      numEdits: 1
      reactions: []
    id: 64b5f8e15b4a27b5dd15cee0
    type: comment
  author: YoungEver
  content: "I want to use batch generation for better performance. I used the toy\
    \ code below but found the results is wrong. A lot of `<|endoftext|>` in the new\
    \ generated tokens. So how to use batch generation with starcoder? Thanks~\n\n\
    ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
    \ncheckpoint = \"/shared/models/huggingface/starcoder/\"\ndevice = \"cuda\" #\
    \ for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\" #\
    \ padding side = left for a decoder-only architecture\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).half().to(device)\n\
    \n#prompt = \"def print_hello_world():\"\nprompt = [\"write a python function\
    \ that caculate the max element in a list\",\n          \"write a c++ function\
    \ that caculate the max element in a list\",\n         ]\ninputs = tokenizer(prompt,\
    \ padding=True, return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs,\
    \ max_new_tokens=30)\ngenerated_txts = tokenizer.batch_decode(outputs)\nprint(generated_txts)\n\
    ```\n\nThe output:\n```\n>>> generated_txts\n['<|endoftext|>write a python function\
    \ that caculate the max element in a list\\n\\ndef max_element(list):\\n    max\
    \ = list[0]\\n    for i in list:\\n        if i > max:\\n            max', 'write\
    \ a c++ function that caculate the max element in a list\\n#\\n#\\n#\\n#\\n#\\\
    n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#']\n```\n\n"
  created_at: 2023-07-18 01:28:49+00:00
  edited: true
  hidden: false
  id: 64b5f8e15b4a27b5dd15cee0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474955ea8661017e3003b3a087f3ecc3.svg
      fullname: wuhiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wuhiu
      type: user
    createdAt: '2023-08-09T09:20:55.000Z'
    data:
      edited: false
      editors:
      - wuhiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9046101570129395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474955ea8661017e3003b3a087f3ecc3.svg
          fullname: wuhiu
          isHf: false
          isPro: false
          name: wuhiu
          type: user
        html: '<p>Have you solved this problem? I encountered the same problem</p>

          '
        raw: Have you solved this problem? I encountered the same problem
        updatedAt: '2023-08-09T09:20:55.183Z'
      numEdits: 0
      reactions: []
    id: 64d35a779ba46988ac323d27
    type: comment
  author: wuhiu
  content: Have you solved this problem? I encountered the same problem
  created_at: 2023-08-09 08:20:55+00:00
  edited: false
  hidden: false
  id: 64d35a779ba46988ac323d27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1635211360068-614a8b805ff360e089b13935.jpeg?w=200&h=200&f=face
      fullname: Brendan King
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Brendan
      type: user
    createdAt: '2023-09-07T23:08:01.000Z'
    data:
      edited: false
      editors:
      - Brendan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6359288692474365
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1635211360068-614a8b805ff360e089b13935.jpeg?w=200&h=200&f=face
          fullname: Brendan King
          isHf: false
          isPro: false
          name: Brendan
          type: user
        html: '<p>If it helps, I think the generations are accurate, the second one
          is just of poor quality. The first seems correct and hits your  <code>max_new_tokens</code>
          limit. The second is bad but those I don''t think those are special tokens,
          and I get the same result when generating  using only the second prompt:</p>

          <pre><code class="language-python">o1 = model.generate(inputs[<span class="hljs-string">''input_ids''</span>][<span
          class="hljs-number">1</span>][<span class="hljs-literal">None</span>, ...],
          max_new_tokens=<span class="hljs-number">30</span>)

          tokenizer.decode(o1[<span class="hljs-number">0</span>])

          </code></pre>

          <p><code>Out[23]: ''write a c++ function that caculate the max element in
          a list\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#''</code></p>

          <pre><code class="language-python">In[<span class="hljs-number">26</span>]:
          tokenizer.decode(o1[<span class="hljs-number">0</span>], skip_special_tokens=<span
          class="hljs-literal">True</span>) == tokenizer.decode(o1[<span class="hljs-number">0</span>],
          skip_special_tokens=<span class="hljs-literal">False</span>)

          Out[<span class="hljs-number">26</span>]: <span class="hljs-literal">True</span>

          </code></pre>

          '
        raw: 'If it helps, I think the generations are accurate, the second one is
          just of poor quality. The first seems correct and hits your  `max_new_tokens`
          limit. The second is bad but those I don''t think those are special tokens,
          and I get the same result when generating  using only the second prompt:


          ```python

          o1 = model.generate(inputs[''input_ids''][1][None, ...], max_new_tokens=30)

          tokenizer.decode(o1[0])

          ```

          `Out[23]: ''write a c++ function that caculate the max element in a list\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#''`

          ```python

          In[26]: tokenizer.decode(o1[0], skip_special_tokens=True) == tokenizer.decode(o1[0],
          skip_special_tokens=False)

          Out[26]: True

          ```'
        updatedAt: '2023-09-07T23:08:01.869Z'
      numEdits: 0
      reactions: []
    id: 64fa57d114636d417a7fd9b2
    type: comment
  author: Brendan
  content: 'If it helps, I think the generations are accurate, the second one is just
    of poor quality. The first seems correct and hits your  `max_new_tokens` limit.
    The second is bad but those I don''t think those are special tokens, and I get
    the same result when generating  using only the second prompt:


    ```python

    o1 = model.generate(inputs[''input_ids''][1][None, ...], max_new_tokens=30)

    tokenizer.decode(o1[0])

    ```

    `Out[23]: ''write a c++ function that caculate the max element in a list\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#''`

    ```python

    In[26]: tokenizer.decode(o1[0], skip_special_tokens=True) == tokenizer.decode(o1[0],
    skip_special_tokens=False)

    Out[26]: True

    ```'
  created_at: 2023-09-07 22:08:01+00:00
  edited: false
  hidden: false
  id: 64fa57d114636d417a7fd9b2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 65
repo_id: bigcode/starcoder
repo_type: model
status: open
target_branch: null
title: Batch generation with starcoder
