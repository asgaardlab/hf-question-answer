!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pgedeon
conflicting_files: null
created_at: 2023-05-04 19:26:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e6e2431a4c6fde43a81cd77d394c1fc.svg
      fullname: peter gedeon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pgedeon
      type: user
    createdAt: '2023-05-04T20:26:08.000Z'
    data:
      edited: false
      editors:
      - pgedeon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e6e2431a4c6fde43a81cd77d394c1fc.svg
          fullname: peter gedeon
          isHf: false
          isPro: false
          name: pgedeon
          type: user
        html: '<p>How much ram would meet the minimum requirement? I can not wait
          for some language specific models, buying an a100 is a bit out of my price
          range.</p>

          '
        raw: How much ram would meet the minimum requirement? I can not wait for some
          language specific models, buying an a100 is a bit out of my price range.
        updatedAt: '2023-05-04T20:26:08.017Z'
      numEdits: 0
      reactions: []
    id: 645414e0f2122886b29a0c12
    type: comment
  author: pgedeon
  content: How much ram would meet the minimum requirement? I can not wait for some
    language specific models, buying an a100 is a bit out of my price range.
  created_at: 2023-05-04 19:26:08+00:00
  edited: false
  hidden: false
  id: 645414e0f2122886b29a0c12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f990ab256ed98853749fad4412edbb5.svg
      fullname: Raymond Lucke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: rwl4
      type: user
    createdAt: '2023-05-04T21:18:34.000Z'
    data:
      edited: false
      editors:
      - rwl4
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f990ab256ed98853749fad4412edbb5.svg
          fullname: Raymond Lucke
          isHf: false
          isPro: true
          name: rwl4
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pgedeon&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pgedeon\">@<span class=\"\
          underline\">pgedeon</span></a></span>\n\n\t</span></span> I was able to\
          \ load it in under 24GB by using bitsandbytes. You can add <code>load_in_8bit=True,\
          \ device_map=\"auto\"</code> to the <code>load_pretrained</code> line.</p>\n"
        raw: '@pgedeon I was able to load it in under 24GB by using bitsandbytes.
          You can add `load_in_8bit=True, device_map="auto"` to the `load_pretrained`
          line.'
        updatedAt: '2023-05-04T21:18:34.868Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - zanghu
        - pgedeon
        - susnato
    id: 6454212ace1cc3ed5fe34e6a
    type: comment
  author: rwl4
  content: '@pgedeon I was able to load it in under 24GB by using bitsandbytes. You
    can add `load_in_8bit=True, device_map="auto"` to the `load_pretrained` line.'
  created_at: 2023-05-04 20:18:34+00:00
  edited: false
  hidden: false
  id: 6454212ace1cc3ed5fe34e6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4adc5a46c5f740a82576b12370286ef.svg
      fullname: GC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cactusthecoder8
      type: user
    createdAt: '2023-05-06T06:01:31.000Z'
    data:
      edited: false
      editors:
      - cactusthecoder8
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4adc5a46c5f740a82576b12370286ef.svg
          fullname: GC
          isHf: false
          isPro: false
          name: cactusthecoder8
          type: user
        html: '<p>I second setting <code>load_in_8bit=True</code>, but be careful
          when setting <code>device_map</code>to <code>auto</code> if you only have
          1 GPU since it may offload some of the layers to the CPU. The BigCode model
          obj class does not have a flag you can set to true to offload them to CPU.
          I ended up setting it to my own device map dict.</p>

          '
        raw: I second setting `load_in_8bit=True`, but be careful when setting `device_map`to
          `auto` if you only have 1 GPU since it may offload some of the layers to
          the CPU. The BigCode model obj class does not have a flag you can set to
          true to offload them to CPU. I ended up setting it to my own device map
          dict.
        updatedAt: '2023-05-06T06:01:31.718Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - noobmldude
        - pgedeon
        - susnato
    id: 6455ed3bbcfbcbd469c107fd
    type: comment
  author: cactusthecoder8
  content: I second setting `load_in_8bit=True`, but be careful when setting `device_map`to
    `auto` if you only have 1 GPU since it may offload some of the layers to the CPU.
    The BigCode model obj class does not have a flag you can set to true to offload
    them to CPU. I ended up setting it to my own device map dict.
  created_at: 2023-05-06 05:01:31+00:00
  edited: false
  hidden: false
  id: 6455ed3bbcfbcbd469c107fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/901567c11581e1b981779611994ef034.svg
      fullname: gamesense
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neverwin
      type: user
    createdAt: '2023-05-10T13:15:10.000Z'
    data:
      edited: false
      editors:
      - neverwin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/901567c11581e1b981779611994ef034.svg
          fullname: gamesense
          isHf: false
          isPro: false
          name: neverwin
          type: user
        html: '<p>Is it possible to run it in a 3070?</p>

          '
        raw: Is it possible to run it in a 3070?
        updatedAt: '2023-05-10T13:15:10.156Z'
      numEdits: 0
      reactions: []
    id: 645b98de337b2ccf07f9f7f6
    type: comment
  author: neverwin
  content: Is it possible to run it in a 3070?
  created_at: 2023-05-10 12:15:10+00:00
  edited: false
  hidden: false
  id: 645b98de337b2ccf07f9f7f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/006866a163e2384066dfd3f2665e6427.svg
      fullname: "Tomasz Zieli\u0144ski"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Warmonger
      type: user
    createdAt: '2023-05-10T14:17:51.000Z'
    data:
      edited: false
      editors:
      - Warmonger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/006866a163e2384066dfd3f2665e6427.svg
          fullname: "Tomasz Zieli\u0144ski"
          isHf: false
          isPro: false
          name: Warmonger
          type: user
        html: '<p>How about 4080 (16 GB)?</p>

          '
        raw: How about 4080 (16 GB)?
        updatedAt: '2023-05-10T14:17:51.630Z'
      numEdits: 0
      reactions: []
    id: 645ba78f8bbb8592d917b7eb
    type: comment
  author: Warmonger
  content: How about 4080 (16 GB)?
  created_at: 2023-05-10 13:17:51+00:00
  edited: false
  hidden: false
  id: 645ba78f8bbb8592d917b7eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76be435b075cc780b5e5e61dd2b5783f.svg
      fullname: Arjun Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AV99
      type: user
    createdAt: '2023-05-12T07:08:28.000Z'
    data:
      edited: false
      editors:
      - AV99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76be435b075cc780b5e5e61dd2b5783f.svg
          fullname: Arjun Verma
          isHf: false
          isPro: false
          name: AV99
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cactusthecoder8&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cactusthecoder8\"\
          >@<span class=\"underline\">cactusthecoder8</span></a></span>\n\n\t</span></span>\
          \ Could you probably share the device map that worked for you?</p>\n"
        raw: '@cactusthecoder8 Could you probably share the device map that worked
          for you?'
        updatedAt: '2023-05-12T07:08:28.383Z'
      numEdits: 0
      reactions: []
    id: 645de5ec9e8b99feea84cc05
    type: comment
  author: AV99
  content: '@cactusthecoder8 Could you probably share the device map that worked for
    you?'
  created_at: 2023-05-12 06:08:28+00:00
  edited: false
  hidden: false
  id: 645de5ec9e8b99feea84cc05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4adc5a46c5f740a82576b12370286ef.svg
      fullname: GC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cactusthecoder8
      type: user
    createdAt: '2023-05-14T04:26:04.000Z'
    data:
      edited: false
      editors:
      - cactusthecoder8
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4adc5a46c5f740a82576b12370286ef.svg
          fullname: GC
          isHf: false
          isPro: false
          name: cactusthecoder8
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;cactusthecoder8&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cactusthecoder8\"\
          >@<span class=\"underline\">cactusthecoder8</span></a></span>\n\n\t</span></span>\
          \ Could you probably share the device map that worked for you?</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;AV99&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AV99\">@<span class=\"\
          underline\">AV99</span></a></span>\n\n\t</span></span> how many GPUs you\
          \ have and what are the sizes of their memory each?</p>\n"
        raw: '> @cactusthecoder8 Could you probably share the device map that worked
          for you?


          @AV99 how many GPUs you have and what are the sizes of their memory each?'
        updatedAt: '2023-05-14T04:26:04.884Z'
      numEdits: 0
      reactions: []
    id: 646062dc446a4fa4695c6ebb
    type: comment
  author: cactusthecoder8
  content: '> @cactusthecoder8 Could you probably share the device map that worked
    for you?


    @AV99 how many GPUs you have and what are the sizes of their memory each?'
  created_at: 2023-05-14 03:26:04+00:00
  edited: false
  hidden: false
  id: 646062dc446a4fa4695c6ebb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4adc5a46c5f740a82576b12370286ef.svg
      fullname: GC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cactusthecoder8
      type: user
    createdAt: '2023-05-14T04:27:13.000Z'
    data:
      edited: false
      editors:
      - cactusthecoder8
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4adc5a46c5f740a82576b12370286ef.svg
          fullname: GC
          isHf: false
          isPro: false
          name: cactusthecoder8
          type: user
        html: '<blockquote>

          <p>How about 4080 (16 GB)?</p>

          </blockquote>

          <p>I tried multiple configurations of the model, and nothing runs successfully
          with only 16GB unfortunately.</p>

          '
        raw: '> How about 4080 (16 GB)?


          I tried multiple configurations of the model, and nothing runs successfully
          with only 16GB unfortunately.'
        updatedAt: '2023-05-14T04:27:13.607Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - noobmldude
    id: 646063216990e120852a41e5
    type: comment
  author: cactusthecoder8
  content: '> How about 4080 (16 GB)?


    I tried multiple configurations of the model, and nothing runs successfully with
    only 16GB unfortunately.'
  created_at: 2023-05-14 03:27:13+00:00
  edited: false
  hidden: false
  id: 646063216990e120852a41e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76be435b075cc780b5e5e61dd2b5783f.svg
      fullname: Arjun Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AV99
      type: user
    createdAt: '2023-05-17T08:34:06.000Z'
    data:
      edited: true
      editors:
      - AV99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76be435b075cc780b5e5e61dd2b5783f.svg
          fullname: Arjun Verma
          isHf: false
          isPro: false
          name: AV99
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cactusthecoder8&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cactusthecoder8\"\
          >@<span class=\"underline\">cactusthecoder8</span></a></span>\n\n\t</span></span>\
          \ I initially started out with a single 16GB GPU and with offloading between\
          \ CPU and GPU (and an hour of inference time later), I was barely able to\
          \ get the \"Hello World\" running. </p>\n<p>I now have 4GPUs, 16GB memory\
          \ each. Any suggestions?</p>\n"
        raw: "@cactusthecoder8 I initially started out with a single 16GB GPU and\
          \ with offloading between CPU and GPU (and an hour of inference time later),\
          \ I was barely able to get the \"Hello World\" running. \n\nI now have 4GPUs,\
          \ 16GB memory each. Any suggestions?"
        updatedAt: '2023-05-17T08:34:34.942Z'
      numEdits: 1
      reactions: []
    id: 6464917e4855e06b95ff83cf
    type: comment
  author: AV99
  content: "@cactusthecoder8 I initially started out with a single 16GB GPU and with\
    \ offloading between CPU and GPU (and an hour of inference time later), I was\
    \ barely able to get the \"Hello World\" running. \n\nI now have 4GPUs, 16GB memory\
    \ each. Any suggestions?"
  created_at: 2023-05-17 07:34:06+00:00
  edited: true
  hidden: false
  id: 6464917e4855e06b95ff83cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e11778d6d7c0e789763a758bc8fca171.svg
      fullname: Loui Sum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LouiSum
      type: user
    createdAt: '2023-05-17T12:55:16.000Z'
    data:
      edited: false
      editors:
      - LouiSum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e11778d6d7c0e789763a758bc8fca171.svg
          fullname: Loui Sum
          isHf: false
          isPro: false
          name: LouiSum
          type: user
        html: '<p>Can I run it locally on my Mac Studio (M1 Max 32 G)?</p>

          '
        raw: Can I run it locally on my Mac Studio (M1 Max 32 G)?
        updatedAt: '2023-05-17T12:55:16.544Z'
      numEdits: 0
      reactions: []
    id: 6464ceb406cd98685aa20e76
    type: comment
  author: LouiSum
  content: Can I run it locally on my Mac Studio (M1 Max 32 G)?
  created_at: 2023-05-17 11:55:16+00:00
  edited: false
  hidden: false
  id: 6464ceb406cd98685aa20e76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/024962d1e0b9a119abe8b87d26bc1785.svg
      fullname: Marcel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcel
      type: user
    createdAt: '2023-05-23T12:32:33.000Z'
    data:
      edited: true
      editors:
      - marcel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/024962d1e0b9a119abe8b87d26bc1785.svg
          fullname: Marcel
          isHf: false
          isPro: false
          name: marcel
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LouiSum&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/LouiSum\">@<span class=\"\
          underline\">LouiSum</span></a></span>\n\n\t</span></span> that would be\
          \ fun</p>\n"
        raw: '@LouiSum that would be fun'
        updatedAt: '2023-05-23T12:32:41.951Z'
      numEdits: 1
      reactions: []
    id: 646cb261393c77ea4b8a4d1c
    type: comment
  author: marcel
  content: '@LouiSum that would be fun'
  created_at: 2023-05-23 11:32:33+00:00
  edited: true
  hidden: false
  id: 646cb261393c77ea4b8a4d1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-05-24T10:02:11.000Z'
    data:
      edited: true
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: "<p>You can try ggml implementation <a rel=\"nofollow\" href=\"https://github.com/bigcode-project/starcoder.cpp\"\
          >starcoder.cpp</a> to run the model locally on your M1 machine.</p>\n<p>In\
          \ fp16/bf16 on one GPU the model takes ~32GB, in 8bit the model requires\
          \ ~22GB, so with 4 GPUs you can split this memory requirement by 4 and fit\
          \ it in less than 10GB on each using the following code (make sure you have\
          \ <code>accelerate</code> installed and <code>bitsandbytes</code> for 8bit\
          \ mode):</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM\n\
          <span class=\"hljs-keyword\">import</span> torch\n\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">get_gpus_max_memory</span>(<span\
          \ class=\"hljs-params\">max_memory</span>):\n    max_memory = {i: max_memory\
          \ <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">range</span>(torch.cuda.device_count())}\n\
          \    <span class=\"hljs-keyword\">return</span> max_memory\n\n<span class=\"\
          hljs-comment\"># for example for a max use of 10GB per GPU</span>\n<span\
          \ class=\"hljs-comment\"># for fp16 replace with  `load_in_8bit=True` with\
          \   `torch_dtype=torch.float16`</span>\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"bigcode/starcoder\"</span>, \n    device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>, \n    load_in_8bit=<span class=\"\
          hljs-literal\">True</span>,\n    max_memory=get_gpus_max_memory(<span class=\"\
          hljs-string\">\"10GB\"</span>),\n)\n</code></pre>\n<p>To understand the\
          \ logic behind this check this <a href=\"https://huggingface.co/docs/transformers/perf_infer_gpu_one#running-mixedint8-models-multi-gpu-setup\"\
          >documentation</a> or this <a href=\"https://huggingface.co/blog/accelerate-large-models\"\
          >blog</a>  for handling large model inference.</p>\n"
        raw: "You can try ggml implementation [starcoder.cpp](https://github.com/bigcode-project/starcoder.cpp)\
          \ to run the model locally on your M1 machine.\n\nIn fp16/bf16 on one GPU\
          \ the model takes ~32GB, in 8bit the model requires ~22GB, so with 4 GPUs\
          \ you can split this memory requirement by 4 and fit it in less than 10GB\
          \ on each using the following code (make sure you have `accelerate` installed\
          \ and `bitsandbytes` for 8bit mode):\n```python\nfrom transformers import\
          \ AutoModelForCausalLM\nimport torch\n\ndef get_gpus_max_memory(max_memory):\n\
          \    max_memory = {i: max_memory for i in range(torch.cuda.device_count())}\n\
          \    return max_memory\n\n# for example for a max use of 10GB per GPU\n\
          # for fp16 replace with  `load_in_8bit=True` with   `torch_dtype=torch.float16`\n\
          model = AutoModelForCausalLM.from_pretrained(\n    \"bigcode/starcoder\"\
          , \n    device_map=\"auto\", \n    load_in_8bit=True,\n    max_memory=get_gpus_max_memory(\"\
          10GB\"),\n)\n```\nTo understand the logic behind this check this [documentation](https://huggingface.co/docs/transformers/perf_infer_gpu_one#running-mixedint8-models-multi-gpu-setup)\
          \ or this [blog](https://huggingface.co/blog/accelerate-large-models)  for\
          \ handling large model inference."
        updatedAt: '2023-05-24T10:07:36.792Z'
      numEdits: 2
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - rnqee
        - noobmldude
        - xiwen426
        - miloskonjusina
        - tqcloud
        - susnato
    id: 646de0a3e34b2ec2d2d75803
    type: comment
  author: loubnabnl
  content: "You can try ggml implementation [starcoder.cpp](https://github.com/bigcode-project/starcoder.cpp)\
    \ to run the model locally on your M1 machine.\n\nIn fp16/bf16 on one GPU the\
    \ model takes ~32GB, in 8bit the model requires ~22GB, so with 4 GPUs you can\
    \ split this memory requirement by 4 and fit it in less than 10GB on each using\
    \ the following code (make sure you have `accelerate` installed and `bitsandbytes`\
    \ for 8bit mode):\n```python\nfrom transformers import AutoModelForCausalLM\n\
    import torch\n\ndef get_gpus_max_memory(max_memory):\n    max_memory = {i: max_memory\
    \ for i in range(torch.cuda.device_count())}\n    return max_memory\n\n# for example\
    \ for a max use of 10GB per GPU\n# for fp16 replace with  `load_in_8bit=True`\
    \ with   `torch_dtype=torch.float16`\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"bigcode/starcoder\", \n    device_map=\"auto\", \n    load_in_8bit=True,\n\
    \    max_memory=get_gpus_max_memory(\"10GB\"),\n)\n```\nTo understand the logic\
    \ behind this check this [documentation](https://huggingface.co/docs/transformers/perf_infer_gpu_one#running-mixedint8-models-multi-gpu-setup)\
    \ or this [blog](https://huggingface.co/blog/accelerate-large-models)  for handling\
    \ large model inference."
  created_at: 2023-05-24 09:02:11+00:00
  edited: true
  hidden: false
  id: 646de0a3e34b2ec2d2d75803
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-06T21:58:35.000Z'
    data:
      status: closed
    id: 647fac0bc0e5f28c5789fdd3
    type: status-change
  author: loubnabnl
  created_at: 2023-06-06 20:58:35+00:00
  id: 647fac0bc0e5f28c5789fdd3
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46d55235cd81f7b5c81840aa409d64c2.svg
      fullname: Shivankar Pilligundla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shivankarzz
      type: user
    createdAt: '2023-09-10T10:11:09.000Z'
    data:
      edited: true
      editors:
      - shivankarzz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8052764534950256
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46d55235cd81f7b5c81840aa409d64c2.svg
          fullname: Shivankar Pilligundla
          isHf: false
          isPro: false
          name: shivankarzz
          type: user
        html: '<pre><code># pip install -q transformers

          from transformers import AutoModelForCausalLM, AutoTokenizer


          checkpoint = "bigcode/starcoder"

          device = "gpu" # for GPU usage or "cpu" for CPU usage


          tokenizer = AutoTokenizer.from_pretrained(checkpoint)

          model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True).to(device)

          </code></pre>

          <p>this code snippet is giving me the following error:<br>python3.10/site-packages/transformers/modeling_utils.py",
          line 2009, in to<br>    raise ValueError(<br>ValueError: <code>.to</code>
          is not supported for <code>4-bit</code> or <code>8-bit</code> bitsandbytes
          models. Please use the model as it is, since the model has already been
          set to the correct devices and casted to the correct <code>dtype</code></p>

          <p>I am unable to run without having load_in_8bit flag. I have a single
          A6000(48 gigs) gpu.</p>

          <p>can anyone please help me in inferencing with starcoder??</p>

          '
        raw: "```\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder\"\ndevice = \"gpu\"\
          \ # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True).to(device)\n\
          ```\n\nthis code snippet is giving me the following error: \npython3.10/site-packages/transformers/modeling_utils.py\"\
          , line 2009, in to\n    raise ValueError(\nValueError: `.to` is not supported\
          \ for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it\
          \ is, since the model has already been set to the correct devices and casted\
          \ to the correct `dtype`\n\nI am unable to run without having load_in_8bit\
          \ flag. I have a single A6000(48 gigs) gpu.\n\ncan anyone please help me\
          \ in inferencing with starcoder??"
        updatedAt: '2023-09-10T10:12:22.899Z'
      numEdits: 1
      reactions: []
    id: 64fd963d31a82e0d406b8ee8
    type: comment
  author: shivankarzz
  content: "```\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder\"\ndevice = \"gpu\" # for\
    \ GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
    model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True).to(device)\n\
    ```\n\nthis code snippet is giving me the following error: \npython3.10/site-packages/transformers/modeling_utils.py\"\
    , line 2009, in to\n    raise ValueError(\nValueError: `.to` is not supported\
    \ for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since\
    \ the model has already been set to the correct devices and casted to the correct\
    \ `dtype`\n\nI am unable to run without having load_in_8bit flag. I have a single\
    \ A6000(48 gigs) gpu.\n\ncan anyone please help me in inferencing with starcoder??"
  created_at: 2023-09-10 09:11:09+00:00
  edited: true
  hidden: false
  id: 64fd963d31a82e0d406b8ee8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/006866a163e2384066dfd3f2665e6427.svg
      fullname: "Tomasz Zieli\u0144ski"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Warmonger
      type: user
    createdAt: '2023-10-18T13:24:48.000Z'
    data:
      edited: false
      editors:
      - Warmonger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9561389684677124
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/006866a163e2384066dfd3f2665e6427.svg
          fullname: "Tomasz Zieli\u0144ski"
          isHf: false
          isPro: false
          name: Warmonger
          type: user
        html: '<p>To report progress after half a year:</p>

          <ul>

          <li>I was able to run multiple small models (7b) quickly and flawlessly
          on my RTX 4080, using LM Studio server out-of-the-box. Could do up to 10b
          quantized, but these models are not common for some reason.</li>

          <li>Their utility is questionable, though. For sure they are not reliable
          as a base of any useful system or process, even if only for personal use.</li>

          <li>Usable only for experiments, learning or just fun.</li>

          </ul>

          '
        raw: 'To report progress after half a year:


          - I was able to run multiple small models (7b) quickly and flawlessly on
          my RTX 4080, using LM Studio server out-of-the-box. Could do up to 10b quantized,
          but these models are not common for some reason.

          - Their utility is questionable, though. For sure they are not reliable
          as a base of any useful system or process, even if only for personal use.

          - Usable only for experiments, learning or just fun.'
        updatedAt: '2023-10-18T13:24:48.475Z'
      numEdits: 0
      reactions: []
    id: 652fdca05615e57807e7c768
    type: comment
  author: Warmonger
  content: 'To report progress after half a year:


    - I was able to run multiple small models (7b) quickly and flawlessly on my RTX
    4080, using LM Studio server out-of-the-box. Could do up to 10b quantized, but
    these models are not common for some reason.

    - Their utility is questionable, though. For sure they are not reliable as a base
    of any useful system or process, even if only for personal use.

    - Usable only for experiments, learning or just fun.'
  created_at: 2023-10-18 12:24:48+00:00
  edited: false
  hidden: false
  id: 652fdca05615e57807e7c768
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: GPU requirement
