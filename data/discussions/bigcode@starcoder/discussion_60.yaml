!!python/object:huggingface_hub.community.DiscussionWithDetails
author: djokowsj90
conflicting_files: null
created_at: 2023-07-02 18:44:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dcb60e8d4250bc94b220ebf7a521c3a.svg
      fullname: Jeremy Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djokowsj90
      type: user
    createdAt: '2023-07-02T19:44:43.000Z'
    data:
      edited: false
      editors:
      - djokowsj90
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9101722836494446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dcb60e8d4250bc94b220ebf7a521c3a.svg
          fullname: Jeremy Wang
          isHf: false
          isPro: false
          name: djokowsj90
          type: user
        html: '<p>I got an error when deploying this model to AWS Sagemaker.</p>

          <p>"No safetensors weights found for model bigcode/starcoder at revision
          None. Converting PyTorch weights to safetensors."</p>

          <p>It seems Sagemaker expects one bin file "model.pth" or "pytorch_model.bin"<br>but
          this repo has many bin files like "pytorch_model-00003-of-00007.bin" etc..<br>I
          don''t think I can simply contact those bin files.<br>Anyone has encountered
          this issue?</p>

          '
        raw: "I got an error when deploying this model to AWS Sagemaker.\r\n\r\n\"\
          No safetensors weights found for model bigcode/starcoder at revision None.\
          \ Converting PyTorch weights to safetensors.\"\r\n\r\nIt seems Sagemaker\
          \ expects one bin file \"model.pth\" or \"pytorch_model.bin\" \r\nbut this\
          \ repo has many bin files like \"pytorch_model-00003-of-00007.bin\" etc..\r\
          \nI don't think I can simply contact those bin files.\r\nAnyone has encountered\
          \ this issue?"
        updatedAt: '2023-07-02T19:44:43.308Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FarziBuilder
    id: 64a1d3abd8aea615f320d0a9
    type: comment
  author: djokowsj90
  content: "I got an error when deploying this model to AWS Sagemaker.\r\n\r\n\"No\
    \ safetensors weights found for model bigcode/starcoder at revision None. Converting\
    \ PyTorch weights to safetensors.\"\r\n\r\nIt seems Sagemaker expects one bin\
    \ file \"model.pth\" or \"pytorch_model.bin\" \r\nbut this repo has many bin files\
    \ like \"pytorch_model-00003-of-00007.bin\" etc..\r\nI don't think I can simply\
    \ contact those bin files.\r\nAnyone has encountered this issue?"
  created_at: 2023-07-02 18:44:43+00:00
  edited: false
  hidden: false
  id: 64a1d3abd8aea615f320d0a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667040186339-noauth.jpeg?w=200&h=200&f=face
      fullname: Ahmad Faraaz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FarziBuilder
      type: user
    createdAt: '2023-07-06T14:43:48.000Z'
    data:
      edited: false
      editors:
      - FarziBuilder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9988018274307251
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667040186339-noauth.jpeg?w=200&h=200&f=face
          fullname: Ahmad Faraaz
          isHf: false
          isPro: false
          name: FarziBuilder
          type: user
        html: '<p>I also faced, don''t know how to solve it</p>

          '
        raw: I also faced, don't know how to solve it
        updatedAt: '2023-07-06T14:43:48.949Z'
      numEdits: 0
      reactions: []
    id: 64a6d324b869cc3dd66f1637
    type: comment
  author: FarziBuilder
  content: I also faced, don't know how to solve it
  created_at: 2023-07-06 13:43:48+00:00
  edited: false
  hidden: false
  id: 64a6d324b869cc3dd66f1637
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dcb60e8d4250bc94b220ebf7a521c3a.svg
      fullname: Jeremy Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djokowsj90
      type: user
    createdAt: '2023-07-08T05:14:46.000Z'
    data:
      edited: false
      editors:
      - djokowsj90
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7822999358177185
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dcb60e8d4250bc94b220ebf7a521c3a.svg
          fullname: Jeremy Wang
          isHf: false
          isPro: false
          name: djokowsj90
          type: user
        html: "<p>I passed this error.<br>Sagemaker will actually do the conversion\
          \ for you. But you need to give it more time.</p>\n<pre><code>predictor\
          \ = huggingface_model.deploy(\n    initial_instance_count=1,\n    instance_type=\"\
          ml.g5.8xlarge\",\n    container_startup_health_check_timeout=1200,\n  )\
          \ \n</code></pre>\n<p>Set up the container_startup_health_check_timeout\
          \ to a bigger number and it will pass this error.</p>\n<p>But I encountered\
          \ the next error</p>\n<pre><code>torch.cuda.OutOfMemoryError: CUDA out of\
          \ memory. Tried to allocate 288.00 MiB (GPU 0; 22.20 GiB total capacity;\
          \ 19.72 GiB already allocated; 143.12 MiB free; \n21.11 GiB reserved in\
          \ total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  \nSee documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n</code></pre>\n<p>I\
          \ upgraded to a bigger instance type, and played with param PYTORCH_CUDA_ALLOC_CONF\
          \ but the error persisted.<br>Let me know if you see the same error. </p>\n"
        raw: "I passed this error.\nSagemaker will actually do the conversion for\
          \ you. But you need to give it more time.\n```\npredictor = huggingface_model.deploy(\n\
          \tinitial_instance_count=1,\n\tinstance_type=\"ml.g5.8xlarge\",\n\tcontainer_startup_health_check_timeout=1200,\n\
          \  ) \n```\nSet up the container_startup_health_check_timeout to a bigger\
          \ number and it will pass this error.\n\nBut I encountered the next error\n\
          ```\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 288.00 MiB (GPU 0; 22.20 GiB total capacity; 19.72 GiB already allocated;\
          \ 143.12 MiB free; \n21.11 GiB reserved in total by PyTorch) If reserved\
          \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  \nSee documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          ```\n\nI upgraded to a bigger instance type, and played with param PYTORCH_CUDA_ALLOC_CONF\
          \ but the error persisted.  \nLet me know if you see the same error. \n"
        updatedAt: '2023-07-08T05:14:46.939Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FarziBuilder
    id: 64a8f0c631a43d7265f68bca
    type: comment
  author: djokowsj90
  content: "I passed this error.\nSagemaker will actually do the conversion for you.\
    \ But you need to give it more time.\n```\npredictor = huggingface_model.deploy(\n\
    \tinitial_instance_count=1,\n\tinstance_type=\"ml.g5.8xlarge\",\n\tcontainer_startup_health_check_timeout=1200,\n\
    \  ) \n```\nSet up the container_startup_health_check_timeout to a bigger number\
    \ and it will pass this error.\n\nBut I encountered the next error\n```\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 22.20 GiB total capacity;\
    \ 19.72 GiB already allocated; 143.12 MiB free; \n21.11 GiB reserved in total\
    \ by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  \nSee documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    ```\n\nI upgraded to a bigger instance type, and played with param PYTORCH_CUDA_ALLOC_CONF\
    \ but the error persisted.  \nLet me know if you see the same error. \n"
  created_at: 2023-07-08 04:14:46+00:00
  edited: false
  hidden: false
  id: 64a8f0c631a43d7265f68bca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
      fullname: Zak KRider
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkrider
      type: user
    createdAt: '2023-07-31T18:57:14.000Z'
    data:
      edited: false
      editors:
      - zkrider
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8919632434844971
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
          fullname: Zak KRider
          isHf: false
          isPro: false
          name: zkrider
          type: user
        html: '<p>It worked by putting it on the AWS instance type: ml.g4dn.12xlarge
          and setting SM_NUM_GPUS: "4"</p>

          '
        raw: 'It worked by putting it on the AWS instance type: ml.g4dn.12xlarge and
          setting SM_NUM_GPUS: "4"'
        updatedAt: '2023-07-31T18:57:14.016Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - djokowsj90
    id: 64c8040a3a3f428da65279bc
    type: comment
  author: zkrider
  content: 'It worked by putting it on the AWS instance type: ml.g4dn.12xlarge and
    setting SM_NUM_GPUS: "4"'
  created_at: 2023-07-31 17:57:14+00:00
  edited: false
  hidden: false
  id: 64c8040a3a3f428da65279bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dcb60e8d4250bc94b220ebf7a521c3a.svg
      fullname: Jeremy Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djokowsj90
      type: user
    createdAt: '2023-08-01T15:25:07.000Z'
    data:
      edited: false
      editors:
      - djokowsj90
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9686277508735657
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dcb60e8d4250bc94b220ebf7a521c3a.svg
          fullname: Jeremy Wang
          isHf: false
          isPro: false
          name: djokowsj90
          type: user
        html: '<p>Yes, I got it worked with these configs. Thank you so much~</p>

          '
        raw: Yes, I got it worked with these configs. Thank you so much~
        updatedAt: '2023-08-01T15:25:07.276Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c923d3c547ed5243f4603b
    id: 64c923d3c547ed5243f4603a
    type: comment
  author: djokowsj90
  content: Yes, I got it worked with these configs. Thank you so much~
  created_at: 2023-08-01 14:25:07+00:00
  edited: false
  hidden: false
  id: 64c923d3c547ed5243f4603a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9dcb60e8d4250bc94b220ebf7a521c3a.svg
      fullname: Jeremy Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djokowsj90
      type: user
    createdAt: '2023-08-01T15:25:07.000Z'
    data:
      status: closed
    id: 64c923d3c547ed5243f4603b
    type: status-change
  author: djokowsj90
  created_at: 2023-08-01 14:25:07+00:00
  id: 64c923d3c547ed5243f4603b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 60
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: Errors when deploying to AWS Sagemaker
