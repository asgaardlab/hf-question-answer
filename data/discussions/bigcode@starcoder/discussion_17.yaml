!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shailja
conflicting_files: null
created_at: 2023-05-08 16:28:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
      fullname: Shailja Thakur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shailja
      type: user
    createdAt: '2023-05-08T17:28:11.000Z'
    data:
      edited: false
      editors:
      - shailja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
          fullname: Shailja Thakur
          isHf: false
          isPro: false
          name: shailja
          type: user
        html: '<hr>

          <p>KeyError                                  Traceback (most recent call
          last)<br>/tmp/ipykernel_1050494/2718782402.py in <br>      2 tokenizer =
          AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)<br>      3
          # to save memory consider using fp16 or bf16 by specifying torch.dtype=torch.float16
          for example<br>----&gt; 4 model = AutoModelForCausalLM.from_pretrained(checkpoint,use_auth_token=True).to(device)</p>

          <p>~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py
          in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)<br>    421         kwargs["_from_auto"]
          = True<br>    422         if not isinstance(config, PretrainedConfig):<br>--&gt;
          423             config, kwargs = AutoConfig.from_pretrained(<br>    424                 pretrained_model_name_or_path,
          return_unused_kwargs=True, trust_remote_code=trust_remote_code, **kwargs<br>    425             )</p>

          <p>~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py
          in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>    743             return
          config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)<br>    744         elif
          "model_type" in config_dict:<br>--&gt; 745             config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>    746             return
          config_class.from_dict(config_dict, **kwargs)<br>    747         else:</p>

          <p>~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py
          in <strong>getitem</strong>(self, key)<br>    450             return self._extra_content[key]<br>    451         if
          key not in self._mapping:<br>--&gt; 452             raise KeyError(key)<br>    453         value
          = self._mapping[key]<br>    454         module_name = model_type_to_module_name(key)</p>

          <p>KeyError: ''gpt_bigcode''</p>

          '
        raw: "---------------------------------------------------------------------------\r\
          \nKeyError                                  Traceback (most recent call\
          \ last)\r\n/tmp/ipykernel_1050494/2718782402.py in <module>\r\n      2 tokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)\r\n  \
          \    3 # to save memory consider using fp16 or bf16 by specifying torch.dtype=torch.float16\
          \ for example\r\n----> 4 model = AutoModelForCausalLM.from_pretrained(checkpoint,use_auth_token=True).to(device)\r\
          \n\r\n~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
          \n    421         kwargs[\"_from_auto\"] = True\r\n    422         if not\
          \ isinstance(config, PretrainedConfig):\r\n--> 423             config, kwargs\
          \ = AutoConfig.from_pretrained(\r\n    424                 pretrained_model_name_or_path,\
          \ return_unused_kwargs=True, trust_remote_code=trust_remote_code, **kwargs\r\
          \n    425             )\r\n\r\n~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\n \
          \   743             return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\r\n    744         elif \"model_type\" in config_dict:\r\n-->\
          \ 745             config_class = CONFIG_MAPPING[config_dict[\"model_type\"\
          ]]\r\n    746             return config_class.from_dict(config_dict, **kwargs)\r\
          \n    747         else:\r\n\r\n~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\
          \ in __getitem__(self, key)\r\n    450             return self._extra_content[key]\r\
          \n    451         if key not in self._mapping:\r\n--> 452             raise\
          \ KeyError(key)\r\n    453         value = self._mapping[key]\r\n    454\
          \         module_name = model_type_to_module_name(key)\r\n\r\nKeyError:\
          \ 'gpt_bigcode'\r\n"
        updatedAt: '2023-05-08T17:28:11.173Z'
      numEdits: 0
      reactions: []
    id: 6459312b6fa580137af81c72
    type: comment
  author: shailja
  content: "---------------------------------------------------------------------------\r\
    \nKeyError                                  Traceback (most recent call last)\r\
    \n/tmp/ipykernel_1050494/2718782402.py in <module>\r\n      2 tokenizer = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)\r\
    \n      3 # to save memory consider using fp16 or bf16 by specifying torch.dtype=torch.float16\
    \ for example\r\n----> 4 model = AutoModelForCausalLM.from_pretrained(checkpoint,use_auth_token=True).to(device)\r\
    \n\r\n~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
    \n    421         kwargs[\"_from_auto\"] = True\r\n    422         if not isinstance(config,\
    \ PretrainedConfig):\r\n--> 423             config, kwargs = AutoConfig.from_pretrained(\r\
    \n    424                 pretrained_model_name_or_path, return_unused_kwargs=True,\
    \ trust_remote_code=trust_remote_code, **kwargs\r\n    425             )\r\n\r\
    \n~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\n    743\
    \             return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\r\n    744         elif \"model_type\" in config_dict:\r\n--> 745\
    \             config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n \
    \   746             return config_class.from_dict(config_dict, **kwargs)\r\n \
    \   747         else:\r\n\r\n~/anaconda3/envs/verilog_gpt/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\
    \ in __getitem__(self, key)\r\n    450             return self._extra_content[key]\r\
    \n    451         if key not in self._mapping:\r\n--> 452             raise KeyError(key)\r\
    \n    453         value = self._mapping[key]\r\n    454         module_name =\
    \ model_type_to_module_name(key)\r\n\r\nKeyError: 'gpt_bigcode'\r\n"
  created_at: 2023-05-08 16:28:11+00:00
  edited: false
  hidden: false
  id: 6459312b6fa580137af81c72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59a8b7893e80f37b0d856b01d764a2ca.svg
      fullname: Vidhi Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vidhij
      type: user
    createdAt: '2023-05-09T00:57:56.000Z'
    data:
      edited: false
      editors:
      - vidhij
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59a8b7893e80f37b0d856b01d764a2ca.svg
          fullname: Vidhi Jain
          isHf: false
          isPro: false
          name: vidhij
          type: user
        html: "<p>I am receiving the same error. </p>\n<pre><code>8 tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          ----&gt; 9 model = AutoModelForCausalLM.from_pretrained(checkpoint)#.to(device)\n\
          \nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    438     if kwargs_copy.get(\"torch_dtype\"\
          , None) == \"auto\":\n    439         _ = kwargs_copy.pop(\"torch_dtype\"\
          )\n--&gt; 441     config, kwargs = AutoConfig.from_pretrained(\n    442\
          \         pretrained_model_name_or_path,\n    443         return_unused_kwargs=True,\n\
          \    444         trust_remote_code=trust_remote_code,\n    445         **hub_kwargs,\n\
          \    446         **kwargs_copy,\n    447     )\n    448 if hasattr(config,\
          \ \"auto_map\") and cls.__name__ in config.auto_map:\n    449     if not\
          \ trust_remote_code:\n\nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:911,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    909     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n    910 elif \"model_type\" in config_dict:\n--&gt; 911   \
          \  config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    912 \
          \    return config_class.from_dict(config_dict, **unused_kwargs)\n    913\
          \ else:\n    914     # Fallback: use pattern matching on the string.\n \
          \   915     # We go from longer names to shorter names to catch roberta\
          \ before bert (for instance)\n\nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:617,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    615     return self._extra_content[key]\n\
          \    616 if key not in self._mapping:\n--&gt; 617     raise KeyError(key)\n\
          \    618 value = self._mapping[key]\n    619 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'gpt_bigcode'\n</code></pre>\n"
        raw: "I am receiving the same error. \n```\n8 tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          ----> 9 model = AutoModelForCausalLM.from_pretrained(checkpoint)#.to(device)\n\
          \nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    438     if kwargs_copy.get(\"torch_dtype\"\
          , None) == \"auto\":\n    439         _ = kwargs_copy.pop(\"torch_dtype\"\
          )\n--> 441     config, kwargs = AutoConfig.from_pretrained(\n    442   \
          \      pretrained_model_name_or_path,\n    443         return_unused_kwargs=True,\n\
          \    444         trust_remote_code=trust_remote_code,\n    445         **hub_kwargs,\n\
          \    446         **kwargs_copy,\n    447     )\n    448 if hasattr(config,\
          \ \"auto_map\") and cls.__name__ in config.auto_map:\n    449     if not\
          \ trust_remote_code:\n\nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:911,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    909     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n    910 elif \"model_type\" in config_dict:\n--> 911     config_class\
          \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    912     return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n    913 else:\n    914     # Fallback: use pattern matching\
          \ on the string.\n    915     # We go from longer names to shorter names\
          \ to catch roberta before bert (for instance)\n\nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:617,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    615     return self._extra_content[key]\n\
          \    616 if key not in self._mapping:\n--> 617     raise KeyError(key)\n\
          \    618 value = self._mapping[key]\n    619 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'gpt_bigcode'\n```"
        updatedAt: '2023-05-09T00:57:56.231Z'
      numEdits: 0
      reactions: []
    id: 64599a9439e6aea69cc873e0
    type: comment
  author: vidhij
  content: "I am receiving the same error. \n```\n8 tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
    ----> 9 model = AutoModelForCausalLM.from_pretrained(checkpoint)#.to(device)\n\
    \nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    438     if kwargs_copy.get(\"torch_dtype\", None) == \"auto\"\
    :\n    439         _ = kwargs_copy.pop(\"torch_dtype\")\n--> 441     config, kwargs\
    \ = AutoConfig.from_pretrained(\n    442         pretrained_model_name_or_path,\n\
    \    443         return_unused_kwargs=True,\n    444         trust_remote_code=trust_remote_code,\n\
    \    445         **hub_kwargs,\n    446         **kwargs_copy,\n    447     )\n\
    \    448 if hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map:\n\
    \    449     if not trust_remote_code:\n\nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:911,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
    \    909     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\n    910 elif \"model_type\" in config_dict:\n--> 911     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    912     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\n    913 else:\n    914     # Fallback: use pattern matching\
    \ on the string.\n    915     # We go from longer names to shorter names to catch\
    \ roberta before bert (for instance)\n\nFile ~/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:617,\
    \ in _LazyConfigMapping.__getitem__(self, key)\n    615     return self._extra_content[key]\n\
    \    616 if key not in self._mapping:\n--> 617     raise KeyError(key)\n    618\
    \ value = self._mapping[key]\n    619 module_name = model_type_to_module_name(key)\n\
    \nKeyError: 'gpt_bigcode'\n```"
  created_at: 2023-05-08 23:57:56+00:00
  edited: false
  hidden: false
  id: 64599a9439e6aea69cc873e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd610ab92fa5bd1bfc23658e262e67b1.svg
      fullname: Khell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khellific
      type: user
    createdAt: '2023-05-09T07:49:45.000Z'
    data:
      edited: false
      editors:
      - khellific
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd610ab92fa5bd1bfc23658e262e67b1.svg
          fullname: Khell
          isHf: false
          isPro: false
          name: khellific
          type: user
        html: '<p>Install the latest version of <code>transformers</code> (should
          be 4.28.1): <a href="https://huggingface.co/docs/transformers/installation">https://huggingface.co/docs/transformers/installation</a></p>

          <p>e.g. <code>conda install -c huggingface transformers</code> installs
          4.28.1 instead of 4.24.0 with <code>conda install transformers</code></p>

          '
        raw: 'Install the latest version of `transformers` (should be 4.28.1): https://huggingface.co/docs/transformers/installation


          e.g. `conda install -c huggingface transformers` installs 4.28.1 instead
          of 4.24.0 with `conda install transformers`'
        updatedAt: '2023-05-09T07:49:45.847Z'
      numEdits: 0
      reactions: []
    id: 6459fb19bdf5b1fa58e95ac5
    type: comment
  author: khellific
  content: 'Install the latest version of `transformers` (should be 4.28.1): https://huggingface.co/docs/transformers/installation


    e.g. `conda install -c huggingface transformers` installs 4.28.1 instead of 4.24.0
    with `conda install transformers`'
  created_at: 2023-05-09 06:49:45+00:00
  edited: false
  hidden: false
  id: 6459fb19bdf5b1fa58e95ac5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
      fullname: Shailja Thakur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shailja
      type: user
    createdAt: '2023-05-09T10:47:03.000Z'
    data:
      edited: false
      editors:
      - shailja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
          fullname: Shailja Thakur
          isHf: false
          isPro: false
          name: shailja
          type: user
        html: '<p>I have the latest version (transformers-4.29.0.dev0) of transformer
          installed from source </p>

          <p>pip install git+<a rel="nofollow" href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p>

          '
        raw: "I have the latest version (transformers-4.29.0.dev0) of transformer\
          \ installed from source \n\npip install git+https://github.com/huggingface/transformers"
        updatedAt: '2023-05-09T10:47:03.032Z'
      numEdits: 0
      reactions: []
    id: 645a24a751b451042624d9d7
    type: comment
  author: shailja
  content: "I have the latest version (transformers-4.29.0.dev0) of transformer installed\
    \ from source \n\npip install git+https://github.com/huggingface/transformers"
  created_at: 2023-05-09 09:47:03+00:00
  edited: false
  hidden: false
  id: 645a24a751b451042624d9d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
      fullname: Shailja Thakur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shailja
      type: user
    createdAt: '2023-05-09T12:00:06.000Z'
    data:
      edited: false
      editors:
      - shailja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
          fullname: Shailja Thakur
          isHf: false
          isPro: false
          name: shailja
          type: user
        html: '<p>Its loading now, there was a conflicting lower version of transformer
          on my machine</p>

          '
        raw: Its loading now, there was a conflicting lower version of transformer
          on my machine
        updatedAt: '2023-05-09T12:00:06.674Z'
      numEdits: 0
      reactions: []
    id: 645a35c690116b4b42526ec0
    type: comment
  author: shailja
  content: Its loading now, there was a conflicting lower version of transformer on
    my machine
  created_at: 2023-05-09 11:00:06+00:00
  edited: false
  hidden: false
  id: 645a35c690116b4b42526ec0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
      fullname: Shailja Thakur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shailja
      type: user
    createdAt: '2023-05-09T12:00:30.000Z'
    data:
      edited: false
      editors:
      - shailja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
          fullname: Shailja Thakur
          isHf: false
          isPro: false
          name: shailja
          type: user
        html: '<p>It''s loading now, there was a conflicting lower version of the
          transformer on my machine.</p>

          '
        raw: It's loading now, there was a conflicting lower version of the transformer
          on my machine.
        updatedAt: '2023-05-09T12:00:30.530Z'
      numEdits: 0
      reactions: []
    id: 645a35de67a99bdbb9ccb956
    type: comment
  author: shailja
  content: It's loading now, there was a conflicting lower version of the transformer
    on my machine.
  created_at: 2023-05-09 11:00:30+00:00
  edited: false
  hidden: false
  id: 645a35de67a99bdbb9ccb956
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
      fullname: Shailja Thakur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shailja
      type: user
    createdAt: '2023-05-09T12:04:55.000Z'
    data:
      edited: false
      editors:
      - shailja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
          fullname: Shailja Thakur
          isHf: false
          isPro: false
          name: shailja
          type: user
        html: '<p>It''s loading now, there was a conflicting lower version of the
          transformer on my machine.</p>

          '
        raw: 'It''s loading now, there was a conflicting lower version of the transformer
          on my machine.

          '
        updatedAt: '2023-05-09T12:04:55.993Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645a36e70f592b2426750145
    id: 645a36e70f592b2426750144
    type: comment
  author: shailja
  content: 'It''s loading now, there was a conflicting lower version of the transformer
    on my machine.

    '
  created_at: 2023-05-09 11:04:55+00:00
  edited: false
  hidden: false
  id: 645a36e70f592b2426750144
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/73909cdde3074ffefed746400e42a909.svg
      fullname: Shailja Thakur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shailja
      type: user
    createdAt: '2023-05-09T12:04:55.000Z'
    data:
      status: closed
    id: 645a36e70f592b2426750145
    type: status-change
  author: shailja
  created_at: 2023-05-09 11:04:55+00:00
  id: 645a36e70f592b2426750145
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: bigcode/starcoder
repo_type: model
status: closed
target_branch: null
title: 'Unable to load model, error '
