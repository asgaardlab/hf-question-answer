!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ijkucsc
conflicting_files: null
created_at: 2023-11-27 20:55:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/197db8cdbd080516b496fd81129ac879.svg
      fullname: Isaac Karth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ijkucsc
      type: user
    createdAt: '2023-11-27T20:55:46.000Z'
    data:
      edited: false
      editors:
      - ijkucsc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609882831573486
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/197db8cdbd080516b496fd81129ac879.svg
          fullname: Isaac Karth
          isHf: false
          isPro: false
          name: ijkucsc
          type: user
        html: '<p>Good work on this, looking forward to your continued R&amp;D. It''s
          great that you''ve been documenting it with your Substack posts.</p>

          <p>You''re kind of an expert at training these now, so I wanted to ask:
          I''ve been looking for information on training for a 200k context length.
          Can you share any information about the code and settings you used to make
          this? Is there a special repo or can it be done off the shelf with existing
          training code?</p>

          '
        raw: "Good work on this, looking forward to your continued R&D. It's great\
          \ that you've been documenting it with your Substack posts.\r\n\r\nYou're\
          \ kind of an expert at training these now, so I wanted to ask: I've been\
          \ looking for information on training for a 200k context length. Can you\
          \ share any information about the code and settings you used to make this?\
          \ Is there a special repo or can it be done off the shelf with existing\
          \ training code?"
        updatedAt: '2023-11-27T20:55:46.284Z'
      numEdits: 0
      reactions: []
    id: 65650252f483a5f0c6ad1b99
    type: comment
  author: ijkucsc
  content: "Good work on this, looking forward to your continued R&D. It's great that\
    \ you've been documenting it with your Substack posts.\r\n\r\nYou're kind of an\
    \ expert at training these now, so I wanted to ask: I've been looking for information\
    \ on training for a 200k context length. Can you share any information about the\
    \ code and settings you used to make this? Is there a special repo or can it be\
    \ done off the shelf with existing training code?"
  created_at: 2023-11-27 20:55:46+00:00
  edited: false
  hidden: false
  id: 65650252f483a5f0c6ad1b99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-11-27T21:39:01.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9347091913223267
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Hey,</p>

          <p>If you want to train at 200K context length, that will require a massive
          amount of GPU resources. </p>

          <p>Instead, what you can do is to find a base model that has been trained
          on longer context length, and then fine-tune that model over your samples
          -- you can use a context length that is reasonable (like 4096 or 8192).
          That way you still get the benefits of the model being able to handle larger
          context lengths, and can be fine-tuned with a reasonable amount of GPUs,
          say on a 4xA100 machine.</p>

          <p>Thanks,<br>Migel</p>

          '
        raw: "Hey,\n\nIf you want to train at 200K context length, that will require\
          \ a massive amount of GPU resources. \n\nInstead, what you can do is to\
          \ find a base model that has been trained on longer context length, and\
          \ then fine-tune that model over your samples -- you can use a context length\
          \ that is reasonable (like 4096 or 8192). That way you still get the benefits\
          \ of the model being able to handle larger context lengths, and can be fine-tuned\
          \ with a reasonable amount of GPUs, say on a 4xA100 machine.\n\nThanks,\n\
          Migel"
        updatedAt: '2023-11-27T21:39:01.094Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65650c750645dd043af546f7
    id: 65650c750645dd043af546f3
    type: comment
  author: migtissera
  content: "Hey,\n\nIf you want to train at 200K context length, that will require\
    \ a massive amount of GPU resources. \n\nInstead, what you can do is to find a\
    \ base model that has been trained on longer context length, and then fine-tune\
    \ that model over your samples -- you can use a context length that is reasonable\
    \ (like 4096 or 8192). That way you still get the benefits of the model being\
    \ able to handle larger context lengths, and can be fine-tuned with a reasonable\
    \ amount of GPUs, say on a 4xA100 machine.\n\nThanks,\nMigel"
  created_at: 2023-11-27 21:39:01+00:00
  edited: false
  hidden: false
  id: 65650c750645dd043af546f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-11-27T21:39:01.000Z'
    data:
      status: closed
    id: 65650c750645dd043af546f7
    type: status-change
  author: migtissera
  created_at: 2023-11-27 21:39:01+00:00
  id: 65650c750645dd043af546f7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: migtissera/Tess-M-v1.3
repo_type: model
status: closed
target_branch: null
title: Training at 200k?
