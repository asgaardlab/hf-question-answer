!!python/object:huggingface_hub.community.DiscussionWithDetails
author: do-me
conflicting_files: null
created_at: 2023-11-25 18:06:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
      fullname: "Dominik Weckm\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: do-me
      type: user
    createdAt: '2023-11-25T18:06:50.000Z'
    data:
      edited: false
      editors:
      - do-me
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4687296748161316
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
          fullname: "Dominik Weckm\xFCller"
          isHf: false
          isPro: false
          name: do-me
          type: user
        html: "<p>I tried to run this model on my CUDA GPU but get this error when\
          \ running the sample code. Do you have any ideas why? </p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModelWithLMHead,\
          \ TranslationPipeline\n\npipeline = TranslationPipeline(\nmodel=AutoModelWithLMHead.from_pretrained(<span\
          \ class=\"hljs-string\">\"SEBIS/legal_t5_small_trans_it_en_small_finetuned\"\
          </span>),\ntokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path\
          \ = <span class=\"hljs-string\">\"SEBIS/legal_t5_small_trans_it_en\"</span>,\
          \ do_lower_case=<span class=\"hljs-literal\">False</span>, \n          \
          \                                  skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>),\n    device=<span class=\"hljs-number\">0</span>\n)\n\nit_text\
          \ = <span class=\"hljs-string\">\"Supplenti presenti al momento della votazione\
          \ finale\"</span>\n\npipeline([it_text], max_length=<span class=\"hljs-number\"\
          >512</span>)\n</code></pre>\n<pre><code>---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          /tmp/ipykernel_2819/3211885403.py in &lt;module&gt;\n      3 pipeline =\
          \ TranslationPipeline(\n      4 model=AutoModelWithLMHead.from_pretrained(\"\
          SEBIS/legal_t5_small_trans_it_en_small_finetuned\"),\n----&gt; 5 tokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path\
          \ = \"SEBIS/legal_t5_small_trans_it_en\", do_lower_case=False, \n      6\
          \                                             skip_special_tokens=True),\n\
          \      7     device=0\n\n~/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\n\
          \    657             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\n\
          \    658             if tokenizer_class_fast and (use_fast or tokenizer_class_py\
          \ is None):\n--&gt; 659                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    660             else:\n    661               \
          \  if tokenizer_class_py is not None:\n\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\n\
          \   1799                 logger.info(f\"loading file {file_path} from cache\
          \ at {resolved_vocab_files[file_id]}\")\n   1800 \n-&gt; 1801         return\
          \ cls._from_pretrained(\n   1802             resolved_vocab_files,\n   1803\
          \             pretrained_model_name_or_path,\n\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
          \ *init_inputs, **kwargs)\n   1954         # Instantiate tokenizer.\n  \
          \ 1955         try:\n-&gt; 1956             tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n   1957         except OSError:\n   1958             raise\
          \ OSError(\n\n~/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py\
          \ in __init__(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token,\
          \ extra_ids, additional_special_tokens, **kwargs)\n    131             \
          \    )\n    132 \n--&gt; 133         super().__init__(\n    134        \
          \     vocab_file,\n    135             tokenizer_file=tokenizer_file,\n\n\
          ~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in __init__(self, *args, **kwargs)\n    112         elif slow_tokenizer\
          \ is not None:\n    113             # We need to convert a slow tokenizer\
          \ to build the backend\n--&gt; 114             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n\
          \    115         elif self.slow_tokenizer_class is not None:\n    116  \
          \           # We need to create and convert a slow tokenizer to build the\
          \ backend\n\n~/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py\
          \ in convert_slow_tokenizer(transformer_tokenizer)\n   1160     converter_class\
          \ = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n   1161 \n-&gt; 1162\
          \     return converter_class(transformer_tokenizer).converted()\n\n~/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py\
          \ in __init__(self, *args)\n    436         super().__init__(*args)\n  \
          \  437 \n--&gt; 438         from .utils import sentencepiece_model_pb2 as\
          \ model_pb2\n    439 \n    440         m = model_pb2.ModelProto()\n\n~/.local/lib/python3.8/site-packages/transformers/utils/sentencepiece_model_pb2.py\
          \ in &lt;module&gt;\n     90     create_key=_descriptor._internal_create_key,\n\
          \     91     values=[\n---&gt; 92         _descriptor.EnumValueDescriptor(\n\
          \     93             name=\"UNIGRAM\",\n     94             index=0,\n\n\
          ~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py in __new__(cls,\
          \ name, index, number, type, options, serialized_options, create_key)\n\
          \    794                 type=None,  # pylint: disable=redefined-builtin\n\
          \    795                 options=None, serialized_options=None, create_key=None):\n\
          --&gt; 796       _message.Message._CheckCalledFromGeneratedFile()\n    797\
          \       # There is no way we can build a complete EnumValueDescriptor with\
          \ the\n    798       # given parameters (the name of the Enum is not known,\
          \ for example).\n\nTypeError: Descriptors cannot not be created directly.\n\
          If this call came from a _pb2.py file, your generated code is out of date\
          \ and must be regenerated with protoc &gt;= 3.19.0.\nIf you cannot immediately\
          \ regenerate your protos, some other possible workarounds are:\n 1. Downgrade\
          \ the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\
          \ (but this will use pure-Python parsing and will be much slower).\n\nMore\
          \ information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n\
          </code></pre>\n"
        raw: "I tried to run this model on my CUDA GPU but get this error when running\
          \ the sample code. Do you have any ideas why? \r\n\r\n```python\r\nfrom\
          \ transformers import AutoTokenizer, AutoModelWithLMHead, TranslationPipeline\r\
          \n\r\npipeline = TranslationPipeline(\r\nmodel=AutoModelWithLMHead.from_pretrained(\"\
          SEBIS/legal_t5_small_trans_it_en_small_finetuned\"),\r\ntokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path\
          \ = \"SEBIS/legal_t5_small_trans_it_en\", do_lower_case=False, \r\n    \
          \                                        skip_special_tokens=True),\r\n\
          \    device=0\r\n)\r\n\r\nit_text = \"Supplenti presenti al momento della\
          \ votazione finale\"\r\n\r\npipeline([it_text], max_length=512)\r\n\r\n\
          ```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
          \nTypeError                                 Traceback (most recent call\
          \ last)\r\n/tmp/ipykernel_2819/3211885403.py in <module>\r\n      3 pipeline\
          \ = TranslationPipeline(\r\n      4 model=AutoModelWithLMHead.from_pretrained(\"\
          SEBIS/legal_t5_small_trans_it_en_small_finetuned\"),\r\n----> 5 tokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path\
          \ = \"SEBIS/legal_t5_small_trans_it_en\", do_lower_case=False, \r\n    \
          \  6                                             skip_special_tokens=True),\r\
          \n      7     device=0\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\
          \n    657             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\
          \n    658             if tokenizer_class_fast and (use_fast or tokenizer_class_py\
          \ is None):\r\n--> 659                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    660             else:\r\n    661           \
          \      if tokenizer_class_py is not None:\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\
          \n   1799                 logger.info(f\"loading file {file_path} from cache\
          \ at {resolved_vocab_files[file_id]}\")\r\n   1800 \r\n-> 1801         return\
          \ cls._from_pretrained(\r\n   1802             resolved_vocab_files,\r\n\
          \   1803             pretrained_model_name_or_path,\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
          \ *init_inputs, **kwargs)\r\n   1954         # Instantiate tokenizer.\r\n\
          \   1955         try:\r\n-> 1956             tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   1957         except OSError:\r\n   1958         \
          \    raise OSError(\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py\
          \ in __init__(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token,\
          \ extra_ids, additional_special_tokens, **kwargs)\r\n    131           \
          \      )\r\n    132 \r\n--> 133         super().__init__(\r\n    134   \
          \          vocab_file,\r\n    135             tokenizer_file=tokenizer_file,\r\
          \n\r\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in __init__(self, *args, **kwargs)\r\n    112         elif slow_tokenizer\
          \ is not None:\r\n    113             # We need to convert a slow tokenizer\
          \ to build the backend\r\n--> 114             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
          \n    115         elif self.slow_tokenizer_class is not None:\r\n    116\
          \             # We need to create and convert a slow tokenizer to build\
          \ the backend\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py\
          \ in convert_slow_tokenizer(transformer_tokenizer)\r\n   1160     converter_class\
          \ = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\r\n   1161 \r\n-> 1162\
          \     return converter_class(transformer_tokenizer).converted()\r\n\r\n\
          ~/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py\
          \ in __init__(self, *args)\r\n    436         super().__init__(*args)\r\n\
          \    437 \r\n--> 438         from .utils import sentencepiece_model_pb2\
          \ as model_pb2\r\n    439 \r\n    440         m = model_pb2.ModelProto()\r\
          \n\r\n~/.local/lib/python3.8/site-packages/transformers/utils/sentencepiece_model_pb2.py\
          \ in <module>\r\n     90     create_key=_descriptor._internal_create_key,\r\
          \n     91     values=[\r\n---> 92         _descriptor.EnumValueDescriptor(\r\
          \n     93             name=\"UNIGRAM\",\r\n     94             index=0,\r\
          \n\r\n~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py\
          \ in __new__(cls, name, index, number, type, options, serialized_options,\
          \ create_key)\r\n    794                 type=None,  # pylint: disable=redefined-builtin\r\
          \n    795                 options=None, serialized_options=None, create_key=None):\r\
          \n--> 796       _message.Message._CheckCalledFromGeneratedFile()\r\n   \
          \ 797       # There is no way we can build a complete EnumValueDescriptor\
          \ with the\r\n    798       # given parameters (the name of the Enum is\
          \ not known, for example).\r\n\r\nTypeError: Descriptors cannot not be created\
          \ directly.\r\nIf this call came from a _pb2.py file, your generated code\
          \ is out of date and must be regenerated with protoc >= 3.19.0.\r\nIf you\
          \ cannot immediately regenerate your protos, some other possible workarounds\
          \ are:\r\n 1. Downgrade the protobuf package to 3.20.x or lower.\r\n 2.\
          \ Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python\
          \ parsing and will be much slower).\r\n\r\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\r\
          \n```"
        updatedAt: '2023-11-25T18:06:50.921Z'
      numEdits: 0
      reactions: []
    id: 656237bad5e3c35c7910a646
    type: comment
  author: do-me
  content: "I tried to run this model on my CUDA GPU but get this error when running\
    \ the sample code. Do you have any ideas why? \r\n\r\n```python\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelWithLMHead, TranslationPipeline\r\n\r\npipeline\
    \ = TranslationPipeline(\r\nmodel=AutoModelWithLMHead.from_pretrained(\"SEBIS/legal_t5_small_trans_it_en_small_finetuned\"\
    ),\r\ntokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path =\
    \ \"SEBIS/legal_t5_small_trans_it_en\", do_lower_case=False, \r\n            \
    \                                skip_special_tokens=True),\r\n    device=0\r\n\
    )\r\n\r\nit_text = \"Supplenti presenti al momento della votazione finale\"\r\n\
    \r\npipeline([it_text], max_length=512)\r\n\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
    \nTypeError                                 Traceback (most recent call last)\r\
    \n/tmp/ipykernel_2819/3211885403.py in <module>\r\n      3 pipeline = TranslationPipeline(\r\
    \n      4 model=AutoModelWithLMHead.from_pretrained(\"SEBIS/legal_t5_small_trans_it_en_small_finetuned\"\
    ),\r\n----> 5 tokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path\
    \ = \"SEBIS/legal_t5_small_trans_it_en\", do_lower_case=False, \r\n      6   \
    \                                          skip_special_tokens=True),\r\n    \
    \  7     device=0\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\n\
    \    657             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\
    \n    658             if tokenizer_class_fast and (use_fast or tokenizer_class_py\
    \ is None):\r\n--> 659                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n    660             else:\r\n    661                 if\
    \ tokenizer_class_py is not None:\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\
    \n   1799                 logger.info(f\"loading file {file_path} from cache at\
    \ {resolved_vocab_files[file_id]}\")\r\n   1800 \r\n-> 1801         return cls._from_pretrained(\r\
    \n   1802             resolved_vocab_files,\r\n   1803             pretrained_model_name_or_path,\r\
    \n\r\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
    \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
    \ *init_inputs, **kwargs)\r\n   1954         # Instantiate tokenizer.\r\n   1955\
    \         try:\r\n-> 1956             tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n   1957         except OSError:\r\n   1958             raise OSError(\r\n\r\n\
    ~/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py\
    \ in __init__(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token,\
    \ extra_ids, additional_special_tokens, **kwargs)\r\n    131                 )\r\
    \n    132 \r\n--> 133         super().__init__(\r\n    134             vocab_file,\r\
    \n    135             tokenizer_file=tokenizer_file,\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
    \ in __init__(self, *args, **kwargs)\r\n    112         elif slow_tokenizer is\
    \ not None:\r\n    113             # We need to convert a slow tokenizer to build\
    \ the backend\r\n--> 114             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
    \n    115         elif self.slow_tokenizer_class is not None:\r\n    116     \
    \        # We need to create and convert a slow tokenizer to build the backend\r\
    \n\r\n~/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py\
    \ in convert_slow_tokenizer(transformer_tokenizer)\r\n   1160     converter_class\
    \ = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\r\n   1161 \r\n-> 1162     return\
    \ converter_class(transformer_tokenizer).converted()\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py\
    \ in __init__(self, *args)\r\n    436         super().__init__(*args)\r\n    437\
    \ \r\n--> 438         from .utils import sentencepiece_model_pb2 as model_pb2\r\
    \n    439 \r\n    440         m = model_pb2.ModelProto()\r\n\r\n~/.local/lib/python3.8/site-packages/transformers/utils/sentencepiece_model_pb2.py\
    \ in <module>\r\n     90     create_key=_descriptor._internal_create_key,\r\n\
    \     91     values=[\r\n---> 92         _descriptor.EnumValueDescriptor(\r\n\
    \     93             name=\"UNIGRAM\",\r\n     94             index=0,\r\n\r\n\
    ~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py in __new__(cls,\
    \ name, index, number, type, options, serialized_options, create_key)\r\n    794\
    \                 type=None,  # pylint: disable=redefined-builtin\r\n    795 \
    \                options=None, serialized_options=None, create_key=None):\r\n\
    --> 796       _message.Message._CheckCalledFromGeneratedFile()\r\n    797    \
    \   # There is no way we can build a complete EnumValueDescriptor with the\r\n\
    \    798       # given parameters (the name of the Enum is not known, for example).\r\
    \n\r\nTypeError: Descriptors cannot not be created directly.\r\nIf this call came\
    \ from a _pb2.py file, your generated code is out of date and must be regenerated\
    \ with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate your protos,\
    \ some other possible workarounds are:\r\n 1. Downgrade the protobuf package to\
    \ 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but\
    \ this will use pure-Python parsing and will be much slower).\r\n\r\nMore information:\
    \ https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\r\
    \n```"
  created_at: 2023-11-25 18:06:50+00:00
  edited: false
  hidden: false
  id: 656237bad5e3c35c7910a646
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: SEBIS/legal_t5_small_trans_it_en_small_finetuned
repo_type: model
status: open
target_branch: null
title: Problems with tokenizer
