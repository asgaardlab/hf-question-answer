!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sgugger
conflicting_files: []
created_at: 2023-06-20 14:30:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-06-20T15:30:29.000Z'
    data:
      edited: false
      editors:
      - sgugger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9755750894546509
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
          fullname: Sylvain Gugger
          isHf: false
          isPro: false
          name: sgugger
          type: user
        html: '<p>There was probably a bug in the initial conversion script that created
          those models, as the weights they have have a<br>different value for <code>lm_head.weight</code>
          and <code>model.decoder.embed_tokens.weight</code>. Those models are tied
          though.</p>

          <p>This was not a problem until now as the model was tied after the load
          and the (wrong) value of <code>lm_head.weight</code> was<br>replaced by
          the value of <code>model.decoder.embed_tokens.weight</code>. This does not
          work any more if we tie the weights before<br>the load however, as the value
          picked might be the one from <code>lm_head.weight</code> depending on how
          the models are tied.<br>As far as I can see, the model stop generating properly
          on Transformers main.</p>

          <p>This should fix the bug without any side effect.</p>

          '
        raw: 'There was probably a bug in the initial conversion script that created
          those models, as the weights they have have a

          different value for `lm_head.weight` and `model.decoder.embed_tokens.weight`.
          Those models are tied though.


          This was not a problem until now as the model was tied after the load and
          the (wrong) value of `lm_head.weight` was

          replaced by the value of `model.decoder.embed_tokens.weight`. This does
          not work any more if we tie the weights before

          the load however, as the value picked might be the one from `lm_head.weight`
          depending on how the models are tied.

          As far as I can see, the model stop generating properly on Transformers
          main.


          This should fix the bug without any side effect.'
        updatedAt: '2023-06-20T15:30:29.310Z'
      numEdits: 0
      reactions: []
    id: 6491c6157a7902248b0c5542
    type: comment
  author: sgugger
  content: 'There was probably a bug in the initial conversion script that created
    those models, as the weights they have have a

    different value for `lm_head.weight` and `model.decoder.embed_tokens.weight`.
    Those models are tied though.


    This was not a problem until now as the model was tied after the load and the
    (wrong) value of `lm_head.weight` was

    replaced by the value of `model.decoder.embed_tokens.weight`. This does not work
    any more if we tie the weights before

    the load however, as the value picked might be the one from `lm_head.weight` depending
    on how the models are tied.

    As far as I can see, the model stop generating properly on Transformers main.


    This should fix the bug without any side effect.'
  created_at: 2023-06-20 14:30:29+00:00
  edited: false
  hidden: false
  id: 6491c6157a7902248b0c5542
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-06-20T15:30:30.000Z'
    data:
      oid: 0eb852d62ee344426003968d32f4c65d4ee720e3
      parents:
      - b1ddcf61422ae3d4499dd89d3ec0678be154c5ed
      subject: Fix weights by putting the right value in `lm_head.weight`
    id: 6491c6160000000000000000
    type: commit
  author: sgugger
  created_at: 2023-06-20 14:30:30+00:00
  id: 6491c6160000000000000000
  oid: 0eb852d62ee344426003968d32f4c65d4ee720e3
  summary: Fix weights by putting the right value in `lm_head.weight`
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-07-21T15:41:03.000Z'
    data:
      status: merged
    id: 64baa70f94c0e3be4aa1c116
    type: status-change
  author: sgugger
  created_at: 2023-07-21 14:41:03+00:00
  id: 64baa70f94c0e3be4aa1c116
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 7804f26c78fb631847d813515558e1993ff489ae
num: 3
repo_id: Helsinki-NLP/opus-tatoeba-es-zh
repo_type: model
status: merged
target_branch: refs/heads/main
title: Fix weights by putting the right value in `lm_head.weight`
