!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-12-10 15:30:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-10T15:30:11.000Z'
    data:
      edited: true
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9717950224876404
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p>This was already A MetaMath, Cybertron &amp; Starling Slerp merge,\
          \ with Cybertron having lots of DPO training.</p>\n<p>Further training might\
          \ have forced the scores up a tiny bit, but it's performing worse than the\
          \ aforementioned <span data-props=\"{&quot;user&quot;:&quot;Q-bert&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Q-bert\"\
          >@<span class=\"underline\">Q-bert</span></a></span>\n\n\t</span></span>\
          \ slerp. For example, it's hallucinating more at the fringes of knowledge\
          \ and ignoring story prompts more (partly by adding NSFW to the stories).</p>\n\
          <p>I'm not a prude. It's just that the last fine-tuning kinda takes over,\
          \ so taking what is currently the best overall performing LLM and adding\
          \ NSFW DPO is far from ideal. It would turn out better if one of the three\
          \ models added such content prior to being merged. And since NSFW role play\
          \ doesn't require a smart model why add it to the top performer so it stick\
          \ out at the top of the leaderboard?</p>\n<p>Merge: <a href=\"https://huggingface.co/Q-bert/MetaMath-Cybertron-Starling\"\
          >https://huggingface.co/Q-bert/MetaMath-Cybertron-Starling</a></p>\n<p>Metamath:\
          \ <a href=\"https://huggingface.co/meta-math/MetaMath-Mistral-7B\">https://huggingface.co/meta-math/MetaMath-Mistral-7B</a><br>Cybertron:\
          \ <a href=\"https://huggingface.co/fblgit/una-cybertron-7b-v2-bf16\">https://huggingface.co/fblgit/una-cybertron-7b-v2-bf16</a><br>Starling:\
          \ <a href=\"https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\"\
          >https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha</a></p>\n"
        raw: 'This was already A MetaMath, Cybertron & Starling Slerp merge, with
          Cybertron having lots of DPO training.


          Further training might have forced the scores up a tiny bit, but it''s performing
          worse than the aforementioned @Q-bert slerp. For example, it''s hallucinating
          more at the fringes of knowledge and ignoring story prompts more (partly
          by adding NSFW to the stories).


          I''m not a prude. It''s just that the last fine-tuning kinda takes over,
          so taking what is currently the best overall performing LLM and adding NSFW
          DPO is far from ideal. It would turn out better if one of the three models
          added such content prior to being merged. And since NSFW role play doesn''t
          require a smart model why add it to the top performer so it stick out at
          the top of the leaderboard?


          Merge: https://huggingface.co/Q-bert/MetaMath-Cybertron-Starling


          Metamath: https://huggingface.co/meta-math/MetaMath-Mistral-7B

          Cybertron: https://huggingface.co/fblgit/una-cybertron-7b-v2-bf16

          Starling: https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha'
        updatedAt: '2023-12-10T16:00:44.288Z'
      numEdits: 2
      reactions: []
    id: 6575d983eb4b4e0bcf503f0c
    type: comment
  author: Phil337
  content: 'This was already A MetaMath, Cybertron & Starling Slerp merge, with Cybertron
    having lots of DPO training.


    Further training might have forced the scores up a tiny bit, but it''s performing
    worse than the aforementioned @Q-bert slerp. For example, it''s hallucinating
    more at the fringes of knowledge and ignoring story prompts more (partly by adding
    NSFW to the stories).


    I''m not a prude. It''s just that the last fine-tuning kinda takes over, so taking
    what is currently the best overall performing LLM and adding NSFW DPO is far from
    ideal. It would turn out better if one of the three models added such content
    prior to being merged. And since NSFW role play doesn''t require a smart model
    why add it to the top performer so it stick out at the top of the leaderboard?


    Merge: https://huggingface.co/Q-bert/MetaMath-Cybertron-Starling


    Metamath: https://huggingface.co/meta-math/MetaMath-Mistral-7B

    Cybertron: https://huggingface.co/fblgit/una-cybertron-7b-v2-bf16

    Starling: https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha'
  created_at: 2023-12-10 15:30:11+00:00
  edited: true
  hidden: false
  id: 6575d983eb4b4e0bcf503f0c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: rwitz/go-bruins-v2
repo_type: model
status: open
target_branch: null
title: Perhaps You Should Use a Different Base Model For Go Bruins
