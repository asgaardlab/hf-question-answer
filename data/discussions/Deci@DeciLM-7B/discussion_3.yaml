!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Manmax31
conflicting_files: null
created_at: 2023-12-19 05:52:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cd8535acd2718c3e94efb64c73f9275.svg
      fullname: Manab Chetia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Manmax31
      type: user
    createdAt: '2023-12-19T05:52:10.000Z'
    data:
      edited: true
      editors:
      - Manmax31
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47655048966407776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cd8535acd2718c3e94efb64c73f9275.svg
          fullname: Manab Chetia
          isHf: false
          isPro: false
          name: Manmax31
          type: user
        html: "<p>I attempted finetuning using DeciLM-7B as the model using PEFT and\
          \ SFT.</p>\n<p>After the peft model is created, during inference, I get\
          \ the error <code>RuntimeError: cutlassF: no kernel found to launch!</code></p>\n\
          <p>I am using TeslaV100 GPUs. I have tried the same with Mistral-7b and\
          \ don't see any issues.</p>\n<p>Here is my code:</p>\n<pre><code>quantization_config\
          \ = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
          \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\"\
          ,\n)\nbase_model_id=\"Deci/DeciLM-7B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    base_model_id,\n    quantization_config=quantization_config,\n    device_map=\"\
          auto\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token =\
          \ tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nft_model =\
          \ PeftModel.from_pretrained(\n    model, \"finetuned_models/deci-7b-tuned-r32-a16\"\
          \n)\npipe = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer)\n\
          \noutputs = pipe(\n    prompt,\n    max_new_tokens=800,\n    do_sample=True,\n\
          \    temperature=0.1,\n    return_full_text=False,\n    repetition_penalty=1.5,\n\
          \    num_beams=1,\n    no_repeat_ngram_size=3,\n    early_stopping=True,\n\
          )\nprint(\"\\nOutput:\", outputs[0][\"generated_text\"])\n</code></pre>\n"
        raw: "I attempted finetuning using DeciLM-7B as the model using PEFT and SFT.\n\
          \nAfter the peft model is created, during inference, I get the error ```RuntimeError:\
          \ cutlassF: no kernel found to launch!```\n\nI am using TeslaV100 GPUs.\
          \ I have tried the same with Mistral-7b and don't see any issues.\n\nHere\
          \ is my code:\n```\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n\
          \    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n\
          \    bnb_4bit_compute_dtype=\"float16\",\n)\nbase_model_id=\"Deci/DeciLM-7B\"\
          \nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n  \
          \  quantization_config=quantization_config,\n    device_map=\"auto\",\n\
          \    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token =\
          \ tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nft_model =\
          \ PeftModel.from_pretrained(\n    model, \"finetuned_models/deci-7b-tuned-r32-a16\"\
          \n)\npipe = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer)\n\
          \noutputs = pipe(\n    prompt,\n    max_new_tokens=800,\n    do_sample=True,\n\
          \    temperature=0.1,\n    return_full_text=False,\n    repetition_penalty=1.5,\n\
          \    num_beams=1,\n    no_repeat_ngram_size=3,\n    early_stopping=True,\n\
          )\nprint(\"\\nOutput:\", outputs[0][\"generated_text\"])\n```\n"
        updatedAt: '2023-12-19T05:52:39.728Z'
      numEdits: 1
      reactions: []
    id: 65812f8a8c6c1cb0373bc245
    type: comment
  author: Manmax31
  content: "I attempted finetuning using DeciLM-7B as the model using PEFT and SFT.\n\
    \nAfter the peft model is created, during inference, I get the error ```RuntimeError:\
    \ cutlassF: no kernel found to launch!```\n\nI am using TeslaV100 GPUs. I have\
    \ tried the same with Mistral-7b and don't see any issues.\n\nHere is my code:\n\
    ```\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
    \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\n\
    base_model_id=\"Deci/DeciLM-7B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    base_model_id,\n    quantization_config=quantization_config,\n    device_map=\"\
    auto\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\
    tokenizer.padding_side = \"right\"\n\nft_model = PeftModel.from_pretrained(\n\
    \    model, \"finetuned_models/deci-7b-tuned-r32-a16\"\n)\npipe = pipeline(\"\
    text-generation\", model=ft_model, tokenizer=tokenizer)\n\noutputs = pipe(\n \
    \   prompt,\n    max_new_tokens=800,\n    do_sample=True,\n    temperature=0.1,\n\
    \    return_full_text=False,\n    repetition_penalty=1.5,\n    num_beams=1,\n\
    \    no_repeat_ngram_size=3,\n    early_stopping=True,\n)\nprint(\"\\nOutput:\"\
    , outputs[0][\"generated_text\"])\n```\n"
  created_at: 2023-12-19 05:52:10+00:00
  edited: true
  hidden: false
  id: 65812f8a8c6c1cb0373bc245
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
      fullname: Najeeb Nabwani
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: NajeebDeci
      type: user
    createdAt: '2023-12-19T20:31:36.000Z'
    data:
      edited: false
      editors:
      - NajeebDeci
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9360302686691284
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
          fullname: Najeeb Nabwani
          isHf: false
          isPro: false
          name: NajeebDeci
          type: user
        html: '<p>Hello, </p>

          <p>This is because SDPA is not supported in your environment, this will
          likely be fixed in a future transformer version as the new version checks
          if SDPA is available.</p>

          <p>Hopefully, that helps.</p>

          '
        raw: "Hello, \n\nThis is because SDPA is not supported in your environment,\
          \ this will likely be fixed in a future transformer version as the new version\
          \ checks if SDPA is available.\n\nHopefully, that helps."
        updatedAt: '2023-12-19T20:31:36.448Z'
      numEdits: 0
      reactions: []
    id: 6581fda8770395b588edc2e5
    type: comment
  author: NajeebDeci
  content: "Hello, \n\nThis is because SDPA is not supported in your environment,\
    \ this will likely be fixed in a future transformer version as the new version\
    \ checks if SDPA is available.\n\nHopefully, that helps."
  created_at: 2023-12-19 20:31:36+00:00
  edited: false
  hidden: false
  id: 6581fda8770395b588edc2e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cd8535acd2718c3e94efb64c73f9275.svg
      fullname: Manab Chetia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Manmax31
      type: user
    createdAt: '2023-12-19T23:04:37.000Z'
    data:
      edited: false
      editors:
      - Manmax31
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9883869886398315
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cd8535acd2718c3e94efb64c73f9275.svg
          fullname: Manab Chetia
          isHf: false
          isPro: false
          name: Manmax31
          type: user
        html: '<p>Thank you.<br>Is there a work around this for now?</p>

          '
        raw: "Thank you. \nIs there a work around this for now?\n"
        updatedAt: '2023-12-19T23:04:37.021Z'
      numEdits: 0
      reactions: []
    id: 658221857cec0a2080c21ebc
    type: comment
  author: Manmax31
  content: "Thank you. \nIs there a work around this for now?\n"
  created_at: 2023-12-19 23:04:37+00:00
  edited: false
  hidden: false
  id: 658221857cec0a2080c21ebc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a84795c5ffe1d019d6c53c/WfquBAh5tFx9PWdjYt4CG.png?w=200&h=200&f=face
      fullname: Stan Kulish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vcoolish
      type: user
    createdAt: '2024-01-22T18:26:09.000Z'
    data:
      edited: false
      editors:
      - vcoolish
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9868455529212952
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a84795c5ffe1d019d6c53c/WfquBAh5tFx9PWdjYt4CG.png?w=200&h=200&f=face
          fullname: Stan Kulish
          isHf: false
          isPro: false
          name: vcoolish
          type: user
        html: '<p>try on A100 GPU, it worked for me</p>

          '
        raw: try on A100 GPU, it worked for me
        updatedAt: '2024-01-22T18:26:09.027Z'
      numEdits: 0
      reactions: []
    id: 65aeb341b64e1c2389bf1df0
    type: comment
  author: vcoolish
  content: try on A100 GPU, it worked for me
  created_at: 2024-01-22 18:26:09+00:00
  edited: false
  hidden: false
  id: 65aeb341b64e1c2389bf1df0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Deci/DeciLM-7B
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: cutlassF: no kernel found to launch'
