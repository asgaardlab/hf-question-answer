!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vmajor
conflicting_files: null
created_at: 2023-10-21 09:36:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-10-21T10:36:59.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9775151610374451
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>I need to start by saying that I have been using your models since
          the 65B versions. I found them to have the highest level of "intelligence"
          and were rivalled only by Guanaco.</p>

          <p>So, now for the conflict part of my post:)</p>

          <p>I appreciate the effort that you took to expand the baseline model capability
          but I personally believe that mandating a certain prompt structure for the
          model to produce reliable response is not the best direction to take. We
          have ML for this and ML is not suitable for tasks that LLMs can do. I do
          not think that we need to seek convergence between ML and LLM spheres.</p>

          <p>Thus, ideally Airoboros 3.1.3 would be robust enough to decide how to
          process the prompt without having to be cajoled or triggered into doing
          it "correctly". </p>

          '
        raw: "I need to start by saying that I have been using your models since the\
          \ 65B versions. I found them to have the highest level of \"intelligence\"\
          \ and were rivalled only by Guanaco.\r\n\r\nSo, now for the conflict part\
          \ of my post:)\r\n\r\nI appreciate the effort that you took to expand the\
          \ baseline model capability but I personally believe that mandating a certain\
          \ prompt structure for the model to produce reliable response is not the\
          \ best direction to take. We have ML for this and ML is not suitable for\
          \ tasks that LLMs can do. I do not think that we need to seek convergence\
          \ between ML and LLM spheres.\r\n\r\nThus, ideally Airoboros 3.1.3 would\
          \ be robust enough to decide how to process the prompt without having to\
          \ be cajoled or triggered into doing it \"correctly\". "
        updatedAt: '2023-10-21T10:36:59.221Z'
      numEdits: 0
      reactions: []
    id: 6533a9cbbadc4978075b878f
    type: comment
  author: vmajor
  content: "I need to start by saying that I have been using your models since the\
    \ 65B versions. I found them to have the highest level of \"intelligence\" and\
    \ were rivalled only by Guanaco.\r\n\r\nSo, now for the conflict part of my post:)\r\
    \n\r\nI appreciate the effort that you took to expand the baseline model capability\
    \ but I personally believe that mandating a certain prompt structure for the model\
    \ to produce reliable response is not the best direction to take. We have ML for\
    \ this and ML is not suitable for tasks that LLMs can do. I do not think that\
    \ we need to seek convergence between ML and LLM spheres.\r\n\r\nThus, ideally\
    \ Airoboros 3.1.3 would be robust enough to decide how to process the prompt without\
    \ having to be cajoled or triggered into doing it \"correctly\". "
  created_at: 2023-10-21 09:36:59+00:00
  edited: false
  hidden: false
  id: 6533a9cbbadc4978075b878f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-10-21T11:01:33.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8982268571853638
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>Which functionality are you referring to specifically?  Or do you
          mean the prompt format?</p>

          '
        raw: Which functionality are you referring to specifically?  Or do you mean
          the prompt format?
        updatedAt: '2023-10-21T11:01:33.590Z'
      numEdits: 0
      reactions: []
    id: 6533af8d25a58aec6583bbdd
    type: comment
  author: jondurbin
  content: Which functionality are you referring to specifically?  Or do you mean
    the prompt format?
  created_at: 2023-10-21 10:01:33+00:00
  edited: false
  hidden: false
  id: 6533af8d25a58aec6583bbdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-10-21T11:45:42.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553860425949097
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>The prompt format. The way I see, and this is the "important" thing
          for what I am saying, meaning just how I would like things to be is to fret
          less about the prompt engineering, and focus more on making use of the output.</p>

          <p>The second reason I see this as important is because of the potential
          breaking changes that changes to prompt formats cause, and divergence between
          how models like to be prompted making it more difficult to maintain the
          code that I write, and to compare and select among different models using
          the conceptually identical prompt.</p>

          '
        raw: 'The prompt format. The way I see, and this is the "important" thing
          for what I am saying, meaning just how I would like things to be is to fret
          less about the prompt engineering, and focus more on making use of the output.


          The second reason I see this as important is because of the potential breaking
          changes that changes to prompt formats cause, and divergence between how
          models like to be prompted making it more difficult to maintain the code
          that I write, and to compare and select among different models using the
          conceptually identical prompt.'
        updatedAt: '2023-10-21T11:45:42.203Z'
      numEdits: 0
      reactions: []
    id: 6533b9e6e760105846136e02
    type: comment
  author: vmajor
  content: 'The prompt format. The way I see, and this is the "important" thing for
    what I am saying, meaning just how I would like things to be is to fret less about
    the prompt engineering, and focus more on making use of the output.


    The second reason I see this as important is because of the potential breaking
    changes that changes to prompt formats cause, and divergence between how models
    like to be prompted making it more difficult to maintain the code that I write,
    and to compare and select among different models using the conceptually identical
    prompt.'
  created_at: 2023-10-21 10:45:42+00:00
  edited: false
  hidden: false
  id: 6533b9e6e760105846136e02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-10-22T01:29:20.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9584590792655945
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>I agree it''s a pain in the butt, but maybe the solution is not
          to make the model respond equally well to any format. I have to imagine
          that has a cost, especially when we have very limited number of parameters
          in a LORA fine tune. In an ideal world, we would all experiment and pick
          a format (egs., ChatML, Llama-chat) that works well, and a tokenizer to
          go with it (to efficiently and consistent encode the start/end/role tokens),
          and we all train models in that format.</p>

          <p>But I think it''s quite messy right now and not everyone agrees on the
          best way. There are formats that are objectively a bad idea or always tokenize
          inconsistently, repos (egs., Axolotl) that make it complicated to modify
          the format, etc. There is some move toward using ChatML now I think with
          Hugging Face starting to support it natively, as well as natively providing
          code for chat input creation using it, but it still needs proper support
          of special tokens and so on, which is also all over the place.</p>

          <p>So yeah, right now I do write code each time and have to add a bunch
          of clauses every time to handle format variations, but I view that as a
          price to pay to get the best quality outputs as we move toward some kind
          of convergence. Best we can do is encourage people to use a specific format
          and argue for it, rather than have them dump resources into training a format-agnostic
          model, IMO.</p>

          <p>Prompt format is also not prompt engineering from the user point-of-view
          I think. There is a difference in handling the prompt format with whatever
          front-end there is (common to all queries with that model), vs having to
          actually smith the wording of each query.</p>

          '
        raw: 'I agree it''s a pain in the butt, but maybe the solution is not to make
          the model respond equally well to any format. I have to imagine that has
          a cost, especially when we have very limited number of parameters in a LORA
          fine tune. In an ideal world, we would all experiment and pick a format
          (egs., ChatML, Llama-chat) that works well, and a tokenizer to go with it
          (to efficiently and consistent encode the start/end/role tokens), and we
          all train models in that format.


          But I think it''s quite messy right now and not everyone agrees on the best
          way. There are formats that are objectively a bad idea or always tokenize
          inconsistently, repos (egs., Axolotl) that make it complicated to modify
          the format, etc. There is some move toward using ChatML now I think with
          Hugging Face starting to support it natively, as well as natively providing
          code for chat input creation using it, but it still needs proper support
          of special tokens and so on, which is also all over the place.


          So yeah, right now I do write code each time and have to add a bunch of
          clauses every time to handle format variations, but I view that as a price
          to pay to get the best quality outputs as we move toward some kind of convergence.
          Best we can do is encourage people to use a specific format and argue for
          it, rather than have them dump resources into training a format-agnostic
          model, IMO.


          Prompt format is also not prompt engineering from the user point-of-view
          I think. There is a difference in handling the prompt format with whatever
          front-end there is (common to all queries with that model), vs having to
          actually smith the wording of each query.'
        updatedAt: '2023-10-22T01:29:20.958Z'
      numEdits: 0
      reactions: []
    id: 65347af06e86d670ffa6d694
    type: comment
  author: grimulkan
  content: 'I agree it''s a pain in the butt, but maybe the solution is not to make
    the model respond equally well to any format. I have to imagine that has a cost,
    especially when we have very limited number of parameters in a LORA fine tune.
    In an ideal world, we would all experiment and pick a format (egs., ChatML, Llama-chat)
    that works well, and a tokenizer to go with it (to efficiently and consistent
    encode the start/end/role tokens), and we all train models in that format.


    But I think it''s quite messy right now and not everyone agrees on the best way.
    There are formats that are objectively a bad idea or always tokenize inconsistently,
    repos (egs., Axolotl) that make it complicated to modify the format, etc. There
    is some move toward using ChatML now I think with Hugging Face starting to support
    it natively, as well as natively providing code for chat input creation using
    it, but it still needs proper support of special tokens and so on, which is also
    all over the place.


    So yeah, right now I do write code each time and have to add a bunch of clauses
    every time to handle format variations, but I view that as a price to pay to get
    the best quality outputs as we move toward some kind of convergence. Best we can
    do is encourage people to use a specific format and argue for it, rather than
    have them dump resources into training a format-agnostic model, IMO.


    Prompt format is also not prompt engineering from the user point-of-view I think.
    There is a difference in handling the prompt format with whatever front-end there
    is (common to all queries with that model), vs having to actually smith the wording
    of each query.'
  created_at: 2023-10-22 00:29:20+00:00
  edited: false
  hidden: false
  id: 65347af06e86d670ffa6d694
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-10-22T05:24:57.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9675436019897461
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>thank you for the detailed answer...and yes it is not "prompt engineering"
          exactly. I was considering the changes to my python programs that I need
          to make, hence the overlap with the "engineering" context. I guess my main
          issue is that of unfamiliarity and the need to adjust my work...so I am
          complaining because I want the perfect future today :)</p>

          '
        raw: 'thank you for the detailed answer...and yes it is not "prompt engineering"
          exactly. I was considering the changes to my python programs that I need
          to make, hence the overlap with the "engineering" context. I guess my main
          issue is that of unfamiliarity and the need to adjust my work...so I am
          complaining because I want the perfect future today :)


          '
        updatedAt: '2023-10-22T05:24:57.608Z'
      numEdits: 0
      reactions: []
    id: 6534b229392f3bcb325e2424
    type: comment
  author: vmajor
  content: 'thank you for the detailed answer...and yes it is not "prompt engineering"
    exactly. I was considering the changes to my python programs that I need to make,
    hence the overlap with the "engineering" context. I guess my main issue is that
    of unfamiliarity and the need to adjust my work...so I am complaining because
    I want the perfect future today :)


    '
  created_at: 2023-10-22 04:24:57+00:00
  edited: false
  hidden: false
  id: 6534b229392f3bcb325e2424
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-10-22T07:34:26.000Z'
    data:
      edited: true
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9009502530097961
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I understand the frustration, and I knew the switch from vicuna
          style to llama-2 chat would be somewhat painful, but it''s for the best,
          especially now that most of the inference backends support this format out
          of the box.</p>

          <p>It was a bandaid that needed to be ripped off.</p>

          <p>re: vicuna USER/ASSISTANT</p>

          <p>USER: can be tokenized in multiple ways depending on surrounding characters,
          and somewhat inherently assigns an extra identity to the model if you use
          a persona as system prompt.</p>

          <p>Alpaca is ok for instructions, but the chance of markdown style header
          "### Instruction" or response happening in the wild is pretty large, so
          it''s probably much easier to have strange results from prompt inputs (e.g.
          RAG)</p>

          <p>chatml is better at deterministic delimiters than vicuna, but IMO llama-2
          chat is better for very clearly separating system from instruction and instruction
          from response, and there''s no identity/role terminology introduced to contend
          with persona in system prompt.</p>

          <pre><code>&lt;|im_start|&gt;system

          you are Jon

          &lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          hello

          &lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant


          vs.


          [INST] &lt;&lt;SYS&gt;&gt;

          You are Jon.

          &lt;&lt;/SYS&gt;&gt;


          hello [/INST]

          </code></pre>

          <p>Much clearer, cleaner, and less ambiguous IMO. </p>

          <p>llama-2 chat format, at least by model download count, is becoming the
          standard:</p>

          <ul>

          <li>mistral-7b-instruct-v0.1 downloads last month: 154,352</li>

          <li>llama-2-7b-chat downloads last month: 1,152,332</li>

          <li>codellama-34b-instruct downloads last month: 211,818</li>

          </ul>

          <p>I don''t plan to change the prompt format again, unless there is some
          proof that a different prompt format is superior.</p>

          <p>The transformers library has an apply_chat_template method which I would
          recommend using to reduce friction.</p>

          '
        raw: "I understand the frustration, and I knew the switch from vicuna style\
          \ to llama-2 chat would be somewhat painful, but it's for the best, especially\
          \ now that most of the inference backends support this format out of the\
          \ box.\n\nIt was a bandaid that needed to be ripped off.\n\nre: vicuna USER/ASSISTANT\n\
          \nUSER: can be tokenized in multiple ways depending on surrounding characters,\
          \ and somewhat inherently assigns an extra identity to the model if you\
          \ use a persona as system prompt.\n\nAlpaca is ok for instructions, but\
          \ the chance of markdown style header \"### Instruction\" or response happening\
          \ in the wild is pretty large, so it's probably much easier to have strange\
          \ results from prompt inputs (e.g. RAG)\n\nchatml is better at deterministic\
          \ delimiters than vicuna, but IMO llama-2 chat is better for very clearly\
          \ separating system from instruction and instruction from response, and\
          \ there's no identity/role terminology introduced to contend with persona\
          \ in system prompt.\n\n```\n<|im_start|>system\nyou are Jon\n<|im_end|>\n\
          <|im_start|>user\nhello\n<|im_end|>\n<|im_start|>assistant\n\nvs.\n\n[INST]\
          \ <<SYS>>\nYou are Jon.\n<</SYS>>\n\nhello [/INST]\n```\nMuch clearer, cleaner,\
          \ and less ambiguous IMO. \n\nllama-2 chat format, at least by model download\
          \ count, is becoming the standard:\n- mistral-7b-instruct-v0.1 downloads\
          \ last month: 154,352\n- llama-2-7b-chat downloads last month: 1,152,332\n\
          - codellama-34b-instruct downloads last month: 211,818\n\nI don't plan to\
          \ change the prompt format again, unless there is some proof that a different\
          \ prompt format is superior.\n\nThe transformers library has an apply_chat_template\
          \ method which I would recommend using to reduce friction."
        updatedAt: '2023-10-22T07:37:21.809Z'
      numEdits: 1
      reactions: []
    id: 6534d0829860c1cb37de340c
    type: comment
  author: jondurbin
  content: "I understand the frustration, and I knew the switch from vicuna style\
    \ to llama-2 chat would be somewhat painful, but it's for the best, especially\
    \ now that most of the inference backends support this format out of the box.\n\
    \nIt was a bandaid that needed to be ripped off.\n\nre: vicuna USER/ASSISTANT\n\
    \nUSER: can be tokenized in multiple ways depending on surrounding characters,\
    \ and somewhat inherently assigns an extra identity to the model if you use a\
    \ persona as system prompt.\n\nAlpaca is ok for instructions, but the chance of\
    \ markdown style header \"### Instruction\" or response happening in the wild\
    \ is pretty large, so it's probably much easier to have strange results from prompt\
    \ inputs (e.g. RAG)\n\nchatml is better at deterministic delimiters than vicuna,\
    \ but IMO llama-2 chat is better for very clearly separating system from instruction\
    \ and instruction from response, and there's no identity/role terminology introduced\
    \ to contend with persona in system prompt.\n\n```\n<|im_start|>system\nyou are\
    \ Jon\n<|im_end|>\n<|im_start|>user\nhello\n<|im_end|>\n<|im_start|>assistant\n\
    \nvs.\n\n[INST] <<SYS>>\nYou are Jon.\n<</SYS>>\n\nhello [/INST]\n```\nMuch clearer,\
    \ cleaner, and less ambiguous IMO. \n\nllama-2 chat format, at least by model\
    \ download count, is becoming the standard:\n- mistral-7b-instruct-v0.1 downloads\
    \ last month: 154,352\n- llama-2-7b-chat downloads last month: 1,152,332\n- codellama-34b-instruct\
    \ downloads last month: 211,818\n\nI don't plan to change the prompt format again,\
    \ unless there is some proof that a different prompt format is superior.\n\nThe\
    \ transformers library has an apply_chat_template method which I would recommend\
    \ using to reduce friction."
  created_at: 2023-10-22 06:34:26+00:00
  edited: true
  hidden: false
  id: 6534d0829860c1cb37de340c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-10-22T19:22:43.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8909109830856323
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Also Chat-ML needs &lt;|im_*|&gt; added to the special tokens list
          to have its prompts tokenize correctly, otherwise once again you have ambiguous/random
          tokenization. This potentially adds more ways for things to go wrong (people
          may resize the tokenizer to non-multiple of power of 2 causing performance
          issues, tokenizer implementations may just not support special tokens, textgen
          UIs may not tokenize special tokens correctly such as Ooba in some cases).</p>

          <p>But Llama-chat uses BOS/EOS to separate its prompts, so has none of those
          issues. Only issue with Llama-chat is the somewhat strange format that doesn''t
          neatly fall into the ways the others are implemented (it''s so easy to mess
          it up!).</p>

          <p>Personally, I''ve been using modified Vicuna:</p>

          <pre><code>&lt;s&gt;SYSTEM: &lt;message&gt;&lt;/s&gt;

          &lt;s&gt;USER: &lt;message&gt;&lt;/s&gt;

          &lt;s&gt;ASSISTANT: &lt;message&gt;&lt;/s&gt;

          </code></pre>

          <p>or</p>

          <pre><code>&lt;s&gt;SYSTEM:&lt;/s&gt;&lt;s&gt;&lt;message&gt;&lt;/s&gt;

          &lt;s&gt;USER:&lt;/s&gt;&lt;s&gt;&lt;message&gt;&lt;/s&gt;

          &lt;s&gt;ASSISTANT:&lt;/s&gt;&lt;s&gt;&lt;message&gt;&lt;/s&gt;

          </code></pre>

          <p>which tokenizes consistently due to the embedded EOS/BOS and also does
          not require special tokens to be added.</p>

          <p>I didn''t think about USER/ASSISTANT getting referenced/confused with
          the actual query, so that''s a good point! However sometimes I want to use
          system messages to reference the user and/or assistant directly (a lot of
          canned sys messages do that), and having those sub-headings maybe helps
          the AI interpret them more easily?</p>

          '
        raw: 'Also Chat-ML needs <|im_*|> added to the special tokens list to have
          its prompts tokenize correctly, otherwise once again you have ambiguous/random
          tokenization. This potentially adds more ways for things to go wrong (people
          may resize the tokenizer to non-multiple of power of 2 causing performance
          issues, tokenizer implementations may just not support special tokens, textgen
          UIs may not tokenize special tokens correctly such as Ooba in some cases).


          But Llama-chat uses BOS/EOS to separate its prompts, so has none of those
          issues. Only issue with Llama-chat is the somewhat strange format that doesn''t
          neatly fall into the ways the others are implemented (it''s so easy to mess
          it up!).


          Personally, I''ve been using modified Vicuna:

          ```

          <s>SYSTEM: <message></s>

          <s>USER: <message></s>

          <s>ASSISTANT: <message></s>

          ```

          or

          ```

          <s>SYSTEM:</s><s><message></s>

          <s>USER:</s><s><message></s>

          <s>ASSISTANT:</s><s><message></s>

          ```

          which tokenizes consistently due to the embedded EOS/BOS and also does not
          require special tokens to be added.


          I didn''t think about USER/ASSISTANT getting referenced/confused with the
          actual query, so that''s a good point! However sometimes I want to use system
          messages to reference the user and/or assistant directly (a lot of canned
          sys messages do that), and having those sub-headings maybe helps the AI
          interpret them more easily?'
        updatedAt: '2023-10-22T19:22:43.093Z'
      numEdits: 0
      reactions: []
    id: 653576836141b3927a067c13
    type: comment
  author: grimulkan
  content: 'Also Chat-ML needs <|im_*|> added to the special tokens list to have its
    prompts tokenize correctly, otherwise once again you have ambiguous/random tokenization.
    This potentially adds more ways for things to go wrong (people may resize the
    tokenizer to non-multiple of power of 2 causing performance issues, tokenizer
    implementations may just not support special tokens, textgen UIs may not tokenize
    special tokens correctly such as Ooba in some cases).


    But Llama-chat uses BOS/EOS to separate its prompts, so has none of those issues.
    Only issue with Llama-chat is the somewhat strange format that doesn''t neatly
    fall into the ways the others are implemented (it''s so easy to mess it up!).


    Personally, I''ve been using modified Vicuna:

    ```

    <s>SYSTEM: <message></s>

    <s>USER: <message></s>

    <s>ASSISTANT: <message></s>

    ```

    or

    ```

    <s>SYSTEM:</s><s><message></s>

    <s>USER:</s><s><message></s>

    <s>ASSISTANT:</s><s><message></s>

    ```

    which tokenizes consistently due to the embedded EOS/BOS and also does not require
    special tokens to be added.


    I didn''t think about USER/ASSISTANT getting referenced/confused with the actual
    query, so that''s a good point! However sometimes I want to use system messages
    to reference the user and/or assistant directly (a lot of canned sys messages
    do that), and having those sub-headings maybe helps the AI interpret them more
    easily?'
  created_at: 2023-10-22 18:22:43+00:00
  edited: false
  hidden: false
  id: 653576836141b3927a067c13
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/airoboros-l2-70b-3.1.2
repo_type: model
status: open
target_branch: null
title: Ability to generalise
