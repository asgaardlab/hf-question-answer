!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Satya93
conflicting_files: null
created_at: 2023-10-21 14:54:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-10-21T15:54:22.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.97251296043396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>I saw that you had to add a space to fix the model, is that just
          one space in total needed after [/INST] and response? I''m trying to start
          fine tuning, using llama 2 chat also, and sometimes get [/INST] in generation
          (with just one space in tuning). </p>

          '
        raw: 'I saw that you had to add a space to fix the model, is that just one
          space in total needed after [/INST] and response? I''m trying to start fine
          tuning, using llama 2 chat also, and sometimes get [/INST] in generation
          (with just one space in tuning). '
        updatedAt: '2023-10-21T15:54:22.676Z'
      numEdits: 0
      reactions: []
    id: 6533f42e31e73689f3f71e5f
    type: comment
  author: Satya93
  content: 'I saw that you had to add a space to fix the model, is that just one space
    in total needed after [/INST] and response? I''m trying to start fine tuning,
    using llama 2 chat also, and sometimes get [/INST] in generation (with just one
    space in tuning). '
  created_at: 2023-10-21 14:54:22+00:00
  edited: false
  hidden: false
  id: 6533f42e31e73689f3f71e5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-10-21T18:05:38.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9307880401611328
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I actually had one too many spaces, the very last [/INST] should
          not have a space.  All intermediate instances should however.</p>

          <p>See this function and format_chat_airoboros below it:<br><a rel="nofollow"
          href="https://github.com/jondurbin/qlora/blob/8cd269bf9bd7753c92164934269019e12f23314f/train.py#L551">https://github.com/jondurbin/qlora/blob/8cd269bf9bd7753c92164934269019e12f23314f/train.py#L551</a></p>

          <p>In the earlier version, I accidentally had one extra space on line 563.</p>

          <p>I''m not sure why you''d see those in generation though, maybe an issue
          with missing eos tokens?  Could just be your dataset length exceeds model
          max length so it''s not getting to an eos or something, hard to say.</p>

          '
        raw: 'I actually had one too many spaces, the very last [/INST] should not
          have a space.  All intermediate instances should however.


          See this function and format_chat_airoboros below it:

          https://github.com/jondurbin/qlora/blob/8cd269bf9bd7753c92164934269019e12f23314f/train.py#L551


          In the earlier version, I accidentally had one extra space on line 563.


          I''m not sure why you''d see those in generation though, maybe an issue
          with missing eos tokens?  Could just be your dataset length exceeds model
          max length so it''s not getting to an eos or something, hard to say.'
        updatedAt: '2023-10-21T18:05:38.994Z'
      numEdits: 0
      reactions: []
    id: 653412f26141b3927add2606
    type: comment
  author: jondurbin
  content: 'I actually had one too many spaces, the very last [/INST] should not have
    a space.  All intermediate instances should however.


    See this function and format_chat_airoboros below it:

    https://github.com/jondurbin/qlora/blob/8cd269bf9bd7753c92164934269019e12f23314f/train.py#L551


    In the earlier version, I accidentally had one extra space on line 563.


    I''m not sure why you''d see those in generation though, maybe an issue with missing
    eos tokens?  Could just be your dataset length exceeds model max length so it''s
    not getting to an eos or something, hard to say.'
  created_at: 2023-10-21 17:05:38+00:00
  edited: false
  hidden: false
  id: 653412f26141b3927add2606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-10-21T21:35:12.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9573289155960083
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>Thanks for the clarification. I have add_eos_token=True as a setting,
          and removed any  tokens in the dataset. I could have some records exceeding
          context, I will have to check (they are foreign language translations).
          I loaded up your dataset to learn more, using functions in the script you
          reference. I see you use llama 2 chat format on input feature, then output
          feature is just the response (after formatting function). I tried to load
          into my own SFTTrainer code, but I think it''s not set up for features other
          than the default ''text''. Also not familiar with the Seq2SeqTrainer you
          use-do you recommend over SFTTrainer? Thanks again!</p>

          '
        raw: Thanks for the clarification. I have add_eos_token=True as a setting,
          and removed any </s> tokens in the dataset. I could have some records exceeding
          context, I will have to check (they are foreign language translations).
          I loaded up your dataset to learn more, using functions in the script you
          reference. I see you use llama 2 chat format on input feature, then output
          feature is just the response (after formatting function). I tried to load
          into my own SFTTrainer code, but I think it's not set up for features other
          than the default 'text'. Also not familiar with the Seq2SeqTrainer you use-do
          you recommend over SFTTrainer? Thanks again!
        updatedAt: '2023-10-21T21:35:12.483Z'
      numEdits: 0
      reactions: []
    id: 653444101922c31a9bf75696
    type: comment
  author: Satya93
  content: Thanks for the clarification. I have add_eos_token=True as a setting, and
    removed any </s> tokens in the dataset. I could have some records exceeding context,
    I will have to check (they are foreign language translations). I loaded up your
    dataset to learn more, using functions in the script you reference. I see you
    use llama 2 chat format on input feature, then output feature is just the response
    (after formatting function). I tried to load into my own SFTTrainer code, but
    I think it's not set up for features other than the default 'text'. Also not familiar
    with the Seq2SeqTrainer you use-do you recommend over SFTTrainer? Thanks again!
  created_at: 2023-10-21 20:35:12+00:00
  edited: false
  hidden: false
  id: 653444101922c31a9bf75696
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-10-21T21:37:13.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.706426203250885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>I have been tweaking from guide here: <a rel="nofollow" href="https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html">https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html</a>,
          but not sure if I can use your dataset with this in SFTTrainer.</p>

          '
        raw: 'I have been tweaking from guide here: https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html,
          but not sure if I can use your dataset with this in SFTTrainer.'
        updatedAt: '2023-10-21T21:37:13.262Z'
      numEdits: 0
      reactions: []
    id: 653444896e86d670ffa0cce9
    type: comment
  author: Satya93
  content: 'I have been tweaking from guide here: https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html,
    but not sure if I can use your dataset with this in SFTTrainer.'
  created_at: 2023-10-21 20:37:13+00:00
  edited: false
  hidden: false
  id: 653444896e86d670ffa0cce9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-11-05T19:45:45.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9525116086006165
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jondurbin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jondurbin\">@<span class=\"\
          underline\">jondurbin</span></a></span>\n\n\t</span></span> When training,\
          \ do you then include that space before the trained AI response (that is,\
          \ is the AI expected to output a space as the first output token)? Otherwise,\
          \ I'm confused why this makes sense; after all, there's a space after [/INST]\
          \ before every other AI response in the history!</p>\n<p>So is the lack\
          \ of a space after [/INST] for the last query just a thing for inference,\
          \ and for masking loss during training, but the space is still there for\
          \ the last response while training?<br>Egs., we would train on:<br><code>\"\
          ...[/INST] Some response &lt;/s&gt;\"</code><br>But (assuming we are not\
          \ training on inputs), the <code>\"...[/INST]\"</code> is masked out for\
          \ loss computation, and the trained completion is <code>\"  Some response\
          \ &lt;/s&gt;\"</code>, including the space at the start (and before <code>&lt;/s&gt;</code>),\
          \ correct?</p>\n"
        raw: '@jondurbin When training, do you then include that space before the
          trained AI response (that is, is the AI expected to output a space as the
          first output token)? Otherwise, I''m confused why this makes sense; after
          all, there''s a space after [/INST] before every other AI response in the
          history!


          So is the lack of a space after [/INST] for the last query just a thing
          for inference, and for masking loss during training, but the space is still
          there for the last response while training?

          Egs., we would train on:

          `"...[/INST] Some response </s>"`

          But (assuming we are not training on inputs), the `"...[/INST]"` is masked
          out for loss computation, and the trained completion is `"  Some response
          </s>"`, including the space at the start (and before `</s>`), correct?'
        updatedAt: '2023-11-05T19:46:42.856Z'
      numEdits: 1
      reactions: []
    id: 6547f0e9c194936469776c6f
    type: comment
  author: grimulkan
  content: '@jondurbin When training, do you then include that space before the trained
    AI response (that is, is the AI expected to output a space as the first output
    token)? Otherwise, I''m confused why this makes sense; after all, there''s a space
    after [/INST] before every other AI response in the history!


    So is the lack of a space after [/INST] for the last query just a thing for inference,
    and for masking loss during training, but the space is still there for the last
    response while training?

    Egs., we would train on:

    `"...[/INST] Some response </s>"`

    But (assuming we are not training on inputs), the `"...[/INST]"` is masked out
    for loss computation, and the trained completion is `"  Some response </s>"`,
    including the space at the start (and before `</s>`), correct?'
  created_at: 2023-11-05 19:45:45+00:00
  edited: true
  hidden: false
  id: 6547f0e9c194936469776c6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-12-06T02:59:45.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9495024085044861
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>After some experimentation, I am getting blank responses in Oobabooga
          with a small probability when I train without the space for the last completion,
          vs adding the space, especially on completions with very long contexts.
          This problem doesn''t seem to be an issue if I include the extra space while
          training (so opposite of jondurbin''s statement?).</p>

          <p>This difference is removed by stripping the space in Ooba, but I believe
          Ooba does not do that by default. In other words, ooba ends its completion
          request with:<br><code>...[/INST]&lt;extra space here&gt;</code> so I think
          we should do the same while training.</p>

          <p>Would be happy to learn if others have figured it out and can explain
          it. There''s a lot of contradictory info out there.</p>

          '
        raw: 'After some experimentation, I am getting blank responses in Oobabooga
          with a small probability when I train without the space for the last completion,
          vs adding the space, especially on completions with very long contexts.
          This problem doesn''t seem to be an issue if I include the extra space while
          training (so opposite of jondurbin''s statement?).


          This difference is removed by stripping the space in Ooba, but I believe
          Ooba does not do that by default. In other words, ooba ends its completion
          request with:

          ```...[/INST]<extra space here>``` so I think we should do the same while
          training.


          Would be happy to learn if others have figured it out and can explain it.
          There''s a lot of contradictory info out there.'
        updatedAt: '2023-12-06T02:59:45.388Z'
      numEdits: 0
      reactions: []
    id: 656fe3a1f7ae78e148409bce
    type: comment
  author: grimulkan
  content: 'After some experimentation, I am getting blank responses in Oobabooga
    with a small probability when I train without the space for the last completion,
    vs adding the space, especially on completions with very long contexts. This problem
    doesn''t seem to be an issue if I include the extra space while training (so opposite
    of jondurbin''s statement?).


    This difference is removed by stripping the space in Ooba, but I believe Ooba
    does not do that by default. In other words, ooba ends its completion request
    with:

    ```...[/INST]<extra space here>``` so I think we should do the same while training.


    Would be happy to learn if others have figured it out and can explain it. There''s
    a lot of contradictory info out there.'
  created_at: 2023-12-06 02:59:45+00:00
  edited: false
  hidden: false
  id: 656fe3a1f7ae78e148409bce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-12-06T08:44:44.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8552882671356201
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;grimulkan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/grimulkan\">@<span class=\"\
          underline\">grimulkan</span></a></span>\n\n\t</span></span> Trust me, I\
          \ am confused about the situation and completely agree with you that intuitively,\
          \ it would be better to train with a single space, but it caused problems\
          \ when using the default llama-2 chat template in inference.</p>\n<p>The\
          \ official repo (llama-recipes) from meta has this:<br><a rel=\"nofollow\"\
          \ href=\"https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/src/llama_recipes/inference/chat_utils.py#L56\"\
          >https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/src/llama_recipes/inference/chat_utils.py#L56</a></p>\n\
          <p>In their chat utils, they format the inference prompt with no space after\
          \ the last <code>[/INST]</code></p>\n<p>Unfortunately, their repo doesn't\
          \ have a complete example for fine-tuning, but they have a custom dataset\
          \ example, which also has no space:<br><a rel=\"nofollow\" href=\"https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/examples/custom_dataset.py#L14\"\
          >https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/examples/custom_dataset.py#L14</a></p>\n\
          <p>I really don't know.</p>\n"
        raw: '@grimulkan Trust me, I am confused about the situation and completely
          agree with you that intuitively, it would be better to train with a single
          space, but it caused problems when using the default llama-2 chat template
          in inference.


          The official repo (llama-recipes) from meta has this:

          https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/src/llama_recipes/inference/chat_utils.py#L56


          In their chat utils, they format the inference prompt with no space after
          the last `[/INST]`


          Unfortunately, their repo doesn''t have a complete example for fine-tuning,
          but they have a custom dataset example, which also has no space:

          https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/examples/custom_dataset.py#L14


          I really don''t know.'
        updatedAt: '2023-12-06T08:44:44.645Z'
      numEdits: 0
      reactions: []
    id: 6570347c24baa8755061cde8
    type: comment
  author: jondurbin
  content: '@grimulkan Trust me, I am confused about the situation and completely
    agree with you that intuitively, it would be better to train with a single space,
    but it caused problems when using the default llama-2 chat template in inference.


    The official repo (llama-recipes) from meta has this:

    https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/src/llama_recipes/inference/chat_utils.py#L56


    In their chat utils, they format the inference prompt with no space after the
    last `[/INST]`


    Unfortunately, their repo doesn''t have a complete example for fine-tuning, but
    they have a custom dataset example, which also has no space:

    https://github.com/facebookresearch/llama-recipes/blob/cecad84841d669413a61b68b95c22c8d33a98f03/examples/custom_dataset.py#L14


    I really don''t know.'
  created_at: 2023-12-06 08:44:44+00:00
  edited: false
  hidden: false
  id: 6570347c24baa8755061cde8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-12-06T09:20:38.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9481802582740784
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Maybe that''s the explanation. Meta used their inference code (in
          chat_utils.py) which didn''t include the space, and that matches with how
          they trained it. But Oobabooga (and probably other clients) that we use,
          do add the space I think. So technically, that means what we''re calling
          Llama-chat format is not really Llama-chat in some clients. I am not even
          sure the Ooba template structure allows for a non-space there (unless they
          forcibly strip() the input each time).</p>

          '
        raw: Maybe that's the explanation. Meta used their inference code (in chat_utils.py)
          which didn't include the space, and that matches with how they trained it.
          But Oobabooga (and probably other clients) that we use, do add the space
          I think. So technically, that means what we're calling Llama-chat format
          is not really Llama-chat in some clients. I am not even sure the Ooba template
          structure allows for a non-space there (unless they forcibly strip() the
          input each time).
        updatedAt: '2023-12-06T09:20:38.473Z'
      numEdits: 0
      reactions: []
    id: 65703ce6bb6a8fc22114b83a
    type: comment
  author: grimulkan
  content: Maybe that's the explanation. Meta used their inference code (in chat_utils.py)
    which didn't include the space, and that matches with how they trained it. But
    Oobabooga (and probably other clients) that we use, do add the space I think.
    So technically, that means what we're calling Llama-chat format is not really
    Llama-chat in some clients. I am not even sure the Ooba template structure allows
    for a non-space there (unless they forcibly strip() the input each time).
  created_at: 2023-12-06 09:20:38+00:00
  edited: false
  hidden: false
  id: 65703ce6bb6a8fc22114b83a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jondurbin/airoboros-l2-70b-3.1.2
repo_type: model
status: open
target_branch: null
title: Space after [/INST]
