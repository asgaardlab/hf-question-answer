!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Fvisticot
conflicting_files: null
created_at: 2023-11-04 21:05:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a84816cfbfd0e5796e9e7bcf5e37e672.svg
      fullname: Frederic Visticot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fvisticot
      type: user
    createdAt: '2023-11-04T22:05:55.000Z'
    data:
      edited: false
      editors:
      - Fvisticot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46325626969337463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a84816cfbfd0e5796e9e7bcf5e37e672.svg
          fullname: Frederic Visticot
          isHf: false
          isPro: false
          name: Fvisticot
          type: user
        html: '<p>import json<br>from sagemaker.huggingface import HuggingFaceModel</p>

          <h1 id="sagemaker-config">sagemaker config</h1>

          <p>instance_type = "ml.g5.48xlarge"<br>number_of_gpu = 8<br>health_check_timeout
          = 900</p>

          <h1 id="define-model-and-endpoint-configuration-parameter">Define Model
          and Endpoint configuration parameter</h1>

          <p>config = {<br>  ''HF_MODEL_ID'': "liuhaotian/llava-v1.5-13b", # model_id
          from hf.co/models<br>  ''SM_NUM_GPUS'': json.dumps(number_of_gpu), # Number
          of GPU used per replica<br>  ''MAX_INPUT_LENGTH'': json.dumps(1024),  #
          Max length of input text<br>  ''MAX_TOTAL_TOKENS'': json.dumps(2048),  #
          Max length of the generation (including input text)<br>  ''MAX_BATCH_TOTAL_TOKENS'':
          json.dumps(8192),  # Limits the number of tokens that can be processed in
          parallel during the generation<br>  ''HF_MODEL_QUANTIZE'': "bitsandbytes",
          # comment in to quantize<br>}</p>

          <h1 id="create-huggingfacemodel-with-the-image-uri">create HuggingFaceModel
          with the image uri</h1>

          <p>llm_model = HuggingFaceModel(<br>  role=role,<br>  image_uri=llm_image,<br>  env=config<br>)</p>

          <h1 id="deploy-model-to-an-endpoint">Deploy model to an endpoint</h1>

          <h1 id="httpssagemakerreadthedocsioenstableapiinferencemodelhtmlsagemakermodelmodeldeploy"><a
          rel="nofollow" href="https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy">https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy</a></h1>

          <p>llm = llm_model.deploy(<br>  initial_instance_count=1,<br>  instance_type=instance_type,<br>  container_startup_health_check_timeout=health_check_timeout,
          # 10 minutes to be able to load the model<br>)</p>

          '
        raw: "import json\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\n\
          \r\n# sagemaker config\r\ninstance_type = \"ml.g5.48xlarge\"\r\nnumber_of_gpu\
          \ = 8\r\nhealth_check_timeout = 900\r\n\r\n# Define Model and Endpoint configuration\
          \ parameter\r\nconfig = {\r\n  'HF_MODEL_ID': \"liuhaotian/llava-v1.5-13b\"\
          , # model_id from hf.co/models\r\n  'SM_NUM_GPUS': json.dumps(number_of_gpu),\
          \ # Number of GPU used per replica\r\n  'MAX_INPUT_LENGTH': json.dumps(1024),\
          \  # Max length of input text\r\n  'MAX_TOTAL_TOKENS': json.dumps(2048),\
          \  # Max length of the generation (including input text)\r\n  'MAX_BATCH_TOTAL_TOKENS':\
          \ json.dumps(8192),  # Limits the number of tokens that can be processed\
          \ in parallel during the generation\r\n  'HF_MODEL_QUANTIZE': \"bitsandbytes\"\
          , # comment in to quantize\r\n}\r\n\r\n# create HuggingFaceModel with the\
          \ image uri\r\nllm_model = HuggingFaceModel(\r\n  role=role,\r\n  image_uri=llm_image,\r\
          \n  env=config\r\n)\r\n\r\n# Deploy model to an endpoint\r\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\r\
          \nllm = llm_model.deploy(\r\n  initial_instance_count=1,\r\n  instance_type=instance_type,\r\
          \n  container_startup_health_check_timeout=health_check_timeout, # 10 minutes\
          \ to be able to load the model\r\n)"
        updatedAt: '2023-11-04T22:05:55.687Z'
      numEdits: 0
      reactions: []
    id: 6546c043cbe50f378d4b7a07
    type: comment
  author: Fvisticot
  content: "import json\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\n\r\
    \n# sagemaker config\r\ninstance_type = \"ml.g5.48xlarge\"\r\nnumber_of_gpu =\
    \ 8\r\nhealth_check_timeout = 900\r\n\r\n# Define Model and Endpoint configuration\
    \ parameter\r\nconfig = {\r\n  'HF_MODEL_ID': \"liuhaotian/llava-v1.5-13b\", #\
    \ model_id from hf.co/models\r\n  'SM_NUM_GPUS': json.dumps(number_of_gpu), #\
    \ Number of GPU used per replica\r\n  'MAX_INPUT_LENGTH': json.dumps(1024),  #\
    \ Max length of input text\r\n  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length\
    \ of the generation (including input text)\r\n  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),\
    \  # Limits the number of tokens that can be processed in parallel during the\
    \ generation\r\n  'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\r\
    \n}\r\n\r\n# create HuggingFaceModel with the image uri\r\nllm_model = HuggingFaceModel(\r\
    \n  role=role,\r\n  image_uri=llm_image,\r\n  env=config\r\n)\r\n\r\n# Deploy\
    \ model to an endpoint\r\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\r\
    \nllm = llm_model.deploy(\r\n  initial_instance_count=1,\r\n  instance_type=instance_type,\r\
    \n  container_startup_health_check_timeout=health_check_timeout, # 10 minutes\
    \ to be able to load the model\r\n)"
  created_at: 2023-11-04 21:05:55+00:00
  edited: false
  hidden: false
  id: 6546c043cbe50f378d4b7a07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-30T10:01:12.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.697651207447052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>llava is not part of HF Transformer, you need to use llava git repo''s
          LLavLlama modle loader</p>

          '
        raw: llava is not part of HF Transformer, you need to use llava git repo's
          LLavLlama modle loader
        updatedAt: '2023-11-30T10:01:12.665Z'
      numEdits: 0
      reactions: []
    id: 65685d686b1e4d61d8f9b8cf
    type: comment
  author: Yhyu13
  content: llava is not part of HF Transformer, you need to use llava git repo's LLavLlama
    modle loader
  created_at: 2023-11-30 10:01:12+00:00
  edited: false
  hidden: false
  id: 65685d686b1e4d61d8f9b8cf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: liuhaotian/llava-v1.5-13b
repo_type: model
status: open
target_branch: null
title: 'Error when deploying the model on Sagemaker : ValueError: sharded is not supported
  for AutoModel'
