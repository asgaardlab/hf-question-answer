!!python/object:huggingface_hub.community.DiscussionWithDetails
author: devymex
conflicting_files: null
created_at: 2023-07-21 04:05:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce40d4eab6b84d420252e40eedff0bbb.svg
      fullname: Devymex Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devymex
      type: user
    createdAt: '2023-07-21T05:05:54.000Z'
    data:
      edited: true
      editors:
      - devymex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3876473307609558
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce40d4eab6b84d420252e40eedff0bbb.svg
          fullname: Devymex Wang
          isHf: false
          isPro: false
          name: devymex
          type: user
        html: "<p>I run the sample code :</p>\n<pre><code class=\"language-py\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">'/data/models/codegen25/7b-instruct'</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">'/data/models/codegen25/7b-instruct'</span>)\n\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >format</span>(<span class=\"hljs-params\">prefix, suffix</span>):\n  <span\
          \ class=\"hljs-keyword\">return</span> prefix + <span class=\"hljs-string\"\
          >\"&lt;mask_1&gt;\"</span> + suffix + <span class=\"hljs-string\">\"&lt;|endoftext|&gt;\"\
          </span> + <span class=\"hljs-string\">\"&lt;sep&gt;\"</span> + <span class=\"\
          hljs-string\">\"&lt;mask_1&gt;\"</span>\n\n\nprefix = <span class=\"hljs-string\"\
          >\"def hello_world():\\n    \"</span>\nsuffix = <span class=\"hljs-string\"\
          >\"    return name\"</span>\ntext = <span class=\"hljs-built_in\">format</span>(prefix,\
          \ suffix)\ninput_ids = tokenizer(text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>).input_ids\ngenerated_ids = model.generate(input_ids, max_length=<span\
          \ class=\"hljs-number\">128</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(generated_ids[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >False</span>)[<span class=\"hljs-built_in\">len</span>(text):])\n</code></pre>\n\
          <p>Got following results:</p>\n<pre><code>    return render_template('&lt;eom&gt;&lt;|endoftext|&gt;#\n\
          </code></pre>\n<p>It doesn't seem to be the expected result.</p>\n<pre><code>$\
          \ md5sum ./*\n72abc1c968a3591ca78b4b3627182151  ./config.json\n185162afdfbe7b61b786b1556233efcb\
          \  ./generation_config.json\na859f8a89685747ffd4171b870540c41  ./gitattributes.txt\n\
          957e7d6eba323e9fadfe67a0fc235fa5  ./pytorch_model-00001-of-00003.bin\n0d25abaa01bde623d3c9b2c7e052f240\
          \  ./pytorch_model-00002-of-00003.bin\n62e4b3239286f72cafc5e3f55b8d1cf2\
          \  ./pytorch_model-00003-of-00003.bin\n238155cf5ccec23d742a2c2347063a15\
          \  ./pytorch_model.bin.index.json\ne0d2431919f2d456fbc22f2aaf4488d7  ./README.md\n\
          cf2859a1a9efba39aa84d82b0f3ef426  ./tokenization_codegen25.py\nfd3285d0e1655a66e051cfb520afb8e0\
          \  ./tokenizer_config.json\n</code></pre>\n"
        raw: "I run the sample code :\n\n```py\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained('/data/models/codegen25/7b-instruct',\
          \ trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('/data/models/codegen25/7b-instruct')\n\
          \n\ndef format(prefix, suffix):\n  return prefix + \"<mask_1>\" + suffix\
          \ + \"<|endoftext|>\" + \"<sep>\" + \"<mask_1>\"\n\n\nprefix = \"def hello_world():\\\
          n    \"\nsuffix = \"    return name\"\ntext = format(prefix, suffix)\ninput_ids\
          \ = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_ids = model.generate(input_ids,\
          \ max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=False)[len(text):])\n\
          ```\n\nGot following results:\n\n```\n    return render_template('<eom><|endoftext|>#\n\
          ```\n\nIt doesn't seem to be the expected result.\n\n```\n$ md5sum ./*\n\
          72abc1c968a3591ca78b4b3627182151  ./config.json\n185162afdfbe7b61b786b1556233efcb\
          \  ./generation_config.json\na859f8a89685747ffd4171b870540c41  ./gitattributes.txt\n\
          957e7d6eba323e9fadfe67a0fc235fa5  ./pytorch_model-00001-of-00003.bin\n0d25abaa01bde623d3c9b2c7e052f240\
          \  ./pytorch_model-00002-of-00003.bin\n62e4b3239286f72cafc5e3f55b8d1cf2\
          \  ./pytorch_model-00003-of-00003.bin\n238155cf5ccec23d742a2c2347063a15\
          \  ./pytorch_model.bin.index.json\ne0d2431919f2d456fbc22f2aaf4488d7  ./README.md\n\
          cf2859a1a9efba39aa84d82b0f3ef426  ./tokenization_codegen25.py\nfd3285d0e1655a66e051cfb520afb8e0\
          \  ./tokenizer_config.json\n\n```"
        updatedAt: '2023-07-21T05:10:04.155Z'
      numEdits: 2
      reactions: []
    id: 64ba1232ad5725d2d3b0c16f
    type: comment
  author: devymex
  content: "I run the sample code :\n\n```py\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained('/data/models/codegen25/7b-instruct',\
    \ trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('/data/models/codegen25/7b-instruct')\n\
    \n\ndef format(prefix, suffix):\n  return prefix + \"<mask_1>\" + suffix + \"\
    <|endoftext|>\" + \"<sep>\" + \"<mask_1>\"\n\n\nprefix = \"def hello_world():\\\
    n    \"\nsuffix = \"    return name\"\ntext = format(prefix, suffix)\ninput_ids\
    \ = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_ids = model.generate(input_ids,\
    \ max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=False)[len(text):])\n\
    ```\n\nGot following results:\n\n```\n    return render_template('<eom><|endoftext|>#\n\
    ```\n\nIt doesn't seem to be the expected result.\n\n```\n$ md5sum ./*\n72abc1c968a3591ca78b4b3627182151\
    \  ./config.json\n185162afdfbe7b61b786b1556233efcb  ./generation_config.json\n\
    a859f8a89685747ffd4171b870540c41  ./gitattributes.txt\n957e7d6eba323e9fadfe67a0fc235fa5\
    \  ./pytorch_model-00001-of-00003.bin\n0d25abaa01bde623d3c9b2c7e052f240  ./pytorch_model-00002-of-00003.bin\n\
    62e4b3239286f72cafc5e3f55b8d1cf2  ./pytorch_model-00003-of-00003.bin\n238155cf5ccec23d742a2c2347063a15\
    \  ./pytorch_model.bin.index.json\ne0d2431919f2d456fbc22f2aaf4488d7  ./README.md\n\
    cf2859a1a9efba39aa84d82b0f3ef426  ./tokenization_codegen25.py\nfd3285d0e1655a66e051cfb520afb8e0\
    \  ./tokenizer_config.json\n\n```"
  created_at: 2023-07-21 04:05:54+00:00
  edited: true
  hidden: false
  id: 64ba1232ad5725d2d3b0c16f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Salesforce/codegen25-7b-instruct
repo_type: model
status: open
target_branch: null
title: The sample codes generates bad code
