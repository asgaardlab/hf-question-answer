!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ymoslem
conflicting_files: null
created_at: 2023-08-04 17:35:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/609bb531a1027a037fdba8a21d6ca586.svg
      fullname: Yasmin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ymoslem
      type: user
    createdAt: '2023-08-04T18:35:59.000Z'
    data:
      edited: false
      editors:
      - ymoslem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7087664008140564
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/609bb531a1027a037fdba8a21d6ca586.svg
          fullname: Yasmin
          isHf: false
          isPro: false
          name: ymoslem
          type: user
        html: '<p>Hello!</p>

          <p>I tried this code:</p>

          <pre><code>from transformers import AutoModelForCausalLM

          model = AutoModelForCausalLM.from_pretrained("amazon/FalconLite", trust_remote_code=True)

          </code></pre>

          <p>It results in this error:</p>

          <pre><code>OSError: amazon/FalconLite does not appear to have a file named
          pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

          </code></pre>

          <p>I am <em>not</em> using SageMaker. Just a regular GPU.</p>

          <p>Thanks!</p>

          '
        raw: "Hello!\r\n\r\nI tried this code:\r\n\r\n```\r\nfrom transformers import\
          \ AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          amazon/FalconLite\", trust_remote_code=True)\r\n```\r\n\r\nIt results in\
          \ this error:\r\n```\r\nOSError: amazon/FalconLite does not appear to have\
          \ a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\r\
          \n```\r\n\r\nI am *not* using SageMaker. Just a regular GPU.\r\n\r\nThanks!"
        updatedAt: '2023-08-04T18:35:59.113Z'
      numEdits: 0
      reactions: []
    id: 64cd450f749587dbe0d3a108
    type: comment
  author: ymoslem
  content: "Hello!\r\n\r\nI tried this code:\r\n\r\n```\r\nfrom transformers import\
    \ AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"amazon/FalconLite\"\
    , trust_remote_code=True)\r\n```\r\n\r\nIt results in this error:\r\n```\r\nOSError:\
    \ amazon/FalconLite does not appear to have a file named pytorch_model.bin, tf_model.h5,\
    \ model.ckpt or flax_model.msgpack.\r\n```\r\n\r\nI am *not* using SageMaker.\
    \ Just a regular GPU.\r\n\r\nThanks!"
  created_at: 2023-08-04 17:35:59+00:00
  edited: false
  hidden: false
  id: 64cd450f749587dbe0d3a108
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12783cc210c57b1508beb5bc33741921.svg
      fullname: Monil Joshi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: moniljo
      type: user
    createdAt: '2023-08-05T02:04:50.000Z'
    data:
      edited: false
      editors:
      - moniljo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8544532060623169
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12783cc210c57b1508beb5bc33741921.svg
          fullname: Monil Joshi
          isHf: false
          isPro: false
          name: moniljo
          type: user
        html: '<p>You''ll need to add the model_basename as well -&gt; gptq_model-4bit-128g.safetensors<br>However,
          it looks like generation_config.json is missing, so it won''t work locally<br>Error:<br>OSError:
          amazon/FalconLite does not appear to have a file named generation_config.json.
          Checkout ''<a href="https://huggingface.co/amazon/FalconLite/main''">https://huggingface.co/amazon/FalconLite/main''</a>
          for available files.</p>

          '
        raw: 'You''ll need to add the model_basename as well -> gptq_model-4bit-128g.safetensors

          However, it looks like generation_config.json is missing, so it won''t work
          locally

          Error:

          OSError: amazon/FalconLite does not appear to have a file named generation_config.json.
          Checkout ''https://huggingface.co/amazon/FalconLite/main'' for available
          files.'
        updatedAt: '2023-08-05T02:04:50.178Z'
      numEdits: 0
      reactions: []
    id: 64cdae42995a0b5d59234a71
    type: comment
  author: moniljo
  content: 'You''ll need to add the model_basename as well -> gptq_model-4bit-128g.safetensors

    However, it looks like generation_config.json is missing, so it won''t work locally

    Error:

    OSError: amazon/FalconLite does not appear to have a file named generation_config.json.
    Checkout ''https://huggingface.co/amazon/FalconLite/main'' for available files.'
  created_at: 2023-08-05 01:04:50+00:00
  edited: false
  hidden: false
  id: 64cdae42995a0b5d59234a71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f2e815b15a0f2e3d2c19a27b6e32ddf.svg
      fullname: Taeuk Jang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jangtu052
      type: user
    createdAt: '2023-08-11T00:17:52.000Z'
    data:
      edited: false
      editors:
      - jangtu052
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9244949817657471
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f2e815b15a0f2e3d2c19a27b6e32ddf.svg
          fullname: Taeuk Jang
          isHf: false
          isPro: false
          name: jangtu052
          type: user
        html: '<p>I guess they only made change in TGI codebase, so it won''t run
          locally with scaledRoPE because there is no code change from the original
          Falcon repo. Please correct me if I am wrong and eager to learn.</p>

          '
        raw: I guess they only made change in TGI codebase, so it won't run locally
          with scaledRoPE because there is no code change from the original Falcon
          repo. Please correct me if I am wrong and eager to learn.
        updatedAt: '2023-08-11T00:17:52.601Z'
      numEdits: 0
      reactions: []
    id: 64d57e30abf475a808949850
    type: comment
  author: jangtu052
  content: I guess they only made change in TGI codebase, so it won't run locally
    with scaledRoPE because there is no code change from the original Falcon repo.
    Please correct me if I am wrong and eager to learn.
  created_at: 2023-08-10 23:17:52+00:00
  edited: false
  hidden: false
  id: 64d57e30abf475a808949850
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c957095433a70209c93f8536a471704.svg
      fullname: thiago martins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thiago-weni
      type: user
    createdAt: '2023-08-11T17:46:05.000Z'
    data:
      edited: false
      editors:
      - thiago-weni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9770983457565308
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c957095433a70209c93f8536a471704.svg
          fullname: thiago martins
          isHf: false
          isPro: false
          name: thiago-weni
          type: user
        html: '<p>Does it means there is no way to finetune it (again)?</p>

          '
        raw: Does it means there is no way to finetune it (again)?
        updatedAt: '2023-08-11T17:46:05.310Z'
      numEdits: 0
      reactions: []
    id: 64d673dd2fe2c112641a27e3
    type: comment
  author: thiago-weni
  content: Does it means there is no way to finetune it (again)?
  created_at: 2023-08-11 16:46:05+00:00
  edited: false
  hidden: false
  id: 64d673dd2fe2c112641a27e3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: amazon/FalconLite
repo_type: model
status: open
target_branch: null
title: 'OSError: no file named pytorch_model.bin'
