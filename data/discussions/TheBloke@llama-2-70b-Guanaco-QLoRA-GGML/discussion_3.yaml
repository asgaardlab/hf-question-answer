!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ThamaluM
conflicting_files: null
created_at: 2023-07-28 07:36:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
      fullname: Thamalu Piyadigama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThamaluM
      type: user
    createdAt: '2023-07-28T08:36:07.000Z'
    data:
      edited: false
      editors:
      - ThamaluM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33814942836761475
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
          fullname: Thamalu Piyadigama
          isHf: false
          isPro: false
          name: ThamaluM
          type: user
        html: '<p>error loading model: llama.cpp: tensor ''layers.0.attention.wk.weight''
          has wrong shape; expected 8192 x 8192, got 8192 x 1024</p>

          <p>Got this error. Loaded using llama-cpp-python in Linux. Python3.11, llama-cpp-python
          0.1.77<br>(model:<br>llama-2-70b.ggmlv3.q2_K.bin)</p>

          '
        raw: "error loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
          \ has wrong shape; expected 8192 x 8192, got 8192 x 1024\r\n\r\nGot this\
          \ error. Loaded using llama-cpp-python in Linux. Python3.11, llama-cpp-python\
          \ 0.1.77 \r\n(model: \r\nllama-2-70b.ggmlv3.q2_K.bin)\r\n"
        updatedAt: '2023-07-28T08:36:07.757Z'
      numEdits: 0
      reactions: []
    id: 64c37df7fcc9fa13aaf84026
    type: comment
  author: ThamaluM
  content: "error loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
    \ has wrong shape; expected 8192 x 8192, got 8192 x 1024\r\n\r\nGot this error.\
    \ Loaded using llama-cpp-python in Linux. Python3.11, llama-cpp-python 0.1.77\
    \ \r\n(model: \r\nllama-2-70b.ggmlv3.q2_K.bin)\r\n"
  created_at: 2023-07-28 07:36:07+00:00
  edited: false
  hidden: false
  id: 64c37df7fcc9fa13aaf84026
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
      fullname: Thamalu Piyadigama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThamaluM
      type: user
    createdAt: '2023-07-28T08:43:00.000Z'
    data:
      edited: true
      editors:
      - ThamaluM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6714812517166138
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
          fullname: Thamalu Piyadigama
          isHf: false
          isPro: false
          name: ThamaluM
          type: user
        html: '<p>Thanks for the answer.. I found the following instruction and it
          worked.</p>

          <blockquote>

          <blockquote>

          <p>Loading llama-2 70b<br>Llama2 70b must set the n_gqa parameter (grouped-query
          attention factor) to 8 when loading:<br><code>llm = Llama(model_path="./models/70B/ggml-model.bin",
          n_gqa=8)</code></p>

          </blockquote>

          </blockquote>

          '
        raw: "Thanks for the answer.. I found the following instruction and it worked.\n\
          \n>> Loading llama-2 70b\n>> Llama2 70b must set the n_gqa parameter (grouped-query\
          \ attention factor) to 8 when loading:\n >> ``llm = Llama(model_path=\"\
          ./models/70B/ggml-model.bin\", n_gqa=8)``"
        updatedAt: '2023-07-28T08:44:36.120Z'
      numEdits: 3
      reactions: []
    id: 64c37f947918ee895ba236de
    type: comment
  author: ThamaluM
  content: "Thanks for the answer.. I found the following instruction and it worked.\n\
    \n>> Loading llama-2 70b\n>> Llama2 70b must set the n_gqa parameter (grouped-query\
    \ attention factor) to 8 when loading:\n >> ``llm = Llama(model_path=\"./models/70B/ggml-model.bin\"\
    , n_gqa=8)``"
  created_at: 2023-07-28 07:43:00+00:00
  edited: true
  hidden: false
  id: 64c37f947918ee895ba236de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd6eb94e9bef729022a944770c00de4f.svg
      fullname: Javier Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: JavierRCam
      type: user
    createdAt: '2023-08-01T20:36:44.000Z'
    data:
      edited: false
      editors:
      - JavierRCam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8712465763092041
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd6eb94e9bef729022a944770c00de4f.svg
          fullname: Javier Rojas
          isHf: false
          isPro: true
          name: JavierRCam
          type: user
        html: '<p>Hello, it says that it is valid with "langchain," but when I try
          to use it with "RetrievalQA," it gives the following error: </p>

          <p>ValidationError: 1 validation error for LLMChain<br>llm value is not
          a valid dict (type=type_error.dict)</p>

          '
        raw: "Hello, it says that it is valid with \"langchain,\" but when I try to\
          \ use it with \"RetrievalQA,\" it gives the following error: \n\nValidationError:\
          \ 1 validation error for LLMChain\nllm value is not a valid dict (type=type_error.dict)"
        updatedAt: '2023-08-01T20:36:44.202Z'
      numEdits: 0
      reactions: []
    id: 64c96cdc166b73558340ec9f
    type: comment
  author: JavierRCam
  content: "Hello, it says that it is valid with \"langchain,\" but when I try to\
    \ use it with \"RetrievalQA,\" it gives the following error: \n\nValidationError:\
    \ 1 validation error for LLMChain\nllm value is not a valid dict (type=type_error.dict)"
  created_at: 2023-08-01 19:36:44+00:00
  edited: false
  hidden: false
  id: 64c96cdc166b73558340ec9f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/llama-2-70b-Guanaco-QLoRA-GGML
repo_type: model
status: open
target_branch: null
title: 'Error loading model '
