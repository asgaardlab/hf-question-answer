!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-07-28 03:35:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-28T04:35:25.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8990761637687683
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Idk if they made an update since you made your model card for the
          airoboros 70b ggml model you posted, but gpu acceleration is working fine
          in oobagooba in this model, llama-2-70b-guanaco-QLoRA-GGML</p>

          '
        raw: Idk if they made an update since you made your model card for the airoboros
          70b ggml model you posted, but gpu acceleration is working fine in oobagooba
          in this model, llama-2-70b-guanaco-QLoRA-GGML
        updatedAt: '2023-07-28T04:35:25.948Z'
      numEdits: 0
      reactions: []
    id: 64c3458d96b84732a328a7da
    type: comment
  author: rombodawg
  content: Idk if they made an update since you made your model card for the airoboros
    70b ggml model you posted, but gpu acceleration is working fine in oobagooba in
    this model, llama-2-70b-guanaco-QLoRA-GGML
  created_at: 2023-07-28 03:35:25+00:00
  edited: false
  hidden: false
  id: 64c3458d96b84732a328a7da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
      fullname: Thamalu Piyadigama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThamaluM
      type: user
    createdAt: '2023-07-28T08:32:56.000Z'
    data:
      edited: false
      editors:
      - ThamaluM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44515031576156616
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
          fullname: Thamalu Piyadigama
          isHf: false
          isPro: false
          name: ThamaluM
          type: user
        html: '<p><code>error loading model: llama.cpp: tensor ''layers.0.attention.wk.weight''
          has wrong shape; expected  8192 x  8192, got  8192 x  1024</code></p>

          <p>Got this error. Loaded using llama-cpp-python in Linux. Python3.11, llama-cpp-python
          0.1.77</p>

          '
        raw: '``error loading model: llama.cpp: tensor ''layers.0.attention.wk.weight''
          has wrong shape; expected  8192 x  8192, got  8192 x  1024``


          Got this error. Loaded using llama-cpp-python in Linux. Python3.11, llama-cpp-python
          0.1.77'
        updatedAt: '2023-07-28T08:32:56.252Z'
      numEdits: 0
      reactions: []
    id: 64c37d38b63057f1fd3b2ac5
    type: comment
  author: ThamaluM
  content: '``error loading model: llama.cpp: tensor ''layers.0.attention.wk.weight''
    has wrong shape; expected  8192 x  8192, got  8192 x  1024``


    Got this error. Loaded using llama-cpp-python in Linux. Python3.11, llama-cpp-python
    0.1.77'
  created_at: 2023-07-28 07:32:56+00:00
  edited: false
  hidden: false
  id: 64c37d38b63057f1fd3b2ac5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-28T08:36:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8916935920715332
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rombodawg&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rombodawg\">@<span class=\"\
          underline\">rombodawg</span></a></span>\n\n\t</span></span> thank you, I've\
          \ updated my READMEs</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;ThamaluM&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ThamaluM\"\
          >@<span class=\"underline\">ThamaluM</span></a></span>\n\n\t</span></span>\
          \ this is probably because you've not passed the <code>-gqa 8</code> parameter.\
          \ I don't know how you do that with llama-cpp-python, but there must be\
          \ a way to do it as text-generation-webui works OK with these models and\
          \ it uses llama-cpp-python.  Check the llama-cpp-python repo and see if\
          \ there's instructions there.</p>\n"
        raw: '@rombodawg thank you, I''ve updated my READMEs


          @ThamaluM this is probably because you''ve not passed the `-gqa 8` parameter.
          I don''t know how you do that with llama-cpp-python, but there must be a
          way to do it as text-generation-webui works OK with these models and it
          uses llama-cpp-python.  Check the llama-cpp-python repo and see if there''s
          instructions there.'
        updatedAt: '2023-07-28T08:36:51.665Z'
      numEdits: 0
      reactions: []
    id: 64c37e235570719ae223a9cb
    type: comment
  author: TheBloke
  content: '@rombodawg thank you, I''ve updated my READMEs


    @ThamaluM this is probably because you''ve not passed the `-gqa 8` parameter.
    I don''t know how you do that with llama-cpp-python, but there must be a way to
    do it as text-generation-webui works OK with these models and it uses llama-cpp-python.  Check
    the llama-cpp-python repo and see if there''s instructions there.'
  created_at: 2023-07-28 07:36:51+00:00
  edited: false
  hidden: false
  id: 64c37e235570719ae223a9cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
      fullname: Thamalu Piyadigama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThamaluM
      type: user
    createdAt: '2023-07-28T08:45:55.000Z'
    data:
      edited: false
      editors:
      - ThamaluM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9800426363945007
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddfbd811a974d0615e4ce9b25747d04b.svg
          fullname: Thamalu Piyadigama
          isHf: false
          isPro: false
          name: ThamaluM
          type: user
        html: '<p>Thank for the answer it worked. </p>

          '
        raw: 'Thank for the answer it worked. '
        updatedAt: '2023-07-28T08:45:55.029Z'
      numEdits: 0
      reactions: []
    id: 64c380432545765de9f4975c
    type: comment
  author: ThamaluM
  content: 'Thank for the answer it worked. '
  created_at: 2023-07-28 07:45:55+00:00
  edited: false
  hidden: false
  id: 64c380432545765de9f4975c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/llama-2-70b-Guanaco-QLoRA-GGML
repo_type: model
status: open
target_branch: null
title: GPU acceleration working in Oobagooba Webui
