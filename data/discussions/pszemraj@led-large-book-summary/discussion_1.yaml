!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kmfoda
conflicting_files: null
created_at: 2022-06-23 08:22:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2022-06-23T09:22:10.000Z'
    data:
      edited: false
      editors:
      - kmfoda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
          fullname: Karim Foda
          isHf: false
          isPro: false
          name: kmfoda
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pszemraj&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pszemraj\"\
          >@<span class=\"underline\">pszemraj</span></a></span>\n\n\t</span></span>!\
          \ This is super cool thanks for working on this and sharing. Do you mind\
          \ sharing your training set up in terms of GPUs etc? I'm trying to partition\
          \ this model across a number of GPUs to avoid OOM errors using DeepSpeed\
          \ but that's failing. I'm using 4 NVDIA Tesla V100 GPUs with 32GB RAM but\
          \ still running out of memory.</p>\n"
        raw: Hi @pszemraj! This is super cool thanks for working on this and sharing.
          Do you mind sharing your training set up in terms of GPUs etc? I'm trying
          to partition this model across a number of GPUs to avoid OOM errors using
          DeepSpeed but that's failing. I'm using 4 NVDIA Tesla V100 GPUs with 32GB
          RAM but still running out of memory.
        updatedAt: '2022-06-23T09:22:10.195Z'
      numEdits: 0
      reactions: []
    id: 62b430c25d5df1b8217da144
    type: comment
  author: kmfoda
  content: Hi @pszemraj! This is super cool thanks for working on this and sharing.
    Do you mind sharing your training set up in terms of GPUs etc? I'm trying to partition
    this model across a number of GPUs to avoid OOM errors using DeepSpeed but that's
    failing. I'm using 4 NVDIA Tesla V100 GPUs with 32GB RAM but still running out
    of memory.
  created_at: 2022-06-23 08:22:10+00:00
  edited: false
  hidden: false
  id: 62b430c25d5df1b8217da144
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-06-24T10:24:21.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Hey! Thanks for uploading the dataset, so I didn''t have to run
          the script off their repo :) </p>

          <p>Sure, I''m currently traveling, so let me know if you want more detail,
          and I can follow up once I''m back:</p>

          <ul>

          <li>I use a single V100 and 52 GB CPU RAM, deepspeed ZeRO 2; see config
          I just uploaded <a href="https://huggingface.co/pszemraj/led-large-book-summary/blob/main/ds_config_zero2.json">here</a></li>

          <li>due to memory constraints, I keep the batch size at one and just crank
          gradient accumulation, usually 64 or higher</li>

          <li>from the length values, I think <code>max_length</code> for outputs
          1024 captures <em>most</em> of the dataset, so I filter out all summaries
          that are longer than that </li>

          <li>if still facing memory constraints, I think you can also keep max_input_length
          (for training) at 8192 and filter out things more extended. Anecdotally,
          the model can still run <code>predict</code> on 16384 and still produce
          good summaries (I guess scaling what it learned when running inference).
          IIRC most of the summaries are shorter anyway</li>

          <li>the <a href="https://huggingface.co/pszemraj/led-base-book-summary">base
          model</a> fits within constraints on V100 so that one always trains with
          inputs 16384</li>

          <li>IMO-while it takes AGES longer to train-the <code>long-t5</code> models
          are starting to perform much better than LED (plus, when I have access to
          A100, you can train on bfloat16), so IMO, better progress can be made vs.
          training these</li>

          </ul>

          <p>Hope that helps to start!</p>

          '
        raw: "Hey! Thanks for uploading the dataset, so I didn't have to run the script\
          \ off their repo :) \n\nSure, I'm currently traveling, so let me know if\
          \ you want more detail, and I can follow up once I'm back:\n\n- I use a\
          \ single V100 and 52 GB CPU RAM, deepspeed ZeRO 2; see config I just uploaded\
          \ [here](https://huggingface.co/pszemraj/led-large-book-summary/blob/main/ds_config_zero2.json)\n\
          - due to memory constraints, I keep the batch size at one and just crank\
          \ gradient accumulation, usually 64 or higher\n- from the length values,\
          \ I think `max_length` for outputs 1024 captures _most_ of the dataset,\
          \ so I filter out all summaries that are longer than that \n- if still facing\
          \ memory constraints, I think you can also keep max_input_length (for training)\
          \ at 8192 and filter out things more extended. Anecdotally, the model can\
          \ still run `predict` on 16384 and still produce good summaries (I guess\
          \ scaling what it learned when running inference). IIRC most of the summaries\
          \ are shorter anyway\n- the [base model](https://huggingface.co/pszemraj/led-base-book-summary)\
          \ fits within constraints on V100 so that one always trains with inputs\
          \ 16384\n- IMO-while it takes AGES longer to train-the `long-t5` models\
          \ are starting to perform much better than LED (plus, when I have access\
          \ to A100, you can train on bfloat16), so IMO, better progress can be made\
          \ vs. training these\n\nHope that helps to start!"
        updatedAt: '2022-06-24T10:24:21.522Z'
      numEdits: 0
      reactions: []
    id: 62b590d538d6f9896e7d6545
    type: comment
  author: pszemraj
  content: "Hey! Thanks for uploading the dataset, so I didn't have to run the script\
    \ off their repo :) \n\nSure, I'm currently traveling, so let me know if you want\
    \ more detail, and I can follow up once I'm back:\n\n- I use a single V100 and\
    \ 52 GB CPU RAM, deepspeed ZeRO 2; see config I just uploaded [here](https://huggingface.co/pszemraj/led-large-book-summary/blob/main/ds_config_zero2.json)\n\
    - due to memory constraints, I keep the batch size at one and just crank gradient\
    \ accumulation, usually 64 or higher\n- from the length values, I think `max_length`\
    \ for outputs 1024 captures _most_ of the dataset, so I filter out all summaries\
    \ that are longer than that \n- if still facing memory constraints, I think you\
    \ can also keep max_input_length (for training) at 8192 and filter out things\
    \ more extended. Anecdotally, the model can still run `predict` on 16384 and still\
    \ produce good summaries (I guess scaling what it learned when running inference).\
    \ IIRC most of the summaries are shorter anyway\n- the [base model](https://huggingface.co/pszemraj/led-base-book-summary)\
    \ fits within constraints on V100 so that one always trains with inputs 16384\n\
    - IMO-while it takes AGES longer to train-the `long-t5` models are starting to\
    \ perform much better than LED (plus, when I have access to A100, you can train\
    \ on bfloat16), so IMO, better progress can be made vs. training these\n\nHope\
    \ that helps to start!"
  created_at: 2022-06-24 09:24:21+00:00
  edited: false
  hidden: false
  id: 62b590d538d6f9896e7d6545
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2022-06-24T11:04:54.000Z'
    data:
      edited: true
      editors:
      - kmfoda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
          fullname: Karim Foda
          isHf: false
          isPro: false
          name: kmfoda
          type: user
        html: '<p>Interesting, thank you very much for this. I didn''t consider using
          deepspeed with just 1 GPU, I was focused on partitioning the model across
          GPUs and I think that fails with these large attention models. Do you use
          gradient checkpointing in this setup? </p>

          <p>Ideally I would like to avoid this as it really slows down training and
          I would like to do many sweeps to granularly fine tune a number of the hyper
          parameters to suit book summarisation. </p>

          '
        raw: "Interesting, thank you very much for this. I didn't consider using deepspeed\
          \ with just 1 GPU, I was focused on partitioning the model across GPUs and\
          \ I think that fails with these large attention models. Do you use gradient\
          \ checkpointing in this setup? \n\nIdeally I would like to avoid this as\
          \ it really slows down training and I would like to do many sweeps to granularly\
          \ fine tune a number of the hyper parameters to suit book summarisation. "
        updatedAt: '2022-06-24T11:08:53.884Z'
      numEdits: 1
      reactions: []
    id: 62b59a561e82c1fbbaed2f1e
    type: comment
  author: kmfoda
  content: "Interesting, thank you very much for this. I didn't consider using deepspeed\
    \ with just 1 GPU, I was focused on partitioning the model across GPUs and I think\
    \ that fails with these large attention models. Do you use gradient checkpointing\
    \ in this setup? \n\nIdeally I would like to avoid this as it really slows down\
    \ training and I would like to do many sweeps to granularly fine tune a number\
    \ of the hyper parameters to suit book summarisation. "
  created_at: 2022-06-24 10:04:54+00:00
  edited: true
  hidden: false
  id: 62b59a561e82c1fbbaed2f1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2022-06-24T11:07:16.000Z'
    data:
      edited: false
      editors:
      - kmfoda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
          fullname: Karim Foda
          isHf: false
          isPro: false
          name: kmfoda
          type: user
        html: '<p>Also thanks for the tips re reducing the output length. Ideally
          I would like to avoid this as I have a private dataset that has a lot of
          datapoints with 16384 tokens. </p>

          <p>I agree completely re long-t5. I''m a big fan and want to move to train
          long-t5-xl at some point I just need to understand how to predict memory
          requirements for these models with a max_length of 16384 and understand
          why deepspeed fails with these types of models.</p>

          <p>One option I''m experimenting with now is TPUs as I believe that''s what
          they used in the paper but progressing quite slowly on that front.</p>

          '
        raw: "Also thanks for the tips re reducing the output length. Ideally I would\
          \ like to avoid this as I have a private dataset that has a lot of datapoints\
          \ with 16384 tokens. \n\nI agree completely re long-t5. I'm a big fan and\
          \ want to move to train long-t5-xl at some point I just need to understand\
          \ how to predict memory requirements for these models with a max_length\
          \ of 16384 and understand why deepspeed fails with these types of models.\n\
          \nOne option I'm experimenting with now is TPUs as I believe that's what\
          \ they used in the paper but progressing quite slowly on that front."
        updatedAt: '2022-06-24T11:07:16.840Z'
      numEdits: 0
      reactions: []
    id: 62b59ae46b29cfa1de3516e8
    type: comment
  author: kmfoda
  content: "Also thanks for the tips re reducing the output length. Ideally I would\
    \ like to avoid this as I have a private dataset that has a lot of datapoints\
    \ with 16384 tokens. \n\nI agree completely re long-t5. I'm a big fan and want\
    \ to move to train long-t5-xl at some point I just need to understand how to predict\
    \ memory requirements for these models with a max_length of 16384 and understand\
    \ why deepspeed fails with these types of models.\n\nOne option I'm experimenting\
    \ with now is TPUs as I believe that's what they used in the paper but progressing\
    \ quite slowly on that front."
  created_at: 2022-06-24 10:07:16+00:00
  edited: false
  hidden: false
  id: 62b59ae46b29cfa1de3516e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-06-25T22:16:45.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Fair enough! I posted a WIP checkpoint and made it public at <a
          href="https://huggingface.co/pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP">pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP</a>,
          you may find it helpful as a starting point. Let''s continue the discussion
          there, but if you have other things on LED, feel free to reopen this. </p>

          <p>Also, if you want to communicate not on hf or collaborate on something
          pick a means of communication on my <a rel="nofollow" href="https://peterszemraj.ch/">site</a>
          and reach out :)</p>

          '
        raw: "Fair enough! I posted a WIP checkpoint and made it public at [pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP](https://huggingface.co/pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP),\
          \ you may find it helpful as a starting point. Let's continue the discussion\
          \ there, but if you have other things on LED, feel free to reopen this.\
          \ \n\nAlso, if you want to communicate not on hf or collaborate on something\
          \ pick a means of communication on my [site](https://peterszemraj.ch/) and\
          \ reach out :)"
        updatedAt: '2022-06-25T22:16:45.418Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kmfoda
    id: 62b7894d60902192788aa664
    type: comment
  author: pszemraj
  content: "Fair enough! I posted a WIP checkpoint and made it public at [pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP](https://huggingface.co/pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP),\
    \ you may find it helpful as a starting point. Let's continue the discussion there,\
    \ but if you have other things on LED, feel free to reopen this. \n\nAlso, if\
    \ you want to communicate not on hf or collaborate on something pick a means of\
    \ communication on my [site](https://peterszemraj.ch/) and reach out :)"
  created_at: 2022-06-25 21:16:45+00:00
  edited: false
  hidden: false
  id: 62b7894d60902192788aa664
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-06-25T22:16:45.000Z'
    data:
      status: closed
    id: 62b7894d60902192788aa665
    type: status-change
  author: pszemraj
  created_at: 2022-06-25 21:16:45+00:00
  id: 62b7894d60902192788aa665
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-06-26T00:51:39.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Oh oops, I forgot one question:</p>

          <blockquote>

          <p>Do you use gradient checkpointing in this setup? </p>

          </blockquote>

          <p>Yes, I do, typically 32-64 steps. I think unless you have access to a
          massive compute setup it''s more or less required - no free lunch, etc etc.
          I believe it''s even used in the PubMed-3K model as well: <a rel="nofollow"
          href="https://wandb.ai/stancld/LongT5/runs/1lwncl8a/overview">see the W&amp;B
          page</a></p>

          '
        raw: "Oh oops, I forgot one question:\n\n> Do you use gradient checkpointing\
          \ in this setup? \n\nYes, I do, typically 32-64 steps. I think unless you\
          \ have access to a massive compute setup it's more or less required - no\
          \ free lunch, etc etc. I believe it's even used in the PubMed-3K model as\
          \ well: [see the W&B page](https://wandb.ai/stancld/LongT5/runs/1lwncl8a/overview)"
        updatedAt: '2022-06-26T00:51:39.381Z'
      numEdits: 0
      reactions: []
    id: 62b7ad9ba911e73f62631c87
    type: comment
  author: pszemraj
  content: "Oh oops, I forgot one question:\n\n> Do you use gradient checkpointing\
    \ in this setup? \n\nYes, I do, typically 32-64 steps. I think unless you have\
    \ access to a massive compute setup it's more or less required - no free lunch,\
    \ etc etc. I believe it's even used in the PubMed-3K model as well: [see the W&B\
    \ page](https://wandb.ai/stancld/LongT5/runs/1lwncl8a/overview)"
  created_at: 2022-06-25 23:51:39+00:00
  edited: false
  hidden: false
  id: 62b7ad9ba911e73f62631c87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672531901326-6345bd89fe134dfd7a0dba40.png?w=200&h=200&f=face
      fullname: "Furkan G\xF6z\xFCkara"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MonsterMMORPG
      type: user
    createdAt: '2022-10-28T21:35:41.000Z'
    data:
      edited: false
      editors:
      - MonsterMMORPG
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672531901326-6345bd89fe134dfd7a0dba40.png?w=200&h=200&f=face
          fullname: "Furkan G\xF6z\xFCkara"
          isHf: false
          isPro: false
          name: MonsterMMORPG
          type: user
        html: "<p>So which longer pre-trained model is best to use for 16k inputs?</p>\n\
          <p>And what are the optional hyper parameters?</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;pszemraj&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/pszemraj\">@<span class=\"underline\">pszemraj</span></a></span>\n\
          \n\t</span></span><br><span data-props=\"{&quot;user&quot;:&quot;kmfoda&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kmfoda\"\
          >@<span class=\"underline\">kmfoda</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>currently I am testing below and yet it is to complete my 8k\
          \ tokens input. it is running on 12 GB vram RTX 3060 GPU</p>\n<p>import\
          \ torch<br>from transformers import pipeline</p>\n<p>hf_name = 'pszemraj/led-large-book-summary'</p>\n\
          <p>summarizer = pipeline(<br>    \"summarization\",<br>    hf_name,<br>\
          \    device=0 if torch.cuda.is_available() else -1,<br>)</p>\n<p>result\
          \ = summarizer(<br>           wall_of_text,<br>           min_length=800,<br>\
          \           no_repeat_ngram_size=3,<br>           encoder_no_repeat_ngram_size\
          \ =3,<br>           repetition_penalty=3.5,<br>           num_beams=4,<br>\
          \           early_stopping=True,<br>    )</p>\n<p>with open('pszemraj-led-large-book-summary.txt',\
          \ 'w') as f:<br>    f.write(result[0]['summary_text'])</p>\n"
        raw: "So which longer pre-trained model is best to use for 16k inputs?\n\n\
          And what are the optional hyper parameters?\n\n@pszemraj \n@kmfoda \n\n\n\
          currently I am testing below and yet it is to complete my 8k tokens input.\
          \ it is running on 12 GB vram RTX 3060 GPU\n\nimport torch\nfrom transformers\
          \ import pipeline\n\nhf_name = 'pszemraj/led-large-book-summary'\n\nsummarizer\
          \ = pipeline(\n    \"summarization\",\n    hf_name,\n    device=0 if torch.cuda.is_available()\
          \ else -1,\n)\n\n\nresult = summarizer(\n           wall_of_text,\n    \
          \       min_length=800, \n           no_repeat_ngram_size=3, \n        \
          \   encoder_no_repeat_ngram_size =3,\n           repetition_penalty=3.5,\n\
          \           num_beams=4,\n           early_stopping=True,\n    )\n \n\n\
          with open('pszemraj-led-large-book-summary.txt', 'w') as f:\n    f.write(result[0]['summary_text'])"
        updatedAt: '2022-10-28T21:35:41.589Z'
      numEdits: 0
      reactions: []
    id: 635c4b2d3cb827d58119cd5c
    type: comment
  author: MonsterMMORPG
  content: "So which longer pre-trained model is best to use for 16k inputs?\n\nAnd\
    \ what are the optional hyper parameters?\n\n@pszemraj \n@kmfoda \n\n\ncurrently\
    \ I am testing below and yet it is to complete my 8k tokens input. it is running\
    \ on 12 GB vram RTX 3060 GPU\n\nimport torch\nfrom transformers import pipeline\n\
    \nhf_name = 'pszemraj/led-large-book-summary'\n\nsummarizer = pipeline(\n    \"\
    summarization\",\n    hf_name,\n    device=0 if torch.cuda.is_available() else\
    \ -1,\n)\n\n\nresult = summarizer(\n           wall_of_text,\n           min_length=800,\
    \ \n           no_repeat_ngram_size=3, \n           encoder_no_repeat_ngram_size\
    \ =3,\n           repetition_penalty=3.5,\n           num_beams=4,\n         \
    \  early_stopping=True,\n    )\n \n\nwith open('pszemraj-led-large-book-summary.txt',\
    \ 'w') as f:\n    f.write(result[0]['summary_text'])"
  created_at: 2022-10-28 20:35:41+00:00
  edited: false
  hidden: false
  id: 635c4b2d3cb827d58119cd5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-10-29T00:59:40.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MonsterMMORPG&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MonsterMMORPG\"\
          >@<span class=\"underline\">MonsterMMORPG</span></a></span>\n\n\t</span></span>\
          \ see <a href=\"https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary/discussions/12#635c78f094619871d0d63c6b\"\
          >here</a> for a response and links on hyperparameters/usage. I would also\
          \ recommend <code>pszemraj/long-t5-tglobal-base-16384-book-summary</code>\
          \ over this one from a quality:compute perspective.</p>\n<p>On this model\
          \ card (see the Colab notebook link), there is a very detailed example notebook\
          \ allowing for parameter adjustment; I would experiment with that too</p>\n"
        raw: '@MonsterMMORPG see [here](https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary/discussions/12#635c78f094619871d0d63c6b)
          for a response and links on hyperparameters/usage. I would also recommend
          `pszemraj/long-t5-tglobal-base-16384-book-summary` over this one from a
          quality:compute perspective.


          On this model card (see the Colab notebook link), there is a very detailed
          example notebook allowing for parameter adjustment; I would experiment with
          that too'
        updatedAt: '2022-10-29T00:59:40.835Z'
      numEdits: 0
      reactions: []
    id: 635c7afc94619871d0d64de1
    type: comment
  author: pszemraj
  content: '@MonsterMMORPG see [here](https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary/discussions/12#635c78f094619871d0d63c6b)
    for a response and links on hyperparameters/usage. I would also recommend `pszemraj/long-t5-tglobal-base-16384-book-summary`
    over this one from a quality:compute perspective.


    On this model card (see the Colab notebook link), there is a very detailed example
    notebook allowing for parameter adjustment; I would experiment with that too'
  created_at: 2022-10-28 23:59:40+00:00
  edited: false
  hidden: false
  id: 635c7afc94619871d0d64de1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: pszemraj/led-large-book-summary
repo_type: model
status: closed
target_branch: null
title: Training Setup + Results
