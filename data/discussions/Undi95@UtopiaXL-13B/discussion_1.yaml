!!python/object:huggingface_hub.community.DiscussionWithDetails
author: win10
conflicting_files: null
created_at: 2023-11-06 04:15:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-06T04:15:42.000Z'
    data:
      edited: false
      editors:
      - win10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9803874492645264
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: '<p>Hello, I would like to ask you how to merge the 20B model?<br>your
          approach to MLewd-ReMM-L2-Chat-20B has produced errors.</p>

          '
        raw: "Hello, I would like to ask you how to merge the 20B model?\r\nyour approach\
          \ to MLewd-ReMM-L2-Chat-20B has produced errors."
        updatedAt: '2023-11-06T04:15:42.857Z'
      numEdits: 0
      reactions: []
    id: 6548686eb8ac1a89ff287854
    type: comment
  author: win10
  content: "Hello, I would like to ask you how to merge the 20B model?\r\nyour approach\
    \ to MLewd-ReMM-L2-Chat-20B has produced errors."
  created_at: 2023-11-06 04:15:42+00:00
  edited: false
  hidden: false
  id: 6548686eb8ac1a89ff287854
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-06T13:34:25.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9030900001525879
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>I use Mergekit, this tool let you use layer of multiple model to
          create only one.<br>The approach used for this model is the same has the
          other, chunk of 16 layers, what are the errors you got? </p>

          '
        raw: 'I use Mergekit, this tool let you use layer of multiple model to create
          only one.

          The approach used for this model is the same has the other, chunk of 16
          layers, what are the errors you got? '
        updatedAt: '2023-11-06T13:34:25.398Z'
      numEdits: 0
      reactions: []
    id: 6548eb61be48e13755505b6f
    type: comment
  author: Undi95
  content: 'I use Mergekit, this tool let you use layer of multiple model to create
    only one.

    The approach used for this model is the same has the other, chunk of 16 layers,
    what are the errors you got? '
  created_at: 2023-11-06 13:34:25+00:00
  edited: false
  hidden: false
  id: 6548eb61be48e13755505b6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-06T14:56:58.000Z'
    data:
      edited: false
      editors:
      - win10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8988621234893799
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: '<blockquote>

          <p>I use Mergekit, this tool let you use layer of multiple model to create
          only one.<br>The approach used for this model is the same has the other,
          chunk of 16 layers, what are the errors you got?</p>

          </blockquote>

          <p>Hello, the problem is solved, how to merge the custom_code model?</p>

          '
        raw: '> I use Mergekit, this tool let you use layer of multiple model to create
          only one.

          > The approach used for this model is the same has the other, chunk of 16
          layers, what are the errors you got?


          Hello, the problem is solved, how to merge the custom_code model?'
        updatedAt: '2023-11-06T14:56:58.853Z'
      numEdits: 0
      reactions: []
    id: 6548feba47c51c815969f1fc
    type: comment
  author: win10
  content: '> I use Mergekit, this tool let you use layer of multiple model to create
    only one.

    > The approach used for this model is the same has the other, chunk of 16 layers,
    what are the errors you got?


    Hello, the problem is solved, how to merge the custom_code model?'
  created_at: 2023-11-06 14:56:58+00:00
  edited: false
  hidden: false
  id: 6548feba47c51c815969f1fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-06T15:19:06.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9742250442504883
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>I don''t really understand your question, what is "custom_code",
          you want to merge the 20B with another model?<br>Sadly, 20B can only be
          merged with other 20B, but you can do it, with Mergekit as always.<br>You
          can try to apply 13B LoRA on 20B model since they have the layers of a 13B
          but I don''t recommand it.<br>Here is the tools : <a rel="nofollow" href="https://github.com/cg123/mergekit">https://github.com/cg123/mergekit</a></p>

          <p>Don''t hesitate to post again if you need help!</p>

          '
        raw: 'I don''t really understand your question, what is "custom_code", you
          want to merge the 20B with another model?

          Sadly, 20B can only be merged with other 20B, but you can do it, with Mergekit
          as always.

          You can try to apply 13B LoRA on 20B model since they have the layers of
          a 13B but I don''t recommand it.

          Here is the tools : https://github.com/cg123/mergekit


          Don''t hesitate to post again if you need help!

          '
        updatedAt: '2023-11-06T15:19:06.974Z'
      numEdits: 0
      reactions: []
    id: 654903eaddc9025bb4bad0da
    type: comment
  author: Undi95
  content: 'I don''t really understand your question, what is "custom_code", you want
    to merge the 20B with another model?

    Sadly, 20B can only be merged with other 20B, but you can do it, with Mergekit
    as always.

    You can try to apply 13B LoRA on 20B model since they have the layers of a 13B
    but I don''t recommand it.

    Here is the tools : https://github.com/cg123/mergekit


    Don''t hesitate to post again if you need help!

    '
  created_at: 2023-11-06 15:19:06+00:00
  edited: false
  hidden: false
  id: 654903eaddc9025bb4bad0da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-07T00:27:05.000Z'
    data:
      edited: false
      editors:
      - win10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4915945529937744
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: '<p>like this :ValueError: The repository for D:/oobabooga_windows/text-generation-webui/models/THUDM_chatglm3-6b-32k
          contains custom code which must be executed to correctlyload the model.
          You can inspect the repository content at <a rel="nofollow" href="https://hf.co/D:/oobabooga_windows/text-generation-webui/models/THUDM_chatglm3-6b-32k">https://hf.co/D:/oobabooga_windows/text-generation-webui/models/THUDM_chatglm3-6b-32k</a>.<br>Please
          pass the argument <code>trust_remote_code=True</code> to allow custom code
          to be run.</p>

          '
        raw: 'like this :ValueError: The repository for D:/oobabooga_windows/text-generation-webui/models/THUDM_chatglm3-6b-32k
          contains custom code which must be executed to correctlyload the model.
          You can inspect the repository content at https://hf.co/D:/oobabooga_windows/text-generation-webui/models/THUDM_chatglm3-6b-32k.

          Please pass the argument `trust_remote_code=True` to allow custom code to
          be run.'
        updatedAt: '2023-11-07T00:27:05.381Z'
      numEdits: 0
      reactions: []
    id: 65498459e546b0737eb8fbb5
    type: comment
  author: win10
  content: 'like this :ValueError: The repository for D:/oobabooga_windows/text-generation-webui/models/THUDM_chatglm3-6b-32k
    contains custom code which must be executed to correctlyload the model. You can
    inspect the repository content at https://hf.co/D:/oobabooga_windows/text-generation-webui/models/THUDM_chatglm3-6b-32k.

    Please pass the argument `trust_remote_code=True` to allow custom code to be run.'
  created_at: 2023-11-07 00:27:05+00:00
  edited: false
  hidden: false
  id: 65498459e546b0737eb8fbb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-07T00:37:10.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.877683162689209
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>It''s because this is a different architecture than llama2 model,
          you can''t merge 2 models from different architecture.<br>Also, you try
          to load this model is ooba but i''m not sure he support it, I don''t use
          ooba anymore and I don''t know this modele haha.<br>Try to merge model with
          the same size and the same architecture, and use <a rel="nofollow" href="https://github.com/cg123/mergekit">mergekit</a>
          to stack layer and do a bigger model :</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/lURT0LTizA2hcloBrX0Mp.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/lURT0LTizA2hcloBrX0Mp.png"></a></p>

          '
        raw: 'It''s because this is a different architecture than llama2 model, you
          can''t merge 2 models from different architecture.

          Also, you try to load this model is ooba but i''m not sure he support it,
          I don''t use ooba anymore and I don''t know this modele haha.

          Try to merge model with the same size and the same architecture, and use
          [mergekit](https://github.com/cg123/mergekit) to stack layer and do a bigger
          model :


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/lURT0LTizA2hcloBrX0Mp.png)

          '
        updatedAt: '2023-11-07T00:37:10.318Z'
      numEdits: 0
      reactions: []
    id: 654986b6bdd4dffd467536d4
    type: comment
  author: Undi95
  content: 'It''s because this is a different architecture than llama2 model, you
    can''t merge 2 models from different architecture.

    Also, you try to load this model is ooba but i''m not sure he support it, I don''t
    use ooba anymore and I don''t know this modele haha.

    Try to merge model with the same size and the same architecture, and use [mergekit](https://github.com/cg123/mergekit)
    to stack layer and do a bigger model :


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/lURT0LTizA2hcloBrX0Mp.png)

    '
  created_at: 2023-11-07 00:37:10+00:00
  edited: false
  hidden: false
  id: 654986b6bdd4dffd467536d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-07T00:39:34.000Z'
    data:
      edited: true
      editors:
      - win10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9194835424423218
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: '<p>How to merge  models with different vocab_sizes?</p>

          '
        raw: How to merge  models with different vocab_sizes?
        updatedAt: '2023-11-07T00:40:44.787Z'
      numEdits: 1
      reactions: []
    id: 65498746e546b0737eb9626f
    type: comment
  author: win10
  content: How to merge  models with different vocab_sizes?
  created_at: 2023-11-07 00:39:34+00:00
  edited: true
  hidden: false
  id: 65498746e546b0737eb9626f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-07T01:01:34.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9183717966079712
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<blockquote>

          <p>How to merge  models with different vocab_sizes?</p>

          </blockquote>

          <p>Edit one config file.<br>If model1 have 32000 vocab and model2 have,
          for exemple, 32021, put 32000 on the config file of model2 for the vocab
          size.<br>It''s dirty, but it''s work.<br>If you just stack layer (bakllama),
          you don''t even need to do that, it work (I tried)</p>

          '
        raw: '> How to merge  models with different vocab_sizes?


          Edit one config file.

          If model1 have 32000 vocab and model2 have, for exemple, 32021, put 32000
          on the config file of model2 for the vocab size.

          It''s dirty, but it''s work.

          If you just stack layer (bakllama), you don''t even need to do that, it
          work (I tried)'
        updatedAt: '2023-11-07T01:01:34.565Z'
      numEdits: 0
      reactions: []
    id: 65498c6e4aecf5964cb19e6c
    type: comment
  author: Undi95
  content: '> How to merge  models with different vocab_sizes?


    Edit one config file.

    If model1 have 32000 vocab and model2 have, for exemple, 32021, put 32000 on the
    config file of model2 for the vocab size.

    It''s dirty, but it''s work.

    If you just stack layer (bakllama), you don''t even need to do that, it work (I
    tried)'
  created_at: 2023-11-07 01:01:34+00:00
  edited: false
  hidden: false
  id: 65498c6e4aecf5964cb19e6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-11T02:51:44.000Z'
    data:
      edited: false
      editors:
      - win10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.30341991782188416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: '<blockquote>

          <blockquote>

          <p>How to merge  models with different vocab_sizes?</p>

          </blockquote>

          <p>Edit one config file.<br>If model1 have 32000 vocab and model2 have,
          for exemple, 32021, put 32000 on the config file of model2 for the vocab
          size.<br>It''s dirty, but it''s work.<br>If you just stack layer (bakllama),
          you don''t even need to do that, it work (I tried)</p>

          </blockquote>

          <p>Sorry if it bothers you, I am getting the following error while trying
          to merge 20b,<br>Traceback (most recent call last):</p>

          <p>  File "D:\mergekit-main\mergekit\scripts\bakllama.py", line 83, in <br>    _main()</p>

          <p>  File "D:\mergekit-main\mergekit\scripts\bakllama.py", line 79, in _main<br>    typer.run(main)</p>

          <p>  File "D:\mergekit-main\mergekit\scripts\bakllama.py", line 72, in main<br>    merge_config
          = MergeConfiguration(<br>                   ^^^^^^^^^^^^^^^^^^^</p>

          <p>  File "C:\Users\jmes1\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py",
          line 164, in <strong>init</strong><br>    <strong>pydantic_self</strong>.<strong>pydantic_validator</strong>.validate_python(data,
          self_instance=<strong>pydantic_self</strong>)</p>

          <p>pydantic_core._pydantic_core.ValidationError: 8 validation errors for
          MergeConfiguration<br>slices.0<br>  Input should be a valid dictionary or
          instance of OutputSliceDefinition [type=model_type, input_value=InputSliceDefinition(mode...e=(0,
          8), parameters={}), input_type=InputSliceDefinition]<br>    For further
          information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a><br>slices.1<br>  Input
          should be a valid dictionary or instance of OutputSliceDefinition [type=model_type,
          input_value=InputSliceDefinition(mode...=(4, 12), parameters={}), input_type=InputSliceDefinition]<br>    For
          further information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a><br>slices.2<br>  Input
          should be a valid dictionary or instance of OutputSliceDefinition [type=model_type,
          input_value=InputSliceDefinition(mode...=(9, 16), parameters={}), input_type=InputSliceDefinition]<br>    For
          further information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a><br>slices.3<br>  Input
          should be a valid dictionary or instance of OutputSliceDefinition [type=model_type,
          input_value=InputSliceDefinition(mode...(13, 22), parameters={}), input_type=InputSliceDefinition]<br>    For
          further information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a><br>slices.4<br>  Input
          should be a valid dictionary or instance of OutputSliceDefinition [type=model_type,
          input_value=InputSliceDefinition(mode...(17, 24), parameters={}), input_type=InputSliceDefinition]<br>    For
          further information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a><br>slices.5<br>  Input
          should be a valid dictionary or instance of OutputSliceDefinition [type=model_type,
          input_value=InputSliceDefinition(mode...(23, 32), parameters={}), input_type=InputSliceDefinition]<br>    For
          further information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a><br>slices.6<br>  Input
          should be a valid dictionary or instance of OutputSliceDefinition [type=model_type,
          input_value=InputSliceDefinition(mode...(25, 32), parameters={}), input_type=InputSliceDefinition]<br>    For
          further information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a><br>slices.7<br>  Input
          should be a valid dictionary or instance of OutputSliceDefinition [type=model_type,
          input_value=InputSliceDefinition(mode...(33, 40), parameters={}), input_type=InputSliceDefinition]<br>    For
          further information visit <a rel="nofollow" href="https://errors.pydantic.dev/2.4/v/model_type">https://errors.pydantic.dev/2.4/v/model_type</a></p>

          '
        raw: "> > How to merge  models with different vocab_sizes?\n> \n> Edit one\
          \ config file.\n> If model1 have 32000 vocab and model2 have, for exemple,\
          \ 32021, put 32000 on the config file of model2 for the vocab size.\n> It's\
          \ dirty, but it's work.\n> If you just stack layer (bakllama), you don't\
          \ even need to do that, it work (I tried)\n\nSorry if it bothers you, I\
          \ am getting the following error while trying to merge 20b,\nTraceback (most\
          \ recent call last):\n\n  File \"D:\\mergekit-main\\mergekit\\scripts\\\
          bakllama.py\", line 83, in <module>\n    _main()\n\n  File \"D:\\mergekit-main\\\
          mergekit\\scripts\\bakllama.py\", line 79, in _main\n    typer.run(main)\n\
          \n  File \"D:\\mergekit-main\\mergekit\\scripts\\bakllama.py\", line 72,\
          \ in main\n    merge_config = MergeConfiguration(\n                   ^^^^^^^^^^^^^^^^^^^\n\
          \n  File \"C:\\Users\\jmes1\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\pydantic\\main.py\", line 164, in __init__\n    __pydantic_self__.__pydantic_validator__.validate_python(data,\
          \ self_instance=__pydantic_self__)\n\npydantic_core._pydantic_core.ValidationError:\
          \ 8 validation errors for MergeConfiguration\nslices.0\n  Input should be\
          \ a valid dictionary or instance of OutputSliceDefinition [type=model_type,\
          \ input_value=InputSliceDefinition(mode...e=(0, 8), parameters={}), input_type=InputSliceDefinition]\n\
          \    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
          slices.1\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
          \ [type=model_type, input_value=InputSliceDefinition(mode...=(4, 12), parameters={}),\
          \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
          slices.2\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
          \ [type=model_type, input_value=InputSliceDefinition(mode...=(9, 16), parameters={}),\
          \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
          slices.3\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
          \ [type=model_type, input_value=InputSliceDefinition(mode...(13, 22), parameters={}),\
          \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
          slices.4\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
          \ [type=model_type, input_value=InputSliceDefinition(mode...(17, 24), parameters={}),\
          \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
          slices.5\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
          \ [type=model_type, input_value=InputSliceDefinition(mode...(23, 32), parameters={}),\
          \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
          slices.6\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
          \ [type=model_type, input_value=InputSliceDefinition(mode...(25, 32), parameters={}),\
          \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
          slices.7\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
          \ [type=model_type, input_value=InputSliceDefinition(mode...(33, 40), parameters={}),\
          \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n"
        updatedAt: '2023-11-11T02:51:44.318Z'
      numEdits: 0
      reactions: []
    id: 654eec408d8c3bf4ddb462e5
    type: comment
  author: win10
  content: "> > How to merge  models with different vocab_sizes?\n> \n> Edit one config\
    \ file.\n> If model1 have 32000 vocab and model2 have, for exemple, 32021, put\
    \ 32000 on the config file of model2 for the vocab size.\n> It's dirty, but it's\
    \ work.\n> If you just stack layer (bakllama), you don't even need to do that,\
    \ it work (I tried)\n\nSorry if it bothers you, I am getting the following error\
    \ while trying to merge 20b,\nTraceback (most recent call last):\n\n  File \"\
    D:\\mergekit-main\\mergekit\\scripts\\bakllama.py\", line 83, in <module>\n  \
    \  _main()\n\n  File \"D:\\mergekit-main\\mergekit\\scripts\\bakllama.py\", line\
    \ 79, in _main\n    typer.run(main)\n\n  File \"D:\\mergekit-main\\mergekit\\\
    scripts\\bakllama.py\", line 72, in main\n    merge_config = MergeConfiguration(\n\
    \                   ^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\jmes1\\AppData\\\
    Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py\", line\
    \ 164, in __init__\n    __pydantic_self__.__pydantic_validator__.validate_python(data,\
    \ self_instance=__pydantic_self__)\n\npydantic_core._pydantic_core.ValidationError:\
    \ 8 validation errors for MergeConfiguration\nslices.0\n  Input should be a valid\
    \ dictionary or instance of OutputSliceDefinition [type=model_type, input_value=InputSliceDefinition(mode...e=(0,\
    \ 8), parameters={}), input_type=InputSliceDefinition]\n    For further information\
    \ visit https://errors.pydantic.dev/2.4/v/model_type\nslices.1\n  Input should\
    \ be a valid dictionary or instance of OutputSliceDefinition [type=model_type,\
    \ input_value=InputSliceDefinition(mode...=(4, 12), parameters={}), input_type=InputSliceDefinition]\n\
    \    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
    slices.2\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
    \ [type=model_type, input_value=InputSliceDefinition(mode...=(9, 16), parameters={}),\
    \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
    slices.3\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
    \ [type=model_type, input_value=InputSliceDefinition(mode...(13, 22), parameters={}),\
    \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
    slices.4\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
    \ [type=model_type, input_value=InputSliceDefinition(mode...(17, 24), parameters={}),\
    \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
    slices.5\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
    \ [type=model_type, input_value=InputSliceDefinition(mode...(23, 32), parameters={}),\
    \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
    slices.6\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
    \ [type=model_type, input_value=InputSliceDefinition(mode...(25, 32), parameters={}),\
    \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n\
    slices.7\n  Input should be a valid dictionary or instance of OutputSliceDefinition\
    \ [type=model_type, input_value=InputSliceDefinition(mode...(33, 40), parameters={}),\
    \ input_type=InputSliceDefinition]\n    For further information visit https://errors.pydantic.dev/2.4/v/model_type\n"
  created_at: 2023-11-11 02:51:44+00:00
  edited: false
  hidden: false
  id: 654eec408d8c3bf4ddb462e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-11T03:05:28.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.656889796257019
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;win10&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/win10\">@<span class=\"\
          underline\">win10</span></a></span>\n\n\t</span></span> Copy/paste your\
          \ merging .yaml I will take a look</p>\n"
        raw: '@win10 Copy/paste your merging .yaml I will take a look'
        updatedAt: '2023-11-11T03:05:28.567Z'
      numEdits: 0
      reactions: []
    id: 654eef78c0e106160e37cf13
    type: comment
  author: Undi95
  content: '@win10 Copy/paste your merging .yaml I will take a look'
  created_at: 2023-11-11 03:05:28+00:00
  edited: false
  hidden: false
  id: 654eef78c0e106160e37cf13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-11T03:16:06.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-11-11T03:24:26.798Z'
      numEdits: 1
      reactions: []
    id: 654ef1f60c8ecb1dae7657fe
    type: comment
  author: win10
  content: This comment has been hidden
  created_at: 2023-11-11 03:16:06+00:00
  edited: true
  hidden: true
  id: 654ef1f60c8ecb1dae7657fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-11T03:23:33.000Z'
    data:
      edited: true
      editors:
      - win10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6417486667633057
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;win10&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/win10\"\
          >@<span class=\"underline\">win10</span></a></span>\n\n\t</span></span>\
          \ Copy/paste your merging .yaml I will take a look</p>\n</blockquote>\n\
          <p>layer_slices:</p>\n<ul>\n<li>model: OpenBuddy/openbuddy-zephyr-7b-v14.1<br>start:\
          \ 0<br>end: 8</li>\n<li>model: Undi95/Toppy-M-7B<br>start: 4<br>end: 12</li>\n\
          <li>model: openchat/openchat_3.5<br>start: 9<br>end: 16</li>\n<li>model:\
          \ Undi95/Toppy-M-7B<br>start: 13<br>end: 22</li>\n<li>model: openchat/openchat_3.5<br>start:\
          \ 17<br>end: 24</li>\n<li>model: Undi95/Toppy-M-7B<br>start: 23<br>end:\
          \ 32</li>\n<li>model: OpenBuddy/openbuddy-zephyr-7b-v14.1<br>start: 25<br>end:\
          \ 32</li>\n<li>model: Undi95/Toppy-M-7B<br>start: 33<br>end: 40</li>\n</ul>\n"
        raw: "> @win10 Copy/paste your merging .yaml I will take a look\n\nlayer_slices:\n\
          \  - model: OpenBuddy/openbuddy-zephyr-7b-v14.1\n    start: 0\n    end:\
          \ 8\n  - model: Undi95/Toppy-M-7B\n    start: 4\n    end: 12\n  - model:\
          \ openchat/openchat_3.5\n    start: 9\n    end: 16\n  - model: Undi95/Toppy-M-7B\n\
          \    start: 13\n    end: 22\n  - model: openchat/openchat_3.5\n    start:\
          \ 17\n    end: 24\n  - model: Undi95/Toppy-M-7B\n    start: 23\n    end:\
          \ 32\n  - model: OpenBuddy/openbuddy-zephyr-7b-v14.1\n    start: 25\n  \
          \  end: 32\n  - model: Undi95/Toppy-M-7B\n    start: 33\n    end: 40"
        updatedAt: '2023-11-11T03:23:46.782Z'
      numEdits: 1
      reactions: []
      relatedEventId: 654ef3b5b79a662d52263b3d
    id: 654ef3b5b79a662d52263b37
    type: comment
  author: win10
  content: "> @win10 Copy/paste your merging .yaml I will take a look\n\nlayer_slices:\n\
    \  - model: OpenBuddy/openbuddy-zephyr-7b-v14.1\n    start: 0\n    end: 8\n  -\
    \ model: Undi95/Toppy-M-7B\n    start: 4\n    end: 12\n  - model: openchat/openchat_3.5\n\
    \    start: 9\n    end: 16\n  - model: Undi95/Toppy-M-7B\n    start: 13\n    end:\
    \ 22\n  - model: openchat/openchat_3.5\n    start: 17\n    end: 24\n  - model:\
    \ Undi95/Toppy-M-7B\n    start: 23\n    end: 32\n  - model: OpenBuddy/openbuddy-zephyr-7b-v14.1\n\
    \    start: 25\n    end: 32\n  - model: Undi95/Toppy-M-7B\n    start: 33\n   \
    \ end: 40"
  created_at: 2023-11-11 03:23:33+00:00
  edited: true
  hidden: false
  id: 654ef3b5b79a662d52263b37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-11T03:23:33.000Z'
    data:
      status: closed
    id: 654ef3b5b79a662d52263b3d
    type: status-change
  author: win10
  created_at: 2023-11-11 03:23:33+00:00
  id: 654ef3b5b79a662d52263b3d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-11-11T03:23:36.000Z'
    data:
      status: open
    id: 654ef3b86c46a91f1207adf3
    type: status-change
  author: win10
  created_at: 2023-11-11 03:23:36+00:00
  id: 654ef3b86c46a91f1207adf3
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-11T03:33:59.000Z'
    data:
      edited: true
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9725284576416016
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;win10&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/win10\">@<span class=\"\
          underline\">win10</span></a></span>\n\n\t</span></span> 7B model only have\
          \ 32 layers, you go up to 40, that is the problem.<br>Try to go only to\
          \ 32 max.<br>Good luck! post here is you need more help.<br>I can't read\
          \ at 4AM lmao, forget about the 3 models, i'm dumb.</p>\n"
        raw: '@win10 7B model only have 32 layers, you go up to 40, that is the problem.

          Try to go only to 32 max.

          Good luck! post here is you need more help.

          I can''t read at 4AM lmao, forget about the 3 models, i''m dumb.'
        updatedAt: '2023-11-11T03:34:51.374Z'
      numEdits: 1
      reactions: []
    id: 654ef627a43b13ee341bd95b
    type: comment
  author: Undi95
  content: '@win10 7B model only have 32 layers, you go up to 40, that is the problem.

    Try to go only to 32 max.

    Good luck! post here is you need more help.

    I can''t read at 4AM lmao, forget about the 3 models, i''m dumb.'
  created_at: 2023-11-11 03:33:59+00:00
  edited: true
  hidden: false
  id: 654ef627a43b13ee341bd95b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Undi95/UtopiaXL-13B
repo_type: model
status: open
target_branch: null
title: hello, I would like to ask you how to merge the 20B model?
