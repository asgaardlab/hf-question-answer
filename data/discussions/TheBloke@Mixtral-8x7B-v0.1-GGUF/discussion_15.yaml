!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jeffwadsworth
conflicting_files: null
created_at: 2023-12-18 04:12:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
      fullname: Jeff Wadsworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffwadsworth
      type: user
    createdAt: '2023-12-18T04:12:50.000Z'
    data:
      edited: false
      editors:
      - jeffwadsworth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32657065987586975
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
          fullname: Jeff Wadsworth
          isHf: false
          isPro: false
          name: jeffwadsworth
          type: user
        html: '<p>I am using llama.cpp and the following initializer for this model.  CPU
          is a Ryzen 9 12 core.<br>main -m mixtral-8x7b-instruct-v0.1.Q8_0.gguf --color
          -c 30000 --temp 0.0 --repeat_penalty 1.1 -n -12 --instruct --override-kv
          llama.expert_used_count=int:3 --instruct --reverse-prompt "### Human:"<br>Only
          50% usage of each of the 12 cores.  Anyone else notice the same?</p>

          '
        raw: "I am using llama.cpp and the following initializer for this model. \
          \ CPU is a Ryzen 9 12 core.\r\nmain -m mixtral-8x7b-instruct-v0.1.Q8_0.gguf\
          \ --color -c 30000 --temp 0.0 --repeat_penalty 1.1 -n -12 --instruct --override-kv\
          \ llama.expert_used_count=int:3 --instruct --reverse-prompt \"### Human:\"\
          \r\nOnly 50% usage of each of the 12 cores.  Anyone else notice the same?"
        updatedAt: '2023-12-18T04:12:50.840Z'
      numEdits: 0
      reactions: []
    id: 657fc6c249ec77d48ec86511
    type: comment
  author: jeffwadsworth
  content: "I am using llama.cpp and the following initializer for this model.  CPU\
    \ is a Ryzen 9 12 core.\r\nmain -m mixtral-8x7b-instruct-v0.1.Q8_0.gguf --color\
    \ -c 30000 --temp 0.0 --repeat_penalty 1.1 -n -12 --instruct --override-kv llama.expert_used_count=int:3\
    \ --instruct --reverse-prompt \"### Human:\"\r\nOnly 50% usage of each of the\
    \ 12 cores.  Anyone else notice the same?"
  created_at: 2023-12-18 04:12:50+00:00
  edited: false
  hidden: false
  id: 657fc6c249ec77d48ec86511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00154d9a3f901567f25b432f3f04e6e2.svg
      fullname: Hans Schmidt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supportend
      type: user
    createdAt: '2023-12-20T23:58:25.000Z'
    data:
      edited: false
      editors:
      - supportend
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8484956622123718
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00154d9a3f901567f25b432f3f04e6e2.svg
          fullname: Hans Schmidt
          isHf: false
          isPro: false
          name: supportend
          type: user
        html: '<p>I set the number of threads with this:</p>

          <p>-t N, --threads N     number of threads to use during generation (default:
          8)</p>

          '
        raw: 'I set the number of threads with this:


          -t N, --threads N     number of threads to use during generation (default:
          8)'
        updatedAt: '2023-12-20T23:58:25.839Z'
      numEdits: 0
      reactions: []
    id: 65837fa1e09e5df03076e515
    type: comment
  author: supportend
  content: 'I set the number of threads with this:


    -t N, --threads N     number of threads to use during generation (default: 8)'
  created_at: 2023-12-20 23:58:25+00:00
  edited: false
  hidden: false
  id: 65837fa1e09e5df03076e515
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-21T03:43:50.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6884034276008606
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<blockquote>

          <p>I am using llama.cpp and the following initializer for this model.  CPU
          is a Ryzen 9 12 core.<br>main -m mixtral-8x7b-instruct-v0.1.Q8_0.gguf --color
          -c 30000 --temp 0.0 --repeat_penalty 1.1 -n -12 --instruct --override-kv
          llama.expert_used_count=int:3 --instruct --reverse-prompt "### Human:"<br>Only
          50% usage of each of the 12 cores.  Anyone else notice the same?</p>

          </blockquote>

          <p>That just mean llamacpp have not run at optimal speed on your computer,
          and cpu is waiting dram to fetch data. Which is called memory bound, and
          this is quite common for memory intensive program like running a llm.</p>

          <p>There are a few methods to mitigate that memory bound effect, such as
          pre-compute sparsity of the model weight and only streaming a selective
          of weights. I am not sure how well it is implemented in llamacpp. But it
          is mostly issue on llamacpp side.</p>

          '
        raw: '> I am using llama.cpp and the following initializer for this model.  CPU
          is a Ryzen 9 12 core.

          > main -m mixtral-8x7b-instruct-v0.1.Q8_0.gguf --color -c 30000 --temp 0.0
          --repeat_penalty 1.1 -n -12 --instruct --override-kv llama.expert_used_count=int:3
          --instruct --reverse-prompt "### Human:"

          > Only 50% usage of each of the 12 cores.  Anyone else notice the same?


          That just mean llamacpp have not run at optimal speed on your computer,
          and cpu is waiting dram to fetch data. Which is called memory bound, and
          this is quite common for memory intensive program like running a llm.


          There are a few methods to mitigate that memory bound effect, such as pre-compute
          sparsity of the model weight and only streaming a selective of weights.
          I am not sure how well it is implemented in llamacpp. But it is mostly issue
          on llamacpp side.'
        updatedAt: '2023-12-21T03:45:45.226Z'
      numEdits: 1
      reactions: []
    id: 6583b4762ae928087397abbb
    type: comment
  author: Yhyu13
  content: '> I am using llama.cpp and the following initializer for this model.  CPU
    is a Ryzen 9 12 core.

    > main -m mixtral-8x7b-instruct-v0.1.Q8_0.gguf --color -c 30000 --temp 0.0 --repeat_penalty
    1.1 -n -12 --instruct --override-kv llama.expert_used_count=int:3 --instruct --reverse-prompt
    "### Human:"

    > Only 50% usage of each of the 12 cores.  Anyone else notice the same?


    That just mean llamacpp have not run at optimal speed on your computer, and cpu
    is waiting dram to fetch data. Which is called memory bound, and this is quite
    common for memory intensive program like running a llm.


    There are a few methods to mitigate that memory bound effect, such as pre-compute
    sparsity of the model weight and only streaming a selective of weights. I am not
    sure how well it is implemented in llamacpp. But it is mostly issue on llamacpp
    side.'
  created_at: 2023-12-21 03:43:50+00:00
  edited: true
  hidden: false
  id: 6583b4762ae928087397abbb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: My cpu is only using 50% of its cores.
