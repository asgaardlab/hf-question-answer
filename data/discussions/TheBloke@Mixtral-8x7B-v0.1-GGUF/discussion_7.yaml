!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CR2022
conflicting_files: null
created_at: 2023-12-12 03:37:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-12-12T03:37:02.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2735387980937958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>llama_model_loader: - type  f32:   65 tensors<br>llama_model_loader:
          - type  f16:   32 tensors<br>llama_model_loader: - type q8_0:   64 tensors<br>llama_model_loader:
          - type q5_K:  833 tensors<br>llama_model_loader: - type q6_K:    1 tensors<br>llm_load_vocab:
          special tokens definition check successful ( 259/32000 ).<br>llm_load_print_meta:
          format           = GGUF V3 (latest)<br>llm_load_print_meta: arch             =
          llama<br>llm_load_print_meta: vocab type       = SPM<br>llm_load_print_meta:
          n_vocab          = 32000<br>llm_load_print_meta: n_merges         = 0<br>llm_load_print_meta:
          n_ctx_train      = 32768<br>llm_load_print_meta: n_embd           = 4096<br>llm_load_print_meta:
          n_head           = 32<br>llm_load_print_meta: n_head_kv        = 8<br>llm_load_print_meta:
          n_layer          = 32<br>llm_load_print_meta: n_rot            = 128<br>llm_load_print_meta:
          n_gqa            = 4<br>llm_load_print_meta: f_norm_eps       = 0.0e+00<br>llm_load_print_meta:
          f_norm_rms_eps   = 1.0e-05<br>llm_load_print_meta: f_clamp_kqv      = 0.0e+00<br>llm_load_print_meta:
          f_max_alibi_bias = 0.0e+00<br>llm_load_print_meta: n_ff             = 14336<br>llm_load_print_meta:
          rope scaling     = linear<br>llm_load_print_meta: freq_base_train  = 1000000.0<br>llm_load_print_meta:
          freq_scale_train = 1<br>llm_load_print_meta: n_yarn_orig_ctx  = 32768<br>llm_load_print_meta:
          rope_finetuned   = unknown<br>llm_load_print_meta: model type       = 7B<br>llm_load_print_meta:
          model ftype      = mostly Q5_K - Medium<br>llm_load_print_meta: model params     =
          46.70 B<br>llm_load_print_meta: model size       = 30.02 GiB (5.52 BPW)<br>llm_load_print_meta:
          general.name   = mistralai_mixtral-8x7b-v0.1<br>llm_load_print_meta: BOS
          token = 1 ''<s>''<br>llm_load_print_meta: EOS token = 2 ''</s>''<br>llm_load_print_meta:
          UNK token = 0 ''''<br>llm_load_print_meta: PAD token = 0 ''''<br>llm_load_print_meta:
          LF token  = 13 ''&lt;0x0A&gt;''<br>llm_load_tensors: ggml ctx size =    0.36
          MiB<br>llm_load_tensors: using CUDA for GPU acceleration<br>error loading
          model: create_tensor: tensor ''blk.0.ffn_gate.weight'' not found<br>llama_load_model_from_file:
          failed to load model<br>AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI =
          0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA
          = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |<br>2023-12-12
          04:34:06 ERROR:Failed to load the model.<br>Traceback (most recent call
          last):<br>  File "K:\text-generation-webui\modules\ui_model_menu.py", line
          209, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "K:\text-generation-webui\modules\models.py", line 88, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "K:\text-generation-webui\modules\models.py", line 253, in llamacpp_loader<br>    model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "K:\text-generation-webui\modules\llamacpp_model.py", line 91, in from_pretrained<br>    result.model
          = Llama(**params)<br>                   ^^^^^^^^^^^^^^^<br>  File "K:\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama.py",
          line 923, in <strong>init</strong><br>    self._n_vocab = self.n_vocab()<br>                    ^^^^^^^^^^^^^^<br>  File
          "K:\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama.py",
          line 2184, in n_vocab<br>    return self._model.n_vocab()<br>           ^^^^^^^^^^^^^^^^^^^^^<br>  File
          "K:\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama.py",
          line 250, in n_vocab<br>    assert self.model is not None<br>           ^^^^^^^^^^^^^^^^^^^^^^<br>AssertionError</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x000001F48DC09D00&gt;<br>Traceback
          (most recent call last):<br>  File "K:\text-generation-webui\modules\llamacpp_model.py",
          line 49, in <strong>del</strong><br>    del self.model<br>        ^^^^^^^^^^<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''</p>

          '
        raw: "llama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader:\
          \ - type  f16:   32 tensors\r\nllama_model_loader: - type q8_0:   64 tensors\r\
          \nllama_model_loader: - type q5_K:  833 tensors\r\nllama_model_loader: -\
          \ type q6_K:    1 tensors\r\nllm_load_vocab: special tokens definition check\
          \ successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF\
          \ V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta:\
          \ vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\
          \nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train\
          \      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta:\
          \ n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\n\
          llm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot\
          \            = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   =\
          \ 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta:\
          \ f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             =\
          \ 14336\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta:\
          \ freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train\
          \ = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta:\
          \ rope_finetuned   = unknown\r\nllm_load_print_meta: model type       =\
          \ 7B\r\nllm_load_print_meta: model ftype      = mostly Q5_K - Medium\r\n\
          llm_load_print_meta: model params     = 46.70 B\r\nllm_load_print_meta:\
          \ model size       = 30.02 GiB (5.52 BPW)\r\nllm_load_print_meta: general.name\
          \   = mistralai_mixtral-8x7b-v0.1\r\nllm_load_print_meta: BOS token = 1\
          \ '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta:\
          \ UNK token = 0 '<unk>'\r\nllm_load_print_meta: PAD token = 0 '<unk>'\r\n\
          llm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx\
          \ size =    0.36 MiB\r\nllm_load_tensors: using CUDA for GPU acceleration\r\
          \nerror loading model: create_tensor: tensor 'blk.0.ffn_gate.weight' not\
          \ found\r\nllama_load_model_from_file: failed to load model\r\nAVX = 1 |\
          \ AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 |\
          \ NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS\
          \ = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\r\n2023-12-12 04:34:06 ERROR:Failed\
          \ to load the model.\r\nTraceback (most recent call last):\r\n  File \"\
          K:\\text-generation-webui\\modules\\ui_model_menu.py\", line 209, in load_model_wrapper\r\
          \n    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"K:\\text-generation-webui\\modules\\models.py\", line 88, in\
          \ load_model\r\n    output = load_func_map[loader](model_name)\r\n     \
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"K:\\text-generation-webui\\\
          modules\\models.py\", line 253, in llamacpp_loader\r\n    model, tokenizer\
          \ = LlamaCppModel.from_pretrained(model_file)\r\n                      \
          \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"K:\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 91, in from_pretrained\r\n    result.model\
          \ = Llama(**params)\r\n                   ^^^^^^^^^^^^^^^\r\n  File \"K:\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 923, in __init__\r\n    self._n_vocab = self.n_vocab()\r\
          \n                    ^^^^^^^^^^^^^^\r\n  File \"K:\\text-generation-webui\\\
          installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line\
          \ 2184, in n_vocab\r\n    return self._model.n_vocab()\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"K:\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
          llama_cpp_cuda\\llama.py\", line 250, in n_vocab\r\n    assert self.model\
          \ is not None\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n\
          \r\nException ignored in: <function LlamaCppModel.__del__ at 0x000001F48DC09D00>\r\
          \nTraceback (most recent call last):\r\n  File \"K:\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 49, in __del__\r\n    del self.model\r\
          \n        ^^^^^^^^^^\r\nAttributeError: 'LlamaCppModel' object has no attribute\
          \ 'model'"
        updatedAt: '2023-12-12T03:37:02.993Z'
      numEdits: 0
      reactions: []
    id: 6577d55e90df9d85fb4c031e
    type: comment
  author: CR2022
  content: "llama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: -\
    \ type  f16:   32 tensors\r\nllama_model_loader: - type q8_0:   64 tensors\r\n\
    llama_model_loader: - type q5_K:  833 tensors\r\nllama_model_loader: - type q6_K:\
    \    1 tensors\r\nllm_load_vocab: special tokens definition check successful (\
    \ 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\n\
    llm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type\
    \       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta:\
    \ n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta:\
    \ n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta:\
    \ n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta:\
    \ n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta:\
    \ f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\
    \nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias\
    \ = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta:\
    \ rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\
    \nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx\
    \  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta:\
    \ model type       = 7B\r\nllm_load_print_meta: model ftype      = mostly Q5_K\
    \ - Medium\r\nllm_load_print_meta: model params     = 46.70 B\r\nllm_load_print_meta:\
    \ model size       = 30.02 GiB (5.52 BPW)\r\nllm_load_print_meta: general.name\
    \   = mistralai_mixtral-8x7b-v0.1\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\
    \nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token\
    \ = 0 '<unk>'\r\nllm_load_print_meta: PAD token = 0 '<unk>'\r\nllm_load_print_meta:\
    \ LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.36 MiB\r\n\
    llm_load_tensors: using CUDA for GPU acceleration\r\nerror loading model: create_tensor:\
    \ tensor 'blk.0.ffn_gate.weight' not found\r\nllama_load_model_from_file: failed\
    \ to load model\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\r\n2023-12-12 04:34:06 ERROR:Failed\
    \ to load the model.\r\nTraceback (most recent call last):\r\n  File \"K:\\text-generation-webui\\\
    modules\\ui_model_menu.py\", line 209, in load_model_wrapper\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\r\n               \
    \                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"K:\\\
    text-generation-webui\\modules\\models.py\", line 88, in load_model\r\n    output\
    \ = load_func_map[loader](model_name)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"K:\\text-generation-webui\\modules\\models.py\", line 253, in llamacpp_loader\r\
    \n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\n       \
    \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"K:\\text-generation-webui\\\
    modules\\llamacpp_model.py\", line 91, in from_pretrained\r\n    result.model\
    \ = Llama(**params)\r\n                   ^^^^^^^^^^^^^^^\r\n  File \"K:\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 923,\
    \ in __init__\r\n    self._n_vocab = self.n_vocab()\r\n                    ^^^^^^^^^^^^^^\r\
    \n  File \"K:\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
    llama_cpp_cuda\\llama.py\", line 2184, in n_vocab\r\n    return self._model.n_vocab()\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"K:\\text-generation-webui\\installer_files\\\
    env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 250, in n_vocab\r\n\
    \    assert self.model is not None\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\
    \n\r\nException ignored in: <function LlamaCppModel.__del__ at 0x000001F48DC09D00>\r\
    \nTraceback (most recent call last):\r\n  File \"K:\\text-generation-webui\\modules\\\
    llamacpp_model.py\", line 49, in __del__\r\n    del self.model\r\n        ^^^^^^^^^^\r\
    \nAttributeError: 'LlamaCppModel' object has no attribute 'model'"
  created_at: 2023-12-12 03:37:02+00:00
  edited: false
  hidden: false
  id: 6577d55e90df9d85fb4c031e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/921171db37d397559d875d3d49a64a3f.svg
      fullname: Jopaul Jose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DarkCoverUnleashed
      type: user
    createdAt: '2023-12-12T06:48:01.000Z'
    data:
      edited: false
      editors:
      - DarkCoverUnleashed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8087266683578491
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/921171db37d397559d875d3d49a64a3f.svg
          fullname: Jopaul Jose
          isHf: false
          isPro: false
          name: DarkCoverUnleashed
          type: user
        html: '<p>I too am getting this same error</p>

          '
        raw: 'I too am getting this same error


          '
        updatedAt: '2023-12-12T06:48:01.875Z'
      numEdits: 0
      reactions: []
    id: 6578022117166d821e094579
    type: comment
  author: DarkCoverUnleashed
  content: 'I too am getting this same error


    '
  created_at: 2023-12-12 06:48:01+00:00
  edited: false
  hidden: false
  id: 6578022117166d821e094579
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-12T08:41:05.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9247841835021973
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>You guys need to build llamacpp from source using Mixtral branch
          for now <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/tree/mixtral">https://github.com/ggerganov/llama.cpp/tree/mixtral</a></p>

          <p>until it is merged into llamacpp main branch and get into the next released</p>

          '
        raw: 'You guys need to build llamacpp from source using Mixtral branch for
          now https://github.com/ggerganov/llama.cpp/tree/mixtral


          until it is merged into llamacpp main branch and get into the next released'
        updatedAt: '2023-12-12T08:41:05.258Z'
      numEdits: 0
      reactions: []
    id: 65781ca12f2e4058aed3bdb5
    type: comment
  author: Yhyu13
  content: 'You guys need to build llamacpp from source using Mixtral branch for now
    https://github.com/ggerganov/llama.cpp/tree/mixtral


    until it is merged into llamacpp main branch and get into the next released'
  created_at: 2023-12-12 08:41:05+00:00
  edited: false
  hidden: false
  id: 65781ca12f2e4058aed3bdb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/137d4fbc5ac6880f349d755800aa04ef.svg
      fullname: Alexander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IZA09
      type: user
    createdAt: '2023-12-12T19:34:42.000Z'
    data:
      edited: false
      editors:
      - IZA09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9575220942497253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/137d4fbc5ac6880f349d755800aa04ef.svg
          fullname: Alexander
          isHf: false
          isPro: false
          name: IZA09
          type: user
        html: '<blockquote>

          <p>You guys need to build llamacpp from source using Mixtral branch for
          now <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/tree/mixtral">https://github.com/ggerganov/llama.cpp/tree/mixtral</a></p>

          <p>until it is merged into llamacpp main branch and get into the next released</p>

          </blockquote>

          <p>I am very noob at this and still learning. what does that mean? how exactly
          do i do that? i run on windows through Conda.</p>

          '
        raw: "> You guys need to build llamacpp from source using Mixtral branch for\
          \ now https://github.com/ggerganov/llama.cpp/tree/mixtral\n> \n> until it\
          \ is merged into llamacpp main branch and get into the next released\n\n\
          I am very noob at this and still learning. what does that mean? how exactly\
          \ do i do that? i run on windows through Conda."
        updatedAt: '2023-12-12T19:34:42.497Z'
      numEdits: 0
      reactions: []
    id: 6578b5d2821f439498f4853c
    type: comment
  author: IZA09
  content: "> You guys need to build llamacpp from source using Mixtral branch for\
    \ now https://github.com/ggerganov/llama.cpp/tree/mixtral\n> \n> until it is merged\
    \ into llamacpp main branch and get into the next released\n\nI am very noob at\
    \ this and still learning. what does that mean? how exactly do i do that? i run\
    \ on windows through Conda."
  created_at: 2023-12-12 19:34:42+00:00
  edited: false
  hidden: false
  id: 6578b5d2821f439498f4853c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-12-12T19:53:20.000Z'
    data:
      edited: true
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8640539646148682
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>I could not get the llama build on Windows 11 but here is how I
          can use the model for now:</p>

          <p><a rel="nofollow" href="https://github.com/Nexesenex/kobold.cpp/releases/tag/1.52_mix">https://github.com/Nexesenex/kobold.cpp/releases/tag/1.52_mix</a></p>

          <p>The cuda version did not work even tough I have a RTX 3060 12GB gpu.<br>the
          cpu version works very fast :)</p>

          '
        raw: 'I could not get the llama build on Windows 11 but here is how I can
          use the model for now:


          https://github.com/Nexesenex/kobold.cpp/releases/tag/1.52_mix


          The cuda version did not work even tough I have a RTX 3060 12GB gpu.

          the cpu version works very fast :)'
        updatedAt: '2023-12-12T19:54:17.283Z'
      numEdits: 1
      reactions: []
    id: 6578ba3073080b490ce5a195
    type: comment
  author: CR2022
  content: 'I could not get the llama build on Windows 11 but here is how I can use
    the model for now:


    https://github.com/Nexesenex/kobold.cpp/releases/tag/1.52_mix


    The cuda version did not work even tough I have a RTX 3060 12GB gpu.

    the cpu version works very fast :)'
  created_at: 2023-12-12 19:53:20+00:00
  edited: true
  hidden: false
  id: 6578ba3073080b490ce5a195
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08ef692a79efbf9ae1448743740f1a99.svg
      fullname: Smaug Leterrible
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AliceThirty
      type: user
    createdAt: '2023-12-13T00:18:22.000Z'
    data:
      edited: true
      editors:
      - AliceThirty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6865147948265076
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08ef692a79efbf9ae1448743740f1a99.svg
          fullname: Smaug Leterrible
          isHf: false
          isPro: false
          name: AliceThirty
          type: user
        html: '<p>I found this<br><a rel="nofollow" href="https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/">https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/</a><br>But
          for me this method removes llama_cpp_python_cuda and doesn''t compile it.
          Can someone help please ?</p>

          '
        raw: 'I found this

          https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/

          But for me this method removes llama_cpp_python_cuda and doesn''t compile
          it. Can someone help please ?'
        updatedAt: '2023-12-13T00:37:31.556Z'
      numEdits: 2
      reactions: []
    id: 6578f84e9999746238c126b2
    type: comment
  author: AliceThirty
  content: 'I found this

    https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/

    But for me this method removes llama_cpp_python_cuda and doesn''t compile it.
    Can someone help please ?'
  created_at: 2023-12-13 00:18:22+00:00
  edited: true
  hidden: false
  id: 6578f84e9999746238c126b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-13T02:29:21.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8823639750480652
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<blockquote>\n<blockquote>\n<p>You guys need to build llamacpp from\
          \ source using Mixtral branch for now <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/tree/mixtral\"\
          >https://github.com/ggerganov/llama.cpp/tree/mixtral</a></p>\n<p>until it\
          \ is merged into llamacpp main branch and get into the next released</p>\n\
          </blockquote>\n<p>I am very noob at this and still learning. what does that\
          \ mean? how exactly do i do that? i run on windows through Conda.</p>\n\
          </blockquote>\n<p>I don't have a windows machine on hand, but here is what\
          \ should go right for building from souce:</p>\n<p>1, Install CUDA toolkit\
          \ 12.1 from Nvidia developer site, and set corresponding environment variables\
          \ correctly \uFF08because building  program needs env var to find your installation\uFF09\
          <br>Nvidia should have a good tutorial on this<br>2, Clone to mixtral branch\
          \ llamacpp to some local directory<br>3, Follow the CuBLAS build instruction\
          \ in the llamacpp README. Use Cmake<br>4, If you miss any packages that\
          \ cmake requires, install them from vcpkg <a rel=\"nofollow\" href=\"https://github.com/microsoft/vcpkg\"\
          >https://github.com/microsoft/vcpkg</a>, and add vcpkg tool chain to llamacpp's\
          \ CMakeFile.txt</p>\n<p>Some bascially, CUDA 12.1 toolkit, mixtral branch\
          \ llamacpp, cmake and vckpg are you need to build llamacpp from source with\
          \ support of CUDA device</p>\n"
        raw: "> > You guys need to build llamacpp from source using Mixtral branch\
          \ for now https://github.com/ggerganov/llama.cpp/tree/mixtral\n> > \n> >\
          \ until it is merged into llamacpp main branch and get into the next released\n\
          > \n> I am very noob at this and still learning. what does that mean? how\
          \ exactly do i do that? i run on windows through Conda.\n\nI don't have\
          \ a windows machine on hand, but here is what should go right for building\
          \ from souce:\n\n1, Install CUDA toolkit 12.1 from Nvidia developer site,\
          \ and set corresponding environment variables correctly \uFF08because building\
          \  program needs env var to find your installation\uFF09\nNvidia should\
          \ have a good tutorial on this\n2, Clone to mixtral branch llamacpp to some\
          \ local directory\n3, Follow the CuBLAS build instruction in the llamacpp\
          \ README. Use Cmake\n4, If you miss any packages that cmake requires, install\
          \ them from vcpkg https://github.com/microsoft/vcpkg, and add vcpkg tool\
          \ chain to llamacpp's CMakeFile.txt\n\nSome bascially, CUDA 12.1 toolkit,\
          \ mixtral branch llamacpp, cmake and vckpg are you need to build llamacpp\
          \ from source with support of CUDA device"
        updatedAt: '2023-12-13T02:29:21.680Z'
      numEdits: 0
      reactions: []
    id: 65791701800326e82537c107
    type: comment
  author: Yhyu13
  content: "> > You guys need to build llamacpp from source using Mixtral branch for\
    \ now https://github.com/ggerganov/llama.cpp/tree/mixtral\n> > \n> > until it\
    \ is merged into llamacpp main branch and get into the next released\n> \n> I\
    \ am very noob at this and still learning. what does that mean? how exactly do\
    \ i do that? i run on windows through Conda.\n\nI don't have a windows machine\
    \ on hand, but here is what should go right for building from souce:\n\n1, Install\
    \ CUDA toolkit 12.1 from Nvidia developer site, and set corresponding environment\
    \ variables correctly \uFF08because building  program needs env var to find your\
    \ installation\uFF09\nNvidia should have a good tutorial on this\n2, Clone to\
    \ mixtral branch llamacpp to some local directory\n3, Follow the CuBLAS build\
    \ instruction in the llamacpp README. Use Cmake\n4, If you miss any packages that\
    \ cmake requires, install them from vcpkg https://github.com/microsoft/vcpkg,\
    \ and add vcpkg tool chain to llamacpp's CMakeFile.txt\n\nSome bascially, CUDA\
    \ 12.1 toolkit, mixtral branch llamacpp, cmake and vckpg are you need to build\
    \ llamacpp from source with support of CUDA device"
  created_at: 2023-12-13 02:29:21+00:00
  edited: false
  hidden: false
  id: 65791701800326e82537c107
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-12-13T15:49:19.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8854941129684448
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>You guys need to build\
          \ llamacpp from source using Mixtral branch for now <a rel=\"nofollow\"\
          \ href=\"https://github.com/ggerganov/llama.cpp/tree/mixtral\">https://github.com/ggerganov/llama.cpp/tree/mixtral</a></p>\n\
          <p>until it is merged into llamacpp main branch and get into the next released</p>\n\
          </blockquote>\n<p>I am very noob at this and still learning. what does that\
          \ mean? how exactly do i do that? i run on windows through Conda.</p>\n\
          </blockquote>\n<p>I don't have a windows machine on hand, but here is what\
          \ should go right for building from souce:</p>\n<p>1, Install CUDA toolkit\
          \ 12.1 from Nvidia developer site, and set corresponding environment variables\
          \ correctly \uFF08because building  program needs env var to find your installation\uFF09\
          <br>Nvidia should have a good tutorial on this<br>2, Clone to mixtral branch\
          \ llamacpp to some local directory<br>3, Follow the CuBLAS build instruction\
          \ in the llamacpp README. Use Cmake<br>4, If you miss any packages that\
          \ cmake requires, install them from vcpkg <a rel=\"nofollow\" href=\"https://github.com/microsoft/vcpkg\"\
          >https://github.com/microsoft/vcpkg</a>, and add vcpkg tool chain to llamacpp's\
          \ CMakeFile.txt</p>\n<p>Some bascially, CUDA 12.1 toolkit, mixtral branch\
          \ llamacpp, cmake and vckpg are you need to build llamacpp from source with\
          \ support of CUDA device</p>\n</blockquote>\n<p>I have the Cuda 12.1 toolkit\
          \ installed and it shows up in my path and environments but still it says\
          \ it cannot find the Cuda 12.1 toolkit.</p>\n"
        raw: "> > > You guys need to build llamacpp from source using Mixtral branch\
          \ for now https://github.com/ggerganov/llama.cpp/tree/mixtral\n> > > \n\
          > > > until it is merged into llamacpp main branch and get into the next\
          \ released\n> > \n> > I am very noob at this and still learning. what does\
          \ that mean? how exactly do i do that? i run on windows through Conda.\n\
          > \n> I don't have a windows machine on hand, but here is what should go\
          \ right for building from souce:\n> \n> 1, Install CUDA toolkit 12.1 from\
          \ Nvidia developer site, and set corresponding environment variables correctly\
          \ \uFF08because building  program needs env var to find your installation\uFF09\
          \n> Nvidia should have a good tutorial on this\n> 2, Clone to mixtral branch\
          \ llamacpp to some local directory\n> 3, Follow the CuBLAS build instruction\
          \ in the llamacpp README. Use Cmake\n> 4, If you miss any packages that\
          \ cmake requires, install them from vcpkg https://github.com/microsoft/vcpkg,\
          \ and add vcpkg tool chain to llamacpp's CMakeFile.txt\n> \n> Some bascially,\
          \ CUDA 12.1 toolkit, mixtral branch llamacpp, cmake and vckpg are you need\
          \ to build llamacpp from source with support of CUDA device\n\nI have the\
          \ Cuda 12.1 toolkit installed and it shows up in my path and environments\
          \ but still it says it cannot find the Cuda 12.1 toolkit."
        updatedAt: '2023-12-13T15:49:19.157Z'
      numEdits: 0
      reactions: []
    id: 6579d27f5dbfcf78e4cffd74
    type: comment
  author: CR2022
  content: "> > > You guys need to build llamacpp from source using Mixtral branch\
    \ for now https://github.com/ggerganov/llama.cpp/tree/mixtral\n> > > \n> > > until\
    \ it is merged into llamacpp main branch and get into the next released\n> > \n\
    > > I am very noob at this and still learning. what does that mean? how exactly\
    \ do i do that? i run on windows through Conda.\n> \n> I don't have a windows\
    \ machine on hand, but here is what should go right for building from souce:\n\
    > \n> 1, Install CUDA toolkit 12.1 from Nvidia developer site, and set corresponding\
    \ environment variables correctly \uFF08because building  program needs env var\
    \ to find your installation\uFF09\n> Nvidia should have a good tutorial on this\n\
    > 2, Clone to mixtral branch llamacpp to some local directory\n> 3, Follow the\
    \ CuBLAS build instruction in the llamacpp README. Use Cmake\n> 4, If you miss\
    \ any packages that cmake requires, install them from vcpkg https://github.com/microsoft/vcpkg,\
    \ and add vcpkg tool chain to llamacpp's CMakeFile.txt\n> \n> Some bascially,\
    \ CUDA 12.1 toolkit, mixtral branch llamacpp, cmake and vckpg are you need to\
    \ build llamacpp from source with support of CUDA device\n\nI have the Cuda 12.1\
    \ toolkit installed and it shows up in my path and environments but still it says\
    \ it cannot find the Cuda 12.1 toolkit."
  created_at: 2023-12-13 15:49:19+00:00
  edited: false
  hidden: false
  id: 6579d27f5dbfcf78e4cffd74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-12-13T16:00:30.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9088075757026672
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>Could it be because you have Cuda 12.1.1 and 12.1.0?</p>

          <p>I only have 12.1.1 installed.</p>

          '
        raw: 'Could it be because you have Cuda 12.1.1 and 12.1.0?


          I only have 12.1.1 installed.'
        updatedAt: '2023-12-13T16:00:30.509Z'
      numEdits: 0
      reactions: []
    id: 6579d51eb685bd95139f6d1e
    type: comment
  author: CR2022
  content: 'Could it be because you have Cuda 12.1.1 and 12.1.0?


    I only have 12.1.1 installed.'
  created_at: 2023-12-13 16:00:30+00:00
  edited: false
  hidden: false
  id: 6579d51eb685bd95139f6d1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-12-13T16:48:06.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5745487213134766
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p><a rel="nofollow" href="https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/">https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/</a></p>

          '
        raw: https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/
        updatedAt: '2023-12-13T16:48:06.806Z'
      numEdits: 0
      reactions: []
    id: 6579e0468ee8830a31e972ad
    type: comment
  author: CR2022
  content: https://www.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/
  created_at: 2023-12-13 16:48:06+00:00
  edited: false
  hidden: false
  id: 6579e0468ee8830a31e972ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9847172ac2c832dfaba68af1a6846c.svg
      fullname: Austin Swanlaw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AuLaSW
      type: user
    createdAt: '2023-12-13T18:58:45.000Z'
    data:
      edited: false
      editors:
      - AuLaSW
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9310756325721741
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9847172ac2c832dfaba68af1a6846c.svg
          fullname: Austin Swanlaw
          isHf: false
          isPro: false
          name: AuLaSW
          type: user
        html: '<p>You can do it with even fewer steps now, I made a push to llama-cpp-python
          to merge the new llama.cpp submodule since the mixtral branch has been merged
          into llama.cpp.</p>

          <p><a rel="nofollow" href="https://www.github.com/abetlen/llama-cpp-python/pull/1007">https://www.github.com/abetlen/llama-cpp-python/pull/1007</a></p>

          <p>Then all you have to do is install with <code>pip install -t &lt;site-packages
          in the webui folder&gt; &lt;path to llama-cpp-install&gt; --upgrade</code>and
          it should work. You will want to set your environment variables as you need
          but this takes a few steps out of the process.</p>

          '
        raw: 'You can do it with even fewer steps now, I made a push to llama-cpp-python
          to merge the new llama.cpp submodule since the mixtral branch has been merged
          into llama.cpp.


          https://www.github.com/abetlen/llama-cpp-python/pull/1007


          Then all you have to do is install with `pip install -t <site-packages in
          the webui folder> <path to llama-cpp-install> --upgrade`and it should work.
          You will want to set your environment variables as you need but this takes
          a few steps out of the process.'
        updatedAt: '2023-12-13T18:58:45.035Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - CR2022
    id: 6579fee5330fe066775cb568
    type: comment
  author: AuLaSW
  content: 'You can do it with even fewer steps now, I made a push to llama-cpp-python
    to merge the new llama.cpp submodule since the mixtral branch has been merged
    into llama.cpp.


    https://www.github.com/abetlen/llama-cpp-python/pull/1007


    Then all you have to do is install with `pip install -t <site-packages in the
    webui folder> <path to llama-cpp-install> --upgrade`and it should work. You will
    want to set your environment variables as you need but this takes a few steps
    out of the process.'
  created_at: 2023-12-13 18:58:45+00:00
  edited: false
  hidden: false
  id: 6579fee5330fe066775cb568
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0b1a6312fd4fc749bbc5ba4da9d23cb.svg
      fullname: emrd 95
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: emrd95
      type: user
    createdAt: '2023-12-13T20:51:57.000Z'
    data:
      edited: false
      editors:
      - emrd95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7743784785270691
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0b1a6312fd4fc749bbc5ba4da9d23cb.svg
          fullname: emrd 95
          isHf: false
          isPro: false
          name: emrd95
          type: user
        html: '<p>Finally managed to run it on windows, got to install <a rel="nofollow"
          href="https://developer.nvidia.com/cuda-12-1-0-download-archive">https://developer.nvidia.com/cuda-12-1-0-download-archive</a>
          (getting the correct version is important) with the other dependencies,
          install the webui manually with conda, (without the one click installer),
          then just follow this <a rel="nofollow" href="https://old.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/">https://old.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/</a></p>

          '
        raw: Finally managed to run it on windows, got to install https://developer.nvidia.com/cuda-12-1-0-download-archive
          (getting the correct version is important) with the other dependencies,
          install the webui manually with conda, (without the one click installer),
          then just follow this https://old.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/
        updatedAt: '2023-12-13T20:51:57.433Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - CR2022
    id: 657a196de221007741c959db
    type: comment
  author: emrd95
  content: Finally managed to run it on windows, got to install https://developer.nvidia.com/cuda-12-1-0-download-archive
    (getting the correct version is important) with the other dependencies, install
    the webui manually with conda, (without the one click installer), then just follow
    this https://old.reddit.com/r/Oobabooga/comments/18gijyx/simple_tutorial_using_mixtral_8x7b_gguf_in_ooba/
  created_at: 2023-12-13 20:51:57+00:00
  edited: false
  hidden: false
  id: 657a196de221007741c959db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-12-14T05:43:33.000Z'
    data:
      status: closed
    id: 657a9605f10a984150a9cbb4
    type: status-change
  author: CR2022
  created_at: 2023-12-14 05:43:33+00:00
  id: 657a9605f10a984150a9cbb4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
repo_type: model
status: closed
target_branch: null
title: Does not work in latest oobabooga text generation web ui.
