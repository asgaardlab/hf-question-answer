!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hoioi
conflicting_files: null
created_at: 2023-12-12 05:20:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-12-12T05:20:41.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9432195425033569
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>Could someone please share the number of tokens per second they
          get from running this model if they are running it only on CPU and RAM without
          GPU? </p>

          '
        raw: 'Could someone please share the number of tokens per second they get
          from running this model if they are running it only on CPU and RAM without
          GPU? '
        updatedAt: '2023-12-12T05:20:41.957Z'
      numEdits: 0
      reactions: []
    id: 6577eda9277dceb60562fd23
    type: comment
  author: Hoioi
  content: 'Could someone please share the number of tokens per second they get from
    running this model if they are running it only on CPU and RAM without GPU? '
  created_at: 2023-12-12 05:20:41+00:00
  edited: false
  hidden: false
  id: 6577eda9277dceb60562fd23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-12-12T05:21:03.000Z'
    data:
      from: How many token per second?
      to: How many tokens per second?
    id: 6577edbf6db22fc06c96f284
    type: title-change
  author: Hoioi
  created_at: 2023-12-12 05:21:03+00:00
  id: 6577edbf6db22fc06c96f284
  new_title: How many tokens per second?
  old_title: How many token per second?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d09fe936b7311ea1ee5b662f4aa83900.svg
      fullname: Tong LI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tli0312
      type: user
    createdAt: '2023-12-12T05:55:30.000Z'
    data:
      edited: false
      editors:
      - tli0312
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7001805305480957
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d09fe936b7311ea1ee5b662f4aa83900.svg
          fullname: Tong LI
          isHf: false
          isPro: false
          name: tli0312
          type: user
        html: '<p>7 t/s on CPU/RAM only (Ryzen 5 3600), 10 t/s with 10 layers off
          load to GPU, 12 t/s with 15 layers off load to GPU</p>

          '
        raw: 7 t/s on CPU/RAM only (Ryzen 5 3600), 10 t/s with 10 layers off load
          to GPU, 12 t/s with 15 layers off load to GPU
        updatedAt: '2023-12-12T05:55:30.882Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - HR1777
        - Hoioi
        - margin707
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Yhyu13
    id: 6577f5d20c22bb8984b17f8c
    type: comment
  author: tli0312
  content: 7 t/s on CPU/RAM only (Ryzen 5 3600), 10 t/s with 10 layers off load to
    GPU, 12 t/s with 15 layers off load to GPU
  created_at: 2023-12-12 05:55:30+00:00
  edited: false
  hidden: false
  id: 6577f5d20c22bb8984b17f8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2023-12-12T07:19:37.000Z'
    data:
      edited: true
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8372358083724976
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: '<blockquote>

          <p>7 t/s on CPU/RAM only (Ryzen 5 3600), 10 t/s with 10 layers off load
          to GPU, 12 t/s with 15 layers off load to GPU</p>

          </blockquote>

          <p>7 t/s on CPU/RAM seems pretty good. How much RAM do you have on your
          computer? And what interface do you use? text-generation-webui? koboldcpp
          or what?</p>

          '
        raw: '> 7 t/s on CPU/RAM only (Ryzen 5 3600), 10 t/s with 10 layers off load
          to GPU, 12 t/s with 15 layers off load to GPU


          7 t/s on CPU/RAM seems pretty good. How much RAM do you have on your computer?
          And what interface do you use? text-generation-webui? koboldcpp or what?

          '
        updatedAt: '2023-12-12T07:19:57.836Z'
      numEdits: 1
      reactions: []
    id: 65780989c37954680aac1c06
    type: comment
  author: HR1777
  content: '> 7 t/s on CPU/RAM only (Ryzen 5 3600), 10 t/s with 10 layers off load
    to GPU, 12 t/s with 15 layers off load to GPU


    7 t/s on CPU/RAM seems pretty good. How much RAM do you have on your computer?
    And what interface do you use? text-generation-webui? koboldcpp or what?

    '
  created_at: 2023-12-12 07:19:37+00:00
  edited: true
  hidden: false
  id: 65780989c37954680aac1c06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-12T07:43:35.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4227924644947052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Llamacpp<br>On my rtx 3090 ...around 40 t/s<br>Version q4k_m ( 30
          layers on GPU ) </p>

          '
        raw: 'Llamacpp

          On my rtx 3090 ...around 40 t/s

          Version q4k_m ( 30 layers on GPU ) '
        updatedAt: '2023-12-12T07:43:35.549Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - HR1777
        - Hoioi
        - Yaiko
        - Igor7777
        - cmod
    id: 65780f27c37954680aad33a7
    type: comment
  author: mirek190
  content: 'Llamacpp

    On my rtx 3090 ...around 40 t/s

    Version q4k_m ( 30 layers on GPU ) '
  created_at: 2023-12-12 07:43:35+00:00
  edited: false
  hidden: false
  id: 65780f27c37954680aad33a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-12-12T09:58:48.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806414842605591
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>Thank you for your replies. If anyone else has the statistics, please
          share with us. </p>

          '
        raw: 'Thank you for your replies. If anyone else has the statistics, please
          share with us. '
        updatedAt: '2023-12-12T09:58:48.295Z'
      numEdits: 0
      reactions: []
    id: 65782ed8cfb2207f11f218ea
    type: comment
  author: Hoioi
  content: 'Thank you for your replies. If anyone else has the statistics, please
    share with us. '
  created_at: 2023-12-12 09:58:48+00:00
  edited: false
  hidden: false
  id: 65782ed8cfb2207f11f218ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/962f480248c32360efc87487187b39e0.svg
      fullname: rahul reddy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rahulrock12
      type: user
    createdAt: '2023-12-12T18:07:47.000Z'
    data:
      edited: false
      editors:
      - rahulrock12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7005398273468018
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/962f480248c32360efc87487187b39e0.svg
          fullname: rahul reddy
          isHf: false
          isPro: false
          name: rahulrock12
          type: user
        html: '<blockquote>

          <p>Llamacpp<br>On my rtx 3090 ...around 40 t/s<br>Version q4k_m ( 30 layers
          on GPU )</p>

          </blockquote>

          <p>Hi, can you please share the python code used to access the model, i
          am struggling to find any.</p>

          '
        raw: '> Llamacpp

          > On my rtx 3090 ...around 40 t/s

          > Version q4k_m ( 30 layers on GPU )


          Hi, can you please share the python code used to access the model, i am
          struggling to find any.'
        updatedAt: '2023-12-12T18:07:47.267Z'
      numEdits: 0
      reactions: []
    id: 6578a173277dceb605828316
    type: comment
  author: rahulrock12
  content: '> Llamacpp

    > On my rtx 3090 ...around 40 t/s

    > Version q4k_m ( 30 layers on GPU )


    Hi, can you please share the python code used to access the model, i am struggling
    to find any.'
  created_at: 2023-12-12 18:07:47+00:00
  edited: false
  hidden: false
  id: 6578a173277dceb605828316
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-12T19:50:03.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7388253211975098
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I''m using llamacpp ( one small binary fie ) to run model. </p>

          '
        raw: 'I''m using llamacpp ( one small binary fie ) to run model. '
        updatedAt: '2023-12-12T19:50:03.014Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Hoioi
    id: 6578b96b1bf8e371c4ed11b1
    type: comment
  author: mirek190
  content: 'I''m using llamacpp ( one small binary fie ) to run model. '
  created_at: 2023-12-12 19:50:03+00:00
  edited: false
  hidden: false
  id: 6578b96b1bf8e371c4ed11b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
      fullname: Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vidyamantra
      type: user
    createdAt: '2023-12-15T12:22:44.000Z'
    data:
      edited: false
      editors:
      - vidyamantra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4433029890060425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec4b537566ad792e0f22d474a68f65c5.svg
          fullname: Gupta
          isHf: false
          isPro: false
          name: vidyamantra
          type: user
        html: "<blockquote>\n<p>Thank you for your replies. If anyone else has the\
          \ statistics, please share with us.</p>\n</blockquote>\n<p>On RTX 4090 &amp;\
          \ i9-14900K. Benchmark using llama-bench from llama.cpp.</p>\n<div class=\"\
          max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th><strong>model</strong></th>\n\
          <th><strong>size</strong></th>\n<th><strong>params</strong></th>\n<th><strong>backend</strong></th>\n\
          <th><strong>ngl</strong></th>\n<th><strong>threads</strong></th>\n<th><strong>t/s\
          \ pp 512</strong></th>\n<th><strong>t/s tg 128</strong></th>\n</tr>\n\n\t\
          \t</thead><tbody><tr>\n<td>llama 7B mostly Q3_K - Medium</td>\n<td>18.96\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n<td>8</td>\n<td>205.07</td>\n\
          <td>83.16</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q3_K - Medium</td>\n<td>18.96\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n<td>16</td>\n\
          <td>204.48</td>\n<td>83.21</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q3_K -\
          \ Medium</td>\n<td>18.96 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>33</td>\n\
          <td>24</td>\n<td>204.28</td>\n<td>83.22</td>\n</tr>\n<tr>\n<td>llama 7B\
          \ mostly Q3_K - Medium</td>\n<td>18.96 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>33</td>\n<td>32</td>\n<td>203.82</td>\n<td>83.17</td>\n</tr>\n<tr>\n\
          <td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62 GiB</td>\n<td>46.70 B</td>\n\
          <td>CUDA</td>\n<td>27</td>\n<td>8</td>\n<td>145.54</td>\n<td>27.75</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62 GiB</td>\n\
          <td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n<td>16</td>\n<td>121.58</td>\n\
          <td>25.57</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q4_K - Medium</td>\n<td>24.62\
          \ GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n<td>24</td>\n\
          <td>147.14</td>\n<td>26.41</td>\n</tr>\n<tr>\n<td>llama 7B mostly Q4_K -\
          \ Medium</td>\n<td>24.62 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n<td>27</td>\n\
          <td>32</td>\n<td>145.23</td>\n<td>9.36</td>\n</tr>\n<tr>\n<td>llama 7B mostly\
          \ Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>8</td>\n<td>58.18</td>\n<td>15.12</td>\n</tr>\n<tr>\n<td>llama\
          \ 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>16</td>\n<td>49.28</td>\n<td>13.8</td>\n</tr>\n<tr>\n<td>llama\
          \ 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n<td>CUDA</td>\n\
          <td>22</td>\n<td>24</td>\n<td>64.25</td>\n<td>15.07</td>\n</tr>\n<tr>\n\
          <td>llama 7B mostly Q5_K - Medium</td>\n<td>30.02 GiB</td>\n<td>46.70 B</td>\n\
          <td>CUDA</td>\n<td>22</td>\n<td>32</td>\n<td>73.69</td>\n<td>12.02</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>8</td>\n<td>33.86</td>\n<td>10.5</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>16</td>\n<td>31.75</td>\n<td>9.5</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>24</td>\n<td>40.37</td>\n<td>10.58</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q6_K</td>\n<td>35.74 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>19</td>\n<td>32</td>\n<td>45.39</td>\n<td>8.8</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>8</td>\n<td>18.02</td>\n<td>7.1</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>16</td>\n<td>19.74</td>\n<td>5.9</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>24</td>\n<td>24.81</td>\n<td>6.74</td>\n\
          </tr>\n<tr>\n<td>llama 7B mostly Q8_0</td>\n<td>46.22 GiB</td>\n<td>46.70\
          \ B</td>\n<td>CUDA</td>\n<td>15</td>\n<td>32</td>\n<td>28.31</td>\n<td>5.62</td>\n\
          </tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: '> Thank you for your replies. If anyone else has the statistics, please
          share with us.


          On RTX 4090 & i9-14900K. Benchmark using llama-bench from llama.cpp.


          | **model**                     | **size**  | **params** | **backend** |
          **ngl** | **threads** | **t/s pp 512** | **t/s tg 128** |

          |-------------------------------|-----------|------------|-------------|---------|-------------|----------------|----------------|

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 8           | 205.07         | 83.16          |

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 16          | 204.48         | 83.21          |

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 24          | 204.28         | 83.22          |

          | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        |
          33      | 32          | 203.82         | 83.17          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 8           | 145.54         | 27.75          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 16          | 121.58         | 25.57          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 24          | 147.14         | 26.41          |

          | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        |
          27      | 32          | 145.23         | 9.36           |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 8           | 58.18          | 15.12          |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 16          | 49.28          | 13.8           |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 24          | 64.25          | 15.07          |

          | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        |
          22      | 32          | 73.69          | 12.02          |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 8           | 33.86          | 10.5           |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 16          | 31.75          | 9.5            |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 24          | 40.37          | 10.58          |

          | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        |
          19      | 32          | 45.39          | 8.8            |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 8           | 18.02          | 7.1            |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 16          | 19.74          | 5.9            |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 24          | 24.81          | 6.74           |

          | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        |
          15      | 32          | 28.31          | 5.62           |

          '
        updatedAt: '2023-12-15T12:22:44.528Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - HR1777
        - Igor7777
        - BahamutRU
        - Mar2ck
    id: 657c45145c6f0b1f360b14e5
    type: comment
  author: vidyamantra
  content: '> Thank you for your replies. If anyone else has the statistics, please
    share with us.


    On RTX 4090 & i9-14900K. Benchmark using llama-bench from llama.cpp.


    | **model**                     | **size**  | **params** | **backend** | **ngl**
    | **threads** | **t/s pp 512** | **t/s tg 128** |

    |-------------------------------|-----------|------------|-------------|---------|-------------|----------------|----------------|

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    8           | 205.07         | 83.16          |

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    16          | 204.48         | 83.21          |

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    24          | 204.28         | 83.22          |

    | llama 7B mostly Q3_K - Medium | 18.96 GiB | 46.70 B    | CUDA        | 33      |
    32          | 203.82         | 83.17          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    8           | 145.54         | 27.75          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    16          | 121.58         | 25.57          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    24          | 147.14         | 26.41          |

    | llama 7B mostly Q4_K - Medium | 24.62 GiB | 46.70 B    | CUDA        | 27      |
    32          | 145.23         | 9.36           |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    8           | 58.18          | 15.12          |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    16          | 49.28          | 13.8           |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    24          | 64.25          | 15.07          |

    | llama 7B mostly Q5_K - Medium | 30.02 GiB | 46.70 B    | CUDA        | 22      |
    32          | 73.69          | 12.02          |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    8           | 33.86          | 10.5           |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    16          | 31.75          | 9.5            |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    24          | 40.37          | 10.58          |

    | llama 7B mostly Q6_K          | 35.74 GiB | 46.70 B    | CUDA        | 19      |
    32          | 45.39          | 8.8            |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    8           | 18.02          | 7.1            |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    16          | 19.74          | 5.9            |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    24          | 24.81          | 6.74           |

    | llama 7B mostly Q8_0          | 46.22 GiB | 46.70 B    | CUDA        | 15      |
    32          | 28.31          | 5.62           |

    '
  created_at: 2023-12-15 12:22:44+00:00
  edited: false
  hidden: false
  id: 657c45145c6f0b1f360b14e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/815d8043c261eb8d4350e4ee71b78501.svg
      fullname: ME
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GIOVANITH2
      type: user
    createdAt: '2023-12-17T21:43:39.000Z'
    data:
      edited: false
      editors:
      - GIOVANITH2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7815608382225037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/815d8043c261eb8d4350e4ee71b78501.svg
          fullname: ME
          isHf: false
          isPro: false
          name: GIOVANITH2
          type: user
        html: '<p>hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam)
          I got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.<br>I tried the 6-bit
          model but it does not run (cuda error, out of memory).<br>I''ll try another
          configs; any improvement I''ll post here later.</p>

          '
        raw: 'hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
          got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

          I tried the 6-bit model but it does not run (cuda error, out of memory).

          I''ll try another configs; any improvement I''ll post here later.'
        updatedAt: '2023-12-17T21:43:39.908Z'
      numEdits: 0
      reactions: []
    id: 657f6b8b3c65a5be2a4ad157
    type: comment
  author: GIOVANITH2
  content: 'hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I got
    just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf, model loader
    = llama.cpp, n-gpu-layers=30; n_ctx=16384.

    I tried the 6-bit model but it does not run (cuda error, out of memory).

    I''ll try another configs; any improvement I''ll post here later.'
  created_at: 2023-12-17 21:43:39+00:00
  edited: false
  hidden: false
  id: 657f6b8b3c65a5be2a4ad157
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-12-17T22:46:12.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8683944344520569
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<blockquote>

          <p>hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
          got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.<br>I tried the 6-bit
          model but it does not run (cuda error, out of memory).<br>I''ll try another
          configs; any improvement I''ll post here later.</p>

          </blockquote>

          <p>That is far worse than i get via an old Xeon CPU only, and im using the
          Q6 model ( latest ooba isn''t using my GPU at all. I need to look into it
          later this week to see what is up )</p>

          '
        raw: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam)
          I got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

          > I tried the 6-bit model but it does not run (cuda error, out of memory).

          > I''ll try another configs; any improvement I''ll post here later.


          That is far worse than i get via an old Xeon CPU only, and im using the
          Q6 model ( latest ooba isn''t using my GPU at all. I need to look into it
          later this week to see what is up )'
        updatedAt: '2023-12-17T22:46:12.573Z'
      numEdits: 0
      reactions: []
    id: 657f7a34eaad53ff67e097c7
    type: comment
  author: Nurb432
  content: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
    got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf, model
    loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

    > I tried the 6-bit model but it does not run (cuda error, out of memory).

    > I''ll try another configs; any improvement I''ll post here later.


    That is far worse than i get via an old Xeon CPU only, and im using the Q6 model
    ( latest ooba isn''t using my GPU at all. I need to look into it later this week
    to see what is up )'
  created_at: 2023-12-17 22:46:12+00:00
  edited: false
  hidden: false
  id: 657f7a34eaad53ff67e097c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-12-17T23:38:08.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.892250120639801
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<blockquote>

          <p>hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
          got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.<br>I tried the 6-bit
          model but it does not run (cuda error, out of memory).<br>I''ll try another
          configs; any improvement I''ll post here later.</p>

          </blockquote>

          <p>It seems far less than what it should be. I''m almost sure something
          is wrong. </p>

          '
        raw: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam)
          I got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

          > I tried the 6-bit model but it does not run (cuda error, out of memory).

          > I''ll try another configs; any improvement I''ll post here later.


          It seems far less than what it should be. I''m almost sure something is
          wrong. '
        updatedAt: '2023-12-17T23:38:08.217Z'
      numEdits: 0
      reactions: []
    id: 657f86601e208703246ad5aa
    type: comment
  author: Hoioi
  content: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
    got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf, model
    loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

    > I tried the 6-bit model but it does not run (cuda error, out of memory).

    > I''ll try another configs; any improvement I''ll post here later.


    It seems far less than what it should be. I''m almost sure something is wrong. '
  created_at: 2023-12-17 23:38:08+00:00
  edited: false
  hidden: false
  id: 657f86601e208703246ad5aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-18T00:06:59.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8001078367233276
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<blockquote>

          <p>hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
          got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.<br>I tried the 6-bit
          model but it does not run (cuda error, out of memory).<br>I''ll try another
          configs; any improvement I''ll post here later.</p>

          </blockquote>

          <p>It is extremally slow ...  I have ryzen 7950x3d and RTX 3090 getting
          30+ tokens/s with q4k_m and with q5 10+ tokens/s  (less layers on gpu )</p>

          '
        raw: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam)
          I got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

          > I tried the 6-bit model but it does not run (cuda error, out of memory).

          > I''ll try another configs; any improvement I''ll post here later.


          It is extremally slow ...  I have ryzen 7950x3d and RTX 3090 getting 30+
          tokens/s with q4k_m and with q5 10+ tokens/s  (less layers on gpu )'
        updatedAt: '2023-12-18T00:08:07.201Z'
      numEdits: 1
      reactions: []
    id: 657f8d23156fdf03010011d9
    type: comment
  author: mirek190
  content: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
    got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf, model
    loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

    > I tried the 6-bit model but it does not run (cuda error, out of memory).

    > I''ll try another configs; any improvement I''ll post here later.


    It is extremally slow ...  I have ryzen 7950x3d and RTX 3090 getting 30+ tokens/s
    with q4k_m and with q5 10+ tokens/s  (less layers on gpu )'
  created_at: 2023-12-18 00:06:59+00:00
  edited: true
  hidden: false
  id: 657f8d23156fdf03010011d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/453710ada8f7227c5e3c6b8f9119fed4.svg
      fullname: Durant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: delphijb
      type: user
    createdAt: '2023-12-18T02:03:39.000Z'
    data:
      edited: false
      editors:
      - delphijb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9316331148147583
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/453710ada8f7227c5e3c6b8f9119fed4.svg
          fullname: Durant
          isHf: false
          isPro: false
          name: delphijb
          type: user
        html: '<blockquote>

          <p>hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
          got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.<br>I tried the 6-bit
          model but it does not run (cuda error, out of memory).<br>I''ll try another
          configs; any improvement I''ll post here later.</p>

          </blockquote>

          <p>Too much layer on GPU, especially with Q5. Try 18 to 20 layers instead.<br>With
          30 GPU layers, it''s likely that the excess will be stored in shared video
          memory (RAM), which is not at all advisable, given that it''s the GPU that
          will be working with this very slow memory.<br>Also, don''t forget that
          the first response after the model has been loaded into memory can take
          much longer...</p>

          '
        raw: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam)
          I got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf,
          model loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

          > I tried the 6-bit model but it does not run (cuda error, out of memory).

          > I''ll try another configs; any improvement I''ll post here later.


          Too much layer on GPU, especially with Q5. Try 18 to 20 layers instead.

          With 30 GPU layers, it''s likely that the excess will be stored in shared
          video memory (RAM), which is not at all advisable, given that it''s the
          GPU that will be working with this very slow memory.

          Also, don''t forget that the first response after the model has been loaded
          into memory can take much longer...'
        updatedAt: '2023-12-18T02:03:39.115Z'
      numEdits: 0
      reactions: []
    id: 657fa87ba982e9093fb87cba
    type: comment
  author: delphijb
  content: '> hello, in my machine (Ryzen 7 1700; 40Gb Ram; RTX 4090 24 Gb VRam) I
    got just a mere 0.96 tokens/s. Oogabooga, mixtral-8x7b-v0.1.Q5_K_M.gguf, model
    loader = llama.cpp, n-gpu-layers=30; n_ctx=16384.

    > I tried the 6-bit model but it does not run (cuda error, out of memory).

    > I''ll try another configs; any improvement I''ll post here later.


    Too much layer on GPU, especially with Q5. Try 18 to 20 layers instead.

    With 30 GPU layers, it''s likely that the excess will be stored in shared video
    memory (RAM), which is not at all advisable, given that it''s the GPU that will
    be working with this very slow memory.

    Also, don''t forget that the first response after the model has been loaded into
    memory can take much longer...'
  created_at: 2023-12-18 02:03:39+00:00
  edited: false
  hidden: false
  id: 657fa87ba982e9093fb87cba
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: How many tokens per second?
