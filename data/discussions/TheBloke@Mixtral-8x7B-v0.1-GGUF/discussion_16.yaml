!!python/object:huggingface_hub.community.DiscussionWithDetails
author: johanteekens
conflicting_files: null
created_at: 2023-12-27 13:58:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3007f156d61c9a59d22ce79d6eb18d5b.svg
      fullname: Johan Teekens
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johanteekens
      type: user
    createdAt: '2023-12-27T13:58:57.000Z'
    data:
      edited: false
      editors:
      - johanteekens
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.35621821880340576
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3007f156d61c9a59d22ce79d6eb18d5b.svg
          fullname: Johan Teekens
          isHf: false
          isPro: false
          name: johanteekens
          type: user
        html: '<p>mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf is amazing!</p>

          <p>Fast, smart, process logic, create code, SQL etc running like crazy on
          my 2 x RTX 3090''s</p>

          <p>llama_print_timings:        load time =     731.83 ms<br>llama_print_timings:      sample
          time =      19.96 ms /   199 runs   (    0.10 ms per token,  9968.94 tokens
          per second)<br>llama_print_timings: prompt eval time =     279.92 ms /    17
          tokens (   16.47 ms per token,    60.73 tokens per second)<br>llama_print_timings:        eval
          time =    6256.11 ms /   198 runs   (   31.60 ms per token,    31.65 tokens
          per second)<br>llama_print_timings:       total time =    6778.80 ms</p>

          '
        raw: "mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf is amazing!\r\n\r\nFast, smart,\
          \ process logic, create code, SQL etc running like crazy on my 2 x RTX 3090's\r\
          \n\r\nllama_print_timings:        load time =     731.83 ms\r\nllama_print_timings:\
          \      sample time =      19.96 ms /   199 runs   (    0.10 ms per token,\
          \  9968.94 tokens per second)\r\nllama_print_timings: prompt eval time =\
          \     279.92 ms /    17 tokens (   16.47 ms per token,    60.73 tokens per\
          \ second)\r\nllama_print_timings:        eval time =    6256.11 ms /   198\
          \ runs   (   31.60 ms per token,    31.65 tokens per second)\r\nllama_print_timings:\
          \       total time =    6778.80 ms"
        updatedAt: '2023-12-27T13:58:57.918Z'
      numEdits: 0
      reactions: []
    id: 658c2da129ef008a129affed
    type: comment
  author: johanteekens
  content: "mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf is amazing!\r\n\r\nFast, smart,\
    \ process logic, create code, SQL etc running like crazy on my 2 x RTX 3090's\r\
    \n\r\nllama_print_timings:        load time =     731.83 ms\r\nllama_print_timings:\
    \      sample time =      19.96 ms /   199 runs   (    0.10 ms per token,  9968.94\
    \ tokens per second)\r\nllama_print_timings: prompt eval time =     279.92 ms\
    \ /    17 tokens (   16.47 ms per token,    60.73 tokens per second)\r\nllama_print_timings:\
    \        eval time =    6256.11 ms /   198 runs   (   31.60 ms per token,    31.65\
    \ tokens per second)\r\nllama_print_timings:       total time =    6778.80 ms"
  created_at: 2023-12-27 13:58:57+00:00
  edited: false
  hidden: false
  id: 658c2da129ef008a129affed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fa144b47413ed5dd211a47f5cef53135.svg
      fullname: Britt C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: itsmebcc
      type: user
    createdAt: '2023-12-27T19:54:21.000Z'
    data:
      edited: false
      editors:
      - itsmebcc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8187970519065857
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fa144b47413ed5dd211a47f5cef53135.svg
          fullname: Britt C
          isHf: false
          isPro: false
          name: itsmebcc
          type: user
        html: '<p>Did you change any of the config settings? If so what?</p>

          '
        raw: Did you change any of the config settings? If so what?
        updatedAt: '2023-12-27T19:54:21.314Z'
      numEdits: 0
      reactions: []
    id: 658c80edf9eec53a12656e94
    type: comment
  author: itsmebcc
  content: Did you change any of the config settings? If so what?
  created_at: 2023-12-27 19:54:21+00:00
  edited: false
  hidden: false
  id: 658c80edf9eec53a12656e94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3007f156d61c9a59d22ce79d6eb18d5b.svg
      fullname: Johan Teekens
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johanteekens
      type: user
    createdAt: '2023-12-29T15:13:53.000Z'
    data:
      edited: false
      editors:
      - johanteekens
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4238200783729553
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3007f156d61c9a59d22ce79d6eb18d5b.svg
          fullname: Johan Teekens
          isHf: false
          isPro: false
          name: johanteekens
          type: user
        html: '<p>Nothing special.</p>

          <p>Hardware: 2 x RTX 3090 on a ASUS B650 creator proart motherboard with
          AMD 7800, 32 GB mem.<br>OS: Ubuntu 22.04, Driver Version: 535.129.03   CUDA
          Version: 12.2, Docker with CUDA support.<br>My standard development container:
          <a rel="nofollow" href="https://github.com/johanteekens/ml-dev-docker">https://github.com/johanteekens/ml-dev-docker</a>  (This
          compiles llama.cpp from github while building, please check Dockerfile.)<br>llm
          = LlamaCPP(<br>    model_url=None,<br>    model_path="mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",<br>    temperature=0.1,<br>    max_new_tokens=4800,<br>    context_window=3900,<br>    generate_kwargs={},<br>    model_kwargs={"n_gpu_layers":
          50},<br>    messages_to_prompt=messages_to_prompt,<br>    completion_to_prompt=completion_to_prompt,<br>    verbose=True,<br>)</p>

          '
        raw: "Nothing special.\n\nHardware: 2 x RTX 3090 on a ASUS B650 creator proart\
          \ motherboard with AMD 7800, 32 GB mem.\nOS: Ubuntu 22.04, Driver Version:\
          \ 535.129.03   CUDA Version: 12.2, Docker with CUDA support.\nMy standard\
          \ development container: https://github.com/johanteekens/ml-dev-docker \
          \ (This compiles llama.cpp from github while building, please check Dockerfile.)\n\
          llm = LlamaCPP(\n    model_url=None,\n    model_path=\"mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\"\
          ,\n    temperature=0.1,\n    max_new_tokens=4800,\n    context_window=3900,\n\
          \    generate_kwargs={},\n    model_kwargs={\"n_gpu_layers\": 50},\n   \
          \ messages_to_prompt=messages_to_prompt,\n    completion_to_prompt=completion_to_prompt,\n\
          \    verbose=True,\n)"
        updatedAt: '2023-12-29T15:13:53.895Z'
      numEdits: 0
      reactions: []
    id: 658ee23187944e494ef73c81
    type: comment
  author: johanteekens
  content: "Nothing special.\n\nHardware: 2 x RTX 3090 on a ASUS B650 creator proart\
    \ motherboard with AMD 7800, 32 GB mem.\nOS: Ubuntu 22.04, Driver Version: 535.129.03\
    \   CUDA Version: 12.2, Docker with CUDA support.\nMy standard development container:\
    \ https://github.com/johanteekens/ml-dev-docker  (This compiles llama.cpp from\
    \ github while building, please check Dockerfile.)\nllm = LlamaCPP(\n    model_url=None,\n\
    \    model_path=\"mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\",\n    temperature=0.1,\n\
    \    max_new_tokens=4800,\n    context_window=3900,\n    generate_kwargs={},\n\
    \    model_kwargs={\"n_gpu_layers\": 50},\n    messages_to_prompt=messages_to_prompt,\n\
    \    completion_to_prompt=completion_to_prompt,\n    verbose=True,\n)"
  created_at: 2023-12-29 15:13:53+00:00
  edited: false
  hidden: false
  id: 658ee23187944e494ef73c81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ac3916f91774ad899c859aed1f7f81d.svg
      fullname: Mu dengke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muyufan
      type: user
    createdAt: '2024-01-02T03:29:43.000Z'
    data:
      edited: false
      editors:
      - muyufan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9933910965919495
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ac3916f91774ad899c859aed1f7f81d.svg
          fullname: Mu dengke
          isHf: false
          isPro: false
          name: muyufan
          type: user
        html: '<p>hi, How many mem does it need?<br>30G maybe?</p>

          '
        raw: "hi, How many mem does it need? \n30G maybe?"
        updatedAt: '2024-01-02T03:29:43.207Z'
      numEdits: 0
      reactions: []
    id: 6593832754f88261734c7095
    type: comment
  author: muyufan
  content: "hi, How many mem does it need? \n30G maybe?"
  created_at: 2024-01-02 03:29:43+00:00
  edited: false
  hidden: false
  id: 6593832754f88261734c7095
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3007f156d61c9a59d22ce79d6eb18d5b.svg
      fullname: Johan Teekens
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johanteekens
      type: user
    createdAt: '2024-01-02T08:41:09.000Z'
    data:
      edited: false
      editors:
      - johanteekens
      hidden: false
      identifiedLanguage:
        language: da
        probability: 0.1794821172952652
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3007f156d61c9a59d22ce79d6eb18d5b.svg
          fullname: Johan Teekens
          isHf: false
          isPro: false
          name: johanteekens
          type: user
        html: '<p>32G</p>

          '
        raw: 32G
        updatedAt: '2024-01-02T08:41:09.152Z'
      numEdits: 0
      reactions: []
    id: 6593cc25ccbc1e2cc767b85b
    type: comment
  author: johanteekens
  content: 32G
  created_at: 2024-01-02 08:41:09+00:00
  edited: false
  hidden: false
  id: 6593cc25ccbc1e2cc767b85b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: I love the mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf
