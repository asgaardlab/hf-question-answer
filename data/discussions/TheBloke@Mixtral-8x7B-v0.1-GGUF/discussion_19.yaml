!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ShivanshMathur007
conflicting_files: null
created_at: 2024-01-10 06:40:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
      fullname: Shivansh Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivanshMathur007
      type: user
    createdAt: '2024-01-10T06:40:36.000Z'
    data:
      edited: true
      editors:
      - ShivanshMathur007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9496040344238281
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
          fullname: Shivansh Mathur
          isHf: false
          isPro: false
          name: ShivanshMathur007
          type: user
        html: '<p>What is the minimum hardware required and what is the recommended
          hardware for Q4_K_M with a decent speed. Any idea?</p>

          '
        raw: What is the minimum hardware required and what is the recommended hardware
          for Q4_K_M with a decent speed. Any idea?
        updatedAt: '2024-01-10T06:40:58.630Z'
      numEdits: 1
      reactions: []
    id: 659e3be495d836113a27bd2e
    type: comment
  author: ShivanshMathur007
  content: What is the minimum hardware required and what is the recommended hardware
    for Q4_K_M with a decent speed. Any idea?
  created_at: 2024-01-10 06:40:36+00:00
  edited: true
  hidden: false
  id: 659e3be495d836113a27bd2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-11T14:59:23.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9389941692352295
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ShivanshMathur007&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ShivanshMathur007\"\
          >@<span class=\"underline\">ShivanshMathur007</span></a></span>\n\n\t</span></span>\
          \ I believe it should fit barely in a 24gb vram gpu and you should get very\
          \ fast speeds. However you might get Out of memory error if you arent using\
          \ a headless machine and you wont really fit a lot of context.</p>\n<p>You\
          \ probably have to offload some layers to cpu and it should still be reasonable\
          \ speed like 10 -15 tokens per second </p>\n"
        raw: '@ShivanshMathur007 I believe it should fit barely in a 24gb vram gpu
          and you should get very fast speeds. However you might get Out of memory
          error if you arent using a headless machine and you wont really fit a lot
          of context.


          You probably have to offload some layers to cpu and it should still be reasonable
          speed like 10 -15 tokens per second '
        updatedAt: '2024-01-11T14:59:23.007Z'
      numEdits: 0
      reactions: []
    id: 65a0024b300957620b786878
    type: comment
  author: YaTharThShaRma999
  content: '@ShivanshMathur007 I believe it should fit barely in a 24gb vram gpu and
    you should get very fast speeds. However you might get Out of memory error if
    you arent using a headless machine and you wont really fit a lot of context.


    You probably have to offload some layers to cpu and it should still be reasonable
    speed like 10 -15 tokens per second '
  created_at: 2024-01-11 14:59:23+00:00
  edited: false
  hidden: false
  id: 65a0024b300957620b786878
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfcf85a149c8d7d3447bc2a96b86257a.svg
      fullname: Marcel Huenninghaus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rhylean
      type: user
    createdAt: '2024-01-24T09:29:04.000Z'
    data:
      edited: true
      editors:
      - Rhylean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9283210039138794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfcf85a149c8d7d3447bc2a96b86257a.svg
          fullname: Marcel Huenninghaus
          isHf: false
          isPro: false
          name: Rhylean
          type: user
        html: '<p>Until now I didn''t even manage to run mixtral-8x7b-v0.1.Q3_K_M.gguf
          on a 3090ti (24GB VRAM), only the mixtral-8x7b-v0.1.Q2_K.gguf works. Even
          with the smallest model, asking questions to a document (embedding_model:
          intfloat/multilingual-e5-large) is not feaseable, for the answers don''t
          refrence uploaded files and the model stalls and has to be "restarted".
          </p>

          <p>That said, this information is with a PrivateGPT frontend, perhaps someone
          knows a workaround.</p>

          '
        raw: "Until now I didn't even manage to run mixtral-8x7b-v0.1.Q3_K_M.gguf\
          \ on a 3090ti (24GB VRAM), only the mixtral-8x7b-v0.1.Q2_K.gguf works. Even\
          \ with the smallest model, asking questions to a document (embedding_model:\
          \ intfloat/multilingual-e5-large) is not feaseable, for the answers don't\
          \ refrence uploaded files and the model stalls and has to be \"restarted\"\
          . \n\nThat said, this information is with a PrivateGPT frontend, perhaps\
          \ someone knows a workaround."
        updatedAt: '2024-01-24T09:53:32.471Z'
      numEdits: 1
      reactions: []
    id: 65b0d86025c7e48fd07039ec
    type: comment
  author: Rhylean
  content: "Until now I didn't even manage to run mixtral-8x7b-v0.1.Q3_K_M.gguf on\
    \ a 3090ti (24GB VRAM), only the mixtral-8x7b-v0.1.Q2_K.gguf works. Even with\
    \ the smallest model, asking questions to a document (embedding_model: intfloat/multilingual-e5-large)\
    \ is not feaseable, for the answers don't refrence uploaded files and the model\
    \ stalls and has to be \"restarted\". \n\nThat said, this information is with\
    \ a PrivateGPT frontend, perhaps someone knows a workaround."
  created_at: 2024-01-24 09:29:04+00:00
  edited: true
  hidden: false
  id: 65b0d86025c7e48fd07039ec
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Hardware Requirements for Q4_K_M
