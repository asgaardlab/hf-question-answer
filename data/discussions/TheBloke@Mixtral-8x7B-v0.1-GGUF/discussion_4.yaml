!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Althenwolf
conflicting_files: null
created_at: 2023-12-11 17:04:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/06c6881d1f39bdac2c5d2cda7ff72168.svg
      fullname: Martin Perez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Althenwolf
      type: user
    createdAt: '2023-12-11T17:04:25.000Z'
    data:
      edited: false
      editors:
      - Althenwolf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6919564008712769
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/06c6881d1f39bdac2c5d2cda7ff72168.svg
          fullname: Martin Perez
          isHf: false
          isPro: false
          name: Althenwolf
          type: user
        html: '<p>Hi! </p>

          <p>I have the following error loading  the mixtral-8x7b-v0.1.Q8_0.gguf:</p>

          <p>llm_load_tensors: ggml ctx size =    0.32 MB<br>llm_load_tensors: using
          CUDA for GPU acceleration<br>ggml_cuda_set_main_device: using device 0 (NVIDIA
          GeForce RTX 3090) as main device<br>error loading model: create_tensor:
          tensor ''blk.0.ffn_gate.weight'' not found<br>llama_load_model_from_file:
          failed to load model<br>2023-12-11 16:55:48 ERROR:Failed to load the model.</p>

          <p>...</p>

          <p>  File "/env/lib/python3.10/site-packages/llama_cpp_cuda/llama.py", line
          365, in <strong>init</strong><br>    assert self.model is not None<br>AssertionError</p>

          <p>Any idea?<br>PD: TheBloke, many many thanks for your work and time!</p>

          '
        raw: "Hi! \r\n\r\nI have the following error loading  the mixtral-8x7b-v0.1.Q8_0.gguf:\r\
          \n\r\nllm_load_tensors: ggml ctx size =    0.32 MB\r\nllm_load_tensors:\
          \ using CUDA for GPU acceleration\r\nggml_cuda_set_main_device: using device\
          \ 0 (NVIDIA GeForce RTX 3090) as main device\r\nerror loading model: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\r\nllama_load_model_from_file:\
          \ failed to load model\r\n2023-12-11 16:55:48 ERROR:Failed to load the model.\r\
          \n\r\n...\r\n\r\n  File \"/env/lib/python3.10/site-packages/llama_cpp_cuda/llama.py\"\
          , line 365, in __init__\r\n    assert self.model is not None\r\nAssertionError\r\
          \n\r\nAny idea?\r\nPD: TheBloke, many many thanks for your work and time!\r\
          \n"
        updatedAt: '2023-12-11T17:04:25.450Z'
      numEdits: 0
      reactions: []
    id: 65774119e1e7233660bab364
    type: comment
  author: Althenwolf
  content: "Hi! \r\n\r\nI have the following error loading  the mixtral-8x7b-v0.1.Q8_0.gguf:\r\
    \n\r\nllm_load_tensors: ggml ctx size =    0.32 MB\r\nllm_load_tensors: using\
    \ CUDA for GPU acceleration\r\nggml_cuda_set_main_device: using device 0 (NVIDIA\
    \ GeForce RTX 3090) as main device\r\nerror loading model: create_tensor: tensor\
    \ 'blk.0.ffn_gate.weight' not found\r\nllama_load_model_from_file: failed to load\
    \ model\r\n2023-12-11 16:55:48 ERROR:Failed to load the model.\r\n\r\n...\r\n\r\
    \n  File \"/env/lib/python3.10/site-packages/llama_cpp_cuda/llama.py\", line 365,\
    \ in __init__\r\n    assert self.model is not None\r\nAssertionError\r\n\r\nAny\
    \ idea?\r\nPD: TheBloke, many many thanks for your work and time!\r\n"
  created_at: 2023-12-11 17:04:25+00:00
  edited: false
  hidden: false
  id: 65774119e1e7233660bab364
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c27ad992f446fb9027e0d8132151515.svg
      fullname: user zyzz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: userzyzz
      type: user
    createdAt: '2023-12-11T17:09:01.000Z'
    data:
      edited: false
      editors:
      - userzyzz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7274000644683838
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c27ad992f446fb9027e0d8132151515.svg
          fullname: user zyzz
          isHf: false
          isPro: false
          name: userzyzz
          type: user
        html: '<p>You need to use the llama.cpp fork with mixtral support <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/tree/mixtral">https://github.com/ggerganov/llama.cpp/tree/mixtral</a></p>

          '
        raw: You need to use the llama.cpp fork with mixtral support https://github.com/ggerganov/llama.cpp/tree/mixtral
        updatedAt: '2023-12-11T17:09:01.311Z'
      numEdits: 0
      reactions:
      - count: 8
        reaction: "\U0001F44D"
        users:
        - TanvirOnHF
        - Althenwolf
        - mirek190
        - hammad93
        - pmdy123
        - tli0312
        - dmitryhd
        - phils666
    id: 6577422d8869730b22787340
    type: comment
  author: userzyzz
  content: You need to use the llama.cpp fork with mixtral support https://github.com/ggerganov/llama.cpp/tree/mixtral
  created_at: 2023-12-11 17:09:01+00:00
  edited: false
  hidden: false
  id: 6577422d8869730b22787340
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-12-12T02:14:28.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8130869269371033
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Is there not a Windows 10 binary compiled for this??? </p>

          '
        raw: 'Is there not a Windows 10 binary compiled for this??? '
        updatedAt: '2023-12-12T02:14:28.179Z'
      numEdits: 0
      reactions: []
    id: 6577c204a87010c9f86e2d8b
    type: comment
  author: jdc4429
  content: 'Is there not a Windows 10 binary compiled for this??? '
  created_at: 2023-12-12 02:14:28+00:00
  edited: false
  hidden: false
  id: 6577c204a87010c9f86e2d8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c759df87ef275a0b93469378f197f05c.svg
      fullname: Vijay D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dvijay
      type: user
    createdAt: '2023-12-12T10:55:50.000Z'
    data:
      edited: false
      editors:
      - dvijay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.713512659072876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c759df87ef275a0b93469378f197f05c.svg
          fullname: Vijay D
          isHf: false
          isPro: false
          name: dvijay
          type: user
        html: '<p>Hi, I''m using the right branch (latest pull from mixtral) but still
          getting the same error:</p>

          <p>llm_load_tensors: ggml ctx size =    0.36 MiB<br>llm_load_tensors: using
          CUDA for GPU acceleration<br>error loading model: create_tensor: tensor
          ''blk.0.ffn_gate.weight'' not found<br>llama_load_model_from_file: failed
          to load model<br>llama_init_from_gpt_params: error: failed to load model
          ''/mnt/e/mixtral-8x7b-v0.1.Q4_K_M.gguf''<br>main: error: unable to load
          model</p>

          <p>In the list of layers produced by the llama _model_loader from running
          main, I don''t see this tensor. I only see tensors like blk.0.ffn_gate.0.weight<br>Am
          I missing anything here?</p>

          '
        raw: 'Hi, I''m using the right branch (latest pull from mixtral) but still
          getting the same error:


          llm_load_tensors: ggml ctx size =    0.36 MiB

          llm_load_tensors: using CUDA for GPU acceleration

          error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight'' not
          found

          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''/mnt/e/mixtral-8x7b-v0.1.Q4_K_M.gguf''

          main: error: unable to load model


          In the list of layers produced by the llama _model_loader from running main,
          I don''t see this tensor. I only see tensors like blk.0.ffn_gate.0.weight

          Am I missing anything here?'
        updatedAt: '2023-12-12T10:55:50.824Z'
      numEdits: 0
      reactions: []
    id: 65783c366db22fc06ca47db8
    type: comment
  author: dvijay
  content: 'Hi, I''m using the right branch (latest pull from mixtral) but still getting
    the same error:


    llm_load_tensors: ggml ctx size =    0.36 MiB

    llm_load_tensors: using CUDA for GPU acceleration

    error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight'' not found

    llama_load_model_from_file: failed to load model

    llama_init_from_gpt_params: error: failed to load model ''/mnt/e/mixtral-8x7b-v0.1.Q4_K_M.gguf''

    main: error: unable to load model


    In the list of layers produced by the llama _model_loader from running main, I
    don''t see this tensor. I only see tensors like blk.0.ffn_gate.0.weight

    Am I missing anything here?'
  created_at: 2023-12-12 10:55:50+00:00
  edited: false
  hidden: false
  id: 65783c366db22fc06ca47db8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a02e519016c8ab8aa619da06c783dea.svg
      fullname: John Puddefoot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pudepiedj
      type: user
    createdAt: '2023-12-12T14:03:59.000Z'
    data:
      edited: false
      editors:
      - pudepiedj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9784949421882629
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a02e519016c8ab8aa619da06c783dea.svg
          fullname: John Puddefoot
          isHf: false
          isPro: false
          name: pudepiedj
          type: user
        html: '<p>I had the same issue on Apple Metal M2 MAX and it was solved by
          pulling the /mixtral branch instead of the master branch from llama.cpp
          then remaking. But you''re right that the supposedly missing tensor doesn''t
          appear in the list of created tensors even when it works!</p>

          '
        raw: I had the same issue on Apple Metal M2 MAX and it was solved by pulling
          the /mixtral branch instead of the master branch from llama.cpp then remaking.
          But you're right that the supposedly missing tensor doesn't appear in the
          list of created tensors even when it works!
        updatedAt: '2023-12-12T14:03:59.160Z'
      numEdits: 0
      reactions: []
    id: 6578684f1bf8e371c4dcd921
    type: comment
  author: pudepiedj
  content: I had the same issue on Apple Metal M2 MAX and it was solved by pulling
    the /mixtral branch instead of the master branch from llama.cpp then remaking.
    But you're right that the supposedly missing tensor doesn't appear in the list
    of created tensors even when it works!
  created_at: 2023-12-12 14:03:59+00:00
  edited: false
  hidden: false
  id: 6578684f1bf8e371c4dcd921
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c759df87ef275a0b93469378f197f05c.svg
      fullname: Vijay D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dvijay
      type: user
    createdAt: '2023-12-12T19:35:02.000Z'
    data:
      edited: true
      editors:
      - dvijay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7799973487854004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c759df87ef275a0b93469378f197f05c.svg
          fullname: Vijay D
          isHf: false
          isPro: false
          name: dvijay
          type: user
        html: '<p>I tried a clean build multiple times but still no luck. Should the
          mixtral branch work as is or are there any additional changes or patches
          that are required? Any help is greatly appreciated.</p>

          <p>llm_load_tensors: ggml ctx size =    0.36 MiB<br>llm_load_tensors: using
          CUDA for GPU acceleration<br>error loading model: create_tensor: tensor
          ''blk.0.ffn_gate.weight'' not found<br>llama_load_model_from_file: failed
          to load model<br>llama_init_from_gpt_params: error: failed to load model
          ''/mnt/models/mixtral-8x7b-v0.1.Q4_K_M.gguf''<br>main: error: unable to
          load model<br>.../llama.cpp/build$ git status<br>On branch mixtral<br>nothing
          to commit, working tree clean</p>

          '
        raw: 'I tried a clean build multiple times but still no luck. Should the mixtral
          branch work as is or are there any additional changes or patches that are
          required? Any help is greatly appreciated.


          llm_load_tensors: ggml ctx size =    0.36 MiB

          llm_load_tensors: using CUDA for GPU acceleration

          error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight'' not
          found

          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''/mnt/models/mixtral-8x7b-v0.1.Q4_K_M.gguf''

          main: error: unable to load model

          .../llama.cpp/build$ git status

          On branch mixtral

          nothing to commit, working tree clean'
        updatedAt: '2023-12-12T19:41:03.968Z'
      numEdits: 1
      reactions: []
    id: 6578b5e6a87010c9f89946ad
    type: comment
  author: dvijay
  content: 'I tried a clean build multiple times but still no luck. Should the mixtral
    branch work as is or are there any additional changes or patches that are required?
    Any help is greatly appreciated.


    llm_load_tensors: ggml ctx size =    0.36 MiB

    llm_load_tensors: using CUDA for GPU acceleration

    error loading model: create_tensor: tensor ''blk.0.ffn_gate.weight'' not found

    llama_load_model_from_file: failed to load model

    llama_init_from_gpt_params: error: failed to load model ''/mnt/models/mixtral-8x7b-v0.1.Q4_K_M.gguf''

    main: error: unable to load model

    .../llama.cpp/build$ git status

    On branch mixtral

    nothing to commit, working tree clean'
  created_at: 2023-12-12 19:35:02+00:00
  edited: true
  hidden: false
  id: 6578b5e6a87010c9f89946ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c759df87ef275a0b93469378f197f05c.svg
      fullname: Vijay D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dvijay
      type: user
    createdAt: '2023-12-13T05:32:37.000Z'
    data:
      edited: false
      editors:
      - dvijay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9862217903137207
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c759df87ef275a0b93469378f197f05c.svg
          fullname: Vijay D
          isHf: false
          isPro: false
          name: dvijay
          type: user
        html: '<p>Final update: I got it working eventually. For some reason building
          from the branch wasn''t working originally but after a few tries it works
          and I can load the models correctly.</p>

          '
        raw: 'Final update: I got it working eventually. For some reason building
          from the branch wasn''t working originally but after a few tries it works
          and I can load the models correctly.'
        updatedAt: '2023-12-13T05:32:37.922Z'
      numEdits: 0
      reactions: []
    id: 657941f590df9d85fb8d589d
    type: comment
  author: dvijay
  content: 'Final update: I got it working eventually. For some reason building from
    the branch wasn''t working originally but after a few tries it works and I can
    load the models correctly.'
  created_at: 2023-12-13 05:32:37+00:00
  edited: false
  hidden: false
  id: 657941f590df9d85fb8d589d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7976fa0738382bbaa248984973abdc75.svg
      fullname: t hadoop
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thadoop
      type: user
    createdAt: '2023-12-13T08:01:12.000Z'
    data:
      edited: false
      editors:
      - thadoop
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7771902680397034
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7976fa0738382bbaa248984973abdc75.svg
          fullname: t hadoop
          isHf: false
          isPro: false
          name: thadoop
          type: user
        html: '<blockquote>

          <p>You need to use the llama.cpp fork with mixtral support <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/tree/mixtral">https://github.com/ggerganov/llama.cpp/tree/mixtral</a></p>

          </blockquote>

          <p>Can you please explain more, how do we do it ?</p>

          '
        raw: '> You need to use the llama.cpp fork with mixtral support https://github.com/ggerganov/llama.cpp/tree/mixtral


          Can you please explain more, how do we do it ?'
        updatedAt: '2023-12-13T08:01:12.589Z'
      numEdits: 0
      reactions: []
    id: 657964c8a87010c9f8b5b135
    type: comment
  author: thadoop
  content: '> You need to use the llama.cpp fork with mixtral support https://github.com/ggerganov/llama.cpp/tree/mixtral


    Can you please explain more, how do we do it ?'
  created_at: 2023-12-13 08:01:12+00:00
  edited: false
  hidden: false
  id: 657964c8a87010c9f8b5b135
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42826154670c7772495c328affe6435d.svg
      fullname: 'Nicolas Goiginger '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nicoolodion
      type: user
    createdAt: '2023-12-13T18:19:38.000Z'
    data:
      edited: false
      editors:
      - Nicoolodion
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9922924041748047
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42826154670c7772495c328affe6435d.svg
          fullname: 'Nicolas Goiginger '
          isHf: false
          isPro: false
          name: Nicoolodion
          type: user
        html: '<p>How do I make it use the newly downloaded llama.cpp (I also think
          we don''t need to use the branch for that anymore since it was merged I
          believe) Where do I install it?</p>

          '
        raw: How do I make it use the newly downloaded llama.cpp (I also think we
          don't need to use the branch for that anymore since it was merged I believe)
          Where do I install it?
        updatedAt: '2023-12-13T18:19:38.814Z'
      numEdits: 0
      reactions: []
    id: 6579f5bab90b4221ec07654a
    type: comment
  author: Nicoolodion
  content: How do I make it use the newly downloaded llama.cpp (I also think we don't
    need to use the branch for that anymore since it was merged I believe) Where do
    I install it?
  created_at: 2023-12-13 18:19:38+00:00
  edited: false
  hidden: false
  id: 6579f5bab90b4221ec07654a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eca3da9f0076c67e4faa67d1dd589402.svg
      fullname: Nick B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nbaughman
      type: user
    createdAt: '2023-12-17T04:57:43.000Z'
    data:
      edited: false
      editors:
      - nbaughman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6556847095489502
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eca3da9f0076c67e4faa67d1dd589402.svg
          fullname: Nick B
          isHf: false
          isPro: false
          name: nbaughman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;thadoop&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/thadoop\">@<span class=\"\
          underline\">thadoop</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;Nicoolodion&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Nicoolodion\">@<span class=\"underline\"\
          >Nicoolodion</span></a></span>\n\n\t</span></span></p>\n<ul>\n<li>Clone\
          \ the repo as per normal: <code>git clone https://github.com/ggerganov/llama.cpp</code></li>\n\
          <li>Before you build, run <code>git checkout mixtral</code></li>\n<li>Build\
          \ as per normal</li>\n</ul>\n"
        raw: '@thadoop @Nicoolodion


          - Clone the repo as per normal: `git clone https://github.com/ggerganov/llama.cpp`

          - Before you build, run `git checkout mixtral`

          - Build as per normal'
        updatedAt: '2023-12-17T04:57:43.307Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sergey1252
    id: 657e7fc7e1116d68e9a7acdc
    type: comment
  author: nbaughman
  content: '@thadoop @Nicoolodion


    - Clone the repo as per normal: `git clone https://github.com/ggerganov/llama.cpp`

    - Before you build, run `git checkout mixtral`

    - Build as per normal'
  created_at: 2023-12-17 04:57:43+00:00
  edited: false
  hidden: false
  id: 657e7fc7e1116d68e9a7acdc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: 'create_tensor: tensor ''blk.0.ffn_gate.weight'' not found'
