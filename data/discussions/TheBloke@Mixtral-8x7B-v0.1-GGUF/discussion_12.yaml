!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BigDeeper
conflicting_files: null
created_at: 2023-12-15 17:24:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ace4395d175d9c1d3d11f191c3709b0.svg
      fullname: Big Deeper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BigDeeper
      type: user
    createdAt: '2023-12-15T17:24:11.000Z'
    data:
      edited: false
      editors:
      - BigDeeper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2785288095474243
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ace4395d175d9c1d3d11f191c3709b0.svg
          fullname: Big Deeper
          isHf: false
          isPro: false
          name: BigDeeper
          type: user
        html: '<p>llm_load_tensors: ggml ctx size =    0.38 MiB<br>llm_load_tensors:
          using CUDA for GPU acceleration<br>llm_load_tensors: mem required  =  102.92
          MiB<br>llm_load_tensors: offloading 32 repeating layers to GPU<br>llm_load_tensors:
          offloading non-repeating layers to GPU<br>llm_load_tensors: offloaded 33/33
          layers to GPU<br>llm_load_tensors: VRAM used: 36497.55 MiB<br>....................................................................................................<br>llama_new_context_with_model:
          n_ctx      = 2048<br>llama_new_context_with_model: freq_base  = 1000000.0<br>llama_new_context_with_model:
          freq_scale = 1<br>llama_kv_cache_init: VRAM kv self = 256.00 MB<br>llama_new_context_with_model:
          KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB<br>llama_build_graph:
          non-view tensors processed: 1124/1124<br>llama_new_context_with_model: compute
          buffer total size = 187.22 MiB<br>llama_new_context_with_model: VRAM scratch
          buffer: 184.04 MiB<br>llama_new_context_with_model: total VRAM used: 36937.59
          MiB (model: 36497.55 MiB, context: 440.04 MiB)</p>

          <p>system_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 0 | AVX512 = 0 |
          AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C
          = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX
          = 0 |<br>sampling:<br>        repeat_last_n = 64, repeat_penalty = 1.100,
          frequency_penalty = 0.000, presence_penalty = 0.000<br>        top_k = 40,
          tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.700<br>        mirostat
          = 0, mirostat_lr = 0.100, mirostat_ent = 5.000<br>sampling order:<br>CFG
          -&gt; Penalties -&gt; top_k -&gt; tfs_z -&gt; typical_p -&gt; top_p -&gt;
          min_p -&gt; temp<br>generate: n_ctx = 2048, n_batch = 512, n_predict = -1,
          n_keep = 0</p>

          <h1 id="write-a-poem-about-fibonacci-sequence">Write a poem about Fibonacci
          sequence.</h1>

          <p>####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
          Death,<br> ,<br> gepubliceerdaders myself hope<br> Mack.... tw_{<br> [end
          of text]</p>

          <p>llama_print_timings:        load time =   16705.83 ms<br>llama_print_timings:      sample
          time =     194.93 ms /   521 runs   (    0.37 ms per token,  2672.78 tokens
          per second)<br>llama_print_timings: prompt eval time =     516.70 ms /    12
          tokens (   43.06 ms per token,    23.22 tokens per second)<br>llama_print_timings:        eval
          time =   30994.44 ms /   520 runs   (   59.60 ms per token,    16.78 tokens
          per second)<br>llama_print_timings:       total time =   32847.43 ms<br>Log
          end</p>

          '
        raw: "llm_load_tensors: ggml ctx size =    0.38 MiB\r\nllm_load_tensors: using\
          \ CUDA for GPU acceleration\r\nllm_load_tensors: mem required  =  102.92\
          \ MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors:\
          \ offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded\
          \ 33/33 layers to GPU\r\nllm_load_tensors: VRAM used: 36497.55 MiB\r\n....................................................................................................\r\
          \nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model:\
          \ freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\
          \nllama_kv_cache_init: VRAM kv self = 256.00 MB\r\nllama_new_context_with_model:\
          \ KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\
          \nllama_build_graph: non-view tensors processed: 1124/1124\r\nllama_new_context_with_model:\
          \ compute buffer total size = 187.22 MiB\r\nllama_new_context_with_model:\
          \ VRAM scratch buffer: 184.04 MiB\r\nllama_new_context_with_model: total\
          \ VRAM used: 36937.59 MiB (model: 36497.55 MiB, context: 440.04 MiB)\r\n\
          \r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 0 | AVX512 = 0 |\
          \ AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 |\
          \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 =\
          \ 1 | VSX = 0 |\r\nsampling:\r\n        repeat_last_n = 64, repeat_penalty\
          \ = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\r\n     \
          \   top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p =\
          \ 1.000, temp = 0.700\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
          \ = 5.000\r\nsampling order:\r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p\
          \ -> top_p -> min_p -> temp\r\ngenerate: n_ctx = 2048, n_batch = 512, n_predict\
          \ = -1, n_keep = 0\r\n\r\n\r\n # Write a poem about Fibonacci sequence.\r\
          \n####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\
          \ Death,\r\n ,\r\n gepubliceerdaders myself hope\r\n Mack.... tw_{\r\n [end\
          \ of text]\r\n\r\nllama_print_timings:        load time =   16705.83 ms\r\
          \nllama_print_timings:      sample time =     194.93 ms /   521 runs   (\
          \    0.37 ms per token,  2672.78 tokens per second)\r\nllama_print_timings:\
          \ prompt eval time =     516.70 ms /    12 tokens (   43.06 ms per token,\
          \    23.22 tokens per second)\r\nllama_print_timings:        eval time =\
          \   30994.44 ms /   520 runs   (   59.60 ms per token,    16.78 tokens per\
          \ second)\r\nllama_print_timings:       total time =   32847.43 ms\r\nLog\
          \ end\r\n"
        updatedAt: '2023-12-15T17:24:11.728Z'
      numEdits: 0
      reactions: []
    id: 657c8bbbe37618d867512353
    type: comment
  author: BigDeeper
  content: "llm_load_tensors: ggml ctx size =    0.38 MiB\r\nllm_load_tensors: using\
    \ CUDA for GPU acceleration\r\nllm_load_tensors: mem required  =  102.92 MiB\r\
    \nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors:\
    \ offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33\
    \ layers to GPU\r\nllm_load_tensors: VRAM used: 36497.55 MiB\r\n....................................................................................................\r\
    \nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model:\
    \ freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:\
    \ VRAM kv self = 256.00 MB\r\nllama_new_context_with_model: KV self size  =  256.00\
    \ MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_build_graph: non-view\
    \ tensors processed: 1124/1124\r\nllama_new_context_with_model: compute buffer\
    \ total size = 187.22 MiB\r\nllama_new_context_with_model: VRAM scratch buffer:\
    \ 184.04 MiB\r\nllama_new_context_with_model: total VRAM used: 36937.59 MiB (model:\
    \ 36497.55 MiB, context: 440.04 MiB)\r\n\r\nsystem_info: n_threads = 16 / 32 |\
    \ AVX = 1 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA =\
    \ 0 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS =\
    \ 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\r\nsampling:\r\n        repeat_last_n =\
    \ 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\r\
    \n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p =\
    \ 1.000, temp = 0.700\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
    \ = 5.000\r\nsampling order:\r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p\
    \ -> top_p -> min_p -> temp\r\ngenerate: n_ctx = 2048, n_batch = 512, n_predict\
    \ = -1, n_keep = 0\r\n\r\n\r\n # Write a poem about Fibonacci sequence.\r\n####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\
    \ Death,\r\n ,\r\n gepubliceerdaders myself hope\r\n Mack.... tw_{\r\n [end of\
    \ text]\r\n\r\nllama_print_timings:        load time =   16705.83 ms\r\nllama_print_timings:\
    \      sample time =     194.93 ms /   521 runs   (    0.37 ms per token,  2672.78\
    \ tokens per second)\r\nllama_print_timings: prompt eval time =     516.70 ms\
    \ /    12 tokens (   43.06 ms per token,    23.22 tokens per second)\r\nllama_print_timings:\
    \        eval time =   30994.44 ms /   520 runs   (   59.60 ms per token,    16.78\
    \ tokens per second)\r\nllama_print_timings:       total time =   32847.43 ms\r\
    \nLog end\r\n"
  created_at: 2023-12-15 17:24:11+00:00
  edited: false
  hidden: false
  id: 657c8bbbe37618d867512353
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/8ace4395d175d9c1d3d11f191c3709b0.svg
      fullname: Big Deeper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BigDeeper
      type: user
    createdAt: '2023-12-15T17:25:25.000Z'
    data:
      from: Anyone else seeing similar behavior? I especially the start "Death, ..."
        plus some gobblygook.
      to: Anyone else seeing similar behavior? I especially like  the start "Death,
        ..." plus some gobblygook.
    id: 657c8c05c7cb69069a2f2adb
    type: title-change
  author: BigDeeper
  created_at: 2023-12-15 17:25:25+00:00
  id: 657c8c05c7cb69069a2f2adb
  new_title: Anyone else seeing similar behavior? I especially like  the start "Death,
    ..." plus some gobblygook.
  old_title: Anyone else seeing similar behavior? I especially the start "Death, ..."
    plus some gobblygook.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-15T18:20:03.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8614104390144348
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;BigDeeper&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/BigDeeper\">@<span class=\"\
          underline\">BigDeeper</span></a></span>\n\n\t</span></span> that is pretty\
          \ wierd but first of all you seem like you want a instruction/chatting model.<br>The\
          \ instruction/chatting version of mixtral is this one<br><a href=\"https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\"\
          >https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF</a></p>\n"
        raw: '@BigDeeper that is pretty wierd but first of all you seem like you want
          a instruction/chatting model.

          The instruction/chatting version of mixtral is this one

          https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF'
        updatedAt: '2023-12-15T18:20:03.205Z'
      numEdits: 0
      reactions: []
    id: 657c98d31e20870324ef15d3
    type: comment
  author: YaTharThShaRma999
  content: '@BigDeeper that is pretty wierd but first of all you seem like you want
    a instruction/chatting model.

    The instruction/chatting version of mixtral is this one

    https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF'
  created_at: 2023-12-15 18:20:03+00:00
  edited: false
  hidden: false
  id: 657c98d31e20870324ef15d3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Anyone else seeing similar behavior? I especially like  the start "Death, ..."
  plus some gobblygook.
