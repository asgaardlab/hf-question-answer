!!python/object:huggingface_hub.community.DiscussionWithDetails
author: justsumguy
conflicting_files: null
created_at: 2023-05-06 07:22:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-05-06T08:22:39.000Z'
    data:
      edited: false
      editors:
      - justsumguy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
          fullname: Jared H
          isHf: false
          isPro: false
          name: justsumguy
          type: user
        html: '<p>This did not run with the version of GPTQ that comes with Oobabooga
          and I had to clone and install the git for GPTQ. Letting the installer re-download
          and reinstall Oobabooga''s fork of GPTQ didn''t work, and I tried both current
          and old CUDA branch and it was about 1/3 of my typical 7B speeds. Trying
          to load it in Oobabooga will cause it to crash, have a pre-layer error,
          or give a CPU OOM error depending on the conditions.</p>

          <p>I''m on a potato and normally get 0.49-0.59 tokens/s on Wizard but with
          this version, I was getting 0.21 tokens/s, so after confirming that the
          cache was active, I assume it''s GPTQ. With the updated GPTQ, I couldn''t
          run any other models I use so couldn''t compare and be sure whether it was
          the model, but thought it was worth mentioning since the main branch is
          supposed to be higher-compatibility.</p>

          '
        raw: "This did not run with the version of GPTQ that comes with Oobabooga\
          \ and I had to clone and install the git for GPTQ. Letting the installer\
          \ re-download and reinstall Oobabooga's fork of GPTQ didn't work, and I\
          \ tried both current and old CUDA branch and it was about 1/3 of my typical\
          \ 7B speeds. Trying to load it in Oobabooga will cause it to crash, have\
          \ a pre-layer error, or give a CPU OOM error depending on the conditions.\r\
          \n\r\nI'm on a potato and normally get 0.49-0.59 tokens/s on Wizard but\
          \ with this version, I was getting 0.21 tokens/s, so after confirming that\
          \ the cache was active, I assume it's GPTQ. With the updated GPTQ, I couldn't\
          \ run any other models I use so couldn't compare and be sure whether it\
          \ was the model, but thought it was worth mentioning since the main branch\
          \ is supposed to be higher-compatibility."
        updatedAt: '2023-05-06T08:22:39.584Z'
      numEdits: 0
      reactions: []
    id: 64560e4fbfdf9c63ce2d99ef
    type: comment
  author: justsumguy
  content: "This did not run with the version of GPTQ that comes with Oobabooga and\
    \ I had to clone and install the git for GPTQ. Letting the installer re-download\
    \ and reinstall Oobabooga's fork of GPTQ didn't work, and I tried both current\
    \ and old CUDA branch and it was about 1/3 of my typical 7B speeds. Trying to\
    \ load it in Oobabooga will cause it to crash, have a pre-layer error, or give\
    \ a CPU OOM error depending on the conditions.\r\n\r\nI'm on a potato and normally\
    \ get 0.49-0.59 tokens/s on Wizard but with this version, I was getting 0.21 tokens/s,\
    \ so after confirming that the cache was active, I assume it's GPTQ. With the\
    \ updated GPTQ, I couldn't run any other models I use so couldn't compare and\
    \ be sure whether it was the model, but thought it was worth mentioning since\
    \ the main branch is supposed to be higher-compatibility."
  created_at: 2023-05-06 07:22:39+00:00
  edited: false
  hidden: false
  id: 64560e4fbfdf9c63ce2d99ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-06T13:20:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry to hear you''re having problems.</p>

          <p>I did test it with ooba''s CUDA and it worked fine for me. But I didn''t
          test pre_layer, and yes I see that that doesn''t work. And it sounds like
          you don''t have enough VRAM to load it without pre_layer.</p>

          <p>I will make another one with ooba''s CUDA and test that and if it''s
          good I''ll upload that as well. I''ll get back to you in an hour or two.</p>

          '
        raw: 'Sorry to hear you''re having problems.


          I did test it with ooba''s CUDA and it worked fine for me. But I didn''t
          test pre_layer, and yes I see that that doesn''t work. And it sounds like
          you don''t have enough VRAM to load it without pre_layer.


          I will make another one with ooba''s CUDA and test that and if it''s good
          I''ll upload that as well. I''ll get back to you in an hour or two.'
        updatedAt: '2023-05-06T13:21:46.292Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ChrisDeng
        - mirek190
    id: 6456543bd10badc9555aa20d
    type: comment
  author: TheBloke
  content: 'Sorry to hear you''re having problems.


    I did test it with ooba''s CUDA and it worked fine for me. But I didn''t test
    pre_layer, and yes I see that that doesn''t work. And it sounds like you don''t
    have enough VRAM to load it without pre_layer.


    I will make another one with ooba''s CUDA and test that and if it''s good I''ll
    upload that as well. I''ll get back to you in an hour or two.'
  created_at: 2023-05-06 12:20:59+00:00
  edited: true
  hidden: false
  id: 6456543bd10badc9555aa20d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-06T14:11:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I''ve made another model, this time using ooba''s CUDA. I''ve
          tested it and confirmed it works with pre_layer.</p>

          <p>It''s in separate branch <code>oobaCUDA</code>: <a href="https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ/tree/oobaCUDA">https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ/tree/oobaCUDA</a></p>

          <p>Please test and let me know it works OK for you.</p>

          '
        raw: 'OK I''ve made another model, this time using ooba''s CUDA. I''ve tested
          it and confirmed it works with pre_layer.


          It''s in separate branch `oobaCUDA`: https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ/tree/oobaCUDA


          Please test and let me know it works OK for you.'
        updatedAt: '2023-05-06T14:11:46.616Z'
      numEdits: 0
      reactions: []
    id: 64566022cd6567f52fb51b72
    type: comment
  author: TheBloke
  content: 'OK I''ve made another model, this time using ooba''s CUDA. I''ve tested
    it and confirmed it works with pre_layer.


    It''s in separate branch `oobaCUDA`: https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ/tree/oobaCUDA


    Please test and let me know it works OK for you.'
  created_at: 2023-05-06 13:11:46+00:00
  edited: false
  hidden: false
  id: 64566022cd6567f52fb51b72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-05-06T22:28:56.000Z'
    data:
      edited: false
      editors:
      - justsumguy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
          fullname: Jared H
          isHf: false
          isPro: false
          name: justsumguy
          type: user
        html: '<p>I unfortunately can''t test if it fixed the issue now thanks to
          oobabooga breaking on me. Something with their installer is utterly broken,
          as trying to revert my GPTQ-for-LLaMa started giving me new issues that
          only got worse when trying to update my install, and a fresh install doesn''t
          let me load any GPTQ models now. I created a bug report already, but don''t
          know if it''ll be fixed in a day or a week. I tried downgrading many packages
          but it just isn''t fixing it. So no more LLMs for me until they fix it,
          unless I want to do an OS migration and try on Linux (which had other issues
          with my device last time I tried, otherwise it''d be my default)</p>

          '
        raw: I unfortunately can't test if it fixed the issue now thanks to oobabooga
          breaking on me. Something with their installer is utterly broken, as trying
          to revert my GPTQ-for-LLaMa started giving me new issues that only got worse
          when trying to update my install, and a fresh install doesn't let me load
          any GPTQ models now. I created a bug report already, but don't know if it'll
          be fixed in a day or a week. I tried downgrading many packages but it just
          isn't fixing it. So no more LLMs for me until they fix it, unless I want
          to do an OS migration and try on Linux (which had other issues with my device
          last time I tried, otherwise it'd be my default)
        updatedAt: '2023-05-06T22:28:56.365Z'
      numEdits: 0
      reactions: []
    id: 6456d4a8cd6567f52fbbfab7
    type: comment
  author: justsumguy
  content: I unfortunately can't test if it fixed the issue now thanks to oobabooga
    breaking on me. Something with their installer is utterly broken, as trying to
    revert my GPTQ-for-LLaMa started giving me new issues that only got worse when
    trying to update my install, and a fresh install doesn't let me load any GPTQ
    models now. I created a bug report already, but don't know if it'll be fixed in
    a day or a week. I tried downgrading many packages but it just isn't fixing it.
    So no more LLMs for me until they fix it, unless I want to do an OS migration
    and try on Linux (which had other issues with my device last time I tried, otherwise
    it'd be my default)
  created_at: 2023-05-06 21:28:56+00:00
  edited: false
  hidden: false
  id: 6456d4a8cd6567f52fbbfab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-05-07T04:48:31.000Z'
    data:
      edited: false
      editors:
      - justsumguy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
          fullname: Jared H
          isHf: false
          isPro: false
          name: justsumguy
          type: user
        html: '<p>I fixed the problems I had. It''s working at same perf as Wizard
          standard for me. Thank you for uploading that!</p>

          '
        raw: I fixed the problems I had. It's working at same perf as Wizard standard
          for me. Thank you for uploading that!
        updatedAt: '2023-05-07T04:48:31.852Z'
      numEdits: 0
      reactions: []
    id: 64572d9f03625871eb8082b0
    type: comment
  author: justsumguy
  content: I fixed the problems I had. It's working at same perf as Wizard standard
    for me. Thank you for uploading that!
  created_at: 2023-05-07 03:48:31+00:00
  edited: false
  hidden: false
  id: 64572d9f03625871eb8082b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-07T06:45:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re welcome.  </p>

          <p>But are you saying that <a href="https://huggingface.co/TheBloke/wizardLM-7B-GPTQ">https://huggingface.co/TheBloke/wizardLM-7B-GPTQ</a>
          doesn''t have these problems for you?  Because I made that the same way
          as I made the original files in this repo, so that''d be confusing</p>

          '
        raw: "You're welcome.  \n\nBut are you saying that https://huggingface.co/TheBloke/wizardLM-7B-GPTQ\
          \ doesn't have these problems for you?  Because I made that the same way\
          \ as I made the original files in this repo, so that'd be confusing"
        updatedAt: '2023-05-07T06:45:21.464Z'
      numEdits: 0
      reactions: []
    id: 645749014553675927603aa4
    type: comment
  author: TheBloke
  content: "You're welcome.  \n\nBut are you saying that https://huggingface.co/TheBloke/wizardLM-7B-GPTQ\
    \ doesn't have these problems for you?  Because I made that the same way as I\
    \ made the original files in this repo, so that'd be confusing"
  created_at: 2023-05-07 05:45:21+00:00
  edited: false
  hidden: false
  id: 645749014553675927603aa4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-05-07T10:45:23.000Z'
    data:
      edited: false
      editors:
      - justsumguy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
          fullname: Jared H
          isHf: false
          isPro: false
          name: justsumguy
          type: user
        html: '<p>Yeah, I didn''t have any problem with standard wizardLM, so a bit
          odd this one had the problem but the other one did not. Couldn''t say why,
          I''m positive I downloaded "compat.the -no-act-orders" file.</p>

          '
        raw: Yeah, I didn't have any problem with standard wizardLM, so a bit odd
          this one had the problem but the other one did not. Couldn't say why, I'm
          positive I downloaded "compat.the -no-act-orders" file.
        updatedAt: '2023-05-07T10:45:23.341Z'
      numEdits: 0
      reactions: []
    id: 64578143711ee86f6eed00ef
    type: comment
  author: justsumguy
  content: Yeah, I didn't have any problem with standard wizardLM, so a bit odd this
    one had the problem but the other one did not. Couldn't say why, I'm positive
    I downloaded "compat.the -no-act-orders" file.
  created_at: 2023-05-07 09:45:23+00:00
  edited: false
  hidden: false
  id: 64578143711ee86f6eed00ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-07T18:36:16.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I don''t know what''s going on there. I''d expect all the models
          I make to break on pre_layer atm, apart from the new file I made for you.</p>

          <p>Regarding your install woes: you don''t need to do an OS migration. You
          can install WSL2, which is Linux running under virtualisation.  It supports
          Nvidia GPUs and CUDA.  It''s easy to install, and once it is installed,
          follow this guide: <a rel="nofollow" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">https://docs.nvidia.com/cuda/wsl-user-guide/index.html</a></p>

          <p>Although that will currently recommend that you install 12.1, and you
          may not want to do that as there''s no pre-built binaries for pytorch with
          CUDA 12.x.   So personally I''m staying with CUDA 11.x, eg CUDA 11.7 or
          11.8. Here''s the WSL download link for CUDA 11.7.1 in WSL: <a rel="nofollow"
          href="https://developer.nvidia.com/cuda-11-7-1-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0&amp;target_type=deb_local">https://developer.nvidia.com/cuda-11-7-1-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0&amp;target_type=deb_local</a></p>

          '
        raw: 'Yeah I don''t know what''s going on there. I''d expect all the models
          I make to break on pre_layer atm, apart from the new file I made for you.


          Regarding your install woes: you don''t need to do an OS migration. You
          can install WSL2, which is Linux running under virtualisation.  It supports
          Nvidia GPUs and CUDA.  It''s easy to install, and once it is installed,
          follow this guide: https://docs.nvidia.com/cuda/wsl-user-guide/index.html


          Although that will currently recommend that you install 12.1, and you may
          not want to do that as there''s no pre-built binaries for pytorch with CUDA
          12.x.   So personally I''m staying with CUDA 11.x, eg CUDA 11.7 or 11.8.
          Here''s the WSL download link for CUDA 11.7.1 in WSL: https://developer.nvidia.com/cuda-11-7-1-download-archive?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local'
        updatedAt: '2023-05-07T18:36:31.373Z'
      numEdits: 1
      reactions: []
    id: 6457efa0116c6b3c62e059a8
    type: comment
  author: TheBloke
  content: 'Yeah I don''t know what''s going on there. I''d expect all the models
    I make to break on pre_layer atm, apart from the new file I made for you.


    Regarding your install woes: you don''t need to do an OS migration. You can install
    WSL2, which is Linux running under virtualisation.  It supports Nvidia GPUs and
    CUDA.  It''s easy to install, and once it is installed, follow this guide: https://docs.nvidia.com/cuda/wsl-user-guide/index.html


    Although that will currently recommend that you install 12.1, and you may not
    want to do that as there''s no pre-built binaries for pytorch with CUDA 12.x.   So
    personally I''m staying with CUDA 11.x, eg CUDA 11.7 or 11.8. Here''s the WSL
    download link for CUDA 11.7.1 in WSL: https://developer.nvidia.com/cuda-11-7-1-download-archive?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local'
  created_at: 2023-05-07 17:36:16+00:00
  edited: true
  hidden: false
  id: 6457efa0116c6b3c62e059a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-05-16T00:24:22.000Z'
    data:
      status: closed
    id: 6462cd36cce92c7d88307d06
    type: status-change
  author: justsumguy
  created_at: 2023-05-15 23:24:22+00:00
  id: 6462cd36cce92c7d88307d06
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/WizardLM-7B-uncensored-GPTQ
repo_type: model
status: closed
target_branch: null
title: I had to update my GPTQ manually to run and it's very slow
