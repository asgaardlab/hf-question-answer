!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rajaswa-postman
conflicting_files: null
created_at: 2023-09-07 17:23:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64abd46e9915c39308ebb6af/Snx1jR_usfRcXTKGTsRLZ.png?w=200&h=200&f=face
      fullname: Rajaswa Patil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rajaswa-postman
      type: user
    createdAt: '2023-09-07T18:23:05.000Z'
    data:
      edited: false
      editors:
      - rajaswa-postman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6656810641288757
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64abd46e9915c39308ebb6af/Snx1jR_usfRcXTKGTsRLZ.png?w=200&h=200&f=face
          fullname: Rajaswa Patil
          isHf: false
          isPro: false
          name: rajaswa-postman
          type: user
        html: "<p>I've been trying to deploy <code>codellama/CodeLlama-13b-Instruct-hf</code>\
          \ on AWS SageMaker with the TGI container for a while now. I am facing two\
          \ issues in particular -</p>\n<ol>\n<li>The tokenizer class mismatch -</li>\n\
          </ol>\n<pre><code>The tokenizer class you load from this checkpoint is not\
          \ the same type as the class this function is called from. It may result\
          \ in unexpected tokenization. \nThe tokenizer class you load from this checkpoint\
          \ is 'CodeLlamaTokenizer'. \nThe class this function is called from is 'LlamaTokenizer'.\n\
          </code></pre>\n<ol start=\"2\">\n<li>Model loading error with TGI -</li>\n\
          </ol>\n<pre><code>File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 142, in serve_inner model = get_model( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 185, in get_model return FlashLlama( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 65, in __init__ model = FlashLlamaForCausalLM(config, weights) File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 452, in __init__ self.model = FlashLlamaModel(config, weights) File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 390, in __init__ [ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 391, in &lt;listcomp&gt; FlashLlamaLayer( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 326, in __init__ self.self_attn = FlashLlamaAttention( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 183, in __init__ self.rotary_emb = PositionRotaryEmbedding.load(\
          \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 395, in load inv_freq = weights.get_tensor(f\"{prefix}.inv_freq\"\
          ) File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 62, in get_tensor filename, tensor_name = self.get_filename(tensor_name)\
          \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 49, in get_filename raise RuntimeError(f\"weight {tensor_name} does\
          \ not exist\")\nRuntimeError: weight model.layers.0.self_attn.rotary_emb.inv_freq\
          \ does not exist\n</code></pre>\n<p>Any idea about how these can be resolved?</p>\n\
          <p>I have tried using the latest transformers version - <code>4.33.1</code>\
          \ as well.</p>\n"
        raw: "I've been trying to deploy `codellama/CodeLlama-13b-Instruct-hf` on\
          \ AWS SageMaker with the TGI container for a while now. I am facing two\
          \ issues in particular -\r\n\r\n1. The tokenizer class mismatch -\r\n```\r\
          \nThe tokenizer class you load from this checkpoint is not the same type\
          \ as the class this function is called from. It may result in unexpected\
          \ tokenization. \r\nThe tokenizer class you load from this checkpoint is\
          \ 'CodeLlamaTokenizer'. \r\nThe class this function is called from is 'LlamaTokenizer'.\r\
          \n```\r\n\r\n2. Model loading error with TGI -\r\n```\r\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 142, in serve_inner model = get_model( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 185, in get_model return FlashLlama( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 65, in __init__ model = FlashLlamaForCausalLM(config, weights) File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 452, in __init__ self.model = FlashLlamaModel(config, weights) File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 390, in __init__ [ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 391, in <listcomp> FlashLlamaLayer( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 326, in __init__ self.self_attn = FlashLlamaAttention( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 183, in __init__ self.rotary_emb = PositionRotaryEmbedding.load(\
          \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 395, in load inv_freq = weights.get_tensor(f\"{prefix}.inv_freq\"\
          ) File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 62, in get_tensor filename, tensor_name = self.get_filename(tensor_name)\
          \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 49, in get_filename raise RuntimeError(f\"weight {tensor_name} does\
          \ not exist\")\r\nRuntimeError: weight model.layers.0.self_attn.rotary_emb.inv_freq\
          \ does not exist\r\n```\r\n\r\nAny idea about how these can be resolved?\r\
          \n\r\nI have tried using the latest transformers version - `4.33.1` as well."
        updatedAt: '2023-09-07T18:23:05.032Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - d4niel92
        - eshromero
    id: 64fa150952e82dd432480014
    type: comment
  author: rajaswa-postman
  content: "I've been trying to deploy `codellama/CodeLlama-13b-Instruct-hf` on AWS\
    \ SageMaker with the TGI container for a while now. I am facing two issues in\
    \ particular -\r\n\r\n1. The tokenizer class mismatch -\r\n```\r\nThe tokenizer\
    \ class you load from this checkpoint is not the same type as the class this function\
    \ is called from. It may result in unexpected tokenization. \r\nThe tokenizer\
    \ class you load from this checkpoint is 'CodeLlamaTokenizer'. \r\nThe class this\
    \ function is called from is 'LlamaTokenizer'.\r\n```\r\n\r\n2. Model loading\
    \ error with TGI -\r\n```\r\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 142, in serve_inner model = get_model( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 185, in get_model return FlashLlama( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 65, in __init__ model = FlashLlamaForCausalLM(config, weights) File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 452, in __init__ self.model = FlashLlamaModel(config, weights) File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 390, in __init__ [ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 391, in <listcomp> FlashLlamaLayer( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 326, in __init__ self.self_attn = FlashLlamaAttention( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 183, in __init__ self.rotary_emb = PositionRotaryEmbedding.load( File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 395, in load inv_freq = weights.get_tensor(f\"{prefix}.inv_freq\") File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 62, in get_tensor filename, tensor_name = self.get_filename(tensor_name)\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 49, in get_filename raise RuntimeError(f\"weight {tensor_name} does not\
    \ exist\")\r\nRuntimeError: weight model.layers.0.self_attn.rotary_emb.inv_freq\
    \ does not exist\r\n```\r\n\r\nAny idea about how these can be resolved?\r\n\r\
    \nI have tried using the latest transformers version - `4.33.1` as well."
  created_at: 2023-09-07 17:23:05+00:00
  edited: false
  hidden: false
  id: 64fa150952e82dd432480014
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e48005437cb5b49818287a5/4uCXGGui-9QifAT4qelxU.png?w=200&h=200&f=face
      fullname: Leandro von Werra
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lvwerra
      type: user
    createdAt: '2023-09-07T18:49:02.000Z'
    data:
      edited: false
      editors:
      - lvwerra
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.17824403941631317
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e48005437cb5b49818287a5/4uCXGGui-9QifAT4qelxU.png?w=200&h=200&f=face
          fullname: Leandro von Werra
          isHf: true
          isPro: false
          name: lvwerra
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;philschmid&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/philschmid\"\
          >@<span class=\"underline\">philschmid</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'cc @philschmid '
        updatedAt: '2023-09-07T18:49:02.075Z'
      numEdits: 0
      reactions: []
    id: 64fa1b1e35a7fc7d4fe7de87
    type: comment
  author: lvwerra
  content: 'cc @philschmid '
  created_at: 2023-09-07 17:49:02+00:00
  edited: false
  hidden: false
  id: 64fa1b1e35a7fc7d4fe7de87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aeca5cee8fbaf5cc5122c13dc186c496.svg
      fullname: Daniel Meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d4niel92
      type: user
    createdAt: '2023-09-09T10:00:31.000Z'
    data:
      edited: true
      editors:
      - d4niel92
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9724360108375549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aeca5cee8fbaf5cc5122c13dc186c496.svg
          fullname: Daniel Meyer
          isHf: false
          isPro: false
          name: d4niel92
          type: user
        html: '<p>Same issue here ... Any help would be greatly appreciated</p>

          <p>I already tried to pip install different transformer versions, but none
          of them was able to fix the problem.</p>

          <pre><code>!pip install git+https://github.com/huggingface/transformers.git@main

          !pip install git+https://github.com/ArthurZucker/transformers.git@main

          !pip install git+https://github.com/ArthurZucker/transformers.git@add-llama-code

          </code></pre>

          '
        raw: 'Same issue here ... Any help would be greatly appreciated


          I already tried to pip install different transformer versions, but none
          of them was able to fix the problem.

          ```

          !pip install git+https://github.com/huggingface/transformers.git@main

          !pip install git+https://github.com/ArthurZucker/transformers.git@main

          !pip install git+https://github.com/ArthurZucker/transformers.git@add-llama-code

          ```'
        updatedAt: '2023-09-16T09:40:35.922Z'
      numEdits: 1
      reactions: []
    id: 64fc423f7021123688dd2429
    type: comment
  author: d4niel92
  content: 'Same issue here ... Any help would be greatly appreciated


    I already tried to pip install different transformer versions, but none of them
    was able to fix the problem.

    ```

    !pip install git+https://github.com/huggingface/transformers.git@main

    !pip install git+https://github.com/ArthurZucker/transformers.git@main

    !pip install git+https://github.com/ArthurZucker/transformers.git@add-llama-code

    ```'
  created_at: 2023-09-09 09:00:31+00:00
  edited: true
  hidden: false
  id: 64fc423f7021123688dd2429
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-09-18T21:19:07.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.882602334022522
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>You should only need <code>pip install git+https://github.com/huggingface/transformers.git@main</code>
          my branch was just for developpement</p>

          '
        raw: You should only need `pip install git+https://github.com/huggingface/transformers.git@main`
          my branch was just for developpement
        updatedAt: '2023-09-18T21:19:07.831Z'
      numEdits: 0
      reactions: []
    id: 6508becb2e4bbde418e5e309
    type: comment
  author: ArthurZ
  content: You should only need `pip install git+https://github.com/huggingface/transformers.git@main`
    my branch was just for developpement
  created_at: 2023-09-18 20:19:07+00:00
  edited: false
  hidden: false
  id: 6508becb2e4bbde418e5e309
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2023-09-18T22:11:50.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.95119309425354
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>This warning is safe to ignore.</p>

          <p>Both tokenizer are the same (for TGI purposes) as TGI doesn''t use the
          codellama in code capabilities, you would need to send the preprompt yourself.<br>For
          the missing <code>inv_freq</code> codellama''s weights didn''t include those
          (essentially it''s llamav2) and old TGI versions expected <code>inv_freq</code>
          to be present.</p>

          <p>This should all be solved with the upcoming Sagemaker release of latest
          TGI.</p>

          '
        raw: 'This warning is safe to ignore.


          Both tokenizer are the same (for TGI purposes) as TGI doesn''t use the codellama
          in code capabilities, you would need to send the preprompt yourself.

          For the missing `inv_freq` codellama''s weights didn''t include those (essentially
          it''s llamav2) and old TGI versions expected `inv_freq` to be present.


          This should all be solved with the upcoming Sagemaker release of latest
          TGI.'
        updatedAt: '2023-09-18T22:11:50.190Z'
      numEdits: 0
      reactions: []
    id: 6508cb26f99720fea1143801
    type: comment
  author: Narsil
  content: 'This warning is safe to ignore.


    Both tokenizer are the same (for TGI purposes) as TGI doesn''t use the codellama
    in code capabilities, you would need to send the preprompt yourself.

    For the missing `inv_freq` codellama''s weights didn''t include those (essentially
    it''s llamav2) and old TGI versions expected `inv_freq` to be present.


    This should all be solved with the upcoming Sagemaker release of latest TGI.'
  created_at: 2023-09-18 21:11:50+00:00
  edited: false
  hidden: false
  id: 6508cb26f99720fea1143801
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aeca5cee8fbaf5cc5122c13dc186c496.svg
      fullname: Daniel Meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d4niel92
      type: user
    createdAt: '2023-09-19T06:37:11.000Z'
    data:
      edited: false
      editors:
      - d4niel92
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7183220386505127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aeca5cee8fbaf5cc5122c13dc186c496.svg
          fullname: Daniel Meyer
          isHf: false
          isPro: false
          name: d4niel92
          type: user
        html: "<p>Thanks for your reply, <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Narsil\"\
          >@<span class=\"underline\">Narsil</span></a></span>\n\n\t</span></span>\
          \ ! Any information on when the upcoming Sagemaker release of the latest\
          \ TGI will be available?</p>\n"
        raw: Thanks for your reply, @Narsil ! Any information on when the upcoming
          Sagemaker release of the latest TGI will be available?
        updatedAt: '2023-09-19T06:37:11.765Z'
      numEdits: 0
      reactions: []
    id: 65094197c19e5b4c8a3d6638
    type: comment
  author: d4niel92
  content: Thanks for your reply, @Narsil ! Any information on when the upcoming Sagemaker
    release of the latest TGI will be available?
  created_at: 2023-09-19 05:37:11+00:00
  edited: false
  hidden: false
  id: 65094197c19e5b4c8a3d6638
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2023-09-19T06:58:21.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9737104177474976
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>Soon I hope, but I can''t make any promises (it''s not in our hands
          at this point)</p>

          '
        raw: Soon I hope, but I can't make any promises (it's not in our hands at
          this point)
        updatedAt: '2023-09-19T06:58:21.572Z'
      numEdits: 0
      reactions: []
    id: 6509468d4882fab5c49ffcd0
    type: comment
  author: Narsil
  content: Soon I hope, but I can't make any promises (it's not in our hands at this
    point)
  created_at: 2023-09-19 05:58:21+00:00
  edited: false
  hidden: false
  id: 6509468d4882fab5c49ffcd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-09-19T07:19:08.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8797010183334351
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: '<p><code>1.0.3</code> is now available on SageMaker. </p>

          '
        raw: '`1.0.3` is now available on SageMaker. '
        updatedAt: '2023-09-19T07:19:08.722Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - d4niel92
        - Narsil
        - Sxxxw
    id: 65094b6cc2d4c8fc378e0058
    type: comment
  author: philschmid
  content: '`1.0.3` is now available on SageMaker. '
  created_at: 2023-09-19 06:19:08+00:00
  edited: false
  hidden: false
  id: 65094b6cc2d4c8fc378e0058
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: codellama/CodeLlama-13b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: Issues while deploying on AWS SageMaker with TGI
