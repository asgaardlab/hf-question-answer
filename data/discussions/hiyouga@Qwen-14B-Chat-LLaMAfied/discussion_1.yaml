!!python/object:huggingface_hub.community.DiscussionWithDetails
author: charry2000
conflicting_files: null
created_at: 2023-12-29 05:47:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
      fullname: charryshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charry2000
      type: user
    createdAt: '2023-12-29T05:47:59.000Z'
    data:
      edited: false
      editors:
      - charry2000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16171512007713318
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
          fullname: charryshi
          isHf: false
          isPro: false
          name: charry2000
          type: user
        html: '<p>python src/evaluate.py     --model_name_or_path ~/model/Qwen-14B-Chat-LLaMAfied
          --finetuning_type full     --template llama2     --task ceval    --split
          validation     --lang zh     --n_shot 5     --batch_size 1</p>

          <p>/models/llama/modeling_llama.py", line 726, in forward<br>    attn_output
          = torch.nn.functional.scaled_dot_product_attention(<br>                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>RuntimeError:
          cutlassF: no kernel found to launch!</p>

          '
        raw: "python src/evaluate.py     --model_name_or_path ~/model/Qwen-14B-Chat-LLaMAfied\
          \ --finetuning_type full     --template llama2     --task ceval    --split\
          \ validation     --lang zh     --n_shot 5     --batch_size 1\r\n\r\n/models/llama/modeling_llama.py\"\
          , line 726, in forward\r\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\r\
          \n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\
          RuntimeError: cutlassF: no kernel found to launch!"
        updatedAt: '2023-12-29T05:47:59.592Z'
      numEdits: 0
      reactions: []
    id: 658e5d8f674349122cddefe5
    type: comment
  author: charry2000
  content: "python src/evaluate.py     --model_name_or_path ~/model/Qwen-14B-Chat-LLaMAfied\
    \ --finetuning_type full     --template llama2     --task ceval    --split validation\
    \     --lang zh     --n_shot 5     --batch_size 1\r\n\r\n/models/llama/modeling_llama.py\"\
    , line 726, in forward\r\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\r\
    \n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError:\
    \ cutlassF: no kernel found to launch!"
  created_at: 2023-12-29 05:47:59+00:00
  edited: false
  hidden: false
  id: 658e5d8f674349122cddefe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-12-29T05:56:18.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6665218472480774
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>Install flash attention2 may help</p>

          '
        raw: Install flash attention2 may help
        updatedAt: '2023-12-29T05:56:18.324Z'
      numEdits: 0
      reactions: []
    id: 658e5f8289145cbc7c6e9eb3
    type: comment
  author: hiyouga
  content: Install flash attention2 may help
  created_at: 2023-12-29 05:56:18+00:00
  edited: false
  hidden: false
  id: 658e5f8289145cbc7c6e9eb3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
      fullname: charryshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charry2000
      type: user
    createdAt: '2023-12-29T06:10:36.000Z'
    data:
      edited: false
      editors:
      - charry2000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.581470251083374
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
          fullname: charryshi
          isHf: false
          isPro: false
          name: charry2000
          type: user
        html: "<p>Thanks<br>I run it at V100, I modified  torch_dtype  from bfloat16\
          \ to  float16 in config.json  just fine .</p>\n<p>But encountered a new\
          \ error</p>\n<p>LLaMA-Factory/src/llmtuner/eval/evaluator.py\", line 44,\
          \ in <br>    word_probs = torch.stack([logits[i, lengths[i] - 1] for i in\
          \ range(len(lengths))], dim=0)<br>                              ~~~~~~^^^^^^^^^^^^^^^^^^^<br>RuntimeError:\
          \ CUDA error: device-side assert triggered<br>CUDA kernel errors might be\
          \ asynchronously reported at some other API call, so the stacktrace below\
          \ might be incorrect.<br>For debugging consider passing CUDA_LAUNCH_BLOCKING=1.<br>Compile\
          \ with <code>TORCH_USE_CUDA_DSA</code> to enable device-side assertions.</p>\n\
          <p>/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [176,0,0], thread: [0,0,0] Assertion <code>-sizes[i]\
          \ &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; \"index out of bounds\"\
          </code> failed.<br>/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [176,0,0], thread: [1,0,0] Assertion <code>-sizes[i]\
          \ &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; \"index out of bounds\"\
          </code> failed.<br>/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [176,0,0], thread: [2,0,0] Assertion <code>-sizes[i]\
          \ &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; \"index out of bounds\"\
          </code> failed.\n </p>\n"
        raw: "Thanks\nI run it at V100, I modified  torch_dtype  from bfloat16 to\
          \  float16 in config.json  just fine .\n\nBut encountered a new error\n\n\
          LLaMA-Factory/src/llmtuner/eval/evaluator.py\", line 44, in <listcomp>\n\
          \    word_probs = torch.stack([logits[i, lengths[i] - 1] for i in range(len(lengths))],\
          \ dim=0)\n                              ~~~~~~^^^^^^^^^^^^^^^^^^^\nRuntimeError:\
          \ CUDA error: device-side assert triggered\nCUDA kernel errors might be\
          \ asynchronously reported at some other API call, so the stacktrace below\
          \ might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\
          /opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [176,0,0], thread: [0,0,0] Assertion `-sizes[i] <=\
          \ index && index < sizes[i] && \"index out of bounds\"` failed.\n/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [176,0,0], thread: [1,0,0] Assertion `-sizes[i] <=\
          \ index && index < sizes[i] && \"index out of bounds\"` failed.\n/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [176,0,0], thread: [2,0,0] Assertion `-sizes[i] <=\
          \ index && index < sizes[i] && \"index out of bounds\"` failed.\n "
        updatedAt: '2023-12-29T06:10:36.013Z'
      numEdits: 0
      reactions: []
    id: 658e62dc674349122cdf0b0c
    type: comment
  author: charry2000
  content: "Thanks\nI run it at V100, I modified  torch_dtype  from bfloat16 to  float16\
    \ in config.json  just fine .\n\nBut encountered a new error\n\nLLaMA-Factory/src/llmtuner/eval/evaluator.py\"\
    , line 44, in <listcomp>\n    word_probs = torch.stack([logits[i, lengths[i] -\
    \ 1] for i in range(len(lengths))], dim=0)\n                              ~~~~~~^^^^^^^^^^^^^^^^^^^\n\
    RuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might\
    \ be asynchronously reported at some other API call, so the stacktrace below might\
    \ be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile\
    \ with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
    \ operator(): block: [176,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index\
    \ && index < sizes[i] && \"index out of bounds\"` failed.\n/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
    \ operator(): block: [176,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index\
    \ && index < sizes[i] && \"index out of bounds\"` failed.\n/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
    \ operator(): block: [176,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index\
    \ && index < sizes[i] && \"index out of bounds\"` failed.\n "
  created_at: 2023-12-29 06:10:36+00:00
  edited: false
  hidden: false
  id: 658e62dc674349122cdf0b0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-12-29T06:20:24.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: ta
        probability: 0.20604082942008972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>use --template qwen</p>

          '
        raw: use --template qwen
        updatedAt: '2023-12-29T06:20:24.648Z'
      numEdits: 0
      reactions: []
    id: 658e652857a556fbe15860b9
    type: comment
  author: hiyouga
  content: use --template qwen
  created_at: 2023-12-29 06:20:24+00:00
  edited: false
  hidden: false
  id: 658e652857a556fbe15860b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
      fullname: charryshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charry2000
      type: user
    createdAt: '2023-12-29T06:39:05.000Z'
    data:
      edited: false
      editors:
      - charry2000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9593494534492493
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
          fullname: charryshi
          isHf: false
          isPro: false
          name: charry2000
          type: user
        html: '<p>I changed to the --template qwen and it''s still the same. I switched
          to the A100 machine and used this model normally. I just tested a fresh
          installation of unsloth update on this V100 server, but it looks like there
          are still issues with the environment.<br>Thanks a lot</p>

          '
        raw: 'I changed to the --template qwen and it''s still the same. I switched
          to the A100 machine and used this model normally. I just tested a fresh
          installation of unsloth update on this V100 server, but it looks like there
          are still issues with the environment.

          Thanks a lot'
        updatedAt: '2023-12-29T06:39:05.393Z'
      numEdits: 0
      reactions: []
      relatedEventId: 658e698977105e6e40a5e70d
    id: 658e698977105e6e40a5e70a
    type: comment
  author: charry2000
  content: 'I changed to the --template qwen and it''s still the same. I switched
    to the A100 machine and used this model normally. I just tested a fresh installation
    of unsloth update on this V100 server, but it looks like there are still issues
    with the environment.

    Thanks a lot'
  created_at: 2023-12-29 06:39:05+00:00
  edited: false
  hidden: false
  id: 658e698977105e6e40a5e70a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
      fullname: charryshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charry2000
      type: user
    createdAt: '2023-12-29T06:39:05.000Z'
    data:
      status: closed
    id: 658e698977105e6e40a5e70d
    type: status-change
  author: charry2000
  created_at: 2023-12-29 06:39:05+00:00
  id: 658e698977105e6e40a5e70d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: hiyouga/Qwen-14B-Chat-LLaMAfied
repo_type: model
status: closed
target_branch: null
title: eval error with LLaMA-Factory
