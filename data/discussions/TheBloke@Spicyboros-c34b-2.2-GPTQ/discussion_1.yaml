!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Leaf45
conflicting_files: null
created_at: 2023-09-12 19:40:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c488e9ca3a7b722a8b1ebe7c3c13436d.svg
      fullname: Leaves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Leaf45
      type: user
    createdAt: '2023-09-12T20:40:35.000Z'
    data:
      edited: false
      editors:
      - Leaf45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4154735505580902
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c488e9ca3a7b722a8b1ebe7c3c13436d.svg
          fullname: Leaves
          isHf: false
          isPro: false
          name: Leaf45
          type: user
        html: '<p>When I try to run the model I see this:</p>

          <p>Traceback (most recent call last):<br>  File "/home/galaxia/text-generation-webui/modules/text_generation.py",
          line 323, in generate_reply_custom<br>    for reply in shared.model.generate_with_streaming(question,
          state):<br>  File "/home/galaxia/text-generation-webui/modules/exllama.py",
          line 81, in generate_with_streaming<br>    self.generator.gen_begin_reuse(ids)<br>  File
          "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py",
          line 193, in gen_begin_reuse<br>    self.gen_begin(in_tokens, max_chunk)<br>  File
          "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py",
          line 177, in gen_begin<br>    self.model.forward(self.sequence[:, a:b],
          self.cache, preprocess_only = True, lora = self.lora)<br>  File "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py",
          line 860, in forward<br>    hidden_states = decoder_layer.forward(hidden_states,
          cache, buffers[device], lora)<br>  File "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py",
          line 466, in forward<br>    hidden_states = self.self_attn.forward(hidden_states,
          cache, buffer, lora)<br>  File "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py",
          line 384, in forward<br>    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,
          self.config.head_dim).transpose(1, 2)<br>RuntimeError: shape ''[1, 188,
          64, 128]'' is invalid for input of size 192512<br>Output generated in 0.00
          seconds (0.00 tokens/s, 0 tokens, context 189, seed 1872453553)</p>

          <p>I can run other llama2 models but this one messes up. Maybe it has something
          to do with being a code llama model</p>

          '
        raw: "When I try to run the model I see this:\r\n\r\nTraceback (most recent\
          \ call last):\r\n  File \"/home/galaxia/text-generation-webui/modules/text_generation.py\"\
          , line 323, in generate_reply_custom\r\n    for reply in shared.model.generate_with_streaming(question,\
          \ state):\r\n  File \"/home/galaxia/text-generation-webui/modules/exllama.py\"\
          , line 81, in generate_with_streaming\r\n    self.generator.gen_begin_reuse(ids)\r\
          \n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
          , line 193, in gen_begin_reuse\r\n    self.gen_begin(in_tokens, max_chunk)\r\
          \n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
          , line 177, in gen_begin\r\n    self.model.forward(self.sequence[:, a:b],\
          \ self.cache, preprocess_only = True, lora = self.lora)\r\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
          , line 860, in forward\r\n    hidden_states = decoder_layer.forward(hidden_states,\
          \ cache, buffers[device], lora)\r\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
          , line 466, in forward\r\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\r\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
          , line 384, in forward\r\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
          \ self.config.head_dim).transpose(1, 2)\r\nRuntimeError: shape '[1, 188,\
          \ 64, 128]' is invalid for input of size 192512\r\nOutput generated in 0.00\
          \ seconds (0.00 tokens/s, 0 tokens, context 189, seed 1872453553)\r\n\r\n\
          I can run other llama2 models but this one messes up. Maybe it has something\
          \ to do with being a code llama model"
        updatedAt: '2023-09-12T20:40:35.508Z'
      numEdits: 0
      reactions: []
    id: 6500ccc3b2078f22bac295b9
    type: comment
  author: Leaf45
  content: "When I try to run the model I see this:\r\n\r\nTraceback (most recent\
    \ call last):\r\n  File \"/home/galaxia/text-generation-webui/modules/text_generation.py\"\
    , line 323, in generate_reply_custom\r\n    for reply in shared.model.generate_with_streaming(question,\
    \ state):\r\n  File \"/home/galaxia/text-generation-webui/modules/exllama.py\"\
    , line 81, in generate_with_streaming\r\n    self.generator.gen_begin_reuse(ids)\r\
    \n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
    , line 193, in gen_begin_reuse\r\n    self.gen_begin(in_tokens, max_chunk)\r\n\
    \  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
    , line 177, in gen_begin\r\n    self.model.forward(self.sequence[:, a:b], self.cache,\
    \ preprocess_only = True, lora = self.lora)\r\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
    , line 860, in forward\r\n    hidden_states = decoder_layer.forward(hidden_states,\
    \ cache, buffers[device], lora)\r\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
    , line 466, in forward\r\n    hidden_states = self.self_attn.forward(hidden_states,\
    \ cache, buffer, lora)\r\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
    , line 384, in forward\r\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
    \ self.config.head_dim).transpose(1, 2)\r\nRuntimeError: shape '[1, 188, 64, 128]'\
    \ is invalid for input of size 192512\r\nOutput generated in 0.00 seconds (0.00\
    \ tokens/s, 0 tokens, context 189, seed 1872453553)\r\n\r\nI can run other llama2\
    \ models but this one messes up. Maybe it has something to do with being a code\
    \ llama model"
  created_at: 2023-09-12 19:40:35+00:00
  edited: false
  hidden: false
  id: 6500ccc3b2078f22bac295b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d1e0c18c0cda4a3338355d9a59c5ce9.svg
      fullname: Iain Eddy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarithel21
      type: user
    createdAt: '2023-09-12T23:19:25.000Z'
    data:
      edited: false
      editors:
      - tarithel21
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9977510571479797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d1e0c18c0cda4a3338355d9a59c5ce9.svg
          fullname: Iain Eddy
          isHf: false
          isPro: false
          name: tarithel21
          type: user
        html: '<p>I was able to load this in oobabooga with exllama ok, couldn''t
          load it with anything else. its seems to work ok</p>

          '
        raw: I was able to load this in oobabooga with exllama ok, couldn't load it
          with anything else. its seems to work ok
        updatedAt: '2023-09-12T23:19:25.413Z'
      numEdits: 0
      reactions: []
    id: 6500f1fde0c94282ab569b2e
    type: comment
  author: tarithel21
  content: I was able to load this in oobabooga with exllama ok, couldn't load it
    with anything else. its seems to work ok
  created_at: 2023-09-12 22:19:25+00:00
  edited: false
  hidden: false
  id: 6500f1fde0c94282ab569b2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c488e9ca3a7b722a8b1ebe7c3c13436d.svg
      fullname: Leaves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Leaf45
      type: user
    createdAt: '2023-09-13T14:21:29.000Z'
    data:
      edited: false
      editors:
      - Leaf45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42950454354286194
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c488e9ca3a7b722a8b1ebe7c3c13436d.svg
          fullname: Leaves
          isHf: false
          isPro: false
          name: Leaf45
          type: user
        html: '<p>When I try to generate with Exllama I get an empty reply and it''s
          basically saying the same thing. An empty reply is more than before but
          still nothing</p>

          <p>Traceback (most recent call last):<br>  File "/home/galaxia/text-generation-webui/modules/text_generation.py",
          line 323, in generate_reply_custom<br>    for reply in shared.model.generate_with_streaming(question,
          state):<br>  File "/home/galaxia/text-generation-webui/modules/exllama.py",
          line 81, in generate_with_streaming<br>    self.generator.gen_begin_reuse(ids)<br>  File
          "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py",
          line 193, in gen_begin_reuse<br>    self.gen_begin(in_tokens, max_chunk)<br>  File
          "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py",
          line 177, in gen_begin<br>    self.model.forward(self.sequence[:, a:b],
          self.cache, preprocess_only = True, lora = self.lora)<br>  File "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py",
          line 860, in forward<br>    hidden_states = decoder_layer.forward(hidden_states,
          cache, buffers[device], lora)<br>  File "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py",
          line 466, in forward<br>    hidden_states = self.self_attn.forward(hidden_states,
          cache, buffer, lora)<br>  File "/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py",
          line 384, in forward<br>    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,
          self.config.head_dim).transpose(1, 2)<br>RuntimeError: shape ''[1, 183,
          64, 128]'' is invalid for input of size 187392<br>Output generated in 0.02
          seconds (0.00 tokens/s, 0 tokens, context 184, seed 1334487251)</p>

          '
        raw: "When I try to generate with Exllama I get an empty reply and it's basically\
          \ saying the same thing. An empty reply is more than before but still nothing\n\
          \nTraceback (most recent call last):\n  File \"/home/galaxia/text-generation-webui/modules/text_generation.py\"\
          , line 323, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
          \ state):\n  File \"/home/galaxia/text-generation-webui/modules/exllama.py\"\
          , line 81, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n\
          \  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
          , line 193, in gen_begin_reuse\n    self.gen_begin(in_tokens, max_chunk)\n\
          \  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
          , line 177, in gen_begin\n    self.model.forward(self.sequence[:, a:b],\
          \ self.cache, preprocess_only = True, lora = self.lora)\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
          , line 860, in forward\n    hidden_states = decoder_layer.forward(hidden_states,\
          \ cache, buffers[device], lora)\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
          , line 466, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
          , line 384, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
          \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 183, 64,\
          \ 128]' is invalid for input of size 187392\nOutput generated in 0.02 seconds\
          \ (0.00 tokens/s, 0 tokens, context 184, seed 1334487251)\n"
        updatedAt: '2023-09-13T14:21:29.116Z'
      numEdits: 0
      reactions: []
    id: 6501c56920cfd12a88ad3126
    type: comment
  author: Leaf45
  content: "When I try to generate with Exllama I get an empty reply and it's basically\
    \ saying the same thing. An empty reply is more than before but still nothing\n\
    \nTraceback (most recent call last):\n  File \"/home/galaxia/text-generation-webui/modules/text_generation.py\"\
    , line 323, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
    \ state):\n  File \"/home/galaxia/text-generation-webui/modules/exllama.py\",\
    \ line 81, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n\
    \  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
    , line 193, in gen_begin_reuse\n    self.gen_begin(in_tokens, max_chunk)\n  File\
    \ \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/generator.py\"\
    , line 177, in gen_begin\n    self.model.forward(self.sequence[:, a:b], self.cache,\
    \ preprocess_only = True, lora = self.lora)\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
    , line 860, in forward\n    hidden_states = decoder_layer.forward(hidden_states,\
    \ cache, buffers[device], lora)\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
    , line 466, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
    \ cache, buffer, lora)\n  File \"/home/galaxia/miniconda3/envs/textgen/lib/python3.10/site-packages/exllama/model.py\"\
    , line 384, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
    \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 183, 64, 128]'\
    \ is invalid for input of size 187392\nOutput generated in 0.02 seconds (0.00\
    \ tokens/s, 0 tokens, context 184, seed 1334487251)\n"
  created_at: 2023-09-13 13:21:29+00:00
  edited: false
  hidden: false
  id: 6501c56920cfd12a88ad3126
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d1e0c18c0cda4a3338355d9a59c5ce9.svg
      fullname: Iain Eddy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarithel21
      type: user
    createdAt: '2023-09-13T14:45:34.000Z'
    data:
      edited: false
      editors:
      - tarithel21
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772420525550842
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d1e0c18c0cda4a3338355d9a59c5ce9.svg
          fullname: Iain Eddy
          isHf: false
          isPro: false
          name: tarithel21
          type: user
        html: '<p>I am getting a reasonably quick reply on a 3090 with well formed
          answers, though maybe a little on the short side.  This was a fresh oobabooga
          2 days ago, I am guessing there is something in your environment/installation
          causing the issue?</p>

          '
        raw: I am getting a reasonably quick reply on a 3090 with well formed answers,
          though maybe a little on the short side.  This was a fresh oobabooga 2 days
          ago, I am guessing there is something in your environment/installation causing
          the issue?
        updatedAt: '2023-09-13T14:45:34.604Z'
      numEdits: 0
      reactions: []
    id: 6501cb0e333476834aa37d5d
    type: comment
  author: tarithel21
  content: I am getting a reasonably quick reply on a 3090 with well formed answers,
    though maybe a little on the short side.  This was a fresh oobabooga 2 days ago,
    I am guessing there is something in your environment/installation causing the
    issue?
  created_at: 2023-09-13 13:45:34+00:00
  edited: false
  hidden: false
  id: 6501cb0e333476834aa37d5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e6fca5a97b454d223384a7c00c07662.svg
      fullname: Calvin Walker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DoctorJayPup
      type: user
    createdAt: '2023-09-19T21:16:13.000Z'
    data:
      edited: false
      editors:
      - DoctorJayPup
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619058966636658
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e6fca5a97b454d223384a7c00c07662.svg
          fullname: Calvin Walker
          isHf: false
          isPro: false
          name: DoctorJayPup
          type: user
        html: '<p>I had trouble loading this with oobabooga until I updated it. Make
          sure to select Exllamav2. I also updated the prompt template to what''s
          listed in the description.</p>

          '
        raw: I had trouble loading this with oobabooga until I updated it. Make sure
          to select Exllamav2. I also updated the prompt template to what's listed
          in the description.
        updatedAt: '2023-09-19T21:16:13.713Z'
      numEdits: 0
      reactions: []
    id: 650a0f9dc19e5b4c8a5ec5c4
    type: comment
  author: DoctorJayPup
  content: I had trouble loading this with oobabooga until I updated it. Make sure
    to select Exllamav2. I also updated the prompt template to what's listed in the
    description.
  created_at: 2023-09-19 20:16:13+00:00
  edited: false
  hidden: false
  id: 650a0f9dc19e5b4c8a5ec5c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c488e9ca3a7b722a8b1ebe7c3c13436d.svg
      fullname: Leaves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Leaf45
      type: user
    createdAt: '2023-10-07T15:29:23.000Z'
    data:
      edited: false
      editors:
      - Leaf45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9942673444747925
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c488e9ca3a7b722a8b1ebe7c3c13436d.svg
          fullname: Leaves
          isHf: false
          isPro: false
          name: Leaf45
          type: user
        html: '<p>Is Exllamav2 meant to become available through upgrading? Because
          I don''t have that option even after upgrading. Though it did happen before
          that I had to reinstall it to get the new features before</p>

          '
        raw: Is Exllamav2 meant to become available through upgrading? Because I don't
          have that option even after upgrading. Though it did happen before that
          I had to reinstall it to get the new features before
        updatedAt: '2023-10-07T15:29:23.442Z'
      numEdits: 0
      reactions: []
    id: 652179532a16045c093d4f81
    type: comment
  author: Leaf45
  content: Is Exllamav2 meant to become available through upgrading? Because I don't
    have that option even after upgrading. Though it did happen before that I had
    to reinstall it to get the new features before
  created_at: 2023-10-07 14:29:23+00:00
  edited: false
  hidden: false
  id: 652179532a16045c093d4f81
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Spicyboros-c34b-2.2-GPTQ
repo_type: model
status: open
target_branch: null
title: Error when running the model
