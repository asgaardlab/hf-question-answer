!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mrwikrom
conflicting_files: null
created_at: 2022-12-17 17:15:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f3cbf0a4e52e78f9c475019103201f9.svg
      fullname: mr wikrom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrwikrom
      type: user
    createdAt: '2022-12-17T17:15:24.000Z'
    data:
      edited: false
      editors:
      - mrwikrom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f3cbf0a4e52e78f9c475019103201f9.svg
          fullname: mr wikrom
          isHf: false
          isPro: false
          name: mrwikrom
          type: user
        html: '<p>Hello, thank you for training this, because I am having memory issues
          when trying to use this model, is this not the "Medium" model of whisper?</p>

          <p>I can use the original medium model of whisper normally, but when I try
          to use this fine tuned model, I am having vram issues, was it not supposed
          to have the same consumption as the original?</p>

          <p>By the way, I can use this model normally on my 3070 ti (8gb) and the
          consumption of this model on it is 8021mb,</p>

          <p>however when I try to use it on my rtx 2060 laptop (6gb) I receive a
          ''CUDA out of memory'' error, but I can use the original medium model normally,</p>

          <p>Why is this error occurring? I await your response, thank you.</p>

          '
        raw: "Hello, thank you for training this, because I am having memory issues\
          \ when trying to use this model, is this not the \"Medium\" model of whisper?\r\
          \n\r\nI can use the original medium model of whisper normally, but when\
          \ I try to use this fine tuned model, I am having vram issues, was it not\
          \ supposed to have the same consumption as the original?\r\n\r\nBy the way,\
          \ I can use this model normally on my 3070 ti (8gb) and the consumption\
          \ of this model on it is 8021mb,\r\n\r\nhowever when I try to use it on\
          \ my rtx 2060 laptop (6gb) I receive a 'CUDA out of memory' error, but I\
          \ can use the original medium model normally,\r\n\r\nWhy is this error occurring?\
          \ I await your response, thank you."
        updatedAt: '2022-12-17T17:15:24.682Z'
      numEdits: 0
      reactions: []
    id: 639df92c7145123e0d4e1b2e
    type: comment
  author: mrwikrom
  content: "Hello, thank you for training this, because I am having memory issues\
    \ when trying to use this model, is this not the \"Medium\" model of whisper?\r\
    \n\r\nI can use the original medium model of whisper normally, but when I try\
    \ to use this fine tuned model, I am having vram issues, was it not supposed to\
    \ have the same consumption as the original?\r\n\r\nBy the way, I can use this\
    \ model normally on my 3070 ti (8gb) and the consumption of this model on it is\
    \ 8021mb,\r\n\r\nhowever when I try to use it on my rtx 2060 laptop (6gb) I receive\
    \ a 'CUDA out of memory' error, but I can use the original medium model normally,\r\
    \n\r\nWhy is this error occurring? I await your response, thank you."
  created_at: 2022-12-17 17:15:24+00:00
  edited: false
  hidden: false
  id: 639df92c7145123e0d4e1b2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670272965540-635e4a1d398ff343c4f3c874.jpeg?w=200&h=200&f=face
      fullname: Jose Londono Botero
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jlondonobo
      type: user
    createdAt: '2022-12-17T19:41:17.000Z'
    data:
      edited: false
      editors:
      - jlondonobo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670272965540-635e4a1d398ff343c4f3c874.jpeg?w=200&h=200&f=face
          fullname: Jose Londono Botero
          isHf: false
          isPro: false
          name: jlondonobo
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;mrwikrom&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mrwikrom\"\
          >@<span class=\"underline\">mrwikrom</span></a></span>\n\n\t</span></span>\
          \ \U0001F44B\U0001F3FB </p>\n<p>Thanks for using this model. I hope you\
          \ are finding it helpful.</p>\n<p>As you say, this is a fine-tuned model\
          \ trained from the original <strong>Whisper Medium</strong> model. As such,\
          \ consumption should be the same as in the original.</p>\n<p>I will investigate\
          \ the issue you describe, and give you a solution as soon as possible. Meanwhile,\
          \ I will link you to <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1EMOwwfm1V1fHxH7eT1LLg7yBjhTooB6j?usp=sharing\"\
          >this Notebook</a>, which shows how to load whisper models in 8bit mode.\
          \ This should reduce consumption and allow you to use the model in your\
          \ RTX2060 laptop.</p>\n<p>If you have trouble using the method described\
          \ in the Notebook please let me know and we can work it out together.</p>\n\
          <p>Talk to you soon,<br>Jos\xE9</p>\n"
        raw: "Hi @mrwikrom \U0001F44B\U0001F3FB \n\nThanks for using this model. I\
          \ hope you are finding it helpful.\n\nAs you say, this is a fine-tuned model\
          \ trained from the original **Whisper Medium** model. As such, consumption\
          \ should be the same as in the original.\n\nI will investigate the issue\
          \ you describe, and give you a solution as soon as possible. Meanwhile,\
          \ I will link you to [this Notebook](https://colab.research.google.com/drive/1EMOwwfm1V1fHxH7eT1LLg7yBjhTooB6j?usp=sharing),\
          \ which shows how to load whisper models in 8bit mode. This should reduce\
          \ consumption and allow you to use the model in your RTX2060 laptop.\n\n\
          If you have trouble using the method described in the Notebook please let\
          \ me know and we can work it out together.\n\nTalk to you soon,\nJos\xE9"
        updatedAt: '2022-12-17T19:41:17.010Z'
      numEdits: 0
      reactions: []
    id: 639e1b5df87da5e2eb17b070
    type: comment
  author: jlondonobo
  content: "Hi @mrwikrom \U0001F44B\U0001F3FB \n\nThanks for using this model. I hope\
    \ you are finding it helpful.\n\nAs you say, this is a fine-tuned model trained\
    \ from the original **Whisper Medium** model. As such, consumption should be the\
    \ same as in the original.\n\nI will investigate the issue you describe, and give\
    \ you a solution as soon as possible. Meanwhile, I will link you to [this Notebook](https://colab.research.google.com/drive/1EMOwwfm1V1fHxH7eT1LLg7yBjhTooB6j?usp=sharing),\
    \ which shows how to load whisper models in 8bit mode. This should reduce consumption\
    \ and allow you to use the model in your RTX2060 laptop.\n\nIf you have trouble\
    \ using the method described in the Notebook please let me know and we can work\
    \ it out together.\n\nTalk to you soon,\nJos\xE9"
  created_at: 2022-12-17 19:41:17+00:00
  edited: false
  hidden: false
  id: 639e1b5df87da5e2eb17b070
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f3cbf0a4e52e78f9c475019103201f9.svg
      fullname: mr wikrom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrwikrom
      type: user
    createdAt: '2022-12-18T04:22:17.000Z'
    data:
      edited: false
      editors:
      - mrwikrom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f3cbf0a4e52e78f9c475019103201f9.svg
          fullname: mr wikrom
          isHf: false
          isPro: false
          name: mrwikrom
          type: user
        html: '<p>Thank you for the response, does this colab not seem to be functional
          for the original whisper (not the one using transformer), or am I wrong?</p>

          <p>And this medium model is indeed using more memory than the original medium..</p>

          <p>Did you manage to figure out why and if there is any way to fix it?</p>

          <p>Once again, thank you.</p>

          '
        raw: 'Thank you for the response, does this colab not seem to be functional
          for the original whisper (not the one using transformer), or am I wrong?


          And this medium model is indeed using more memory than the original medium..


          Did you manage to figure out why and if there is any way to fix it?


          Once again, thank you.'
        updatedAt: '2022-12-18T04:22:17.020Z'
      numEdits: 0
      reactions: []
    id: 639e95797145123e0d5b72e0
    type: comment
  author: mrwikrom
  content: 'Thank you for the response, does this colab not seem to be functional
    for the original whisper (not the one using transformer), or am I wrong?


    And this medium model is indeed using more memory than the original medium..


    Did you manage to figure out why and if there is any way to fix it?


    Once again, thank you.'
  created_at: 2022-12-18 04:22:17+00:00
  edited: false
  hidden: false
  id: 639e95797145123e0d5b72e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670272965540-635e4a1d398ff343c4f3c874.jpeg?w=200&h=200&f=face
      fullname: Jose Londono Botero
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jlondonobo
      type: user
    createdAt: '2022-12-27T04:26:10.000Z'
    data:
      edited: false
      editors:
      - jlondonobo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670272965540-635e4a1d398ff343c4f3c874.jpeg?w=200&h=200&f=face
          fullname: Jose Londono Botero
          isHf: false
          isPro: false
          name: jlondonobo
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;mrwikrom&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mrwikrom\"\
          >@<span class=\"underline\">mrwikrom</span></a></span>\n\n\t</span></span>\
          \ were you able to solve this issue?</p>\n<p>I would like to better understand\
          \ your problem. Does it happen when you use OpenAI's original implementation?\
          \ Or is it a problem when using the <code>transformers</code> API?</p>\n"
        raw: 'Hi @mrwikrom were you able to solve this issue?


          I would like to better understand your problem. Does it happen when you
          use OpenAI''s original implementation? Or is it a problem when using the
          `transformers` API?'
        updatedAt: '2022-12-27T04:26:10.694Z'
      numEdits: 0
      reactions: []
    id: 63aa73e20c51f01ed61ab044
    type: comment
  author: jlondonobo
  content: 'Hi @mrwikrom were you able to solve this issue?


    I would like to better understand your problem. Does it happen when you use OpenAI''s
    original implementation? Or is it a problem when using the `transformers` API?'
  created_at: 2022-12-27 04:26:10+00:00
  edited: false
  hidden: false
  id: 63aa73e20c51f01ed61ab044
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1db62bbf39c798c08b20b051f72bb4ef.svg
      fullname: Liyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Annie-Li
      type: user
    createdAt: '2023-02-21T10:34:15.000Z'
    data:
      edited: true
      editors:
      - Annie-Li
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1db62bbf39c798c08b20b051f72bb4ef.svg
          fullname: Liyan
          isHf: false
          isPro: false
          name: Annie-Li
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;jlondonobo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jlondonobo\"\
          >@<span class=\"underline\">jlondonobo</span></a></span>\n\n\t</span></span><br>I\
          \ have noticed that you have attained very good performance of fine-tuning\
          \ on whisper. I am very interesting about this, and want to make some attempts\
          \ to fine-tune. However, I encounter a problem \"CUDA out of memory\". Could\
          \ you tell me your GPU and computing power on whisper-medium-model? Thanks!</p>\n"
        raw: "Hello @jlondonobo \nI have noticed that you have attained very good\
          \ performance of fine-tuning on whisper. I am very interesting about this,\
          \ and want to make some attempts to fine-tune. However, I encounter a problem\
          \ \"CUDA out of memory\". Could you tell me your GPU and computing power\
          \ on whisper-medium-model? Thanks!"
        updatedAt: '2023-02-21T10:35:43.838Z'
      numEdits: 1
      reactions: []
    id: 63f49e275d5278a0163566cd
    type: comment
  author: Annie-Li
  content: "Hello @jlondonobo \nI have noticed that you have attained very good performance\
    \ of fine-tuning on whisper. I am very interesting about this, and want to make\
    \ some attempts to fine-tune. However, I encounter a problem \"CUDA out of memory\"\
    . Could you tell me your GPU and computing power on whisper-medium-model? Thanks!"
  created_at: 2023-02-21 10:34:15+00:00
  edited: true
  hidden: false
  id: 63f49e275d5278a0163566cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f3cbf0a4e52e78f9c475019103201f9.svg
      fullname: mr wikrom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrwikrom
      type: user
    createdAt: '2023-03-19T17:25:00.000Z'
    data:
      edited: false
      editors:
      - mrwikrom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f3cbf0a4e52e78f9c475019103201f9.svg
          fullname: mr wikrom
          isHf: false
          isPro: false
          name: mrwikrom
          type: user
        html: "<blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;jlondonobo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jlondonobo\"\
          >@<span class=\"underline\">jlondonobo</span></a></span>\n\n\t</span></span><br>I\
          \ have noticed that you have attained very good performance of fine-tuning\
          \ on whisper. I am very interesting about this, and want to make some attempts\
          \ to fine-tune. However, I encounter a problem \"CUDA out of memory\". Could\
          \ you tell me your GPU and computing power on whisper-medium-model? Thanks!</p>\n\
          </blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;Annie-Li&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Annie-Li\"\
          >@<span class=\"underline\">Annie-Li</span></a></span>\n\n\t</span></span>,\
          \ upon using the whisper-medium-pt model, I noticed that it consumes approximately\
          \ 7.8 GB of vRAM. If you are experiencing vRAM issues, just like me, I suggest\
          \ using the Faster Whisper (<a rel=\"nofollow\" href=\"https://github.com/guillaumekln/faster-whisper\"\
          >https://github.com/guillaumekln/faster-whisper</a>) in conjunction with\
          \ this model: <a href=\"https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3\"\
          >https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3</a>.</p>\n<p>By\
          \ doing so, the vRAM consumption will be reduced to around 4.6 GB instead\
          \ of 10 GB, and the inference speed will be improved. Although there might\
          \ be a slight decrease in accuracy, the results will still be of high quality!</p>\n"
        raw: "> Hello @jlondonobo \n> I have noticed that you have attained very good\
          \ performance of fine-tuning on whisper. I am very interesting about this,\
          \ and want to make some attempts to fine-tune. However, I encounter a problem\
          \ \"CUDA out of memory\". Could you tell me your GPU and computing power\
          \ on whisper-medium-model? Thanks!\n\nHello @Annie-Li, upon using the whisper-medium-pt\
          \ model, I noticed that it consumes approximately 7.8 GB of vRAM. If you\
          \ are experiencing vRAM issues, just like me, I suggest using the Faster\
          \ Whisper (https://github.com/guillaumekln/faster-whisper) in conjunction\
          \ with this model: https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3.\n\
          \nBy doing so, the vRAM consumption will be reduced to around 4.6 GB instead\
          \ of 10 GB, and the inference speed will be improved. Although there might\
          \ be a slight decrease in accuracy, the results will still be of high quality!"
        updatedAt: '2023-03-19T17:25:00.732Z'
      numEdits: 0
      reactions: []
    id: 6417456cb8f44a037290a78f
    type: comment
  author: mrwikrom
  content: "> Hello @jlondonobo \n> I have noticed that you have attained very good\
    \ performance of fine-tuning on whisper. I am very interesting about this, and\
    \ want to make some attempts to fine-tune. However, I encounter a problem \"CUDA\
    \ out of memory\". Could you tell me your GPU and computing power on whisper-medium-model?\
    \ Thanks!\n\nHello @Annie-Li, upon using the whisper-medium-pt model, I noticed\
    \ that it consumes approximately 7.8 GB of vRAM. If you are experiencing vRAM\
    \ issues, just like me, I suggest using the Faster Whisper (https://github.com/guillaumekln/faster-whisper)\
    \ in conjunction with this model: https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3.\n\
    \nBy doing so, the vRAM consumption will be reduced to around 4.6 GB instead of\
    \ 10 GB, and the inference speed will be improved. Although there might be a slight\
    \ decrease in accuracy, the results will still be of high quality!"
  created_at: 2023-03-19 16:25:00+00:00
  edited: false
  hidden: false
  id: 6417456cb8f44a037290a78f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1db62bbf39c798c08b20b051f72bb4ef.svg
      fullname: Liyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Annie-Li
      type: user
    createdAt: '2023-03-20T14:01:52.000Z'
    data:
      edited: false
      editors:
      - Annie-Li
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1db62bbf39c798c08b20b051f72bb4ef.svg
          fullname: Liyan
          isHf: false
          isPro: false
          name: Annie-Li
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;jlondonobo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jlondonobo\"\
          >@<span class=\"underline\">jlondonobo</span></a></span>\n\n\t</span></span><br>I\
          \ have noticed that you have attained very good performance of fine-tuning\
          \ on whisper. I am very interesting about this, and want to make some attempts\
          \ to fine-tune. However, I encounter a problem \"CUDA out of memory\". Could\
          \ you tell me your GPU and computing power on whisper-medium-model? Thanks!</p>\n\
          </blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;Annie-Li&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Annie-Li\"\
          >@<span class=\"underline\">Annie-Li</span></a></span>\n\n\t</span></span>,\
          \ upon using the whisper-medium-pt model, I noticed that it consumes approximately\
          \ 7.8 GB of vRAM. If you are experiencing vRAM issues, just like me, I suggest\
          \ using the Faster Whisper (<a rel=\"nofollow\" href=\"https://github.com/guillaumekln/faster-whisper\"\
          >https://github.com/guillaumekln/faster-whisper</a>) in conjunction with\
          \ this model: <a href=\"https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3\"\
          >https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3</a>.</p>\n<p>By\
          \ doing so, the vRAM consumption will be reduced to around 4.6 GB instead\
          \ of 10 GB, and the inference speed will be improved. Although there might\
          \ be a slight decrease in accuracy, the results will still be of high quality!</p>\n\
          </blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;jlondonobo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jlondonobo\"\
          >@<span class=\"underline\">jlondonobo</span></a></span>\n\n\t</span></span>,\
          \ thanks for your answer! It is very useful for me. And I will make a new\
          \ attempt on the Faster Whisper. Thanks!</p>\n"
        raw: "> > Hello @jlondonobo \n> > I have noticed that you have attained very\
          \ good performance of fine-tuning on whisper. I am very interesting about\
          \ this, and want to make some attempts to fine-tune. However, I encounter\
          \ a problem \"CUDA out of memory\". Could you tell me your GPU and computing\
          \ power on whisper-medium-model? Thanks!\n> \n> Hello @Annie-Li, upon using\
          \ the whisper-medium-pt model, I noticed that it consumes approximately\
          \ 7.8 GB of vRAM. If you are experiencing vRAM issues, just like me, I suggest\
          \ using the Faster Whisper (https://github.com/guillaumekln/faster-whisper)\
          \ in conjunction with this model: https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3.\n\
          > \n> By doing so, the vRAM consumption will be reduced to around 4.6 GB\
          \ instead of 10 GB, and the inference speed will be improved. Although there\
          \ might be a slight decrease in accuracy, the results will still be of high\
          \ quality!\n\nHello @jlondonobo, thanks for your answer! It is very useful\
          \ for me. And I will make a new attempt on the Faster Whisper. Thanks!"
        updatedAt: '2023-03-20T14:01:52.750Z'
      numEdits: 0
      reactions: []
    id: 641867502fcc7664a1f6b900
    type: comment
  author: Annie-Li
  content: "> > Hello @jlondonobo \n> > I have noticed that you have attained very\
    \ good performance of fine-tuning on whisper. I am very interesting about this,\
    \ and want to make some attempts to fine-tune. However, I encounter a problem\
    \ \"CUDA out of memory\". Could you tell me your GPU and computing power on whisper-medium-model?\
    \ Thanks!\n> \n> Hello @Annie-Li, upon using the whisper-medium-pt model, I noticed\
    \ that it consumes approximately 7.8 GB of vRAM. If you are experiencing vRAM\
    \ issues, just like me, I suggest using the Faster Whisper (https://github.com/guillaumekln/faster-whisper)\
    \ in conjunction with this model: https://huggingface.co/jlondonobo/whisper-large-v2-pt-v3.\n\
    > \n> By doing so, the vRAM consumption will be reduced to around 4.6 GB instead\
    \ of 10 GB, and the inference speed will be improved. Although there might be\
    \ a slight decrease in accuracy, the results will still be of high quality!\n\n\
    Hello @jlondonobo, thanks for your answer! It is very useful for me. And I will\
    \ make a new attempt on the Faster Whisper. Thanks!"
  created_at: 2023-03-20 13:01:52+00:00
  edited: false
  hidden: false
  id: 641867502fcc7664a1f6b900
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5f3cbf0a4e52e78f9c475019103201f9.svg
      fullname: mr wikrom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrwikrom
      type: user
    createdAt: '2023-03-20T16:19:45.000Z'
    data:
      status: closed
    id: 641887a1be72e3e47820c0eb
    type: status-change
  author: mrwikrom
  created_at: 2023-03-20 15:19:45+00:00
  id: 641887a1be72e3e47820c0eb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jlondonobo/whisper-medium-pt
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: CUDA out of memory'
