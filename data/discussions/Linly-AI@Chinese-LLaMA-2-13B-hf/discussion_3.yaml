!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MagiaSN
conflicting_files: null
created_at: 2023-09-13 10:59:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c21bbffd5837de2123c21e88169c8351.svg
      fullname: Kuo Liao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MagiaSN
      type: user
    createdAt: '2023-09-13T11:59:37.000Z'
    data:
      edited: false
      editors:
      - MagiaSN
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37716639041900635
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c21bbffd5837de2123c21e88169c8351.svg
          fullname: Kuo Liao
          isHf: false
          isPro: false
          name: MagiaSN
          type: user
        html: "<p>I got nearly random results with <code>tokenizer_config.json</code>\
          \ in <code>Chinese-LLaMA-2-13B-hf</code>. After I replaced it with <code>tokenizer_config.json</code>\
          \ in <code>Chinese-LLaMA-2-7B-hf</code>, I got readable results.</p>\n<p><code>tokenizer_config.json</code>\
          \ in <code>Chinese-LLaMA-2-13B-hf</code>:</p>\n<pre><code>{\"bos_token\"\
          : \"\", \"eos_token\": \"\", \"model_max_length\": 1000000000000000019884624838656,\
          \ \"tokenizer_class\": \"LlamaTokenizer\", \"unk_token\": \"\"}\n</code></pre>\n\
          <p><code>tokenizer_config.json</code> in <code>Chinese-LLaMA-2-7B-hf</code>:</p>\n\
          <pre><code>{\n  \"add_bos_token\": true,\n  \"add_eos_token\": false,\n\
          \  \"bos_token\": {\n    \"__type\": \"AddedToken\",\n    \"content\": \"\
          &lt;s&gt;\",\n    \"lstrip\": false,\n    \"normalized\": false,\n    \"\
          rstrip\": false,\n    \"single_word\": false\n  },\n  \"clean_up_tokenization_spaces\"\
          : false,\n  \"eos_token\": {\n    \"__type\": \"AddedToken\",\n    \"content\"\
          : \"&lt;/s&gt;\",\n    \"lstrip\": false,\n    \"normalized\": false,\n\
          \    \"rstrip\": false,\n    \"single_word\": false\n  },\n  \"legacy\"\
          : false,\n  \"model_max_length\": 1000000000000000019884624838656,\n  \"\
          pad_token\": null,\n  \"padding_side\": \"right\",\n  \"sp_model_kwargs\"\
          : {},\n  \"tokenizer_class\": \"LlamaTokenizer\",\n  \"unk_token\": {\n\
          \    \"__type\": \"AddedToken\",\n    \"content\": \"&lt;unk&gt;\",\n  \
          \  \"lstrip\": false,\n    \"normalized\": false,\n    \"rstrip\": false,\n\
          \    \"single_word\": false\n  }\n}\n</code></pre>\n"
        raw: "I got nearly random results with `tokenizer_config.json` in `Chinese-LLaMA-2-13B-hf`.\
          \ After I replaced it with `tokenizer_config.json` in `Chinese-LLaMA-2-7B-hf`,\
          \ I got readable results.\r\n\r\n`tokenizer_config.json` in `Chinese-LLaMA-2-13B-hf`:\r\
          \n\r\n```\r\n{\"bos_token\": \"\", \"eos_token\": \"\", \"model_max_length\"\
          : 1000000000000000019884624838656, \"tokenizer_class\": \"LlamaTokenizer\"\
          , \"unk_token\": \"\"}\r\n```\r\n\r\n`tokenizer_config.json` in `Chinese-LLaMA-2-7B-hf`:\r\
          \n\r\n```\r\n{\r\n  \"add_bos_token\": true,\r\n  \"add_eos_token\": false,\r\
          \n  \"bos_token\": {\r\n    \"__type\": \"AddedToken\",\r\n    \"content\"\
          : \"<s>\",\r\n    \"lstrip\": false,\r\n    \"normalized\": false,\r\n \
          \   \"rstrip\": false,\r\n    \"single_word\": false\r\n  },\r\n  \"clean_up_tokenization_spaces\"\
          : false,\r\n  \"eos_token\": {\r\n    \"__type\": \"AddedToken\",\r\n  \
          \  \"content\": \"</s>\",\r\n    \"lstrip\": false,\r\n    \"normalized\"\
          : false,\r\n    \"rstrip\": false,\r\n    \"single_word\": false\r\n  },\r\
          \n  \"legacy\": false,\r\n  \"model_max_length\": 1000000000000000019884624838656,\r\
          \n  \"pad_token\": null,\r\n  \"padding_side\": \"right\",\r\n  \"sp_model_kwargs\"\
          : {},\r\n  \"tokenizer_class\": \"LlamaTokenizer\",\r\n  \"unk_token\":\
          \ {\r\n    \"__type\": \"AddedToken\",\r\n    \"content\": \"<unk>\",\r\n\
          \    \"lstrip\": false,\r\n    \"normalized\": false,\r\n    \"rstrip\"\
          : false,\r\n    \"single_word\": false\r\n  }\r\n}\r\n```"
        updatedAt: '2023-09-13T11:59:37.209Z'
      numEdits: 0
      reactions: []
    id: 6501a429169b2741d933a79d
    type: comment
  author: MagiaSN
  content: "I got nearly random results with `tokenizer_config.json` in `Chinese-LLaMA-2-13B-hf`.\
    \ After I replaced it with `tokenizer_config.json` in `Chinese-LLaMA-2-7B-hf`,\
    \ I got readable results.\r\n\r\n`tokenizer_config.json` in `Chinese-LLaMA-2-13B-hf`:\r\
    \n\r\n```\r\n{\"bos_token\": \"\", \"eos_token\": \"\", \"model_max_length\":\
    \ 1000000000000000019884624838656, \"tokenizer_class\": \"LlamaTokenizer\", \"\
    unk_token\": \"\"}\r\n```\r\n\r\n`tokenizer_config.json` in `Chinese-LLaMA-2-7B-hf`:\r\
    \n\r\n```\r\n{\r\n  \"add_bos_token\": true,\r\n  \"add_eos_token\": false,\r\n\
    \  \"bos_token\": {\r\n    \"__type\": \"AddedToken\",\r\n    \"content\": \"\
    <s>\",\r\n    \"lstrip\": false,\r\n    \"normalized\": false,\r\n    \"rstrip\"\
    : false,\r\n    \"single_word\": false\r\n  },\r\n  \"clean_up_tokenization_spaces\"\
    : false,\r\n  \"eos_token\": {\r\n    \"__type\": \"AddedToken\",\r\n    \"content\"\
    : \"</s>\",\r\n    \"lstrip\": false,\r\n    \"normalized\": false,\r\n    \"\
    rstrip\": false,\r\n    \"single_word\": false\r\n  },\r\n  \"legacy\": false,\r\
    \n  \"model_max_length\": 1000000000000000019884624838656,\r\n  \"pad_token\"\
    : null,\r\n  \"padding_side\": \"right\",\r\n  \"sp_model_kwargs\": {},\r\n  \"\
    tokenizer_class\": \"LlamaTokenizer\",\r\n  \"unk_token\": {\r\n    \"__type\"\
    : \"AddedToken\",\r\n    \"content\": \"<unk>\",\r\n    \"lstrip\": false,\r\n\
    \    \"normalized\": false,\r\n    \"rstrip\": false,\r\n    \"single_word\":\
    \ false\r\n  }\r\n}\r\n```"
  created_at: 2023-09-13 10:59:37+00:00
  edited: false
  hidden: false
  id: 6501a429169b2741d933a79d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Linly-AI/Chinese-LLaMA-2-13B-hf
repo_type: model
status: open
target_branch: null
title: Tokenizer_config.json seems broken
