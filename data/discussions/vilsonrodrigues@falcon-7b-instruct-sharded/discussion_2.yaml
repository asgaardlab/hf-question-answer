!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tanshuai
conflicting_files: null
created_at: 2023-07-09 14:32:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d06a60c14291943b60074d8f24e9568d.svg
      fullname: tanshuai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tanshuai
      type: user
    createdAt: '2023-07-09T15:32:30.000Z'
    data:
      edited: false
      editors:
      - tanshuai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5416530966758728
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d06a60c14291943b60074d8f24e9568d.svg
          fullname: tanshuai
          isHf: false
          isPro: false
          name: tanshuai
          type: user
        html: "<p>I have two NVIDIA GeForce RTX 3080 GPUs on my system with total\
          \ 20GB GPU memory. It was successful running some LLM models which are required\
          \ up to 10GB GPU memory.</p>\n<p>But it cannot run  falcon-7b-instruct-sharded\
          \ model with the <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >huggingface's text-generation-inference</a>.</p>\n<p>For the text-generation-inference\
          \ command with official recommended parameters:</p>\n<p><code>docker run\
          \ --gpus all  --shm-size 1g -p 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded --num-shard 2</code></p>\n\
          <p>It reports error \"sharded is not supported for this model\".</p>\n<p>I've\
          \ also tried removed all shard related parameters and run the text-generation-inference\
          \ command with only use single GPU.</p>\n<pre><code>docker run --gpus 1\
          \ -p 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded\n</code></pre>\n\
          <p>It reports error \"torch.cuda.OutOfMemoryError: Allocation on device\
          \ 0 would exceed allowed memory. (out of memory)<br>Currently allocated\
          \     : 8.73 GiB<br>Requested               : 157.53 MiB<br>Device limit\
          \            : 9.78 GiB<br>Free (according to CUDA): 19.31 MiB\"</p>\n<p>Please\
          \ check the following logs:</p>\n<pre><code># docker run --gpus all  --shm-size\
          \ 1g -p 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded --num-shard 2\n\
          2023-07-09T15:25:17.922667Z  INFO text_generation_launcher: Args { model_id:\
          \ \"vilsonrodrigues/falcon-7b-instruct-sharded\", revision: None, sharded:\
          \ None, num_shard: Some(2), quantize: None, dtype: None, trust_remote_code:\
          \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
          \ 4, max_input_length: 1024, max_total_tokens: 2048, waiting_served_ratio:\
          \ 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 16000, max_waiting_tokens:\
          \ 20, hostname: \"10dd21d407e5\", port: 80, shard_uds_path: \"/tmp/text-generation-server\"\
          , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache:\
          \ Some(\"/data\"), weights_cache_override: None, disable_custom_kernels:\
          \ false, json_output: false, otlp_endpoint: None, cors_allow_origin: [],\
          \ watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
          \ None, ngrok_domain: None, ngrok_username: None, ngrok_password: None,\
          \ env: false }\n2023-07-09T15:25:17.922719Z  INFO text_generation_launcher:\
          \ Sharding model on 2 processes\n2023-07-09T15:25:17.922871Z  INFO text_generation_launcher:\
          \ Starting download process.\n2023-07-09T15:25:21.330343Z  INFO download:\
          \ text_generation_launcher: Files are already present on the host. Skipping\
          \ download.\n\n2023-07-09T15:25:22.026919Z  INFO text_generation_launcher:\
          \ Successfully downloaded weights.\n2023-07-09T15:25:22.027240Z  INFO text_generation_launcher:\
          \ Starting shard 0\n2023-07-09T15:25:22.027310Z  INFO text_generation_launcher:\
          \ Starting shard 1\n2023-07-09T15:25:25.942328Z  WARN shard-manager: text_generation_launcher:\
          \ We're not using custom kernels.\n rank=1\n2023-07-09T15:25:25.970715Z\
          \  WARN shard-manager: text_generation_launcher: We're not using custom\
          \ kernels.\n rank=0\n2023-07-09T15:25:26.454690Z ERROR shard-manager: text_generation_launcher:\
          \ Error when initializing model\nTraceback (most recent call last):\n  File\
          \ \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n\
          \    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 240, in get_model\n    raise NotImplementedError(\"sharded is not\
          \ supported for this model\")\nNotImplementedError: sharded is not supported\
          \ for this model\n rank=0\n2023-07-09T15:25:26.839306Z ERROR shard-manager:\
          \ text_generation_launcher: Error when initializing model\nTraceback (most\
          \ recent call last):\n  File \"/opt/conda/bin/text-generation-server\",\
          \ line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 240, in get_model\n    raise NotImplementedError(\"sharded is not\
          \ supported for this model\")\nNotImplementedError: sharded is not supported\
          \ for this model\n rank=1\n2023-07-09T15:25:27.331120Z ERROR text_generation_launcher:\
          \ Shard 0 failed to start\n2023-07-09T15:25:27.331151Z ERROR text_generation_launcher:\
          \ Traceback (most recent call last):\n\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"\
          /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
          \    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 240, in get_model\n    raise NotImplementedError(\"sharded is not\
          \ supported for this model\")\n\nNotImplementedError: sharded is not supported\
          \ for this model\n\n\n2023-07-09T15:25:27.331177Z  INFO text_generation_launcher:\
          \ Shutting down shards\n2023-07-09T15:25:27.760486Z  INFO text_generation_launcher:\
          \ Shard 1 terminated\n</code></pre>\n<pre><code># docker run --gpus 1 -p\
          \ 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded\n2023-07-09T15:31:46.866685Z\
          \  INFO text_generation_launcher: Args { model_id: \"vilsonrodrigues/falcon-7b-instruct-sharded\"\
          , revision: None, sharded: None, num_shard: None, quantize: None, dtype:\
          \ None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of:\
          \ 2, max_stop_sequences: 4, max_input_length: 1024, max_total_tokens: 2048,\
          \ waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:\
          \ 16000, max_waiting_tokens: 20, hostname: \"651ed0250429\", port: 80, shard_uds_path:\
          \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:\
          \ 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, json_output: false, otlp_endpoint:\
          \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None,\
          \ ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username:\
          \ None, ngrok_password: None, env: false }\n2023-07-09T15:31:46.866936Z\
          \  INFO text_generation_launcher: Starting download process.\n2023-07-09T15:31:50.637241Z\
          \  INFO download: text_generation_launcher: Files are already present on\
          \ the host. Skipping download.\n\n2023-07-09T15:31:51.270900Z  INFO text_generation_launcher:\
          \ Successfully downloaded weights.\n2023-07-09T15:31:51.271200Z  INFO text_generation_launcher:\
          \ Starting shard 0\n2023-07-09T15:31:54.552445Z  WARN shard-manager: text_generation_launcher:\
          \ We're not using custom kernels.\n rank=0\n2023-07-09T15:32:00.971897Z\
          \ ERROR shard-manager: text_generation_launcher: Error when initializing\
          \ model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 253, in get_model\n    return FlashRWSharded(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 56, in __init__\n    model = FlashRWForCausalLM(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 628, in __init__\n    self.transformer = FlashRWModel(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 558, in __init__\n    [\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 559, in &lt;listcomp&gt;\n    FlashRWLayer(layer_id, config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 411, in __init__\n    self.mlp = FlashMLP(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 363, in __init__\n    self.dense_h_to_4h = TensorParallelColumnLinear.load(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 234, in load\n    return cls.load_multi(config, [prefix], weights,\
          \ bias, dim=0)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 238, in load_multi\n    weight = weights.get_multi_weights_col(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in get_multi_weights_col\n    w = [self.get_sharded(f\"{p}.weight\"\
          , dim=0) for p in prefixes]\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in &lt;listcomp&gt;\n    w = [self.get_sharded(f\"{p}.weight\"\
          , dim=0) for p in prefixes]\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 98, in get_sharded\n    tensor = tensor.to(device=self.device)\n\
          torch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed\
          \ memory. (out of memory)\nCurrently allocated     : 8.73 GiB\nRequested\
          \               : 157.53 MiB\nDevice limit            : 9.78 GiB\nFree (according\
          \ to CUDA): 17.31 MiB\nPyTorch limit (set by user-supplied memory fraction)\n\
          \                        : 17179869184.00 GiB\n rank=0\n2023-07-09T15:32:01.279858Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\nError:\
          \ ShardCannotStart\n2023-07-09T15:32:02.979796Z ERROR text_generation_launcher:\
          \ Shard 0 failed to start\n2023-07-09T15:32:02.979844Z ERROR text_generation_launcher:\
          \ You are using a model of type RefinedWebModel to instantiate a model of\
          \ type . This is not supported for all configurations of models and can\
          \ yield errors.\nTraceback (most recent call last):\n\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"\
          /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
          \    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 253, in get_model\n    return FlashRWSharded(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 56, in __init__\n    model = FlashRWForCausalLM(config, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 628, in __init__\n    self.transformer = FlashRWModel(config, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 558, in __init__\n    [\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 559, in &lt;listcomp&gt;\n    FlashRWLayer(layer_id, config, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 411, in __init__\n    self.mlp = FlashMLP(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 363, in __init__\n    self.dense_h_to_4h = TensorParallelColumnLinear.load(\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 234, in load\n    return cls.load_multi(config, [prefix], weights,\
          \ bias, dim=0)\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 238, in load_multi\n    weight = weights.get_multi_weights_col(\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in get_multi_weights_col\n    w = [self.get_sharded(f\"{p}.weight\"\
          , dim=0) for p in prefixes]\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in &lt;listcomp&gt;\n    w = [self.get_sharded(f\"{p}.weight\"\
          , dim=0) for p in prefixes]\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 98, in get_sharded\n    tensor = tensor.to(device=self.device)\n\n\
          torch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed\
          \ memory. (out of memory)\nCurrently allocated     : 8.73 GiB\nRequested\
          \               : 157.53 MiB\nDevice limit            : 9.78 GiB\nFree (according\
          \ to CUDA): 17.31 MiB\nPyTorch limit (set by user-supplied memory fraction)\n\
          \                        : 17179869184.00 GiB\n\n\n2023-07-09T15:32:02.979915Z\
          \  INFO text_generation_launcher: Shutting down shards\n</code></pre>\n\
          <pre><code># nvidia-smi\nSun Jul  9 23:19:14 2023\n+-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\n|                               |                     \
          \ |               MIG M. |\n|===============================+======================+======================|\n\
          |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |               \
          \   N/A |\n| 30%   39C    P0    87W / 320W |      0MiB / 10240MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |               \
          \   N/A |\n| 30%   38C    P0    N/A / 320W |      0MiB / 10240MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          \n+-----------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \       |\n|  GPU   GI   CI        PID   Type   Process name           \
          \       GPU Memory |\n|        ID   ID                                 \
          \                  Usage      |\n|=============================================================================|\n\
          |  No running processes found                                          \
          \       |\n+-----------------------------------------------------------------------------+\n\
          </code></pre>\n"
        raw: "I have two NVIDIA GeForce RTX 3080 GPUs on my system with total 20GB\
          \ GPU memory. It was successful running some LLM models which are required\
          \ up to 10GB GPU memory.\r\n\r\nBut it cannot run  falcon-7b-instruct-sharded\
          \ model with the [huggingface's text-generation-inference](https://github.com/huggingface/text-generation-inference).\r\
          \n\r\nFor the text-generation-inference command with official recommended\
          \ parameters:\r\n\r\n```docker run --gpus all  --shm-size 1g -p 8080:80\
          \ -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded --num-shard 2```\r\
          \n\r\nIt reports error \"sharded is not supported for this model\".\r\n\r\
          \nI've also tried removed all shard related parameters and run the text-generation-inference\
          \ command with only use single GPU.\r\n\r\n```\r\ndocker run --gpus 1 -p\
          \ 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded\r\n```\r\nIt reports\
          \ error \"torch.cuda.OutOfMemoryError: Allocation on device 0 would exceed\
          \ allowed memory. (out of memory)\r\nCurrently allocated     : 8.73 GiB\r\
          \nRequested               : 157.53 MiB\r\nDevice limit            : 9.78\
          \ GiB\r\nFree (according to CUDA): 19.31 MiB\"\r\n\r\nPlease check the following\
          \ logs:\r\n\r\n```\r\n# docker run --gpus all  --shm-size 1g -p 8080:80\
          \ -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded --num-shard 2\r\n\
          2023-07-09T15:25:17.922667Z  INFO text_generation_launcher: Args { model_id:\
          \ \"vilsonrodrigues/falcon-7b-instruct-sharded\", revision: None, sharded:\
          \ None, num_shard: Some(2), quantize: None, dtype: None, trust_remote_code:\
          \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
          \ 4, max_input_length: 1024, max_total_tokens: 2048, waiting_served_ratio:\
          \ 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 16000, max_waiting_tokens:\
          \ 20, hostname: \"10dd21d407e5\", port: 80, shard_uds_path: \"/tmp/text-generation-server\"\
          , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache:\
          \ Some(\"/data\"), weights_cache_override: None, disable_custom_kernels:\
          \ false, json_output: false, otlp_endpoint: None, cors_allow_origin: [],\
          \ watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
          \ None, ngrok_domain: None, ngrok_username: None, ngrok_password: None,\
          \ env: false }\r\n2023-07-09T15:25:17.922719Z  INFO text_generation_launcher:\
          \ Sharding model on 2 processes\r\n2023-07-09T15:25:17.922871Z  INFO text_generation_launcher:\
          \ Starting download process.\r\n2023-07-09T15:25:21.330343Z  INFO download:\
          \ text_generation_launcher: Files are already present on the host. Skipping\
          \ download.\r\n\r\n2023-07-09T15:25:22.026919Z  INFO text_generation_launcher:\
          \ Successfully downloaded weights.\r\n2023-07-09T15:25:22.027240Z  INFO\
          \ text_generation_launcher: Starting shard 0\r\n2023-07-09T15:25:22.027310Z\
          \  INFO text_generation_launcher: Starting shard 1\r\n2023-07-09T15:25:25.942328Z\
          \  WARN shard-manager: text_generation_launcher: We're not using custom\
          \ kernels.\r\n rank=1\r\n2023-07-09T15:25:25.970715Z  WARN shard-manager:\
          \ text_generation_launcher: We're not using custom kernels.\r\n rank=0\r\
          \n2023-07-09T15:25:26.454690Z ERROR shard-manager: text_generation_launcher:\
          \ Error when initializing model\r\nTraceback (most recent call last):\r\n\
          \  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 240, in get_model\r\n    raise NotImplementedError(\"sharded is not\
          \ supported for this model\")\r\nNotImplementedError: sharded is not supported\
          \ for this model\r\n rank=0\r\n2023-07-09T15:25:26.839306Z ERROR shard-manager:\
          \ text_generation_launcher: Error when initializing model\r\nTraceback (most\
          \ recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 240, in get_model\r\n    raise NotImplementedError(\"sharded is not\
          \ supported for this model\")\r\nNotImplementedError: sharded is not supported\
          \ for this model\r\n rank=1\r\n2023-07-09T15:25:27.331120Z ERROR text_generation_launcher:\
          \ Shard 0 failed to start\r\n2023-07-09T15:25:27.331151Z ERROR text_generation_launcher:\
          \ Traceback (most recent call last):\r\n\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
          \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 240, in get_model\r\n    raise NotImplementedError(\"sharded is not\
          \ supported for this model\")\r\n\r\nNotImplementedError: sharded is not\
          \ supported for this model\r\n\r\n\r\n2023-07-09T15:25:27.331177Z  INFO\
          \ text_generation_launcher: Shutting down shards\r\n2023-07-09T15:25:27.760486Z\
          \  INFO text_generation_launcher: Shard 1 terminated\r\n```\r\n\r\n```\r\
          \n# docker run --gpus 1 -p 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
          \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded\r\n2023-07-09T15:31:46.866685Z\
          \  INFO text_generation_launcher: Args { model_id: \"vilsonrodrigues/falcon-7b-instruct-sharded\"\
          , revision: None, sharded: None, num_shard: None, quantize: None, dtype:\
          \ None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of:\
          \ 2, max_stop_sequences: 4, max_input_length: 1024, max_total_tokens: 2048,\
          \ waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:\
          \ 16000, max_waiting_tokens: 20, hostname: \"651ed0250429\", port: 80, shard_uds_path:\
          \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:\
          \ 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, json_output: false, otlp_endpoint:\
          \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None,\
          \ ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username:\
          \ None, ngrok_password: None, env: false }\r\n2023-07-09T15:31:46.866936Z\
          \  INFO text_generation_launcher: Starting download process.\r\n2023-07-09T15:31:50.637241Z\
          \  INFO download: text_generation_launcher: Files are already present on\
          \ the host. Skipping download.\r\n\r\n2023-07-09T15:31:51.270900Z  INFO\
          \ text_generation_launcher: Successfully downloaded weights.\r\n2023-07-09T15:31:51.271200Z\
          \  INFO text_generation_launcher: Starting shard 0\r\n2023-07-09T15:31:54.552445Z\
          \  WARN shard-manager: text_generation_launcher: We're not using custom\
          \ kernels.\r\n rank=0\r\n2023-07-09T15:32:00.971897Z ERROR shard-manager:\
          \ text_generation_launcher: Error when initializing model\r\nTraceback (most\
          \ recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 253, in get_model\r\n    return FlashRWSharded(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 56, in __init__\r\n    model = FlashRWForCausalLM(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 628, in __init__\r\n    self.transformer = FlashRWModel(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 558, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 559, in <listcomp>\r\n    FlashRWLayer(layer_id, config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 411, in __init__\r\n    self.mlp = FlashMLP(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 363, in __init__\r\n    self.dense_h_to_4h = TensorParallelColumnLinear.load(\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 234, in load\r\n    return cls.load_multi(config, [prefix], weights,\
          \ bias, dim=0)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 238, in load_multi\r\n    weight = weights.get_multi_weights_col(\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in get_multi_weights_col\r\n    w = [self.get_sharded(f\"{p}.weight\"\
          , dim=0) for p in prefixes]\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in <listcomp>\r\n    w = [self.get_sharded(f\"{p}.weight\",\
          \ dim=0) for p in prefixes]\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 98, in get_sharded\r\n    tensor = tensor.to(device=self.device)\r\
          \ntorch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed\
          \ memory. (out of memory)\r\nCurrently allocated     : 8.73 GiB\r\nRequested\
          \               : 157.53 MiB\r\nDevice limit            : 9.78 GiB\r\nFree\
          \ (according to CUDA): 17.31 MiB\r\nPyTorch limit (set by user-supplied\
          \ memory fraction)\r\n                        : 17179869184.00 GiB\r\n rank=0\r\
          \n2023-07-09T15:32:01.279858Z  INFO text_generation_launcher: Waiting for\
          \ shard 0 to be ready...\r\nError: ShardCannotStart\r\n2023-07-09T15:32:02.979796Z\
          \ ERROR text_generation_launcher: Shard 0 failed to start\r\n2023-07-09T15:32:02.979844Z\
          \ ERROR text_generation_launcher: You are using a model of type RefinedWebModel\
          \ to instantiate a model of type . This is not supported for all configurations\
          \ of models and can yield errors.\r\nTraceback (most recent call last):\r\
          \n\r\n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 166, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
          \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 133, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 253, in get_model\r\n    return FlashRWSharded(\r\n\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 56, in __init__\r\n    model = FlashRWForCausalLM(config, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 628, in __init__\r\n    self.transformer = FlashRWModel(config, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 558, in __init__\r\n    [\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 559, in <listcomp>\r\n    FlashRWLayer(layer_id, config, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 411, in __init__\r\n    self.mlp = FlashMLP(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 363, in __init__\r\n    self.dense_h_to_4h = TensorParallelColumnLinear.load(\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 234, in load\r\n    return cls.load_multi(config, [prefix], weights,\
          \ bias, dim=0)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 238, in load_multi\r\n    weight = weights.get_multi_weights_col(\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in get_multi_weights_col\r\n    w = [self.get_sharded(f\"{p}.weight\"\
          , dim=0) for p in prefixes]\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 127, in <listcomp>\r\n    w = [self.get_sharded(f\"{p}.weight\",\
          \ dim=0) for p in prefixes]\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 98, in get_sharded\r\n    tensor = tensor.to(device=self.device)\r\
          \n\r\ntorch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed\
          \ memory. (out of memory)\r\nCurrently allocated     : 8.73 GiB\r\nRequested\
          \               : 157.53 MiB\r\nDevice limit            : 9.78 GiB\r\nFree\
          \ (according to CUDA): 17.31 MiB\r\nPyTorch limit (set by user-supplied\
          \ memory fraction)\r\n                        : 17179869184.00 GiB\r\n\r\
          \n\r\n2023-07-09T15:32:02.979915Z  INFO text_generation_launcher: Shutting\
          \ down shards\r\n```\r\n\r\n\r\n```\r\n# nvidia-smi\r\nSun Jul  9 23:19:14\
          \ 2023\r\n+-----------------------------------------------------------------------------+\r\
          \n| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8\
          \     |\r\n|-------------------------------+----------------------+----------------------+\r\
          \n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\r\n|                               |                   \
          \   |               MIG M. |\r\n|===============================+======================+======================|\r\
          \n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |             \
          \     N/A |\r\n| 30%   39C    P0    87W / 320W |      0MiB / 10240MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   1  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |             \
          \     N/A |\r\n| 30%   38C    P0    N/A / 320W |      0MiB / 10240MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n\r\n+-----------------------------------------------------------------------------+\r\
          \n| Processes:                                                         \
          \         |\r\n|  GPU   GI   CI        PID   Type   Process name       \
          \           GPU Memory |\r\n|        ID   ID                           \
          \                        Usage      |\r\n|=============================================================================|\r\
          \n|  No running processes found                                        \
          \         |\r\n+-----------------------------------------------------------------------------+\r\
          \n```"
        updatedAt: '2023-07-09T15:32:30.658Z'
      numEdits: 0
      reactions: []
    id: 64aad30e6e727d56aeba9d34
    type: comment
  author: tanshuai
  content: "I have two NVIDIA GeForce RTX 3080 GPUs on my system with total 20GB GPU\
    \ memory. It was successful running some LLM models which are required up to 10GB\
    \ GPU memory.\r\n\r\nBut it cannot run  falcon-7b-instruct-sharded model with\
    \ the [huggingface's text-generation-inference](https://github.com/huggingface/text-generation-inference).\r\
    \n\r\nFor the text-generation-inference command with official recommended parameters:\r\
    \n\r\n```docker run --gpus all  --shm-size 1g -p 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
    \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded --num-shard 2```\r\n\r\
    \nIt reports error \"sharded is not supported for this model\".\r\n\r\nI've also\
    \ tried removed all shard related parameters and run the text-generation-inference\
    \ command with only use single GPU.\r\n\r\n```\r\ndocker run --gpus 1 -p 8080:80\
    \ -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9 --model-id\
    \ vilsonrodrigues/falcon-7b-instruct-sharded\r\n```\r\nIt reports error \"torch.cuda.OutOfMemoryError:\
    \ Allocation on device 0 would exceed allowed memory. (out of memory)\r\nCurrently\
    \ allocated     : 8.73 GiB\r\nRequested               : 157.53 MiB\r\nDevice limit\
    \            : 9.78 GiB\r\nFree (according to CUDA): 19.31 MiB\"\r\n\r\nPlease\
    \ check the following logs:\r\n\r\n```\r\n# docker run --gpus all  --shm-size\
    \ 1g -p 8080:80 -v /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9\
    \ --model-id vilsonrodrigues/falcon-7b-instruct-sharded --num-shard 2\r\n2023-07-09T15:25:17.922667Z\
    \  INFO text_generation_launcher: Args { model_id: \"vilsonrodrigues/falcon-7b-instruct-sharded\"\
    , revision: None, sharded: None, num_shard: Some(2), quantize: None, dtype: None,\
    \ trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
    \ 4, max_input_length: 1024, max_total_tokens: 2048, waiting_served_ratio: 1.2,\
    \ max_batch_prefill_tokens: 4096, max_batch_total_tokens: 16000, max_waiting_tokens:\
    \ 20, hostname: \"10dd21d407e5\", port: 80, shard_uds_path: \"/tmp/text-generation-server\"\
    , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache: Some(\"\
    /data\"), weights_cache_override: None, disable_custom_kernels: false, json_output:\
    \ false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta:\
    \ None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username:\
    \ None, ngrok_password: None, env: false }\r\n2023-07-09T15:25:17.922719Z  INFO\
    \ text_generation_launcher: Sharding model on 2 processes\r\n2023-07-09T15:25:17.922871Z\
    \  INFO text_generation_launcher: Starting download process.\r\n2023-07-09T15:25:21.330343Z\
    \  INFO download: text_generation_launcher: Files are already present on the host.\
    \ Skipping download.\r\n\r\n2023-07-09T15:25:22.026919Z  INFO text_generation_launcher:\
    \ Successfully downloaded weights.\r\n2023-07-09T15:25:22.027240Z  INFO text_generation_launcher:\
    \ Starting shard 0\r\n2023-07-09T15:25:22.027310Z  INFO text_generation_launcher:\
    \ Starting shard 1\r\n2023-07-09T15:25:25.942328Z  WARN shard-manager: text_generation_launcher:\
    \ We're not using custom kernels.\r\n rank=1\r\n2023-07-09T15:25:25.970715Z  WARN\
    \ shard-manager: text_generation_launcher: We're not using custom kernels.\r\n\
    \ rank=0\r\n2023-07-09T15:25:26.454690Z ERROR shard-manager: text_generation_launcher:\
    \ Error when initializing model\r\nTraceback (most recent call last):\r\n  File\
    \ \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n    sys.exit(app())\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 311, in\
    \ __call__\r\n    return get_command(self)(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\n\
    \    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 166, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 133, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 240, in get_model\r\n    raise NotImplementedError(\"sharded is not supported\
    \ for this model\")\r\nNotImplementedError: sharded is not supported for this\
    \ model\r\n rank=0\r\n2023-07-09T15:25:26.839306Z ERROR shard-manager: text_generation_launcher:\
    \ Error when initializing model\r\nTraceback (most recent call last):\r\n  File\
    \ \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n    sys.exit(app())\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 311, in\
    \ __call__\r\n    return get_command(self)(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\n\
    \    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 166, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 133, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 240, in get_model\r\n    raise NotImplementedError(\"sharded is not supported\
    \ for this model\")\r\nNotImplementedError: sharded is not supported for this\
    \ model\r\n rank=1\r\n2023-07-09T15:25:27.331120Z ERROR text_generation_launcher:\
    \ Shard 0 failed to start\r\n2023-07-09T15:25:27.331151Z ERROR text_generation_launcher:\
    \ Traceback (most recent call last):\r\n\r\n  File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\r\n    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 78, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 166, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File \"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
    \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 133, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 240, in get_model\r\n    raise NotImplementedError(\"sharded is not supported\
    \ for this model\")\r\n\r\nNotImplementedError: sharded is not supported for this\
    \ model\r\n\r\n\r\n2023-07-09T15:25:27.331177Z  INFO text_generation_launcher:\
    \ Shutting down shards\r\n2023-07-09T15:25:27.760486Z  INFO text_generation_launcher:\
    \ Shard 1 terminated\r\n```\r\n\r\n```\r\n# docker run --gpus 1 -p 8080:80 -v\
    \ /root/data:/data ghcr.io/huggingface/text-generation-inference:0.9 --model-id\
    \ vilsonrodrigues/falcon-7b-instruct-sharded\r\n2023-07-09T15:31:46.866685Z  INFO\
    \ text_generation_launcher: Args { model_id: \"vilsonrodrigues/falcon-7b-instruct-sharded\"\
    , revision: None, sharded: None, num_shard: None, quantize: None, dtype: None,\
    \ trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
    \ 4, max_input_length: 1024, max_total_tokens: 2048, waiting_served_ratio: 1.2,\
    \ max_batch_prefill_tokens: 4096, max_batch_total_tokens: 16000, max_waiting_tokens:\
    \ 20, hostname: \"651ed0250429\", port: 80, shard_uds_path: \"/tmp/text-generation-server\"\
    , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache: Some(\"\
    /data\"), weights_cache_override: None, disable_custom_kernels: false, json_output:\
    \ false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta:\
    \ None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username:\
    \ None, ngrok_password: None, env: false }\r\n2023-07-09T15:31:46.866936Z  INFO\
    \ text_generation_launcher: Starting download process.\r\n2023-07-09T15:31:50.637241Z\
    \  INFO download: text_generation_launcher: Files are already present on the host.\
    \ Skipping download.\r\n\r\n2023-07-09T15:31:51.270900Z  INFO text_generation_launcher:\
    \ Successfully downloaded weights.\r\n2023-07-09T15:31:51.271200Z  INFO text_generation_launcher:\
    \ Starting shard 0\r\n2023-07-09T15:31:54.552445Z  WARN shard-manager: text_generation_launcher:\
    \ We're not using custom kernels.\r\n rank=0\r\n2023-07-09T15:32:00.971897Z ERROR\
    \ shard-manager: text_generation_launcher: Error when initializing model\r\nTraceback\
    \ (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 166, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 133, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 253, in get_model\r\n    return FlashRWSharded(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
    , line 56, in __init__\r\n    model = FlashRWForCausalLM(config, weights)\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 628, in __init__\r\n    self.transformer = FlashRWModel(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 558, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 559, in <listcomp>\r\n    FlashRWLayer(layer_id, config, weights)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 411, in __init__\r\n    self.mlp = FlashMLP(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 363, in __init__\r\n    self.dense_h_to_4h = TensorParallelColumnLinear.load(\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 234, in load\r\n    return cls.load_multi(config, [prefix], weights, bias,\
    \ dim=0)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 238, in load_multi\r\n    weight = weights.get_multi_weights_col(\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 127, in get_multi_weights_col\r\n    w = [self.get_sharded(f\"{p}.weight\"\
    , dim=0) for p in prefixes]\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 127, in <listcomp>\r\n    w = [self.get_sharded(f\"{p}.weight\", dim=0)\
    \ for p in prefixes]\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 98, in get_sharded\r\n    tensor = tensor.to(device=self.device)\r\ntorch.cuda.OutOfMemoryError:\
    \ Allocation on device 0 would exceed allowed memory. (out of memory)\r\nCurrently\
    \ allocated     : 8.73 GiB\r\nRequested               : 157.53 MiB\r\nDevice limit\
    \            : 9.78 GiB\r\nFree (according to CUDA): 17.31 MiB\r\nPyTorch limit\
    \ (set by user-supplied memory fraction)\r\n                        : 17179869184.00\
    \ GiB\r\n rank=0\r\n2023-07-09T15:32:01.279858Z  INFO text_generation_launcher:\
    \ Waiting for shard 0 to be ready...\r\nError: ShardCannotStart\r\n2023-07-09T15:32:02.979796Z\
    \ ERROR text_generation_launcher: Shard 0 failed to start\r\n2023-07-09T15:32:02.979844Z\
    \ ERROR text_generation_launcher: You are using a model of type RefinedWebModel\
    \ to instantiate a model of type . This is not supported for all configurations\
    \ of models and can yield errors.\r\nTraceback (most recent call last):\r\n\r\n\
    \  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n   \
    \ sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 78, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 166, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File \"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
    \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 133, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 253, in get_model\r\n    return FlashRWSharded(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
    , line 56, in __init__\r\n    model = FlashRWForCausalLM(config, weights)\r\n\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 628, in __init__\r\n    self.transformer = FlashRWModel(config, weights)\r\
    \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 558, in __init__\r\n    [\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 559, in <listcomp>\r\n    FlashRWLayer(layer_id, config, weights)\r\n\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 411, in __init__\r\n    self.mlp = FlashMLP(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 363, in __init__\r\n    self.dense_h_to_4h = TensorParallelColumnLinear.load(\r\
    \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 234, in load\r\n    return cls.load_multi(config, [prefix], weights, bias,\
    \ dim=0)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 238, in load_multi\r\n    weight = weights.get_multi_weights_col(\r\n\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 127, in get_multi_weights_col\r\n    w = [self.get_sharded(f\"{p}.weight\"\
    , dim=0) for p in prefixes]\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 127, in <listcomp>\r\n    w = [self.get_sharded(f\"{p}.weight\", dim=0)\
    \ for p in prefixes]\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 98, in get_sharded\r\n    tensor = tensor.to(device=self.device)\r\n\r\n\
    torch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed memory.\
    \ (out of memory)\r\nCurrently allocated     : 8.73 GiB\r\nRequested         \
    \      : 157.53 MiB\r\nDevice limit            : 9.78 GiB\r\nFree (according to\
    \ CUDA): 17.31 MiB\r\nPyTorch limit (set by user-supplied memory fraction)\r\n\
    \                        : 17179869184.00 GiB\r\n\r\n\r\n2023-07-09T15:32:02.979915Z\
    \  INFO text_generation_launcher: Shutting down shards\r\n```\r\n\r\n\r\n```\r\
    \n# nvidia-smi\r\nSun Jul  9 23:19:14 2023\r\n+-----------------------------------------------------------------------------+\r\
    \n| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8  \
    \   |\r\n|-------------------------------+----------------------+----------------------+\r\
    \n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC\
    \ |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\
    \ M. |\r\n|                               |                      |           \
    \    MIG M. |\r\n|===============================+======================+======================|\r\
    \n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A\
    \ |\r\n| 30%   39C    P0    87W / 320W |      0MiB / 10240MiB |      0%      Default\
    \ |\r\n|                               |                      |              \
    \    N/A |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   1  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |                  N/A\
    \ |\r\n| 30%   38C    P0    N/A / 320W |      0MiB / 10240MiB |      0%      Default\
    \ |\r\n|                               |                      |              \
    \    N/A |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n\r\n+-----------------------------------------------------------------------------+\r\
    \n| Processes:                                                               \
    \   |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU\
    \ Memory |\r\n|        ID   ID                                               \
    \    Usage      |\r\n|=============================================================================|\r\
    \n|  No running processes found                                              \
    \   |\r\n+-----------------------------------------------------------------------------+\r\
    \n```"
  created_at: 2023-07-09 14:32:30+00:00
  edited: false
  hidden: false
  id: 64aad30e6e727d56aeba9d34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ad957ed2b9bb6f4cdb9531d72175c73.svg
      fullname: vilson
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: vilsonrodrigues
      type: user
    createdAt: '2023-07-09T17:34:12.000Z'
    data:
      edited: true
      editors:
      - vilsonrodrigues
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9881992936134338
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ad957ed2b9bb6f4cdb9531d72175c73.svg
          fullname: vilson
          isHf: false
          isPro: false
          name: vilsonrodrigues
          type: user
        html: '<p>probably the 3080 doesn''t support the Falcon 7b.  V100 also does
          not support</p>

          <p>#update</p>

          <p>as Falcon-7b does not allow sharding you will need a gpu with at least
          16gb.  an alternative would be to apply quantization (gptq or bitsandbytes)</p>

          '
        raw: 'probably the 3080 doesn''t support the Falcon 7b.  V100 also does not
          support


          #update


          as Falcon-7b does not allow sharding you will need a gpu with at least 16gb.  an
          alternative would be to apply quantization (gptq or bitsandbytes)'
        updatedAt: '2023-07-09T17:42:01.676Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - tanshuai
    id: 64aaef946e727d56aebd9fa0
    type: comment
  author: vilsonrodrigues
  content: 'probably the 3080 doesn''t support the Falcon 7b.  V100 also does not
    support


    #update


    as Falcon-7b does not allow sharding you will need a gpu with at least 16gb.  an
    alternative would be to apply quantization (gptq or bitsandbytes)'
  created_at: 2023-07-09 16:34:12+00:00
  edited: true
  hidden: false
  id: 64aaef946e727d56aebd9fa0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d06a60c14291943b60074d8f24e9568d.svg
      fullname: tanshuai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tanshuai
      type: user
    createdAt: '2023-07-10T10:48:06.000Z'
    data:
      edited: false
      editors:
      - tanshuai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9144157767295837
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d06a60c14291943b60074d8f24e9568d.svg
          fullname: tanshuai
          isHf: false
          isPro: false
          name: tanshuai
          type: user
        html: '<p>Thank you for your swift response.</p>

          <p>I suspect this might be an issue related to text-generation-inference.
          In my system, your model works with Huggingface Transformers.</p>

          <p>For further discussion, I''ve posted an issue at <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/issues/574">https://github.com/huggingface/text-generation-inference/issues/574</a>.</p>

          <blockquote>

          <p>probably the 3080 doesn''t support the Falcon 7b.  V100 also does not
          support</p>

          <p>#update</p>

          <p>as Falcon-7b does not allow sharding you will need a gpu with at least
          16gb.  an alternative would be to apply quantization (gptq or bitsandbytes)</p>

          </blockquote>

          '
        raw: "Thank you for your swift response.\n\nI suspect this might be an issue\
          \ related to text-generation-inference. In my system, your model works with\
          \ Huggingface Transformers.\n\nFor further discussion, I've posted an issue\
          \ at https://github.com/huggingface/text-generation-inference/issues/574.\n\
          \n\n> probably the 3080 doesn't support the Falcon 7b.  V100 also does not\
          \ support\n> \n> #update\n> \n> as Falcon-7b does not allow sharding you\
          \ will need a gpu with at least 16gb.  an alternative would be to apply\
          \ quantization (gptq or bitsandbytes)\n\n\n"
        updatedAt: '2023-07-10T10:48:06.980Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64abe1e70e2e93502a690112
    id: 64abe1e60e2e93502a690106
    type: comment
  author: tanshuai
  content: "Thank you for your swift response.\n\nI suspect this might be an issue\
    \ related to text-generation-inference. In my system, your model works with Huggingface\
    \ Transformers.\n\nFor further discussion, I've posted an issue at https://github.com/huggingface/text-generation-inference/issues/574.\n\
    \n\n> probably the 3080 doesn't support the Falcon 7b.  V100 also does not support\n\
    > \n> #update\n> \n> as Falcon-7b does not allow sharding you will need a gpu\
    \ with at least 16gb.  an alternative would be to apply quantization (gptq or\
    \ bitsandbytes)\n\n\n"
  created_at: 2023-07-10 09:48:06+00:00
  edited: false
  hidden: false
  id: 64abe1e60e2e93502a690106
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d06a60c14291943b60074d8f24e9568d.svg
      fullname: tanshuai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tanshuai
      type: user
    createdAt: '2023-07-10T10:48:07.000Z'
    data:
      status: closed
    id: 64abe1e70e2e93502a690112
    type: status-change
  author: tanshuai
  created_at: 2023-07-10 09:48:07+00:00
  id: 64abe1e70e2e93502a690112
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: vilsonrodrigues/falcon-7b-instruct-sharded
repo_type: model
status: closed
target_branch: null
title: ' falcon-7b-instruct-sharded not work with huggingface/text-generation-inference'
