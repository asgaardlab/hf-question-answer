!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kapil1611
conflicting_files: null
created_at: 2023-10-02 18:35:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c386b219276353b0df6825da473e129.svg
      fullname: kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kapil1611
      type: user
    createdAt: '2023-10-02T19:35:29.000Z'
    data:
      edited: true
      editors:
      - kapil1611
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4424540102481842
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c386b219276353b0df6825da473e129.svg
          fullname: kumar
          isHf: false
          isPro: false
          name: kapil1611
          type: user
        html: '<p>I am wrapping my head around</p>

          <p>Trying to understand Why A is faster than B</p>

          <h1 id="a">A.</h1>

          <p>tokenizer_large = AutoTokenizer.from_pretrained(f"google/flan-t5-large")<br>model_large
          = AutoModelForSeq2SeqLM.from_pretrained(f"google/flan-t5-large", torch_dtype=torch.float16,
          device_map="auto")<br>IS FASTER THEN</p>

          <h1 id="b">B.</h1>

          <p>model_id = "google/flan-t5-large"<br>quantization_config = BitsAndBytesConfig(<br>    load_in_4bit=True,<br>    bnb_4bit_use_double_quant=False<br>)<br>model_large
          = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=quantization_config)<br>tokenizer_large
          = AutoTokenizer.from_pretrained(model_id) (edited) </p>

          '
        raw: "I am wrapping my head around\n\nTrying to understand Why A is faster\
          \ than B\n# A.\n\ntokenizer_large = AutoTokenizer.from_pretrained(f\"google/flan-t5-large\"\
          )\nmodel_large = AutoModelForSeq2SeqLM.from_pretrained(f\"google/flan-t5-large\"\
          , torch_dtype=torch.float16, device_map=\"auto\")\nIS FASTER THEN\n# B.\n\
          model_id = \"google/flan-t5-large\"\nquantization_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False\n)\nmodel_large\
          \ = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=quantization_config)\n\
          tokenizer_large = AutoTokenizer.from_pretrained(model_id) (edited) "
        updatedAt: '2023-10-02T19:36:13.463Z'
      numEdits: 2
      reactions: []
    id: 651b1b8114145f2a00f1f699
    type: comment
  author: kapil1611
  content: "I am wrapping my head around\n\nTrying to understand Why A is faster than\
    \ B\n# A.\n\ntokenizer_large = AutoTokenizer.from_pretrained(f\"google/flan-t5-large\"\
    )\nmodel_large = AutoModelForSeq2SeqLM.from_pretrained(f\"google/flan-t5-large\"\
    , torch_dtype=torch.float16, device_map=\"auto\")\nIS FASTER THEN\n# B.\nmodel_id\
    \ = \"google/flan-t5-large\"\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n\
    \    bnb_4bit_use_double_quant=False\n)\nmodel_large = AutoModelForSeq2SeqLM.from_pretrained(model_id,\
    \ quantization_config=quantization_config)\ntokenizer_large = AutoTokenizer.from_pretrained(model_id)\
    \ (edited) "
  created_at: 2023-10-02 18:35:29+00:00
  edited: true
  hidden: false
  id: 651b1b8114145f2a00f1f699
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-03T09:13:44.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: tr
        probability: 0.10496710985898972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'cc @ybelkada '
        updatedAt: '2023-10-03T09:13:44.565Z'
      numEdits: 0
      reactions: []
    id: 651bdb48ca84a54d9cf31351
    type: comment
  author: lysandre
  content: 'cc @ybelkada '
  created_at: 2023-10-03 08:13:44+00:00
  edited: false
  hidden: false
  id: 651bdb48ca84a54d9cf31351
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-09T16:04:44.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6443291306495667
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;kapil1611&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kapil1611\"\
          >@<span class=\"underline\">kapil1611</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>load_in_4bit flag activates the 4bit quantization described in\
          \ this paper: <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2305.14314\"\
          >https://arxiv.org/abs/2305.14314</a> - that method iteratively quantizes\
          \ and de-quantizes linear layers in 4bit and makes the matmul computation\
          \ either in float32 (default) or half precision. The quantization / de-quantization\
          \ adds some overhead, making it slower in most cases compared to half-precision\
          \ models.</p>\n<p>By default we use <code>bnb_4bit_compute_dtype=torch.float32</code>:\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/utils/quantization_config.py#L204\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/utils/quantization_config.py#L204</a><br>For\
          \ faster generation, you can benefit from the optimized kernels described\
          \ here: <a rel=\"nofollow\" href=\"https://twitter.com/Tim_Dettmers/status/1683118705956491264?s=20\"\
          >https://twitter.com/Tim_Dettmers/status/1683118705956491264?s=20</a> -\
          \ first make sure to use the latest stable <code>bitsandbytes</code> package\
          \ <code>pip install -U bitsandbytes</code>, then run:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n\
          \nmodel_id = <span class=\"hljs-string\">\"google/flan-t5-large\"</span>\n\
          quantization_config = BitsAndBytesConfig(\n    load_in_4bit=<span class=\"\
          hljs-literal\">True</span>,\n    bnb_4bit_use_double_quant=<span class=\"\
          hljs-literal\">False</span>,\n    bnb_4bit_compute_dtype=torch.float16\n\
          )\nmodel_large = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=quantization_config)\n\
          tokenizer_large = AutoTokenizer.from_pretrained(model_id)\n</code></pre>\n\
          <p>That should hopefully lead to much faster inference speed compared to\
          \ default 4bit models, and maybe similar or faster inference speed with\
          \ batch_size=1 depending on the hardware</p>\n"
        raw: "Hi @kapil1611 \n\nload_in_4bit flag activates the 4bit quantization\
          \ described in this paper: https://arxiv.org/abs/2305.14314 - that method\
          \ iteratively quantizes and de-quantizes linear layers in 4bit and makes\
          \ the matmul computation either in float32 (default) or half precision.\
          \ The quantization / de-quantization adds some overhead, making it slower\
          \ in most cases compared to half-precision models.\n\nBy default we use\
          \ `bnb_4bit_compute_dtype=torch.float32`: https://github.com/huggingface/transformers/blob/main/src/transformers/utils/quantization_config.py#L204\
          \ \nFor faster generation, you can benefit from the optimized kernels described\
          \ here: https://twitter.com/Tim_Dettmers/status/1683118705956491264?s=20\
          \ - first make sure to use the latest stable `bitsandbytes` package `pip\
          \ install -U bitsandbytes`, then run:\n\n```python\nimport torch\nfrom transformers\
          \ import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_id\
          \ = \"google/flan-t5-large\"\nquantization_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_compute_dtype=torch.float16\n\
          )\nmodel_large = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=quantization_config)\n\
          tokenizer_large = AutoTokenizer.from_pretrained(model_id)\n```\n\nThat should\
          \ hopefully lead to much faster inference speed compared to default 4bit\
          \ models, and maybe similar or faster inference speed with batch_size=1\
          \ depending on the hardware"
        updatedAt: '2023-10-09T16:04:44.715Z'
      numEdits: 0
      reactions: []
    id: 6524249ca9a710554b0d3723
    type: comment
  author: ybelkada
  content: "Hi @kapil1611 \n\nload_in_4bit flag activates the 4bit quantization described\
    \ in this paper: https://arxiv.org/abs/2305.14314 - that method iteratively quantizes\
    \ and de-quantizes linear layers in 4bit and makes the matmul computation either\
    \ in float32 (default) or half precision. The quantization / de-quantization adds\
    \ some overhead, making it slower in most cases compared to half-precision models.\n\
    \nBy default we use `bnb_4bit_compute_dtype=torch.float32`: https://github.com/huggingface/transformers/blob/main/src/transformers/utils/quantization_config.py#L204\
    \ \nFor faster generation, you can benefit from the optimized kernels described\
    \ here: https://twitter.com/Tim_Dettmers/status/1683118705956491264?s=20 - first\
    \ make sure to use the latest stable `bitsandbytes` package `pip install -U bitsandbytes`,\
    \ then run:\n\n```python\nimport torch\nfrom transformers import BitsAndBytesConfig,\
    \ AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_id = \"google/flan-t5-large\"\n\
    quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n\
    \    bnb_4bit_compute_dtype=torch.float16\n)\nmodel_large = AutoModelForSeq2SeqLM.from_pretrained(model_id,\
    \ quantization_config=quantization_config)\ntokenizer_large = AutoTokenizer.from_pretrained(model_id)\n\
    ```\n\nThat should hopefully lead to much faster inference speed compared to default\
    \ 4bit models, and maybe similar or faster inference speed with batch_size=1 depending\
    \ on the hardware"
  created_at: 2023-10-09 15:04:44+00:00
  edited: false
  hidden: false
  id: 6524249ca9a710554b0d3723
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: google/flan-t5-large
repo_type: model
status: open
target_branch: null
title: Why 4bit quantised performance is slower than fp 16?
