!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nineves
conflicting_files: null
created_at: 2023-10-09 03:28:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd00d14dce7a2859abef8598543f448a.svg
      fullname: Wang Yiying
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nineves
      type: user
    createdAt: '2023-10-09T04:28:14.000Z'
    data:
      edited: false
      editors:
      - Nineves
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.35595548152923584
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd00d14dce7a2859abef8598543f448a.svg
          fullname: Wang Yiying
          isHf: false
          isPro: false
          name: Nineves
          type: user
        html: '<p>Anyone met this problem?</p>

          <pre><code>Some weights of the model checkpoint at Flan-T5-large were not
          used when initializing T5ForConditionalGeneration: [''encoder.block.0.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.0.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.1.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.1.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.2.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.2.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.3.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.3.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.4.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.4.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.5.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.5.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.6.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.6.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.7.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.7.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.8.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.8.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.9.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.9.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.10.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.10.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.11.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.11.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.12.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.12.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.13.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.13.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.14.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.14.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.15.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.15.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.16.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.16.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.17.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.17.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.18.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.18.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.19.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.19.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.20.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.20.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.21.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.21.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.22.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.22.layer.1.DenseReluDense.wi_1.weight'', ''encoder.block.23.layer.1.DenseReluDense.wi_0.weight'',
          ''encoder.block.23.layer.1.DenseReluDense.wi_1.weight'', ''decoder.block.0.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.0.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.1.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.1.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.2.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.2.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.3.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.3.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.4.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.4.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.5.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.5.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.6.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.6.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.7.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.7.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.8.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.8.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.9.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.9.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.10.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.10.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.11.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.11.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.12.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.12.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.13.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.13.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.14.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.14.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.15.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.15.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.16.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.16.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.17.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.17.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.18.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.18.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.19.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.19.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.20.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.20.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.21.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.21.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.22.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.22.layer.2.DenseReluDense.wi_1.weight'', ''decoder.block.23.layer.2.DenseReluDense.wi_0.weight'',
          ''decoder.block.23.layer.2.DenseReluDense.wi_1.weight'']

          - This IS expected if you are initializing T5ForConditionalGeneration from
          the checkpoint of a model trained on another task or with another architecture
          (e.g. initializing a BertForSequenceClassification model from a BertForPretraining
          model).

          - This IS NOT expected if you are initializing T5ForConditionalGeneration
          from the checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).

          Some weights of T5ForConditionalGeneration were not initialized from the
          model checkpoint at Flan-T5-large and are newly initialized: [''encoder.block.0.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.1.layer.1.DenseReluDense.wi.weight'', ''encoder.block.2.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.3.layer.1.DenseReluDense.wi.weight'', ''encoder.block.4.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.5.layer.1.DenseReluDense.wi.weight'', ''encoder.block.6.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.7.layer.1.DenseReluDense.wi.weight'', ''encoder.block.8.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.9.layer.1.DenseReluDense.wi.weight'', ''encoder.block.10.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.11.layer.1.DenseReluDense.wi.weight'', ''encoder.block.12.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.13.layer.1.DenseReluDense.wi.weight'', ''encoder.block.14.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.15.layer.1.DenseReluDense.wi.weight'', ''encoder.block.16.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.17.layer.1.DenseReluDense.wi.weight'', ''encoder.block.18.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.19.layer.1.DenseReluDense.wi.weight'', ''encoder.block.20.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.21.layer.1.DenseReluDense.wi.weight'', ''encoder.block.22.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.23.layer.1.DenseReluDense.wi.weight'', ''decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight'',
          ''decoder.block.0.layer.2.DenseReluDense.wi.weight'', ''decoder.block.1.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.2.layer.2.DenseReluDense.wi.weight'', ''decoder.block.3.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.4.layer.2.DenseReluDense.wi.weight'', ''decoder.block.5.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.6.layer.2.DenseReluDense.wi.weight'', ''decoder.block.7.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.8.layer.2.DenseReluDense.wi.weight'', ''decoder.block.9.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.10.layer.2.DenseReluDense.wi.weight'', ''decoder.block.11.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.12.layer.2.DenseReluDense.wi.weight'', ''decoder.block.13.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.14.layer.2.DenseReluDense.wi.weight'', ''decoder.block.15.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.16.layer.2.DenseReluDense.wi.weight'', ''decoder.block.17.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.18.layer.2.DenseReluDense.wi.weight'', ''decoder.block.19.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.20.layer.2.DenseReluDense.wi.weight'', ''decoder.block.21.layer.2.DenseReluDense.wi.weight'',
          ''decoder.block.22.layer.2.DenseReluDense.wi.weight'', ''decoder.block.23.layer.2.DenseReluDense.wi.weight'']

          You should probably TRAIN this model on a down-stream task to be able to
          use it for predictions and inference.

          Special tokens have been added in the vocabulary, make sure the associated
          word emebedding are fine-tuned or trained.

          </code></pre>

          '
        raw: "Anyone met this problem?\r\n```\r\nSome weights of the model checkpoint\
          \ at Flan-T5-large were not used when initializing T5ForConditionalGeneration:\
          \ ['encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.12.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.12.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.13.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.13.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.14.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.14.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.15.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.15.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.16.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.16.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.17.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.17.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.18.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.18.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.19.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.19.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.20.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.20.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.21.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.21.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.22.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.22.layer.1.DenseReluDense.wi_1.weight',\
          \ 'encoder.block.23.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.23.layer.1.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight',\
          \ 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight']\r\
          \n- This IS expected if you are initializing T5ForConditionalGeneration\
          \ from the checkpoint of a model trained on another task or with another\
          \ architecture (e.g. initializing a BertForSequenceClassification model\
          \ from a BertForPretraining model).\r\n- This IS NOT expected if you are\
          \ initializing T5ForConditionalGeneration from the checkpoint of a model\
          \ that you expect to be exactly identical (initializing a BertForSequenceClassification\
          \ model from a BertForSequenceClassification model).\r\nSome weights of\
          \ T5ForConditionalGeneration were not initialized from the model checkpoint\
          \ at Flan-T5-large and are newly initialized: ['encoder.block.0.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.12.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.13.layer.1.DenseReluDense.wi.weight', 'encoder.block.14.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.15.layer.1.DenseReluDense.wi.weight', 'encoder.block.16.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.17.layer.1.DenseReluDense.wi.weight', 'encoder.block.18.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.19.layer.1.DenseReluDense.wi.weight', 'encoder.block.20.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.21.layer.1.DenseReluDense.wi.weight', 'encoder.block.22.layer.1.DenseReluDense.wi.weight',\
          \ 'encoder.block.23.layer.1.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight',\
          \ 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight',\
          \ 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight']\r\
          \nYou should probably TRAIN this model on a down-stream task to be able\
          \ to use it for predictions and inference.\r\nSpecial tokens have been added\
          \ in the vocabulary, make sure the associated word emebedding are fine-tuned\
          \ or trained.\r\n```"
        updatedAt: '2023-10-09T04:28:14.517Z'
      numEdits: 0
      reactions: []
    id: 6523815e5e7247c29189198d
    type: comment
  author: Nineves
  content: "Anyone met this problem?\r\n```\r\nSome weights of the model checkpoint\
    \ at Flan-T5-large were not used when initializing T5ForConditionalGeneration:\
    \ ['encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.12.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.12.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.13.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.13.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.14.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.14.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.15.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.15.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.16.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.16.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.17.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.17.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.18.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.18.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.19.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.19.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.20.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.20.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.21.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.21.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.22.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.22.layer.1.DenseReluDense.wi_1.weight',\
    \ 'encoder.block.23.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.23.layer.1.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight',\
    \ 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight']\r\
    \n- This IS expected if you are initializing T5ForConditionalGeneration from the\
    \ checkpoint of a model trained on another task or with another architecture (e.g.\
    \ initializing a BertForSequenceClassification model from a BertForPretraining\
    \ model).\r\n- This IS NOT expected if you are initializing T5ForConditionalGeneration\
    \ from the checkpoint of a model that you expect to be exactly identical (initializing\
    \ a BertForSequenceClassification model from a BertForSequenceClassification model).\r\
    \nSome weights of T5ForConditionalGeneration were not initialized from the model\
    \ checkpoint at Flan-T5-large and are newly initialized: ['encoder.block.0.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.12.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.13.layer.1.DenseReluDense.wi.weight', 'encoder.block.14.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.15.layer.1.DenseReluDense.wi.weight', 'encoder.block.16.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.17.layer.1.DenseReluDense.wi.weight', 'encoder.block.18.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.19.layer.1.DenseReluDense.wi.weight', 'encoder.block.20.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.21.layer.1.DenseReluDense.wi.weight', 'encoder.block.22.layer.1.DenseReluDense.wi.weight',\
    \ 'encoder.block.23.layer.1.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight',\
    \ 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight',\
    \ 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight']\r\
    \nYou should probably TRAIN this model on a down-stream task to be able to use\
    \ it for predictions and inference.\r\nSpecial tokens have been added in the vocabulary,\
    \ make sure the associated word emebedding are fine-tuned or trained.\r\n```"
  created_at: 2023-10-09 03:28:14+00:00
  edited: false
  hidden: false
  id: 6523815e5e7247c29189198d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-09T08:38:53.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6676781177520752
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Nineves&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Nineves\">@<span class=\"\
          underline\">Nineves</span></a></span>\n\n\t</span></span> can you share\
          \ a reproducible snippet?</p>\n<p>the script below:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> T5ForConditionalGeneration, AutoTokenizer\n\nmodel_id = <span\
          \ class=\"hljs-string\">\"google/flan-t5-large\"</span>\n\nmodel = T5ForConditionalGeneration.from_pretrained(model_id,\
          \ torch_dtype=torch.bfloat16, low_cpu_mem_usage=<span class=\"hljs-literal\"\
          >True</span>)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n</code></pre>\n\
          <p>Works fine on my end, can you make sure to use the latest transformers?</p>\n\
          <pre><code class=\"language-bash\">pip install -U transformers\n</code></pre>\n"
        raw: 'Hi @Nineves can you share a reproducible snippet?


          the script below:


          ```python

          import torch

          from transformers import T5ForConditionalGeneration, AutoTokenizer


          model_id = "google/flan-t5-large"


          model = T5ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,
          low_cpu_mem_usage=True)

          tokenizer = AutoTokenizer.from_pretrained(model_id)

          ```


          Works fine on my end, can you make sure to use the latest transformers?


          ```bash

          pip install -U transformers

          ```'
        updatedAt: '2023-10-09T08:38:53.600Z'
      numEdits: 0
      reactions: []
    id: 6523bc1dfb5b5f51e6201885
    type: comment
  author: ybelkada
  content: 'Hi @Nineves can you share a reproducible snippet?


    the script below:


    ```python

    import torch

    from transformers import T5ForConditionalGeneration, AutoTokenizer


    model_id = "google/flan-t5-large"


    model = T5ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True)

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    ```


    Works fine on my end, can you make sure to use the latest transformers?


    ```bash

    pip install -U transformers

    ```'
  created_at: 2023-10-09 07:38:53+00:00
  edited: false
  hidden: false
  id: 6523bc1dfb5b5f51e6201885
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: google/flan-t5-large
repo_type: model
status: open
target_branch: null
title: Some weights of the model checkpoint at Flan-T5-large were not used when initializing
  T5ForConditionalGeneration
