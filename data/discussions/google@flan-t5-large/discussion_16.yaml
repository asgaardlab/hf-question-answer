!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LeandroArg
conflicting_files: null
created_at: 2023-05-02 14:47:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/55a961164c05661f8b511fed3c0f7890.svg
      fullname: "Leandro Ismael Arg\xFCello"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeandroArg
      type: user
    createdAt: '2023-05-02T15:47:49.000Z'
    data:
      edited: true
      editors:
      - LeandroArg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/55a961164c05661f8b511fed3c0f7890.svg
          fullname: "Leandro Ismael Arg\xFCello"
          isHf: false
          isPro: false
          name: LeandroArg
          type: user
        html: "<p>Or what hardware did you use to fine-tune it? </p>\n<p>Are 2 NVIDIA\
          \ A30 GPUs with 24GB each sufficient?  \U0001F914</p>\n"
        raw: "Or what hardware did you use to fine-tune it? \n\nAre 2 NVIDIA A30 GPUs\
          \ with 24GB each sufficient?  \U0001F914"
        updatedAt: '2023-05-02T15:49:15.025Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rlekhwani-umass
    id: 645130a5103a50b1f707d73a
    type: comment
  author: LeandroArg
  content: "Or what hardware did you use to fine-tune it? \n\nAre 2 NVIDIA A30 GPUs\
    \ with 24GB each sufficient?  \U0001F914"
  created_at: 2023-05-02 14:47:49+00:00
  edited: true
  hidden: false
  id: 645130a5103a50b1f707d73a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-09T16:08:20.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8054778575897217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;LeandroArg&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LeandroArg\"\
          >@<span class=\"underline\">LeandroArg</span></a></span>\n\n\t</span></span><br>if\
          \ you use LoRA or QLoRA this should be more than sufficient. By fine-tuning\
          \ only adapters you drastically reduce the number of trainable parameters\
          \ of the model, making it possible to fine-tune large models on consumer-type\
          \ hardware.<br>Please have a look at: <a href=\"https://huggingface.co/docs/transformers/peft\"\
          >https://huggingface.co/docs/transformers/peft</a> or the examples here:\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/peft/tree/main/examples\"\
          >https://github.com/huggingface/peft/tree/main/examples</a> to understand\
          \ how to use PEFT to fine-tune large models at low cost.</p>\n"
        raw: "Hi @LeandroArg \nif you use LoRA or QLoRA this should be more than sufficient.\
          \ By fine-tuning only adapters you drastically reduce the number of trainable\
          \ parameters of the model, making it possible to fine-tune large models\
          \ on consumer-type hardware.\nPlease have a look at: https://huggingface.co/docs/transformers/peft\
          \ or the examples here: https://github.com/huggingface/peft/tree/main/examples\
          \ to understand how to use PEFT to fine-tune large models at low cost.\n"
        updatedAt: '2023-10-09T16:08:20.462Z'
      numEdits: 0
      reactions: []
    id: 65242574db79e539485b7481
    type: comment
  author: ybelkada
  content: "Hi @LeandroArg \nif you use LoRA or QLoRA this should be more than sufficient.\
    \ By fine-tuning only adapters you drastically reduce the number of trainable\
    \ parameters of the model, making it possible to fine-tune large models on consumer-type\
    \ hardware.\nPlease have a look at: https://huggingface.co/docs/transformers/peft\
    \ or the examples here: https://github.com/huggingface/peft/tree/main/examples\
    \ to understand how to use PEFT to fine-tune large models at low cost.\n"
  created_at: 2023-10-09 15:08:20+00:00
  edited: false
  hidden: false
  id: 65242574db79e539485b7481
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: google/flan-t5-large
repo_type: model
status: open
target_branch: null
title: Does anyone know the minimum hardware requirements to fine-tune this Flan-T5-Large
  model?
