!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NhatHoang2002
conflicting_files: null
created_at: 2023-11-08 14:15:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b6308751363b484df6c46bf6dcf460d.svg
      fullname: Hoang Minh Nhat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NhatHoang2002
      type: user
    createdAt: '2023-11-08T14:15:31.000Z'
    data:
      edited: false
      editors:
      - NhatHoang2002
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9457215070724487
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b6308751363b484df6c46bf6dcf460d.svg
          fullname: Hoang Minh Nhat
          isHf: false
          isPro: false
          name: NhatHoang2002
          type: user
        html: "<p>Thank you for the excellent work. I am new to the concept and want\
          \ to understand how DPO works.</p>\n<p>From your reported training results,\
          \ I observed this trend:</p>\n<pre><code>Validation Loss \u2193\nRewards/chosen\
          \ \u2193\nRewards/rejected \u2193\nRewards/accuracies \u2191 \nRewards/margins\
          \ \u2191\n</code></pre>\n<p>I assume that these trends are correct to achieve\
          \ a good performance.</p>\n<p>However, when I tried to fine-tune a smaller\
          \ model (to learn the concept) following this <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/trl/tree/main/examples/scripts\">tutorial</a>.\
          \ I got a different trend:</p>\n<pre><code>Validation Loss \u2193\nRewards/chosen\
          \ \u2191\nRewards/rejected \u2191\nRewards/accuracies \u2191 \nRewards/margins\
          \ \u2191\n</code></pre>\n<p>I am only tuning with different <code>learning_rate</code>\
          \ now and my model can't achieve a trend in yours. May I ask which trend\
          \ is correct and if there is any secret recipe behind it?</p>\n<p>I know\
          \ that you guys will release \"The Alignment Handbook\" but I am having\
          \ an assignment deadline soon. Thus, I hope that you guys can help me have\
          \ a sneak peek into the recipes.</p>\n"
        raw: "Thank you for the excellent work. I am new to the concept and want to\
          \ understand how DPO works.\r\n\r\nFrom your reported training results,\
          \ I observed this trend:\r\n```\r\nValidation Loss \u2193\r\nRewards/chosen\
          \ \u2193\r\nRewards/rejected \u2193\r\nRewards/accuracies \u2191 \r\nRewards/margins\
          \ \u2191\r\n```\r\n\r\nI assume that these trends are correct to achieve\
          \ a good performance.\r\n\r\nHowever, when I tried to fine-tune a smaller\
          \ model (to learn the concept) following this [tutorial](https://github.com/huggingface/trl/tree/main/examples/scripts).\
          \ I got a different trend:\r\n```\r\nValidation Loss \u2193\r\nRewards/chosen\
          \ \u2191\r\nRewards/rejected \u2191\r\nRewards/accuracies \u2191 \r\nRewards/margins\
          \ \u2191\r\n```\r\nI am only tuning with different `learning_rate` now and\
          \ my model can't achieve a trend in yours. May I ask which trend is correct\
          \ and if there is any secret recipe behind it?\r\n\r\nI know that you guys\
          \ will release \"The Alignment Handbook\" but I am having an assignment\
          \ deadline soon. Thus, I hope that you guys can help me have a sneak peek\
          \ into the recipes."
        updatedAt: '2023-11-08T14:15:32.000Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PlanetDOGE
    id: 654b9803dc772c8e80bee84c
    type: comment
  author: NhatHoang2002
  content: "Thank you for the excellent work. I am new to the concept and want to\
    \ understand how DPO works.\r\n\r\nFrom your reported training results, I observed\
    \ this trend:\r\n```\r\nValidation Loss \u2193\r\nRewards/chosen \u2193\r\nRewards/rejected\
    \ \u2193\r\nRewards/accuracies \u2191 \r\nRewards/margins \u2191\r\n```\r\n\r\n\
    I assume that these trends are correct to achieve a good performance.\r\n\r\n\
    However, when I tried to fine-tune a smaller model (to learn the concept) following\
    \ this [tutorial](https://github.com/huggingface/trl/tree/main/examples/scripts).\
    \ I got a different trend:\r\n```\r\nValidation Loss \u2193\r\nRewards/chosen\
    \ \u2191\r\nRewards/rejected \u2191\r\nRewards/accuracies \u2191 \r\nRewards/margins\
    \ \u2191\r\n```\r\nI am only tuning with different `learning_rate` now and my\
    \ model can't achieve a trend in yours. May I ask which trend is correct and if\
    \ there is any secret recipe behind it?\r\n\r\nI know that you guys will release\
    \ \"The Alignment Handbook\" but I am having an assignment deadline soon. Thus,\
    \ I hope that you guys can help me have a sneak peek into the recipes."
  created_at: 2023-11-08 14:15:31+00:00
  edited: false
  hidden: false
  id: 654b9803dc772c8e80bee84c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6500c7c912c1442d994c36e5/e9_lURKTRR_zY6-pcPZLj.jpeg?w=200&h=200&f=face
      fullname: Sir Doge
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PlanetDOGE
      type: user
    createdAt: '2023-11-08T15:38:02.000Z'
    data:
      edited: true
      editors:
      - PlanetDOGE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9275681376457214
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6500c7c912c1442d994c36e5/e9_lURKTRR_zY6-pcPZLj.jpeg?w=200&h=200&f=face
          fullname: Sir Doge
          isHf: false
          isPro: false
          name: PlanetDOGE
          type: user
        html: '<p>I checked GitHub, Reddit, and even FutureSpot, and found that many
          other people are having this same problem with smaller models. I have found
          a different tutorial-like article that may help you. While I am not on the
          H4 team, I do have some knowledge regarding DPOs.<br>I hope the link below
          will help solve your problem!</p>

          <p><a rel="nofollow" href="https://ai.plainenglish.io/direct-preference-optimization-dpo-a-simplified-approach-to-fine-tuning-large-language-models-bae1c6d7ec29">https://ai.plainenglish.io/direct-preference-optimization-dpo-a-simplified-approach-to-fine-tuning-large-language-models-bae1c6d7ec29</a></p>

          '
        raw: 'I checked GitHub, Reddit, and even FutureSpot, and found that many other
          people are having this same problem with smaller models. I have found a
          different tutorial-like article that may help you. While I am not on the
          H4 team, I do have some knowledge regarding DPOs.

          I hope the link below will help solve your problem!


          https://ai.plainenglish.io/direct-preference-optimization-dpo-a-simplified-approach-to-fine-tuning-large-language-models-bae1c6d7ec29'
        updatedAt: '2023-11-08T15:39:08.153Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F92F"
        users:
        - Yhyu13
        - Sigmally
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Sigmally
    id: 654bab5abac6e6e4987503ee
    type: comment
  author: PlanetDOGE
  content: 'I checked GitHub, Reddit, and even FutureSpot, and found that many other
    people are having this same problem with smaller models. I have found a different
    tutorial-like article that may help you. While I am not on the H4 team, I do have
    some knowledge regarding DPOs.

    I hope the link below will help solve your problem!


    https://ai.plainenglish.io/direct-preference-optimization-dpo-a-simplified-approach-to-fine-tuning-large-language-models-bae1c6d7ec29'
  created_at: 2023-11-08 15:38:02+00:00
  edited: true
  hidden: false
  id: 654bab5abac6e6e4987503ee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: Understand reward metrics
