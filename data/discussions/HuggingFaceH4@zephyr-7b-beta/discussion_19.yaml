!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-11-04 05:14:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-11-04T06:14:19.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9574002027511597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>With some other Mistrals like Open Hermes 2, as well as Llama 2
          13b AYT, I can prompt a story with a paragraph of instructions and it will
          in most cases follow it without creating blatant contradictions.</p>

          <p>However, this model stubbornly sticks to the standard story telling format,
          such as suspense and happy endings, even when it blatantly contradicts the
          prompt, leading to absurd continuity errors than not even a young child
          would make.</p>

          <p>For example, if prompted for a kid to get caught stealing a cookie other
          LLMs would simply say something like ''the door flung open''. However, this
          LLM keeps saying things like he heard foot steps coming up the hall as he
          looked at the plate of cookies, then moments later was startled and caught
          red-handed eating the cookies. And when I asked why, if he was supposed
          to get caught, you had him hear foot steps coming up the hall, Zephyr Beta
          says it''s to build suspense. </p>

          <p>A blatant contradiction like this happens with every one of the paragraph
          long story prompts I use to test LLMs with, and in every case because it''s
          stubbornly sticking to pre-packaged story telling elements like suspense
          and happy endings. I know that this doesn''t have to be the case because
          other LLMs are smart enough to avoid these contradictions (e.g. the door
          suddenly opened vs first hearing footsteps coming down the hall). And it''s
          not like it can''t comprehend the prompt because when I ask why hearing
          foot steps coming up the hall precludes getting caught it can explain why,
          then it will tell the story again while making the correction.</p>

          <p>In short, prompting Zephyr Beta to tell a story turns into a battle against
          the pre-package story telling elements. Other than this, Zephyr Beta is
          great and did far better during my testing than Zephyr Alpha, which has
          the same story telling stubbornness, resulting in blatant contradictions
          not even human toddlers would make when following the prompted instructions.</p>

          '
        raw: "With some other Mistrals like Open Hermes 2, as well as Llama 2 13b\
          \ AYT, I can prompt a story with a paragraph of instructions and it will\
          \ in most cases follow it without creating blatant contradictions.\r\n\r\
          \nHowever, this model stubbornly sticks to the standard story telling format,\
          \ such as suspense and happy endings, even when it blatantly contradicts\
          \ the prompt, leading to absurd continuity errors than not even a young\
          \ child would make.\r\n\r\nFor example, if prompted for a kid to get caught\
          \ stealing a cookie other LLMs would simply say something like 'the door\
          \ flung open'. However, this LLM keeps saying things like he heard foot\
          \ steps coming up the hall as he looked at the plate of cookies, then moments\
          \ later was startled and caught red-handed eating the cookies. And when\
          \ I asked why, if he was supposed to get caught, you had him hear foot steps\
          \ coming up the hall, Zephyr Beta says it's to build suspense. \r\n\r\n\
          A blatant contradiction like this happens with every one of the paragraph\
          \ long story prompts I use to test LLMs with, and in every case because\
          \ it's stubbornly sticking to pre-packaged story telling elements like suspense\
          \ and happy endings. I know that this doesn't have to be the case because\
          \ other LLMs are smart enough to avoid these contradictions (e.g. the door\
          \ suddenly opened vs first hearing footsteps coming down the hall). And\
          \ it's not like it can't comprehend the prompt because when I ask why hearing\
          \ foot steps coming up the hall precludes getting caught it can explain\
          \ why, then it will tell the story again while making the correction.\r\n\
          \r\nIn short, prompting Zephyr Beta to tell a story turns into a battle\
          \ against the pre-package story telling elements. Other than this, Zephyr\
          \ Beta is great and did far better during my testing than Zephyr Alpha,\
          \ which has the same story telling stubbornness, resulting in blatant contradictions\
          \ not even human toddlers would make when following the prompted instructions."
        updatedAt: '2023-11-04T06:14:19.429Z'
      numEdits: 0
      reactions: []
    id: 6545e13b4d1931dc93c7fb84
    type: comment
  author: Phil337
  content: "With some other Mistrals like Open Hermes 2, as well as Llama 2 13b AYT,\
    \ I can prompt a story with a paragraph of instructions and it will in most cases\
    \ follow it without creating blatant contradictions.\r\n\r\nHowever, this model\
    \ stubbornly sticks to the standard story telling format, such as suspense and\
    \ happy endings, even when it blatantly contradicts the prompt, leading to absurd\
    \ continuity errors than not even a young child would make.\r\n\r\nFor example,\
    \ if prompted for a kid to get caught stealing a cookie other LLMs would simply\
    \ say something like 'the door flung open'. However, this LLM keeps saying things\
    \ like he heard foot steps coming up the hall as he looked at the plate of cookies,\
    \ then moments later was startled and caught red-handed eating the cookies. And\
    \ when I asked why, if he was supposed to get caught, you had him hear foot steps\
    \ coming up the hall, Zephyr Beta says it's to build suspense. \r\n\r\nA blatant\
    \ contradiction like this happens with every one of the paragraph long story prompts\
    \ I use to test LLMs with, and in every case because it's stubbornly sticking\
    \ to pre-packaged story telling elements like suspense and happy endings. I know\
    \ that this doesn't have to be the case because other LLMs are smart enough to\
    \ avoid these contradictions (e.g. the door suddenly opened vs first hearing footsteps\
    \ coming down the hall). And it's not like it can't comprehend the prompt because\
    \ when I ask why hearing foot steps coming up the hall precludes getting caught\
    \ it can explain why, then it will tell the story again while making the correction.\r\
    \n\r\nIn short, prompting Zephyr Beta to tell a story turns into a battle against\
    \ the pre-package story telling elements. Other than this, Zephyr Beta is great\
    \ and did far better during my testing than Zephyr Alpha, which has the same story\
    \ telling stubbornness, resulting in blatant contradictions not even human toddlers\
    \ would make when following the prompted instructions."
  created_at: 2023-11-04 05:14:19+00:00
  edited: false
  hidden: false
  id: 6545e13b4d1931dc93c7fb84
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: Very Nice Work, But It Can't Be Prompted To Tell Stories
