!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mhelmy
conflicting_files: null
created_at: 2023-11-12 12:20:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a2ec61b9d2babd75c6a1d4fa61bf780.svg
      fullname: mhelmy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mhelmy
      type: user
    createdAt: '2023-11-12T12:20:11.000Z'
    data:
      edited: false
      editors:
      - mhelmy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5153172612190247
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a2ec61b9d2babd75c6a1d4fa61bf780.svg
          fullname: mhelmy
          isHf: false
          isPro: false
          name: mhelmy
          type: user
        html: "<p>Hello,</p>\n<p>I am an absolute  newb to machine learning and the\
          \ torch/transformers libraries, and I'm trying to run the model on MacOS,\
          \ but I'm getting an error saying that BFloat16 is not supported on MPS.\
          \ Can someone please advise how I can resolve this issue? </p>\n<p>System:</p>\n\
          <pre><code>$ sw_vers\nProductName:\t\tmacOS\nProductVersion:\t\t13.5.1\n\
          BuildVersion:\t\t22G90\n</code></pre>\n<p>Code:</p>\n<pre><code>import torch\n\
          from transformers import pipeline\n\npipe = pipeline(\"text-generation\"\
          , model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"\
          auto\")\n</code></pre>\n<p>Output:</p>\n<pre><code>Loading checkpoint shards:\
          \   0%|                                                                \
          \                                                                      \
          \                                                       | 0/8 [00:00&lt;?,\
          \ ?it/s]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\",\
          \ line 1, in &lt;module&gt;\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
          , line 870, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\n    return model_class.from_pretrained(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3480, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3870, in _load_pretrained_model\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\n               \
          \                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 743, in _load_state_dict_into_meta_model\n    set_module_tensor_to_device(model,\
          \ param_name, param_device, **set_module_kwargs)\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/accelerate/utils/modeling.py\"\
          , line 317, in set_module_tensor_to_device\n    new_value = value.to(device)\n\
          \                ^^^^^^^^^^^^^^^^\nTypeError: BFloat16 is not supported\
          \ on MPS\n</code></pre>\n"
        raw: "Hello,\r\n\r\nI am an absolute  newb to machine learning and the torch/transformers\
          \ libraries, and I'm trying to run the model on MacOS, but I'm getting an\
          \ error saying that BFloat16 is not supported on MPS. Can someone please\
          \ advise how I can resolve this issue? \r\n\r\nSystem:\r\n\r\n```\r\n$ sw_vers\r\
          \nProductName:\t\tmacOS\r\nProductVersion:\t\t13.5.1\r\nBuildVersion:\t\t\
          22G90\r\n```\r\n\r\nCode:\r\n```\r\nimport torch\r\nfrom transformers import\
          \ pipeline\r\n\r\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\"\
          , torch_dtype=torch.bfloat16, device_map=\"auto\")\r\n```\r\n\r\nOutput:\r\
          \n```\r\nLoading checkpoint shards:   0%|                              \
          \                                                                      \
          \                                                                      \
          \                   | 0/8 [00:00<?, ?it/s]\r\nTraceback (most recent call\
          \ last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
          , line 870, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
          \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3480, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3870, in _load_pretrained_model\r\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\r\n             \
          \                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 743, in _load_state_dict_into_meta_model\r\n    set_module_tensor_to_device(model,\
          \ param_name, param_device, **set_module_kwargs)\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/accelerate/utils/modeling.py\"\
          , line 317, in set_module_tensor_to_device\r\n    new_value = value.to(device)\r\
          \n                ^^^^^^^^^^^^^^^^\r\nTypeError: BFloat16 is not supported\
          \ on MPS\r\n```"
        updatedAt: '2023-11-12T12:20:11.621Z'
      numEdits: 0
      reactions: []
    id: 6550c2fb2382f591ee02eb18
    type: comment
  author: mhelmy
  content: "Hello,\r\n\r\nI am an absolute  newb to machine learning and the torch/transformers\
    \ libraries, and I'm trying to run the model on MacOS, but I'm getting an error\
    \ saying that BFloat16 is not supported on MPS. Can someone please advise how\
    \ I can resolve this issue? \r\n\r\nSystem:\r\n\r\n```\r\n$ sw_vers\r\nProductName:\t\
    \tmacOS\r\nProductVersion:\t\t13.5.1\r\nBuildVersion:\t\t22G90\r\n```\r\n\r\n\
    Code:\r\n```\r\nimport torch\r\nfrom transformers import pipeline\r\n\r\npipe\
    \ = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16,\
    \ device_map=\"auto\")\r\n```\r\n\r\nOutput:\r\n```\r\nLoading checkpoint shards:\
    \   0%|                                                                      \
    \                                                                            \
    \                                           | 0/8 [00:00<?, ?it/s]\r\nTraceback\
    \ (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File\
    \ \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
    , line 870, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
    \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
    \ \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 566, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3480, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n   \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3870, in _load_pretrained_model\r\n    new_error_msgs, offload_index, state_dict_index\
    \ = _load_state_dict_into_meta_model(\r\n                                    \
    \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 743, in _load_state_dict_into_meta_model\r\n    set_module_tensor_to_device(model,\
    \ param_name, param_device, **set_module_kwargs)\r\n  File \"/Users/myself/repos/local/vai/backend/.env/lib/python3.11/site-packages/accelerate/utils/modeling.py\"\
    , line 317, in set_module_tensor_to_device\r\n    new_value = value.to(device)\r\
    \n                ^^^^^^^^^^^^^^^^\r\nTypeError: BFloat16 is not supported on\
    \ MPS\r\n```"
  created_at: 2023-11-12 12:20:11+00:00
  edited: false
  hidden: false
  id: 6550c2fb2382f591ee02eb18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
      fullname: Alvaro Bartolome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvarobartt
      type: user
    createdAt: '2023-11-28T08:01:22.000Z'
    data:
      edited: false
      editors:
      - alvarobartt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6767488718032837
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
          fullname: Alvaro Bartolome
          isHf: false
          isPro: false
          name: alvarobartt
          type: user
        html: '<p>Hey! Just change your snippet with</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          pipeline


          pipe = pipeline(<span class="hljs-string">"text-generation"</span>, model=<span
          class="hljs-string">"HuggingFaceH4/zephyr-7b-beta"</span>, torch_dtype=torch.float16,
          device_map=<span class="hljs-string">"auto"</span>)

          </code></pre>

          <p>Or just use a quantized version of the model via <code>llama-cpp-python</code>
          as it will run faster (q5_0, q5_k_s, q5_k_m, q4_k_s, q4_k_m) variants recommended,
          find all the quantized models at <a href="https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/tree/main">https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/tree/main</a></p>

          '
        raw: 'Hey! Just change your snippet with


          ```python

          import torch

          from transformers import pipeline


          pipe = pipeline("text-generation", model="HuggingFaceH4/zephyr-7b-beta",
          torch_dtype=torch.float16, device_map="auto")

          ```


          Or just use a quantized version of the model via `llama-cpp-python` as it
          will run faster (q5_0, q5_k_s, q5_k_m, q4_k_s, q4_k_m) variants recommended,
          find all the quantized models at https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/tree/main'
        updatedAt: '2023-11-28T08:01:22.685Z'
      numEdits: 0
      reactions: []
    id: 65659e525de5b89145bd05bf
    type: comment
  author: alvarobartt
  content: 'Hey! Just change your snippet with


    ```python

    import torch

    from transformers import pipeline


    pipe = pipeline("text-generation", model="HuggingFaceH4/zephyr-7b-beta", torch_dtype=torch.float16,
    device_map="auto")

    ```


    Or just use a quantized version of the model via `llama-cpp-python` as it will
    run faster (q5_0, q5_k_s, q5_k_m, q4_k_s, q4_k_m) variants recommended, find all
    the quantized models at https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/tree/main'
  created_at: 2023-11-28 08:01:22+00:00
  edited: false
  hidden: false
  id: 65659e525de5b89145bd05bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56ff3e836ef48b70911bee1dc3be0a7a.svg
      fullname: Venkat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 010gvr
      type: user
    createdAt: '2023-11-28T11:08:23.000Z'
    data:
      edited: false
      editors:
      - 010gvr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8525804877281189
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56ff3e836ef48b70911bee1dc3be0a7a.svg
          fullname: Venkat
          isHf: false
          isPro: false
          name: 010gvr
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mhelmy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mhelmy\">@<span class=\"\
          underline\">mhelmy</span></a></span>\n\n\t</span></span> </p>\n<p>M1 doesn't\
          \ support BFloat16. Interestingly, M2 does. To workaround, on top of the\
          \ suggestion from <span data-props=\"{&quot;user&quot;:&quot;alvarobartt&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alvarobartt\"\
          >@<span class=\"underline\">alvarobartt</span></a></span>\n\n\t</span></span>\
          \ , also run <code>accelerate config</code> to configure the option for\
          \  <em>mixed_precision</em>  to fp16. Verify by running <code>accelerate\
          \ env</code> and then try executing the code.</p>\n"
        raw: "@mhelmy \n\nM1 doesn't support BFloat16. Interestingly, M2 does. To\
          \ workaround, on top of the suggestion from @alvarobartt , also run `accelerate\
          \ config` to configure the option for  _mixed_precision_  to fp16. Verify\
          \ by running `accelerate env` and then try executing the code."
        updatedAt: '2023-11-28T11:08:23.225Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - alvarobartt
    id: 6565ca27e67d21ed94b9e27e
    type: comment
  author: 010gvr
  content: "@mhelmy \n\nM1 doesn't support BFloat16. Interestingly, M2 does. To workaround,\
    \ on top of the suggestion from @alvarobartt , also run `accelerate config` to\
    \ configure the option for  _mixed_precision_  to fp16. Verify by running `accelerate\
    \ env` and then try executing the code."
  created_at: 2023-11-28 11:08:23+00:00
  edited: false
  hidden: false
  id: 6565ca27e67d21ed94b9e27e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: BFloat16 is not supported on MPS
