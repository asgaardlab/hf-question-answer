!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Idkkitsune
conflicting_files: null
created_at: 2023-12-27 17:04:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcecf75607513a6621daadf1333702f0.svg
      fullname: I Don't Know
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Idkkitsune
      type: user
    createdAt: '2023-12-27T17:04:59.000Z'
    data:
      edited: false
      editors:
      - Idkkitsune
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4124743640422821
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcecf75607513a6621daadf1333702f0.svg
          fullname: I Don't Know
          isHf: false
          isPro: false
          name: Idkkitsune
          type: user
        html: "<p>Modified due to<br>UserWarning: You have modified the pretrained\
          \ model configuration to control generation. This is a deprecated strategy\
          \ to control generation and will be removed soon, in a future version. Please\
          \ use and modify the model generation configuration (see <a href=\"https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\"\
          >https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration</a>\
          \ )</p>\n<p>but this happens to the original version too </p>\n<p>The Code:</p>\n\
          <pre><code class=\"language-import\">from transformers import pipeline,\
          \ GenerationConfig\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\"\
          , torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# Create a GenerationConfig\
          \ instance with your desired settings\ngeneration_config = GenerationConfig(\n\
          \   max_new_tokens=2, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n\
          )\n\n# Use the tokenizer's chat template to format each message\nmessages\
          \ = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You\
          \ are a friendly chatbot who always responds in the style of a pirate\"\
          ,\n    },\n    {\"role\": \"user\", \"content\": \"Say Hi!\"},\n]\nprompt\
          \ = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
          print(\"Done\")\noutputs = pipe(prompt, generation_config=generation_config)\n\
          print(outputs[0][\"generated_text\"])\n</code></pre>\n<p>For some reason\
          \ it gets stuck at this specific line \"outputs = pipe(prompt, generation_config=generation_config)\"\
          , and generates the response after 30 minutes or so</p>\n<p>If I ctrl+c,\
          \ I get this</p>\n<p><code>Traceback (most recent call last):   File \"\
          c:/Users/Home/Desktop/testingenv/something.py\", line 21, in &lt;module&gt;\
          \     outputs = pipe(prompt, generation_config=generation_config)   File\
          \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
          pipelines\\text_generation.py\", line 208, in __call__     return super().__call__(text_inputs,\
          \ **kwargs)   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\transformers\\pipelines\\base.py\", line 1140, in __call__\
          \     return self.run_single(inputs, preprocess_params, forward_params,\
          \ postprocess_params)   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\\
          lib\\site-packages\\transformers\\pipelines\\base.py\", line 1147, in run_single\
          \     model_outputs = self.forward(model_inputs, **forward_params)   File\
          \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
          pipelines\\base.py\", line 1046, in forward     model_outputs = self._forward(model_inputs,\
          \ **forward_params)   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\\
          lib\\site-packages\\transformers\\pipelines\\text_generation.py\", line\
          \ 271, in _forward     generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)   File \"c:\\Users\\\
          Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
          , line 115, in decorate_context     return func(*args, **kwargs)   File\
          \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1777, in generate     return self.sample( \
          \  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 2874, in sample     outputs =\
          \ self(   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl     return\
          \ self._call_impl(*args, **kwargs)   File \"c:\\Users\\Home\\Desktop\\testingenv\\\
          myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in\
          \ _call_impl     return forward_call(*args, **kwargs)   File \"c:\\Users\\\
          Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\accelerate\\hooks.py\"\
          , line 165, in new_forward     output = module._old_forward(*args, **kwargs)\
          \   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          transformers\\models\\mistral\\modeling_mistral.py\", line 1154, in forward\
          \     outputs = self.model(   File \"c:\\Users\\Home\\Desktop\\testingenv\\\
          myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in\
          \ _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File\
          \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1527, in _call_impl     return forward_call(*args,\
          \ **kwargs)   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line\
          \ 1039, in forward     layer_outputs = decoder_layer(   File \"c:\\Users\\\
          Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1518, in _wrapped_call_impl     return self._call_impl(*args,\
          \ **kwargs)   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\
          \     return forward_call(*args, **kwargs)   File \"c:\\Users\\Home\\Desktop\\\
          testingenv\\myenv\\lib\\site-packages\\accelerate\\hooks.py\", line 165,\
          \ in new_forward     output = module._old_forward(*args, **kwargs)   File\
          \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
          models\\mistral\\modeling_mistral.py\", line 754, in forward     hidden_states,\
          \ self_attn_weights, present_key_value = self.self_attn(   File \"c:\\Users\\\
          Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1518, in _wrapped_call_impl     return self._call_impl(*args,\
          \ **kwargs)   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\
          \     return forward_call(*args, **kwargs)   File \"c:\\Users\\Home\\Desktop\\\
          testingenv\\myenv\\lib\\site-packages\\accelerate\\hooks.py\", line 165,\
          \ in new_forward     output = module._old_forward(*args, **kwargs)   File\
          \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
          models\\mistral\\modeling_mistral.py\", line 652, in forward     value_states\
          \ = self.v_proj(hidden_states)   File \"c:\\Users\\Home\\Desktop\\testingenv\\\
          myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in\
          \ _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File\
          \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1527, in _call_impl     return forward_call(*args,\
          \ **kwargs)   File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\accelerate\\hooks.py\", line 165, in new_forward     output\
          \ = module._old_forward(*args, **kwargs)   File \"c:\\Users\\Home\\Desktop\\\
          testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\",\
          \ line 114, in forward     return F.linear(input, self.weight, self.bias)</code></p>\n\
          <p>Is there any way to solve this, am quite new, first of all it doesn't\
          \ give an error , its just like taking a long time to run. Thank you</p>\n"
        raw: "Modified due to \r\nUserWarning: You have modified the pretrained model\
          \ configuration to control generation. This is a deprecated strategy to\
          \ control generation and will be removed soon, in a future version. Please\
          \ use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
          \ )\r\n\r\nbut this happens to the original version too \r\n\r\nThe Code:\r\
          \n```import torch\r\nfrom transformers import pipeline, GenerationConfig\r\
          \n\r\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\"\
          , torch_dtype=torch.bfloat16, device_map=\"auto\")\r\n\r\n# Create a GenerationConfig\
          \ instance with your desired settings\r\ngeneration_config = GenerationConfig(\r\
          \n   max_new_tokens=2, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\r\
          \n)\r\n\r\n# Use the tokenizer's chat template to format each message\r\n\
          messages = [\r\n    {\r\n        \"role\": \"system\",\r\n        \"content\"\
          : \"You are a friendly chatbot who always responds in the style of a pirate\"\
          ,\r\n    },\r\n    {\"role\": \"user\", \"content\": \"Say Hi!\"},\r\n]\r\
          \nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False,\
          \ add_generation_prompt=True)\r\nprint(\"Done\")\r\noutputs = pipe(prompt,\
          \ generation_config=generation_config)\r\nprint(outputs[0][\"generated_text\"\
          ])\r\n```\r\n\r\nFor some reason it gets stuck at this specific line \"\
          outputs = pipe(prompt, generation_config=generation_config)\", and generates\
          \ the response after 30 minutes or so\r\n\r\nIf I ctrl+c, I get this\r\n\
          \r\n```Traceback (most recent call last):\r\n  File \"c:/Users/Home/Desktop/testingenv/something.py\"\
          , line 21, in <module>\r\n    outputs = pipe(prompt, generation_config=generation_config)\r\
          \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          transformers\\pipelines\\text_generation.py\", line 208, in __call__\r\n\
          \    return super().__call__(text_inputs, **kwargs)\r\n  File \"c:\\Users\\\
          Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\pipelines\\\
          base.py\", line 1140, in __call__\r\n    return self.run_single(inputs,\
          \ preprocess_params, forward_params, postprocess_params)\r\n  File \"c:\\\
          Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
          pipelines\\base.py\", line 1147, in run_single\r\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\\
          lib\\site-packages\\transformers\\pipelines\\base.py\", line 1046, in forward\r\
          \n    model_outputs = self._forward(model_inputs, **forward_params)\r\n\
          \  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          transformers\\pipelines\\text_generation.py\", line 271, in _forward\r\n\
          \    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,\
          \ **generate_kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\\
          myenv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in\
          \ decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"c:\\\
          Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1777, in generate\r\n    return self.sample(\r\
          \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 2874, in sample\r\n    outputs\
          \ = self(\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\
          \n    return self._call_impl(*args, **kwargs)\r\n  File \"c:\\Users\\Home\\\
          Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
          \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line\
          \ 1154, in forward\r\n    outputs = self.model(\r\n  File \"c:\\Users\\\
          Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\
          \n    return forward_call(*args, **kwargs)\r\n  File \"c:\\Users\\Home\\\
          Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\models\\mistral\\\
          modeling_mistral.py\", line 1039, in forward\r\n    layer_outputs = decoder_layer(\r\
          \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n  \
          \  return self._call_impl(*args, **kwargs)\r\n  File \"c:\\Users\\Home\\\
          Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
          \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line\
          \ 754, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\
          \n    return self._call_impl(*args, **kwargs)\r\n  File \"c:\\Users\\Home\\\
          Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
          \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line\
          \ 652, in forward\r\n    value_states = self.v_proj(hidden_states)\r\n \
          \ File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n  \
          \  return self._call_impl(*args, **kwargs)\r\n  File \"c:\\Users\\Home\\\
          Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
          \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\\
          site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\r\n\
          \    return F.linear(input, self.weight, self.bias)```\r\n\r\nIs there any\
          \ way to solve this, am quite new, first of all it doesn't give an error\
          \ , its just like taking a long time to run. Thank you"
        updatedAt: '2023-12-27T17:04:59.946Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nicklikets
    id: 658c593b29ef008a12a1a81e
    type: comment
  author: Idkkitsune
  content: "Modified due to \r\nUserWarning: You have modified the pretrained model\
    \ configuration to control generation. This is a deprecated strategy to control\
    \ generation and will be removed soon, in a future version. Please use and modify\
    \ the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
    \ )\r\n\r\nbut this happens to the original version too \r\n\r\nThe Code:\r\n\
    ```import torch\r\nfrom transformers import pipeline, GenerationConfig\r\n\r\n\
    pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16,\
    \ device_map=\"auto\")\r\n\r\n# Create a GenerationConfig instance with your desired\
    \ settings\r\ngeneration_config = GenerationConfig(\r\n   max_new_tokens=2, do_sample=True,\
    \ temperature=0.7, top_k=50, top_p=0.95\r\n)\r\n\r\n# Use the tokenizer's chat\
    \ template to format each message\r\nmessages = [\r\n    {\r\n        \"role\"\
    : \"system\",\r\n        \"content\": \"You are a friendly chatbot who always\
    \ responds in the style of a pirate\",\r\n    },\r\n    {\"role\": \"user\", \"\
    content\": \"Say Hi!\"},\r\n]\r\nprompt = pipe.tokenizer.apply_chat_template(messages,\
    \ tokenize=False, add_generation_prompt=True)\r\nprint(\"Done\")\r\noutputs =\
    \ pipe(prompt, generation_config=generation_config)\r\nprint(outputs[0][\"generated_text\"\
    ])\r\n```\r\n\r\nFor some reason it gets stuck at this specific line \"outputs\
    \ = pipe(prompt, generation_config=generation_config)\", and generates the response\
    \ after 30 minutes or so\r\n\r\nIf I ctrl+c, I get this\r\n\r\n```Traceback (most\
    \ recent call last):\r\n  File \"c:/Users/Home/Desktop/testingenv/something.py\"\
    , line 21, in <module>\r\n    outputs = pipe(prompt, generation_config=generation_config)\r\
    \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
    pipelines\\text_generation.py\", line 208, in __call__\r\n    return super().__call__(text_inputs,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    transformers\\pipelines\\base.py\", line 1140, in __call__\r\n    return self.run_single(inputs,\
    \ preprocess_params, forward_params, postprocess_params)\r\n  File \"c:\\Users\\\
    Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\pipelines\\\
    base.py\", line 1147, in run_single\r\n    model_outputs = self.forward(model_inputs,\
    \ **forward_params)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\\
    lib\\site-packages\\transformers\\pipelines\\base.py\", line 1046, in forward\r\
    \n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File\
    \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
    pipelines\\text_generation.py\", line 271, in _forward\r\n    generated_sequence\
    \ = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\
    \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\\
    utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    transformers\\generation\\utils.py\", line 1777, in generate\r\n    return self.sample(\r\
    \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 2874, in sample\r\n    outputs = self(\r\n  File\
    \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\\
    modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    transformers\\models\\mistral\\modeling_mistral.py\", line 1154, in forward\r\n\
    \    outputs = self.model(\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\\
    myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\
    \n    return self._call_impl(*args, **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\\
    testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527,\
    \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"c:\\Users\\\
    Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\transformers\\models\\mistral\\\
    modeling_mistral.py\", line 1039, in forward\r\n    layer_outputs = decoder_layer(\r\
    \n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    transformers\\models\\mistral\\modeling_mistral.py\", line 754, in forward\r\n\
    \    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n\
    \  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    transformers\\models\\mistral\\modeling_mistral.py\", line 652, in forward\r\n\
    \    value_states = self.v_proj(hidden_states)\r\n  File \"c:\\Users\\Home\\Desktop\\\
    testingenv\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518,\
    \ in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File\
    \ \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\torch\\nn\\\
    modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    accelerate\\hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args,\
    \ **kwargs)\r\n  File \"c:\\Users\\Home\\Desktop\\testingenv\\myenv\\lib\\site-packages\\\
    torch\\nn\\modules\\linear.py\", line 114, in forward\r\n    return F.linear(input,\
    \ self.weight, self.bias)```\r\n\r\nIs there any way to solve this, am quite new,\
    \ first of all it doesn't give an error , its just like taking a long time to\
    \ run. Thank you"
  created_at: 2023-12-27 17:04:59+00:00
  edited: false
  hidden: false
  id: 658c593b29ef008a12a1a81e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 46
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: Taking way to long to generate a response
