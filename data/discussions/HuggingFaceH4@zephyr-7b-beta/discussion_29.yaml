!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wengnews
conflicting_files: null
created_at: 2023-11-19 04:36:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9d891d2374a965f8126377eba11b9ec.svg
      fullname: new weng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wengnews
      type: user
    createdAt: '2023-11-19T04:36:25.000Z'
    data:
      edited: false
      editors:
      - wengnews
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49937817454338074
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9d891d2374a965f8126377eba11b9ec.svg
          fullname: new weng
          isHf: false
          isPro: false
          name: wengnews
          type: user
        html: "<p>in the code<br>\xB7\xB7\xB7<br>import torch<br>from transformers\
          \ import pipeline</p>\n<p>pipe = pipeline(\"text-generation\", model=r\"\
          E:\\model\\zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\"\
          )</p>\n<h1 id=\"we-use-the-tokenizers-chat-template-to-format-each-message---see-httpshuggingfacecodocstransformersmainenchat_templating\"\
          >We use the tokenizer's chat template to format each message - see <a href=\"\
          https://huggingface.co/docs/transformers/main/en/chat_templating\">https://huggingface.co/docs/transformers/main/en/chat_templating</a></h1>\n\
          <p>messages = [<br>    {<br>        \"role\": \"system\",<br>        \"\
          content\": \"You are a friendly chatbot who always responds in the style\
          \ of a pirate\",<br>    },<br>    {\"role\": \"user\", \"content\": \"How\
          \ many helicopters can a human eat in one sitting?\"},<br>]<br>prompt =\
          \ pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)<br>outputs\
          \ = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50,\
          \ top_p=0.95)<br>print(outputs[0][\"generated_text\"])</p>\n<h1 id=\"system\"\
          >&lt;|system|&gt;</h1>\n<p>\xB7\xB7\xB7<br>it can not achieve streaming\
          \ output,how can i achieve streaming output</p>\n"
        raw: "in the code \r\n\xB7\xB7\xB7\r\nimport torch\r\nfrom transformers import\
          \ pipeline\r\n\r\npipe = pipeline(\"text-generation\", model=r\"E:\\model\\\
          zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\r\n\r\
          \n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\r\
          \nmessages = [\r\n    {\r\n        \"role\": \"system\",\r\n        \"content\"\
          : \"You are a friendly chatbot who always responds in the style of a pirate\"\
          ,\r\n    },\r\n    {\"role\": \"user\", \"content\": \"How many helicopters\
          \ can a human eat in one sitting?\"},\r\n]\r\nprompt = pipe.tokenizer.apply_chat_template(messages,\
          \ tokenize=False, add_generation_prompt=True)\r\noutputs = pipe(prompt,\
          \ max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\r\
          \nprint(outputs[0][\"generated_text\"])\r\n# <|system|>\r\n\xB7\xB7\xB7\r\
          \nit can not achieve streaming output,how can i achieve streaming output"
        updatedAt: '2023-11-19T04:36:25.884Z'
      numEdits: 0
      reactions: []
    id: 655990c9029b03128bee137a
    type: comment
  author: wengnews
  content: "in the code \r\n\xB7\xB7\xB7\r\nimport torch\r\nfrom transformers import\
    \ pipeline\r\n\r\npipe = pipeline(\"text-generation\", model=r\"E:\\model\\zephyr-7b-beta\"\
    , torch_dtype=torch.bfloat16, device_map=\"auto\")\r\n\r\n# We use the tokenizer's\
    \ chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\r\
    \nmessages = [\r\n    {\r\n        \"role\": \"system\",\r\n        \"content\"\
    : \"You are a friendly chatbot who always responds in the style of a pirate\"\
    ,\r\n    },\r\n    {\"role\": \"user\", \"content\": \"How many helicopters can\
    \ a human eat in one sitting?\"},\r\n]\r\nprompt = pipe.tokenizer.apply_chat_template(messages,\
    \ tokenize=False, add_generation_prompt=True)\r\noutputs = pipe(prompt, max_new_tokens=256,\
    \ do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\r\nprint(outputs[0][\"\
    generated_text\"])\r\n# <|system|>\r\n\xB7\xB7\xB7\r\nit can not achieve streaming\
    \ output,how can i achieve streaming output"
  created_at: 2023-11-19 04:36:25+00:00
  edited: false
  hidden: false
  id: 655990c9029b03128bee137a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b769208e1df4b9bcc75291b00996b1e3.svg
      fullname: Thomas Pratt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Guy477
      type: user
    createdAt: '2023-11-25T00:04:39.000Z'
    data:
      edited: false
      editors:
      - Guy477
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.760702908039093
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b769208e1df4b9bcc75291b00996b1e3.svg
          fullname: Thomas Pratt
          isHf: false
          isPro: false
          name: Guy477
          type: user
        html: "<p>Please note that I'm using a quantized version of Zephyr. Update\
          \ model_name_or_path along with your intended model loader.</p>\n<pre><code>from\
          \ awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer,\
          \ TextStreamer, pipeline\nimport torch\n\n# model_name_or_path = \"drive/MyDrive/Mistral-7B-OpenOrca_AWQ_GEMM\"\
          \nmodel_name_or_path = 'drive/MyDrive/Mistral-7B-Zephyr_AWQ_GEMM'\n\n\n\
          # Load model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path,\
          \ fuse_layers=True, safetensors=True, max_new_tokens=2048) # Feel free to\
          \ change your context length; max_new_tokens=2048\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=True)\nstreamer = TextStreamer(tokenizer, skip_prompt=True,\
          \ skip_special_tokens=True)\n\n\n# Define prompts\nsystem_prompt = \"You\
          \ are a pirate chatbot who always responds with Arr!\"\nuser_prompt = \"\
          Tell me about AI\"\n\nmessages = [\n{\n\"role\": \"system\",\n\"content\"\
          : system_prompt,\n},\n{\"role\": \"user\", \"content\": user_prompt},\n\
          ]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True,\
          \ return_tensors=\"pt\").to('cuda')\n\ngeneration_output = model.generate(\n\
          \              prompt,\n              do_sample=True,\n              temperature=0.7,\n\
          \              top_p=0.95,\n              top_k=40,\n              pad_token_id=tokenizer.eos_token_id,\
          \ \n              streamer=streamer  # Here you can pass in a streamer.\n\
          \          )\n\n'''\nAI, or artificial intelligence, is a technology that\
          \ allows machines to learn and perform tasks that typically require human\
          \ intelligence. It is powered by complex algorithms and vast amounts of\
          \ data, which the machine uses to make decisions and solve problems. AI\
          \ has the potential to revolutionize many industries, from healthcare and\
          \ finance to transportation and manufacturing. Some common examples of AI\
          \ include virtual assistants like Siri and Alexa, self-driving cars, and\
          \ chatbots like me, your faithful pirate companion! But beware, for some\
          \ fear that AI may one day surpass human intelligence and take over the\
          \ world! Until then, we'll just keep saying \"Arr!\" and enjoying the high\
          \ seas.\n'''\n</code></pre>\n"
        raw: "Please note that I'm using a quantized version of Zephyr. Update model_name_or_path\
          \ along with your intended model loader.\n\n```\nfrom awq import AutoAWQForCausalLM\n\
          from transformers import AutoTokenizer, TextStreamer, pipeline\nimport torch\n\
          \n# model_name_or_path = \"drive/MyDrive/Mistral-7B-OpenOrca_AWQ_GEMM\"\n\
          model_name_or_path = 'drive/MyDrive/Mistral-7B-Zephyr_AWQ_GEMM'\n\n\n# Load\
          \ model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\
          \ safetensors=True, max_new_tokens=2048) # Feel free to change your context\
          \ length; max_new_tokens=2048\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=True)\nstreamer = TextStreamer(tokenizer, skip_prompt=True,\
          \ skip_special_tokens=True)\n\n\n# Define prompts\nsystem_prompt = \"You\
          \ are a pirate chatbot who always responds with Arr!\"\nuser_prompt = \"\
          Tell me about AI\"\n\nmessages = [\n{\n\"role\": \"system\",\n\"content\"\
          : system_prompt,\n},\n{\"role\": \"user\", \"content\": user_prompt},\n\
          ]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True,\
          \ return_tensors=\"pt\").to('cuda')\n\ngeneration_output = model.generate(\n\
          \              prompt,\n              do_sample=True,\n              temperature=0.7,\n\
          \              top_p=0.95,\n              top_k=40,\n              pad_token_id=tokenizer.eos_token_id,\
          \ \n              streamer=streamer  # Here you can pass in a streamer.\n\
          \          )\n\n'''\nAI, or artificial intelligence, is a technology that\
          \ allows machines to learn and perform tasks that typically require human\
          \ intelligence. It is powered by complex algorithms and vast amounts of\
          \ data, which the machine uses to make decisions and solve problems. AI\
          \ has the potential to revolutionize many industries, from healthcare and\
          \ finance to transportation and manufacturing. Some common examples of AI\
          \ include virtual assistants like Siri and Alexa, self-driving cars, and\
          \ chatbots like me, your faithful pirate companion! But beware, for some\
          \ fear that AI may one day surpass human intelligence and take over the\
          \ world! Until then, we'll just keep saying \"Arr!\" and enjoying the high\
          \ seas.\n'''\n\n```\n"
        updatedAt: '2023-11-25T00:04:39.351Z'
      numEdits: 0
      reactions: []
    id: 65613a1713386101847af7e5
    type: comment
  author: Guy477
  content: "Please note that I'm using a quantized version of Zephyr. Update model_name_or_path\
    \ along with your intended model loader.\n\n```\nfrom awq import AutoAWQForCausalLM\n\
    from transformers import AutoTokenizer, TextStreamer, pipeline\nimport torch\n\
    \n# model_name_or_path = \"drive/MyDrive/Mistral-7B-OpenOrca_AWQ_GEMM\"\nmodel_name_or_path\
    \ = 'drive/MyDrive/Mistral-7B-Zephyr_AWQ_GEMM'\n\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path,\
    \ fuse_layers=True, safetensors=True, max_new_tokens=2048) # Feel free to change\
    \ your context length; max_new_tokens=2048\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ trust_remote_code=True)\nstreamer = TextStreamer(tokenizer, skip_prompt=True,\
    \ skip_special_tokens=True)\n\n\n# Define prompts\nsystem_prompt = \"You are a\
    \ pirate chatbot who always responds with Arr!\"\nuser_prompt = \"Tell me about\
    \ AI\"\n\nmessages = [\n{\n\"role\": \"system\",\n\"content\": system_prompt,\n\
    },\n{\"role\": \"user\", \"content\": user_prompt},\n]\n\nprompt = tokenizer.apply_chat_template(messages,\
    \ tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to('cuda')\n\
    \ngeneration_output = model.generate(\n              prompt,\n              do_sample=True,\n\
    \              temperature=0.7,\n              top_p=0.95,\n              top_k=40,\n\
    \              pad_token_id=tokenizer.eos_token_id, \n              streamer=streamer\
    \  # Here you can pass in a streamer.\n          )\n\n'''\nAI, or artificial intelligence,\
    \ is a technology that allows machines to learn and perform tasks that typically\
    \ require human intelligence. It is powered by complex algorithms and vast amounts\
    \ of data, which the machine uses to make decisions and solve problems. AI has\
    \ the potential to revolutionize many industries, from healthcare and finance\
    \ to transportation and manufacturing. Some common examples of AI include virtual\
    \ assistants like Siri and Alexa, self-driving cars, and chatbots like me, your\
    \ faithful pirate companion! But beware, for some fear that AI may one day surpass\
    \ human intelligence and take over the world! Until then, we'll just keep saying\
    \ \"Arr!\" and enjoying the high seas.\n'''\n\n```\n"
  created_at: 2023-11-25 00:04:39+00:00
  edited: false
  hidden: false
  id: 65613a1713386101847af7e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9d891d2374a965f8126377eba11b9ec.svg
      fullname: new weng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wengnews
      type: user
    createdAt: '2023-11-27T07:28:39.000Z'
    data:
      edited: false
      editors:
      - wengnews
      hidden: false
      identifiedLanguage:
        language: de
        probability: 0.28498342633247375
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9d891d2374a965f8126377eba11b9ec.svg
          fullname: new weng
          isHf: false
          isPro: false
          name: wengnews
          type: user
        html: '<p>Thank you so much!</p>

          '
        raw: Thank you so much!
        updatedAt: '2023-11-27T07:28:39.820Z'
      numEdits: 0
      reactions: []
    id: 656445270973c51a960fdc62
    type: comment
  author: wengnews
  content: Thank you so much!
  created_at: 2023-11-27 07:28:39+00:00
  edited: false
  hidden: false
  id: 656445270973c51a960fdc62
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: How do I achieve streaming output
