!!python/object:huggingface_hub.community.DiscussionWithDetails
author: timlim123
conflicting_files: null
created_at: 2023-11-20 08:24:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
      fullname: Lim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timlim123
      type: user
    createdAt: '2023-11-20T08:24:52.000Z'
    data:
      edited: false
      editors:
      - timlim123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463034272193909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
          fullname: Lim
          isHf: false
          isPro: false
          name: timlim123
          type: user
        html: '<p>Hi there,</p>

          <p>There are two beta-model that was published which is:</p>

          <ol>

          <li>Full finetuning: <a href="https://huggingface.co/alignment-handbook/zephyr-7b-dpo-full">https://huggingface.co/alignment-handbook/zephyr-7b-dpo-full</a></li>

          <li>LoRA: <a href="https://huggingface.co/alignment-handbook/zephyr-7b-dpo-lora">https://huggingface.co/alignment-handbook/zephyr-7b-dpo-lora</a></li>

          </ol>

          <p>In the paper, it was mentioned:</p>

          <p>"""<br>All models are trained with the AdamW optimizer and no weight
          decay. We did not<br>experiment with parameter-efficient techniques such
          as LoRA (Hu et al., 2021), but expect similar<br>results to hold with these
          methods<br>"""</p>

          <p>Any conclusion regarding this now?</p>

          <p>LoRA and full finetuning training cost differs by quite a bit.  It seems
          that the LoRA model achieve a lower loss in the evaluation set.  I believe
          this model published that everyone is using is based on full-SFT?</p>

          '
        raw: "Hi there,\r\n\r\nThere are two beta-model that was published which is:\r\
          \n\r\n1. Full finetuning: https://huggingface.co/alignment-handbook/zephyr-7b-dpo-full\r\
          \n2. LoRA: https://huggingface.co/alignment-handbook/zephyr-7b-dpo-lora\r\
          \n\r\n\r\nIn the paper, it was mentioned:\r\n\r\n\"\"\"\r\nAll models are\
          \ trained with the AdamW optimizer and no weight decay. We did not\r\nexperiment\
          \ with parameter-efficient techniques such as LoRA (Hu et al., 2021), but\
          \ expect similar\r\nresults to hold with these methods\r\n\"\"\"\r\n\r\n\
          Any conclusion regarding this now?\r\n\r\n\r\nLoRA and full finetuning training\
          \ cost differs by quite a bit.  It seems that the LoRA model achieve a lower\
          \ loss in the evaluation set.  I believe this model published that everyone\
          \ is using is based on full-SFT?\r\n\r\n\r\n\r\n\r\n\r\n"
        updatedAt: '2023-11-20T08:24:52.569Z'
      numEdits: 0
      reactions: []
    id: 655b17d47d187b097e217fd3
    type: comment
  author: timlim123
  content: "Hi there,\r\n\r\nThere are two beta-model that was published which is:\r\
    \n\r\n1. Full finetuning: https://huggingface.co/alignment-handbook/zephyr-7b-dpo-full\r\
    \n2. LoRA: https://huggingface.co/alignment-handbook/zephyr-7b-dpo-lora\r\n\r\n\
    \r\nIn the paper, it was mentioned:\r\n\r\n\"\"\"\r\nAll models are trained with\
    \ the AdamW optimizer and no weight decay. We did not\r\nexperiment with parameter-efficient\
    \ techniques such as LoRA (Hu et al., 2021), but expect similar\r\nresults to\
    \ hold with these methods\r\n\"\"\"\r\n\r\nAny conclusion regarding this now?\r\
    \n\r\n\r\nLoRA and full finetuning training cost differs by quite a bit.  It seems\
    \ that the LoRA model achieve a lower loss in the evaluation set.  I believe this\
    \ model published that everyone is using is based on full-SFT?\r\n\r\n\r\n\r\n\
    \r\n\r\n"
  created_at: 2023-11-20 08:24:52+00:00
  edited: false
  hidden: false
  id: 655b17d47d187b097e217fd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7592afec2e489c47238e8abd5a768e68.svg
      fullname: Alex Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexli-tifin
      type: user
    createdAt: '2023-11-27T17:18:29.000Z'
    data:
      edited: false
      editors:
      - alexli-tifin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9959285855293274
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7592afec2e489c47238e8abd5a768e68.svg
          fullname: Alex Li
          isHf: false
          isPro: false
          name: alexli-tifin
          type: user
        html: '<p>Following this as well. I did see that the preference accuracy is
          significantly less too. </p>

          '
        raw: 'Following this as well. I did see that the preference accuracy is significantly
          less too. '
        updatedAt: '2023-11-27T17:18:29.479Z'
      numEdits: 0
      reactions: []
    id: 6564cf654eb2f552402891e4
    type: comment
  author: alexli-tifin
  content: 'Following this as well. I did see that the preference accuracy is significantly
    less too. '
  created_at: 2023-11-27 17:18:29+00:00
  edited: false
  hidden: false
  id: 6564cf654eb2f552402891e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: Did the LoRa finetuned model end up performing the same compared to full-finetuning?
