!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ryann
conflicting_files: null
created_at: 2023-11-02 11:47:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f87299c48a4eafeac7aca04ab2c68fd7.svg
      fullname: Ryan Grippeling
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ryann
      type: user
    createdAt: '2023-11-02T12:47:56.000Z'
    data:
      edited: false
      editors:
      - Ryann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9511411190032959
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f87299c48a4eafeac7aca04ab2c68fd7.svg
          fullname: Ryan Grippeling
          isHf: false
          isPro: false
          name: Ryann
          type: user
        html: '<p>Amazing job. This is really uncanny. I''m running this on a rtx2070
          and an i5 from years ago on my second computer. This is nuts, I''m getting
          6 tokens per second. This opens up a whole other world of possibilities
          of running local applications for me. Of course it''s not 100% perfect,  but
          what do you expect with a model this size? How incredibly far we''ve gotten.</p>

          '
        raw: Amazing job. This is really uncanny. I'm running this on a rtx2070 and
          an i5 from years ago on my second computer. This is nuts, I'm getting 6
          tokens per second. This opens up a whole other world of possibilities of
          running local applications for me. Of course it's not 100% perfect,  but
          what do you expect with a model this size? How incredibly far we've gotten.
        updatedAt: '2023-11-02T12:47:56.635Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - atstim731
        - oshaw
        - Roy-Shih
        - JJhooww
        - msze
        - stormos251
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - lewtun
        - stormos251
    id: 65439a7cbf8a6039fbee5e8e
    type: comment
  author: Ryann
  content: Amazing job. This is really uncanny. I'm running this on a rtx2070 and
    an i5 from years ago on my second computer. This is nuts, I'm getting 6 tokens
    per second. This opens up a whole other world of possibilities of running local
    applications for me. Of course it's not 100% perfect,  but what do you expect
    with a model this size? How incredibly far we've gotten.
  created_at: 2023-11-02 11:47:56+00:00
  edited: false
  hidden: false
  id: 65439a7cbf8a6039fbee5e8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd05efdf4bd781f0fda8e5df082cd59c.svg
      fullname: Lucas Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucastan
      type: user
    createdAt: '2023-11-06T18:17:57.000Z'
    data:
      edited: false
      editors:
      - lucastan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8904718160629272
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd05efdf4bd781f0fda8e5df082cd59c.svg
          fullname: Lucas Tan
          isHf: false
          isPro: false
          name: lucastan
          type: user
        html: '<p>Hey Ryann, would you mind sharing how you did that?<br>I am trying
          on RTX3070 but failed with error</p>

          <p>Some modules are dispatched on the CPU or the disk. Make sure you have
          enough GPU RAM to fit<br>                        the quantized model. If
          you want to dispatch the model on the CPU or the disk while keeping<br>                        these
          modules in 32-bit, you need to set <code>load_in_8bit_fp32_cpu_offload=True</code>
          and pass a custom<br>                        <code>device_map</code> to
          <code>from_pretrained</code>. Check<br>                        <a href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu">https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu</a><br>                        for
          more details.</p>

          '
        raw: "Hey Ryann, would you mind sharing how you did that?\nI am trying on\
          \ RTX3070 but failed with error\n\nSome modules are dispatched on the CPU\
          \ or the disk. Make sure you have enough GPU RAM to fit\n              \
          \          the quantized model. If you want to dispatch the model on the\
          \ CPU or the disk while keeping\n                        these modules in\
          \ 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass\
          \ a custom\n                        `device_map` to `from_pretrained`. Check\n\
          \                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \                        for more details."
        updatedAt: '2023-11-06T18:17:57.598Z'
      numEdits: 0
      reactions: []
    id: 65492dd50e40047ab823017f
    type: comment
  author: lucastan
  content: "Hey Ryann, would you mind sharing how you did that?\nI am trying on RTX3070\
    \ but failed with error\n\nSome modules are dispatched on the CPU or the disk.\
    \ Make sure you have enough GPU RAM to fit\n                        the quantized\
    \ model. If you want to dispatch the model on the CPU or the disk while keeping\n\
    \                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
    \ and pass a custom\n                        `device_map` to `from_pretrained`.\
    \ Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
    \                        for more details."
  created_at: 2023-11-06 18:17:57+00:00
  edited: false
  hidden: false
  id: 65492dd50e40047ab823017f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd05efdf4bd781f0fda8e5df082cd59c.svg
      fullname: Lucas Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucastan
      type: user
    createdAt: '2023-11-06T18:18:46.000Z'
    data:
      edited: false
      editors:
      - lucastan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9893251061439514
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd05efdf4bd781f0fda8e5df082cd59c.svg
          fullname: Lucas Tan
          isHf: false
          isPro: false
          name: lucastan
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Ryann&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Ryann\">@<span class=\"\
          underline\">Ryann</span></a></span>\n\n\t</span></span> How much GPU RAM\
          \ it took?</p>\n"
        raw: '@Ryann How much GPU RAM it took?'
        updatedAt: '2023-11-06T18:18:46.843Z'
      numEdits: 0
      reactions: []
    id: 65492e063ee6a84ff2d9e165
    type: comment
  author: lucastan
  content: '@Ryann How much GPU RAM it took?'
  created_at: 2023-11-06 18:18:46+00:00
  edited: false
  hidden: false
  id: 65492e063ee6a84ff2d9e165
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: I just wanna thank everyone that worked on this
