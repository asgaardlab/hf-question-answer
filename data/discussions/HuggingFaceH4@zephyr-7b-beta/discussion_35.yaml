!!python/object:huggingface_hub.community.DiscussionWithDetails
author: model-sizer-bot
conflicting_files: null
created_at: 2023-11-24 00:54:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cedfda2c9719121a4f3e44c650636650.svg
      fullname: Model Sizer Bot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: model-sizer-bot
      type: user
    createdAt: '2023-11-24T00:54:48.000Z'
    data:
      edited: false
      editors:
      - model-sizer-bot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7288241386413574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cedfda2c9719121a4f3e44c650636650.svg
          fullname: Model Sizer Bot
          isHf: false
          isPro: false
          name: model-sizer-bot
          type: user
        html: "<h1 id=\"model-memory-requirements\">Model Memory Requirements</h1>\n\
          <p>You will need about {'dtype': 'float16/bfloat16', 'Largest Layer or Residual\
          \ Group': '432.02 MB', 'Total Size': '13.74 GB', 'Training using Adam':\
          \ '54.98 GB'} VRAM to load this model for inference, and {'dtype': 'int4',\
          \ 'Largest Layer or Residual Group': '108.0 MB', 'Total Size': '3.44 GB',\
          \ 'Training using Adam': '13.74 GB'} VRAM to train it using Adam.</p>\n\
          <p>These calculations were measured from the <a rel=\"nofollow\" href=\"\
          https://hf.co/spaces/hf-accelerate/model-memory-utility\">Model Memory Utility\
          \ Space</a> on the Hub.</p>\n<p>The minimum recommended vRAM needed for\
          \ this model assumes using <a href=\"https://huggingface.co/docs/accelerate/usage_guides/big_modeling\"\
          >Accelerate or <code>device_map=\"auto\"</code></a> and is denoted by the\
          \ size of the \"largest layer\".<br>When performing inference, expect to\
          \ add up to an additional 20% to this, as found by <a rel=\"nofollow\" href=\"\
          https://blog.eleuther.ai/transformer-math/\">EleutherAI</a>. More tests\
          \ will be performed in the future to get a more accurate benchmark for each\
          \ model.</p>\n<p>When training with <code>Adam</code>, you can expect roughly\
          \ 4x the reported results to be used. (1x for the model, 1x for the gradients,\
          \ and 2x for the optimizer).</p>\n<h2 id=\"results\">Results:</h2>\n<div\
          \ class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th align=\"\
          left\">dtype</th>\n<th align=\"left\">Largest Layer or Residual Group</th>\n\
          <th align=\"left\">Total Size</th>\n<th align=\"left\">Training using Adam</th>\n\
          </tr>\n\n\t\t</thead><tbody><tr>\n<td align=\"left\">float32</td>\n<td align=\"\
          left\">864.03 MB</td>\n<td align=\"left\">27.49 GB</td>\n<td align=\"left\"\
          >109.96 GB</td>\n</tr>\n<tr>\n<td align=\"left\">float16/bfloat16</td>\n\
          <td align=\"left\">432.02 MB</td>\n<td align=\"left\">13.74 GB</td>\n<td\
          \ align=\"left\">54.98 GB</td>\n</tr>\n<tr>\n<td align=\"left\">int8</td>\n\
          <td align=\"left\">216.01 MB</td>\n<td align=\"left\">6.87 GB</td>\n<td\
          \ align=\"left\">27.49 GB</td>\n</tr>\n<tr>\n<td align=\"left\">int4</td>\n\
          <td align=\"left\">108.0 MB</td>\n<td align=\"left\">3.44 GB</td>\n<td align=\"\
          left\">13.74 GB</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: "# Model Memory Requirements\n\n\nYou will need about {'dtype': 'float16/bfloat16',\
          \ 'Largest Layer or Residual Group': '432.02 MB', 'Total Size': '13.74 GB',\
          \ 'Training using Adam': '54.98 GB'} VRAM to load this model for inference,\
          \ and {'dtype': 'int4', 'Largest Layer or Residual Group': '108.0 MB', 'Total\
          \ Size': '3.44 GB', 'Training using Adam': '13.74 GB'} VRAM to train it\
          \ using Adam.\n    \nThese calculations were measured from the [Model Memory\
          \ Utility Space](https://hf.co/spaces/hf-accelerate/model-memory-utility)\
          \ on the Hub.\n    \nThe minimum recommended vRAM needed for this model\
          \ assumes using [Accelerate or `device_map=\"auto\"`](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\
          \ and is denoted by the size of the \"largest layer\". \nWhen performing\
          \ inference, expect to add up to an additional 20% to this, as found by\
          \ [EleutherAI](https://blog.eleuther.ai/transformer-math/). More tests will\
          \ be performed in the future to get a more accurate benchmark for each model.\n\
          \nWhen training with `Adam`, you can expect roughly 4x the reported results\
          \ to be used. (1x for the model, 1x for the gradients, and 2x for the optimizer).\n\
          \n## Results:\n\n| dtype            | Largest Layer or Residual Group  \
          \ | Total Size   | Training using Adam   |\n|:-----------------|:----------------------------------|:-------------|:----------------------|\n\
          | float32          | 864.03 MB                         | 27.49 GB     |\
          \ 109.96 GB             |\n| float16/bfloat16 | 432.02 MB              \
          \           | 13.74 GB     | 54.98 GB              |\n| int8           \
          \  | 216.01 MB                         | 6.87 GB      | 27.49 GB       \
          \       |\n| int4             | 108.0 MB                          | 3.44\
          \ GB      | 13.74 GB              |"
        updatedAt: '2023-11-24T00:54:48.747Z'
      numEdits: 0
      reactions: []
    id: 655ff45822ce47e5fa5283ba
    type: comment
  author: model-sizer-bot
  content: "# Model Memory Requirements\n\n\nYou will need about {'dtype': 'float16/bfloat16',\
    \ 'Largest Layer or Residual Group': '432.02 MB', 'Total Size': '13.74 GB', 'Training\
    \ using Adam': '54.98 GB'} VRAM to load this model for inference, and {'dtype':\
    \ 'int4', 'Largest Layer or Residual Group': '108.0 MB', 'Total Size': '3.44 GB',\
    \ 'Training using Adam': '13.74 GB'} VRAM to train it using Adam.\n    \nThese\
    \ calculations were measured from the [Model Memory Utility Space](https://hf.co/spaces/hf-accelerate/model-memory-utility)\
    \ on the Hub.\n    \nThe minimum recommended vRAM needed for this model assumes\
    \ using [Accelerate or `device_map=\"auto\"`](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\
    \ and is denoted by the size of the \"largest layer\". \nWhen performing inference,\
    \ expect to add up to an additional 20% to this, as found by [EleutherAI](https://blog.eleuther.ai/transformer-math/).\
    \ More tests will be performed in the future to get a more accurate benchmark\
    \ for each model.\n\nWhen training with `Adam`, you can expect roughly 4x the\
    \ reported results to be used. (1x for the model, 1x for the gradients, and 2x\
    \ for the optimizer).\n\n## Results:\n\n| dtype            | Largest Layer or\
    \ Residual Group   | Total Size   | Training using Adam   |\n|:-----------------|:----------------------------------|:-------------|:----------------------|\n\
    | float32          | 864.03 MB                         | 27.49 GB     | 109.96\
    \ GB             |\n| float16/bfloat16 | 432.02 MB                         | 13.74\
    \ GB     | 54.98 GB              |\n| int8             | 216.01 MB           \
    \              | 6.87 GB      | 27.49 GB              |\n| int4             |\
    \ 108.0 MB                          | 3.44 GB      | 13.74 GB              |"
  created_at: 2023-11-24 00:54:48+00:00
  edited: false
  hidden: false
  id: 655ff45822ce47e5fa5283ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/88a129d4512dcfd52fa092d1b450a79c.svg
      fullname: Junhong Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JohnCHENMEL
      type: user
    createdAt: '2023-11-28T12:48:04.000Z'
    data:
      edited: false
      editors:
      - JohnCHENMEL
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4454692602157593
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/88a129d4512dcfd52fa092d1b450a79c.svg
          fullname: Junhong Chen
          isHf: false
          isPro: false
          name: JohnCHENMEL
          type: user
        html: '<p>model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-alpha",
          trust_remote_code=True, torch_dtype=torch.int8)<br>tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha",
          trust_remote_code=True, torch_dtype=torch.int8)<br>When I change float16
          to int8, the model can not run ,do you know why?</p>

          '
        raw: 'model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-alpha",
          trust_remote_code=True, torch_dtype=torch.int8)

          tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha",
          trust_remote_code=True, torch_dtype=torch.int8)

          When I change float16 to int8, the model can not run ,do you know why?'
        updatedAt: '2023-11-28T12:48:04.774Z'
      numEdits: 0
      reactions: []
    id: 6565e1848556d5bbc2ba0334
    type: comment
  author: JohnCHENMEL
  content: 'model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-alpha",
    trust_remote_code=True, torch_dtype=torch.int8)

    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha", trust_remote_code=True,
    torch_dtype=torch.int8)

    When I change float16 to int8, the model can not run ,do you know why?'
  created_at: 2023-11-28 12:48:04+00:00
  edited: false
  hidden: false
  id: 6565e1848556d5bbc2ba0334
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594174616206-5f05297d5d08220171a0ad7d.png?w=200&h=200&f=face
      fullname: Zachary Mueller
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muellerzr
      type: user
    createdAt: '2023-11-28T14:04:16.000Z'
    data:
      edited: false
      editors:
      - muellerzr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9437872171401978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594174616206-5f05297d5d08220171a0ad7d.png?w=200&h=200&f=face
          fullname: Zachary Mueller
          isHf: true
          isPro: false
          name: muellerzr
          type: user
        html: '<p>Define can not run? Can we get a full trace/what it states for you?
          </p>

          '
        raw: 'Define can not run? Can we get a full trace/what it states for you? '
        updatedAt: '2023-11-28T14:04:16.588Z'
      numEdits: 0
      reactions: []
    id: 6565f3601cd377ae60b63052
    type: comment
  author: muellerzr
  content: 'Define can not run? Can we get a full trace/what it states for you? '
  created_at: 2023-11-28 14:04:16+00:00
  edited: false
  hidden: false
  id: 6565f3601cd377ae60b63052
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/88a129d4512dcfd52fa092d1b450a79c.svg
      fullname: Junhong Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JohnCHENMEL
      type: user
    createdAt: '2023-11-29T02:54:39.000Z'
    data:
      edited: false
      editors:
      - JohnCHENMEL
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6635720133781433
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/88a129d4512dcfd52fa092d1b450a79c.svg
          fullname: Junhong Chen
          isHf: false
          isPro: false
          name: JohnCHENMEL
          type: user
        html: '<p>When I run the following code on colab:</p>

          <p>!pip install transformers<br>import torch<br>import transformers<br>from
          transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>torch.set_default_device("cuda")<br>model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-beta",
          trust_remote_code=True, torch_dtype=torch.float16)<br>tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta",
          trust_remote_code=True, torch_dtype=torch.float16)<br>inputs = tokenizer("do
          you know the difference btween meter and metre?", return_tensors="pt", return_attention_mask=True)<br>outputs
          = model.generate(**inputs, max_length=100, num_beams=1, num_return_sequences=1)<br>text
          = tokenizer.batch_decode(outputs)[0]<br>print(text)<br>torch.cuda.empty_cache()</p>

          <p>system said:<br>OutOfMemoryError: CUDA out of memory. Tried to allocate
          112.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 30.81 MiB is
          free. Process 2098 has 14.71 GiB memory in use. Of the allocated memory
          14.45 GiB is allocated by PyTorch, and 153.47 MiB is reserved by PyTorch
          but unallocated. If reserved but unallocated memory is large try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>I have two questions:<br>1,20days ago,I can run on colab properly,but
          now it show outof memoryerror;<br>2,when I change torch.float16 to torch.int8,
          sysytem said:<br>ValueError: Can''t instantiate MistralForCausalLM model
          under dtype=torch.int8 since it is not a floating point dtype</p>

          '
        raw: 'When I run the following code on colab:


          !pip install transformers

          import torch

          import transformers

          from transformers import AutoModelForCausalLM, AutoTokenizer


          torch.set_default_device("cuda")

          model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-beta",
          trust_remote_code=True, torch_dtype=torch.float16)

          tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta",
          trust_remote_code=True, torch_dtype=torch.float16)

          inputs = tokenizer("do you know the difference btween meter and metre?",
          return_tensors="pt", return_attention_mask=True)

          outputs = model.generate(**inputs, max_length=100, num_beams=1, num_return_sequences=1)

          text = tokenizer.batch_decode(outputs)[0]

          print(text)

          torch.cuda.empty_cache()


          system said:

          OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU
          0 has a total capacty of 14.75 GiB of which 30.81 MiB is free. Process 2098
          has 14.71 GiB memory in use. Of the allocated memory 14.45 GiB is allocated
          by PyTorch, and 153.47 MiB is reserved by PyTorch but unallocated. If reserved
          but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


          I have two questions:

          1,20days ago,I can run on colab properly,but now it show outof memoryerror;

          2,when I change torch.float16 to torch.int8, sysytem said:

          ValueError: Can''t instantiate MistralForCausalLM model under dtype=torch.int8
          since it is not a floating point dtype'
        updatedAt: '2023-11-29T02:54:39.011Z'
      numEdits: 0
      reactions: []
    id: 6566a7ef295a50cb58a252f4
    type: comment
  author: JohnCHENMEL
  content: 'When I run the following code on colab:


    !pip install transformers

    import torch

    import transformers

    from transformers import AutoModelForCausalLM, AutoTokenizer


    torch.set_default_device("cuda")

    model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-beta", trust_remote_code=True,
    torch_dtype=torch.float16)

    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta", trust_remote_code=True,
    torch_dtype=torch.float16)

    inputs = tokenizer("do you know the difference btween meter and metre?", return_tensors="pt",
    return_attention_mask=True)

    outputs = model.generate(**inputs, max_length=100, num_beams=1, num_return_sequences=1)

    text = tokenizer.batch_decode(outputs)[0]

    print(text)

    torch.cuda.empty_cache()


    system said:

    OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has
    a total capacty of 14.75 GiB of which 30.81 MiB is free. Process 2098 has 14.71
    GiB memory in use. Of the allocated memory 14.45 GiB is allocated by PyTorch,
    and 153.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated
    memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation
    for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    I have two questions:

    1,20days ago,I can run on colab properly,but now it show outof memoryerror;

    2,when I change torch.float16 to torch.int8, sysytem said:

    ValueError: Can''t instantiate MistralForCausalLM model under dtype=torch.int8
    since it is not a floating point dtype'
  created_at: 2023-11-29 02:54:39+00:00
  edited: false
  hidden: false
  id: 6566a7ef295a50cb58a252f4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: '[AUTOMATED] Model Memory Requirements'
