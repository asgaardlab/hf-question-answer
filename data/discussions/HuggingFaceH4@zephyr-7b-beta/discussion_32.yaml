!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alvarobartt
conflicting_files: null
created_at: 2023-11-20 15:59:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
      fullname: Alvaro Bartolome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvarobartt
      type: user
    createdAt: '2023-11-20T15:59:53.000Z'
    data:
      edited: true
      editors:
      - alvarobartt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7679412961006165
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
          fullname: Alvaro Bartolome
          isHf: false
          isPro: false
          name: alvarobartt
          type: user
        html: "<p>Hi here \U0001F917</p>\n<p>Asking from my misunderstanding, why\
          \ is the <code>model_max_length</code> within the <code>tokenizer_config.json</code>\
          \ set to 1000000000000000019884624838656? Shouldn't it be 2048 as per the\
          \ Zephyr paper? Does this have any side/unintended effect? Is there any\
          \ rationale behind it?</p>\n<p>See <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/blob/main/tokenizer_config.json#L38\"\
          >https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/blob/main/tokenizer_config.json#L38</a></p>\n"
        raw: "Hi here \U0001F917\n\nAsking from my misunderstanding, why is the `model_max_length`\
          \ within the `tokenizer_config.json` set to 1000000000000000019884624838656?\
          \ Shouldn't it be 2048 as per the Zephyr paper? Does this have any side/unintended\
          \ effect? Is there any rationale behind it?\n\nSee https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/blob/main/tokenizer_config.json#L38"
        updatedAt: '2023-11-28T07:58:39.628Z'
      numEdits: 1
      reactions: []
    id: 655b8279e8a8971e898aea0c
    type: comment
  author: alvarobartt
  content: "Hi here \U0001F917\n\nAsking from my misunderstanding, why is the `model_max_length`\
    \ within the `tokenizer_config.json` set to 1000000000000000019884624838656? Shouldn't\
    \ it be 2048 as per the Zephyr paper? Does this have any side/unintended effect?\
    \ Is there any rationale behind it?\n\nSee https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/blob/main/tokenizer_config.json#L38"
  created_at: 2023-11-20 15:59:53+00:00
  edited: true
  hidden: false
  id: 655b8279e8a8971e898aea0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/006767a89285b2d90169a191b70809a8.svg
      fullname: Lcorleone
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davidlu
      type: user
    createdAt: '2023-11-28T01:57:10.000Z'
    data:
      edited: false
      editors:
      - davidlu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9305601716041565
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/006767a89285b2d90169a191b70809a8.svg
          fullname: Lcorleone
          isHf: false
          isPro: false
          name: davidlu
          type: user
        html: '<p>aha, i have the same question.</p>

          '
        raw: aha, i have the same question.
        updatedAt: '2023-11-28T01:57:10.063Z'
      numEdits: 0
      reactions: []
    id: 656548f6ba64f727ddaaf231
    type: comment
  author: davidlu
  content: aha, i have the same question.
  created_at: 2023-11-28 01:57:10+00:00
  edited: false
  hidden: false
  id: 656548f6ba64f727ddaaf231
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 32
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: Why isn't the `model_max_length` set to 2048?
