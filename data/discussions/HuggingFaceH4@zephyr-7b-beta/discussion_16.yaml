!!python/object:huggingface_hub.community.DiscussionWithDetails
author: uuguuguu
conflicting_files: null
created_at: 2023-11-02 14:13:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f5ca09e3279afbb6b44ca714cf1a360.svg
      fullname: Youjin Chung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: uuguuguu
      type: user
    createdAt: '2023-11-02T15:13:36.000Z'
    data:
      edited: true
      editors:
      - uuguuguu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8733378052711487
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f5ca09e3279afbb6b44ca714cf1a360.svg
          fullname: Youjin Chung
          isHf: false
          isPro: false
          name: uuguuguu
          type: user
        html: '<p>Hello H4 team,</p>

          <p>Thank you for the great work!<br>Is there any way to use a chat template
          when loading the model using  <code>AutoTokenizer</code>, and <code>AutoModelForCausalLM</code>?<br>I
          was testing the loaded directly, not through the pipeline, but the answers
          to the questions were totally different when I was testing on the chat demo.</p>

          '
        raw: 'Hello H4 team,


          Thank you for the great work!

          Is there any way to use a chat template when loading the model using  `AutoTokenizer`,
          and `AutoModelForCausalLM`?

          I was testing the loaded directly, not through the pipeline, but the answers
          to the questions were totally different when I was testing on the chat demo.'
        updatedAt: '2023-11-02T19:05:37.227Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - atstim731
        - twinshyun82
    id: 6543bca06d87d83c6d261f0a
    type: comment
  author: uuguuguu
  content: 'Hello H4 team,


    Thank you for the great work!

    Is there any way to use a chat template when loading the model using  `AutoTokenizer`,
    and `AutoModelForCausalLM`?

    I was testing the loaded directly, not through the pipeline, but the answers to
    the questions were totally different when I was testing on the chat demo.'
  created_at: 2023-11-02 14:13:36+00:00
  edited: true
  hidden: false
  id: 6543bca06d87d83c6d261f0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f5ca09e3279afbb6b44ca714cf1a360.svg
      fullname: Youjin Chung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: uuguuguu
      type: user
    createdAt: '2023-11-07T15:55:56.000Z'
    data:
      edited: false
      editors:
      - uuguuguu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32231733202934265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f5ca09e3279afbb6b44ca714cf1a360.svg
          fullname: Youjin Chung
          isHf: false
          isPro: false
          name: uuguuguu
          type: user
        html: "<p>For people who want to load it directly using chat template.</p>\n\
          <pre><code># Apply the chat template\nwith tokenizer.as_target_tokenizer():\n\
          \    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
          \n# Tokenize the prompt\ninput_ids = tokenizer(prompt, return_tensors=\"\
          pt\").input_ids.to(device)\n\n# Generate a response\noutputs = model.generate(\n\
          \    input_ids,\n    max_new_tokens=4096,\n    do_sample=True,\n    temperature=0.7,\n\
          \    top_k=50,\n    top_p=0.95,\n    pad_token_id=tokenizer.eos_token_id\n\
          )\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True)\nprint(generated_text)\n\n# The output will\
          \ be the generated text, formatted similarly to the example given\n</code></pre>\n"
        raw: "For people who want to load it directly using chat template.\n```\n\
          # Apply the chat template\nwith tokenizer.as_target_tokenizer():\n    prompt\
          \ = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
          \n# Tokenize the prompt\ninput_ids = tokenizer(prompt, return_tensors=\"\
          pt\").input_ids.to(device)\n\n# Generate a response\noutputs = model.generate(\n\
          \    input_ids,\n    max_new_tokens=4096,\n    do_sample=True,\n    temperature=0.7,\n\
          \    top_k=50,\n    top_p=0.95,\n    pad_token_id=tokenizer.eos_token_id\n\
          )\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True)\nprint(generated_text)\n\n# The output will\
          \ be the generated text, formatted similarly to the example given\n```\n"
        updatedAt: '2023-11-07T15:55:56.142Z'
      numEdits: 0
      reactions: []
    id: 654a5e0c9e2766147793673a
    type: comment
  author: uuguuguu
  content: "For people who want to load it directly using chat template.\n```\n# Apply\
    \ the chat template\nwith tokenizer.as_target_tokenizer():\n    prompt = tokenizer.apply_chat_template(messages,\
    \ tokenize=False, add_generation_prompt=True)\n\n# Tokenize the prompt\ninput_ids\
    \ = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# Generate\
    \ a response\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=4096,\n\
    \    do_sample=True,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n \
    \   pad_token_id=tokenizer.eos_token_id\n)\n\n# Decode the generated text\ngenerated_text\
    \ = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n\
    \n# The output will be the generated text, formatted similarly to the example\
    \ given\n```\n"
  created_at: 2023-11-07 15:55:56+00:00
  edited: false
  hidden: false
  id: 654a5e0c9e2766147793673a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6027937110d99b9b7f1e193cebf94675.svg
      fullname: dev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ripeer
      type: user
    createdAt: '2023-11-14T20:43:42.000Z'
    data:
      edited: false
      editors:
      - Ripeer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8361363410949707
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6027937110d99b9b7f1e193cebf94675.svg
          fullname: dev
          isHf: false
          isPro: false
          name: Ripeer
          type: user
        html: '<p>Hi !<br>Thanks for your work !<br>Where did you find some informations
          to do like this ? Is this the best way to use zephyr-7b-Beta with AutoModelForCausalLM
          ?</p>

          '
        raw: "Hi !\nThanks for your work ! \nWhere did you find some informations\
          \ to do like this ? Is this the best way to use zephyr-7b-Beta with AutoModelForCausalLM\
          \ ?"
        updatedAt: '2023-11-14T20:43:42.082Z'
      numEdits: 0
      reactions: []
    id: 6553dbfe4045e53f45f21ee5
    type: comment
  author: Ripeer
  content: "Hi !\nThanks for your work ! \nWhere did you find some informations to\
    \ do like this ? Is this the best way to use zephyr-7b-Beta with AutoModelForCausalLM\
    \ ?"
  created_at: 2023-11-14 20:43:42+00:00
  edited: false
  hidden: false
  id: 6553dbfe4045e53f45f21ee5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: HuggingFaceH4/zephyr-7b-beta
repo_type: model
status: open
target_branch: null
title: Load chat model directly
