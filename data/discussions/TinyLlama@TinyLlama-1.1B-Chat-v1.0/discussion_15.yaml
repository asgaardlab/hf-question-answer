!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sachinmyneni
conflicting_files: null
created_at: 2024-01-07 15:55:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/516547e315a2fd11b6224f5b7c691583.svg
      fullname: Sachin Myneni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sachinmyneni
      type: user
    createdAt: '2024-01-07T15:55:59.000Z'
    data:
      edited: false
      editors:
      - sachinmyneni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6875328421592712
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/516547e315a2fd11b6224f5b7c691583.svg
          fullname: Sachin Myneni
          isHf: false
          isPro: false
          name: sachinmyneni
          type: user
        html: '<p>Hello,<br>This is my first foray into downloading and attempting
          to run an LLM locally.<br>I am trying this on a late 2013 MacPro with 12-core
          Intel Xeon E5 with AMD FirePro D300 2 GB GPU memory.<br>Is this a "sufficient"
          system to run inference? After making a seemingly logical ''downgrade''
          I don''t get the error but I don''t get any results either.</p>

          <p>When I ran the example, I get the error:</p>

          <p>TypeError: BFloat16 is not supported on MPS</p>

          <p>So I changed the pipeline call to:</p>

          <p>pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          torch_dtype=torch.float16, device_map="auto")</p>

          <p>And reran:<br>I get a few warnings, but no errors:</p>

          <p>  position_ids = attention_mask.long().cumsum(-1) - 1<br>/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/logits_process.py:509:
          UserWarning: torch.topk support for k&gt;16 by MPS on MacOS 13+, please
          upgrade (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Shape.mm:71.)<br>  indices_to_remove
          = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]<br>/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/logits_process.py:447:
          UserWarning: torch.sort is supported by MPS on MacOS 13+, please upgrade.
          Falling back to CPU (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Sort.mm:41.)<br>  sorted_logits,
          sorted_indices = torch.sort(scores, descending=False)<br>/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/utils.py:2920:
          UserWarning: MPS: no support for int64 for min_max, downcasting to a smaller
          data type (int32/float32). Native support for int64 has been added in macOS
          13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:621.)<br>  if
          unfinished_sequences.max() == 0:</p>

          <blockquote>

          <blockquote>

          <blockquote>

          </blockquote>

          </blockquote>

          </blockquote>

          <p>But I get nothing in the output or any error beyond the warnings above:</p>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7,
          top_k=50, top_p=0.95)<br>print(outputs[0]["generated_text"])<br>&lt;|system|&gt;<br>You
          are a friendly chatbot who always responds in the style of a pirate<br>&lt;|user|&gt;<br>How
          many helicopters can a human eat in one sitting?<br>&lt;|assistant|&gt;</p>

          </blockquote>

          </blockquote>

          </blockquote>

          '
        raw: "Hello,\r\nThis is my first foray into downloading and attempting to\
          \ run an LLM locally.\r\nI am trying this on a late 2013 MacPro with 12-core\
          \ Intel Xeon E5 with AMD FirePro D300 2 GB GPU memory.\r\nIs this a \"sufficient\"\
          \ system to run inference? After making a seemingly logical 'downgrade'\
          \ I don't get the error but I don't get any results either.\r\n\r\nWhen\
          \ I ran the example, I get the error:\r\n\r\nTypeError: BFloat16 is not\
          \ supported on MPS\r\n\r\nSo I changed the pipeline call to:\r\n\r\npipe\
          \ = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\
          , torch_dtype=torch.float16, device_map=\"auto\")\r\n\r\nAnd reran:\r\n\
          I get a few warnings, but no errors:\r\n\r\n  position_ids = attention_mask.long().cumsum(-1)\
          \ - 1\r\n/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/logits_process.py:509:\
          \ UserWarning: torch.topk support for k>16 by MPS on MacOS 13+, please upgrade\
          \ (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Shape.mm:71.)\r\
          \n  indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\r\
          \n/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/logits_process.py:447:\
          \ UserWarning: torch.sort is supported by MPS on MacOS 13+, please upgrade.\
          \ Falling back to CPU (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Sort.mm:41.)\r\
          \n  sorted_logits, sorted_indices = torch.sort(scores, descending=False)\r\
          \n/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/utils.py:2920:\
          \ UserWarning: MPS: no support for int64 for min_max, downcasting to a smaller\
          \ data type (int32/float32). Native support for int64 has been added in\
          \ macOS 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:621.)\r\
          \n  if unfinished_sequences.max() == 0:\r\n>>> \r\n\r\nBut I get nothing\
          \ in the output or any error beyond the warnings above:\r\n>>> outputs =\
          \ pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50,\
          \ top_p=0.95)\r\n>>> print(outputs[0][\"generated_text\"])\r\n<|system|>\r\
          \nYou are a friendly chatbot who always responds in the style of a pirate</s>\r\
          \n<|user|>\r\nHow many helicopters can a human eat in one sitting?</s>\r\
          \n<|assistant|>\r\n\r\n\r\n\r\n\r\n"
        updatedAt: '2024-01-07T15:55:59.913Z'
      numEdits: 0
      reactions: []
    id: 659ac98fb2ec894c51f7ecfe
    type: comment
  author: sachinmyneni
  content: "Hello,\r\nThis is my first foray into downloading and attempting to run\
    \ an LLM locally.\r\nI am trying this on a late 2013 MacPro with 12-core Intel\
    \ Xeon E5 with AMD FirePro D300 2 GB GPU memory.\r\nIs this a \"sufficient\" system\
    \ to run inference? After making a seemingly logical 'downgrade' I don't get the\
    \ error but I don't get any results either.\r\n\r\nWhen I ran the example, I get\
    \ the error:\r\n\r\nTypeError: BFloat16 is not supported on MPS\r\n\r\nSo I changed\
    \ the pipeline call to:\r\n\r\npipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\
    , torch_dtype=torch.float16, device_map=\"auto\")\r\n\r\nAnd reran:\r\nI get a\
    \ few warnings, but no errors:\r\n\r\n  position_ids = attention_mask.long().cumsum(-1)\
    \ - 1\r\n/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/logits_process.py:509:\
    \ UserWarning: torch.topk support for k>16 by MPS on MacOS 13+, please upgrade\
    \ (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Shape.mm:71.)\r\
    \n  indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\r\
    \n/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/logits_process.py:447:\
    \ UserWarning: torch.sort is supported by MPS on MacOS 13+, please upgrade. Falling\
    \ back to CPU (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Sort.mm:41.)\r\
    \n  sorted_logits, sorted_indices = torch.sort(scores, descending=False)\r\n/Users/sachin/ai/llamaindex-openllm/llamaindex-openllm/lib/python3.10/site-packages/transformers/generation/utils.py:2920:\
    \ UserWarning: MPS: no support for int64 for min_max, downcasting to a smaller\
    \ data type (int32/float32). Native support for int64 has been added in macOS\
    \ 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:621.)\r\
    \n  if unfinished_sequences.max() == 0:\r\n>>> \r\n\r\nBut I get nothing in the\
    \ output or any error beyond the warnings above:\r\n>>> outputs = pipe(prompt,\
    \ max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\r\
    \n>>> print(outputs[0][\"generated_text\"])\r\n<|system|>\r\nYou are a friendly\
    \ chatbot who always responds in the style of a pirate</s>\r\n<|user|>\r\nHow\
    \ many helicopters can a human eat in one sitting?</s>\r\n<|assistant|>\r\n\r\n\
    \r\n\r\n\r\n"
  created_at: 2024-01-07 15:55:59+00:00
  edited: false
  hidden: false
  id: 659ac98fb2ec894c51f7ecfe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661193551560-noauth.png?w=200&h=200&f=face
      fullname: Nathan Simons
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoeySalmons
      type: user
    createdAt: '2024-01-08T07:49:42.000Z'
    data:
      edited: false
      editors:
      - JoeySalmons
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9194713234901428
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661193551560-noauth.png?w=200&h=200&f=face
          fullname: Nathan Simons
          isHf: false
          isPro: false
          name: JoeySalmons
          type: user
        html: '<p>"This is my first foray into downloading and attempting to run an
          LLM locally"<br>If this is true, I highly recommend trying out running GGUF
          quantized models from TheBloke: <a href="https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF">https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF</a>
          using a backend like llama.cpp. TheBloke also lists several other clients
          and libraries that support GGUF, most of which also support running LLM
          inference on a mac. I suggest this method because it <em>should</em> be
          a very simple and beginner-friendly intro to running LLMs locally, the main
          requirement in your case being the support for older hardware.</p>

          <p>Also, you could check out discussions in the llama.cpp github as well
          as the r/localLlama subreddit for posts like these: <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/16j29s3/old_comp_running_llm_i_got_llama27bchatq2_kgguf/">https://www.reddit.com/r/LocalLLaMA/comments/16j29s3/old_comp_running_llm_i_got_llama27bchatq2_kgguf/</a></p>

          '
        raw: '"This is my first foray into downloading and attempting to run an LLM
          locally"

          If this is true, I highly recommend trying out running GGUF quantized models
          from TheBloke: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF
          using a backend like llama.cpp. TheBloke also lists several other clients
          and libraries that support GGUF, most of which also support running LLM
          inference on a mac. I suggest this method because it *should* be a very
          simple and beginner-friendly intro to running LLMs locally, the main requirement
          in your case being the support for older hardware.


          Also, you could check out discussions in the llama.cpp github as well as
          the r/localLlama subreddit for posts like these: https://www.reddit.com/r/LocalLLaMA/comments/16j29s3/old_comp_running_llm_i_got_llama27bchatq2_kgguf/'
        updatedAt: '2024-01-08T07:49:42.803Z'
      numEdits: 0
      reactions: []
    id: 659ba9160e574e59dc906eb6
    type: comment
  author: JoeySalmons
  content: '"This is my first foray into downloading and attempting to run an LLM
    locally"

    If this is true, I highly recommend trying out running GGUF quantized models from
    TheBloke: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF using
    a backend like llama.cpp. TheBloke also lists several other clients and libraries
    that support GGUF, most of which also support running LLM inference on a mac.
    I suggest this method because it *should* be a very simple and beginner-friendly
    intro to running LLMs locally, the main requirement in your case being the support
    for older hardware.


    Also, you could check out discussions in the llama.cpp github as well as the r/localLlama
    subreddit for posts like these: https://www.reddit.com/r/LocalLLaMA/comments/16j29s3/old_comp_running_llm_i_got_llama27bchatq2_kgguf/'
  created_at: 2024-01-08 07:49:42+00:00
  edited: false
  hidden: false
  id: 659ba9160e574e59dc906eb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/516547e315a2fd11b6224f5b7c691583.svg
      fullname: Sachin Myneni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sachinmyneni
      type: user
    createdAt: '2024-01-15T05:44:56.000Z'
    data:
      edited: false
      editors:
      - sachinmyneni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676603078842163
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/516547e315a2fd11b6224f5b7c691583.svg
          fullname: Sachin Myneni
          isHf: false
          isPro: false
          name: sachinmyneni
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;JoeySalmons&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JoeySalmons\"\
          >@<span class=\"underline\">JoeySalmons</span></a></span>\n\n\t</span></span>\
          \   That was very helpful.  I could use llama-cpp-python and a few models\
          \ successfully. I then tried the text-generator-webui but the installation\
          \ was not as smooth as the one in the reddit thread. I will keep at that.\
          \ But with llama-cpp-python, I have a good start.<br>I am now playing with\
          \ different parameters and looking to see if I can train a model with my\
          \ data. llama-cpp-python does not seem to be there yet according to this\
          \ thread atleast: <a rel=\"nofollow\" href=\"https://github.com/abetlen/llama-cpp-python/issues/814\"\
          >https://github.com/abetlen/llama-cpp-python/issues/814</a><br>Oh.. and\
          \ I went ahead and got into google colab since the free tier itself gives\
          \ some GPU resources. </p>\n"
        raw: "Thanks @JoeySalmons   That was very helpful.  I could use llama-cpp-python\
          \ and a few models successfully. I then tried the text-generator-webui but\
          \ the installation was not as smooth as the one in the reddit thread. I\
          \ will keep at that. But with llama-cpp-python, I have a good start. \n\
          I am now playing with different parameters and looking to see if I can train\
          \ a model with my data. llama-cpp-python does not seem to be there yet according\
          \ to this thread atleast: https://github.com/abetlen/llama-cpp-python/issues/814\n\
          Oh.. and I went ahead and got into google colab since the free tier itself\
          \ gives some GPU resources. "
        updatedAt: '2024-01-15T05:44:56.367Z'
      numEdits: 0
      reactions: []
    id: 65a4c658895d1eca73d37400
    type: comment
  author: sachinmyneni
  content: "Thanks @JoeySalmons   That was very helpful.  I could use llama-cpp-python\
    \ and a few models successfully. I then tried the text-generator-webui but the\
    \ installation was not as smooth as the one in the reddit thread. I will keep\
    \ at that. But with llama-cpp-python, I have a good start. \nI am now playing\
    \ with different parameters and looking to see if I can train a model with my\
    \ data. llama-cpp-python does not seem to be there yet according to this thread\
    \ atleast: https://github.com/abetlen/llama-cpp-python/issues/814\nOh.. and I\
    \ went ahead and got into google colab since the free tier itself gives some GPU\
    \ resources. "
  created_at: 2024-01-15 05:44:56+00:00
  edited: false
  hidden: false
  id: 65a4c658895d1eca73d37400
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
repo_type: model
status: open
target_branch: null
title: Minimum supported device?
