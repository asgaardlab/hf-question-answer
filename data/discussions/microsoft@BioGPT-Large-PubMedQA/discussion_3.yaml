!!python/object:huggingface_hub.community.DiscussionWithDetails
author: droxi-raveh
conflicting_files: null
created_at: 2023-07-25 13:57:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6f0427aa6d2ed99bbdc6d76fa845dfe.svg
      fullname: Raveh Ben Simon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: droxi-raveh
      type: user
    createdAt: '2023-07-25T14:57:34.000Z'
    data:
      edited: false
      editors:
      - droxi-raveh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5940243601799011
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6f0427aa6d2ed99bbdc6d76fa845dfe.svg
          fullname: Raveh Ben Simon
          isHf: false
          isPro: false
          name: droxi-raveh
          type: user
        html: '<p>Hi,<br>I''m trying to deploy an endpoint for of this model using
          sagemaker, specifically the code offered under ''Deploy'':</p>

          <p>import sagemaker<br>import boto3<br>from sagemaker.huggingface import
          HuggingFaceModel</p>

          <p>try:<br>    role = sagemaker.get_execution_role()<br>except ValueError:<br>    iam
          = boto3.client(''iam'')<br>    role = iam.get_role(RoleName=''sagemaker_execution_role'')[''Role''][''Arn'']</p>

          <h1 id="hub-model-configuration-httpshuggingfacecomodels">Hub Model configuration.
          <a href="https://huggingface.co/models">https://huggingface.co/models</a></h1>

          <p>hub = {<br>    ''HF_MODEL_ID'':''microsoft/BioGPT-Large-PubMedQA'',<br>    ''HF_TASK'':''text-generation''<br>}</p>

          <h1 id="create-hugging-face-model-class">create Hugging Face Model Class</h1>

          <p>huggingface_model = HuggingFaceModel(<br>    transformers_version=''4.26.0'',<br>    pytorch_version=''1.13.1'',<br>    py_version=''py39'',<br>    env=hub,<br>    role=role,<br>)</p>

          <h1 id="deploy-model-to-sagemaker-inference">deploy model to SageMaker Inference</h1>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,
          # number of instances<br>    instance_type=''ml.m5.xlarge'' # ec2 instance
          type<br>)</p>

          <p>predictor.predict({<br>    "inputs": "Can you please let us know more
          details about your ",<br>})</p>

          <p>The deployment succeeded, but when calling predict, the workers die because
          they cannot load the model. When looking at the Cloudwatch logs, I see the
          following error: </p>

          <p>2023-07-25T14:48:59,331 [INFO ] W-9000-microsoft__BioGPT-Large-P-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ValueError: Could not load model
          /.sagemaker/mms/models/microsoft__BioGPT-Large-PubMedQA with any of the
          following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class ''transformers.models.biogpt.modeling_biogpt.BioGptForCausalLM''&gt;).</p>

          <p>Any idea on how to solve? Perhaps I need to use a newer version of transformers?
          </p>

          '
        raw: "Hi,\r\nI'm trying to deploy an endpoint for of this model using sagemaker,\
          \ specifically the code offered under 'Deploy':\r\n\r\nimport sagemaker\r\
          \nimport boto3\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\n\
          \r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\nexcept ValueError:\r\
          \n\tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
          \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub =\
          \ {\r\n\t'HF_MODEL_ID':'microsoft/BioGPT-Large-PubMedQA',\r\n\t'HF_TASK':'text-generation'\r\
          \n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
          \n\ttransformers_version='4.26.0',\r\n\tpytorch_version='1.13.1',\r\n\t\
          py_version='py39',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model\
          \ to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\n\t\
          initial_instance_count=1, # number of instances\r\n\tinstance_type='ml.m5.xlarge'\
          \ # ec2 instance type\r\n)\r\n\r\npredictor.predict({\r\n\t\"inputs\": \"\
          Can you please let us know more details about your \",\r\n})\r\n\r\nThe\
          \ deployment succeeded, but when calling predict, the workers die because\
          \ they cannot load the model. When looking at the Cloudwatch logs, I see\
          \ the following error: \r\n\r\n2023-07-25T14:48:59,331 [INFO ] W-9000-microsoft__BioGPT-Large-P-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ValueError: Could not load\
          \ model /.sagemaker/mms/models/microsoft__BioGPT-Large-PubMedQA with any\
          \ of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.biogpt.modeling_biogpt.BioGptForCausalLM'>).\r\
          \n\r\nAny idea on how to solve? Perhaps I need to use a newer version of\
          \ transformers? "
        updatedAt: '2023-07-25T14:57:34.130Z'
      numEdits: 0
      reactions: []
    id: 64bfe2dee033d3c61a5d8d87
    type: comment
  author: droxi-raveh
  content: "Hi,\r\nI'm trying to deploy an endpoint for of this model using sagemaker,\
    \ specifically the code offered under 'Deploy':\r\n\r\nimport sagemaker\r\nimport\
    \ boto3\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\n\r\ntry:\r\n\t\
    role = sagemaker.get_execution_role()\r\nexcept ValueError:\r\n\tiam = boto3.client('iam')\r\
    \n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\n\
    \r\n# Hub Model configuration. https://huggingface.co/models\r\nhub = {\r\n\t\
    'HF_MODEL_ID':'microsoft/BioGPT-Large-PubMedQA',\r\n\t'HF_TASK':'text-generation'\r\
    \n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
    \n\ttransformers_version='4.26.0',\r\n\tpytorch_version='1.13.1',\r\n\tpy_version='py39',\r\
    \n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\
    \npredictor = huggingface_model.deploy(\r\n\tinitial_instance_count=1, # number\
    \ of instances\r\n\tinstance_type='ml.m5.xlarge' # ec2 instance type\r\n)\r\n\r\
    \npredictor.predict({\r\n\t\"inputs\": \"Can you please let us know more details\
    \ about your \",\r\n})\r\n\r\nThe deployment succeeded, but when calling predict,\
    \ the workers die because they cannot load the model. When looking at the Cloudwatch\
    \ logs, I see the following error: \r\n\r\n2023-07-25T14:48:59,331 [INFO ] W-9000-microsoft__BioGPT-Large-P-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ValueError: Could not load model\
    \ /.sagemaker/mms/models/microsoft__BioGPT-Large-PubMedQA with any of the following\
    \ classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class 'transformers.models.biogpt.modeling_biogpt.BioGptForCausalLM'>).\r\n\
    \r\nAny idea on how to solve? Perhaps I need to use a newer version of transformers? "
  created_at: 2023-07-25 13:57:34+00:00
  edited: false
  hidden: false
  id: 64bfe2dee033d3c61a5d8d87
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: microsoft/BioGPT-Large-PubMedQA
repo_type: model
status: open
target_branch: null
title: 'Deployment error on Sagemaker: could not load model'
