!!python/object:huggingface_hub.community.DiscussionWithDetails
author: b0968
conflicting_files: null
created_at: 2023-09-29 12:00:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
      fullname: b680
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: b0968
      type: user
    createdAt: '2023-09-29T13:00:48.000Z'
    data:
      edited: false
      editors:
      - b0968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9158240556716919
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
          fullname: b680
          isHf: false
          isPro: false
          name: b0968
          type: user
        html: '<p>is the file "tokenizer.model" missing?<br>can you add it (also to
          all the other repos)?</p>

          '
        raw: "is the file \"tokenizer.model\" missing?\r\ncan you add it (also to\
          \ all the other repos)?"
        updatedAt: '2023-09-29T13:00:48.261Z'
      numEdits: 0
      reactions: []
    id: 6516ca80dda42101c37f98c0
    type: comment
  author: b0968
  content: "is the file \"tokenizer.model\" missing?\r\ncan you add it (also to all\
    \ the other repos)?"
  created_at: 2023-09-29 12:00:48+00:00
  edited: false
  hidden: false
  id: 6516ca80dda42101c37f98c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-09-29T13:10:52.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532291889190674
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Oh yeah, I could have seen that. Will also do :) Afaik this is not
          necessary if you do not set <code>use_fast=False</code>.</p>

          '
        raw: Oh yeah, I could have seen that. Will also do :) Afaik this is not necessary
          if you do not set `use_fast=False`.
        updatedAt: '2023-09-29T13:10:52.631Z'
      numEdits: 0
      reactions: []
    id: 6516ccdcbec7c0ed6e3989ad
    type: comment
  author: bjoernp
  content: Oh yeah, I could have seen that. Will also do :) Afaik this is not necessary
    if you do not set `use_fast=False`.
  created_at: 2023-09-29 12:10:52+00:00
  edited: false
  hidden: false
  id: 6516ccdcbec7c0ed6e3989ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
      fullname: b680
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: b0968
      type: user
    createdAt: '2023-09-29T13:35:31.000Z'
    data:
      edited: false
      editors:
      - b0968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8753464818000793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
          fullname: b680
          isHf: false
          isPro: false
          name: b0968
          type: user
        html: '<p>i''m trying to use it with llama.cpp, but there seems to be a different
          tokenizer for the chat version?<br><code>Exception: Vocab size mismatch
          (model has 32128, but ../leo-hessianai-7b-chat/tokenizer.model has 32000).</code></p>

          '
        raw: 'i''m trying to use it with llama.cpp, but there seems to be a different
          tokenizer for the chat version?

          ```Exception: Vocab size mismatch (model has 32128, but ../leo-hessianai-7b-chat/tokenizer.model
          has 32000).```'
        updatedAt: '2023-09-29T13:35:31.475Z'
      numEdits: 0
      reactions: []
    id: 6516d2a3682832d9af107bd3
    type: comment
  author: b0968
  content: 'i''m trying to use it with llama.cpp, but there seems to be a different
    tokenizer for the chat version?

    ```Exception: Vocab size mismatch (model has 32128, but ../leo-hessianai-7b-chat/tokenizer.model
    has 32000).```'
  created_at: 2023-09-29 12:35:31+00:00
  edited: false
  hidden: false
  id: 6516d2a3682832d9af107bd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-09-29T14:39:00.000Z'
    data:
      edited: true
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9887331128120422
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Seems I''ve uploaded a wrong tokenizer model. I''ve now uploaded
          the correct version in this repo. Please confirm that it works for you?
          Will then update for the others.</p>

          '
        raw: Seems I've uploaded a wrong tokenizer model. I've now uploaded the correct
          version in this repo. Please confirm that it works for you? Will then update
          for the others.
        updatedAt: '2023-09-29T14:39:16.699Z'
      numEdits: 1
      reactions: []
    id: 6516e184ff197684a5edef01
    type: comment
  author: bjoernp
  content: Seems I've uploaded a wrong tokenizer model. I've now uploaded the correct
    version in this repo. Please confirm that it works for you? Will then update for
    the others.
  created_at: 2023-09-29 13:39:00+00:00
  edited: true
  hidden: false
  id: 6516e184ff197684a5edef01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
      fullname: b680
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: b0968
      type: user
    createdAt: '2023-09-29T15:35:25.000Z'
    data:
      edited: false
      editors:
      - b0968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46031495928764343
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
          fullname: b680
          isHf: false
          isPro: false
          name: b0968
          type: user
        html: "<p>Sadly no, still same <code>tokenizer.model</code>:</p>\n<pre><code>\
          \ ~/git/leo-hessianai-7b-chat \uE0B0 main \uE0B0                       \
          \                                                                      \
          \                                                               \uE0B2 base\
          \ \uE73C \uE0B2 15:33:36\n\u276F md5sum tokenizer.model\neeec4125e9c7560836b4873b6f8e3025\
          \  tokenizer.model\n\n ~/git/leo-hessianai-7b-chat \uE0B0 main \uE0B0  \
          \                                                                      \
          \                                                                      \
          \              \uE0B2 base \uE73C \uE0B2 15:33:38\n\u276F git pull\nremote:\
          \ Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\n\
          remote: Compressing objects: 100% (4/4), done.\nremote: Total 4 (delta 2),\
          \ reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (4/4), 526\
          \ bytes | 175.00 KiB/s, done.\nFrom https://huggingface.co/LeoLM/leo-hessianai-7b-chat\n\
          \   <a href=\"/LeoLM/leo-hessianai-7b-chat/commit/b96b28f\">b96b28f</a>..7c343a5\
          \  main       -&gt; origin/main\nUpdating <a href=\"/LeoLM/leo-hessianai-7b-chat/commit/b96b28f\"\
          >b96b28f</a>..7c343a5\nFast-forward\n\n ~/git/leo-hessianai-7b-chat \uE0B0\
          \ main \uE0B0                                                          \
          \                                                                      \
          \                            \uE0B2 base \uE73C \uE0B2 17:29:52\n\u276F\
          \ md5sum tokenizer.model\neeec4125e9c7560836b4873b6f8e3025  tokenizer.model\n\
          </code></pre>\n"
        raw: "Sadly no, still same `tokenizer.model`:\n\n```\n ~/git/leo-hessianai-7b-chat\
          \ \uE0B0 main \uE0B0                                                   \
          \                                                                      \
          \                                   \uE0B2 base \uE73C \uE0B2 15:33:36\n\
          \u276F md5sum tokenizer.model\neeec4125e9c7560836b4873b6f8e3025  tokenizer.model\n\
          \n ~/git/leo-hessianai-7b-chat \uE0B0 main \uE0B0                      \
          \                                                                      \
          \                                                                \uE0B2\
          \ base \uE73C \uE0B2 15:33:38\n\u276F git pull\nremote: Enumerating objects:\
          \ 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing\
          \ objects: 100% (4/4), done.\nremote: Total 4 (delta 2), reused 0 (delta\
          \ 0), pack-reused 0\nUnpacking objects: 100% (4/4), 526 bytes | 175.00 KiB/s,\
          \ done.\nFrom https://huggingface.co/LeoLM/leo-hessianai-7b-chat\n   b96b28f..7c343a5\
          \  main       -> origin/main\nUpdating b96b28f..7c343a5\nFast-forward\n\n\
          \ ~/git/leo-hessianai-7b-chat \uE0B0 main \uE0B0                       \
          \                                                                      \
          \                                                               \uE0B2 base\
          \ \uE73C \uE0B2 17:29:52\n\u276F md5sum tokenizer.model\neeec4125e9c7560836b4873b6f8e3025\
          \  tokenizer.model\n```"
        updatedAt: '2023-09-29T15:35:25.917Z'
      numEdits: 0
      reactions: []
    id: 6516eebd69c938b3d6dacb66
    type: comment
  author: b0968
  content: "Sadly no, still same `tokenizer.model`:\n\n```\n ~/git/leo-hessianai-7b-chat\
    \ \uE0B0 main \uE0B0                                                         \
    \                                                                            \
    \                       \uE0B2 base \uE73C \uE0B2 15:33:36\n\u276F md5sum tokenizer.model\n\
    eeec4125e9c7560836b4873b6f8e3025  tokenizer.model\n\n ~/git/leo-hessianai-7b-chat\
    \ \uE0B0 main \uE0B0                                                         \
    \                                                                            \
    \                       \uE0B2 base \uE73C \uE0B2 15:33:38\n\u276F git pull\n\
    remote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\n\
    remote: Compressing objects: 100% (4/4), done.\nremote: Total 4 (delta 2), reused\
    \ 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (4/4), 526 bytes | 175.00\
    \ KiB/s, done.\nFrom https://huggingface.co/LeoLM/leo-hessianai-7b-chat\n   b96b28f..7c343a5\
    \  main       -> origin/main\nUpdating b96b28f..7c343a5\nFast-forward\n\n ~/git/leo-hessianai-7b-chat\
    \ \uE0B0 main \uE0B0                                                         \
    \                                                                            \
    \                       \uE0B2 base \uE73C \uE0B2 17:29:52\n\u276F md5sum tokenizer.model\n\
    eeec4125e9c7560836b4873b6f8e3025  tokenizer.model\n```"
  created_at: 2023-09-29 14:35:25+00:00
  edited: false
  hidden: false
  id: 6516eebd69c938b3d6dacb66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-09-29T19:50:06.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8864567875862122
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Not quite sure how to help with this then. After some testing on
          my side, the tokenizer.model seems correct (i.e. has 32006 tokens in the
          vocab) and also encodes correctly. The model has 32128 vocab size after
          padding to a multiple of 128.</p>

          <p>Is there any option on your side to use the default huggingface tokenizer?
          Or otherwise see if other users of llama.cpp have run into similar issues?</p>

          '
        raw: 'Not quite sure how to help with this then. After some testing on my
          side, the tokenizer.model seems correct (i.e. has 32006 tokens in the vocab)
          and also encodes correctly. The model has 32128 vocab size after padding
          to a multiple of 128.


          Is there any option on your side to use the default huggingface tokenizer?
          Or otherwise see if other users of llama.cpp have run into similar issues?'
        updatedAt: '2023-09-29T19:50:06.544Z'
      numEdits: 0
      reactions: []
    id: 65172a6eff197684a5f8e80f
    type: comment
  author: bjoernp
  content: 'Not quite sure how to help with this then. After some testing on my side,
    the tokenizer.model seems correct (i.e. has 32006 tokens in the vocab) and also
    encodes correctly. The model has 32128 vocab size after padding to a multiple
    of 128.


    Is there any option on your side to use the default huggingface tokenizer? Or
    otherwise see if other users of llama.cpp have run into similar issues?'
  created_at: 2023-09-29 18:50:06+00:00
  edited: false
  hidden: false
  id: 65172a6eff197684a5f8e80f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e153c1cab90dd0d1d2104197131098c.svg
      fullname: arv lem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arv2023
      type: user
    createdAt: '2023-09-30T06:40:29.000Z'
    data:
      edited: true
      editors:
      - arv2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7694292068481445
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e153c1cab90dd0d1d2104197131098c.svg
          fullname: arv lem
          isHf: false
          isPro: false
          name: arv2023
          type: user
        html: '<p>same here. Replacing "32128" with "32000" for entry  "vocab_size"
          in config.json lets llama.cpp convert the model. But then it fails with
          "error loading model: create_tensor: tensor ''token_embd.weight'' has wrong
          shape; expected  4096, 32000, got  4096, 32128,     1,     1".</p>

          '
        raw: 'same here. Replacing "32128" with "32000" for entry  "vocab_size" in
          config.json lets llama.cpp convert the model. But then it fails with "error
          loading model: create_tensor: tensor ''token_embd.weight'' has wrong shape;
          expected  4096, 32000, got  4096, 32128,     1,     1".'
        updatedAt: '2023-10-02T04:00:47.460Z'
      numEdits: 1
      reactions: []
    id: 6517c2dd7069441423424d9d
    type: comment
  author: arv2023
  content: 'same here. Replacing "32128" with "32000" for entry  "vocab_size" in config.json
    lets llama.cpp convert the model. But then it fails with "error loading model:
    create_tensor: tensor ''token_embd.weight'' has wrong shape; expected  4096, 32000,
    got  4096, 32128,     1,     1".'
  created_at: 2023-09-30 05:40:29+00:00
  edited: true
  hidden: false
  id: 6517c2dd7069441423424d9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
      fullname: b680
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: b0968
      type: user
    createdAt: '2023-10-02T09:11:02.000Z'
    data:
      edited: false
      editors:
      - b0968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8613461852073669
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3efd32df7dfacfa57bd4e2591ed3f39.svg
          fullname: b680
          isHf: false
          isPro: false
          name: b0968
          type: user
        html: '<p>it seems, it boils down to a mismatch of vocabulary dimensions in
          tokenizer.model, config.json, tokenizer.json, and token_embd.weight - huggingface
          does not check this or is smart enough to adjust on the fly.</p>

          '
        raw: it seems, it boils down to a mismatch of vocabulary dimensions in tokenizer.model,
          config.json, tokenizer.json, and token_embd.weight - huggingface does not
          check this or is smart enough to adjust on the fly.
        updatedAt: '2023-10-02T09:11:02.333Z'
      numEdits: 0
      reactions: []
    id: 651a8926819b712bde2ec348
    type: comment
  author: b0968
  content: it seems, it boils down to a mismatch of vocabulary dimensions in tokenizer.model,
    config.json, tokenizer.json, and token_embd.weight - huggingface does not check
    this or is smart enough to adjust on the fly.
  created_at: 2023-10-02 08:11:02+00:00
  edited: false
  hidden: false
  id: 651a8926819b712bde2ec348
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: LeoLM/leo-hessianai-7b-chat
repo_type: model
status: open
target_branch: null
title: missing tokenizer.model?
