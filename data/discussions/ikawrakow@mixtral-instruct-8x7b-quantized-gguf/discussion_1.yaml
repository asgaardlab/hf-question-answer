!!python/object:huggingface_hub.community.DiscussionWithDetails
author: muzika38
conflicting_files: null
created_at: 2024-01-09 06:05:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/97705f30a63d902ce361bb4ace04d997.svg
      fullname: Helbert Gascon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muzika38
      type: user
    createdAt: '2024-01-09T06:05:09.000Z'
    data:
      edited: true
      editors:
      - muzika38
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9741420745849609
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/97705f30a63d902ce361bb4ace04d997.svg
          fullname: Helbert Gascon
          isHf: false
          isPro: false
          name: muzika38
          type: user
        html: '<p>Can you also upload the Q5_1 &amp; Q5_K_M for mixtral-instruct please?</p>

          '
        raw: Can you also upload the Q5_1 & Q5_K_M for mixtral-instruct please?
        updatedAt: '2024-01-09T06:08:25.112Z'
      numEdits: 1
      reactions: []
    id: 659ce21599aeb9b5de40620c
    type: comment
  author: muzika38
  content: Can you also upload the Q5_1 & Q5_K_M for mixtral-instruct please?
  created_at: 2024-01-09 06:05:09+00:00
  edited: true
  hidden: false
  id: 659ce21599aeb9b5de40620c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/97705f30a63d902ce361bb4ace04d997.svg
      fullname: Helbert Gascon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muzika38
      type: user
    createdAt: '2024-01-09T06:08:35.000Z'
    data:
      from: Q5_1 quant
      to: Q5_1 & Q5_K_M quant
    id: 659ce2e3e942a8f717c1c22c
    type: title-change
  author: muzika38
  created_at: 2024-01-09 06:08:35+00:00
  id: 659ce2e3e942a8f717c1c22c
  new_title: Q5_1 & Q5_K_M quant
  old_title: Q5_1 quant
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-09T06:32:01.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9750745892524719
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<p>With this quantization approach <code>Q5_1</code> is about the same
          as <code>Q5_0</code>, and <code>Q5_K_M</code> is about the same as <code>Q5_K_S</code>.
          </p>

          '
        raw: 'With this quantization approach `Q5_1` is about the same as `Q5_0`,
          and `Q5_K_M` is about the same as `Q5_K_S`. '
        updatedAt: '2024-01-09T06:32:01.259Z'
      numEdits: 0
      reactions: []
    id: 659ce86193d383899cfc490b
    type: comment
  author: ikawrakow
  content: 'With this quantization approach `Q5_1` is about the same as `Q5_0`, and
    `Q5_K_M` is about the same as `Q5_K_S`. '
  created_at: 2024-01-09 06:32:01+00:00
  edited: false
  hidden: false
  id: 659ce86193d383899cfc490b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/97705f30a63d902ce361bb4ace04d997.svg
      fullname: Helbert Gascon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muzika38
      type: user
    createdAt: '2024-01-09T06:59:49.000Z'
    data:
      edited: false
      editors:
      - muzika38
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9285155534744263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/97705f30a63d902ce361bb4ace04d997.svg
          fullname: Helbert Gascon
          isHf: false
          isPro: false
          name: muzika38
          type: user
        html: '<blockquote>

          <p>With this quantization approach <code>Q5_1</code> is about the same as
          <code>Q5_0</code>, and <code>Q5_K_M</code> is about the same as <code>Q5_K_S</code>.</p>

          </blockquote>

          <p>Interesting... Is this a case exclusive to MoE? Because the difference
          between Q5_0 &amp; Q5_1 on the openhermes model is more than double on the
          quantization error percentage.</p>

          '
        raw: '> With this quantization approach `Q5_1` is about the same as `Q5_0`,
          and `Q5_K_M` is about the same as `Q5_K_S`.


          Interesting... Is this a case exclusive to MoE? Because the difference between
          Q5_0 & Q5_1 on the openhermes model is more than double on the quantization
          error percentage.'
        updatedAt: '2024-01-09T06:59:49.458Z'
      numEdits: 0
      reactions: []
    id: 659ceee575fa67e6f2368309
    type: comment
  author: muzika38
  content: '> With this quantization approach `Q5_1` is about the same as `Q5_0`,
    and `Q5_K_M` is about the same as `Q5_K_S`.


    Interesting... Is this a case exclusive to MoE? Because the difference between
    Q5_0 & Q5_1 on the openhermes model is more than double on the quantization error
    percentage.'
  created_at: 2024-01-09 06:59:49+00:00
  edited: false
  hidden: false
  id: 659ceee575fa67e6f2368309
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-09T08:42:38.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9485061764717102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<blockquote>

          <blockquote>

          <p>With this quantization approach <code>Q5_1</code> is about the same as
          <code>Q5_0</code>, and <code>Q5_K_M</code> is about the same as <code>Q5_K_S</code>.</p>

          </blockquote>

          <p>Interesting... Is this a case exclusive to MoE? Because the difference
          between Q5_0 &amp; Q5_1 on the openhermes model is more than double on the
          quantization error percentage.</p>

          </blockquote>

          <p>You can see in the OpenHermes table that <code>Q5_K_M</code> quantization
          error is about the same as <code>Q5_K_S</code>. <code>Q5_1</code> has always
          behaved in erratic ways, for some models the quantization error being significantly
          higher than <code>Q5_0</code>. With this new quantization that utilizes
          an "importance matrix", <code>Q5_1</code> behaves much better in the sense
          that it is as good as, or better than, <code>Q5_0</code>.  In the case of
          the base and instruct tuned Mixtral-8x7b models it is about the same as
          <code>Q5_0</code>. I''m not sure if it is related to the MoE architecture
          as the number of models quantized that way is too small at this point to
          draw this conclusion. </p>

          '
        raw: "> > With this quantization approach `Q5_1` is about the same as `Q5_0`,\
          \ and `Q5_K_M` is about the same as `Q5_K_S`.\n> \n> Interesting... Is this\
          \ a case exclusive to MoE? Because the difference between Q5_0 & Q5_1 on\
          \ the openhermes model is more than double on the quantization error percentage.\n\
          \nYou can see in the OpenHermes table that `Q5_K_M` quantization error is\
          \ about the same as `Q5_K_S`. `Q5_1` has always behaved in erratic ways,\
          \ for some models the quantization error being significantly higher than\
          \ `Q5_0`. With this new quantization that utilizes an \"importance matrix\"\
          , `Q5_1` behaves much better in the sense that it is as good as, or better\
          \ than, `Q5_0`.  In the case of the base and instruct tuned Mixtral-8x7b\
          \ models it is about the same as `Q5_0`. I'm not sure if it is related to\
          \ the MoE architecture as the number of models quantized that way is too\
          \ small at this point to draw this conclusion. "
        updatedAt: '2024-01-09T08:42:38.490Z'
      numEdits: 0
      reactions: []
    id: 659d06fead0e1c73f49c7e1c
    type: comment
  author: ikawrakow
  content: "> > With this quantization approach `Q5_1` is about the same as `Q5_0`,\
    \ and `Q5_K_M` is about the same as `Q5_K_S`.\n> \n> Interesting... Is this a\
    \ case exclusive to MoE? Because the difference between Q5_0 & Q5_1 on the openhermes\
    \ model is more than double on the quantization error percentage.\n\nYou can see\
    \ in the OpenHermes table that `Q5_K_M` quantization error is about the same as\
    \ `Q5_K_S`. `Q5_1` has always behaved in erratic ways, for some models the quantization\
    \ error being significantly higher than `Q5_0`. With this new quantization that\
    \ utilizes an \"importance matrix\", `Q5_1` behaves much better in the sense that\
    \ it is as good as, or better than, `Q5_0`.  In the case of the base and instruct\
    \ tuned Mixtral-8x7b models it is about the same as `Q5_0`. I'm not sure if it\
    \ is related to the MoE architecture as the number of models quantized that way\
    \ is too small at this point to draw this conclusion. "
  created_at: 2024-01-09 08:42:38+00:00
  edited: false
  hidden: false
  id: 659d06fead0e1c73f49c7e1c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ikawrakow/mixtral-instruct-8x7b-quantized-gguf
repo_type: model
status: open
target_branch: null
title: Q5_1 & Q5_K_M quant
