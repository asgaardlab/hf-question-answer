!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Alex-B
conflicting_files: null
created_at: 2023-05-21 08:19:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2db1bcdfb45f1114d2ba9e63bb7c766f.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alex-B
      type: user
    createdAt: '2023-05-21T09:19:52.000Z'
    data:
      edited: false
      editors:
      - Alex-B
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2db1bcdfb45f1114d2ba9e63bb7c766f.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: Alex-B
          type: user
        html: '<p>When trying to convert this model to a f16 version, I get the following
          error (using llama.cpp''s convert.py script):</p>

          <blockquote>

          <p>Exception: Vocab size mismatch (model has 32016, but ../models/GPT4-X-Alpasta-30b/tokenizer.model
          combined with ../models/GPT4-X-Alpasta-30b/added_tokens.json has 32005).</p>

          </blockquote>

          <p>Am I doing something wrong? The files are up-to-date with the main branch
          version.</p>

          <p>Command I used for reference: python3 convert.py --outfile ../models/GPT4-X-Alpasta-30b/ggml-model-f16.bin
          --outtype f16 ../models/GPT4-X-Alpasta-30b/</p>

          '
        raw: "When trying to convert this model to a f16 version, I get the following\
          \ error (using llama.cpp's convert.py script):\r\n> Exception: Vocab size\
          \ mismatch (model has 32016, but ../models/GPT4-X-Alpasta-30b/tokenizer.model\
          \ combined with ../models/GPT4-X-Alpasta-30b/added_tokens.json has 32005).\r\
          \n\r\nAm I doing something wrong? The files are up-to-date with the main\
          \ branch version.\r\n\r\nCommand I used for reference: python3 convert.py\
          \ --outfile ../models/GPT4-X-Alpasta-30b/ggml-model-f16.bin --outtype f16\
          \ ../models/GPT4-X-Alpasta-30b/"
        updatedAt: '2023-05-21T09:19:52.880Z'
      numEdits: 0
      reactions: []
    id: 6469e23831fbfc5df81b25ee
    type: comment
  author: Alex-B
  content: "When trying to convert this model to a f16 version, I get the following\
    \ error (using llama.cpp's convert.py script):\r\n> Exception: Vocab size mismatch\
    \ (model has 32016, but ../models/GPT4-X-Alpasta-30b/tokenizer.model combined\
    \ with ../models/GPT4-X-Alpasta-30b/added_tokens.json has 32005).\r\n\r\nAm I\
    \ doing something wrong? The files are up-to-date with the main branch version.\r\
    \n\r\nCommand I used for reference: python3 convert.py --outfile ../models/GPT4-X-Alpasta-30b/ggml-model-f16.bin\
    \ --outtype f16 ../models/GPT4-X-Alpasta-30b/"
  created_at: 2023-05-21 08:19:52+00:00
  edited: false
  hidden: false
  id: 6469e23831fbfc5df81b25ee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: MetaIX/GPT4-X-Alpasta-30b
repo_type: model
status: open
target_branch: null
title: Vocab size mismatch error when converting to f16
