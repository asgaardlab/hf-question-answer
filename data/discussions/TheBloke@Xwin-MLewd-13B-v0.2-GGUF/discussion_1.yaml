!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mpasila
conflicting_files: null
created_at: 2023-10-16 09:27:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
      fullname: minipasila
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mpasila
      type: user
    createdAt: '2023-10-16T10:27:26.000Z'
    data:
      edited: false
      editors:
      - mpasila
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47553038597106934
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
          fullname: minipasila
          isHf: false
          isPro: false
          name: mpasila
          type: user
        html: "<p><a href=\"https://huggingface.co/Undi95/Xwin-MLewd-13B-V0.2-GGUF\"\
          >Undi</a>'s own provided GGUF files seem to work fine but not these.<br>I\
          \ keep getting errors when trying to load them in oobabooga's text generation\
          \ webui. I tried both llamacpp and llamacpp_hf loaders and neither of them\
          \ work.</p>\n<p>Llamacpp loader error:</p>\n<pre><code>error loading model:\
          \ create_tensor: tensor 'token_embd.weight' has wrong shape; expected  5120,\
          \ 32001, got  5120, 32000,     1,     1\nllama_load_model_from_file: failed\
          \ to load model\nTraceback (most recent call last):\n  File \"C:\\Users\\\
          pasil\\text-generation-webui\\server.py\", line 223, in &lt;module&gt;\n\
          \    shared.model, shared.tokenizer = load_model(model_name)\n  File \"\
          C:\\Users\\pasil\\text-generation-webui\\modules\\models.py\", line 79,\
          \ in load_model\n    output = load_func_map[loader](model_name)\n  File\
          \ \"C:\\Users\\pasil\\text-generation-webui\\modules\\models.py\", line\
          \ 225, in llamacpp_loader\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\
          \  File \"C:\\Users\\pasil\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 91, in from_pretrained\n    result.model = Llama(**params)\n  File\
          \ \"C:\\Users\\pasil\\anaconda3\\envs\\textgen\\lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 365, in __init__\n    assert self.model is not None\nAssertionError\n\
          Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x000001ABCECE8AF0&gt;\n\
          Traceback (most recent call last):\n  File \"C:\\Users\\pasil\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 49, in __del__\n    self.model.__del__()\n\
          AttributeError: 'LlamaCppModel' object has no attribute 'model'\n</code></pre>\n\
          <p>Llamacpp_hf loader error:</p>\n<pre><code>error loading model: create_tensor:\
          \ tensor 'token_embd.weight' has wrong shape; expected  5120, 32001, got\
          \  5120, 32000,     1,     1\nllama_load_model_from_file: failed to load\
          \ model\nTraceback (most recent call last):\n  File \"C:\\Users\\pasil\\\
          text-generation-webui\\server.py\", line 223, in &lt;module&gt;\n    shared.model,\
          \ shared.tokenizer = load_model(model_name)\n  File \"C:\\Users\\pasil\\\
          text-generation-webui\\modules\\models.py\", line 79, in load_model\n  \
          \  output = load_func_map[loader](model_name)\n  File \"C:\\Users\\pasil\\\
          text-generation-webui\\modules\\models.py\", line 250, in llamacpp_HF_loader\n\
          \    model = LlamacppHF.from_pretrained(model_name)\n  File \"C:\\Users\\\
          pasil\\text-generation-webui\\modules\\llamacpp_hf.py\", line 211, in from_pretrained\n\
          \    model = Llama(**params)\n  File \"C:\\Users\\pasil\\anaconda3\\envs\\\
          textgen\\lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 365, in __init__\n\
          \    assert self.model is not None\nAssertionError\n</code></pre>\n"
        raw: "[Undi](https://huggingface.co/Undi95/Xwin-MLewd-13B-V0.2-GGUF)'s own\
          \ provided GGUF files seem to work fine but not these.\r\nI keep getting\
          \ errors when trying to load them in oobabooga's text generation webui.\
          \ I tried both llamacpp and llamacpp_hf loaders and neither of them work.\r\
          \n\r\nLlamacpp loader error:\r\n```\r\nerror loading model: create_tensor:\
          \ tensor 'token_embd.weight' has wrong shape; expected  5120, 32001, got\
          \  5120, 32000,     1,     1\r\nllama_load_model_from_file: failed to load\
          \ model\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pasil\\\
          text-generation-webui\\server.py\", line 223, in <module>\r\n    shared.model,\
          \ shared.tokenizer = load_model(model_name)\r\n  File \"C:\\Users\\pasil\\\
          text-generation-webui\\modules\\models.py\", line 79, in load_model\r\n\
          \    output = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\\
          pasil\\text-generation-webui\\modules\\models.py\", line 225, in llamacpp_loader\r\
          \n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\n \
          \ File \"C:\\Users\\pasil\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 91, in from_pretrained\r\n    result.model = Llama(**params)\r\n\
          \  File \"C:\\Users\\pasil\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp_cuda\\llama.py\", line 365, in __init__\r\n    assert self.model\
          \ is not None\r\nAssertionError\r\nException ignored in: <function LlamaCppModel.__del__\
          \ at 0x000001ABCECE8AF0>\r\nTraceback (most recent call last):\r\n  File\
          \ \"C:\\Users\\pasil\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 49, in __del__\r\n    self.model.__del__()\r\nAttributeError: 'LlamaCppModel'\
          \ object has no attribute 'model'\r\n```\r\nLlamacpp_hf loader error:\r\n\
          ```\r\nerror loading model: create_tensor: tensor 'token_embd.weight' has\
          \ wrong shape; expected  5120, 32001, got  5120, 32000,     1,     1\r\n\
          llama_load_model_from_file: failed to load model\r\nTraceback (most recent\
          \ call last):\r\n  File \"C:\\Users\\pasil\\text-generation-webui\\server.py\"\
          , line 223, in <module>\r\n    shared.model, shared.tokenizer = load_model(model_name)\r\
          \n  File \"C:\\Users\\pasil\\text-generation-webui\\modules\\models.py\"\
          , line 79, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"C:\\Users\\pasil\\text-generation-webui\\modules\\models.py\"\
          , line 250, in llamacpp_HF_loader\r\n    model = LlamacppHF.from_pretrained(model_name)\r\
          \n  File \"C:\\Users\\pasil\\text-generation-webui\\modules\\llamacpp_hf.py\"\
          , line 211, in from_pretrained\r\n    model = Llama(**params)\r\n  File\
          \ \"C:\\Users\\pasil\\anaconda3\\envs\\textgen\\lib\\site-packages\\llama_cpp_cuda\\\
          llama.py\", line 365, in __init__\r\n    assert self.model is not None\r\
          \nAssertionError\r\n```"
        updatedAt: '2023-10-16T10:27:26.907Z'
      numEdits: 0
      reactions: []
    id: 652d100ea7275ea703fdecf1
    type: comment
  author: mpasila
  content: "[Undi](https://huggingface.co/Undi95/Xwin-MLewd-13B-V0.2-GGUF)'s own provided\
    \ GGUF files seem to work fine but not these.\r\nI keep getting errors when trying\
    \ to load them in oobabooga's text generation webui. I tried both llamacpp and\
    \ llamacpp_hf loaders and neither of them work.\r\n\r\nLlamacpp loader error:\r\
    \n```\r\nerror loading model: create_tensor: tensor 'token_embd.weight' has wrong\
    \ shape; expected  5120, 32001, got  5120, 32000,     1,     1\r\nllama_load_model_from_file:\
    \ failed to load model\r\nTraceback (most recent call last):\r\n  File \"C:\\\
    Users\\pasil\\text-generation-webui\\server.py\", line 223, in <module>\r\n  \
    \  shared.model, shared.tokenizer = load_model(model_name)\r\n  File \"C:\\Users\\\
    pasil\\text-generation-webui\\modules\\models.py\", line 79, in load_model\r\n\
    \    output = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\pasil\\\
    text-generation-webui\\modules\\models.py\", line 225, in llamacpp_loader\r\n\
    \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\n  File \"\
    C:\\Users\\pasil\\text-generation-webui\\modules\\llamacpp_model.py\", line 91,\
    \ in from_pretrained\r\n    result.model = Llama(**params)\r\n  File \"C:\\Users\\\
    pasil\\anaconda3\\envs\\textgen\\lib\\site-packages\\llama_cpp_cuda\\llama.py\"\
    , line 365, in __init__\r\n    assert self.model is not None\r\nAssertionError\r\
    \nException ignored in: <function LlamaCppModel.__del__ at 0x000001ABCECE8AF0>\r\
    \nTraceback (most recent call last):\r\n  File \"C:\\Users\\pasil\\text-generation-webui\\\
    modules\\llamacpp_model.py\", line 49, in __del__\r\n    self.model.__del__()\r\
    \nAttributeError: 'LlamaCppModel' object has no attribute 'model'\r\n```\r\nLlamacpp_hf\
    \ loader error:\r\n```\r\nerror loading model: create_tensor: tensor 'token_embd.weight'\
    \ has wrong shape; expected  5120, 32001, got  5120, 32000,     1,     1\r\nllama_load_model_from_file:\
    \ failed to load model\r\nTraceback (most recent call last):\r\n  File \"C:\\\
    Users\\pasil\\text-generation-webui\\server.py\", line 223, in <module>\r\n  \
    \  shared.model, shared.tokenizer = load_model(model_name)\r\n  File \"C:\\Users\\\
    pasil\\text-generation-webui\\modules\\models.py\", line 79, in load_model\r\n\
    \    output = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\pasil\\\
    text-generation-webui\\modules\\models.py\", line 250, in llamacpp_HF_loader\r\
    \n    model = LlamacppHF.from_pretrained(model_name)\r\n  File \"C:\\Users\\pasil\\\
    text-generation-webui\\modules\\llamacpp_hf.py\", line 211, in from_pretrained\r\
    \n    model = Llama(**params)\r\n  File \"C:\\Users\\pasil\\anaconda3\\envs\\\
    textgen\\lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 365, in __init__\r\
    \n    assert self.model is not None\r\nAssertionError\r\n```"
  created_at: 2023-10-16 09:27:26+00:00
  edited: false
  hidden: false
  id: 652d100ea7275ea703fdecf1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-16T10:35:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659759402275085
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Thanks for the report. It looks like another issue of <span data-props=\"\
          {&quot;user&quot;:&quot;Undi95&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Undi95\">@<span class=\"underline\">Undi95</span></a></span>\n\
          \n\t</span></span> 's repo having a JSON misconfiguration.  It has an <code>added_tokens.json</code>\
          \ file which extends the vocab to 32,001, but it seems it's not needed.</p>\n\
          <p>I will remake the files and let you know when to download again.</p>\n"
        raw: 'Thanks for the report. It looks like another issue of @Undi95 ''s repo
          having a JSON misconfiguration.  It has an `added_tokens.json` file which
          extends the vocab to 32,001, but it seems it''s not needed.


          I will remake the files and let you know when to download again.'
        updatedAt: '2023-10-16T10:35:41.690Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mpasila
    id: 652d11fdb45e702eb053eec0
    type: comment
  author: TheBloke
  content: 'Thanks for the report. It looks like another issue of @Undi95 ''s repo
    having a JSON misconfiguration.  It has an `added_tokens.json` file which extends
    the vocab to 32,001, but it seems it''s not needed.


    I will remake the files and let you know when to download again.'
  created_at: 2023-10-16 09:35:41+00:00
  edited: false
  hidden: false
  id: 652d11fdb45e702eb053eec0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-16T10:35:41.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-10-16T10:35:59.049Z'
      numEdits: 0
      reactions: []
    id: 652d11fdbba7176a8bab90d4
    type: comment
  author: TheBloke
  content: This comment has been hidden
  created_at: 2023-10-16 09:35:41+00:00
  edited: true
  hidden: true
  id: 652d11fdbba7176a8bab90d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-16T10:44:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9752004146575928
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Actually, although the repo is misconfigured, when I try to make
          a new GGUF, it correctly ignores added_tokens.json.  So it''s possible I
          made a mistake when making the first quants, like maybe I edited my local
          files in the wrong way - I can''t remember now what I did for this model
          specifically, but I can see I had to make some local edit to it.</p>

          <p>Anyway, I''ve got them working now and the new upload will start in a
          moment.</p>

          '
        raw: 'Actually, although the repo is misconfigured, when I try to make a new
          GGUF, it correctly ignores added_tokens.json.  So it''s possible I made
          a mistake when making the first quants, like maybe I edited my local files
          in the wrong way - I can''t remember now what I did for this model specifically,
          but I can see I had to make some local edit to it.


          Anyway, I''ve got them working now and the new upload will start in a moment.'
        updatedAt: '2023-10-16T10:44:20.640Z'
      numEdits: 0
      reactions: []
    id: 652d140458c30b12887c040d
    type: comment
  author: TheBloke
  content: 'Actually, although the repo is misconfigured, when I try to make a new
    GGUF, it correctly ignores added_tokens.json.  So it''s possible I made a mistake
    when making the first quants, like maybe I edited my local files in the wrong
    way - I can''t remember now what I did for this model specifically, but I can
    see I had to make some local edit to it.


    Anyway, I''ve got them working now and the new upload will start in a moment.'
  created_at: 2023-10-16 09:44:20+00:00
  edited: false
  hidden: false
  id: 652d140458c30b12887c040d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-16T10:51:06.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416650533676147
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>New quants are uploaded and are working fine</p>\n<p>Q3_K_M:</p>\n\
          <pre><code>system_info: n_threads = 15 / 30 | AVX = 1 | AVX2 = 1 | AVX512\
          \ = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA\
          \ = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3\
          \ = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000,\
          \ presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40,\
          \ tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000,\
          \ mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate:\
          \ n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n Below is\
          \ an instruction that describes a task. Write a response that appropriately\
          \ completes the request.\n\n### Instruction:\nHow much wood would a woodchuck\
          \ chuck if a woodchuck could chuck wood?\n\n### Response:\n A woodchuck,\
          \ also known as a groundhog, is a herbivore and does not typically \"chuck\"\
          \ (throw) wood. They are more likely to burrow into the ground or move leaves\
          \ and debris with their strong front legs. However, if we were to assume\
          \ that a woodchuck could chuck wood like an axe, it's impossible to determine\
          \ how much wood they would be able to throw due to lack of information about\
          \ their physical strength or any context regarding the size and type of\
          \ wood being referred to. [end of text]\n</code></pre>\n"
        raw: "New quants are uploaded and are working fine\n\nQ3_K_M:\n\n```\nsystem_info:\
          \ n_threads = 15 / 30 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI =\
          \ 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA\
          \ = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling:\
          \ repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000,\
          \ frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000,\
          \ typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000,\
          \ mirostat_ent = 5.000000\ngenerate: n_ctx = 4096, n_batch = 512, n_predict\
          \ = -1, n_keep = 0\n\n\n Below is an instruction that describes a task.\
          \ Write a response that appropriately completes the request.\n\n### Instruction:\n\
          How much wood would a woodchuck chuck if a woodchuck could chuck wood?\n\
          \n### Response:\n A woodchuck, also known as a groundhog, is a herbivore\
          \ and does not typically \"chuck\" (throw) wood. They are more likely to\
          \ burrow into the ground or move leaves and debris with their strong front\
          \ legs. However, if we were to assume that a woodchuck could chuck wood\
          \ like an axe, it's impossible to determine how much wood they would be\
          \ able to throw due to lack of information about their physical strength\
          \ or any context regarding the size and type of wood being referred to.\
          \ [end of text]\n```\n "
        updatedAt: '2023-10-16T10:51:21.016Z'
      numEdits: 1
      reactions: []
    id: 652d159ab45e702eb05457f7
    type: comment
  author: TheBloke
  content: "New quants are uploaded and are working fine\n\nQ3_K_M:\n\n```\nsystem_info:\
    \ n_threads = 15 / 30 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI\
    \ = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n =\
    \ 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty\
    \ = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000,\
    \ temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n\
    generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n Below\
    \ is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\n\n### Instruction:\nHow much wood would a woodchuck\
    \ chuck if a woodchuck could chuck wood?\n\n### Response:\n A woodchuck, also\
    \ known as a groundhog, is a herbivore and does not typically \"chuck\" (throw)\
    \ wood. They are more likely to burrow into the ground or move leaves and debris\
    \ with their strong front legs. However, if we were to assume that a woodchuck\
    \ could chuck wood like an axe, it's impossible to determine how much wood they\
    \ would be able to throw due to lack of information about their physical strength\
    \ or any context regarding the size and type of wood being referred to. [end of\
    \ text]\n```\n "
  created_at: 2023-10-16 09:51:06+00:00
  edited: true
  hidden: false
  id: 652d159ab45e702eb05457f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
      fullname: minipasila
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mpasila
      type: user
    createdAt: '2023-10-16T11:08:43.000Z'
    data:
      edited: false
      editors:
      - mpasila
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9983311295509338
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
          fullname: minipasila
          isHf: false
          isPro: false
          name: mpasila
          type: user
        html: '<p>yeah thanks now it''s working for me as well.</p>

          '
        raw: yeah thanks now it's working for me as well.
        updatedAt: '2023-10-16T11:08:43.997Z'
      numEdits: 0
      reactions: []
      relatedEventId: 652d19bc9e28b3ce9b4a9a2c
    id: 652d19bb9e28b3ce9b4a9a2b
    type: comment
  author: mpasila
  content: yeah thanks now it's working for me as well.
  created_at: 2023-10-16 10:08:43+00:00
  edited: false
  hidden: false
  id: 652d19bb9e28b3ce9b4a9a2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
      fullname: minipasila
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mpasila
      type: user
    createdAt: '2023-10-16T11:08:44.000Z'
    data:
      status: closed
    id: 652d19bc9e28b3ce9b4a9a2c
    type: status-change
  author: mpasila
  created_at: 2023-10-16 10:08:44+00:00
  id: 652d19bc9e28b3ce9b4a9a2c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Xwin-MLewd-13B-v0.2-GGUF
repo_type: model
status: closed
target_branch: null
title: Q3 and Q2 quants broken
