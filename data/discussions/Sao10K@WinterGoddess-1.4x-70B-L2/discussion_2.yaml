!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kriss
conflicting_files: null
created_at: 2023-12-26 14:04:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/99a9e22c0c58b8fa6eb9d1905c750f36.svg
      fullname: Ofir kr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kriss
      type: user
    createdAt: '2023-12-26T14:04:25.000Z'
    data:
      edited: false
      editors:
      - kriss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8821154832839966
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/99a9e22c0c58b8fa6eb9d1905c750f36.svg
          fullname: Ofir kr
          isHf: false
          isPro: false
          name: kriss
          type: user
        html: '<p>Great model!!<br>Max tokens is 4K still? or could it accept longer
          context?</p>

          '
        raw: "Great model!!\r\nMax tokens is 4K still? or could it accept longer context?"
        updatedAt: '2023-12-26T14:04:25.814Z'
      numEdits: 0
      reactions: []
    id: 658add698ea79e4df4e635f6
    type: comment
  author: kriss
  content: "Great model!!\r\nMax tokens is 4K still? or could it accept longer context?"
  created_at: 2023-12-26 14:04:25+00:00
  edited: false
  hidden: false
  id: 658add698ea79e4df4e635f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
      fullname: Saofiq
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Sao10K
      type: user
    createdAt: '2023-12-27T01:48:04.000Z'
    data:
      edited: true
      editors:
      - Sao10K
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9152934551239014
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
          fullname: Saofiq
          isHf: false
          isPro: false
          name: Sao10K
          type: user
        html: '<p>4k context as usual for llama 2, but with RoPE scaling you can go
          around 10k before it you feel any *noticable degradation. I usually keep
          at at 8k context with rope alpha 2.5-2.7</p>

          '
        raw: 4k context as usual for llama 2, but with RoPE scaling you can go around
          10k before it you feel any *noticable degradation. I usually keep at at
          8k context with rope alpha 2.5-2.7
        updatedAt: '2023-12-27T03:32:31.986Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kriss
    id: 658b82541783d1ebc75c75f0
    type: comment
  author: Sao10K
  content: 4k context as usual for llama 2, but with RoPE scaling you can go around
    10k before it you feel any *noticable degradation. I usually keep at at 8k context
    with rope alpha 2.5-2.7
  created_at: 2023-12-27 01:48:04+00:00
  edited: true
  hidden: false
  id: 658b82541783d1ebc75c75f0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Sao10K/WinterGoddess-1.4x-70B-L2
repo_type: model
status: open
target_branch: null
title: Max tokens?
