!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mvetter
conflicting_files: null
created_at: 2023-08-09 11:44:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c5bb80c2f7424cc2d51a67254f1c31c.svg
      fullname: Marco Vetter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mvetter
      type: user
    createdAt: '2023-08-09T12:44:42.000Z'
    data:
      edited: false
      editors:
      - mvetter
      hidden: false
      identifiedLanguage:
        language: de
        probability: 0.502045214176178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c5bb80c2f7424cc2d51a67254f1c31c.svg
          fullname: Marco Vetter
          isHf: false
          isPro: false
          name: mvetter
          type: user
        html: "<p>I'm not sure I'm doing this right, but if I hand the model a prompt\
          \ such as</p>\n<p>\"&lt;|prompter|&gt;Beschreibe das Sonnensystem.&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          </p>\n<p>I get a whole dialogue as response:</p>\n<p>\"&lt;s&gt; &lt;|prompter|&gt;Beschreibe\
          \ das Sonnensystem ausf\xFChrlich und im Detail.&lt;|endoftext|&gt;&lt;|assistant|&gt;Das\
          \ Sonnensystem besteht aus der Sonne, den Planeten, Kometen und anderen\
          \ Himmelsk\xF6rpern. &lt;|prompter|&gt;Wie viele Planeten gibt es im Sonnensystem?&lt;|endoftext|&gt;&lt;|assistant|&gt;Im\
          \ Sonnensystem gibt es neun Planeten: Merkurs, Venus, Erde, Mars, Jupiter,\
          \ Saturn, Uranus und Neptun. &lt;|prompter|&gt; [...]\"</p>\n<p>This goes\
          \ on until the model hits the max_length. </p>\n<p>Is this expected behavior\
          \ and should I just manually cut off everything after the latest reply?\
          \ That seems like a waste of compute/latency.</p>\n"
        raw: "I'm not sure I'm doing this right, but if I hand the model a prompt\
          \ such as\r\n\r\n\"<|prompter|>Beschreibe das Sonnensystem.<|endoftext|><|assistant|>\"\
          \r\n\r\nI get a whole dialogue as response:\r\n\r\n\"\\<s> <|prompter|>Beschreibe\
          \ das Sonnensystem ausf\xFChrlich und im Detail.<|endoftext|><|assistant|>Das\
          \ Sonnensystem besteht aus der Sonne, den Planeten, Kometen und anderen\
          \ Himmelsk\xF6rpern. <|prompter|>Wie viele Planeten gibt es im Sonnensystem?<|endoftext|><|assistant|>Im\
          \ Sonnensystem gibt es neun Planeten: Merkurs, Venus, Erde, Mars, Jupiter,\
          \ Saturn, Uranus und Neptun. <|prompter|> [...]\"\r\n\r\nThis goes on until\
          \ the model hits the max_length. \r\n\r\nIs this expected behavior and should\
          \ I just manually cut off everything after the latest reply? That seems\
          \ like a waste of compute/latency."
        updatedAt: '2023-08-09T12:44:42.826Z'
      numEdits: 0
      reactions: []
    id: 64d38a3a1074f37ed549a9ae
    type: comment
  author: mvetter
  content: "I'm not sure I'm doing this right, but if I hand the model a prompt such\
    \ as\r\n\r\n\"<|prompter|>Beschreibe das Sonnensystem.<|endoftext|><|assistant|>\"\
    \r\n\r\nI get a whole dialogue as response:\r\n\r\n\"\\<s> <|prompter|>Beschreibe\
    \ das Sonnensystem ausf\xFChrlich und im Detail.<|endoftext|><|assistant|>Das\
    \ Sonnensystem besteht aus der Sonne, den Planeten, Kometen und anderen Himmelsk\xF6\
    rpern. <|prompter|>Wie viele Planeten gibt es im Sonnensystem?<|endoftext|><|assistant|>Im\
    \ Sonnensystem gibt es neun Planeten: Merkurs, Venus, Erde, Mars, Jupiter, Saturn,\
    \ Uranus und Neptun. <|prompter|> [...]\"\r\n\r\nThis goes on until the model\
    \ hits the max_length. \r\n\r\nIs this expected behavior and should I just manually\
    \ cut off everything after the latest reply? That seems like a waste of compute/latency."
  created_at: 2023-08-09 11:44:42+00:00
  edited: false
  hidden: false
  id: 64d38a3a1074f37ed549a9ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-08-12T20:31:39.000Z'
    data:
      edited: false
      editors:
      - flozi00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9019257426261902
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
          fullname: Florian Zimmermeister
          isHf: false
          isPro: false
          name: flozi00
          type: user
        html: '<p>Hmm strange<br>Didn''t see that while validation.<br>Actually I
          am using the latest 13b model for inference, the 7b was for experimental
          usage</p>

          '
        raw: 'Hmm strange

          Didn''t see that while validation.

          Actually I am using the latest 13b model for inference, the 7b was for experimental
          usage'
        updatedAt: '2023-08-12T20:31:39.073Z'
      numEdits: 0
      reactions: []
    id: 64d7ec2baa89fb548accc479
    type: comment
  author: flozi00
  content: 'Hmm strange

    Didn''t see that while validation.

    Actually I am using the latest 13b model for inference, the 7b was for experimental
    usage'
  created_at: 2023-08-12 19:31:39+00:00
  edited: false
  hidden: false
  id: 64d7ec2baa89fb548accc479
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c5bb80c2f7424cc2d51a67254f1c31c.svg
      fullname: Marco Vetter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mvetter
      type: user
    createdAt: '2023-08-15T14:08:48.000Z'
    data:
      edited: false
      editors:
      - mvetter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9830382466316223
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c5bb80c2f7424cc2d51a67254f1c31c.svg
          fullname: Marco Vetter
          isHf: false
          isPro: false
          name: mvetter
          type: user
        html: '<p>Thanks, I tried the 13B v4 and it worked  (apart from having to
          load the tokenizer from the GPTQ version). So there has to be something
          going on with the 7B model.</p>

          '
        raw: Thanks, I tried the 13B v4 and it worked  (apart from having to load
          the tokenizer from the GPTQ version). So there has to be something going
          on with the 7B model.
        updatedAt: '2023-08-15T14:08:48.395Z'
      numEdits: 0
      reactions: []
    id: 64db86f0d68a6ddcc7c2fc68
    type: comment
  author: mvetter
  content: Thanks, I tried the 13B v4 and it worked  (apart from having to load the
    tokenizer from the GPTQ version). So there has to be something going on with the
    7B model.
  created_at: 2023-08-15 13:08:48+00:00
  edited: false
  hidden: false
  id: 64db86f0d68a6ddcc7c2fc68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-08-15T16:59:11.000Z'
    data:
      edited: true
      editors:
      - flozi00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.858593761920929
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
          fullname: Florian Zimmermeister
          isHf: false
          isPro: false
          name: flozi00
          type: user
        html: '<p>Ah thanks for reminder, still forgot to add the tokenizer to the
          base model :)</p>

          '
        raw: Ah thanks for reminder, still forgot to add the tokenizer to the base
          model :)
        updatedAt: '2023-08-15T17:01:26.136Z'
      numEdits: 1
      reactions: []
    id: 64dbaedfd0645cd60ca3c333
    type: comment
  author: flozi00
  content: Ah thanks for reminder, still forgot to add the tokenizer to the base model
    :)
  created_at: 2023-08-15 15:59:11+00:00
  edited: true
  hidden: false
  id: 64dbaedfd0645cd60ca3c333
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-08-15T17:01:16.000Z'
    data:
      status: closed
    id: 64dbaf5caf71513f292cf2df
    type: status-change
  author: flozi00
  created_at: 2023-08-15 16:01:16+00:00
  id: 64dbaf5caf71513f292cf2df
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flozi00/Llama-2-7b-german-assistant-v2
repo_type: model
status: closed
target_branch: null
title: Model not honoring suggested prompt format?
