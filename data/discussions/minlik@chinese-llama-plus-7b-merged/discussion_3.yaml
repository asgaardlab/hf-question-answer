!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zhehuderek
conflicting_files: null
created_at: 2023-05-31 08:01:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/99736de1bc0d5decf4a6eda86e3c7937.svg
      fullname: Derek Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhehuderek
      type: user
    createdAt: '2023-05-31T09:01:33.000Z'
    data:
      edited: false
      editors:
      - zhehuderek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/99736de1bc0d5decf4a6eda86e3c7937.svg
          fullname: Derek Hu
          isHf: false
          isPro: false
          name: zhehuderek
          type: user
        html: '<p>Thanks for sharing the model! I have one question: As in the original
          github repo, I noticed they use lora to train the model, while you directly
          load the model using LlamaForCausalLM (w/o lora params). So I wonder what''s
          the difference between the model and the original one? Thank you!</p>

          '
        raw: 'Thanks for sharing the model! I have one question: As in the original
          github repo, I noticed they use lora to train the model, while you directly
          load the model using LlamaForCausalLM (w/o lora params). So I wonder what''s
          the difference between the model and the original one? Thank you!'
        updatedAt: '2023-05-31T09:01:33.607Z'
      numEdits: 0
      reactions: []
    id: 64770ced5ef58684691e4bca
    type: comment
  author: zhehuderek
  content: 'Thanks for sharing the model! I have one question: As in the original
    github repo, I noticed they use lora to train the model, while you directly load
    the model using LlamaForCausalLM (w/o lora params). So I wonder what''s the difference
    between the model and the original one? Thank you!'
  created_at: 2023-05-31 08:01:33+00:00
  edited: false
  hidden: false
  id: 64770ced5ef58684691e4bca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
      fullname: kuan li
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: minlik
      type: user
    createdAt: '2023-05-31T11:33:07.000Z'
    data:
      edited: false
      editors:
      - minlik
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
          fullname: kuan li
          isHf: false
          isPro: false
          name: minlik
          type: user
        html: '<p>The weights are the same. I merged the Lora weight with the original
          model weight, allowing this model to be loaded with LlamaForCausalLM and
          fine-tuned directly.</p>

          '
        raw: The weights are the same. I merged the Lora weight with the original
          model weight, allowing this model to be loaded with LlamaForCausalLM and
          fine-tuned directly.
        updatedAt: '2023-05-31T11:33:07.792Z'
      numEdits: 0
      reactions: []
    id: 6477307340c99df87603e98e
    type: comment
  author: minlik
  content: The weights are the same. I merged the Lora weight with the original model
    weight, allowing this model to be loaded with LlamaForCausalLM and fine-tuned
    directly.
  created_at: 2023-05-31 10:33:07+00:00
  edited: false
  hidden: false
  id: 6477307340c99df87603e98e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/99736de1bc0d5decf4a6eda86e3c7937.svg
      fullname: Derek Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhehuderek
      type: user
    createdAt: '2023-05-31T12:18:43.000Z'
    data:
      edited: false
      editors:
      - zhehuderek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/99736de1bc0d5decf4a6eda86e3c7937.svg
          fullname: Derek Hu
          isHf: false
          isPro: false
          name: zhehuderek
          type: user
        html: '<p>Thanks for answering. I think if the lora weights exist, usually
          you need to load model with codes like:</p>

          <pre><code>based_model = LlamaForCausalLM.from_pretrained(xxx)

          model = PeftModel.from_pretrained(based_model, xxx)  # say if you use Peft
          for lora implementation

          </code></pre>

          <p>Thus I''m still confused that how did you merge the lora weights and
          load the model with LlamaForCausalLM, as the huggingface implementation
          of LlamaForCausalLM does not included any params of lora, right? I would
          really appreciate your help on this.</p>

          <p>Best</p>

          '
        raw: 'Thanks for answering. I think if the lora weights exist, usually you
          need to load model with codes like:

          ```

          based_model = LlamaForCausalLM.from_pretrained(xxx)

          model = PeftModel.from_pretrained(based_model, xxx)  # say if you use Peft
          for lora implementation

          ```

          Thus I''m still confused that how did you merge the lora weights and load
          the model with LlamaForCausalLM, as the huggingface implementation of LlamaForCausalLM
          does not included any params of lora, right? I would really appreciate your
          help on this.


          Best'
        updatedAt: '2023-05-31T12:18:43.176Z'
      numEdits: 0
      reactions: []
    id: 64773b2340c99df87604daea
    type: comment
  author: zhehuderek
  content: 'Thanks for answering. I think if the lora weights exist, usually you need
    to load model with codes like:

    ```

    based_model = LlamaForCausalLM.from_pretrained(xxx)

    model = PeftModel.from_pretrained(based_model, xxx)  # say if you use Peft for
    lora implementation

    ```

    Thus I''m still confused that how did you merge the lora weights and load the
    model with LlamaForCausalLM, as the huggingface implementation of LlamaForCausalLM
    does not included any params of lora, right? I would really appreciate your help
    on this.


    Best'
  created_at: 2023-05-31 11:18:43+00:00
  edited: false
  hidden: false
  id: 64773b2340c99df87604daea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
      fullname: kuan li
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: minlik
      type: user
    createdAt: '2023-06-01T02:30:44.000Z'
    data:
      edited: false
      editors:
      - minlik
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
          fullname: kuan li
          isHf: false
          isPro: false
          name: minlik
          type: user
        html: '<p>You can refer to the following links, including the project instruction
          and the code script.<br><a rel="nofollow" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus">https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus</a><br><a
          rel="nofollow" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_llama_with_chinese_lora.py">https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_llama_with_chinese_lora.py</a></p>

          '
        raw: "You can refer to the following links, including the project instruction\
          \ and the code script. \nhttps://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus\n\
          https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_llama_with_chinese_lora.py"
        updatedAt: '2023-06-01T02:30:44.812Z'
      numEdits: 0
      reactions: []
    id: 647802d433a888101f831816
    type: comment
  author: minlik
  content: "You can refer to the following links, including the project instruction\
    \ and the code script. \nhttps://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus\n\
    https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_llama_with_chinese_lora.py"
  created_at: 2023-06-01 01:30:44+00:00
  edited: false
  hidden: false
  id: 647802d433a888101f831816
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/99736de1bc0d5decf4a6eda86e3c7937.svg
      fullname: Derek Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhehuderek
      type: user
    createdAt: '2023-06-02T07:03:19.000Z'
    data:
      status: closed
    id: 6479943735535d8bd80919b8
    type: status-change
  author: zhehuderek
  created_at: 2023-06-02 06:03:19+00:00
  id: 6479943735535d8bd80919b8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: minlik/chinese-llama-plus-7b-merged
repo_type: model
status: closed
target_branch: null
title: Question regarding lora params
