!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jackpark
conflicting_files: null
created_at: 2023-12-02 22:04:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b5b155efbd18dd72c074e3f94e3b024.svg
      fullname: Jack Park
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackpark
      type: user
    createdAt: '2023-12-02T22:04:34.000Z'
    data:
      edited: false
      editors:
      - jackpark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8500930070877075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b5b155efbd18dd72c074e3f94e3b024.svg
          fullname: Jack Park
          isHf: false
          isPro: false
          name: jackpark
          type: user
        html: '<p>I have the cuda toolkit installed but it''s failing as follows<br><code>building
          ggml-cuda with nvcc -arch=native... nvcc fatal   : Value ''native'' is not
          defined for option ''gpu-architecture'' /usr/bin/nvcc: returned nonzero
          exit status building nvidia compute capability detector... cudaGetDeviceCount()
          failed: no CUDA-capable device is detected</code></p>

          <p>I am simply booting the binary as suggested here<br><a rel="nofollow"
          href="https://simonwillison.net/2023/Nov/29/llamafile/">https://simonwillison.net/2023/Nov/29/llamafile/</a></p>

          <p>Thanks in advance<br>Jack</p>

          '
        raw: "I have the cuda toolkit installed but it's failing as follows\r\n`building\
          \ ggml-cuda with nvcc -arch=native...\r\nnvcc fatal   : Value 'native' is\
          \ not defined for option 'gpu-architecture'\r\n/usr/bin/nvcc: returned nonzero\
          \ exit status\r\nbuilding nvidia compute capability detector...\r\ncudaGetDeviceCount()\
          \ failed: no CUDA-capable device is detected`\r\n\r\nI am simply booting\
          \ the binary as suggested here\r\nhttps://simonwillison.net/2023/Nov/29/llamafile/\r\
          \n\r\nThanks in advance\r\nJack"
        updatedAt: '2023-12-02T22:04:34.675Z'
      numEdits: 0
      reactions: []
    id: 656ba9f23dbac3a83df05e9d
    type: comment
  author: jackpark
  content: "I have the cuda toolkit installed but it's failing as follows\r\n`building\
    \ ggml-cuda with nvcc -arch=native...\r\nnvcc fatal   : Value 'native' is not\
    \ defined for option 'gpu-architecture'\r\n/usr/bin/nvcc: returned nonzero exit\
    \ status\r\nbuilding nvidia compute capability detector...\r\ncudaGetDeviceCount()\
    \ failed: no CUDA-capable device is detected`\r\n\r\nI am simply booting the binary\
    \ as suggested here\r\nhttps://simonwillison.net/2023/Nov/29/llamafile/\r\n\r\n\
    Thanks in advance\r\nJack"
  created_at: 2023-12-02 22:04:34+00:00
  edited: false
  hidden: false
  id: 656ba9f23dbac3a83df05e9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
      fullname: Justine
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jartine
      type: user
    createdAt: '2023-12-03T08:01:44.000Z'
    data:
      edited: false
      editors:
      - jartine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769658446311951
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
          fullname: Justine
          isHf: false
          isPro: false
          name: jartine
          type: user
        html: '<p>If CUDA says your video card doesn''t support CUDA then there''s
          really not a whole lot we can do on our end. You probably need to buy a
          different graphics card.</p>

          '
        raw: If CUDA says your video card doesn't support CUDA then there's really
          not a whole lot we can do on our end. You probably need to buy a different
          graphics card.
        updatedAt: '2023-12-03T08:01:44.522Z'
      numEdits: 0
      reactions: []
    id: 656c35e8ede71189ad2f7f0a
    type: comment
  author: jartine
  content: If CUDA says your video card doesn't support CUDA then there's really not
    a whole lot we can do on our end. You probably need to buy a different graphics
    card.
  created_at: 2023-12-03 08:01:44+00:00
  edited: false
  hidden: false
  id: 656c35e8ede71189ad2f7f0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b5b155efbd18dd72c074e3f94e3b024.svg
      fullname: Jack Park
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackpark
      type: user
    createdAt: '2023-12-03T23:21:57.000Z'
    data:
      edited: false
      editors:
      - jackpark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9301227927207947
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b5b155efbd18dd72c074e3f94e3b024.svg
          fullname: Jack Park
          isHf: false
          isPro: false
          name: jackpark
          type: user
        html: '<p>Thanks.<br>I just now have an update on this situation.<br>I booted
          the Llava server on the same box, and it runs just fine.<br>That leaves
          me curious: why does the Mistral server fail but the Llava server does not
          fail?</p>

          '
        raw: "Thanks. \nI just now have an update on this situation.  \nI booted the\
          \ Llava server on the same box, and it runs just fine.\nThat leaves me curious:\
          \ why does the Mistral server fail but the Llava server does not fail?"
        updatedAt: '2023-12-03T23:21:57.312Z'
      numEdits: 0
      reactions: []
    id: 656d0d9502a56b531abb1ee3
    type: comment
  author: jackpark
  content: "Thanks. \nI just now have an update on this situation.  \nI booted the\
    \ Llava server on the same box, and it runs just fine.\nThat leaves me curious:\
    \ why does the Mistral server fail but the Llava server does not fail?"
  created_at: 2023-12-03 23:21:57+00:00
  edited: false
  hidden: false
  id: 656d0d9502a56b531abb1ee3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
      fullname: Justine
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jartine
      type: user
    createdAt: '2023-12-04T02:18:18.000Z'
    data:
      edited: false
      editors:
      - jartine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754316806793213
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
          fullname: Justine
          isHf: false
          isPro: false
          name: jartine
          type: user
        html: '<p>Chances are you downloaded mistral vs. llava at different times,
          and you ended up with different release versions. The files on this HF have
          been updated about five times over the last few days as new bug fix releases
          have come out. If you''ve got the bandwidth to spare, I''d say just try
          redownloading it.</p>

          '
        raw: Chances are you downloaded mistral vs. llava at different times, and
          you ended up with different release versions. The files on this HF have
          been updated about five times over the last few days as new bug fix releases
          have come out. If you've got the bandwidth to spare, I'd say just try redownloading
          it.
        updatedAt: '2023-12-04T02:18:18.430Z'
      numEdits: 0
      reactions: []
    id: 656d36eac4d6794d7f9b56bb
    type: comment
  author: jartine
  content: Chances are you downloaded mistral vs. llava at different times, and you
    ended up with different release versions. The files on this HF have been updated
    about five times over the last few days as new bug fix releases have come out.
    If you've got the bandwidth to spare, I'd say just try redownloading it.
  created_at: 2023-12-04 02:18:18+00:00
  edited: false
  hidden: false
  id: 656d36eac4d6794d7f9b56bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
      fullname: Justine
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jartine
      type: user
    createdAt: '2023-12-04T02:29:00.000Z'
    data:
      edited: false
      editors:
      - jartine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6161746382713318
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
          fullname: Justine
          isHf: false
          isPro: false
          name: jartine
          type: user
        html: '<p>Think I found the cause. Follow <a rel="nofollow" href="https://github.com/Mozilla-Ocho/llamafile/issues/50">https://github.com/Mozilla-Ocho/llamafile/issues/50</a>
          for updates.</p>

          '
        raw: Think I found the cause. Follow https://github.com/Mozilla-Ocho/llamafile/issues/50
          for updates.
        updatedAt: '2023-12-04T02:29:00.829Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656d396c3dbac3a83d3b4abe
    id: 656d396c3dbac3a83d3b4ab7
    type: comment
  author: jartine
  content: Think I found the cause. Follow https://github.com/Mozilla-Ocho/llamafile/issues/50
    for updates.
  created_at: 2023-12-04 02:29:00+00:00
  edited: false
  hidden: false
  id: 656d396c3dbac3a83d3b4ab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
      fullname: Justine
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jartine
      type: user
    createdAt: '2023-12-04T02:29:00.000Z'
    data:
      status: closed
    id: 656d396c3dbac3a83d3b4abe
    type: status-change
  author: jartine
  created_at: 2023-12-04 02:29:00+00:00
  id: 656d396c3dbac3a83d3b4abe
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jartine/mistral-7b.llamafile
repo_type: model
status: closed
target_branch: null
title: how to run on ubuntu without a gpu?
