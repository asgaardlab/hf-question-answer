!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KatyKunXD
conflicting_files: null
created_at: 2023-12-04 14:27:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ba11f383d0eabbdeedc6b309c9410d6.svg
      fullname: Katy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KatyKunXD
      type: user
    createdAt: '2023-12-04T14:27:18.000Z'
    data:
      edited: false
      editors:
      - KatyKunXD
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7477689981460571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ba11f383d0eabbdeedc6b309c9410d6.svg
          fullname: Katy
          isHf: false
          isPro: false
          name: KatyKunXD
          type: user
        html: '<p>Would you be able to make a single llamafile of NousResearch/Obsidian-3B-V0.5,
          like the llava 1.5 server model? Thank you for creating this!</p>

          '
        raw: "Would you be able to make a single llamafile of NousResearch/Obsidian-3B-V0.5,\
          \ like the llava 1.5 server model? Thank you for creating this!\r\n"
        updatedAt: '2023-12-04T14:27:18.778Z'
      numEdits: 0
      reactions: []
    id: 656de1c6b9fa60e33d2d59d2
    type: comment
  author: KatyKunXD
  content: "Would you be able to make a single llamafile of NousResearch/Obsidian-3B-V0.5,\
    \ like the llava 1.5 server model? Thank you for creating this!\r\n"
  created_at: 2023-12-04 14:27:18+00:00
  edited: false
  hidden: false
  id: 656de1c6b9fa60e33d2d59d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
      fullname: Justine
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jartine
      type: user
    createdAt: '2023-12-05T10:53:34.000Z'
    data:
      edited: false
      editors:
      - jartine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9016837477684021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
          fullname: Justine
          isHf: false
          isPro: false
          name: jartine
          type: user
        html: '<p>You''re able to do it. See <a rel="nofollow" href="https://github.com/mozilla-Ocho/llamafile#source-instructions">https://github.com/mozilla-Ocho/llamafile#source-instructions</a>
          for instructions on how you can create your own llamafiles. It''d be nice
          to see other people posting their own llamafiles on Hugging Face. If you
          do it, then I recommend that you document what llamafile release or git
          sha you''re using for the binary content. It''s also a good idea to include
          the <code>cosmocc --version</code>. That way, anyone who needs to, will
          be able to reproduce your builds.</p>

          '
        raw: You're able to do it. See https://github.com/mozilla-Ocho/llamafile#source-instructions
          for instructions on how you can create your own llamafiles. It'd be nice
          to see other people posting their own llamafiles on Hugging Face. If you
          do it, then I recommend that you document what llamafile release or git
          sha you're using for the binary content. It's also a good idea to include
          the `cosmocc --version`. That way, anyone who needs to, will be able to
          reproduce your builds.
        updatedAt: '2023-12-05T10:53:34.556Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656f012ed4de03a07d3b3f6d
    id: 656f012ed4de03a07d3b3f6b
    type: comment
  author: jartine
  content: You're able to do it. See https://github.com/mozilla-Ocho/llamafile#source-instructions
    for instructions on how you can create your own llamafiles. It'd be nice to see
    other people posting their own llamafiles on Hugging Face. If you do it, then
    I recommend that you document what llamafile release or git sha you're using for
    the binary content. It's also a good idea to include the `cosmocc --version`.
    That way, anyone who needs to, will be able to reproduce your builds.
  created_at: 2023-12-05 10:53:34+00:00
  edited: false
  hidden: false
  id: 656f012ed4de03a07d3b3f6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/8B1H1ApkAfeBdH03lmYll.png?w=200&h=200&f=face
      fullname: Justine
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jartine
      type: user
    createdAt: '2023-12-05T10:53:34.000Z'
    data:
      status: closed
    id: 656f012ed4de03a07d3b3f6d
    type: status-change
  author: jartine
  created_at: 2023-12-05 10:53:34+00:00
  id: 656f012ed4de03a07d3b3f6d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jartine/mistral-7b.llamafile
repo_type: model
status: closed
target_branch: null
title: Obsidian llamafile?
