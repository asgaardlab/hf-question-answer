!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mstachow
conflicting_files: null
created_at: 2023-09-11 14:10:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-09-11T15:10:56.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9160528182983398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>I''ve run it so far with fairly long inputs, but what is the max
          token length of the model?</p>

          '
        raw: I've run it so far with fairly long inputs, but what is the max token
          length of the model?
        updatedAt: '2023-09-11T15:10:56.679Z'
      numEdits: 0
      reactions: []
    id: 64ff2e009a384a7593423699
    type: comment
  author: mstachow
  content: I've run it so far with fairly long inputs, but what is the max token length
    of the model?
  created_at: 2023-09-11 14:10:56+00:00
  edited: false
  hidden: false
  id: 64ff2e009a384a7593423699
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-09-28T18:57:22.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.871745228767395
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mstachow&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mstachow\"\
          >@<span class=\"underline\">mstachow</span></a></span>\n\n\t</span></span>!\
          \ The model uses relative positional embeddings, instead of absolute positional\
          \ embeddings, so the max length is not bound by the architecture. Theoretically,\
          \ it has infinite max length if you have infinite compute. Instead, you'll\
          \ be bound by the <strong>memory</strong> of the model, which scales with\
          \ input length squared, and will depend on your hardware. However, you the\
          \ performance might degrade for super large input lengths as you lose prosody\
          \ over such long sequences.</p>\n"
        raw: Hey @mstachow! The model uses relative positional embeddings, instead
          of absolute positional embeddings, so the max length is not bound by the
          architecture. Theoretically, it has infinite max length if you have infinite
          compute. Instead, you'll be bound by the **memory** of the model, which
          scales with input length squared, and will depend on your hardware. However,
          you the performance might degrade for super large input lengths as you lose
          prosody over such long sequences.
        updatedAt: '2023-09-28T18:57:54.769Z'
      numEdits: 1
      reactions: []
    id: 6515cc927f18cec973a74050
    type: comment
  author: sanchit-gandhi
  content: Hey @mstachow! The model uses relative positional embeddings, instead of
    absolute positional embeddings, so the max length is not bound by the architecture.
    Theoretically, it has infinite max length if you have infinite compute. Instead,
    you'll be bound by the **memory** of the model, which scales with input length
    squared, and will depend on your hardware. However, you the performance might
    degrade for super large input lengths as you lose prosody over such long sequences.
  created_at: 2023-09-28 17:57:22+00:00
  edited: true
  hidden: false
  id: 6515cc927f18cec973a74050
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: facebook/mms-tts-eng
repo_type: model
status: open
target_branch: null
title: What is the maximum token length of the model?
