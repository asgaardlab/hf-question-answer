!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AlfredWALLACE
conflicting_files: null
created_at: 2023-12-08 08:05:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3322fa94233c0f5c04c356c2a79e7ef2.svg
      fullname: Alfred WALLACE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlfredWALLACE
      type: user
    createdAt: '2023-12-08T08:05:33.000Z'
    data:
      edited: true
      editors:
      - AlfredWALLACE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9813501238822937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3322fa94233c0f5c04c356c2a79e7ef2.svg
          fullname: Alfred WALLACE
          isHf: false
          isPro: false
          name: AlfredWALLACE
          type: user
        html: '<p>Thanks for the quantized model which allows us to test this great
          AI.<br>Would you share your conversion method as I was not able to do it
          myself with llama.cpp scripts and would like to quantize more versions ?</p>

          '
        raw: 'Thanks for the quantized model which allows us to test this great AI.

          Would you share your conversion method as I was not able to do it myself
          with llama.cpp scripts and would like to quantize more versions ?'
        updatedAt: '2023-12-08T08:06:21.758Z'
      numEdits: 1
      reactions: []
    id: 6572ce4d059d651f18de3b0e
    type: comment
  author: AlfredWALLACE
  content: 'Thanks for the quantized model which allows us to test this great AI.

    Would you share your conversion method as I was not able to do it myself with
    llama.cpp scripts and would like to quantize more versions ?'
  created_at: 2023-12-08 08:05:33+00:00
  edited: true
  hidden: false
  id: 6572ce4d059d651f18de3b0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-12T14:41:50.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5832400918006897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>Sure <span data-props=\"{&quot;user&quot;:&quot;AlfredWALLACE&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AlfredWALLACE\"\
          >@<span class=\"underline\">AlfredWALLACE</span></a></span>\n\n\t</span></span><br>You\
          \ have to download and compile llama.cpp from github </p>\n<pre><code>git\
          \ clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\nmake LLAMA_CUBLAS=1\
          \ \n</code></pre>\n<p>then you need to create python env and install requirements\
          \ of llama.cpp </p>\n<pre><code> pip install -r requirements.txt\n</code></pre>\n\
          <p>then run the convert script to make the f16 format </p>\n<pre><code>python\
          \ ~/dev/llama.cpp/convert.py ./Magicoder-S-CL-7B --outtype f16\n</code></pre>\n\
          <p>then run the compiled app <code>quantize</code> which will be generated\
          \ after compiling llama.cpp</p>\n<pre><code>quantize ./Magicoder-S-CL-7B/ggml-model-f16.gguf\
          \ q5_k_m\n</code></pre>\n<p>Good Luck.</p>\n"
        raw: "Sure @AlfredWALLACE \nYou have to download and compile llama.cpp from\
          \ github \n```\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\
          make LLAMA_CUBLAS=1 \n```\n\nthen you need to create python env and install\
          \ requirements of llama.cpp \n\n```\n pip install -r requirements.txt\n\
          ```\n\nthen run the convert script to make the f16 format \n```\npython\
          \ ~/dev/llama.cpp/convert.py ./Magicoder-S-CL-7B --outtype f16\n ``` \n\
          then run the compiled app `quantize` which will be generated after compiling\
          \ llama.cpp\n```\nquantize ./Magicoder-S-CL-7B/ggml-model-f16.gguf q5_k_m\n\
          ```\n\nGood Luck."
        updatedAt: '2023-12-12T14:41:50.994Z'
      numEdits: 0
      reactions: []
    id: 6578712e0c22bb8984c65463
    type: comment
  author: eramax
  content: "Sure @AlfredWALLACE \nYou have to download and compile llama.cpp from\
    \ github \n```\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\
    make LLAMA_CUBLAS=1 \n```\n\nthen you need to create python env and install requirements\
    \ of llama.cpp \n\n```\n pip install -r requirements.txt\n```\n\nthen run the\
    \ convert script to make the f16 format \n```\npython ~/dev/llama.cpp/convert.py\
    \ ./Magicoder-S-CL-7B --outtype f16\n ``` \nthen run the compiled app `quantize`\
    \ which will be generated after compiling llama.cpp\n```\nquantize ./Magicoder-S-CL-7B/ggml-model-f16.gguf\
    \ q5_k_m\n```\n\nGood Luck."
  created_at: 2023-12-12 14:41:50+00:00
  edited: false
  hidden: false
  id: 6578712e0c22bb8984c65463
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3322fa94233c0f5c04c356c2a79e7ef2.svg
      fullname: Alfred WALLACE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlfredWALLACE
      type: user
    createdAt: '2023-12-16T21:23:48.000Z'
    data:
      edited: false
      editors:
      - AlfredWALLACE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9796339273452759
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3322fa94233c0f5c04c356c2a79e7ef2.svg
          fullname: Alfred WALLACE
          isHf: false
          isPro: false
          name: AlfredWALLACE
          type: user
        html: '<p>Thanks! I had no luck with loading the model quantized with the
          same commands, previous to my post, but with a S-DS model.</p>

          '
        raw: Thanks! I had no luck with loading the model quantized with the same
          commands, previous to my post, but with a S-DS model.
        updatedAt: '2023-12-16T21:23:48.482Z'
      numEdits: 0
      reactions: []
    id: 657e15649a9e347d4bb152cd
    type: comment
  author: AlfredWALLACE
  content: Thanks! I had no luck with loading the model quantized with the same commands,
    previous to my post, but with a S-DS model.
  created_at: 2023-12-16 21:23:48+00:00
  edited: false
  hidden: false
  id: 657e15649a9e347d4bb152cd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: eramax/Magicoder-S-CL-7B-gguf
repo_type: model
status: open
target_branch: null
title: Conversion process
