!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Mr1gh
conflicting_files: null
created_at: 2023-05-10 14:05:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8edf9127dc830edd3a9b2c89dc0f8ae.svg
      fullname: Marwan gh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mr1gh
      type: user
    createdAt: '2023-05-10T15:05:17.000Z'
    data:
      edited: false
      editors:
      - Mr1gh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8edf9127dc830edd3a9b2c89dc0f8ae.svg
          fullname: Marwan gh
          isHf: false
          isPro: false
          name: Mr1gh
          type: user
        html: '<p>for wav more than 6 s this problem occurs, when I search I get that
          "Whisper decoder uses a learned position embedding which has the max length
          of 448 tokens. Therefore it cannot decode any transcription of more than
          448 label ids." is that mean that whisper can be trained on only fixed max
          length of tokens, and it can''t be changed?  </p>

          '
        raw: 'for wav more than 6 s this problem occurs, when I search I get that
          "Whisper decoder uses a learned position embedding which has the max length
          of 448 tokens. Therefore it cannot decode any transcription of more than
          448 label ids." is that mean that whisper can be trained on only fixed max
          length of tokens, and it can''t be changed?  '
        updatedAt: '2023-05-10T15:05:17.962Z'
      numEdits: 0
      reactions: []
    id: 645bb2ad1c24cd669dd83581
    type: comment
  author: Mr1gh
  content: 'for wav more than 6 s this problem occurs, when I search I get that "Whisper
    decoder uses a learned position embedding which has the max length of 448 tokens.
    Therefore it cannot decode any transcription of more than 448 label ids." is that
    mean that whisper can be trained on only fixed max length of tokens, and it can''t
    be changed?  '
  created_at: 2023-05-10 14:05:17+00:00
  edited: false
  hidden: false
  id: 645bb2ad1c24cd669dd83581
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: openai/whisper-base
repo_type: model
status: open
target_branch: null
title: 'The size of tensor a (449) must match the size of tensor b (448) at non-singleton
  dimension 1 '
