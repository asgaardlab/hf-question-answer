!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GroundSpyder
conflicting_files: null
created_at: 2023-07-28 11:16:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e311308e15c7c689fec2bdc700d9c78.svg
      fullname: Chris Fruin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GroundSpyder
      type: user
    createdAt: '2023-07-28T12:16:23.000Z'
    data:
      edited: false
      editors:
      - GroundSpyder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48624515533447266
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e311308e15c7c689fec2bdc700d9c78.svg
          fullname: Chris Fruin
          isHf: false
          isPro: false
          name: GroundSpyder
          type: user
        html: "<p>I loaded the model into oogabooga just as showing in this video:\
          \ <a rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=k-LUHw4Hb_w&amp;t=145s\"\
          >https://www.youtube.com/watch?v=k-LUHw4Hb_w&amp;t=145s</a></p>\n<p>But\
          \ when I load the model I get the following:<br>2023-07-28 08:06:59 INFO:Loading\
          \ georgesung_llama2_7b_chat_uncensored...<br>Loading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:12&lt;00:00,\
          \  4.14s/it]<br>2023-07-28 08:07:12 WARNING:models\\georgesung_llama2_7b_chat_uncensored\\\
          special_tokens_map.json is different from the original LlamaTokenizer file.\
          \ It is either customized or outdated.<br>2023-07-28 08:07:12 INFO:Loaded\
          \ the model in 12.68 seconds.</p>\n<p>And when I try to generate any kind\
          \ of response I get the following error and no response:<br>Traceback (most\
          \ recent call last):<br>  File \"C:\\oobabooga_windows\\text-generation-webui\\\
          modules\\callbacks.py\", line 55, in gentask<br>    ret = self.mfunc(callback=_callback,\
          \ *args, **self.kwargs)<br>  File \"C:\\oobabooga_windows\\text-generation-webui\\\
          modules\\text_generation.py\", line 293, in generate_with_callback<br> \
          \   shared.model.generate(**kwargs)<br>  File \"C:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\utils_contextlib.py\",\
          \ line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 1335, in generate<br>    and\
          \ torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) &gt;\
          \ 0<br>IndexError: index -1 is out of bounds for dimension 1 with size 0<br>Output\
          \ generated in 0.25 seconds (0.00 tokens/s, 0 tokens, context 0, seed 1811491874)</p>\n\
          <p>Any idea what I have done wrong?</p>\n"
        raw: "I loaded the model into oogabooga just as showing in this video: https://www.youtube.com/watch?v=k-LUHw4Hb_w&t=145s\r\
          \n\r\nBut when I load the model I get the following: \r\n2023-07-28 08:06:59\
          \ INFO:Loading georgesung_llama2_7b_chat_uncensored...\r\nLoading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3\
          \ [00:12<00:00,  4.14s/it]\r\n2023-07-28 08:07:12 WARNING:models\\georgesung_llama2_7b_chat_uncensored\\\
          special_tokens_map.json is different from the original LlamaTokenizer file.\
          \ It is either customized or outdated.\r\n2023-07-28 08:07:12 INFO:Loaded\
          \ the model in 12.68 seconds.\r\n\r\nAnd when I try to generate any kind\
          \ of response I get the following error and no response:\r\nTraceback (most\
          \ recent call last):\r\n  File \"C:\\oobabooga_windows\\text-generation-webui\\\
          modules\\callbacks.py\", line 55, in gentask\r\n    ret = self.mfunc(callback=_callback,\
          \ *args, **self.kwargs)\r\n  File \"C:\\oobabooga_windows\\text-generation-webui\\\
          modules\\text_generation.py\", line 293, in generate_with_callback\r\n \
          \   shared.model.generate(**kwargs)\r\n  File \"C:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 1335, in generate\r\n    and\
          \ torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) > 0\r\
          \nIndexError: index -1 is out of bounds for dimension 1 with size 0\r\n\
          Output generated in 0.25 seconds (0.00 tokens/s, 0 tokens, context 0, seed\
          \ 1811491874)\r\n\r\n\r\nAny idea what I have done wrong?"
        updatedAt: '2023-07-28T12:16:23.289Z'
      numEdits: 0
      reactions: []
    id: 64c3b197e4e2212a4fd92b83
    type: comment
  author: GroundSpyder
  content: "I loaded the model into oogabooga just as showing in this video: https://www.youtube.com/watch?v=k-LUHw4Hb_w&t=145s\r\
    \n\r\nBut when I load the model I get the following: \r\n2023-07-28 08:06:59 INFO:Loading\
    \ georgesung_llama2_7b_chat_uncensored...\r\nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 3/3 [00:12<00:00,  4.14s/it]\r\n2023-07-28 08:07:12\
    \ WARNING:models\\georgesung_llama2_7b_chat_uncensored\\special_tokens_map.json\
    \ is different from the original LlamaTokenizer file. It is either customized\
    \ or outdated.\r\n2023-07-28 08:07:12 INFO:Loaded the model in 12.68 seconds.\r\
    \n\r\nAnd when I try to generate any kind of response I get the following error\
    \ and no response:\r\nTraceback (most recent call last):\r\n  File \"C:\\oobabooga_windows\\\
    text-generation-webui\\modules\\callbacks.py\", line 55, in gentask\r\n    ret\
    \ = self.mfunc(callback=_callback, *args, **self.kwargs)\r\n  File \"C:\\oobabooga_windows\\\
    text-generation-webui\\modules\\text_generation.py\", line 293, in generate_with_callback\r\
    \n    shared.model.generate(**kwargs)\r\n  File \"C:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\
    \n    return func(*args, **kwargs)\r\n  File \"C:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1335, in generate\r\
    \n    and torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) >\
    \ 0\r\nIndexError: index -1 is out of bounds for dimension 1 with size 0\r\nOutput\
    \ generated in 0.25 seconds (0.00 tokens/s, 0 tokens, context 0, seed 1811491874)\r\
    \n\r\n\r\nAny idea what I have done wrong?"
  created_at: 2023-07-28 11:16:23+00:00
  edited: false
  hidden: false
  id: 64c3b197e4e2212a4fd92b83
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: georgesung/llama2_7b_chat_uncensored
repo_type: model
status: open
target_branch: null
title: special_tokens_map.json is customized or outdated.
