!!python/object:huggingface_hub.community.DiscussionWithDetails
author: littleevillin
conflicting_files: null
created_at: 2023-06-29 05:15:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16d7fbdfdabac1f97188ce6032afd5e2.svg
      fullname: lin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleevillin
      type: user
    createdAt: '2023-06-29T06:15:07.000Z'
    data:
      edited: false
      editors:
      - littleevillin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3853168487548828
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16d7fbdfdabac1f97188ce6032afd5e2.svg
          fullname: lin
          isHf: false
          isPro: false
          name: littleevillin
          type: user
        html: "<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\"\
          , trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\n\
          inputs = tokenizer('\u767B\u9E73\u96C0\u697C-&gt;\u738B\u4E4B\u6DA3\\n\u591C\
          \u96E8\u5BC4\u5317-&gt;', return_tensors='pt')\ninputs = inputs.to('cuda:0')\n\
          pred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\n\
          print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\nresult\uFF1A\
          \n\u767B\u9E73\u96C0\u697C-&gt;\u738B\u4E4B\u6DA3\n\u591C\u96E8\u5BC4\u5317\
          -&gt;\u674E\u5546\u9690\n\u8FC7\u96F6\u4E01\u6D0B-&gt;\u6587\u5929\u7965\
          \n\u5DF1\u4EA5\u6742\u8BD7(\u5176\u4E94)-&gt;\u9F9A\u81EA\u73CD\n</code></pre>\n\
          <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer,\
          \ TextStreamer\nfrom peft import PeftModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          baichuan-inc/baichuan-7B\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\n\
          model = PeftModel.from_pretrained(model, \"hiyouga/baichuan-7b-sft\")\n\
          streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\
          query = \"\u665A\u4E0A\u7761\u4E0D\u7740\u600E\u4E48\u529E\"\ninputs = tokenizer([\"\
          &lt;human&gt;:{}\\n&lt;bot&gt;:\".format(query)], return_tensors=\"pt\"\
          )\ninputs = inputs.to(\"cuda\")\ngenerate_ids = model.generate(**inputs,\
          \ max_new_tokens=256, streamer=streamer)\n\nresult\uFF1A\nTraceback (most\
          \ recent call last):\n  File \"model-sft.py\", line 10, in &lt;module&gt;\n\
          \    model = PeftModel.from_pretrained(model, \"/root/.cache/huggingface/hub/models--hiyouga--baichuan-7b-sft/snapshots/64cf906a964bc94bf754cef8aa0d8c05107c7745\"\
          )\n  File \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\"\
          , line 181, in from_pretrained\n    model.load_adapter(model_id, adapter_name,\
          \ **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\"\
          , line 406, in load_adapter\n    dispatch_model(\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py\"\
          , line 345, in dispatch_model\n    raise ValueError(\nValueError: We need\
          \ an `offload_dir` to dispatch this model according to this `device_map`,\
          \ the following submodules need to be offloaded: base_model.model.model.layers.18,\
          \ base_model.model.model.layers.19, base_model.model.model.layers.20, base_model.model.model.layers.21,\
          \ base_model.model.model.layers.22, base_model.model.model.layers.23, base_model.model.model.layers.24,\
          \ base_model.model.model.layers.25, base_model.model.model.layers.26, base_model.model.model.layers.27,\
          \ base_model.model.model.layers.28, base_model.model.model.layers.29, base_model.model.model.layers.30,\
          \ base_model.model.model.layers.31, base_model.model.model.norm, base_model.model.lm_head.\n\
          </code></pre>\n"
        raw: "```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\"\
          , trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\r\
          \ninputs = tokenizer('\u767B\u9E73\u96C0\u697C->\u738B\u4E4B\u6DA3\\n\u591C\
          \u96E8\u5BC4\u5317->', return_tensors='pt')\r\ninputs = inputs.to('cuda:0')\r\
          \npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\r\
          \nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\r\n\r\
          \nresult\uFF1A\r\n\u767B\u9E73\u96C0\u697C->\u738B\u4E4B\u6DA3\r\n\u591C\
          \u96E8\u5BC4\u5317->\u674E\u5546\u9690\r\n\u8FC7\u96F6\u4E01\u6D0B->\u6587\
          \u5929\u7965\r\n\u5DF1\u4EA5\u6742\u8BD7(\u5176\u4E94)->\u9F9A\u81EA\u73CD\
          \r\n```\r\n\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
          \ TextStreamer\r\nfrom peft import PeftModel\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          baichuan-inc/baichuan-7B\", trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\r\
          \nmodel = PeftModel.from_pretrained(model, \"hiyouga/baichuan-7b-sft\")\r\
          \nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\
          \nquery = \"\u665A\u4E0A\u7761\u4E0D\u7740\u600E\u4E48\u529E\"\r\ninputs\
          \ = tokenizer([\"<human>:{}\\n<bot>:\".format(query)], return_tensors=\"\
          pt\")\r\ninputs = inputs.to(\"cuda\")\r\ngenerate_ids = model.generate(**inputs,\
          \ max_new_tokens=256, streamer=streamer)\r\n\r\nresult\uFF1A\r\nTraceback\
          \ (most recent call last):\r\n  File \"model-sft.py\", line 10, in <module>\r\
          \n    model = PeftModel.from_pretrained(model, \"/root/.cache/huggingface/hub/models--hiyouga--baichuan-7b-sft/snapshots/64cf906a964bc94bf754cef8aa0d8c05107c7745\"\
          )\r\n  File \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\"\
          , line 181, in from_pretrained\r\n    model.load_adapter(model_id, adapter_name,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\"\
          , line 406, in load_adapter\r\n    dispatch_model(\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py\"\
          , line 345, in dispatch_model\r\n    raise ValueError(\r\nValueError: We\
          \ need an `offload_dir` to dispatch this model according to this `device_map`,\
          \ the following submodules need to be offloaded: base_model.model.model.layers.18,\
          \ base_model.model.model.layers.19, base_model.model.model.layers.20, base_model.model.model.layers.21,\
          \ base_model.model.model.layers.22, base_model.model.model.layers.23, base_model.model.model.layers.24,\
          \ base_model.model.model.layers.25, base_model.model.model.layers.26, base_model.model.model.layers.27,\
          \ base_model.model.model.layers.28, base_model.model.model.layers.29, base_model.model.model.layers.30,\
          \ base_model.model.model.layers.31, base_model.model.model.norm, base_model.model.lm_head.\r\
          \n```"
        updatedAt: '2023-06-29T06:15:07.457Z'
      numEdits: 0
      reactions: []
    id: 649d216baebf20f09a0e68f6
    type: comment
  author: littleevillin
  content: "```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\
    \r\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", device_map=\"\
    auto\", trust_remote_code=True)\r\ninputs = tokenizer('\u767B\u9E73\u96C0\u697C\
    ->\u738B\u4E4B\u6DA3\\n\u591C\u96E8\u5BC4\u5317->', return_tensors='pt')\r\ninputs\
    \ = inputs.to('cuda:0')\r\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\r\
    \nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\r\n\r\nresult\uFF1A\
    \r\n\u767B\u9E73\u96C0\u697C->\u738B\u4E4B\u6DA3\r\n\u591C\u96E8\u5BC4\u5317->\u674E\
    \u5546\u9690\r\n\u8FC7\u96F6\u4E01\u6D0B->\u6587\u5929\u7965\r\n\u5DF1\u4EA5\u6742\
    \u8BD7(\u5176\u4E94)->\u9F9A\u81EA\u73CD\r\n```\r\n\r\n```\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer, TextStreamer\r\nfrom peft import\
    \ PeftModel\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\"\
    , trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\"\
    , device_map=\"auto\", trust_remote_code=True)\r\nmodel = PeftModel.from_pretrained(model,\
    \ \"hiyouga/baichuan-7b-sft\")\r\nstreamer = TextStreamer(tokenizer, skip_prompt=True,\
    \ skip_special_tokens=True)\r\nquery = \"\u665A\u4E0A\u7761\u4E0D\u7740\u600E\u4E48\
    \u529E\"\r\ninputs = tokenizer([\"<human>:{}\\n<bot>:\".format(query)], return_tensors=\"\
    pt\")\r\ninputs = inputs.to(\"cuda\")\r\ngenerate_ids = model.generate(**inputs,\
    \ max_new_tokens=256, streamer=streamer)\r\n\r\nresult\uFF1A\r\nTraceback (most\
    \ recent call last):\r\n  File \"model-sft.py\", line 10, in <module>\r\n    model\
    \ = PeftModel.from_pretrained(model, \"/root/.cache/huggingface/hub/models--hiyouga--baichuan-7b-sft/snapshots/64cf906a964bc94bf754cef8aa0d8c05107c7745\"\
    )\r\n  File \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\", line\
    \ 181, in from_pretrained\r\n    model.load_adapter(model_id, adapter_name, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\", line 406,\
    \ in load_adapter\r\n    dispatch_model(\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py\"\
    , line 345, in dispatch_model\r\n    raise ValueError(\r\nValueError: We need\
    \ an `offload_dir` to dispatch this model according to this `device_map`, the\
    \ following submodules need to be offloaded: base_model.model.model.layers.18,\
    \ base_model.model.model.layers.19, base_model.model.model.layers.20, base_model.model.model.layers.21,\
    \ base_model.model.model.layers.22, base_model.model.model.layers.23, base_model.model.model.layers.24,\
    \ base_model.model.model.layers.25, base_model.model.model.layers.26, base_model.model.model.layers.27,\
    \ base_model.model.model.layers.28, base_model.model.model.layers.29, base_model.model.model.layers.30,\
    \ base_model.model.model.layers.31, base_model.model.model.norm, base_model.model.lm_head.\r\
    \n```"
  created_at: 2023-06-29 05:15:07+00:00
  edited: false
  hidden: false
  id: 649d216baebf20f09a0e68f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16d7fbdfdabac1f97188ce6032afd5e2.svg
      fullname: lin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleevillin
      type: user
    createdAt: '2023-06-29T06:51:13.000Z'
    data:
      edited: false
      editors:
      - littleevillin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23914138972759247
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16d7fbdfdabac1f97188ce6032afd5e2.svg
          fullname: lin
          isHf: false
          isPro: false
          name: littleevillin
          type: user
        html: "<p>update\uFF1Amodel = PeftModel.from_pretrained(model, \"hiyouga/baichuan-7b-sft\"\
          , offload_folder='./')</p>\n"
        raw: "update\uFF1Amodel = PeftModel.from_pretrained(model, \"hiyouga/baichuan-7b-sft\"\
          , offload_folder='./')"
        updatedAt: '2023-06-29T06:51:13.064Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649d29e19813ca2ced123523
    id: 649d29e19813ca2ced123520
    type: comment
  author: littleevillin
  content: "update\uFF1Amodel = PeftModel.from_pretrained(model, \"hiyouga/baichuan-7b-sft\"\
    , offload_folder='./')"
  created_at: 2023-06-29 05:51:13+00:00
  edited: false
  hidden: false
  id: 649d29e19813ca2ced123520
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/16d7fbdfdabac1f97188ce6032afd5e2.svg
      fullname: lin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleevillin
      type: user
    createdAt: '2023-06-29T06:51:13.000Z'
    data:
      status: closed
    id: 649d29e19813ca2ced123523
    type: status-change
  author: littleevillin
  created_at: 2023-06-29 05:51:13+00:00
  id: 649d29e19813ca2ced123523
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee301142efb5542eed5727c7ca4c1524.svg
      fullname: "\u5458\u529B"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicotie
      type: user
    createdAt: '2023-12-28T05:02:06.000Z'
    data:
      edited: false
      editors:
      - nicotie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.986366331577301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee301142efb5542eed5727c7ca4c1524.svg
          fullname: "\u5458\u529B"
          isHf: false
          isPro: false
          name: nicotie
          type: user
        html: '<p>I encountered same problem, did you solve it? </p>

          '
        raw: 'I encountered same problem, did you solve it? '
        updatedAt: '2023-12-28T05:02:06.729Z'
      numEdits: 0
      reactions: []
    id: 658d014e46cf834ec3c48884
    type: comment
  author: nicotie
  content: 'I encountered same problem, did you solve it? '
  created_at: 2023-12-28 05:02:06+00:00
  edited: false
  hidden: false
  id: 658d014e46cf834ec3c48884
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: hiyouga/Baichuan-7B-sft
repo_type: model
status: closed
target_branch: null
title: We need an `offload_dir` to dispatch this model according to this `device_map`
