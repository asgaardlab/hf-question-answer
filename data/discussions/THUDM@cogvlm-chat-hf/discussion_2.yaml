!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lzxcgnkhnrlnto
conflicting_files: null
created_at: 2023-11-21 10:38:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ae6ecc764b6c2dcdf3c951d82b1c4eb.svg
      fullname: Ares Valoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lzxcgnkhnrlnto
      type: user
    createdAt: '2023-11-21T10:38:27.000Z'
    data:
      edited: true
      editors:
      - lzxcgnkhnrlnto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9507126212120056
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ae6ecc764b6c2dcdf3c951d82b1c4eb.svg
          fullname: Ares Valoran
          isHf: false
          isPro: false
          name: lzxcgnkhnrlnto
          type: user
        html: '<p>The HF version still requires at least 40gb VRAM and my attempts
          so far to split it across two 3090s have failed.<br>There''s also no requirements
          file, leaving you guessing which pytorch, einops, transformers, and sentencepiece
          to use.</p>

          '
        raw: 'The HF version still requires at least 40gb VRAM and my attempts so
          far to split it across two 3090s have failed.

          There''s also no requirements file, leaving you guessing which pytorch,
          einops, transformers, and sentencepiece to use.'
        updatedAt: '2023-11-21T11:28:41.752Z'
      numEdits: 1
      reactions: []
    id: 655c88a3d49f7f62afe9a447
    type: comment
  author: lzxcgnkhnrlnto
  content: 'The HF version still requires at least 40gb VRAM and my attempts so far
    to split it across two 3090s have failed.

    There''s also no requirements file, leaving you guessing which pytorch, einops,
    transformers, and sentencepiece to use.'
  created_at: 2023-11-21 10:38:27+00:00
  edited: true
  hidden: false
  id: 655c88a3d49f7f62afe9a447
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff14b64339217858927ae01c2b2fa28d.svg
      fullname: Qingsong Lv
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: qingsonglv
      type: user
    createdAt: '2023-11-21T11:24:29.000Z'
    data:
      edited: false
      editors:
      - qingsonglv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.927320122718811
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff14b64339217858927ae01c2b2fa28d.svg
          fullname: Qingsong Lv
          isHf: false
          isPro: false
          name: qingsonglv
          type: user
        html: '<p>Yes, you are right. The huggingface version does not support model
          parallel, and we suggest use the official sat version: <a rel="nofollow"
          href="https://github.com/THUDM/CogVLM">https://github.com/THUDM/CogVLM</a></p>

          '
        raw: 'Yes, you are right. The huggingface version does not support model parallel,
          and we suggest use the official sat version: https://github.com/THUDM/CogVLM'
        updatedAt: '2023-11-21T11:24:29.000Z'
      numEdits: 0
      reactions: []
    id: 655c936d99aaa64cc1f75d39
    type: comment
  author: qingsonglv
  content: 'Yes, you are right. The huggingface version does not support model parallel,
    and we suggest use the official sat version: https://github.com/THUDM/CogVLM'
  created_at: 2023-11-21 11:24:29+00:00
  edited: false
  hidden: false
  id: 655c936d99aaa64cc1f75d39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ae6ecc764b6c2dcdf3c951d82b1c4eb.svg
      fullname: Ares Valoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lzxcgnkhnrlnto
      type: user
    createdAt: '2023-11-21T11:27:33.000Z'
    data:
      edited: false
      editors:
      - lzxcgnkhnrlnto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.916761040687561
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ae6ecc764b6c2dcdf3c951d82b1c4eb.svg
          fullname: Ares Valoran
          isHf: false
          isPro: false
          name: lzxcgnkhnrlnto
          type: user
        html: '<blockquote>

          <p>Yes, you are right. The huggingface version does not support model parallel,
          and we suggest use the official sat version: <a rel="nofollow" href="https://github.com/THUDM/CogVLM">https://github.com/THUDM/CogVLM</a></p>

          </blockquote>

          <p>If you have the time, consider checking this issue as it is the primary
          one keeping dual-gpu users from using CogVLM on WSL2: <a rel="nofollow"
          href="https://github.com/THUDM/CogVLM/issues/56">https://github.com/THUDM/CogVLM/issues/56</a></p>

          '
        raw: '> Yes, you are right. The huggingface version does not support model
          parallel, and we suggest use the official sat version: https://github.com/THUDM/CogVLM


          If you have the time, consider checking this issue as it is the primary
          one keeping dual-gpu users from using CogVLM on WSL2: https://github.com/THUDM/CogVLM/issues/56'
        updatedAt: '2023-11-21T11:27:33.047Z'
      numEdits: 0
      reactions: []
    id: 655c9425b4c01c1f76bae7a7
    type: comment
  author: lzxcgnkhnrlnto
  content: '> Yes, you are right. The huggingface version does not support model parallel,
    and we suggest use the official sat version: https://github.com/THUDM/CogVLM


    If you have the time, consider checking this issue as it is the primary one keeping
    dual-gpu users from using CogVLM on WSL2: https://github.com/THUDM/CogVLM/issues/56'
  created_at: 2023-11-21 11:27:33+00:00
  edited: false
  hidden: false
  id: 655c9425b4c01c1f76bae7a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff14b64339217858927ae01c2b2fa28d.svg
      fullname: Qingsong Lv
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: qingsonglv
      type: user
    createdAt: '2023-11-21T11:40:48.000Z'
    data:
      edited: false
      editors:
      - qingsonglv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769355654716492
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff14b64339217858927ae01c2b2fa28d.svg
          fullname: Qingsong Lv
          isHf: false
          isPro: false
          name: qingsonglv
          type: user
        html: '<p>It seems like a problem of WSL2 and torch multi-gpu support... I
          have no idea... sorry</p>

          '
        raw: It seems like a problem of WSL2 and torch multi-gpu support... I have
          no idea... sorry
        updatedAt: '2023-11-21T11:40:48.241Z'
      numEdits: 0
      reactions: []
    id: 655c974095959f36af5c2cc9
    type: comment
  author: qingsonglv
  content: It seems like a problem of WSL2 and torch multi-gpu support... I have no
    idea... sorry
  created_at: 2023-11-21 11:40:48+00:00
  edited: false
  hidden: false
  id: 655c974095959f36af5c2cc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-11-21T12:38:54.000Z'
    data:
      edited: true
      editors:
      - chenkq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.504133939743042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
          fullname: chenkq
          isHf: false
          isPro: false
          name: chenkq
          type: user
        html: "<p>if u have two 24GB devices, u can use <code>accelerate</code> to\
          \ dispatch model as demonstrated in the following. it seems that the <code>load_checkpoint_and_dispatch</code>\
          \ function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf',\
          \ the local path for the model ckpt is needed. I have personally tested\
          \ this code on my own device, and observed that the peak GPU usage reached\
          \ approximately 22GB.</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span>\
          \ requests\n<span class=\"hljs-keyword\">from</span> PIL <span class=\"\
          hljs-keyword\">import</span> Image\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ LlamaTokenizer\n<span class=\"hljs-keyword\">from</span> accelerate <span\
          \ class=\"hljs-keyword\">import</span> init_empty_weights, infer_auto_device_map,\
          \ load_checkpoint_and_dispatch\n\ntokenizer = LlamaTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">'lmsys/vicuna-7b-v1.5'</span>)\n<span class=\"hljs-keyword\"\
          >with</span> init_empty_weights():\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        <span class=\"hljs-string\">'THUDM/cogvlm-chat-hf'</span>,\n  \
          \      torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n    )\ndevice_map = infer_auto_device_map(model, max_memory={<span\
          \ class=\"hljs-number\">0</span>:<span class=\"hljs-string\">'20GiB'</span>,<span\
          \ class=\"hljs-number\">1</span>:<span class=\"hljs-string\">'20GiB'</span>,<span\
          \ class=\"hljs-string\">'cpu'</span>:<span class=\"hljs-string\">'16GiB'</span>},\
          \ no_split_module_classes=<span class=\"hljs-string\">'CogVLMDecoderLayer'</span>)\n\
          model = load_checkpoint_and_dispatch(\n    model,\n    <span class=\"hljs-string\"\
          >'local/path/to/hf/version/chat/model'</span>,   <span class=\"hljs-comment\"\
          ># typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'</span>\n\
          \    device_map=device_map,\n)\nmodel = model.<span class=\"hljs-built_in\"\
          >eval</span>()\n\n<span class=\"hljs-comment\"># check device for weights\
          \ if u want to</span>\n<span class=\"hljs-keyword\">for</span> n, p <span\
          \ class=\"hljs-keyword\">in</span> model.named_parameters():\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"<span\
          \ class=\"hljs-subst\">{n}</span>: <span class=\"hljs-subst\">{p.device}</span>\"\
          </span>)\n\n<span class=\"hljs-comment\"># chat example</span>\nquery =\
          \ <span class=\"hljs-string\">'Describe this image'</span>\nimage = Image.<span\
          \ class=\"hljs-built_in\">open</span>(requests.get(<span class=\"hljs-string\"\
          >'https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true'</span>,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw).convert(<span class=\"\
          hljs-string\">'RGB'</span>)\ninputs = model.build_conversation_input_ids(tokenizer,\
          \ query=query, history=[], images=[image])  <span class=\"hljs-comment\"\
          ># chat mode</span>\ninputs = {\n    <span class=\"hljs-string\">'input_ids'</span>:\
          \ inputs[<span class=\"hljs-string\">'input_ids'</span>].unsqueeze(<span\
          \ class=\"hljs-number\">0</span>).to(<span class=\"hljs-string\">'cuda'</span>),\n\
          \    <span class=\"hljs-string\">'token_type_ids'</span>: inputs[<span class=\"\
          hljs-string\">'token_type_ids'</span>].unsqueeze(<span class=\"hljs-number\"\
          >0</span>).to(<span class=\"hljs-string\">'cuda'</span>),\n    <span class=\"\
          hljs-string\">'attention_mask'</span>: inputs[<span class=\"hljs-string\"\
          >'attention_mask'</span>].unsqueeze(<span class=\"hljs-number\">0</span>).to(<span\
          \ class=\"hljs-string\">'cuda'</span>),\n    <span class=\"hljs-string\"\
          >'images'</span>: [[inputs[<span class=\"hljs-string\">'images'</span>][<span\
          \ class=\"hljs-number\">0</span>].to(<span class=\"hljs-string\">'cuda'</span>).to(torch.bfloat16)]],\n\
          }\ngen_kwargs = {<span class=\"hljs-string\">\"max_length\"</span>: <span\
          \ class=\"hljs-number\">2048</span>, <span class=\"hljs-string\">\"do_sample\"\
          </span>: <span class=\"hljs-literal\">False</span>}\n\n<span class=\"hljs-keyword\"\
          >with</span> torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n\
          \    outputs = outputs[:, inputs[<span class=\"hljs-string\">'input_ids'</span>].shape[<span\
          \ class=\"hljs-number\">1</span>]:]\n    <span class=\"hljs-built_in\">print</span>(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "if u have two 24GB devices, u can use `accelerate` to dispatch model\
          \ as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch`\
          \ function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf',\
          \ the local path for the model ckpt is needed. I have personally tested\
          \ this code on my own device, and observed that the peak GPU usage reached\
          \ approximately 22GB.\n\n```python\nimport torch\nimport requests\nfrom\
          \ PIL import Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\n\
          from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n\
          \ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\nwith\
          \ init_empty_weights():\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        'THUDM/cogvlm-chat-hf',\n        torch_dtype=torch.bfloat16,\n\
          \        low_cpu_mem_usage=True,\n        trust_remote_code=True,\n    )\n\
          device_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'},\
          \ no_split_module_classes='CogVLMDecoderLayer')\nmodel = load_checkpoint_and_dispatch(\n\
          \    model,\n    'local/path/to/hf/version/chat/model',   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n\
          \    device_map=device_map,\n)\nmodel = model.eval()\n\n# check device for\
          \ weights if u want to\nfor n, p in model.named_parameters():\n    print(f\"\
          {n}: {p.device}\")\n\n# chat example\nquery = 'Describe this image'\nimage\
          \ = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
          \ stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer,\
          \ query=query, history=[], images=[image])  # chat mode\ninputs = {\n  \
          \  'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids':\
          \ inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n    'attention_mask':\
          \ inputs['attention_mask'].unsqueeze(0).to('cuda'),\n    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n\
          }\ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n\
          \    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:,\
          \ inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n\
          ```"
        updatedAt: '2023-11-21T12:39:10.932Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - dddraxxx
        - 2thousand
    id: 655ca4de93ac084b7d802358
    type: comment
  author: chenkq
  content: "if u have two 24GB devices, u can use `accelerate` to dispatch model as\
    \ demonstrated in the following. it seems that the `load_checkpoint_and_dispatch`\
    \ function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf',\
    \ the local path for the model ckpt is needed. I have personally tested this code\
    \ on my own device, and observed that the peak GPU usage reached approximately\
    \ 22GB.\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom\
    \ transformers import AutoModelForCausalLM, LlamaTokenizer\nfrom accelerate import\
    \ init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n\n\
    tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\nwith init_empty_weights():\n\
    \    model = AutoModelForCausalLM.from_pretrained(\n        'THUDM/cogvlm-chat-hf',\n\
    \        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True,\n      \
    \  trust_remote_code=True,\n    )\ndevice_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'},\
    \ no_split_module_classes='CogVLMDecoderLayer')\nmodel = load_checkpoint_and_dispatch(\n\
    \    model,\n    'local/path/to/hf/version/chat/model',   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n\
    \    device_map=device_map,\n)\nmodel = model.eval()\n\n# check device for weights\
    \ if u want to\nfor n, p in model.named_parameters():\n    print(f\"{n}: {p.device}\"\
    )\n\n# chat example\nquery = 'Describe this image'\nimage = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
    \ stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer,\
    \ query=query, history=[], images=[image])  # chat mode\ninputs = {\n    'input_ids':\
    \ inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n\
    \    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n   \
    \ 'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n}\ngen_kwargs\
    \ = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n  \
    \  outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:,\
    \ inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n```"
  created_at: 2023-11-21 12:38:54+00:00
  edited: true
  hidden: false
  id: 655ca4de93ac084b7d802358
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-11-21T13:25:11.000Z'
    data:
      edited: false
      editors:
      - chenkq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9076133370399475
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
          fullname: chenkq
          isHf: false
          isPro: false
          name: chenkq
          type: user
        html: '<p>also, thanks for the reminder. the requirement is added in README</p>

          '
        raw: also, thanks for the reminder. the requirement is added in README
        updatedAt: '2023-11-21T13:25:11.175Z'
      numEdits: 0
      reactions: []
    id: 655cafb74f896c9b4a51e4f3
    type: comment
  author: chenkq
  content: also, thanks for the reminder. the requirement is added in README
  created_at: 2023-11-21 13:25:11+00:00
  edited: false
  hidden: false
  id: 655cafb74f896c9b4a51e4f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ae6ecc764b6c2dcdf3c951d82b1c4eb.svg
      fullname: Ares Valoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lzxcgnkhnrlnto
      type: user
    createdAt: '2023-11-21T15:30:30.000Z'
    data:
      edited: true
      editors:
      - lzxcgnkhnrlnto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.952777624130249
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ae6ecc764b6c2dcdf3c951d82b1c4eb.svg
          fullname: Ares Valoran
          isHf: false
          isPro: false
          name: lzxcgnkhnrlnto
          type: user
        html: '<blockquote>

          <p>if u have two 24GB devices, u can use <code>accelerate</code> to dispatch
          model as demonstrated in the following. it seems that the <code>load_checkpoint_and_dispatch</code>
          function does not support remote Hugging Face model paths  like ''THUDM/cogvlm-chat-hf'',
          the local path for the model ckpt is needed. I have personally tested this
          code on my own device, and observed that the peak GPU usage reached approximately
          22GB.</p>

          <pre><code>

          </code></pre>

          </blockquote>

          <p>This works in WSL2 with two gpus, thank you!<br>CogVLM is the best captioner
          out there and to finally get this to run is a great relief.<br>(And, I see
          you''ve already added this as an example, great work ^^ )</p>

          '
        raw: "> if u have two 24GB devices, u can use `accelerate` to dispatch model\
          \ as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch`\
          \ function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf',\
          \ the local path for the model ckpt is needed. I have personally tested\
          \ this code on my own device, and observed that the peak GPU usage reached\
          \ approximately 22GB.\n > ```\n\nThis works in WSL2 with two gpus, thank\
          \ you!\nCogVLM is the best captioner out there and to finally get this to\
          \ run is a great relief.\n(And, I see you've already added this as an example,\
          \ great work ^^ )"
        updatedAt: '2023-11-21T15:35:05.211Z'
      numEdits: 3
      reactions: []
    id: 655ccd16605e1e91fb226cd8
    type: comment
  author: lzxcgnkhnrlnto
  content: "> if u have two 24GB devices, u can use `accelerate` to dispatch model\
    \ as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch`\
    \ function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf',\
    \ the local path for the model ckpt is needed. I have personally tested this code\
    \ on my own device, and observed that the peak GPU usage reached approximately\
    \ 22GB.\n > ```\n\nThis works in WSL2 with two gpus, thank you!\nCogVLM is the\
    \ best captioner out there and to finally get this to run is a great relief.\n\
    (And, I see you've already added this as an example, great work ^^ )"
  created_at: 2023-11-21 15:30:30+00:00
  edited: true
  hidden: false
  id: 655ccd16605e1e91fb226cd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-11-24T05:44:34.000Z'
    data:
      status: closed
    id: 6560384247d95544ef97c24f
    type: status-change
  author: chenkq
  created_at: 2023-11-24 05:44:34+00:00
  id: 6560384247d95544ef97c24f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da9455cd077d7e94cca051690b0a47e1.svg
      fullname: Xiujie Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2thousand
      type: user
    createdAt: '2023-12-10T13:21:50.000Z'
    data:
      edited: false
      editors:
      - 2thousand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5073193311691284
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da9455cd077d7e94cca051690b0a47e1.svg
          fullname: Xiujie Song
          isHf: false
          isPro: false
          name: 2thousand
          type: user
        html: "<blockquote>\n<p>if u have two 24GB devices, u can use <code>accelerate</code>\
          \ to dispatch model as demonstrated in the following. it seems that the\
          \ <code>load_checkpoint_and_dispatch</code> function does not support remote\
          \ Hugging Face model paths  like 'THUDM/cogvlm-chat-hf', the local path\
          \ for the model ckpt is needed. I have personally tested this code on my\
          \ own device, and observed that the peak GPU usage reached approximately\
          \ 22GB.</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">import</span> requests\n\
          <span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\"\
          >import</span> Image\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, LlamaTokenizer\n\
          <span class=\"hljs-keyword\">from</span> accelerate <span class=\"hljs-keyword\"\
          >import</span> init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n\
          \ntokenizer = LlamaTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >'lmsys/vicuna-7b-v1.5'</span>)\n<span class=\"hljs-keyword\">with</span>\
          \ init_empty_weights():\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        <span class=\"hljs-string\">'THUDM/cogvlm-chat-hf'</span>,\n  \
          \      torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n    )\ndevice_map = infer_auto_device_map(model, max_memory={<span\
          \ class=\"hljs-number\">0</span>:<span class=\"hljs-string\">'20GiB'</span>,<span\
          \ class=\"hljs-number\">1</span>:<span class=\"hljs-string\">'20GiB'</span>,<span\
          \ class=\"hljs-string\">'cpu'</span>:<span class=\"hljs-string\">'16GiB'</span>},\
          \ no_split_module_classes=<span class=\"hljs-string\">'CogVLMDecoderLayer'</span>)\n\
          model = load_checkpoint_and_dispatch(\n    model,\n    <span class=\"hljs-string\"\
          >'local/path/to/hf/version/chat/model'</span>,   <span class=\"hljs-comment\"\
          ># typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'</span>\n\
          \    device_map=device_map,\n)\nmodel = model.<span class=\"hljs-built_in\"\
          >eval</span>()\n\n<span class=\"hljs-comment\"># check device for weights\
          \ if u want to</span>\n<span class=\"hljs-keyword\">for</span> n, p <span\
          \ class=\"hljs-keyword\">in</span> model.named_parameters():\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"<span\
          \ class=\"hljs-subst\">{n}</span>: <span class=\"hljs-subst\">{p.device}</span>\"\
          </span>)\n\n<span class=\"hljs-comment\"># chat example</span>\nquery =\
          \ <span class=\"hljs-string\">'Describe this image'</span>\nimage = Image.<span\
          \ class=\"hljs-built_in\">open</span>(requests.get(<span class=\"hljs-string\"\
          >'https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true'</span>,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw).convert(<span class=\"\
          hljs-string\">'RGB'</span>)\ninputs = model.build_conversation_input_ids(tokenizer,\
          \ query=query, history=[], images=[image])  <span class=\"hljs-comment\"\
          ># chat mode</span>\ninputs = {\n    <span class=\"hljs-string\">'input_ids'</span>:\
          \ inputs[<span class=\"hljs-string\">'input_ids'</span>].unsqueeze(<span\
          \ class=\"hljs-number\">0</span>).to(<span class=\"hljs-string\">'cuda'</span>),\n\
          \    <span class=\"hljs-string\">'token_type_ids'</span>: inputs[<span class=\"\
          hljs-string\">'token_type_ids'</span>].unsqueeze(<span class=\"hljs-number\"\
          >0</span>).to(<span class=\"hljs-string\">'cuda'</span>),\n    <span class=\"\
          hljs-string\">'attention_mask'</span>: inputs[<span class=\"hljs-string\"\
          >'attention_mask'</span>].unsqueeze(<span class=\"hljs-number\">0</span>).to(<span\
          \ class=\"hljs-string\">'cuda'</span>),\n    <span class=\"hljs-string\"\
          >'images'</span>: [[inputs[<span class=\"hljs-string\">'images'</span>][<span\
          \ class=\"hljs-number\">0</span>].to(<span class=\"hljs-string\">'cuda'</span>).to(torch.bfloat16)]],\n\
          }\ngen_kwargs = {<span class=\"hljs-string\">\"max_length\"</span>: <span\
          \ class=\"hljs-number\">2048</span>, <span class=\"hljs-string\">\"do_sample\"\
          </span>: <span class=\"hljs-literal\">False</span>}\n\n<span class=\"hljs-keyword\"\
          >with</span> torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n\
          \    outputs = outputs[:, inputs[<span class=\"hljs-string\">'input_ids'</span>].shape[<span\
          \ class=\"hljs-number\">1</span>]:]\n    <span class=\"hljs-built_in\">print</span>(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n</blockquote>\n<p>Anyone\
          \ tried to deploy cogvlm (4bit quantization) on multiple GPUs with accelerate?\
          \ </p>\n"
        raw: "> if u have two 24GB devices, u can use `accelerate` to dispatch model\
          \ as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch`\
          \ function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf',\
          \ the local path for the model ckpt is needed. I have personally tested\
          \ this code on my own device, and observed that the peak GPU usage reached\
          \ approximately 22GB.\n> \n> ```python\n> import torch\n> import requests\n\
          > from PIL import Image\n> from transformers import AutoModelForCausalLM,\
          \ LlamaTokenizer\n> from accelerate import init_empty_weights, infer_auto_device_map,\
          \ load_checkpoint_and_dispatch\n> \n> tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n\
          > with init_empty_weights():\n>     model = AutoModelForCausalLM.from_pretrained(\n\
          >         'THUDM/cogvlm-chat-hf',\n>         torch_dtype=torch.bfloat16,\n\
          >         low_cpu_mem_usage=True,\n>         trust_remote_code=True,\n>\
          \     )\n> device_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'},\
          \ no_split_module_classes='CogVLMDecoderLayer')\n> model = load_checkpoint_and_dispatch(\n\
          >     model,\n>     'local/path/to/hf/version/chat/model',   # typical,\
          \ '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n\
          >     device_map=device_map,\n> )\n> model = model.eval()\n> \n> # check\
          \ device for weights if u want to\n> for n, p in model.named_parameters():\n\
          >     print(f\"{n}: {p.device}\")\n> \n> # chat example\n> query = 'Describe\
          \ this image'\n> image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
          \ stream=True).raw).convert('RGB')\n> inputs = model.build_conversation_input_ids(tokenizer,\
          \ query=query, history=[], images=[image])  # chat mode\n> inputs = {\n\
          >     'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n>     'token_type_ids':\
          \ inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n>     'attention_mask':\
          \ inputs['attention_mask'].unsqueeze(0).to('cuda'),\n>     'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n\
          > }\n> gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n> \n>\
          \ with torch.no_grad():\n>     outputs = model.generate(**inputs, **gen_kwargs)\n\
          >     outputs = outputs[:, inputs['input_ids'].shape[1]:]\n>     print(tokenizer.decode(outputs[0]))\n\
          > ```\n\nAnyone tried to deploy cogvlm (4bit quantization) on multiple GPUs\
          \ with accelerate? "
        updatedAt: '2023-12-10T13:21:50.896Z'
      numEdits: 0
      reactions: []
    id: 6575bb6ef9898ed3ab93d467
    type: comment
  author: 2thousand
  content: "> if u have two 24GB devices, u can use `accelerate` to dispatch model\
    \ as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch`\
    \ function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf',\
    \ the local path for the model ckpt is needed. I have personally tested this code\
    \ on my own device, and observed that the peak GPU usage reached approximately\
    \ 22GB.\n> \n> ```python\n> import torch\n> import requests\n> from PIL import\
    \ Image\n> from transformers import AutoModelForCausalLM, LlamaTokenizer\n> from\
    \ accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n\
    > \n> tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n> with\
    \ init_empty_weights():\n>     model = AutoModelForCausalLM.from_pretrained(\n\
    >         'THUDM/cogvlm-chat-hf',\n>         torch_dtype=torch.bfloat16,\n>  \
    \       low_cpu_mem_usage=True,\n>         trust_remote_code=True,\n>     )\n\
    > device_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'},\
    \ no_split_module_classes='CogVLMDecoderLayer')\n> model = load_checkpoint_and_dispatch(\n\
    >     model,\n>     'local/path/to/hf/version/chat/model',   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n\
    >     device_map=device_map,\n> )\n> model = model.eval()\n> \n> # check device\
    \ for weights if u want to\n> for n, p in model.named_parameters():\n>     print(f\"\
    {n}: {p.device}\")\n> \n> # chat example\n> query = 'Describe this image'\n> image\
    \ = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
    \ stream=True).raw).convert('RGB')\n> inputs = model.build_conversation_input_ids(tokenizer,\
    \ query=query, history=[], images=[image])  # chat mode\n> inputs = {\n>     'input_ids':\
    \ inputs['input_ids'].unsqueeze(0).to('cuda'),\n>     'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n\
    >     'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n> \
    \    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n> }\n>\
    \ gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n> \n> with torch.no_grad():\n\
    >     outputs = model.generate(**inputs, **gen_kwargs)\n>     outputs = outputs[:,\
    \ inputs['input_ids'].shape[1]:]\n>     print(tokenizer.decode(outputs[0]))\n\
    > ```\n\nAnyone tried to deploy cogvlm (4bit quantization) on multiple GPUs with\
    \ accelerate? "
  created_at: 2023-12-10 13:21:50+00:00
  edited: false
  hidden: false
  id: 6575bb6ef9898ed3ab93d467
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-11T04:00:44.000Z'
    data:
      edited: true
      editors:
      - chenkq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3667529225349426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
          fullname: chenkq
          isHf: false
          isPro: false
          name: chenkq
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;2thousand&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/2thousand\">@<span class=\"\
          underline\">2thousand</span></a></span>\n\n\t</span></span> see if <a href=\"\
          https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362\"\
          >this</a> can help</p>\n"
        raw: '@2thousand see if [this](https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362)
          can help'
        updatedAt: '2023-12-11T04:01:20.682Z'
      numEdits: 1
      reactions: []
    id: 6576896c15b1ca184b3bddf6
    type: comment
  author: chenkq
  content: '@2thousand see if [this](https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362)
    can help'
  created_at: 2023-12-11 04:00:44+00:00
  edited: true
  hidden: false
  id: 6576896c15b1ca184b3bddf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da9455cd077d7e94cca051690b0a47e1.svg
      fullname: Xiujie Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2thousand
      type: user
    createdAt: '2023-12-11T06:04:55.000Z'
    data:
      edited: true
      editors:
      - 2thousand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.25454238057136536
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da9455cd077d7e94cca051690b0a47e1.svg
          fullname: Xiujie Song
          isHf: false
          isPro: false
          name: 2thousand
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;2thousand&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/2thousand\"\
          >@<span class=\"underline\">2thousand</span></a></span>\n\n\t</span></span>\
          \ see if <a href=\"https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362\"\
          >this</a> can help</p>\n</blockquote>\n<p>Thanks, I just figured it out.\
          \ we can directly add device_map=\"auto\" in AutoModelForCausalLM.from_pretrained()</p>\n\
          <pre><code class=\"language-python\">tokenizer = LlamaTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">'vicuna-7b-v1.5'</span>)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \        <span class=\"hljs-string\">'THUDM/cogvlm-chat-hf'</span>,\n  \
          \      load_in_4bit=<span class=\"hljs-literal\">True</span>,\n        trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n        device_map=<span class=\"\
          hljs-string\">\"auto\"</span>\n    ).<span class=\"hljs-built_in\">eval</span>()\n\
          query = <span class=\"hljs-string\">'Describe this image in details.'</span>\n\
          image = Image.<span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\"\
          >'image-path'</span>).convert(<span class=\"hljs-string\">'RGB'</span>)\n\
          inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[],\
          \ images=[image])  <span class=\"hljs-comment\"># chat mode</span>\ninputs\
          \ = {\n    <span class=\"hljs-string\">'input_ids'</span>: inputs[<span\
          \ class=\"hljs-string\">'input_ids'</span>].unsqueeze(<span class=\"hljs-number\"\
          >0</span>).to(<span class=\"hljs-string\">'cuda'</span>),\n    <span class=\"\
          hljs-string\">'token_type_ids'</span>: inputs[<span class=\"hljs-string\"\
          >'token_type_ids'</span>].unsqueeze(<span class=\"hljs-number\">0</span>).to(<span\
          \ class=\"hljs-string\">'cuda'</span>),\n    <span class=\"hljs-string\"\
          >'attention_mask'</span>: inputs[<span class=\"hljs-string\">'attention_mask'</span>].unsqueeze(<span\
          \ class=\"hljs-number\">0</span>).to(<span class=\"hljs-string\">'cuda'</span>),\n\
          \    <span class=\"hljs-string\">'images'</span>: [[inputs[<span class=\"\
          hljs-string\">'images'</span>][<span class=\"hljs-number\">0</span>].to(<span\
          \ class=\"hljs-string\">'cuda'</span>).to(torch.float16)]],\n}\ngen_kwargs\
          \ = {<span class=\"hljs-string\">\"max_length\"</span>: <span class=\"hljs-number\"\
          >2048</span>, <span class=\"hljs-string\">\"do_sample\"</span>: <span class=\"\
          hljs-literal\">False</span>}\n\n<span class=\"hljs-keyword\">with</span>\
          \ torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n\
          \    outputs = outputs[:, inputs[<span class=\"hljs-string\">'input_ids'</span>].shape[<span\
          \ class=\"hljs-number\">1</span>]:]\n    <span class=\"hljs-built_in\">print</span>(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "> @2thousand see if [this](https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362)\
          \ can help\n\nThanks, I just figured it out. we can directly add device_map=\"\
          auto\" in AutoModelForCausalLM.from_pretrained()\n\n```python\ntokenizer\
          \ = LlamaTokenizer.from_pretrained('vicuna-7b-v1.5')\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \        'THUDM/cogvlm-chat-hf',\n        load_in_4bit=True,\n        trust_remote_code=True,\n\
          \        device_map=\"auto\"\n    ).eval()\nquery = 'Describe this image\
          \ in details.'\nimage = Image.open('image-path').convert('RGB')\ninputs\
          \ = model.build_conversation_input_ids(tokenizer, query=query, history=[],\
          \ images=[image])  # chat mode\ninputs = {\n    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n\
          \    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n\
          \    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n\
          \    'images': [[inputs['images'][0].to('cuda').to(torch.float16)]],\n}\n\
          gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n\
          \    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:,\
          \ inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n\
          ```"
        updatedAt: '2023-12-11T06:06:15.383Z'
      numEdits: 1
      reactions: []
    id: 6576a687d7f487de5f23f1a9
    type: comment
  author: 2thousand
  content: "> @2thousand see if [this](https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362)\
    \ can help\n\nThanks, I just figured it out. we can directly add device_map=\"\
    auto\" in AutoModelForCausalLM.from_pretrained()\n\n```python\ntokenizer = LlamaTokenizer.from_pretrained('vicuna-7b-v1.5')\n\
    model = AutoModelForCausalLM.from_pretrained(\n        'THUDM/cogvlm-chat-hf',\n\
    \        load_in_4bit=True,\n        trust_remote_code=True,\n        device_map=\"\
    auto\"\n    ).eval()\nquery = 'Describe this image in details.'\nimage = Image.open('image-path').convert('RGB')\n\
    inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[],\
    \ images=[image])  # chat mode\ninputs = {\n    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n\
    \    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n   \
    \ 'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n    'images':\
    \ [[inputs['images'][0].to('cuda').to(torch.float16)]],\n}\ngen_kwargs = {\"max_length\"\
    : 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs,\
    \ **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n   \
    \ print(tokenizer.decode(outputs[0]))\n```"
  created_at: 2023-12-11 06:04:55+00:00
  edited: true
  hidden: false
  id: 6576a687d7f487de5f23f1a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
      fullname: Richard Meyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hisma
      type: user
    createdAt: '2023-12-16T15:35:47.000Z'
    data:
      edited: false
      editors:
      - Hisma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9226258993148804
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c60e51e4d167c3c6385dc26b1f50452.svg
          fullname: Richard Meyer
          isHf: false
          isPro: false
          name: Hisma
          type: user
        html: '<p>can someone create a web demo version of this?  I tried adapting
          the cogvlm web demo using the accelerate code above to allow multi-gpu support
          in wsl2, but couldn''t get it to work.<br>has anyone gotten a gradio UI
          version of cogvlm working in wsl2? </p>

          '
        raw: "can someone create a web demo version of this?  I tried adapting the\
          \ cogvlm web demo using the accelerate code above to allow multi-gpu support\
          \ in wsl2, but couldn't get it to work.  \nhas anyone gotten a gradio UI\
          \ version of cogvlm working in wsl2? "
        updatedAt: '2023-12-16T15:35:47.562Z'
      numEdits: 0
      reactions: []
    id: 657dc3d3112a9ca545367e38
    type: comment
  author: Hisma
  content: "can someone create a web demo version of this?  I tried adapting the cogvlm\
    \ web demo using the accelerate code above to allow multi-gpu support in wsl2,\
    \ but couldn't get it to work.  \nhas anyone gotten a gradio UI version of cogvlm\
    \ working in wsl2? "
  created_at: 2023-12-16 15:35:47+00:00
  edited: false
  hidden: false
  id: 657dc3d3112a9ca545367e38
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: THUDM/cogvlm-chat-hf
repo_type: model
status: closed
target_branch: null
title: Note to others trying to run this
