!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Soraheart1988
conflicting_files: null
created_at: 2023-12-06 06:59:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66ff8c0a8675fb01ade951ac70db4c0b.svg
      fullname: SoraHeart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Soraheart1988
      type: user
    createdAt: '2023-12-06T06:59:36.000Z'
    data:
      edited: true
      editors:
      - Soraheart1988
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6832159757614136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66ff8c0a8675fb01ade951ac70db4c0b.svg
          fullname: SoraHeart
          isHf: false
          isPro: false
          name: Soraheart1988
          type: user
        html: "<p>I have spinup a AWS Sagemaker instance of ml.g4dn.12xlarge\t(4x16\
          \ GPU). Tried the code that uses accelerate library however got following\
          \ error.</p>\n<p>The section of code where I have modify using the provided\
          \ code.</p>\n<pre><code>device_map = infer_auto_device_map(model, max_memory={0:'10GiB',1:'10GiB',\
          \ 2: '10GiB', 3:'10GiB','cpu':'48GiB'}, no_split_module_classes=['CogVLMDecoderLayer',\
          \ 'TransformerLayer'])\nmodel = load_checkpoint_and_dispatch(\n    model,\n\
          \    checkpoint,   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n\
          \    device_map=device_map,\n</code></pre>\n<p>Error</p>\n<pre><code>NotImplementedError\
          \                       Traceback (most recent call last)\nCell In[3], line\
          \ 44\n     41 gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\
          \     43 with torch.no_grad():\n---&gt; 44     outputs = model.generate(**inputs,\
          \ **gen_kwargs)\n    114     Raises:\n   (...)\n    118         AttentionOp:\
          \ The best operator for the configuration\n    119     \"\"\"\n--&gt; 120\
          \     return _run_priority_list(\n    121         \"memory_efficient_attention_forward\"\
          ,\n    122         _dispatch_fw_priority_list(inp, needs_gradient),\n  \
          \  123         inp,\n    124     )\n......\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:63,\
          \ in _run_priority_list(name, priority_list, inp)\n     61 for op, not_supported\
          \ in zip(priority_list, not_supported_reasons):\n     62     msg += \"\\\
          n\" + _format_not_supported_reasons(op, not_supported)\n---&gt; 63 raise\
          \ NotImplementedError(msg)\n\nNotImplementedError: No operator found for\
          \ `memory_efficient_attention_forward` with inputs:\n     query       :\
          \ shape=(1, 1226, 16, 112) (torch.bfloat16)\n     key         : shape=(1,\
          \ 1226, 16, 112) (torch.bfloat16)\n     value       : shape=(1, 1226, 16,\
          \ 112) (torch.bfloat16)\n     attn_bias   : &lt;class 'NoneType'&gt;\n \
          \    p           : 0.0\n`decoderF` is not supported because:\n    attn_bias\
          \ type is &lt;class 'NoneType'&gt;\n    bf16 is only supported on A100+\
          \ GPUs\n`flshattF@v2.3.2` is not supported because:\n    requires device\
          \ with capability &gt; (8, 0) but your GPU has capability (7, 5) (too old)\n\
          \    bf16 is only supported on A100+ GPUs\n`tritonflashattF` is not supported\
          \ because:\n    requires device with capability &gt; (8, 0) but your GPU\
          \ has capability (7, 5) (too old)\n    bf16 is only supported on A100+ GPUs\n\
          \    operator wasn't built - see `python -m xformers.info` for more info\n\
          \    triton is not available\n    requires GPU with sm80 minimum compute\
          \ capacity, e.g., A100/H100/L4\n    Only work on pre-MLIR triton for now\n\
          `cutlassF` is not supported because:\n    bf16 is only supported on A100+\
          \ GPUs\n`smallkF` is not supported because:\n    max(query.shape[-1] !=\
          \ value.shape[-1]) &gt; 32\n    dtype=torch.bfloat16 (supported: {torch.float32})\n\
          \    has custom scale\n    bf16 is only supported on A100+ GPUs\n    unsupported\
          \ embed per head: 112\n`\nPlease advise what have I done wrong?\n</code></pre>\n"
        raw: "I have spinup a AWS Sagemaker instance of ml.g4dn.12xlarge\t(4x16 GPU).\
          \ Tried the code that uses accelerate library however got following error.\n\
          \nThe section of code where I have modify using the provided code.\n\n```\n\
          device_map = infer_auto_device_map(model, max_memory={0:'10GiB',1:'10GiB',\
          \ 2: '10GiB', 3:'10GiB','cpu':'48GiB'}, no_split_module_classes=['CogVLMDecoderLayer',\
          \ 'TransformerLayer'])\nmodel = load_checkpoint_and_dispatch(\n    model,\n\
          \    checkpoint,   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n\
          \    device_map=device_map,\n```\n\nError\n```\nNotImplementedError    \
          \                   Traceback (most recent call last)\nCell In[3], line\
          \ 44\n     41 gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\
          \     43 with torch.no_grad():\n---> 44     outputs = model.generate(**inputs,\
          \ **gen_kwargs)\n    114     Raises:\n   (...)\n    118         AttentionOp:\
          \ The best operator for the configuration\n    119     \"\"\"\n--> 120 \
          \    return _run_priority_list(\n    121         \"memory_efficient_attention_forward\"\
          ,\n    122         _dispatch_fw_priority_list(inp, needs_gradient),\n  \
          \  123         inp,\n    124     )\n......\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:63,\
          \ in _run_priority_list(name, priority_list, inp)\n     61 for op, not_supported\
          \ in zip(priority_list, not_supported_reasons):\n     62     msg += \"\\\
          n\" + _format_not_supported_reasons(op, not_supported)\n---> 63 raise NotImplementedError(msg)\n\
          \nNotImplementedError: No operator found for `memory_efficient_attention_forward`\
          \ with inputs:\n     query       : shape=(1, 1226, 16, 112) (torch.bfloat16)\n\
          \     key         : shape=(1, 1226, 16, 112) (torch.bfloat16)\n     value\
          \       : shape=(1, 1226, 16, 112) (torch.bfloat16)\n     attn_bias   :\
          \ <class 'NoneType'>\n     p           : 0.0\n`decoderF` is not supported\
          \ because:\n    attn_bias type is <class 'NoneType'>\n    bf16 is only supported\
          \ on A100+ GPUs\n`flshattF@v2.3.2` is not supported because:\n    requires\
          \ device with capability > (8, 0) but your GPU has capability (7, 5) (too\
          \ old)\n    bf16 is only supported on A100+ GPUs\n`tritonflashattF` is not\
          \ supported because:\n    requires device with capability > (8, 0) but your\
          \ GPU has capability (7, 5) (too old)\n    bf16 is only supported on A100+\
          \ GPUs\n    operator wasn't built - see `python -m xformers.info` for more\
          \ info\n    triton is not available\n    requires GPU with sm80 minimum\
          \ compute capacity, e.g., A100/H100/L4\n    Only work on pre-MLIR triton\
          \ for now\n`cutlassF` is not supported because:\n    bf16 is only supported\
          \ on A100+ GPUs\n`smallkF` is not supported because:\n    max(query.shape[-1]\
          \ != value.shape[-1]) > 32\n    dtype=torch.bfloat16 (supported: {torch.float32})\n\
          \    has custom scale\n    bf16 is only supported on A100+ GPUs\n    unsupported\
          \ embed per head: 112\n`\nPlease advise what have I done wrong?\n```"
        updatedAt: '2023-12-06T07:36:15.029Z'
      numEdits: 2
      reactions: []
    id: 65701bd8a2c7dd8174cd6ec0
    type: comment
  author: Soraheart1988
  content: "I have spinup a AWS Sagemaker instance of ml.g4dn.12xlarge\t(4x16 GPU).\
    \ Tried the code that uses accelerate library however got following error.\n\n\
    The section of code where I have modify using the provided code.\n\n```\ndevice_map\
    \ = infer_auto_device_map(model, max_memory={0:'10GiB',1:'10GiB', 2: '10GiB',\
    \ 3:'10GiB','cpu':'48GiB'}, no_split_module_classes=['CogVLMDecoderLayer', 'TransformerLayer'])\n\
    model = load_checkpoint_and_dispatch(\n    model,\n    checkpoint,   # typical,\
    \ '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n\
    \    device_map=device_map,\n```\n\nError\n```\nNotImplementedError          \
    \             Traceback (most recent call last)\nCell In[3], line 44\n     41\
    \ gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n     43 with torch.no_grad():\n\
    ---> 44     outputs = model.generate(**inputs, **gen_kwargs)\n    114     Raises:\n\
    \   (...)\n    118         AttentionOp: The best operator for the configuration\n\
    \    119     \"\"\"\n--> 120     return _run_priority_list(\n    121         \"\
    memory_efficient_attention_forward\",\n    122         _dispatch_fw_priority_list(inp,\
    \ needs_gradient),\n    123         inp,\n    124     )\n......\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:63,\
    \ in _run_priority_list(name, priority_list, inp)\n     61 for op, not_supported\
    \ in zip(priority_list, not_supported_reasons):\n     62     msg += \"\\n\" +\
    \ _format_not_supported_reasons(op, not_supported)\n---> 63 raise NotImplementedError(msg)\n\
    \nNotImplementedError: No operator found for `memory_efficient_attention_forward`\
    \ with inputs:\n     query       : shape=(1, 1226, 16, 112) (torch.bfloat16)\n\
    \     key         : shape=(1, 1226, 16, 112) (torch.bfloat16)\n     value    \
    \   : shape=(1, 1226, 16, 112) (torch.bfloat16)\n     attn_bias   : <class 'NoneType'>\n\
    \     p           : 0.0\n`decoderF` is not supported because:\n    attn_bias type\
    \ is <class 'NoneType'>\n    bf16 is only supported on A100+ GPUs\n`flshattF@v2.3.2`\
    \ is not supported because:\n    requires device with capability > (8, 0) but\
    \ your GPU has capability (7, 5) (too old)\n    bf16 is only supported on A100+\
    \ GPUs\n`tritonflashattF` is not supported because:\n    requires device with\
    \ capability > (8, 0) but your GPU has capability (7, 5) (too old)\n    bf16 is\
    \ only supported on A100+ GPUs\n    operator wasn't built - see `python -m xformers.info`\
    \ for more info\n    triton is not available\n    requires GPU with sm80 minimum\
    \ compute capacity, e.g., A100/H100/L4\n    Only work on pre-MLIR triton for now\n\
    `cutlassF` is not supported because:\n    bf16 is only supported on A100+ GPUs\n\
    `smallkF` is not supported because:\n    max(query.shape[-1] != value.shape[-1])\
    \ > 32\n    dtype=torch.bfloat16 (supported: {torch.float32})\n    has custom\
    \ scale\n    bf16 is only supported on A100+ GPUs\n    unsupported embed per head:\
    \ 112\n`\nPlease advise what have I done wrong?\n```"
  created_at: 2023-12-06 06:59:36+00:00
  edited: true
  hidden: false
  id: 65701bd8a2c7dd8174cd6ec0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/66ff8c0a8675fb01ade951ac70db4c0b.svg
      fullname: SoraHeart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Soraheart1988
      type: user
    createdAt: '2023-12-06T07:42:20.000Z'
    data:
      from: Error at load_checkpoint_and_dispatch(
      to: NotImplementedError
    id: 657025dc1c90ddf8fbe2a893
    type: title-change
  author: Soraheart1988
  created_at: 2023-12-06 07:42:20+00:00
  id: 657025dc1c90ddf8fbe2a893
  new_title: NotImplementedError
  old_title: Error at load_checkpoint_and_dispatch(
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-06T13:04:30.000Z'
    data:
      edited: false
      editors:
      - chenkq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6323162913322449
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
          fullname: chenkq
          isHf: false
          isPro: false
          name: chenkq
          type: user
        html: '<p>try fp16 maybe? <a href="https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/5#6567f0b792d9319907436b10">https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/5#6567f0b792d9319907436b10</a></p>

          '
        raw: try fp16 maybe? https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/5#6567f0b792d9319907436b10
        updatedAt: '2023-12-06T13:04:30.586Z'
      numEdits: 0
      reactions: []
    id: 6570715ec1b36e8e576baf8f
    type: comment
  author: chenkq
  content: try fp16 maybe? https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/5#6567f0b792d9319907436b10
  created_at: 2023-12-06 13:04:30+00:00
  edited: false
  hidden: false
  id: 6570715ec1b36e8e576baf8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-11T04:03:22.000Z'
    data:
      status: closed
    id: 65768a0ad0ed8f5761638c50
    type: status-change
  author: chenkq
  created_at: 2023-12-11 04:03:22+00:00
  id: 65768a0ad0ed8f5761638c50
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: THUDM/cogvlm-chat-hf
repo_type: model
status: closed
target_branch: null
title: NotImplementedError
