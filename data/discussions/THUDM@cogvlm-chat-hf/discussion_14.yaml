!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sidnb13
conflicting_files: null
created_at: 2023-12-07 02:22:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f68cbabcdfc96de3ef84d094ba1d60c.svg
      fullname: Sidharth Baskaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sidnb13
      type: user
    createdAt: '2023-12-07T02:22:42.000Z'
    data:
      edited: false
      editors:
      - sidnb13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9162704348564148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f68cbabcdfc96de3ef84d094ba1d60c.svg
          fullname: Sidharth Baskaran
          isHf: false
          isPro: false
          name: sidnb13
          type: user
        html: '<p>Any plans to support gradient checkpointing and flash attention
          for training/finetuning? Would be very helpful to get this working on fewer
          resources.</p>

          '
        raw: Any plans to support gradient checkpointing and flash attention for training/finetuning?
          Would be very helpful to get this working on fewer resources.
        updatedAt: '2023-12-07T02:22:42.131Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - z3ugma
        - shotarok
    id: 65712c729435343c81efbd89
    type: comment
  author: sidnb13
  content: Any plans to support gradient checkpointing and flash attention for training/finetuning?
    Would be very helpful to get this working on fewer resources.
  created_at: 2023-12-07 02:22:42+00:00
  edited: false
  hidden: false
  id: 65712c729435343c81efbd89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-07T03:06:43.000Z'
    data:
      pinned: true
    id: 657136c34c829fb8f5e54390
    type: pinning-change
  author: chenkq
  created_at: 2023-12-07 03:06:43+00:00
  id: 657136c34c829fb8f5e54390
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ed49d2c82fb1ea0ac20c6cc91ce7f58.svg
      fullname: Kristian Klemon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: randhash
      type: user
    createdAt: '2023-12-29T14:15:53.000Z'
    data:
      edited: false
      editors:
      - randhash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8048003315925598
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ed49d2c82fb1ea0ac20c6cc91ce7f58.svg
          fullname: Kristian Klemon
          isHf: false
          isPro: false
          name: randhash
          type: user
        html: "<p>I think FlashAttn is already used under specific conditions. The\
          \ <a href=\"https://huggingface.co/THUDM/cogvlm-chat-hf/blob/main/modeling_cogvlm.py#L129\"\
          >attention implementation</a> calls PyTorch's <code>scaled_dot_product_attention</code>\
          \ function which calls into a FlashAttn kernel if some conditions are met.\
          \ You can actually enforce the use of this kernel for debugging purposes\
          \ with an appropriate context manager:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">with</span> torch.backends.cuda.sdp_kernel(\n\
          \    enable_flash=<span class=\"hljs-literal\">True</span>, \n    enable_math=<span\
          \ class=\"hljs-literal\">False</span>, \n    enable_mem_efficient=<span\
          \ class=\"hljs-literal\">False</span>\n):\n    model.generate(**<span class=\"\
          hljs-built_in\">input</span>)\n</code></pre>\n<p>Note, that in the referenced\
          \ code there is a branch which may execute a naive attention implementation\
          \ so even though you are using enforcing FA use in PyTorch, you would still\
          \ make sure that the if-statement runs into the first branch.</p>\n"
        raw: "I think FlashAttn is already used under specific conditions. The [attention\
          \ implementation](https://huggingface.co/THUDM/cogvlm-chat-hf/blob/main/modeling_cogvlm.py#L129)\
          \ calls PyTorch's `scaled_dot_product_attention` function which calls into\
          \ a FlashAttn kernel if some conditions are met. You can actually enforce\
          \ the use of this kernel for debugging purposes with an appropriate context\
          \ manager:\n\n```python\nwith torch.backends.cuda.sdp_kernel(\n    enable_flash=True,\
          \ \n    enable_math=False, \n    enable_mem_efficient=False\n):\n    model.generate(**input)\n\
          ```\n\nNote, that in the referenced code there is a branch which may execute\
          \ a naive attention implementation so even though you are using enforcing\
          \ FA use in PyTorch, you would still make sure that the if-statement runs\
          \ into the first branch."
        updatedAt: '2023-12-29T14:15:53.212Z'
      numEdits: 0
      reactions: []
    id: 658ed499a41c3cbad544e7d3
    type: comment
  author: randhash
  content: "I think FlashAttn is already used under specific conditions. The [attention\
    \ implementation](https://huggingface.co/THUDM/cogvlm-chat-hf/blob/main/modeling_cogvlm.py#L129)\
    \ calls PyTorch's `scaled_dot_product_attention` function which calls into a FlashAttn\
    \ kernel if some conditions are met. You can actually enforce the use of this\
    \ kernel for debugging purposes with an appropriate context manager:\n\n```python\n\
    with torch.backends.cuda.sdp_kernel(\n    enable_flash=True, \n    enable_math=False,\
    \ \n    enable_mem_efficient=False\n):\n    model.generate(**input)\n```\n\nNote,\
    \ that in the referenced code there is a branch which may execute a naive attention\
    \ implementation so even though you are using enforcing FA use in PyTorch, you\
    \ would still make sure that the if-statement runs into the first branch."
  created_at: 2023-12-29 14:15:53+00:00
  edited: false
  hidden: false
  id: 658ed499a41c3cbad544e7d3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: THUDM/cogvlm-chat-hf
repo_type: model
status: open
target_branch: null
title: Support for gradient checkpointing and Flash Attention
