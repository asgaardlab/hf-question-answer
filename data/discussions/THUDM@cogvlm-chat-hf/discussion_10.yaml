!!python/object:huggingface_hub.community.DiscussionWithDetails
author: miguelcarv
conflicting_files: null
created_at: 2023-12-03 16:04:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f72b4e15c6b1743ef88c6bd9eb5d8de9.svg
      fullname: Miguel Carvalho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miguelcarv
      type: user
    createdAt: '2023-12-03T16:04:40.000Z'
    data:
      edited: false
      editors:
      - miguelcarv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.27276116609573364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f72b4e15c6b1743ef88c6bd9eb5d8de9.svg
          fullname: Miguel Carvalho
          isHf: false
          isPro: false
          name: miguelcarv
          type: user
        html: "<p>I'm trying to run the chat example  on a A100 GPU (with the specified\
          \ dependencies installed like described in the model page) but I get an\
          \ error related to a ldconfig binary, like this:</p>\n<pre><code class=\"\
          language-Traceback\">  File \"/cfs/home/u021543/test_cogvlm.py\", line 27,\
          \ in &lt;module&gt;\n    outputs = model.generate(**inputs, **gen_kwargs)\n\
          \              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n    \
          \       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py\"\
          , line 1673, in generate\n    return self.greedy_search(\n           ^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py\"\
          , line 2521, in greedy_search\n    outputs = self(\n              ^^^^^\n\
          \  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 610, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 392, in forward\n    return self.llm_forward(\n           ^^^^^^^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 476, in llm_forward\n    layer_outputs = decoder_layer(\n       \
          \             ^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 249, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n                                                  \
          \        ^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 197, in forward\n    query_states, key_states = self.rotary_emb(query_states,\
          \ key_states, position_ids=position_ids, max_seqlen=position_ids.max() +\
          \ 1)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 469, in forward\n    q = apply_rotary_emb_func(\n        ^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 329, in apply_rotary_emb\n    return ApplyRotaryEmb.apply(\n    \
          \       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py\"\
          , line 539, in apply\n    return super().apply(*args, **kwargs)  # type:\
          \ ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 255, in forward\n    out = apply_rotary(\n          ^^^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 212, in apply_rotary\n    rotary_kernel[grid](\n  File \"&lt;string&gt;\"\
          , line 63, in rotary_kernel\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/compiler/compiler.py\"\
          , line 425, in compile\n    so_path = make_stub(name, signature, constants)\n\
          \              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/compiler/make_launcher.py\"\
          , line 39, in make_stub\n    so = _build(name, src_path, tmpdir)\n     \
          \    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/common/build.py\"\
          , line 61, in _build\n    cuda_lib_dirs = libcuda_dirs()\n             \
          \       ^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/common/build.py\"\
          , line 21, in libcuda_dirs\n    libs = subprocess.check_output([\"ldconfig\"\
          , \"-p\"]).decode()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\", line\
          \ 466, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout,\
          \ check=True,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\", line\
          \ 548, in run\n    with Popen(*popenargs, **kwargs) as process:\n      \
          \   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\"\
          , line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn,\
          \ close_fds,\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\"\
          , line 1950, in _execute_child\n    raise child_exception_type(errno_num,\
          \ err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory:\
          \ 'ldconfig'\n</code></pre>\n"
        raw: "I'm trying to run the chat example  on a A100 GPU (with the specified\
          \ dependencies installed like described in the model page) but I get an\
          \ error related to a ldconfig binary, like this:\r\n\r\n```Traceback (most\
          \ recent call last):\r\n  File \"/cfs/home/u021543/test_cogvlm.py\", line\
          \ 27, in <module>\r\n    outputs = model.generate(**inputs, **gen_kwargs)\r\
          \n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py\"\
          , line 1673, in generate\r\n    return self.greedy_search(\r\n         \
          \  ^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py\"\
          , line 2521, in greedy_search\r\n    outputs = self(\r\n              ^^^^^\r\
          \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 610, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\
          \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 392, in forward\r\n    return self.llm_forward(\r\n           ^^^^^^^^^^^^^^^^^\r\
          \n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 476, in llm_forward\r\n    layer_outputs = decoder_layer(\r\n   \
          \                 ^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 249, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n                                                \
          \          ^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
          , line 197, in forward\r\n    query_states, key_states = self.rotary_emb(query_states,\
          \ key_states, position_ids=position_ids, max_seqlen=position_ids.max() +\
          \ 1)\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 469, in forward\r\n    q = apply_rotary_emb_func(\r\n        ^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 329, in apply_rotary_emb\r\n    return ApplyRotaryEmb.apply(\r\n\
          \           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py\"\
          , line 539, in apply\r\n    return super().apply(*args, **kwargs)  # type:\
          \ ignore[misc]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 255, in forward\r\n    out = apply_rotary(\r\n          ^^^^^^^^^^^^^\r\
          \n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
          , line 212, in apply_rotary\r\n    rotary_kernel[grid](\r\n  File \"<string>\"\
          , line 63, in rotary_kernel\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/compiler/compiler.py\"\
          , line 425, in compile\r\n    so_path = make_stub(name, signature, constants)\r\
          \n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/compiler/make_launcher.py\"\
          , line 39, in make_stub\r\n    so = _build(name, src_path, tmpdir)\r\n \
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/common/build.py\"\
          , line 61, in _build\r\n    cuda_lib_dirs = libcuda_dirs()\r\n         \
          \           ^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/common/build.py\"\
          , line 21, in libcuda_dirs\r\n    libs = subprocess.check_output([\"ldconfig\"\
          , \"-p\"]).decode()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\",\
          \ line 466, in check_output\r\n    return run(*popenargs, stdout=PIPE, timeout=timeout,\
          \ check=True,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\",\
          \ line 548, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\
          \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\"\
          , line 1026, in __init__\r\n    self._execute_child(args, executable, preexec_fn,\
          \ close_fds,\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\"\
          , line 1950, in _execute_child\r\n    raise child_exception_type(errno_num,\
          \ err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or\
          \ directory: 'ldconfig'\r\n```"
        updatedAt: '2023-12-03T16:04:40.822Z'
      numEdits: 0
      reactions: []
    id: 656ca7187c934a7b3c15d52d
    type: comment
  author: miguelcarv
  content: "I'm trying to run the chat example  on a A100 GPU (with the specified\
    \ dependencies installed like described in the model page) but I get an error\
    \ related to a ldconfig binary, like this:\r\n\r\n```Traceback (most recent call\
    \ last):\r\n  File \"/cfs/home/u021543/test_cogvlm.py\", line 27, in <module>\r\
    \n    outputs = model.generate(**inputs, **gen_kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n      \
    \     ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py\"\
    , line 1673, in generate\r\n    return self.greedy_search(\r\n           ^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py\"\
    , line 2521, in greedy_search\r\n    outputs = self(\r\n              ^^^^^\r\n\
    \  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
    , line 610, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
    , line 392, in forward\r\n    return self.llm_forward(\r\n           ^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
    , line 476, in llm_forward\r\n    layer_outputs = decoder_layer(\r\n         \
    \           ^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
    , line 249, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n                                                      \
    \    ^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/modeling_cogvlm.py\"\
    , line 197, in forward\r\n    query_states, key_states = self.rotary_emb(query_states,\
    \ key_states, position_ids=position_ids, max_seqlen=position_ids.max() + 1)\r\n\
    \                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
    , line 469, in forward\r\n    q = apply_rotary_emb_func(\r\n        ^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
    , line 329, in apply_rotary_emb\r\n    return ApplyRotaryEmb.apply(\r\n      \
    \     ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py\"\
    , line 539, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
    , line 255, in forward\r\n    out = apply_rotary(\r\n          ^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/2ecffe1437c99c459f202cce1458e8ccfa4a34c5/util.py\"\
    , line 212, in apply_rotary\r\n    rotary_kernel[grid](\r\n  File \"<string>\"\
    , line 63, in rotary_kernel\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/compiler/compiler.py\"\
    , line 425, in compile\r\n    so_path = make_stub(name, signature, constants)\r\
    \n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/compiler/make_launcher.py\"\
    , line 39, in make_stub\r\n    so = _build(name, src_path, tmpdir)\r\n       \
    \  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/common/build.py\"\
    , line 61, in _build\r\n    cuda_lib_dirs = libcuda_dirs()\r\n               \
    \     ^^^^^^^^^^^^^^\r\n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/site-packages/triton/common/build.py\"\
    , line 21, in libcuda_dirs\r\n    libs = subprocess.check_output([\"ldconfig\"\
    , \"-p\"]).decode()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\", line 466,\
    \ in check_output\r\n    return run(*popenargs, stdout=PIPE, timeout=timeout,\
    \ check=True,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\", line 548,\
    \ in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\", line 1026,\
    \ in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\
    \n  File \"/cfs/home/u021543/miniconda3/lib/python3.11/subprocess.py\", line 1950,\
    \ in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\
    \nFileNotFoundError: [Errno 2] No such file or directory: 'ldconfig'\r\n```"
  created_at: 2023-12-03 16:04:40+00:00
  edited: false
  hidden: false
  id: 656ca7187c934a7b3c15d52d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f68cbabcdfc96de3ef84d094ba1d60c.svg
      fullname: Sidharth Baskaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sidnb13
      type: user
    createdAt: '2023-12-03T18:56:51.000Z'
    data:
      edited: true
      editors:
      - sidnb13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8293700218200684
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f68cbabcdfc96de3ef84d094ba1d60c.svg
          fullname: Sidharth Baskaran
          isHf: false
          isPro: false
          name: sidnb13
          type: user
        html: '<p>I went into <code>&lt;venv-path&gt;/site-packages/triton/common/build.py</code>
          and changed <code>subprocess.check_output(["ldconfig", "-p"]).decode()</code>
          to <code>subprocess.check_output(["/sbin/ldconfig", "-p"]).decode()</code>.  Even
          after adding <code>ldconfig</code> to path since <code>check_output</code>
          isn''t called with <code>shell=True</code>, you don''t have access to env
          variables. I ran into more issues installing the latest version of triton
          so the hotfix was the best solution.</p>

          '
        raw: I went into `<venv-path>/site-packages/triton/common/build.py` and changed
          `subprocess.check_output(["ldconfig", "-p"]).decode()` to `subprocess.check_output(["/sbin/ldconfig",
          "-p"]).decode()`.  Even after adding `ldconfig` to path since `check_output`
          isn't called with `shell=True`, you don't have access to env variables.
          I ran into more issues installing the latest version of triton so the hotfix
          was the best solution.
        updatedAt: '2023-12-03T18:57:03.118Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - miguelcarv
    id: 656ccf739496f21be88d56d4
    type: comment
  author: sidnb13
  content: I went into `<venv-path>/site-packages/triton/common/build.py` and changed
    `subprocess.check_output(["ldconfig", "-p"]).decode()` to `subprocess.check_output(["/sbin/ldconfig",
    "-p"]).decode()`.  Even after adding `ldconfig` to path since `check_output` isn't
    called with `shell=True`, you don't have access to env variables. I ran into more
    issues installing the latest version of triton so the hotfix was the best solution.
  created_at: 2023-12-03 18:56:51+00:00
  edited: true
  hidden: false
  id: 656ccf739496f21be88d56d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f72b4e15c6b1743ef88c6bd9eb5d8de9.svg
      fullname: Miguel Carvalho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miguelcarv
      type: user
    createdAt: '2023-12-04T17:17:30.000Z'
    data:
      status: closed
    id: 656e09aa90d556ffa650f528
    type: status-change
  author: miguelcarv
  created_at: 2023-12-04 17:17:30+00:00
  id: 656e09aa90d556ffa650f528
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-05T07:49:03.000Z'
    data:
      edited: false
      editors:
      - chenkq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9720156192779541
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
          fullname: chenkq
          isHf: false
          isPro: false
          name: chenkq
          type: user
        html: '<p>the modeling_cogvlm.py is updated. the latest version of the model
          now includes a rotary embedding, which has no dependency on triton.<br>This
          update should address the issue you were experiencing. Please check and
          let me know if your problem has been resolved.</p>

          '
        raw: 'the modeling_cogvlm.py is updated. the latest version of the model now
          includes a rotary embedding, which has no dependency on triton.

          This update should address the issue you were experiencing. Please check
          and let me know if your problem has been resolved.'
        updatedAt: '2023-12-05T07:49:03.385Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656ed5ef8e7f2775ac022a80
    id: 656ed5ef8e7f2775ac022a7b
    type: comment
  author: chenkq
  content: 'the modeling_cogvlm.py is updated. the latest version of the model now
    includes a rotary embedding, which has no dependency on triton.

    This update should address the issue you were experiencing. Please check and let
    me know if your problem has been resolved.'
  created_at: 2023-12-05 07:49:03+00:00
  edited: false
  hidden: false
  id: 656ed5ef8e7f2775ac022a7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-05T07:49:03.000Z'
    data:
      status: open
    id: 656ed5ef8e7f2775ac022a80
    type: status-change
  author: chenkq
  created_at: 2023-12-05 07:49:03+00:00
  id: 656ed5ef8e7f2775ac022a80
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-11T04:03:31.000Z'
    data:
      status: closed
    id: 65768a13b4379e65a8cd6d80
    type: status-change
  author: chenkq
  created_at: 2023-12-11 04:03:31+00:00
  id: 65768a13b4379e65a8cd6d80
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: THUDM/cogvlm-chat-hf
repo_type: model
status: closed
target_branch: null
title: 'No such file or directory: ''ldconfig'''
