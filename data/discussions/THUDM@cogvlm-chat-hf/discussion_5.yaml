!!python/object:huggingface_hub.community.DiscussionWithDetails
author: grim3000
conflicting_files: null
created_at: 2023-11-28 14:27:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/032da43b6484fbc5b7b02a05d3db11ba.svg
      fullname: Marlon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grim3000
      type: user
    createdAt: '2023-11-28T14:27:42.000Z'
    data:
      edited: true
      editors:
      - grim3000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9284113049507141
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/032da43b6484fbc5b7b02a05d3db11ba.svg
          fullname: Marlon
          isHf: false
          isPro: false
          name: grim3000
          type: user
        html: '<p>Hi, I''m trying to run the using 4x T4 (16gb each), but encountering
          the error: <code>bf16 is only supported on A100+ GPUs</code></p>

          <p>While I wait for a quota increase to access A100''s or 2x A10''s, I''m
          curious how this model can be run with fp16 instead? I''ve seen some mentions
          of this in other discussions and on the Github repo but no clear examples</p>

          <p>Separately, are there any plans to update this repo so that the model
          can be easily deployed on HF inference endpoints? At the moment, it seems
          to require setting up a custom handler among other things</p>

          <p>Any help is appreciated!</p>

          '
        raw: 'Hi, I''m trying to run the using 4x T4 (16gb each), but encountering
          the error: `bf16 is only supported on A100+ GPUs`


          While I wait for a quota increase to access A100''s or 2x A10''s, I''m curious
          how this model can be run with fp16 instead? I''ve seen some mentions of
          this in other discussions and on the Github repo but no clear examples


          Separately, are there any plans to update this repo so that the model can
          be easily deployed on HF inference endpoints? At the moment, it seems to
          require setting up a custom handler among other things


          Any help is appreciated!'
        updatedAt: '2023-11-28T14:29:11.757Z'
      numEdits: 1
      reactions: []
    id: 6565f8dedd9e04e4cc804915
    type: comment
  author: grim3000
  content: 'Hi, I''m trying to run the using 4x T4 (16gb each), but encountering the
    error: `bf16 is only supported on A100+ GPUs`


    While I wait for a quota increase to access A100''s or 2x A10''s, I''m curious
    how this model can be run with fp16 instead? I''ve seen some mentions of this
    in other discussions and on the Github repo but no clear examples


    Separately, are there any plans to update this repo so that the model can be easily
    deployed on HF inference endpoints? At the moment, it seems to require setting
    up a custom handler among other things


    Any help is appreciated!'
  created_at: 2023-11-28 14:27:42+00:00
  edited: true
  hidden: false
  id: 6565f8dedd9e04e4cc804915
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-11-30T02:17:27.000Z'
    data:
      edited: false
      editors:
      - chenkq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24670176208019257
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
          fullname: chenkq
          isHf: false
          isPro: false
          name: chenkq
          type: user
        html: "<p>just change all <code>torch.bfloat16</code> to <code>torch.float16</code>\
          \ in example.</p>\n<pre><code>import torch\nimport requests\nfrom PIL import\
          \ Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\n\
          \ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\n    'THUDM/cogvlm-chat-hf',\n\
          \    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n\
          ).to('cuda').eval()\n\n\n# chat example\nquery = 'Describe this image'\n\
          image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
          \ stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer,\
          \ query=query, history=[], images=[image])  # chat mode\ninputs = {\n  \
          \  'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids':\
          \ inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n    'attention_mask':\
          \ inputs['attention_mask'].unsqueeze(0).to('cuda'),\n    'images': [[inputs['images'][0].to('cuda').to(torch.float16)]],\n\
          }\ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n\
          \    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:,\
          \ inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n\
          </code></pre>\n"
        raw: "just change all `torch.bfloat16` to `torch.float16` in example.\n\n\
          ```\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers\
          \ import AutoModelForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n\
          model = AutoModelForCausalLM.from_pretrained(\n    'THUDM/cogvlm-chat-hf',\n\
          \    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n\
          ).to('cuda').eval()\n\n\n# chat example\nquery = 'Describe this image'\n\
          image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
          \ stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer,\
          \ query=query, history=[], images=[image])  # chat mode\ninputs = {\n  \
          \  'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids':\
          \ inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n    'attention_mask':\
          \ inputs['attention_mask'].unsqueeze(0).to('cuda'),\n    'images': [[inputs['images'][0].to('cuda').to(torch.float16)]],\n\
          }\ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n\
          \    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:,\
          \ inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n\
          ```"
        updatedAt: '2023-11-30T02:17:27.615Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - zRzRzRzRzRzRzR
    id: 6567f0b792d9319907436b10
    type: comment
  author: chenkq
  content: "just change all `torch.bfloat16` to `torch.float16` in example.\n\n```\n\
    import torch\nimport requests\nfrom PIL import Image\nfrom transformers import\
    \ AutoModelForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n\
    model = AutoModelForCausalLM.from_pretrained(\n    'THUDM/cogvlm-chat-hf',\n \
    \   torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n\
    ).to('cuda').eval()\n\n\n# chat example\nquery = 'Describe this image'\nimage\
    \ = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
    \ stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer,\
    \ query=query, history=[], images=[image])  # chat mode\ninputs = {\n    'input_ids':\
    \ inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n\
    \    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n   \
    \ 'images': [[inputs['images'][0].to('cuda').to(torch.float16)]],\n}\ngen_kwargs\
    \ = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n  \
    \  outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:,\
    \ inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n```"
  created_at: 2023-11-30 02:17:27+00:00
  edited: false
  hidden: false
  id: 6567f0b792d9319907436b10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-04T02:11:49.000Z'
    data:
      status: closed
    id: 656d3565adba74cd5e8cb39d
    type: status-change
  author: chenkq
  created_at: 2023-12-04 02:11:49+00:00
  id: 656d3565adba74cd5e8cb39d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: THUDM/cogvlm-chat-hf
repo_type: model
status: closed
target_branch: null
title: How to run fp16?
