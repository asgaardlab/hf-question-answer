!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ptx0
conflicting_files: null
created_at: 2023-11-30 00:29:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/447d8fe5c7126f590c0333f0693e6c5c.svg
      fullname: PseudoTerminal X
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ptx0
      type: user
    createdAt: '2023-11-30T00:29:22.000Z'
    data:
      edited: false
      editors:
      - ptx0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754969477653503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/447d8fe5c7126f590c0333f0693e6c5c.svg
          fullname: PseudoTerminal X
          isHf: false
          isPro: false
          name: ptx0
          type: user
        html: '<p>Will it support batched inputs? One at a time is very slow. It only
          uses 37G VRAM, which is not all of the 80G we use here.</p>

          '
        raw: Will it support batched inputs? One at a time is very slow. It only uses
          37G VRAM, which is not all of the 80G we use here.
        updatedAt: '2023-11-30T00:29:22.215Z'
      numEdits: 0
      reactions: []
    id: 6567d762456d7733de551534
    type: comment
  author: ptx0
  content: Will it support batched inputs? One at a time is very slow. It only uses
    37G VRAM, which is not all of the 80G we use here.
  created_at: 2023-11-30 00:29:22+00:00
  edited: false
  hidden: false
  id: 6567d762456d7733de551534
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-11-30T02:50:21.000Z'
    data:
      edited: false
      editors:
      - chenkq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.335936963558197
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
          fullname: chenkq
          isHf: false
          isPro: false
          name: chenkq
          type: user
        html: "<p>yes, try sth like this.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >import</span> requests\n<span class=\"hljs-keyword\">from</span> PIL <span\
          \ class=\"hljs-keyword\">import</span> Image\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(<span class=\"\
          hljs-string\">'lmsys/vicuna-7b-v1.5'</span>)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    <span class=\"hljs-string\">'THUDM/cogvlm-chat-hf'</span>,\n    torch_dtype=torch.bfloat16,\n\
          \    low_cpu_mem_usage=<span class=\"hljs-literal\">True</span>,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>\n).to(<span class=\"hljs-string\">'cuda'</span>).<span\
          \ class=\"hljs-built_in\">eval</span>()\n\ninput_sample1 = model.build_conversation_input_ids(\n\
          \    tokenizer,\n    images=[Image.<span class=\"hljs-built_in\">open</span>(requests.get(<span\
          \ class=\"hljs-string\">'https://github.com/THUDM/CogVLM/blob/main/openai_demo/demo.jpg?raw=true'</span>,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw).convert(<span class=\"\
          hljs-string\">'RGB'</span>),],\n    query=<span class=\"hljs-string\">'Do\
          \ you think this is a spring or winter photo?'</span>,  <span class=\"hljs-comment\"\
          ># Q2</span>\n    history=[\n        (\n            <span class=\"hljs-string\"\
          >\"What's in this image?\"</span>,   <span class=\"hljs-comment\"># Q1</span>\n\
          \            <span class=\"hljs-string\">'The image displays a wooden boardwalk\
          \ extending through a vibrant green grassy wetland.'</span>  <span class=\"\
          hljs-comment\"># A1</span>\n         )\n        ], \n    )\ninput_sample2\
          \ = model.build_conversation_input_ids(\n    tokenizer,\n    images=[Image.<span\
          \ class=\"hljs-built_in\">open</span>(requests.get(<span class=\"hljs-string\"\
          >'https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true'</span>,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw).convert(<span class=\"\
          hljs-string\">'RGB'</span>),],\n    query=<span class=\"hljs-string\">'Describe\
          \ this image'</span>,  <span class=\"hljs-comment\"># Q1</span>\n    history=[],\
          \ \n    )\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">recur_move_to</span>(<span class=\"hljs-params\">item, tgt,\
          \ criterion_func</span>):\n    <span class=\"hljs-keyword\">if</span> criterion_func(item):\n\
          \        device_copy = item.to(tgt)\n        <span class=\"hljs-keyword\"\
          >return</span> device_copy\n    <span class=\"hljs-keyword\">elif</span>\
          \ <span class=\"hljs-built_in\">isinstance</span>(item, <span class=\"hljs-built_in\"\
          >list</span>):\n        <span class=\"hljs-keyword\">return</span> [recur_move_to(v,\
          \ tgt, criterion_func) <span class=\"hljs-keyword\">for</span> v <span class=\"\
          hljs-keyword\">in</span> item]\n    <span class=\"hljs-keyword\">elif</span>\
          \ <span class=\"hljs-built_in\">isinstance</span>(item, <span class=\"hljs-built_in\"\
          >tuple</span>):\n        <span class=\"hljs-keyword\">return</span> <span\
          \ class=\"hljs-built_in\">tuple</span>([recur_move_to(v, tgt, criterion_func)\
          \ <span class=\"hljs-keyword\">for</span> v <span class=\"hljs-keyword\"\
          >in</span> item])\n    <span class=\"hljs-keyword\">elif</span> <span class=\"\
          hljs-built_in\">isinstance</span>(item, <span class=\"hljs-built_in\">dict</span>):\n\
          \        <span class=\"hljs-keyword\">return</span> {k: recur_move_to(v,\
          \ tgt, criterion_func) <span class=\"hljs-keyword\">for</span> k, v <span\
          \ class=\"hljs-keyword\">in</span> item.items()}\n    <span class=\"hljs-keyword\"\
          >else</span>:\n        <span class=\"hljs-keyword\">return</span> item\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >collate_fn</span>(<span class=\"hljs-params\">features, tokenizer</span>)\
          \ -&gt; <span class=\"hljs-built_in\">dict</span>:\n    images = [feature.pop(<span\
          \ class=\"hljs-string\">'images'</span>) <span class=\"hljs-keyword\">for</span>\
          \ feature <span class=\"hljs-keyword\">in</span> features]\n    tokenizer.padding_side\
          \ = <span class=\"hljs-string\">'left'</span>\n    padded_features = tokenizer.pad(features)\n\
          \    inputs = {**padded_features, <span class=\"hljs-string\">'images'</span>:\
          \ images}\n    <span class=\"hljs-keyword\">return</span> inputs\n\ninput_batch\
          \ = collate_fn([input_sample1, input_sample2], tokenizer)\ninput_batch =\
          \ recur_move_to(input_batch, <span class=\"hljs-string\">'cuda'</span>,\
          \ <span class=\"hljs-keyword\">lambda</span> x: <span class=\"hljs-built_in\"\
          >isinstance</span>(x, torch.Tensor))\ninput_batch = recur_move_to(input_batch,\
          \ torch.bfloat16, <span class=\"hljs-keyword\">lambda</span> x: <span class=\"\
          hljs-built_in\">isinstance</span>(x, torch.Tensor) <span class=\"hljs-keyword\"\
          >and</span> torch.is_floating_point(x))\n\ngen_kwargs = {<span class=\"\
          hljs-string\">\"max_length\"</span>: <span class=\"hljs-number\">2048</span>,\
          \ <span class=\"hljs-string\">\"do_sample\"</span>: <span class=\"hljs-literal\"\
          >False</span>}\n\n<span class=\"hljs-keyword\">with</span> torch.no_grad():\n\
          \    outputs = model.generate(**input_batch, **gen_kwargs)\n    outputs\
          \ = outputs[:, input_batch[<span class=\"hljs-string\">'input_ids'</span>].shape[<span\
          \ class=\"hljs-number\">1</span>]:]\n    <span class=\"hljs-built_in\">print</span>(tokenizer.batch_decode(outputs))\n\
          </code></pre>\n"
        raw: "yes, try sth like this.\n\n```python\nimport torch\nimport requests\n\
          from PIL import Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\n\
          \ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\n    'THUDM/cogvlm-chat-hf',\n\
          \    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n\
          ).to('cuda').eval()\n\ninput_sample1 = model.build_conversation_input_ids(\n\
          \    tokenizer,\n    images=[Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/openai_demo/demo.jpg?raw=true',\
          \ stream=True).raw).convert('RGB'),],\n    query='Do you think this is a\
          \ spring or winter photo?',  # Q2\n    history=[\n        (\n          \
          \  \"What's in this image?\",   # Q1\n            'The image displays a\
          \ wooden boardwalk extending through a vibrant green grassy wetland.'  #\
          \ A1\n         )\n        ], \n    )\ninput_sample2 = model.build_conversation_input_ids(\n\
          \    tokenizer,\n    images=[Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
          \ stream=True).raw).convert('RGB'),],\n    query='Describe this image',\
          \  # Q1\n    history=[], \n    )\n\ndef recur_move_to(item, tgt, criterion_func):\n\
          \    if criterion_func(item):\n        device_copy = item.to(tgt)\n    \
          \    return device_copy\n    elif isinstance(item, list):\n        return\
          \ [recur_move_to(v, tgt, criterion_func) for v in item]\n    elif isinstance(item,\
          \ tuple):\n        return tuple([recur_move_to(v, tgt, criterion_func) for\
          \ v in item])\n    elif isinstance(item, dict):\n        return {k: recur_move_to(v,\
          \ tgt, criterion_func) for k, v in item.items()}\n    else:\n        return\
          \ item\n\ndef collate_fn(features, tokenizer) -> dict:\n    images = [feature.pop('images')\
          \ for feature in features]\n    tokenizer.padding_side = 'left'\n    padded_features\
          \ = tokenizer.pad(features)\n    inputs = {**padded_features, 'images':\
          \ images}\n    return inputs\n\ninput_batch = collate_fn([input_sample1,\
          \ input_sample2], tokenizer)\ninput_batch = recur_move_to(input_batch, 'cuda',\
          \ lambda x: isinstance(x, torch.Tensor))\ninput_batch = recur_move_to(input_batch,\
          \ torch.bfloat16, lambda x: isinstance(x, torch.Tensor) and torch.is_floating_point(x))\n\
          \ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n\
          \    outputs = model.generate(**input_batch, **gen_kwargs)\n    outputs\
          \ = outputs[:, input_batch['input_ids'].shape[1]:]\n    print(tokenizer.batch_decode(outputs))\n\
          ```"
        updatedAt: '2023-11-30T02:50:21.234Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mactavish91
        - zRzRzRzRzRzRzR
    id: 6567f86d49653cac901050e0
    type: comment
  author: chenkq
  content: "yes, try sth like this.\n\n```python\nimport torch\nimport requests\n\
    from PIL import Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\n\
    \ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\nmodel =\
    \ AutoModelForCausalLM.from_pretrained(\n    'THUDM/cogvlm-chat-hf',\n    torch_dtype=torch.bfloat16,\n\
    \    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).to('cuda').eval()\n\
    \ninput_sample1 = model.build_conversation_input_ids(\n    tokenizer,\n    images=[Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/openai_demo/demo.jpg?raw=true',\
    \ stream=True).raw).convert('RGB'),],\n    query='Do you think this is a spring\
    \ or winter photo?',  # Q2\n    history=[\n        (\n            \"What's in\
    \ this image?\",   # Q1\n            'The image displays a wooden boardwalk extending\
    \ through a vibrant green grassy wetland.'  # A1\n         )\n        ], \n  \
    \  )\ninput_sample2 = model.build_conversation_input_ids(\n    tokenizer,\n  \
    \  images=[Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true',\
    \ stream=True).raw).convert('RGB'),],\n    query='Describe this image',  # Q1\n\
    \    history=[], \n    )\n\ndef recur_move_to(item, tgt, criterion_func):\n  \
    \  if criterion_func(item):\n        device_copy = item.to(tgt)\n        return\
    \ device_copy\n    elif isinstance(item, list):\n        return [recur_move_to(v,\
    \ tgt, criterion_func) for v in item]\n    elif isinstance(item, tuple):\n   \
    \     return tuple([recur_move_to(v, tgt, criterion_func) for v in item])\n  \
    \  elif isinstance(item, dict):\n        return {k: recur_move_to(v, tgt, criterion_func)\
    \ for k, v in item.items()}\n    else:\n        return item\n\ndef collate_fn(features,\
    \ tokenizer) -> dict:\n    images = [feature.pop('images') for feature in features]\n\
    \    tokenizer.padding_side = 'left'\n    padded_features = tokenizer.pad(features)\n\
    \    inputs = {**padded_features, 'images': images}\n    return inputs\n\ninput_batch\
    \ = collate_fn([input_sample1, input_sample2], tokenizer)\ninput_batch = recur_move_to(input_batch,\
    \ 'cuda', lambda x: isinstance(x, torch.Tensor))\ninput_batch = recur_move_to(input_batch,\
    \ torch.bfloat16, lambda x: isinstance(x, torch.Tensor) and torch.is_floating_point(x))\n\
    \ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n\
    \    outputs = model.generate(**input_batch, **gen_kwargs)\n    outputs = outputs[:,\
    \ input_batch['input_ids'].shape[1]:]\n    print(tokenizer.batch_decode(outputs))\n\
    ```"
  created_at: 2023-11-30 02:50:21+00:00
  edited: false
  hidden: false
  id: 6567f86d49653cac901050e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0d95d65d30f6672ec09dc92155324d7f.svg
      fullname: chenkq
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenkq
      type: user
    createdAt: '2023-12-04T02:11:10.000Z'
    data:
      status: closed
    id: 656d353e53703dd78fa93fe1
    type: status-change
  author: chenkq
  created_at: 2023-12-04 02:11:10+00:00
  id: 656d353e53703dd78fa93fe1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: THUDM/cogvlm-chat-hf
repo_type: model
status: closed
target_branch: null
title: batch input
