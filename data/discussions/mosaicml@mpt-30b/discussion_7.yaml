!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deepakkrish92
conflicting_files: null
created_at: 2023-06-29 15:02:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9383b2aa87e0ca8011e2e0ddebc62a6.svg
      fullname: Deepak krishna AR
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkrish92
      type: user
    createdAt: '2023-06-29T16:02:55.000Z'
    data:
      edited: false
      editors:
      - deepakkrish92
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8041782975196838
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9383b2aa87e0ca8011e2e0ddebc62a6.svg
          fullname: Deepak krishna AR
          isHf: false
          isPro: false
          name: deepakkrish92
          type: user
        html: "<p>I was trying to use 'mosaicml/mpt-30b' in databricks. compute resource\
          \ is A100 1 GPU 220 GB. I think  it is mentioned in model card that it is\
          \ easy to deploy mpt-30b on a single GPU\u20141xA100-80GB. my dependencies\
          \ are transformers==4.30.2 &amp; torch Version: 1.13.1+cu117. Getting the\
          \ memory error when I am calling the final inference  here -<br>with torch.autocast('cuda',\
          \ dtype=torch.bfloat16):<br>    sequences = pipeline(\"What is Machine Learning?\"\
          ,<br>            max_new_tokens=100,<br>            do_sample=True,<br>\
          \            use_cache=True,<br>            )<br>print(sequences) </p>\n"
        raw: "I was trying to use 'mosaicml/mpt-30b' in databricks. compute resource\
          \ is A100 1 GPU 220 GB. I think  it is mentioned in model card that it is\
          \ easy to deploy mpt-30b on a single GPU\u20141xA100-80GB. my dependencies\
          \ are transformers==4.30.2 & torch Version: 1.13.1+cu117. Getting the memory\
          \ error when I am calling the final inference  here - \r\nwith torch.autocast('cuda',\
          \ dtype=torch.bfloat16):\r\n    sequences = pipeline(\"What is Machine Learning?\"\
          ,\r\n            max_new_tokens=100,\r\n            do_sample=True,\r\n\
          \            use_cache=True,\r\n            ) \r\nprint(sequences) "
        updatedAt: '2023-06-29T16:02:55.020Z'
      numEdits: 0
      reactions: []
    id: 649dab2f6363fcb6bb8beeeb
    type: comment
  author: deepakkrish92
  content: "I was trying to use 'mosaicml/mpt-30b' in databricks. compute resource\
    \ is A100 1 GPU 220 GB. I think  it is mentioned in model card that it is easy\
    \ to deploy mpt-30b on a single GPU\u20141xA100-80GB. my dependencies are transformers==4.30.2\
    \ & torch Version: 1.13.1+cu117. Getting the memory error when I am calling the\
    \ final inference  here - \r\nwith torch.autocast('cuda', dtype=torch.bfloat16):\r\
    \n    sequences = pipeline(\"What is Machine Learning?\",\r\n            max_new_tokens=100,\r\
    \n            do_sample=True,\r\n            use_cache=True,\r\n            )\
    \ \r\nprint(sequences) "
  created_at: 2023-06-29 15:02:55+00:00
  edited: false
  hidden: false
  id: 649dab2f6363fcb6bb8beeeb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: mosaicml/mpt-30b
repo_type: model
status: open
target_branch: null
title: OutOfMemoryError
