!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BadatFreaks
conflicting_files: null
created_at: 2023-12-26 07:29:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tcvlgRCegpFnDOwF85Tz_.png?w=200&h=200&f=face
      fullname: Muhammad Anwar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BadatFreaks
      type: user
    createdAt: '2023-12-26T07:29:09.000Z'
    data:
      edited: false
      editors:
      - BadatFreaks
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4997981786727905
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tcvlgRCegpFnDOwF85Tz_.png?w=200&h=200&f=face
          fullname: Muhammad Anwar
          isHf: false
          isPro: false
          name: BadatFreaks
          type: user
        html: '<p>I have my code as under, and I am getting the error attached, where
          can I find the missing files? I am new to LLMs please guide me!</p>

          <p>import os<br>import torch<br>from transformers import AutoModelForCausalLM,
          AutoTokenizer, pipeline</p>

          <h1 id="define-the-model-name">Define the model name</h1>

          <p>model_name = "TheBloke/Llama-2-70B-Chat-GGUF"<br>local_model_dir = "./Llama-2-70B-Model"</p>

          <h1 id="check-if-gpu-is-available">Check if GPU is available</h1>

          <p>device = 0 if torch.cuda.is_available() else -1</p>

          <h1 id="function-to-load-or-download-the-model">Function to load or download
          the model</h1>

          <p>def load_model():<br>    if not os.path.exists(local_model_dir):<br>        os.makedirs(local_model_dir,
          exist_ok=True)<br>        print(f"Downloading model {model_name} for the
          first time...")<br>        model = AutoModelForCausalLM.from_pretrained(model_name)<br>        tokenizer
          = AutoTokenizer.from_pretrained(model_name)<br>        model.save_pretrained(local_model_dir)<br>        tokenizer.save_pretrained(local_model_dir)<br>    else:<br>        print(f"Loading
          model {model_name} from local storage...")<br>        model = AutoModelForCausalLM.from_pretrained(local_model_dir)<br>        tokenizer
          = AutoTokenizer.from_pretrained(local_model_dir)<br>    return model, tokenizer</p>

          <h1 id="load-the-model">Load the model</h1>

          <p>model, tokenizer = load_model()</p>

          <h1 id="create-a-pipeline-with-the-local-model">Create a pipeline with the
          local model</h1>

          <p>pipe = pipeline("text-generation", model=model, tokenizer=tokenizer,
          device=device)</p>

          <h1 id="interactive-prompt">Interactive prompt</h1>

          <p>while True:<br>    prompt = input("Enter your prompt (type ''exit'' to
          stop): ")<br>    if prompt.lower() == "exit":<br>        break<br>    response
          = pipe(prompt, max_length=50)  # Adjust max_length as needed<br>    print(response[0][''generated_text''])</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65716aa19c503c4a6a19f1ef/aQ-C3XFDJUyoeZKVtvYPd.png"><img
          alt="Screenshot from 2023-12-26 12-28-50.png" src="https://cdn-uploads.huggingface.co/production/uploads/65716aa19c503c4a6a19f1ef/aQ-C3XFDJUyoeZKVtvYPd.png"></a></p>

          '
        raw: "I have my code as under, and I am getting the error attached, where\
          \ can I find the missing files? I am new to LLMs please guide me!\r\n\r\n\
          import os\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, pipeline\r\n\r\n# Define the model name\r\nmodel_name =\
          \ \"TheBloke/Llama-2-70B-Chat-GGUF\"\r\nlocal_model_dir = \"./Llama-2-70B-Model\"\
          \r\n\r\n# Check if GPU is available\r\ndevice = 0 if torch.cuda.is_available()\
          \ else -1\r\n\r\n# Function to load or download the model\r\ndef load_model():\r\
          \n    if not os.path.exists(local_model_dir):\r\n        os.makedirs(local_model_dir,\
          \ exist_ok=True)\r\n        print(f\"Downloading model {model_name} for\
          \ the first time...\")\r\n        model = AutoModelForCausalLM.from_pretrained(model_name)\r\
          \n        tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n    \
          \    model.save_pretrained(local_model_dir)\r\n        tokenizer.save_pretrained(local_model_dir)\r\
          \n    else:\r\n        print(f\"Loading model {model_name} from local storage...\"\
          )\r\n        model = AutoModelForCausalLM.from_pretrained(local_model_dir)\r\
          \n        tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\r\n\
          \    return model, tokenizer\r\n\r\n# Load the model\r\nmodel, tokenizer\
          \ = load_model()\r\n\r\n# Create a pipeline with the local model\r\npipe\
          \ = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\r\
          \n\r\n# Interactive prompt\r\nwhile True:\r\n    prompt = input(\"Enter\
          \ your prompt (type 'exit' to stop): \")\r\n    if prompt.lower() == \"\
          exit\":\r\n        break\r\n    response = pipe(prompt, max_length=50) \
          \ # Adjust max_length as needed\r\n    print(response[0]['generated_text'])\r\
          \n\r\n![Screenshot from 2023-12-26 12-28-50.png](https://cdn-uploads.huggingface.co/production/uploads/65716aa19c503c4a6a19f1ef/aQ-C3XFDJUyoeZKVtvYPd.png)\r\
          \n"
        updatedAt: '2023-12-26T07:29:09.163Z'
      numEdits: 0
      reactions: []
    id: 658a80c5367c76b8ee52d6fb
    type: comment
  author: BadatFreaks
  content: "I have my code as under, and I am getting the error attached, where can\
    \ I find the missing files? I am new to LLMs please guide me!\r\n\r\nimport os\r\
    \nimport torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
    \ pipeline\r\n\r\n# Define the model name\r\nmodel_name = \"TheBloke/Llama-2-70B-Chat-GGUF\"\
    \r\nlocal_model_dir = \"./Llama-2-70B-Model\"\r\n\r\n# Check if GPU is available\r\
    \ndevice = 0 if torch.cuda.is_available() else -1\r\n\r\n# Function to load or\
    \ download the model\r\ndef load_model():\r\n    if not os.path.exists(local_model_dir):\r\
    \n        os.makedirs(local_model_dir, exist_ok=True)\r\n        print(f\"Downloading\
    \ model {model_name} for the first time...\")\r\n        model = AutoModelForCausalLM.from_pretrained(model_name)\r\
    \n        tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n        model.save_pretrained(local_model_dir)\r\
    \n        tokenizer.save_pretrained(local_model_dir)\r\n    else:\r\n        print(f\"\
    Loading model {model_name} from local storage...\")\r\n        model = AutoModelForCausalLM.from_pretrained(local_model_dir)\r\
    \n        tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\r\n    return\
    \ model, tokenizer\r\n\r\n# Load the model\r\nmodel, tokenizer = load_model()\r\
    \n\r\n# Create a pipeline with the local model\r\npipe = pipeline(\"text-generation\"\
    , model=model, tokenizer=tokenizer, device=device)\r\n\r\n# Interactive prompt\r\
    \nwhile True:\r\n    prompt = input(\"Enter your prompt (type 'exit' to stop):\
    \ \")\r\n    if prompt.lower() == \"exit\":\r\n        break\r\n    response =\
    \ pipe(prompt, max_length=50)  # Adjust max_length as needed\r\n    print(response[0]['generated_text'])\r\
    \n\r\n![Screenshot from 2023-12-26 12-28-50.png](https://cdn-uploads.huggingface.co/production/uploads/65716aa19c503c4a6a19f1ef/aQ-C3XFDJUyoeZKVtvYPd.png)\r\
    \n"
  created_at: 2023-12-26 07:29:09+00:00
  edited: false
  hidden: false
  id: 658a80c5367c76b8ee52d6fb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Llama-2-70B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: Missing files
