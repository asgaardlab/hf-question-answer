!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JuLuComputing
conflicting_files: null
created_at: 2023-11-28 16:15:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-11-28T16:15:20.000Z'
    data:
      edited: false
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7149962782859802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: "<p>Hello all!</p>\n<p>I was testing several of these Switch models\
          \ and decided to share some of my code for testing.  It has simple chat\
          \ history which, unless you know what you're doing with a Switch model,\
          \ the history can really confuse things.</p>\n<p>Anyway, I tested using\
          \ CPU only.  This model while running, used an additional 60GB of ram, I\
          \ would make sure you use this on a computer with at least 96GB of ram.\
          \  Here is the script, if you have any questions, feel free to ask me:</p>\n\
          <pre><code>import time\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\
          \nprint(f\"Loading model, please wait...\")\n\n# Start timer for the entire\
          \ script\nstart_time = time.time()\n\n# Timer for model loading\nloading_start_time\
          \ = time.time()\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-256\"\
          )\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"\
          /whole/path/to/google_switch-base-256/\", local_files_only=True)\n\nloading_end_time\
          \ = time.time()\nloading_time = loading_end_time - loading_start_time\n\n\
          print(f\"Model loading time: {loading_time:.2f} seconds\")\n\n# Simple conversation\
          \ memory\nconversation_history = []\n\nwhile True:\n    # Get user input\n\
          \    user_input = input(\"User: \")\n\n    # Add user input to conversation\
          \ history\n    conversation_history.append(f\"User: {user_input}\")\n\n\
          \    # Combine conversation history into a single string\n    conversation_text\
          \ = \" \".join(conversation_history)\n    \n    # Timer for generating the\
          \ response\n    generation_start_time = time.time()\n\n    # Tokenize and\
          \ generate response\n    input_ids = tokenizer(conversation_text, return_tensors=\"\
          pt\").input_ids\n    decoder_start_token_id = tokenizer.convert_tokens_to_ids(\"\
          &lt;pad&gt;\")\n    max_new_tokens = 2048  # Adjust the value as needed\n\
          \    outputs = model.generate(input_ids, decoder_start_token_id=decoder_start_token_id,\
          \ max_new_tokens=max_new_tokens)\n\n    generation_end_time = time.time()\n\
          \    generation_time = generation_end_time - generation_start_time\n\n \
          \   # Print the generated output\n    generated_text = tokenizer.decode(outputs[0])\n\
          \    print(f\"SwitchBot: {generated_text}\")\n\n    print(f\"Response generation\
          \ time: {generation_time:.2f} seconds\")\n\n    # Token counter\n    response_tokens\
          \ = len(outputs[0])\n    tokens_per_second = response_tokens / generation_time\n\
          \n    print(f\"Number of response tokens: {response_tokens}\")\n    print(f\"\
          Tokens per second: {tokens_per_second:.2f}\")\n\n    # Add SwitchBot's response\
          \ to conversation history\n    conversation_history.append(f\"SwitchBot:\
          \ {generated_text}\")\n</code></pre>\n<p>Note:  I downloaded all the files\
          \ to a local directory and ran the script from that same directory.  You\
          \ will need to edit this script to tell it that same whole directory path.</p>\n"
        raw: "Hello all!\r\n\r\nI was testing several of these Switch models and decided\
          \ to share some of my code for testing.  It has simple chat history which,\
          \ unless you know what you're doing with a Switch model, the history can\
          \ really confuse things.\r\n\r\nAnyway, I tested using CPU only.  This model\
          \ while running, used an additional 60GB of ram, I would make sure you use\
          \ this on a computer with at least 96GB of ram.  Here is the script, if\
          \ you have any questions, feel free to ask me:\r\n\r\n```\r\nimport time\r\
          \nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\r\
          \n\r\nprint(f\"Loading model, please wait...\")\r\n\r\n# Start timer for\
          \ the entire script\r\nstart_time = time.time()\r\n\r\n# Timer for model\
          \ loading\r\nloading_start_time = time.time()\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          google/switch-base-256\")\r\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"\
          /whole/path/to/google_switch-base-256/\", local_files_only=True)\r\n\r\n\
          loading_end_time = time.time()\r\nloading_time = loading_end_time - loading_start_time\r\
          \n\r\nprint(f\"Model loading time: {loading_time:.2f} seconds\")\r\n\r\n\
          # Simple conversation memory\r\nconversation_history = []\r\n\r\nwhile True:\r\
          \n    # Get user input\r\n    user_input = input(\"User: \")\r\n\r\n   \
          \ # Add user input to conversation history\r\n    conversation_history.append(f\"\
          User: {user_input}\")\r\n\r\n    # Combine conversation history into a single\
          \ string\r\n    conversation_text = \" \".join(conversation_history)\r\n\
          \    \r\n    # Timer for generating the response\r\n    generation_start_time\
          \ = time.time()\r\n\r\n    # Tokenize and generate response\r\n    input_ids\
          \ = tokenizer(conversation_text, return_tensors=\"pt\").input_ids\r\n  \
          \  decoder_start_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\r\
          \n    max_new_tokens = 2048  # Adjust the value as needed\r\n    outputs\
          \ = model.generate(input_ids, decoder_start_token_id=decoder_start_token_id,\
          \ max_new_tokens=max_new_tokens)\r\n\r\n    generation_end_time = time.time()\r\
          \n    generation_time = generation_end_time - generation_start_time\r\n\r\
          \n    # Print the generated output\r\n    generated_text = tokenizer.decode(outputs[0])\r\
          \n    print(f\"SwitchBot: {generated_text}\")\r\n\r\n    print(f\"Response\
          \ generation time: {generation_time:.2f} seconds\")\r\n\r\n    # Token counter\r\
          \n    response_tokens = len(outputs[0])\r\n    tokens_per_second = response_tokens\
          \ / generation_time\r\n\r\n    print(f\"Number of response tokens: {response_tokens}\"\
          )\r\n    print(f\"Tokens per second: {tokens_per_second:.2f}\")\r\n\r\n\
          \    # Add SwitchBot's response to conversation history\r\n    conversation_history.append(f\"\
          SwitchBot: {generated_text}\")\r\n```\r\n\r\nNote:  I downloaded all the\
          \ files to a local directory and ran the script from that same directory.\
          \  You will need to edit this script to tell it that same whole directory\
          \ path."
        updatedAt: '2023-11-28T16:15:20.320Z'
      numEdits: 0
      reactions: []
    id: 6566121866d5f87c6297fbeb
    type: comment
  author: JuLuComputing
  content: "Hello all!\r\n\r\nI was testing several of these Switch models and decided\
    \ to share some of my code for testing.  It has simple chat history which, unless\
    \ you know what you're doing with a Switch model, the history can really confuse\
    \ things.\r\n\r\nAnyway, I tested using CPU only.  This model while running, used\
    \ an additional 60GB of ram, I would make sure you use this on a computer with\
    \ at least 96GB of ram.  Here is the script, if you have any questions, feel free\
    \ to ask me:\r\n\r\n```\r\nimport time\r\nfrom transformers import AutoTokenizer,\
    \ SwitchTransformersForConditionalGeneration\r\n\r\nprint(f\"Loading model, please\
    \ wait...\")\r\n\r\n# Start timer for the entire script\r\nstart_time = time.time()\r\
    \n\r\n# Timer for model loading\r\nloading_start_time = time.time()\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"google/switch-base-256\")\r\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"\
    /whole/path/to/google_switch-base-256/\", local_files_only=True)\r\n\r\nloading_end_time\
    \ = time.time()\r\nloading_time = loading_end_time - loading_start_time\r\n\r\n\
    print(f\"Model loading time: {loading_time:.2f} seconds\")\r\n\r\n# Simple conversation\
    \ memory\r\nconversation_history = []\r\n\r\nwhile True:\r\n    # Get user input\r\
    \n    user_input = input(\"User: \")\r\n\r\n    # Add user input to conversation\
    \ history\r\n    conversation_history.append(f\"User: {user_input}\")\r\n\r\n\
    \    # Combine conversation history into a single string\r\n    conversation_text\
    \ = \" \".join(conversation_history)\r\n    \r\n    # Timer for generating the\
    \ response\r\n    generation_start_time = time.time()\r\n\r\n    # Tokenize and\
    \ generate response\r\n    input_ids = tokenizer(conversation_text, return_tensors=\"\
    pt\").input_ids\r\n    decoder_start_token_id = tokenizer.convert_tokens_to_ids(\"\
    <pad>\")\r\n    max_new_tokens = 2048  # Adjust the value as needed\r\n    outputs\
    \ = model.generate(input_ids, decoder_start_token_id=decoder_start_token_id, max_new_tokens=max_new_tokens)\r\
    \n\r\n    generation_end_time = time.time()\r\n    generation_time = generation_end_time\
    \ - generation_start_time\r\n\r\n    # Print the generated output\r\n    generated_text\
    \ = tokenizer.decode(outputs[0])\r\n    print(f\"SwitchBot: {generated_text}\"\
    )\r\n\r\n    print(f\"Response generation time: {generation_time:.2f} seconds\"\
    )\r\n\r\n    # Token counter\r\n    response_tokens = len(outputs[0])\r\n    tokens_per_second\
    \ = response_tokens / generation_time\r\n\r\n    print(f\"Number of response tokens:\
    \ {response_tokens}\")\r\n    print(f\"Tokens per second: {tokens_per_second:.2f}\"\
    )\r\n\r\n    # Add SwitchBot's response to conversation history\r\n    conversation_history.append(f\"\
    SwitchBot: {generated_text}\")\r\n```\r\n\r\nNote:  I downloaded all the files\
    \ to a local directory and ran the script from that same directory.  You will\
    \ need to edit this script to tell it that same whole directory path."
  created_at: 2023-11-28 16:15:20+00:00
  edited: false
  hidden: false
  id: 6566121866d5f87c6297fbeb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
      fullname: Susnato Dhar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: susnato
      type: user
    createdAt: '2024-01-10T16:02:48.000Z'
    data:
      edited: false
      editors:
      - susnato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7952556610107422
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
          fullname: Susnato Dhar
          isHf: false
          isPro: false
          name: susnato
          type: user
        html: "<p>Thanks for sharing <span data-props=\"{&quot;user&quot;:&quot;JuLuComputing&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JuLuComputing\"\
          >@<span class=\"underline\">JuLuComputing</span></a></span>\n\n\t</span></span>\
          \  !</p>\n"
        raw: Thanks for sharing @JuLuComputing  !
        updatedAt: '2024-01-10T16:02:48.824Z'
      numEdits: 0
      reactions: []
    id: 659ebfa8812d722d8ac12f0f
    type: comment
  author: susnato
  content: Thanks for sharing @JuLuComputing  !
  created_at: 2024-01-10 16:02:48+00:00
  edited: false
  hidden: false
  id: 659ebfa8812d722d8ac12f0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T18:47:36.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6457774639129639
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;JuLuComputing&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JuLuComputing\"\
          >@<span class=\"underline\">JuLuComputing</span></a></span>\n\n\t</span></span>\
          \ !</p>\n"
        raw: Thanks @JuLuComputing !
        updatedAt: '2024-01-10T18:47:36.915Z'
      numEdits: 0
      reactions: []
    id: 659ee64824e3a2aa72394eda
    type: comment
  author: ybelkada
  content: Thanks @JuLuComputing !
  created_at: 2024-01-10 18:47:36+00:00
  edited: false
  hidden: false
  id: 659ee64824e3a2aa72394eda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2024-01-10T19:29:40.000Z'
    data:
      edited: false
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9453559517860413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: "<p>You bet, peeps! \U0001F601  I'm glad to help the community.  Let\
          \ me know if there are any questions on the script.</p>\n"
        raw: "You bet, peeps! \U0001F601  I'm glad to help the community.  Let me\
          \ know if there are any questions on the script."
        updatedAt: '2024-01-10T19:29:40.182Z'
      numEdits: 0
      reactions: []
    id: 659ef024ab4293c46d44c7fc
    type: comment
  author: JuLuComputing
  content: "You bet, peeps! \U0001F601  I'm glad to help the community.  Let me know\
    \ if there are any questions on the script."
  created_at: 2024-01-10 19:29:40+00:00
  edited: false
  hidden: false
  id: 659ef024ab4293c46d44c7fc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: google/switch-base-256
repo_type: model
status: open
target_branch: null
title: Example script for using this model
