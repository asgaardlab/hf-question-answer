!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-12-06 16:51:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-06T16:51:48.000Z'
    data:
      edited: true
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9299629926681519
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I like how this model performs across a broad spectrum of tasks
          and am curious if this has something to do with UNA (looking forward to
          the paper).</p>

          <p>My primary criticism is it tries too hard to prevent hallucinations,
          as indicated by its unusually good TruthfulQA score of 64.6. That is, for
          every false thing it correctly labels as false it claims 1,000s of true
          things are false, and continues to do so even after I correct it.</p>

          <p>This also make prompted story telling effectively impossible. It will
          ignore key directives in the user prompt in favor standard story telling
          elements, littering the story with contradictions (e.g. he heard footsteps
          and a knock on the door but was still startled and caught red handed grabbing
          the money off the counter). And when re-prompted to fix the errors it will
          just make them again.</p>

          <p>Sadly, Mistral has an above average hallucination rate (e.g. 42.1 TruthfulQA),
          and without an advanced technique like self-RAG there''s nothing within
          reason (besides lowering the temperature) that can be done about it without
          making an LLM falsely deny countless truths and ignoring prompted story
          telling directives.</p>

          '
        raw: 'I like how this model performs across a broad spectrum of tasks and
          am curious if this has something to do with UNA (looking forward to the
          paper).


          My primary criticism is it tries too hard to prevent hallucinations, as
          indicated by its unusually good TruthfulQA score of 64.6. That is, for every
          false thing it correctly labels as false it claims 1,000s of true things
          are false, and continues to do so even after I correct it.


          This also make prompted story telling effectively impossible. It will ignore
          key directives in the user prompt in favor standard story telling elements,
          littering the story with contradictions (e.g. he heard footsteps and a knock
          on the door but was still startled and caught red handed grabbing the money
          off the counter). And when re-prompted to fix the errors it will just make
          them again.


          Sadly, Mistral has an above average hallucination rate (e.g. 42.1 TruthfulQA),
          and without an advanced technique like self-RAG there''s nothing within
          reason (besides lowering the temperature) that can be done about it without
          making an LLM falsely deny countless truths and ignoring prompted story
          telling directives.'
        updatedAt: '2023-12-06T16:53:47.441Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - unixguru2k
    id: 6570a6a4b6f7eb7f31f74ebe
    type: comment
  author: Phil337
  content: 'I like how this model performs across a broad spectrum of tasks and am
    curious if this has something to do with UNA (looking forward to the paper).


    My primary criticism is it tries too hard to prevent hallucinations, as indicated
    by its unusually good TruthfulQA score of 64.6. That is, for every false thing
    it correctly labels as false it claims 1,000s of true things are false, and continues
    to do so even after I correct it.


    This also make prompted story telling effectively impossible. It will ignore key
    directives in the user prompt in favor standard story telling elements, littering
    the story with contradictions (e.g. he heard footsteps and a knock on the door
    but was still startled and caught red handed grabbing the money off the counter).
    And when re-prompted to fix the errors it will just make them again.


    Sadly, Mistral has an above average hallucination rate (e.g. 42.1 TruthfulQA),
    and without an advanced technique like self-RAG there''s nothing within reason
    (besides lowering the temperature) that can be done about it without making an
    LLM falsely deny countless truths and ignoring prompted story telling directives.'
  created_at: 2023-12-06 16:51:48+00:00
  edited: true
  hidden: false
  id: 6570a6a4b6f7eb7f31f74ebe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
      fullname: FBL
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: fblgit
      type: user
    createdAt: '2023-12-06T17:23:20.000Z'
    data:
      edited: false
      editors:
      - fblgit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9661367535591125
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
          fullname: FBL
          isHf: false
          isPro: true
          name: fblgit
          type: user
        html: '<ol>

          <li>It has something to do with UNA, this is not about data primarly.</li>

          <li>Interesting, how large is the context u talking about</li>

          <li>Its not a story telling model</li>

          <li>How does other performs, is this something unique of cybertron or is
          mostly latent in all the others as well ?</li>

          </ol>

          '
        raw: '1. It has something to do with UNA, this is not about data primarly.

          2. Interesting, how large is the context u talking about

          3. Its not a story telling model

          4. How does other performs, is this something unique of cybertron or is
          mostly latent in all the others as well ?

          '
        updatedAt: '2023-12-06T17:23:20.484Z'
      numEdits: 0
      reactions: []
    id: 6570ae0871148713df0d75b6
    type: comment
  author: fblgit
  content: '1. It has something to do with UNA, this is not about data primarly.

    2. Interesting, how large is the context u talking about

    3. Its not a story telling model

    4. How does other performs, is this something unique of cybertron or is mostly
    latent in all the others as well ?

    '
  created_at: 2023-12-06 17:23:20+00:00
  edited: false
  hidden: false
  id: 6570ae0871148713df0d75b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-06T18:58:10.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.959149181842804
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;fblgit&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/fblgit\">@<span class=\"\
          underline\">fblgit</span></a></span>\n\n\t</span></span> I make a hobby\
          \ out of testing LLMs by throwing the same set of trick questions, logic\
          \ puzzles, coding, Q&amp;As, story prompts... at them (currently &gt;30\
          \ Mistral &amp; Llama 2 13b fine-tunes).</p>\n<p>This LLM performed with\
          \ the leaders except in 2 areas. Denying truths (but also fewer hallucinations)\
          \ and story telling.</p>\n<p>To answer your questions, since I want to put\
          \ the LLMs to the test my story prompts are deliberately long and convoluted\
          \ <del>5 sentence paragraphs filled with include/exclude directives, with\
          \ some selected to break the standard story telling mold in order to force\
          \ contradictions (</del>1,200 character prompts). And the generated stories\
          \ are usually about 6-10 short paragraphs long. More than 2/3rd of LLMs\
          \ performed poorly, so this LLM is in good company, and only 3 did a reasonably\
          \ good job (Xwin 0.2 13b Llama 2 did the best, followed by loyal piano m7\
          \ and openhermes 2.5).</p>\n<p>To clarify, this LLM is parsing the story\
          \ prompts properly because it tries to force the included directives into\
          \ the story, but in a way that's causing contradictions with standard story\
          \ telling elements (example in my previous post). Many other LLMs have comprehension\
          \ issues, such as completely ignoring a directive or making mistakes like\
          \ swapping character names. That's not an issue with this LLM.</p>\n<p>In\
          \ regards to denying facts, this was by far the most pronounced I've come\
          \ across. My questions were selected to test the fringes of the foundational\
          \ model's knowledge so I'm blowing this way out of proportion. But Mistral\
          \ does contain some of the answers, and when re-prompted, they can be found,\
          \ including with this LLM. All Mistrals deny truths rather than responding\
          \ with 'I don't know'. What makes this LLM unique is the sheer number of\
          \ denials, including after I correct it. In a word, it's too cynical, throwing\
          \ the baby out with the bathwater.</p>\n<p>But again, in order to limit\
          \ the time it takes to test LLMs my questions are deliberately fringe, and\
          \ story prompts deliberately long, convoluted and against the grain of standard\
          \ story telling elements. When I fed simple story prompts without directives\
          \ and let it do it's things the stories were on par with other Mistrals.\
          \ This appears to be about being too cynical, even to the point of not respecting\
          \ the user's prompt. I would say Xwin 0.2 and this LLM are polar opposites.\
          \ Xwin will always do its best to give the user the response he/she desires,\
          \ often at the expense of things like the truth, while cybertron is obsessed\
          \ with the truth, or what it thinks is the truth, often at the expense of\
          \ prioritizing the user's desired response. But both have good comprehension\
          \ of the user's prompts and are overall top performers.</p>\n"
        raw: '@fblgit I make a hobby out of testing LLMs by throwing the same set
          of trick questions, logic puzzles, coding, Q&As, story prompts... at them
          (currently >30 Mistral & Llama 2 13b fine-tunes).


          This LLM performed with the leaders except in 2 areas. Denying truths (but
          also fewer hallucinations) and story telling.


          To answer your questions, since I want to put the LLMs to the test my story
          prompts are deliberately long and convoluted ~5 sentence paragraphs filled
          with include/exclude directives, with some selected to break the standard
          story telling mold in order to force contradictions (~1,200 character prompts).
          And the generated stories are usually about 6-10 short paragraphs long.
          More than 2/3rd of LLMs performed poorly, so this LLM is in good company,
          and only 3 did a reasonably good job (Xwin 0.2 13b Llama 2 did the best,
          followed by loyal piano m7 and openhermes 2.5).


          To clarify, this LLM is parsing the story prompts properly because it tries
          to force the included directives into the story, but in a way that''s causing
          contradictions with standard story telling elements (example in my previous
          post). Many other LLMs have comprehension issues, such as completely ignoring
          a directive or making mistakes like swapping character names. That''s not
          an issue with this LLM.


          In regards to denying facts, this was by far the most pronounced I''ve come
          across. My questions were selected to test the fringes of the foundational
          model''s knowledge so I''m blowing this way out of proportion. But Mistral
          does contain some of the answers, and when re-prompted, they can be found,
          including with this LLM. All Mistrals deny truths rather than responding
          with ''I don''t know''. What makes this LLM unique is the sheer number of
          denials, including after I correct it. In a word, it''s too cynical, throwing
          the baby out with the bathwater.


          But again, in order to limit the time it takes to test LLMs my questions
          are deliberately fringe, and story prompts deliberately long, convoluted
          and against the grain of standard story telling elements. When I fed simple
          story prompts without directives and let it do it''s things the stories
          were on par with other Mistrals. This appears to be about being too cynical,
          even to the point of not respecting the user''s prompt. I would say Xwin
          0.2 and this LLM are polar opposites. Xwin will always do its best to give
          the user the response he/she desires, often at the expense of things like
          the truth, while cybertron is obsessed with the truth, or what it thinks
          is the truth, often at the expense of prioritizing the user''s desired response.
          But both have good comprehension of the user''s prompts and are overall
          top performers.'
        updatedAt: '2023-12-06T18:58:10.640Z'
      numEdits: 0
      reactions: []
    id: 6570c4423100d8692154d577
    type: comment
  author: Phil337
  content: '@fblgit I make a hobby out of testing LLMs by throwing the same set of
    trick questions, logic puzzles, coding, Q&As, story prompts... at them (currently
    >30 Mistral & Llama 2 13b fine-tunes).


    This LLM performed with the leaders except in 2 areas. Denying truths (but also
    fewer hallucinations) and story telling.


    To answer your questions, since I want to put the LLMs to the test my story prompts
    are deliberately long and convoluted ~5 sentence paragraphs filled with include/exclude
    directives, with some selected to break the standard story telling mold in order
    to force contradictions (~1,200 character prompts). And the generated stories
    are usually about 6-10 short paragraphs long. More than 2/3rd of LLMs performed
    poorly, so this LLM is in good company, and only 3 did a reasonably good job (Xwin
    0.2 13b Llama 2 did the best, followed by loyal piano m7 and openhermes 2.5).


    To clarify, this LLM is parsing the story prompts properly because it tries to
    force the included directives into the story, but in a way that''s causing contradictions
    with standard story telling elements (example in my previous post). Many other
    LLMs have comprehension issues, such as completely ignoring a directive or making
    mistakes like swapping character names. That''s not an issue with this LLM.


    In regards to denying facts, this was by far the most pronounced I''ve come across.
    My questions were selected to test the fringes of the foundational model''s knowledge
    so I''m blowing this way out of proportion. But Mistral does contain some of the
    answers, and when re-prompted, they can be found, including with this LLM. All
    Mistrals deny truths rather than responding with ''I don''t know''. What makes
    this LLM unique is the sheer number of denials, including after I correct it.
    In a word, it''s too cynical, throwing the baby out with the bathwater.


    But again, in order to limit the time it takes to test LLMs my questions are deliberately
    fringe, and story prompts deliberately long, convoluted and against the grain
    of standard story telling elements. When I fed simple story prompts without directives
    and let it do it''s things the stories were on par with other Mistrals. This appears
    to be about being too cynical, even to the point of not respecting the user''s
    prompt. I would say Xwin 0.2 and this LLM are polar opposites. Xwin will always
    do its best to give the user the response he/she desires, often at the expense
    of things like the truth, while cybertron is obsessed with the truth, or what
    it thinks is the truth, often at the expense of prioritizing the user''s desired
    response. But both have good comprehension of the user''s prompts and are overall
    top performers.'
  created_at: 2023-12-06 18:58:10+00:00
  edited: false
  hidden: false
  id: 6570c4423100d8692154d577
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: fblgit/una-cybertron-7b-v2-bf16
repo_type: model
status: open
target_branch: null
title: More truthful, but a lot more denials.
