!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yaop
conflicting_files: null
created_at: 2022-09-13 00:46:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f76ef092f49e4237a8f449a485312cdd.svg
      fullname: dcdethan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yaop
      type: user
    createdAt: '2022-09-13T01:46:19.000Z'
    data:
      edited: false
      editors:
      - yaop
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f76ef092f49e4237a8f449a485312cdd.svg
          fullname: dcdethan
          isHf: false
          isPro: false
          name: yaop
          type: user
        html: '<p>Hello, thank you for your great job ,now ,i have a question,why
          ''microsoft/trocr-small-printed'' don''t have vocab.json?where is it?</p>

          '
        raw: Hello, thank you for your great job ,now ,i have a question,why 'microsoft/trocr-small-printed'
          don't have vocab.json?where is it?
        updatedAt: '2022-09-13T01:46:19.258Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wmax
    id: 631fe0eb5ba8c026340f66be
    type: comment
  author: yaop
  content: Hello, thank you for your great job ,now ,i have a question,why 'microsoft/trocr-small-printed'
    don't have vocab.json?where is it?
  created_at: 2022-09-13 00:46:19+00:00
  edited: false
  hidden: false
  id: 631fe0eb5ba8c026340f66be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c178cc2ea962a7045ce3a0cc66cd77b.svg
      fullname: Wel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wmax
      type: user
    createdAt: '2022-09-14T10:43:08.000Z'
    data:
      edited: false
      editors:
      - wmax
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c178cc2ea962a7045ce3a0cc66cd77b.svg
          fullname: Wel
          isHf: false
          isPro: false
          name: wmax
          type: user
        html: '<p>Hey yaop, I had the same problem. After checking the <a rel="nofollow"
          href="https://github.com/huggingface/transformers/issues/15289">issue</a>  I
          installed the <a rel="nofollow" href="https://github.com/google/sentencepiece">SentencePiece</a>
          library:</p>

          <pre><code class="language-bash">pip install sentencepiece

          </code></pre>

          <p>and the problem disappeared. I''m guessing the  *sentencepiece.bpe.model
          * file is the representative vocab.</p>

          '
        raw: 'Hey yaop, I had the same problem. After checking the [issue](https://github.com/huggingface/transformers/issues/15289)  I
          installed the [SentencePiece](https://github.com/google/sentencepiece) library:

          ```bash

          pip install sentencepiece

          ```

          and the problem disappeared. I''m guessing the  *sentencepiece.bpe.model
          * file is the representative vocab.'
        updatedAt: '2022-09-14T10:43:08.301Z'
      numEdits: 0
      reactions: []
    id: 6321b03c95d6f717a8c0f323
    type: comment
  author: wmax
  content: 'Hey yaop, I had the same problem. After checking the [issue](https://github.com/huggingface/transformers/issues/15289)  I
    installed the [SentencePiece](https://github.com/google/sentencepiece) library:

    ```bash

    pip install sentencepiece

    ```

    and the problem disappeared. I''m guessing the  *sentencepiece.bpe.model * file
    is the representative vocab.'
  created_at: 2022-09-14 09:43:08+00:00
  edited: false
  hidden: false
  id: 6321b03c95d6f717a8c0f323
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-09-14T17:22:49.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>It looks like the TrOCR authors used a different tokenization algorithm
          for the small variants (SentencePiece instead of Byte Pair Encoding).</p>

          <p>Hence, you indeed need the Sentence Piece library. You can load the tokenizer
          as follows:</p>

          <pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer

          &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("microsoft/trocr-small-printed")

          &gt;&gt;&gt; type(tokenizer)

          &lt;class ''transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast''&gt;

          </code></pre>

          '
        raw: 'It looks like the TrOCR authors used a different tokenization algorithm
          for the small variants (SentencePiece instead of Byte Pair Encoding).


          Hence, you indeed need the Sentence Piece library. You can load the tokenizer
          as follows:


          ```

          >>> from transformers import AutoTokenizer

          >>> tokenizer = AutoTokenizer.from_pretrained("microsoft/trocr-small-printed")

          >>> type(tokenizer)

          <class ''transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast''>

          ```'
        updatedAt: '2022-09-14T17:22:49.873Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yaop
    id: 63220de9e4399dd613999a30
    type: comment
  author: nielsr
  content: 'It looks like the TrOCR authors used a different tokenization algorithm
    for the small variants (SentencePiece instead of Byte Pair Encoding).


    Hence, you indeed need the Sentence Piece library. You can load the tokenizer
    as follows:


    ```

    >>> from transformers import AutoTokenizer

    >>> tokenizer = AutoTokenizer.from_pretrained("microsoft/trocr-small-printed")

    >>> type(tokenizer)

    <class ''transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast''>

    ```'
  created_at: 2022-09-14 16:22:49+00:00
  edited: false
  hidden: false
  id: 63220de9e4399dd613999a30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f76ef092f49e4237a8f449a485312cdd.svg
      fullname: dcdethan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yaop
      type: user
    createdAt: '2022-10-14T05:54:53.000Z'
    data:
      edited: false
      editors:
      - yaop
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f76ef092f49e4237a8f449a485312cdd.svg
          fullname: dcdethan
          isHf: false
          isPro: false
          name: yaop
          type: user
        html: '<blockquote>

          <p>It looks like the TrOCR authors used a different tokenization algorithm
          for the small variants (SentencePiece instead of Byte Pair Encoding).</p>

          <p>Hence, you indeed need the Sentence Piece library. You can load the tokenizer
          as follows:</p>

          <pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer

          &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("microsoft/trocr-small-printed")

          &gt;&gt;&gt; type(tokenizer)

          &lt;class ''transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast''&gt;

          </code></pre>

          </blockquote>

          <p>Thank you,it really helped me.</p>

          '
        raw: "> It looks like the TrOCR authors used a different tokenization algorithm\
          \ for the small variants (SentencePiece instead of Byte Pair Encoding).\n\
          > \n> Hence, you indeed need the Sentence Piece library. You can load the\
          \ tokenizer as follows:\n> \n> ```\n> >>> from transformers import AutoTokenizer\n\
          > >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/trocr-small-printed\"\
          )\n> >>> type(tokenizer)\n> <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>\n\
          > ```\n\nThank you,it really helped me."
        updatedAt: '2022-10-14T05:54:53.291Z'
      numEdits: 0
      reactions: []
    id: 6348f9ad1ca70ea12509680f
    type: comment
  author: yaop
  content: "> It looks like the TrOCR authors used a different tokenization algorithm\
    \ for the small variants (SentencePiece instead of Byte Pair Encoding).\n> \n\
    > Hence, you indeed need the Sentence Piece library. You can load the tokenizer\
    \ as follows:\n> \n> ```\n> >>> from transformers import AutoTokenizer\n> >>>\
    \ tokenizer = AutoTokenizer.from_pretrained(\"microsoft/trocr-small-printed\"\
    )\n> >>> type(tokenizer)\n> <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>\n\
    > ```\n\nThank you,it really helped me."
  created_at: 2022-10-14 04:54:53+00:00
  edited: false
  hidden: false
  id: 6348f9ad1ca70ea12509680f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f76ef092f49e4237a8f449a485312cdd.svg
      fullname: dcdethan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yaop
      type: user
    createdAt: '2022-10-14T05:56:03.000Z'
    data:
      edited: false
      editors:
      - yaop
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f76ef092f49e4237a8f449a485312cdd.svg
          fullname: dcdethan
          isHf: false
          isPro: false
          name: yaop
          type: user
        html: '<blockquote>

          <p>Hey yaop, I had the same problem. After checking the <a rel="nofollow"
          href="https://github.com/huggingface/transformers/issues/15289">issue</a>  I
          installed the <a rel="nofollow" href="https://github.com/google/sentencepiece">SentencePiece</a>
          library:</p>

          <pre><code class="language-bash">pip install sentencepiece

          </code></pre>

          <p>and the problem disappeared. I''m guessing the  *sentencepiece.bpe.model
          * file is the representative vocab.</p>

          </blockquote>

          <p>Thanks a lot , i will try it.</p>

          '
        raw: '> Hey yaop, I had the same problem. After checking the [issue](https://github.com/huggingface/transformers/issues/15289)  I
          installed the [SentencePiece](https://github.com/google/sentencepiece) library:

          > ```bash

          > pip install sentencepiece

          > ```

          > and the problem disappeared. I''m guessing the  *sentencepiece.bpe.model
          * file is the representative vocab.


          Thanks a lot , i will try it.'
        updatedAt: '2022-10-14T05:56:03.893Z'
      numEdits: 0
      reactions: []
    id: 6348f9f35b1a5329cc2a78e3
    type: comment
  author: yaop
  content: '> Hey yaop, I had the same problem. After checking the [issue](https://github.com/huggingface/transformers/issues/15289)  I
    installed the [SentencePiece](https://github.com/google/sentencepiece) library:

    > ```bash

    > pip install sentencepiece

    > ```

    > and the problem disappeared. I''m guessing the  *sentencepiece.bpe.model * file
    is the representative vocab.


    Thanks a lot , i will try it.'
  created_at: 2022-10-14 04:56:03+00:00
  edited: false
  hidden: false
  id: 6348f9f35b1a5329cc2a78e3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: microsoft/trocr-small-printed
repo_type: model
status: open
target_branch: null
title: why 'microsoft/trocr-small-printed' don't have vocab.json?
