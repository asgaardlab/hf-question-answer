!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JesusUned
conflicting_files: null
created_at: 2024-01-23 08:25:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b78bbc219a9a4fbdc0aa38b86c522887.svg
      fullname: F
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JesusUned
      type: user
    createdAt: '2024-01-23T08:25:37.000Z'
    data:
      edited: true
      editors:
      - JesusUned
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6063666939735413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b78bbc219a9a4fbdc0aa38b86c522887.svg
          fullname: F
          isHf: false
          isPro: false
          name: JesusUned
          type: user
        html: "<p>I'm trying to use the model with 4x NVIDIA RTX A5000 25GB RAM each.\
          \ When i try to load the model i get this error (i get this error when i\
          \ use auto and sequential in device_map variable) </p>\n<p>Thanks!!</p>\n\
          <pre><code>      1 gpu='auto'\n----&gt; 2 model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct',\
          \ device_map=gpu).cuda()\n      3 tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n\
          \      5 max_length = 4096\n\nFile ~/llama/lib/python3.11/site-packages/accelerate/big_modeling.py:416,\
          \ in dispatch_model.&lt;locals&gt;.add_warning.&lt;locals&gt;.wrapper(*args,\
          \ **kwargs)\n    414     if param.device == torch.device(\"meta\"):\n  \
          \  415         raise RuntimeError(\"You can't move a model that has some\
          \ modules offloaded to cpu or disk.\")\n--&gt; 416 return fn(*args, **kwargs)\n\
          \nFile ~/llama/lib/python3.11/site-packages/transformers/modeling_utils.py:2243,\
          \ in PreTrainedModel.cuda(self, *args, **kwargs)\n   2238     raise ValueError(\n\
          \   2239         \"Calling `cuda()` is not supported for `4-bit` or `8-bit`\
          \ quantized models. Please use the model as it is, since the\"\n   2240\
          \         \" model has already been set to the correct devices and casted\
          \ to the correct `dtype`.\"\n   2241     )\n   2242 else:\n-&gt; 2243  \
          \   return super().cuda(*args, **kwargs)\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:918,\
          \ in Module.cuda(self, device)\n    901 def cuda(self: T, device: Optional[Union[int,\
          \ device]] = None) -&gt; T:\n    902     r\"\"\"Moves all model parameters\
          \ and buffers to the GPU.\n    903 \n    904     This also makes associated\
          \ parameters and buffers different objects. So\n   (...)\n    916      \
          \   Module: self\n    917     \"\"\"\n--&gt; 918     return self._apply(lambda\
          \ t: t.cuda(device))\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
          \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809    \
          \ for module in self.children():\n--&gt; 810         module._apply(fn)\n\
          \    812 def compute_should_use_set_data(tensor, tensor_applied):\n    813\
          \     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    814         # If the new tensor has compatible tensor type as the existing\
          \ tensor,\n    815         # the current behavior is to change the tensor\
          \ in-place using `.data =`,\n   (...)\n    820         # global flag to\
          \ let the user control whether they want the future\n    821         # behavior\
          \ of overwriting the existing tensor or not.\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
          \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809    \
          \ for module in self.children():\n--&gt; 810         module._apply(fn)\n\
          \    812 def compute_should_use_set_data(tensor, tensor_applied):\n    813\
          \     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    814         # If the new tensor has compatible tensor type as the existing\
          \ tensor,\n    815         # the current behavior is to change the tensor\
          \ in-place using `.data =`,\n   (...)\n    820         # global flag to\
          \ let the user control whether they want the future\n    821         # behavior\
          \ of overwriting the existing tensor or not.\n\n    [... skipping similar\
          \ frames: Module._apply at line 810 (1 times)]\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
          \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809    \
          \ for module in self.children():\n--&gt; 810         module._apply(fn)\n\
          \    812 def compute_should_use_set_data(tensor, tensor_applied):\n    813\
          \     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    814         # If the new tensor has compatible tensor type as the existing\
          \ tensor,\n    815         # the current behavior is to change the tensor\
          \ in-place using `.data =`,\n   (...)\n    820         # global flag to\
          \ let the user control whether they want the future\n    821         # behavior\
          \ of overwriting the existing tensor or not.\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:833,\
          \ in Module._apply(self, fn, recurse)\n    829 # Tensors stored in modules\
          \ are graph leaves, and we don't want to\n    830 # track autograd history\
          \ of `param_applied`, so we have to use\n    831 # `with torch.no_grad():`\n\
          \    832 with torch.no_grad():\n--&gt; 833     param_applied = fn(param)\n\
          \    834 should_use_set_data = compute_should_use_set_data(param, param_applied)\n\
          \    835 if should_use_set_data:\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:918,\
          \ in Module.cuda.&lt;locals&gt;.&lt;lambda&gt;(t)\n    901 def cuda(self:\
          \ T, device: Optional[Union[int, device]] = None) -&gt; T:\n    902    \
          \ r\"\"\"Moves all model parameters and buffers to the GPU.\n    903 \n\
          \    904     This also makes associated parameters and buffers different\
          \ objects. So\n   (...)\n    916         Module: self\n    917     \"\"\"\
          \n--&gt; 918     return self._apply(lambda t: t.cuda(device))\n\nOutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty\
          \ of 23.68 GiB of which 1.27 GiB is free. Including non-PyTorch memory,\
          \ this process has 22.41 GiB memory in use. Of the allocated memory 22.21\
          \ GiB is allocated by PyTorch, and 1.21 MiB is reserved by PyTorch but unallocated.\
          \ If reserved but unallocated memory is large try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON\n\
          </code></pre>\n"
        raw: "I'm trying to use the model with 4x NVIDIA RTX A5000 25GB RAM each.\
          \ When i try to load the model i get this error (i get this error when i\
          \ use auto and sequential in device_map variable) \n\nThanks!!\n\n```\n\
          \      1 gpu='auto'\n----> 2 model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct',\
          \ device_map=gpu).cuda()\n      3 tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n\
          \      5 max_length = 4096\n\nFile ~/llama/lib/python3.11/site-packages/accelerate/big_modeling.py:416,\
          \ in dispatch_model.<locals>.add_warning.<locals>.wrapper(*args, **kwargs)\n\
          \    414     if param.device == torch.device(\"meta\"):\n    415       \
          \  raise RuntimeError(\"You can't move a model that has some modules offloaded\
          \ to cpu or disk.\")\n--> 416 return fn(*args, **kwargs)\n\nFile ~/llama/lib/python3.11/site-packages/transformers/modeling_utils.py:2243,\
          \ in PreTrainedModel.cuda(self, *args, **kwargs)\n   2238     raise ValueError(\n\
          \   2239         \"Calling `cuda()` is not supported for `4-bit` or `8-bit`\
          \ quantized models. Please use the model as it is, since the\"\n   2240\
          \         \" model has already been set to the correct devices and casted\
          \ to the correct `dtype`.\"\n   2241     )\n   2242 else:\n-> 2243     return\
          \ super().cuda(*args, **kwargs)\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:918,\
          \ in Module.cuda(self, device)\n    901 def cuda(self: T, device: Optional[Union[int,\
          \ device]] = None) -> T:\n    902     r\"\"\"Moves all model parameters\
          \ and buffers to the GPU.\n    903 \n    904     This also makes associated\
          \ parameters and buffers different objects. So\n   (...)\n    916      \
          \   Module: self\n    917     \"\"\"\n--> 918     return self._apply(lambda\
          \ t: t.cuda(device))\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
          \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809    \
          \ for module in self.children():\n--> 810         module._apply(fn)\n  \
          \  812 def compute_should_use_set_data(tensor, tensor_applied):\n    813\
          \     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    814         # If the new tensor has compatible tensor type as the existing\
          \ tensor,\n    815         # the current behavior is to change the tensor\
          \ in-place using `.data =`,\n   (...)\n    820         # global flag to\
          \ let the user control whether they want the future\n    821         # behavior\
          \ of overwriting the existing tensor or not.\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
          \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809    \
          \ for module in self.children():\n--> 810         module._apply(fn)\n  \
          \  812 def compute_should_use_set_data(tensor, tensor_applied):\n    813\
          \     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    814         # If the new tensor has compatible tensor type as the existing\
          \ tensor,\n    815         # the current behavior is to change the tensor\
          \ in-place using `.data =`,\n   (...)\n    820         # global flag to\
          \ let the user control whether they want the future\n    821         # behavior\
          \ of overwriting the existing tensor or not.\n\n    [... skipping similar\
          \ frames: Module._apply at line 810 (1 times)]\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
          \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809    \
          \ for module in self.children():\n--> 810         module._apply(fn)\n  \
          \  812 def compute_should_use_set_data(tensor, tensor_applied):\n    813\
          \     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    814         # If the new tensor has compatible tensor type as the existing\
          \ tensor,\n    815         # the current behavior is to change the tensor\
          \ in-place using `.data =`,\n   (...)\n    820         # global flag to\
          \ let the user control whether they want the future\n    821         # behavior\
          \ of overwriting the existing tensor or not.\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:833,\
          \ in Module._apply(self, fn, recurse)\n    829 # Tensors stored in modules\
          \ are graph leaves, and we don't want to\n    830 # track autograd history\
          \ of `param_applied`, so we have to use\n    831 # `with torch.no_grad():`\n\
          \    832 with torch.no_grad():\n--> 833     param_applied = fn(param)\n\
          \    834 should_use_set_data = compute_should_use_set_data(param, param_applied)\n\
          \    835 if should_use_set_data:\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:918,\
          \ in Module.cuda.<locals>.<lambda>(t)\n    901 def cuda(self: T, device:\
          \ Optional[Union[int, device]] = None) -> T:\n    902     r\"\"\"Moves all\
          \ model parameters and buffers to the GPU.\n    903 \n    904     This also\
          \ makes associated parameters and buffers different objects. So\n   (...)\n\
          \    916         Module: self\n    917     \"\"\"\n--> 918     return self._apply(lambda\
          \ t: t.cuda(device))\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 224.00 MiB. GPU 0 has a total capacty of 23.68 GiB of which 1.27 GiB is\
          \ free. Including non-PyTorch memory, this process has 22.41 GiB memory\
          \ in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and\
          \ 1.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON\n\
          ```"
        updatedAt: '2024-01-23T08:27:16.148Z'
      numEdits: 1
      reactions: []
    id: 65af78015f62b7644416c06b
    type: comment
  author: JesusUned
  content: "I'm trying to use the model with 4x NVIDIA RTX A5000 25GB RAM each. When\
    \ i try to load the model i get this error (i get this error when i use auto and\
    \ sequential in device_map variable) \n\nThanks!!\n\n```\n      1 gpu='auto'\n\
    ----> 2 model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct', device_map=gpu).cuda()\n\
    \      3 tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n\
    \      5 max_length = 4096\n\nFile ~/llama/lib/python3.11/site-packages/accelerate/big_modeling.py:416,\
    \ in dispatch_model.<locals>.add_warning.<locals>.wrapper(*args, **kwargs)\n \
    \   414     if param.device == torch.device(\"meta\"):\n    415         raise\
    \ RuntimeError(\"You can't move a model that has some modules offloaded to cpu\
    \ or disk.\")\n--> 416 return fn(*args, **kwargs)\n\nFile ~/llama/lib/python3.11/site-packages/transformers/modeling_utils.py:2243,\
    \ in PreTrainedModel.cuda(self, *args, **kwargs)\n   2238     raise ValueError(\n\
    \   2239         \"Calling `cuda()` is not supported for `4-bit` or `8-bit` quantized\
    \ models. Please use the model as it is, since the\"\n   2240         \" model\
    \ has already been set to the correct devices and casted to the correct `dtype`.\"\
    \n   2241     )\n   2242 else:\n-> 2243     return super().cuda(*args, **kwargs)\n\
    \nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:918, in\
    \ Module.cuda(self, device)\n    901 def cuda(self: T, device: Optional[Union[int,\
    \ device]] = None) -> T:\n    902     r\"\"\"Moves all model parameters and buffers\
    \ to the GPU.\n    903 \n    904     This also makes associated parameters and\
    \ buffers different objects. So\n   (...)\n    916         Module: self\n    917\
    \     \"\"\"\n--> 918     return self._apply(lambda t: t.cuda(device))\n\nFile\
    \ ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810, in Module._apply(self,\
    \ fn, recurse)\n    808 if recurse:\n    809     for module in self.children():\n\
    --> 810         module._apply(fn)\n    812 def compute_should_use_set_data(tensor,\
    \ tensor_applied):\n    813     if torch._has_compatible_shallow_copy_type(tensor,\
    \ tensor_applied):\n    814         # If the new tensor has compatible tensor\
    \ type as the existing tensor,\n    815         # the current behavior is to change\
    \ the tensor in-place using `.data =`,\n   (...)\n    820         # global flag\
    \ to let the user control whether they want the future\n    821         # behavior\
    \ of overwriting the existing tensor or not.\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
    \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809     for module\
    \ in self.children():\n--> 810         module._apply(fn)\n    812 def compute_should_use_set_data(tensor,\
    \ tensor_applied):\n    813     if torch._has_compatible_shallow_copy_type(tensor,\
    \ tensor_applied):\n    814         # If the new tensor has compatible tensor\
    \ type as the existing tensor,\n    815         # the current behavior is to change\
    \ the tensor in-place using `.data =`,\n   (...)\n    820         # global flag\
    \ to let the user control whether they want the future\n    821         # behavior\
    \ of overwriting the existing tensor or not.\n\n    [... skipping similar frames:\
    \ Module._apply at line 810 (1 times)]\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:810,\
    \ in Module._apply(self, fn, recurse)\n    808 if recurse:\n    809     for module\
    \ in self.children():\n--> 810         module._apply(fn)\n    812 def compute_should_use_set_data(tensor,\
    \ tensor_applied):\n    813     if torch._has_compatible_shallow_copy_type(tensor,\
    \ tensor_applied):\n    814         # If the new tensor has compatible tensor\
    \ type as the existing tensor,\n    815         # the current behavior is to change\
    \ the tensor in-place using `.data =`,\n   (...)\n    820         # global flag\
    \ to let the user control whether they want the future\n    821         # behavior\
    \ of overwriting the existing tensor or not.\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:833,\
    \ in Module._apply(self, fn, recurse)\n    829 # Tensors stored in modules are\
    \ graph leaves, and we don't want to\n    830 # track autograd history of `param_applied`,\
    \ so we have to use\n    831 # `with torch.no_grad():`\n    832 with torch.no_grad():\n\
    --> 833     param_applied = fn(param)\n    834 should_use_set_data = compute_should_use_set_data(param,\
    \ param_applied)\n    835 if should_use_set_data:\n\nFile ~/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:918,\
    \ in Module.cuda.<locals>.<lambda>(t)\n    901 def cuda(self: T, device: Optional[Union[int,\
    \ device]] = None) -> T:\n    902     r\"\"\"Moves all model parameters and buffers\
    \ to the GPU.\n    903 \n    904     This also makes associated parameters and\
    \ buffers different objects. So\n   (...)\n    916         Module: self\n    917\
    \     \"\"\"\n--> 918     return self._apply(lambda t: t.cuda(device))\n\nOutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty\
    \ of 23.68 GiB of which 1.27 GiB is free. Including non-PyTorch memory, this process\
    \ has 22.41 GiB memory in use. Of the allocated memory 22.21 GiB is allocated\
    \ by PyTorch, and 1.21 MiB is reserved by PyTorch but unallocated. If reserved\
    \ but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON\n```"
  created_at: 2024-01-23 08:25:37+00:00
  edited: true
  hidden: false
  id: 65af78015f62b7644416c06b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2024-01-23T09:04:53.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46279215812683105
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: '<p>You may try to load with float16:<br><code>model = AutoModel.from_pretrained(''intfloat/e5-mistral-7b-instruct'',
          torch_dtype=torch.float16).cuda()</code> </p>

          <p>This will halve the GPU memory requirements.</p>

          '
        raw: "You may try to load with float16:\n`model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct',\
          \ torch_dtype=torch.float16).cuda()` \n\nThis will halve the GPU memory\
          \ requirements."
        updatedAt: '2024-01-23T09:04:53.264Z'
      numEdits: 0
      reactions: []
    id: 65af81353db2280ece8d3f1b
    type: comment
  author: intfloat
  content: "You may try to load with float16:\n`model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct',\
    \ torch_dtype=torch.float16).cuda()` \n\nThis will halve the GPU memory requirements."
  created_at: 2024-01-23 09:04:53+00:00
  edited: false
  hidden: false
  id: 65af81353db2280ece8d3f1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b78bbc219a9a4fbdc0aa38b86c522887.svg
      fullname: F
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JesusUned
      type: user
    createdAt: '2024-01-23T11:15:54.000Z'
    data:
      edited: false
      editors:
      - JesusUned
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9709983468055725
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b78bbc219a9a4fbdc0aa38b86c522887.svg
          fullname: F
          isHf: false
          isPro: false
          name: JesusUned
          type: user
        html: '<p>Thanks, problem solved!!</p>

          '
        raw: Thanks, problem solved!!
        updatedAt: '2024-01-23T11:15:54.041Z'
      numEdits: 0
      reactions: []
    id: 65af9feafd56b86c9ce8a4bc
    type: comment
  author: JesusUned
  content: Thanks, problem solved!!
  created_at: 2024-01-23 11:15:54+00:00
  edited: false
  hidden: false
  id: 65af9feafd56b86c9ce8a4bc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: intfloat/e5-mistral-7b-instruct
repo_type: model
status: open
target_branch: null
title: Error using multiple GPUs
