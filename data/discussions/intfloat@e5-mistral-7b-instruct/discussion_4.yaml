!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nalzok
conflicting_files: null
created_at: 2024-01-03 02:49:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58cbbd83f9c9945b633adcf1318109b0.svg
      fullname: Qingyao Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nalzok
      type: user
    createdAt: '2024-01-03T02:49:41.000Z'
    data:
      edited: true
      editors:
      - nalzok
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957449197769165
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58cbbd83f9c9945b633adcf1318109b0.svg
          fullname: Qingyao Sun
          isHf: false
          isPro: false
          name: nalzok
          type: user
        html: '<p>In the code example, you measure the similarity between two sentences
          with the inner product between normalized embeddings. However, in Section
          3.2 of your technical report, you wrote "In this paper, we adopt the temperature-scaled
          cosine similarity function as follows".</p>

          <p>I have two sets of documents in my use case, denoted A and B. The task
          is to find the top-k documents in B that are most semantically similar to
          those in A (by computing the average distance between each document in B
          and all documents in A). What do you recommend me to do to use as a distance
          measure between two embeddings, traditional cosine distance, or the temperature-scaled
          cosine similarity function?</p>

          <p>As a bonus question, do you think it would be better if I prepend the
          instructions to documents in A in addition to those in B, i.e. prepend them
          to documents in addition to queries? After all, this is a symmetric task,
          and I suppose some symmetry will help.</p>

          '
        raw: 'In the code example, you measure the similarity between two sentences
          with the inner product between normalized embeddings. However, in Section
          3.2 of your technical report, you wrote "In this paper, we adopt the temperature-scaled
          cosine similarity function as follows".


          I have two sets of documents in my use case, denoted A and B. The task is
          to find the top-k documents in B that are most semantically similar to those
          in A (by computing the average distance between each document in B and all
          documents in A). What do you recommend me to do to use as a distance measure
          between two embeddings, traditional cosine distance, or the temperature-scaled
          cosine similarity function?


          As a bonus question, do you think it would be better if I prepend the instructions
          to documents in A in addition to those in B, i.e. prepend them to documents
          in addition to queries? After all, this is a symmetric task, and I suppose
          some symmetry will help.'
        updatedAt: '2024-01-03T03:39:12.219Z'
      numEdits: 2
      reactions: []
    id: 6594cb4550d39af7f4b56f42
    type: comment
  author: nalzok
  content: 'In the code example, you measure the similarity between two sentences
    with the inner product between normalized embeddings. However, in Section 3.2
    of your technical report, you wrote "In this paper, we adopt the temperature-scaled
    cosine similarity function as follows".


    I have two sets of documents in my use case, denoted A and B. The task is to find
    the top-k documents in B that are most semantically similar to those in A (by
    computing the average distance between each document in B and all documents in
    A). What do you recommend me to do to use as a distance measure between two embeddings,
    traditional cosine distance, or the temperature-scaled cosine similarity function?


    As a bonus question, do you think it would be better if I prepend the instructions
    to documents in A in addition to those in B, i.e. prepend them to documents in
    addition to queries? After all, this is a symmetric task, and I suppose some symmetry
    will help.'
  created_at: 2024-01-03 02:49:41+00:00
  edited: true
  hidden: false
  id: 6594cb4550d39af7f4b56f42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/58cbbd83f9c9945b633adcf1318109b0.svg
      fullname: Qingyao Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nalzok
      type: user
    createdAt: '2024-01-03T02:49:56.000Z'
    data:
      from: Question about code sample
      to: Temperature-scaled cosine similarity function?
    id: 6594cb5489145cbc7c86c8e7
    type: title-change
  author: nalzok
  created_at: 2024-01-03 02:49:56+00:00
  id: 6594cb5489145cbc7c86c8e7
  new_title: Temperature-scaled cosine similarity function?
  old_title: Question about code sample
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2024-01-03T06:29:59.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7899274230003357
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nalzok&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nalzok\">@<span class=\"\
          underline\">nalzok</span></a></span>\n\n\t</span></span> ,</p>\n<p>The inner\
          \ product between normalized embeddings is mathematically equivalent to\
          \ cosine similarity function, so they are the same thing.</p>\n<p>About\
          \ the instructions, yes, we add instructions to both for symmetric tasks\
          \ such as STS (see <a rel=\"nofollow\" href=\"https://github.com/microsoft/unilm/blob/78b3a48de27c388a0212cfee49fd6dc470c9ecb5/e5/mteb_except_retrieval_eval.py#L68\"\
          >https://github.com/microsoft/unilm/blob/78b3a48de27c388a0212cfee49fd6dc470c9ecb5/e5/mteb_except_retrieval_eval.py#L68</a>).</p>\n"
        raw: 'Hi @nalzok ,


          The inner product between normalized embeddings is mathematically equivalent
          to cosine similarity function, so they are the same thing.


          About the instructions, yes, we add instructions to both for symmetric tasks
          such as STS (see https://github.com/microsoft/unilm/blob/78b3a48de27c388a0212cfee49fd6dc470c9ecb5/e5/mteb_except_retrieval_eval.py#L68).'
        updatedAt: '2024-01-03T06:29:59.920Z'
      numEdits: 0
      reactions: []
    id: 6594fee750d39af7f4c0c2f2
    type: comment
  author: intfloat
  content: 'Hi @nalzok ,


    The inner product between normalized embeddings is mathematically equivalent to
    cosine similarity function, so they are the same thing.


    About the instructions, yes, we add instructions to both for symmetric tasks such
    as STS (see https://github.com/microsoft/unilm/blob/78b3a48de27c388a0212cfee49fd6dc470c9ecb5/e5/mteb_except_retrieval_eval.py#L68).'
  created_at: 2024-01-03 06:29:59+00:00
  edited: false
  hidden: false
  id: 6594fee750d39af7f4c0c2f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58cbbd83f9c9945b633adcf1318109b0.svg
      fullname: Qingyao Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nalzok
      type: user
    createdAt: '2024-01-04T23:36:47.000Z'
    data:
      edited: false
      editors:
      - nalzok
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8803004622459412
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58cbbd83f9c9945b633adcf1318109b0.svg
          fullname: Qingyao Sun
          isHf: false
          isPro: false
          name: nalzok
          type: user
        html: '<p>Thanks for the reply! As a follow-up question: what''s the recommended
          way to process long texts, particularly those with multiple lines? I''m
          asking because your template <code>f''Instruct: {task_description}\nQuery:
          {query}''</code> includes a <code>\n</code> character. Would the newline
          characters in <code>query</code> interfere with the template?</p>

          '
        raw: 'Thanks for the reply! As a follow-up question: what''s the recommended
          way to process long texts, particularly those with multiple lines? I''m
          asking because your template `f''Instruct: {task_description}\nQuery: {query}''`
          includes a `\n` character. Would the newline characters in `query` interfere
          with the template?'
        updatedAt: '2024-01-04T23:36:47.388Z'
      numEdits: 0
      reactions: []
    id: 6597410f49dae38371e88878
    type: comment
  author: nalzok
  content: 'Thanks for the reply! As a follow-up question: what''s the recommended
    way to process long texts, particularly those with multiple lines? I''m asking
    because your template `f''Instruct: {task_description}\nQuery: {query}''` includes
    a `\n` character. Would the newline characters in `query` interfere with the template?'
  created_at: 2024-01-04 23:36:47+00:00
  edited: false
  hidden: false
  id: 6597410f49dae38371e88878
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2024-01-05T04:58:58.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9528865218162537
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: '<p>No, it is okay to include <code>\n</code> in either the query or
          documents.</p>

          '
        raw: No, it is okay to include `\n` in either the query or documents.
        updatedAt: '2024-01-05T04:58:58.944Z'
      numEdits: 0
      reactions: []
    id: 65978c92260bcd5fb53ad34f
    type: comment
  author: intfloat
  content: No, it is okay to include `\n` in either the query or documents.
  created_at: 2024-01-05 04:58:58+00:00
  edited: false
  hidden: false
  id: 65978c92260bcd5fb53ad34f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/58cbbd83f9c9945b633adcf1318109b0.svg
      fullname: Qingyao Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nalzok
      type: user
    createdAt: '2024-01-07T21:45:04.000Z'
    data:
      status: closed
    id: 659b1b6094cadc9cdc4ae3cc
    type: status-change
  author: nalzok
  created_at: 2024-01-07 21:45:04+00:00
  id: 659b1b6094cadc9cdc4ae3cc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: intfloat/e5-mistral-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Temperature-scaled cosine similarity function?
