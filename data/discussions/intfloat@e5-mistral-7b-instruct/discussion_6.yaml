!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SeanLee97
conflicting_files: null
created_at: 2024-01-04 14:32:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2024-01-04T14:32:14.000Z'
    data:
      edited: false
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6049171090126038
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: '<p>Excellent work!</p>

          <p>I''ve read the technical paper. It mentions the inclusion of LoRA adapters
          in all linear layers.<br>Based on my understanding, you specified the target_modules
          as follows: <code>[''v_proj'', ''q_proj'', ''down_proj'', ''k_proj'', ''gate_proj'',
          ''o_proj'', ''up_proj'']</code>.<br>Is my understanding correct?</p>

          <p>Could you share the detailed Lora config? Thx!</p>

          '
        raw: "Excellent work!\r\n\r\nI've read the technical paper. It mentions the\
          \ inclusion of LoRA adapters in all linear layers. \r\nBased on my understanding,\
          \ you specified the target_modules as follows: `['v_proj', 'q_proj', 'down_proj',\
          \ 'k_proj', 'gate_proj', 'o_proj', 'up_proj']`.\r\nIs my understanding correct?\r\
          \n\r\nCould you share the detailed Lora config? Thx!\r\n"
        updatedAt: '2024-01-04T14:32:14.287Z'
      numEdits: 0
      reactions: []
    id: 6596c16e558292bf9f0ce190
    type: comment
  author: SeanLee97
  content: "Excellent work!\r\n\r\nI've read the technical paper. It mentions the\
    \ inclusion of LoRA adapters in all linear layers. \r\nBased on my understanding,\
    \ you specified the target_modules as follows: `['v_proj', 'q_proj', 'down_proj',\
    \ 'k_proj', 'gate_proj', 'o_proj', 'up_proj']`.\r\nIs my understanding correct?\r\
    \n\r\nCould you share the detailed Lora config? Thx!\r\n"
  created_at: 2024-01-04 14:32:14+00:00
  edited: false
  hidden: false
  id: 6596c16e558292bf9f0ce190
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2024-01-05T05:04:59.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46713387966156006
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;SeanLee97&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SeanLee97\"\
          >@<span class=\"underline\">SeanLee97</span></a></span>\n\n\t</span></span>\
          \ !  I learn a lot from your AnglE paper!</p>\n<p>The released model contains\
          \ merged lora weights, I thought this would make it more convenient to use\
          \ without depending on PEFT library.</p>\n<p>Here is our LoRA config:</p>\n\
          <pre><code>{\n  \"auto_mapping\": null,\n  \"base_model_name_or_path\":\
          \ \"mistralai/Mistral-7B-v0.1\",\n  \"bias\": \"none\",\n  \"fan_in_fan_out\"\
          : false,\n  \"inference_mode\": true,\n  \"init_lora_weights\": true,\n\
          \  \"layers_pattern\": null,\n  \"layers_to_transform\": null,\n  \"lora_alpha\"\
          : 32,\n  \"lora_dropout\": 0.1,\n  \"modules_to_save\": null,\n  \"peft_type\"\
          : \"LORA\",\n  \"r\": 16,\n  \"revision\": null,\n  \"target_modules\":\
          \ [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n\
          \    \"down_proj\",\n    \"up_proj\",\n    \"gate_proj\"\n  ],\n  \"task_type\"\
          : \"FEATURE_EXTRACTION\"\n}\n</code></pre>\n"
        raw: "Thanks @SeanLee97 !  I learn a lot from your AnglE paper!\n\nThe released\
          \ model contains merged lora weights, I thought this would make it more\
          \ convenient to use without depending on PEFT library.\n\nHere is our LoRA\
          \ config:\n\n```\n{\n  \"auto_mapping\": null,\n  \"base_model_name_or_path\"\
          : \"mistralai/Mistral-7B-v0.1\",\n  \"bias\": \"none\",\n  \"fan_in_fan_out\"\
          : false,\n  \"inference_mode\": true,\n  \"init_lora_weights\": true,\n\
          \  \"layers_pattern\": null,\n  \"layers_to_transform\": null,\n  \"lora_alpha\"\
          : 32,\n  \"lora_dropout\": 0.1,\n  \"modules_to_save\": null,\n  \"peft_type\"\
          : \"LORA\",\n  \"r\": 16,\n  \"revision\": null,\n  \"target_modules\":\
          \ [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n\
          \    \"down_proj\",\n    \"up_proj\",\n    \"gate_proj\"\n  ],\n  \"task_type\"\
          : \"FEATURE_EXTRACTION\"\n}\n```"
        updatedAt: '2024-01-05T05:04:59.669Z'
      numEdits: 0
      reactions: []
    id: 65978dfbce92304a7161c5cd
    type: comment
  author: intfloat
  content: "Thanks @SeanLee97 !  I learn a lot from your AnglE paper!\n\nThe released\
    \ model contains merged lora weights, I thought this would make it more convenient\
    \ to use without depending on PEFT library.\n\nHere is our LoRA config:\n\n```\n\
    {\n  \"auto_mapping\": null,\n  \"base_model_name_or_path\": \"mistralai/Mistral-7B-v0.1\"\
    ,\n  \"bias\": \"none\",\n  \"fan_in_fan_out\": false,\n  \"inference_mode\":\
    \ true,\n  \"init_lora_weights\": true,\n  \"layers_pattern\": null,\n  \"layers_to_transform\"\
    : null,\n  \"lora_alpha\": 32,\n  \"lora_dropout\": 0.1,\n  \"modules_to_save\"\
    : null,\n  \"peft_type\": \"LORA\",\n  \"r\": 16,\n  \"revision\": null,\n  \"\
    target_modules\": [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"\
    o_proj\",\n    \"down_proj\",\n    \"up_proj\",\n    \"gate_proj\"\n  ],\n  \"\
    task_type\": \"FEATURE_EXTRACTION\"\n}\n```"
  created_at: 2024-01-05 05:04:59+00:00
  edited: false
  hidden: false
  id: 65978dfbce92304a7161c5cd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: intfloat/e5-mistral-7b-instruct
repo_type: model
status: open
target_branch: null
title: lora config
