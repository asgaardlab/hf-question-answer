!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ivankeller
conflicting_files: null
created_at: 2024-01-09 10:47:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d206b162a5593f17c7c72e/undwgFEhwIQ-IIRj9K35A.png?w=200&h=200&f=face
      fullname: Ivan Keller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivankeller
      type: user
    createdAt: '2024-01-09T10:47:28.000Z'
    data:
      edited: true
      editors:
      - ivankeller
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.524095356464386
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d206b162a5593f17c7c72e/undwgFEhwIQ-IIRj9K35A.png?w=200&h=200&f=face
          fullname: Ivan Keller
          isHf: false
          isPro: false
          name: ivankeller
          type: user
        html: "<p>Hi,<br>I'm trying to deploy with SageMaker SDK and I get this error:</p>\n\
          <pre><code>&gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 321, in __init__\n    model = FlashMistralForCausalLM(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 486, in __init__\n    self.model = MistralModel(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 418, in __init__\n    self.embed_tokens = TensorParallelEmbedding(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 482, in __init__\n    weight = weights.get_partial_sharded(f\"{prefix}.weight\"\
          , dim=0)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 78, in get_partial_sharded\n    filename, tensor_name = self.get_filename(tensor_name)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 53, in get_filename\n    raise RuntimeError(f\"weight {tensor_name}\
          \ does not exist\")\n\nRuntimeError: weight model.embed_tokens.weight does\
          \ not exist\n</code></pre>\n<p>I'm using this configuration:</p>\n<pre><code>\
          \    \"e5-mistral-7b-instruct\": {\n        \"model_type\": \"huggingface\"\
          ,\n        \"model_id\": \"intfloat/e5-mistral-7b-instruct\",\n        \"\
          instance_type\": \"ml.g5.2xlarge\",\n        \"num_gpus\": 1,\n        \"\
          image_uri\": \"763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\"\
          ,\n        \"model_hub\": {\n            \"MODEL_SERVER_TIMEOUT\": \"120\"\
          ,\n            \"MAX_INPUT_LENGTH\": \"2048\",\n            \"MAX_TOTAL_TOKENS\"\
          : \"4096\"\n        }\n    }\n</code></pre>\n<p>The <code>image_uri</code>\
          \ corresponds to the output of <code>get_huggingface_llm_image_uri(\"huggingface\"\
          ,version=\"1.1.0\")</code></p>\n<p>Any idea? </p>\n"
        raw: "Hi, \nI'm trying to deploy with SageMaker SDK and I get this error:\n\
          ```\n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 321, in __init__\n    model = FlashMistralForCausalLM(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 486, in __init__\n    self.model = MistralModel(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 418, in __init__\n    self.embed_tokens = TensorParallelEmbedding(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 482, in __init__\n    weight = weights.get_partial_sharded(f\"{prefix}.weight\"\
          , dim=0)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 78, in get_partial_sharded\n    filename, tensor_name = self.get_filename(tensor_name)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 53, in get_filename\n    raise RuntimeError(f\"weight {tensor_name}\
          \ does not exist\")\n\nRuntimeError: weight model.embed_tokens.weight does\
          \ not exist\n```\n\nI'm using this configuration:\n\n```\n    \"e5-mistral-7b-instruct\"\
          : {\n        \"model_type\": \"huggingface\",\n        \"model_id\": \"\
          intfloat/e5-mistral-7b-instruct\",\n        \"instance_type\": \"ml.g5.2xlarge\"\
          ,\n        \"num_gpus\": 1,\n        \"image_uri\": \"763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\"\
          ,\n        \"model_hub\": {\n            \"MODEL_SERVER_TIMEOUT\": \"120\"\
          ,\n            \"MAX_INPUT_LENGTH\": \"2048\",\n            \"MAX_TOTAL_TOKENS\"\
          : \"4096\"\n        }\n    }\n```\n\nThe `image_uri` corresponds to the\
          \ output of `get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"\
          )`\n\nAny idea? "
        updatedAt: '2024-01-09T10:48:00.479Z'
      numEdits: 1
      reactions: []
    id: 659d2440a3d1908ff3b206ad
    type: comment
  author: ivankeller
  content: "Hi, \nI'm trying to deploy with SageMaker SDK and I get this error:\n\
    ```\n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 252, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 321, in __init__\n    model = FlashMistralForCausalLM(config, weights)\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 486, in __init__\n    self.model = MistralModel(config, weights)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 418, in __init__\n    self.embed_tokens = TensorParallelEmbedding(\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 482, in __init__\n    weight = weights.get_partial_sharded(f\"{prefix}.weight\"\
    , dim=0)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 78, in get_partial_sharded\n    filename, tensor_name = self.get_filename(tensor_name)\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 53, in get_filename\n    raise RuntimeError(f\"weight {tensor_name} does\
    \ not exist\")\n\nRuntimeError: weight model.embed_tokens.weight does not exist\n\
    ```\n\nI'm using this configuration:\n\n```\n    \"e5-mistral-7b-instruct\": {\n\
    \        \"model_type\": \"huggingface\",\n        \"model_id\": \"intfloat/e5-mistral-7b-instruct\"\
    ,\n        \"instance_type\": \"ml.g5.2xlarge\",\n        \"num_gpus\": 1,\n \
    \       \"image_uri\": \"763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\"\
    ,\n        \"model_hub\": {\n            \"MODEL_SERVER_TIMEOUT\": \"120\",\n\
    \            \"MAX_INPUT_LENGTH\": \"2048\",\n            \"MAX_TOTAL_TOKENS\"\
    : \"4096\"\n        }\n    }\n```\n\nThe `image_uri` corresponds to the output\
    \ of `get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\")`\n\nAny\
    \ idea? "
  created_at: 2024-01-09 10:47:28+00:00
  edited: true
  hidden: false
  id: 659d2440a3d1908ff3b206ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2024-01-10T02:56:15.000Z'
    data:
      edited: true
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.540918231010437
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ivankeller&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ivankeller\">@<span class=\"\
          underline\">ivankeller</span></a></span>\n\n\t</span></span> It's an embedding\
          \ model without LM head, please load with <code>AutoModel.from_pretrained</code>\
          \ instead of <code>AutoModelForCausalLM.from_pretrained</code>.</p>\n<p>Also,\
          \ this model does not have text generation capability.</p>\n"
        raw: '@ivankeller It''s an embedding model without LM head, please load with
          `AutoModel.from_pretrained` instead of `AutoModelForCausalLM.from_pretrained`.


          Also, this model does not have text generation capability.'
        updatedAt: '2024-01-10T02:56:40.677Z'
      numEdits: 1
      reactions: []
    id: 659e074ff59b66e60fcf26f7
    type: comment
  author: intfloat
  content: '@ivankeller It''s an embedding model without LM head, please load with
    `AutoModel.from_pretrained` instead of `AutoModelForCausalLM.from_pretrained`.


    Also, this model does not have text generation capability.'
  created_at: 2024-01-10 02:56:15+00:00
  edited: true
  hidden: false
  id: 659e074ff59b66e60fcf26f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d206b162a5593f17c7c72e/undwgFEhwIQ-IIRj9K35A.png?w=200&h=200&f=face
      fullname: Ivan Keller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivankeller
      type: user
    createdAt: '2024-01-10T12:30:47.000Z'
    data:
      edited: true
      editors:
      - ivankeller
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8418725728988647
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d206b162a5593f17c7c72e/undwgFEhwIQ-IIRj9K35A.png?w=200&h=200&f=face
          fullname: Ivan Keller
          isHf: false
          isPro: false
          name: ivankeller
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;intfloat&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/intfloat\"\
          >@<span class=\"underline\">intfloat</span></a></span>\n\n\t</span></span>\
          \ , I'm trying now with packaging the model like described here:<br><a rel=\"\
          nofollow\" href=\"https://www.philschmid.de/custom-inference-huggingface-sagemaker\"\
          >https://www.philschmid.de/custom-inference-huggingface-sagemaker</a><br>I\
          \ know it does not have text generation capability. I want the embedding\
          \ vector corresponding to a text.<br>By the way, why does the embedding\
          \ depends on a specific task? Is it possible just to have the embedding\
          \ of a text itself, regardless of the task?</p>\n"
        raw: "Thank you @intfloat , I'm trying now with packaging the model like described\
          \ here: \nhttps://www.philschmid.de/custom-inference-huggingface-sagemaker\n\
          I know it does not have text generation capability. I want the embedding\
          \ vector corresponding to a text.   \nBy the way, why does the embedding\
          \ depends on a specific task? Is it possible just to have the embedding\
          \ of a text itself, regardless of the task?"
        updatedAt: '2024-01-10T12:32:47.704Z'
      numEdits: 1
      reactions: []
    id: 659e8df7b5dfb45bfff22e84
    type: comment
  author: ivankeller
  content: "Thank you @intfloat , I'm trying now with packaging the model like described\
    \ here: \nhttps://www.philschmid.de/custom-inference-huggingface-sagemaker\nI\
    \ know it does not have text generation capability. I want the embedding vector\
    \ corresponding to a text.   \nBy the way, why does the embedding depends on a\
    \ specific task? Is it possible just to have the embedding of a text itself, regardless\
    \ of the task?"
  created_at: 2024-01-10 12:30:47+00:00
  edited: true
  hidden: false
  id: 659e8df7b5dfb45bfff22e84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d206b162a5593f17c7c72e/undwgFEhwIQ-IIRj9K35A.png?w=200&h=200&f=face
      fullname: Ivan Keller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivankeller
      type: user
    createdAt: '2024-01-10T13:47:49.000Z'
    data:
      edited: false
      editors:
      - ivankeller
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6678902506828308
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d206b162a5593f17c7c72e/undwgFEhwIQ-IIRj9K35A.png?w=200&h=200&f=face
          fullname: Ivan Keller
          isHf: false
          isPro: false
          name: ivankeller
          type: user
        html: "<p>I still get the same error <code>ValueError: Tokenizer class LlamaTokenizer\
          \ does not exist or is not currently imported.</code><br>Following the ref\
          \ above (<a rel=\"nofollow\" href=\"https://www.philschmid.de/custom-inference-huggingface-sagemaker\"\
          >https://www.philschmid.de/custom-inference-huggingface-sagemaker</a>) I\
          \ use <code>.from_pretrained</code> :</p>\n<pre><code>def model_fn(model_dir):\n\
          \    # Load model from HuggingFace Hub\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n\
          \    model = AutoModel.from_pretrained(model_dir)\n    return model, tokenizer\n\
          </code></pre>\n<p>The model and tokenizer are packages into `model.tar.gz\
          \ on located on S3. </p>\n"
        raw: "I still get the same error `ValueError: Tokenizer class LlamaTokenizer\
          \ does not exist or is not currently imported.` \nFollowing the ref above\
          \ (https://www.philschmid.de/custom-inference-huggingface-sagemaker) I use\
          \ `.from_pretrained` :\n```\ndef model_fn(model_dir):\n    # Load model\
          \ from HuggingFace Hub\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n\
          \    model = AutoModel.from_pretrained(model_dir)\n    return model, tokenizer\n\
          ```\n\nThe model and tokenizer are packages into `model.tar.gz on located\
          \ on S3. "
        updatedAt: '2024-01-10T13:47:49.096Z'
      numEdits: 0
      reactions: []
    id: 659ea0055b7e2734a3602c65
    type: comment
  author: ivankeller
  content: "I still get the same error `ValueError: Tokenizer class LlamaTokenizer\
    \ does not exist or is not currently imported.` \nFollowing the ref above (https://www.philschmid.de/custom-inference-huggingface-sagemaker)\
    \ I use `.from_pretrained` :\n```\ndef model_fn(model_dir):\n    # Load model\
    \ from HuggingFace Hub\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n\
    \    model = AutoModel.from_pretrained(model_dir)\n    return model, tokenizer\n\
    ```\n\nThe model and tokenizer are packages into `model.tar.gz on located on S3. "
  created_at: 2024-01-10 13:47:49+00:00
  edited: false
  hidden: false
  id: 659ea0055b7e2734a3602c65
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: intfloat/e5-mistral-7b-instruct
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: weight model.embed_tokens.weight does not exist'
