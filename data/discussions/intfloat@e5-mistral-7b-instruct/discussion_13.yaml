!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomaarsen
conflicting_files: null
created_at: 2024-01-10 18:32:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-10T18:32:02.000Z'
    data:
      edited: false
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8711078763008118
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: "<p>Hello!</p>\n<p>I've quite enjoyed watching this model release. In\
          \ truth, I did not expect an LLM to be capable of producing such valuable\
          \ embeddings. To me, this begs the question: could we distill these high\
          \ quality embeddings into a smaller model (e.g. <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5\"\
          >bge-small</a>) to 1) improve the performance of the smaller student model\
          \ and 2) create longer sequence length models without requiring long-sequence\
          \ labeled training data.</p>\n<p>Additionally, I'm very interested in implementing\
          \ first-party support for models such as this one in <a rel=\"nofollow\"\
          \ href=\"https://github.com/UKPLab/sentence-transformers\">Sentence Transformers</a>.\
          \ One key aspect that is currently lacking is prompt template support. I\
          \ was picturing adding a <code>prompts</code> configuration option like\
          \ so:</p>\n<pre><code class=\"language-json\"><span class=\"hljs-punctuation\"\
          >{</span>\n    ...\n    <span class=\"hljs-attr\">\"prompts\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n\
          \        <span class=\"hljs-attr\">\"retrieval\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"Retrieve semantically similar text.\
          \ {}\"</span><span class=\"hljs-punctuation\">,</span>\n        <span class=\"\
          hljs-attr\">\"summarization\"</span><span class=\"hljs-punctuation\">:</span>\
          \ <span class=\"hljs-string\">\"Given a news summary, retrieve other semantically\
          \ similar summaries. {}\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \        ...\n    <span class=\"hljs-punctuation\">}</span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"default_prompt_key\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\"\
          >\"retrieval\"</span><span class=\"hljs-punctuation\">,</span>\n<span class=\"\
          hljs-punctuation\">}</span>\n</code></pre>\n<p>Then, models can be ran like\
          \ so:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> sentence_transformers <span class=\"hljs-keyword\">import</span>\
          \ SentenceTransformers\n\nmodel = SentenceTransformers(<span class=\"hljs-string\"\
          >\"intfloat/multilingual-e5-base\"</span>)\nmodel.encode(<span class=\"\
          hljs-string\">\"how much protein should a female eat\"</span>, prompt_key=<span\
          \ class=\"hljs-string\">\"query\"</span>)\nmodel.encode(<span class=\"hljs-string\"\
          >\"As a general guideline, the CDC's average requirement of protein for\
          \ women ages 19 to 70 is 46 grams per day.\"</span>, prompt_key=<span class=\"\
          hljs-string\">\"passage\"</span>)\n</code></pre>\n<p>The same would work\
          \ for <code>intfloat/e5-mistral-7b-instruct</code>.<br>I have this change\
          \ planned alongside various other improvements of Sentence Transformers,\
          \ such as a stronger Trainer, more modern objective functions (InfoNCE,\
          \ AngLE), multi-gpu training, and more. I'd be very curious to hear your\
          \ thoughts on all of this.</p>\n<p>Keep up the great work.</p>\n<ul>\n<li>Tom\
          \ Aarsen</li>\n</ul>\n"
        raw: "Hello!\r\n\r\nI've quite enjoyed watching this model release. In truth,\
          \ I did not expect an LLM to be capable of producing such valuable embeddings.\
          \ To me, this begs the question: could we distill these high quality embeddings\
          \ into a smaller model (e.g. [bge-small](https://huggingface.co/BAAI/bge-small-en-v1.5))\
          \ to 1) improve the performance of the smaller student model and 2) create\
          \ longer sequence length models without requiring long-sequence labeled\
          \ training data.\r\n\r\nAdditionally, I'm very interested in implementing\
          \ first-party support for models such as this one in [Sentence Transformers](https://github.com/UKPLab/sentence-transformers).\
          \ One key aspect that is currently lacking is prompt template support. I\
          \ was picturing adding a `prompts` configuration option like so:\r\n```json\r\
          \n{\r\n    ...\r\n    \"prompts\": {\r\n        \"retrieval\": \"Retrieve\
          \ semantically similar text. {}\",\r\n        \"summarization\": \"Given\
          \ a news summary, retrieve other semantically similar summaries. {}\",\r\
          \n        ...\r\n    },\r\n    \"default_prompt_key\": \"retrieval\",\r\n\
          }\r\n```\r\n\r\nThen, models can be ran like so:\r\n```python\r\nfrom sentence_transformers\
          \ import SentenceTransformers\r\n\r\nmodel = SentenceTransformers(\"intfloat/multilingual-e5-base\"\
          )\r\nmodel.encode(\"how much protein should a female eat\", prompt_key=\"\
          query\")\r\nmodel.encode(\"As a general guideline, the CDC's average requirement\
          \ of protein for women ages 19 to 70 is 46 grams per day.\", prompt_key=\"\
          passage\")\r\n```\r\n\r\nThe same would work for `intfloat/e5-mistral-7b-instruct`.\
          \ \r\nI have this change planned alongside various other improvements of\
          \ Sentence Transformers, such as a stronger Trainer, more modern objective\
          \ functions (InfoNCE, AngLE), multi-gpu training, and more. I'd be very\
          \ curious to hear your thoughts on all of this.\r\n\r\nKeep up the great\
          \ work.\r\n\r\n- Tom Aarsen"
        updatedAt: '2024-01-10T18:32:02.135Z'
      numEdits: 0
      reactions: []
    id: 659ee2a244a230e92ca320b0
    type: comment
  author: tomaarsen
  content: "Hello!\r\n\r\nI've quite enjoyed watching this model release. In truth,\
    \ I did not expect an LLM to be capable of producing such valuable embeddings.\
    \ To me, this begs the question: could we distill these high quality embeddings\
    \ into a smaller model (e.g. [bge-small](https://huggingface.co/BAAI/bge-small-en-v1.5))\
    \ to 1) improve the performance of the smaller student model and 2) create longer\
    \ sequence length models without requiring long-sequence labeled training data.\r\
    \n\r\nAdditionally, I'm very interested in implementing first-party support for\
    \ models such as this one in [Sentence Transformers](https://github.com/UKPLab/sentence-transformers).\
    \ One key aspect that is currently lacking is prompt template support. I was picturing\
    \ adding a `prompts` configuration option like so:\r\n```json\r\n{\r\n    ...\r\
    \n    \"prompts\": {\r\n        \"retrieval\": \"Retrieve semantically similar\
    \ text. {}\",\r\n        \"summarization\": \"Given a news summary, retrieve other\
    \ semantically similar summaries. {}\",\r\n        ...\r\n    },\r\n    \"default_prompt_key\"\
    : \"retrieval\",\r\n}\r\n```\r\n\r\nThen, models can be ran like so:\r\n```python\r\
    \nfrom sentence_transformers import SentenceTransformers\r\n\r\nmodel = SentenceTransformers(\"\
    intfloat/multilingual-e5-base\")\r\nmodel.encode(\"how much protein should a female\
    \ eat\", prompt_key=\"query\")\r\nmodel.encode(\"As a general guideline, the CDC's\
    \ average requirement of protein for women ages 19 to 70 is 46 grams per day.\"\
    , prompt_key=\"passage\")\r\n```\r\n\r\nThe same would work for `intfloat/e5-mistral-7b-instruct`.\
    \ \r\nI have this change planned alongside various other improvements of Sentence\
    \ Transformers, such as a stronger Trainer, more modern objective functions (InfoNCE,\
    \ AngLE), multi-gpu training, and more. I'd be very curious to hear your thoughts\
    \ on all of this.\r\n\r\nKeep up the great work.\r\n\r\n- Tom Aarsen"
  created_at: 2024-01-10 18:32:02+00:00
  edited: false
  hidden: false
  id: 659ee2a244a230e92ca320b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2024-01-11T05:27:39.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9521916508674622
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;tomaarsen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tomaarsen\"\
          >@<span class=\"underline\">tomaarsen</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>Wow, that's really some impressive improvements of Sentence\
          \ Transformers!<br>I have been using Sentence Transformers for quite some\
          \ time, its interface is simple and easy to understand. However, when it\
          \ comes to customizations such as tweaking the tokenization a bit or adding\
          \ instructions, some inelegant hack is often necessary. I very much look\
          \ forward to the new releases.</p>\n<p>About the distillation of smaller\
          \ models, it certainly makes sense from the point of inference efficiency\
          \ and storage cost. The current <code>e5-mistral-7b-instruct</code> model\
          \ is expensive to run even on GPUs.<br>The ideas behind the <a rel=\"nofollow\"\
          \ href=\"https://arxiv.org/abs/2010.11386\">Tightly Coupled Teacher</a>\
          \ paper might fit here. Another thought is that cross-encoders are usually\
          \ more powerful than bi-encoders at the same model size, distilling from\
          \ LLM-based cross-encoders such as <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2310.08319\"\
          >RankLLaMA</a> is also worth trying.</p>\n<p>I have not run such distillation\
          \ experiments yet, but this is definitely a promising research direction.</p>\n\
          <p>Liang</p>\n"
        raw: "Hi @tomaarsen ,\n\nWow, that's really some impressive improvements of\
          \ Sentence Transformers!\nI have been using Sentence Transformers for quite\
          \ some time, its interface is simple and easy to understand. However, when\
          \ it comes to customizations such as tweaking the tokenization a bit or\
          \ adding instructions, some inelegant hack is often necessary. I very much\
          \ look forward to the new releases.\n\nAbout the distillation of smaller\
          \ models, it certainly makes sense from the point of inference efficiency\
          \ and storage cost. The current `e5-mistral-7b-instruct` model is expensive\
          \ to run even on GPUs. \nThe ideas behind the [Tightly Coupled Teacher](https://arxiv.org/abs/2010.11386)\
          \ paper might fit here. Another thought is that cross-encoders are usually\
          \ more powerful than bi-encoders at the same model size, distilling from\
          \ LLM-based cross-encoders such as [RankLLaMA](https://arxiv.org/abs/2310.08319)\
          \ is also worth trying.\n\nI have not run such distillation experiments\
          \ yet, but this is definitely a promising research direction.\n\nLiang"
        updatedAt: '2024-01-11T05:27:39.830Z'
      numEdits: 0
      reactions: []
    id: 659f7c4bb62804e6f43ccac5
    type: comment
  author: intfloat
  content: "Hi @tomaarsen ,\n\nWow, that's really some impressive improvements of\
    \ Sentence Transformers!\nI have been using Sentence Transformers for quite some\
    \ time, its interface is simple and easy to understand. However, when it comes\
    \ to customizations such as tweaking the tokenization a bit or adding instructions,\
    \ some inelegant hack is often necessary. I very much look forward to the new\
    \ releases.\n\nAbout the distillation of smaller models, it certainly makes sense\
    \ from the point of inference efficiency and storage cost. The current `e5-mistral-7b-instruct`\
    \ model is expensive to run even on GPUs. \nThe ideas behind the [Tightly Coupled\
    \ Teacher](https://arxiv.org/abs/2010.11386) paper might fit here. Another thought\
    \ is that cross-encoders are usually more powerful than bi-encoders at the same\
    \ model size, distilling from LLM-based cross-encoders such as [RankLLaMA](https://arxiv.org/abs/2310.08319)\
    \ is also worth trying.\n\nI have not run such distillation experiments yet, but\
    \ this is definitely a promising research direction.\n\nLiang"
  created_at: 2024-01-11 05:27:39+00:00
  edited: false
  hidden: false
  id: 659f7c4bb62804e6f43ccac5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: intfloat/e5-mistral-7b-instruct
repo_type: model
status: open
target_branch: null
title: Knowledge distillation into smaller model
