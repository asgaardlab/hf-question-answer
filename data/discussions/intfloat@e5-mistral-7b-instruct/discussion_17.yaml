!!python/object:huggingface_hub.community.DiscussionWithDetails
author: daisyyayueyue
conflicting_files: null
created_at: 2024-01-17 18:23:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6979d2bc37ed74822bcb115dc2d1b43f.svg
      fullname: daisy wag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daisyyayueyue
      type: user
    createdAt: '2024-01-17T18:23:43.000Z'
    data:
      edited: false
      editors:
      - daisyyayueyue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9026069641113281
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6979d2bc37ed74822bcb115dc2d1b43f.svg
          fullname: daisy wag
          isHf: false
          isPro: false
          name: daisyyayueyue
          type: user
        html: '<p>is it possible you can share an example of extracting general text
          embedding ? I would love to test the clustering task based on semantic similarity</p>

          '
        raw: is it possible you can share an example of extracting general text embedding
          ? I would love to test the clustering task based on semantic similarity
        updatedAt: '2024-01-17T18:23:43.215Z'
      numEdits: 0
      reactions: []
    id: 65a81b2f539e211436bfbb1e
    type: comment
  author: daisyyayueyue
  content: is it possible you can share an example of extracting general text embedding
    ? I would love to test the clustering task based on semantic similarity
  created_at: 2024-01-17 18:23:43+00:00
  edited: false
  hidden: false
  id: 65a81b2f539e211436bfbb1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5704334d953cd1980b07e1d4b9fa0b50.svg
      fullname: Himanshu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrhimanshu
      type: user
    createdAt: '2024-01-21T11:48:15.000Z'
    data:
      edited: true
      editors:
      - mrhimanshu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33430933952331543
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5704334d953cd1980b07e1d4b9fa0b50.svg
          fullname: Himanshu
          isHf: false
          isPro: false
          name: mrhimanshu
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;daisyyayueyue&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daisyyayueyue\"\
          >@<span class=\"underline\">daisyyayueyue</span></a></span>\n\n\t</span></span><br>Hope\
          \ the below code will help you.</p>\n<p>def embedding_generation(query):<br>\
          \    input_ids = tokenizer(query, max_length=4096, padding=True, truncation=True,\
          \ return_tensors='pt')<br>    inputs_embeds = model.model.embed_tokens(input_ids.input_ids)<br>\
          \    embeddings = average_pool(inputs_embeds, input_ids['attention_mask'])<br>\
          \    embeddings = F.normalize(embeddings, p=2, dim=1)<br>    # scores =\
          \ (embeddings[:2] @ embeddings[2:].T) * 100<br>    # print(scores.tolist())<br>\
          \    return embeddings</p>\n"
        raw: "Hi @daisyyayueyue \nHope the below code will help you.\n\ndef embedding_generation(query):\n\
          \    input_ids = tokenizer(query, max_length=4096, padding=True, truncation=True,\
          \ return_tensors='pt')\n    inputs_embeds = model.model.embed_tokens(input_ids.input_ids)\n\
          \    embeddings = average_pool(inputs_embeds, input_ids['attention_mask'])\n\
          \    embeddings = F.normalize(embeddings, p=2, dim=1)\n    # scores = (embeddings[:2]\
          \ @ embeddings[2:].T) * 100\n    # print(scores.tolist())\n    return embeddings"
        updatedAt: '2024-01-21T11:50:02.912Z'
      numEdits: 1
      reactions: []
    id: 65ad047f0844d9e0d6b01b3c
    type: comment
  author: mrhimanshu
  content: "Hi @daisyyayueyue \nHope the below code will help you.\n\ndef embedding_generation(query):\n\
    \    input_ids = tokenizer(query, max_length=4096, padding=True, truncation=True,\
    \ return_tensors='pt')\n    inputs_embeds = model.model.embed_tokens(input_ids.input_ids)\n\
    \    embeddings = average_pool(inputs_embeds, input_ids['attention_mask'])\n \
    \   embeddings = F.normalize(embeddings, p=2, dim=1)\n    # scores = (embeddings[:2]\
    \ @ embeddings[2:].T) * 100\n    # print(scores.tolist())\n    return embeddings"
  created_at: 2024-01-21 11:48:15+00:00
  edited: true
  hidden: false
  id: 65ad047f0844d9e0d6b01b3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: intfloat/e5-mistral-7b-instruct
repo_type: model
status: open
target_branch: null
title: how to extract general text embedding
