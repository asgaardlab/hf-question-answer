!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lightapple
conflicting_files: null
created_at: 2024-01-23 05:41:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa12efb67bcbf412d50fb9318f1db4b3.svg
      fullname: june kwon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lightapple
      type: user
    createdAt: '2024-01-23T05:41:41.000Z'
    data:
      edited: true
      editors:
      - lightapple
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9128614068031311
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa12efb67bcbf412d50fb9318f1db4b3.svg
          fullname: june kwon
          isHf: false
          isPro: false
          name: lightapple
          type: user
        html: '<p>Hi, I am really interested in your research using LLM in dense retrieval
          task.</p>

          <p>I am trying to replicate your training method without synthetic data
          using GPT, but I have some trouble with following your negative sampling
          method.<br>As you mentioned in your paper, the training set of following
          benchmark dataset was used during training;<br>ELI5, HotpotQA, FEVER, MIRACL,
          MS MARCO passage ranking and document ranking, NQ, NLI, SQuAD, TriviaQA,
          Quora Duplicate Questions, MrTyDi, DuReader, and T2Ranking</p>

          <p>On the other hand, you wrote that for the dataset without hard negatives,
          you used mE5 top 100 for the negative sampling instead.<br>I am wondering
          that</p>

          <ol>

          <li>Did you use 100 negative sample for the dataset without hard negatives?</li>

          <li>Which benchmark dataset did you use mE5 for negative sampling? I might
          guess all the dataset without MS MARCO, MR. TyDi and T2Ranking, but i am
          not certain.</li>

          </ol>

          <p>Thanks for your great research and sharing. </p>

          '
        raw: 'Hi, I am really interested in your research using LLM in dense retrieval
          task.


          I am trying to replicate your training method without synthetic data using
          GPT, but I have some trouble with following your negative sampling method.

          As you mentioned in your paper, the training set of following benchmark
          dataset was used during training;

          ELI5, HotpotQA, FEVER, MIRACL, MS MARCO passage ranking and document ranking,
          NQ, NLI, SQuAD, TriviaQA, Quora Duplicate Questions, MrTyDi, DuReader, and
          T2Ranking


          On the other hand, you wrote that for the dataset without hard negatives,
          you used mE5 top 100 for the negative sampling instead.

          I am wondering that

          1) Did you use 100 negative sample for the dataset without hard negatives?

          2) Which benchmark dataset did you use mE5 for negative sampling? I might
          guess all the dataset without MS MARCO, MR. TyDi and T2Ranking, but i am
          not certain.


          Thanks for your great research and sharing. '
        updatedAt: '2024-01-23T05:49:51.026Z'
      numEdits: 2
      reactions: []
    id: 65af5195de38fbe922b8e98c
    type: comment
  author: lightapple
  content: 'Hi, I am really interested in your research using LLM in dense retrieval
    task.


    I am trying to replicate your training method without synthetic data using GPT,
    but I have some trouble with following your negative sampling method.

    As you mentioned in your paper, the training set of following benchmark dataset
    was used during training;

    ELI5, HotpotQA, FEVER, MIRACL, MS MARCO passage ranking and document ranking,
    NQ, NLI, SQuAD, TriviaQA, Quora Duplicate Questions, MrTyDi, DuReader, and T2Ranking


    On the other hand, you wrote that for the dataset without hard negatives, you
    used mE5 top 100 for the negative sampling instead.

    I am wondering that

    1) Did you use 100 negative sample for the dataset without hard negatives?

    2) Which benchmark dataset did you use mE5 for negative sampling? I might guess
    all the dataset without MS MARCO, MR. TyDi and T2Ranking, but i am not certain.


    Thanks for your great research and sharing. '
  created_at: 2024-01-23 05:41:41+00:00
  edited: true
  hidden: false
  id: 65af5195de38fbe922b8e98c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2024-01-23T09:15:56.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8219797611236572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;lightapple&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lightapple\"\
          >@<span class=\"underline\">lightapple</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<ol>\n<li><p>Did you use 100 negative sample for the dataset without\
          \ hard negatives?<br>We use <code>multilingual-e5-base</code> to get top\
          \ 100 candidates from the document pool for that dataset, and then randomly\
          \ sample <code>1</code> as hard negative from these 100 candidates during\
          \ training.</p>\n</li>\n<li><p>Which benchmark dataset did you use mE5 for\
          \ negative sampling?<br>If I remember correctly, NLI / MrTyDi / DuReader\
          \ / T2Ranking / MIRACL already provide hard negatives, so no need to mine\
          \ by myself. For MS MARCO passage ranking and NQ, we re-use hard negatives\
          \ from the <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2207.02578\"\
          >SimLM</a> paper. For the remaining datasets, we use <code>multilingual-e5-base</code>\
          \ to mine hard negatives.</p>\n</li>\n</ol>\n"
        raw: 'Hi @lightapple ,


          1. Did you use 100 negative sample for the dataset without hard negatives?

          We use `multilingual-e5-base` to get top 100 candidates from the document
          pool for that dataset, and then randomly sample `1` as hard negative from
          these 100 candidates during training.


          2. Which benchmark dataset did you use mE5 for negative sampling?

          If I remember correctly, NLI / MrTyDi / DuReader / T2Ranking / MIRACL already
          provide hard negatives, so no need to mine by myself. For MS MARCO passage
          ranking and NQ, we re-use hard negatives from the [SimLM](https://arxiv.org/abs/2207.02578)
          paper. For the remaining datasets, we use `multilingual-e5-base` to mine
          hard negatives.'
        updatedAt: '2024-01-23T09:15:56.337Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lightapple
    id: 65af83cce0ea35ad61cfd33a
    type: comment
  author: intfloat
  content: 'Hi @lightapple ,


    1. Did you use 100 negative sample for the dataset without hard negatives?

    We use `multilingual-e5-base` to get top 100 candidates from the document pool
    for that dataset, and then randomly sample `1` as hard negative from these 100
    candidates during training.


    2. Which benchmark dataset did you use mE5 for negative sampling?

    If I remember correctly, NLI / MrTyDi / DuReader / T2Ranking / MIRACL already
    provide hard negatives, so no need to mine by myself. For MS MARCO passage ranking
    and NQ, we re-use hard negatives from the [SimLM](https://arxiv.org/abs/2207.02578)
    paper. For the remaining datasets, we use `multilingual-e5-base` to mine hard
    negatives.'
  created_at: 2024-01-23 09:15:56+00:00
  edited: false
  hidden: false
  id: 65af83cce0ea35ad61cfd33a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa12efb67bcbf412d50fb9318f1db4b3.svg
      fullname: june kwon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lightapple
      type: user
    createdAt: '2024-01-24T02:02:39.000Z'
    data:
      edited: false
      editors:
      - lightapple
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8998180627822876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa12efb67bcbf412d50fb9318f1db4b3.svg
          fullname: june kwon
          isHf: false
          isPro: false
          name: lightapple
          type: user
        html: '<p>Thanks! your kind answer helps me a lot!</p>

          '
        raw: Thanks! your kind answer helps me a lot!
        updatedAt: '2024-01-24T02:02:39.471Z'
      numEdits: 0
      reactions: []
    id: 65b06fbf3d8a48439cb18114
    type: comment
  author: lightapple
  content: Thanks! your kind answer helps me a lot!
  created_at: 2024-01-24 02:02:39+00:00
  edited: false
  hidden: false
  id: 65b06fbf3d8a48439cb18114
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: intfloat/e5-mistral-7b-instruct
repo_type: model
status: open
target_branch: null
title: Can you tell me the detail about negative sampling for benchmark data?
