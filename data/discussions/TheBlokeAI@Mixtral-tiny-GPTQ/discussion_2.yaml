!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NePe
conflicting_files: null
created_at: 2023-12-12 22:58:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
      fullname: Peter Kis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NePe
      type: user
    createdAt: '2023-12-12T22:58:45.000Z'
    data:
      edited: true
      editors:
      - NePe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5459614992141724
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
          fullname: Peter Kis
          isHf: false
          isPro: false
          name: NePe
          type: user
        html: "<p>for the bigger models i get:<br>RuntimeError: cannot reshape tensor\
          \ of 0 elements into shape [-1, 1, 0] because the unspecified dimension\
          \ size -1 can be any value and is ambiguous in self.gate...</p>\n<p>for\
          \ this test one i get:</p>\n<pre><code>...\n File \"/home/nepe/.local/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 708, in forward\n    router_logits = self.gate(hidden_states)\n \
          \ File \"/home/nepe/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/nepe/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/nepe/.local/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = module._old_forward(*args, **kwargs)\n\
          \  File \"/home/nepe/.local/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\"\
          , line 227, in forward\n    zeros = zeros.reshape(self.scales.shape)\nRuntimeError:\
          \ shape '[8, 8]' is invalid for input of size 0\n</code></pre>\n<p>The non\
          \ GPTQ version of the test model works perfectly.</p>\n"
        raw: "for the bigger models i get:\nRuntimeError: cannot reshape tensor of\
          \ 0 elements into shape [-1, 1, 0] because the unspecified dimension size\
          \ -1 can be any value and is ambiguous in self.gate...\n\nfor this test\
          \ one i get:\n```\n...\n File \"/home/nepe/.local/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 708, in forward\n    router_logits = self.gate(hidden_states)\n \
          \ File \"/home/nepe/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/nepe/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/nepe/.local/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = module._old_forward(*args, **kwargs)\n\
          \  File \"/home/nepe/.local/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\"\
          , line 227, in forward\n    zeros = zeros.reshape(self.scales.shape)\nRuntimeError:\
          \ shape '[8, 8]' is invalid for input of size 0\n```\n\nThe non GPTQ version\
          \ of the test model works perfectly."
        updatedAt: '2023-12-12T22:59:54.395Z'
      numEdits: 1
      reactions: []
    id: 6578e5a52f2e4058aef6eb51
    type: comment
  author: NePe
  content: "for the bigger models i get:\nRuntimeError: cannot reshape tensor of 0\
    \ elements into shape [-1, 1, 0] because the unspecified dimension size -1 can\
    \ be any value and is ambiguous in self.gate...\n\nfor this test one i get:\n\
    ```\n...\n File \"/home/nepe/.local/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
    , line 708, in forward\n    router_logits = self.gate(hidden_states)\n  File \"\
    /home/nepe/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line\
    \ 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n \
    \ File \"/home/nepe/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/nepe/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 165,\
    \ in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"\
    /home/nepe/.local/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\"\
    , line 227, in forward\n    zeros = zeros.reshape(self.scales.shape)\nRuntimeError:\
    \ shape '[8, 8]' is invalid for input of size 0\n```\n\nThe non GPTQ version of\
    \ the test model works perfectly."
  created_at: 2023-12-12 22:58:45+00:00
  edited: true
  hidden: false
  id: 6578e5a52f2e4058aef6eb51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-12T23:37:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8822228312492371
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah, see the READMEs of the proper GPTQs for how to load them -
          you still need an AutoGPTQ PR at the moment</p>

          '
        raw: Yeah, see the READMEs of the proper GPTQs for how to load them - you
          still need an AutoGPTQ PR at the moment
        updatedAt: '2023-12-12T23:37:33.406Z'
      numEdits: 0
      reactions: []
    id: 6578eebda87010c9f8a24797
    type: comment
  author: TheBloke
  content: Yeah, see the READMEs of the proper GPTQs for how to load them - you still
    need an AutoGPTQ PR at the moment
  created_at: 2023-12-12 23:37:33+00:00
  edited: false
  hidden: false
  id: 6578eebda87010c9f8a24797
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
      fullname: Peter Kis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NePe
      type: user
    createdAt: '2023-12-12T23:43:32.000Z'
    data:
      edited: false
      editors:
      - NePe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9262712597846985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
          fullname: Peter Kis
          isHf: false
          isPro: false
          name: NePe
          type: user
        html: '<p>I tried both the old and the fix branches, same error. I even tried
          to quantize this model, same error.</p>

          '
        raw: I tried both the old and the fix branches, same error. I even tried to
          quantize this model, same error.
        updatedAt: '2023-12-12T23:43:32.274Z'
      numEdits: 0
      reactions: []
    id: 6578f0247685e1ce5c3958c1
    type: comment
  author: NePe
  content: I tried both the old and the fix branches, same error. I even tried to
    quantize this model, same error.
  created_at: 2023-12-12 23:43:32+00:00
  edited: false
  hidden: false
  id: 6578f0247685e1ce5c3958c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
      fullname: Peter Kis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NePe
      type: user
    createdAt: '2023-12-13T00:02:26.000Z'
    data:
      edited: false
      editors:
      - NePe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8891981244087219
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
          fullname: Peter Kis
          isHf: false
          isPro: false
          name: NePe
          type: user
        html: '<p>As far as i understand there''s still some more things to do.<br>Based
          on this:<br><a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/pull/480">https://github.com/PanQiWei/AutoGPTQ/pull/480</a><br>Have
          to apply this:<br><a rel="nofollow" href="https://github.com/huggingface/transformers/pull/27956">https://github.com/huggingface/transformers/pull/27956</a><br>And
          maybe this one too:<br><a rel="nofollow" href="https://github.com/huggingface/optimum/pull/1585">https://github.com/huggingface/optimum/pull/1585</a></p>

          '
        raw: 'As far as i understand there''s still some more things to do.

          Based on this:

          https://github.com/PanQiWei/AutoGPTQ/pull/480

          Have to apply this:

          https://github.com/huggingface/transformers/pull/27956

          And maybe this one too:

          https://github.com/huggingface/optimum/pull/1585'
        updatedAt: '2023-12-13T00:02:26.692Z'
      numEdits: 0
      reactions: []
    id: 6578f49258d7a2cc89613f8f
    type: comment
  author: NePe
  content: 'As far as i understand there''s still some more things to do.

    Based on this:

    https://github.com/PanQiWei/AutoGPTQ/pull/480

    Have to apply this:

    https://github.com/huggingface/transformers/pull/27956

    And maybe this one too:

    https://github.com/huggingface/optimum/pull/1585'
  created_at: 2023-12-13 00:02:26+00:00
  edited: false
  hidden: false
  id: 6578f49258d7a2cc89613f8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
      fullname: Peter Kis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NePe
      type: user
    createdAt: '2023-12-13T19:59:19.000Z'
    data:
      edited: false
      editors:
      - NePe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7735118865966797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
          fullname: Peter Kis
          isHf: false
          isPro: false
          name: NePe
          type: user
        html: '<p>My mistake, tried it with AutoModelForCausalLM.from_pretrained instead
          of AutoGPTQForCausalLM.from_quantized</p>

          '
        raw: My mistake, tried it with AutoModelForCausalLM.from_pretrained instead
          of AutoGPTQForCausalLM.from_quantized
        updatedAt: '2023-12-13T19:59:19.610Z'
      numEdits: 0
      reactions: []
    id: 657a0d17c9ca8f018e0ab0da
    type: comment
  author: NePe
  content: My mistake, tried it with AutoModelForCausalLM.from_pretrained instead
    of AutoGPTQForCausalLM.from_quantized
  created_at: 2023-12-13 19:59:19+00:00
  edited: false
  hidden: false
  id: 657a0d17c9ca8f018e0ab0da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/793d0a07bc37dacc5b0a486e4bf11d7f.svg
      fullname: Peter Kis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NePe
      type: user
    createdAt: '2023-12-13T19:59:21.000Z'
    data:
      status: closed
    id: 657a0d191b1c5bff9bf84bfa
    type: status-change
  author: NePe
  created_at: 2023-12-13 19:59:21+00:00
  id: 657a0d191b1c5bff9bf84bfa
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBlokeAI/Mixtral-tiny-GPTQ
repo_type: model
status: closed
target_branch: null
title: Seems like the GPTQ versions are broken
