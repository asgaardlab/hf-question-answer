!!python/object:huggingface_hub.community.DiscussionWithDetails
author: luthis
conflicting_files: null
created_at: 2023-06-28 04:25:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b799931f41be46d6c7b18963b436920a.svg
      fullname: Matt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luthis
      type: user
    createdAt: '2023-06-28T05:25:13.000Z'
    data:
      edited: false
      editors:
      - luthis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4735157787799835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b799931f41be46d6c7b18963b436920a.svg
          fullname: Matt
          isHf: false
          isPro: false
          name: luthis
          type: user
        html: '<p>I have played around with --pre_layer 10 up to 48, all have the
          same error:</p>

          <p>Traceback (most recent call last):<br>  File "/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/callbacks.py",
          line 55, in gentask<br>    ret = self.mfunc(callback=_callback, *args, **self.kwargs)<br>  File
          "/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/text_generation.py",
          line 289, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py",
          line 1572, in generate<br>    return self.sample(<br>  File "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py",
          line 2619, in sample<br>    outputs = self(<br>  File "/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/exllama_hf.py",
          line 57, in <strong>call</strong><br>    self.ex_model.forward(torch.tensor([seq[:-1]],
          dtype=torch.long), cache, preprocess_only=True, lora=self.lora)<br>  File
          "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py",
          line 860, in forward<br>    hidden_states = decoder_layer.forward(hidden_states,
          cache, buffers[device], lora)<br>  File "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py",
          line 466, in forward<br>    hidden_states = self.self_attn.forward(hidden_states,
          cache, buffer, lora)<br>  File "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py",
          line 377, in forward<br>    query_states = self.q_proj.forward(hidden_states,
          lora)<br>  File "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py",
          line 195, in forward<br>    out = cuda_ext.ext_q4_matmul(x, self.q4, self.width)<br>  File
          "/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/cuda_ext.py",
          line 49, in ext_q4_matmul<br>    q4_matmul(x, q4, output)<br>RuntimeError:
          CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling <code>cublasCreate(handle)</code><br>Output
          generated in 0.73 seconds (0.00 tokens/s, 0 tokens, context 9, seed 439549603)</p>

          '
        raw: "I have played around with --pre_layer 10 up to 48, all have the same\
          \ error:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/callbacks.py\"\
          , line 55, in gentask\r\n    ret = self.mfunc(callback=_callback, *args,\
          \ **self.kwargs)\r\n  File \"/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/text_generation.py\"\
          , line 289, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\r\n    return self.sample(\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2619, in sample\r\n    outputs = self(\r\n  File \"/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/exllama_hf.py\"\
          , line 57, in __call__\r\n    self.ex_model.forward(torch.tensor([seq[:-1]],\
          \ dtype=torch.long), cache, preprocess_only=True, lora=self.lora)\r\n  File\
          \ \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
          , line 860, in forward\r\n    hidden_states = decoder_layer.forward(hidden_states,\
          \ cache, buffers[device], lora)\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
          , line 466, in forward\r\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
          , line 377, in forward\r\n    query_states = self.q_proj.forward(hidden_states,\
          \ lora)\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
          , line 195, in forward\r\n    out = cuda_ext.ext_q4_matmul(x, self.q4, self.width)\r\
          \n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/cuda_ext.py\"\
          , line 49, in ext_q4_matmul\r\n    q4_matmul(x, q4, output)\r\nRuntimeError:\
          \ CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\r\
          \nOutput generated in 0.73 seconds (0.00 tokens/s, 0 tokens, context 9,\
          \ seed 439549603)\r\n"
        updatedAt: '2023-06-28T05:25:13.564Z'
      numEdits: 0
      reactions: []
    id: 649bc4398f4b425bdeb27459
    type: comment
  author: luthis
  content: "I have played around with --pre_layer 10 up to 48, all have the same error:\r\
    \n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/callbacks.py\"\
    , line 55, in gentask\r\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\r\
    \n  File \"/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/text_generation.py\"\
    , line 289, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
    \n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1572, in generate\r\n    return self.sample(\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2619, in sample\r\n    outputs = self(\r\n  File \"/home/st/GIT/oobabooga_linux_2/text-generation-webui/modules/exllama_hf.py\"\
    , line 57, in __call__\r\n    self.ex_model.forward(torch.tensor([seq[:-1]], dtype=torch.long),\
    \ cache, preprocess_only=True, lora=self.lora)\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
    , line 860, in forward\r\n    hidden_states = decoder_layer.forward(hidden_states,\
    \ cache, buffers[device], lora)\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
    , line 466, in forward\r\n    hidden_states = self.self_attn.forward(hidden_states,\
    \ cache, buffer, lora)\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
    , line 377, in forward\r\n    query_states = self.q_proj.forward(hidden_states,\
    \ lora)\r\n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/model.py\"\
    , line 195, in forward\r\n    out = cuda_ext.ext_q4_matmul(x, self.q4, self.width)\r\
    \n  File \"/home/st/GIT/oobabooga_linux_2/installer_files/env/lib/python3.10/site-packages/exllama/cuda_ext.py\"\
    , line 49, in ext_q4_matmul\r\n    q4_matmul(x, q4, output)\r\nRuntimeError: CUDA\
    \ error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\r\n\
    Output generated in 0.73 seconds (0.00 tokens/s, 0 tokens, context 9, seed 439549603)\r\
    \n"
  created_at: 2023-06-28 04:25:13+00:00
  edited: false
  hidden: false
  id: 649bc4398f4b425bdeb27459
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2023-06-28T19:51:45.000Z'
    data:
      edited: false
      editors:
      - underlines
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9517759680747986
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
          fullname: Jan Badertscher
          isHf: false
          isPro: false
          name: underlines
          type: user
        html: '<p>I am not sure, but I thought pre layer doesn''t work with ExLlama?</p>

          <p>What value did you set max_seq_len and compress_pos_emb?</p>

          <p>8192 context size currently doesn''t fit into your 24gb vram, and using
          ---pre_layer probably doesn''t work on ExLlama (but I might be wrong)</p>

          '
        raw: 'I am not sure, but I thought pre layer doesn''t work with ExLlama?


          What value did you set max_seq_len and compress_pos_emb?


          8192 context size currently doesn''t fit into your 24gb vram, and using
          ---pre_layer probably doesn''t work on ExLlama (but I might be wrong)'
        updatedAt: '2023-06-28T19:51:45.324Z'
      numEdits: 0
      reactions: []
    id: 649c8f51e89a76501596eece
    type: comment
  author: underlines
  content: 'I am not sure, but I thought pre layer doesn''t work with ExLlama?


    What value did you set max_seq_len and compress_pos_emb?


    8192 context size currently doesn''t fit into your 24gb vram, and using ---pre_layer
    probably doesn''t work on ExLlama (but I might be wrong)'
  created_at: 2023-06-28 18:51:45+00:00
  edited: false
  hidden: false
  id: 649c8f51e89a76501596eece
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-07-01T06:17:56.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9580652117729187
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>For 8K models (using a 4090), I''ve been using 4096 with ExLlama
          and compress_pos_emb set to 2</p>

          <p>Try that...I will try this model soon as well, using that and report
          back.</p>

          '
        raw: 'For 8K models (using a 4090), I''ve been using 4096 with ExLlama and
          compress_pos_emb set to 2


          Try that...I will try this model soon as well, using that and report back.'
        updatedAt: '2023-07-01T06:17:56.691Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 649fc514b248aaaa2d1e2536
    type: comment
  author: cleverest
  content: 'For 8K models (using a 4090), I''ve been using 4096 with ExLlama and compress_pos_emb
    set to 2


    Try that...I will try this model soon as well, using that and report back.'
  created_at: 2023-07-01 05:17:56+00:00
  edited: false
  hidden: false
  id: 649fc514b248aaaa2d1e2536
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-07-01T23:18:06.000Z'
    data:
      edited: true
      editors:
      - cleverest
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9590863585472107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>Confirmed it works fine with the settings I gave above (on 4090,
          should be fine on 3090 too)</p>

          '
        raw: Confirmed it works fine with the settings I gave above (on 4090, should
          be fine on 3090 too)
        updatedAt: '2023-07-10T15:02:30.015Z'
      numEdits: 1
      reactions: []
    id: 64a0b42e90fafb90236444d9
    type: comment
  author: cleverest
  content: Confirmed it works fine with the settings I gave above (on 4090, should
    be fine on 3090 too)
  created_at: 2023-07-01 22:18:06+00:00
  edited: true
  hidden: false
  id: 64a0b42e90fafb90236444d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-08T09:51:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.959510862827301
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>FYI I recently heard from someone that pre_layer does work on ExLlama
          - although the performance is pretty slow, understandably, so it might be
          better to use GGML with KoboldCpp instead.</p>

          '
        raw: FYI I recently heard from someone that pre_layer does work on ExLlama
          - although the performance is pretty slow, understandably, so it might be
          better to use GGML with KoboldCpp instead.
        updatedAt: '2023-07-08T09:51:21.394Z'
      numEdits: 0
      reactions: []
    id: 64a931996e727d56ae95b2aa
    type: comment
  author: TheBloke
  content: FYI I recently heard from someone that pre_layer does work on ExLlama -
    although the performance is pretty slow, understandably, so it might be better
    to use GGML with KoboldCpp instead.
  created_at: 2023-07-08 08:51:21+00:00
  edited: false
  hidden: false
  id: 64a931996e727d56ae95b2aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-07-10T15:03:39.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9807460904121399
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>I''ve noticed ExLlama is sometimes twice as fast as using other
          modes...at least with 30-33b model stuff...not sure how that could be considered
          slow...</p>

          '
        raw: I've noticed ExLlama is sometimes twice as fast as using other modes...at
          least with 30-33b model stuff...not sure how that could be considered slow...
        updatedAt: '2023-07-10T15:03:39.747Z'
      numEdits: 0
      reactions: []
    id: 64ac1dcbaf539898ef3baa75
    type: comment
  author: cleverest
  content: I've noticed ExLlama is sometimes twice as fast as using other modes...at
    least with 30-33b model stuff...not sure how that could be considered slow...
  created_at: 2023-07-10 14:03:39+00:00
  edited: false
  hidden: false
  id: 64ac1dcbaf539898ef3baa75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-10T15:14:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.941352903842926
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The performance with pre_layer, which offloads part of the model
          to RAM instead of putting it on the GPU, will be slow.  Like no more than
          2-3 tokens/s.  That is done when one wants to load a model larger than they
          have VRAM for.</p>

          <p>Yes Exllama is much faster than other methods when no CPU/RAM offloading
          is done.</p>

          '
        raw: 'The performance with pre_layer, which offloads part of the model to
          RAM instead of putting it on the GPU, will be slow.  Like no more than 2-3
          tokens/s.  That is done when one wants to load a model larger than they
          have VRAM for.


          Yes Exllama is much faster than other methods when no CPU/RAM offloading
          is done.'
        updatedAt: '2023-07-10T15:14:33.679Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cleverest
    id: 64ac20595d3d1fa49a998876
    type: comment
  author: TheBloke
  content: 'The performance with pre_layer, which offloads part of the model to RAM
    instead of putting it on the GPU, will be slow.  Like no more than 2-3 tokens/s.  That
    is done when one wants to load a model larger than they have VRAM for.


    Yes Exllama is much faster than other methods when no CPU/RAM offloading is done.'
  created_at: 2023-07-10 14:14:33+00:00
  edited: false
  hidden: false
  id: 64ac20595d3d1fa49a998876
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-07-10T15:29:52.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9496380090713501
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>Yes, I notice this with 65B models despite having 96GB of DDR5 memory,
          but it''s more like 0.6-1 tokens in my case :-(</p>

          '
        raw: Yes, I notice this with 65B models despite having 96GB of DDR5 memory,
          but it's more like 0.6-1 tokens in my case :-(
        updatedAt: '2023-07-10T15:29:52.568Z'
      numEdits: 0
      reactions: []
    id: 64ac23f0ab35a74f73079df8
    type: comment
  author: cleverest
  content: Yes, I notice this with 65B models despite having 96GB of DDR5 memory,
    but it's more like 0.6-1 tokens in my case :-(
  created_at: 2023-07-10 14:29:52+00:00
  edited: false
  hidden: false
  id: 64ac23f0ab35a74f73079df8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ
repo_type: model
status: open
target_branch: null
title: Tips on running on 3090 24gb VRAM?
