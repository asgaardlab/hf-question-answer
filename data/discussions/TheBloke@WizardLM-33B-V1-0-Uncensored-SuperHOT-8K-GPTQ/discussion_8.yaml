!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zxbc2023
conflicting_files: null
created_at: 2023-07-15 21:51:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a0ec35761132f59fcd0016f771cbcb8.svg
      fullname: Zxbc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zxbc2023
      type: user
    createdAt: '2023-07-15T22:51:52.000Z'
    data:
      edited: false
      editors:
      - zxbc2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9610458612442017
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a0ec35761132f59fcd0016f771cbcb8.svg
          fullname: Zxbc
          isHf: false
          isPro: false
          name: zxbc2023
          type: user
        html: '<p>Just to help other folks who happen to stumble on this model, I
          had some severe performance issues until I tweaked my settings using text-generation-webui
          (oobabooga, windows). I have a RTX 2070 with 8GB VRAM and was getting as
          low as 1-2 tokens/s for a while until I changed my load settings. Now I
          am able to get a consistent 10+ tokens/s and sometimes even 20+t/s, and
          performance remains stable in long sessions. All I did was to load the model
          using ExLlama, with max_seq_len 4096 and compress_pos_emb 2. Generation
          starts very fast and there''s hardly any delay, and it''s very usable now.</p>

          <p>Hope this helps others who are struggling with performance.</p>

          '
        raw: "Just to help other folks who happen to stumble on this model, I had\
          \ some severe performance issues until I tweaked my settings using text-generation-webui\
          \ (oobabooga, windows). I have a RTX 2070 with 8GB VRAM and was getting\
          \ as low as 1-2 tokens/s for a while until I changed my load settings. Now\
          \ I am able to get a consistent 10+ tokens/s and sometimes even 20+t/s,\
          \ and performance remains stable in long sessions. All I did was to load\
          \ the model using ExLlama, with max_seq_len 4096 and compress_pos_emb 2.\
          \ Generation starts very fast and there's hardly any delay, and it's very\
          \ usable now.\r\n\r\nHope this helps others who are struggling with performance."
        updatedAt: '2023-07-15T22:51:52.602Z'
      numEdits: 0
      reactions: []
    id: 64b32308a248169796f497b1
    type: comment
  author: zxbc2023
  content: "Just to help other folks who happen to stumble on this model, I had some\
    \ severe performance issues until I tweaked my settings using text-generation-webui\
    \ (oobabooga, windows). I have a RTX 2070 with 8GB VRAM and was getting as low\
    \ as 1-2 tokens/s for a while until I changed my load settings. Now I am able\
    \ to get a consistent 10+ tokens/s and sometimes even 20+t/s, and performance\
    \ remains stable in long sessions. All I did was to load the model using ExLlama,\
    \ with max_seq_len 4096 and compress_pos_emb 2. Generation starts very fast and\
    \ there's hardly any delay, and it's very usable now.\r\n\r\nHope this helps others\
    \ who are struggling with performance."
  created_at: 2023-07-15 21:51:52+00:00
  edited: false
  hidden: false
  id: 64b32308a248169796f497b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8a0ec35761132f59fcd0016f771cbcb8.svg
      fullname: Zxbc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zxbc2023
      type: user
    createdAt: '2023-07-15T23:29:06.000Z'
    data:
      status: closed
    id: 64b32bc2f44fd957490687e7
    type: status-change
  author: zxbc2023
  created_at: 2023-07-15 22:29:06+00:00
  id: 64b32bc2f44fd957490687e7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ
repo_type: model
status: closed
target_branch: null
title: Performance tweaks
