!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kk-envision
conflicting_files: null
created_at: 2023-10-19 15:01:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e181eaa3da702a451c7a65cc0d484bcc.svg
      fullname: Karthik Kannan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kk-envision
      type: user
    createdAt: '2023-10-19T16:01:28.000Z'
    data:
      edited: false
      editors:
      - kk-envision
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5359297394752502
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e181eaa3da702a451c7a65cc0d484bcc.svg
          fullname: Karthik Kannan
          isHf: false
          isPro: false
          name: kk-envision
          type: user
        html: '<p>Hi, </p>

          <p>I''m trying to run the model through HF Inference Endpoints for a quick
          POC. I''m running into this particular issue: </p>

          <p>2023/10/19 17:56:21 ~ INFO | No custom pipeline found at /repository/handler.py<br>2023/10/19
          17:56:21 ~ INFO | Using device GPU<br>2023/10/19 17:56:21 ~ 2023-10-19 15:56:21,563
          | INFO | Initializing model from directory:/repository<br>2023/10/19 17:56:21
          ~ KeyError: ''llava_mistral''<br>2023/10/19 17:56:21 ~ self.pipeline = get_pipeline(model_dir=model_dir,
          task=task)<br>2023/10/19 17:56:21 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,
          task=HF_TASK)<br>2023/10/19 17:56:21 ~ config = AutoConfig.from_pretrained(model,
          _from_pipeline=task, **hub_kwargs, **model_kwargs)<br>2023/10/19 17:56:21
          ~ File "/app/huggingface_inference_toolkit/handler.py", line 17, in <strong>init</strong><br>2023/10/19
          17:56:21 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py",
          line 710, in <strong>getitem</strong><br>2023/10/19 17:56:21 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py",
          line 998, in from_pretrained<br>2023/10/19 17:56:21 ~ File "/app/huggingface_inference_toolkit/handler.py",
          line 45, in get_inference_handler_either_custom_or_default_handler<br>2023/10/19
          17:56:21 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 584, in <strong>aenter</strong><br>2023/10/19 17:56:21 ~ async with
          self.lifespan_context(app) as maybe_state:<br>2023/10/19 17:56:21 ~ File
          "/app/huggingface_inference_toolkit/utils.py", line 261, in get_pipeline<br>2023/10/19
          17:56:21 ~ await handler()<br>2023/10/19 17:56:21 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/<strong>init</strong>.py",
          line 705, in pipeline<br>2023/10/19 17:56:21 ~ await self._router.startup()<br>2023/10/19
          17:56:21 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 705, in lifespan<br>2023/10/19 17:56:21 ~ Traceback (most recent call
          last):<br>2023/10/19 17:56:21 ~ File "/app/webservice_starlette.py", line
          57, in some_startup_task<br>2023/10/19 17:56:21 ~ return HuggingFaceHandler(model_dir=model_dir,
          task=task)<br>2023/10/19 17:56:21 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 682, in startup<br>2023/10/19 17:56:21 ~ Application startup failed.
          Exiting.</p>

          <p>Can you let me know if there''s anything going wrong with my setup? </p>

          <p>Thanks!</p>

          '
        raw: "Hi, \r\n\r\nI'm trying to run the model through HF Inference Endpoints\
          \ for a quick POC. I'm running into this particular issue: \r\n\r\n2023/10/19\
          \ 17:56:21 ~ INFO | No custom pipeline found at /repository/handler.py\r\
          \n2023/10/19 17:56:21 ~ INFO | Using device GPU\r\n2023/10/19 17:56:21 ~\
          \ 2023-10-19 15:56:21,563 | INFO | Initializing model from directory:/repository\r\
          \n2023/10/19 17:56:21 ~ KeyError: 'llava_mistral'\r\n2023/10/19 17:56:21\
          \ ~ self.pipeline = get_pipeline(model_dir=model_dir, task=task)\r\n2023/10/19\
          \ 17:56:21 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,\
          \ task=HF_TASK)\r\n2023/10/19 17:56:21 ~ config = AutoConfig.from_pretrained(model,\
          \ _from_pipeline=task, **hub_kwargs, **model_kwargs)\r\n2023/10/19 17:56:21\
          \ ~ File \"/app/huggingface_inference_toolkit/handler.py\", line 17, in\
          \ __init__\r\n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 710, in __getitem__\r\n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 998, in from_pretrained\r\n2023/10/19 17:56:21 ~ File \"/app/huggingface_inference_toolkit/handler.py\"\
          , line 45, in get_inference_handler_either_custom_or_default_handler\r\n\
          2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
          , line 584, in __aenter__\r\n2023/10/19 17:56:21 ~ async with self.lifespan_context(app)\
          \ as maybe_state:\r\n2023/10/19 17:56:21 ~ File \"/app/huggingface_inference_toolkit/utils.py\"\
          , line 261, in get_pipeline\r\n2023/10/19 17:56:21 ~ await handler()\r\n\
          2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
          , line 705, in pipeline\r\n2023/10/19 17:56:21 ~ await self._router.startup()\r\
          \n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
          , line 705, in lifespan\r\n2023/10/19 17:56:21 ~ Traceback (most recent\
          \ call last):\r\n2023/10/19 17:56:21 ~ File \"/app/webservice_starlette.py\"\
          , line 57, in some_startup_task\r\n2023/10/19 17:56:21 ~ return HuggingFaceHandler(model_dir=model_dir,\
          \ task=task)\r\n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
          , line 682, in startup\r\n2023/10/19 17:56:21 ~ Application startup failed.\
          \ Exiting.\r\n\r\nCan you let me know if there's anything going wrong with\
          \ my setup? \r\n\r\nThanks!"
        updatedAt: '2023-10-19T16:01:28.329Z'
      numEdits: 0
      reactions: []
    id: 653152d842f892bed1702792
    type: comment
  author: kk-envision
  content: "Hi, \r\n\r\nI'm trying to run the model through HF Inference Endpoints\
    \ for a quick POC. I'm running into this particular issue: \r\n\r\n2023/10/19\
    \ 17:56:21 ~ INFO | No custom pipeline found at /repository/handler.py\r\n2023/10/19\
    \ 17:56:21 ~ INFO | Using device GPU\r\n2023/10/19 17:56:21 ~ 2023-10-19 15:56:21,563\
    \ | INFO | Initializing model from directory:/repository\r\n2023/10/19 17:56:21\
    \ ~ KeyError: 'llava_mistral'\r\n2023/10/19 17:56:21 ~ self.pipeline = get_pipeline(model_dir=model_dir,\
    \ task=task)\r\n2023/10/19 17:56:21 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,\
    \ task=HF_TASK)\r\n2023/10/19 17:56:21 ~ config = AutoConfig.from_pretrained(model,\
    \ _from_pipeline=task, **hub_kwargs, **model_kwargs)\r\n2023/10/19 17:56:21 ~\
    \ File \"/app/huggingface_inference_toolkit/handler.py\", line 17, in __init__\r\
    \n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 710, in __getitem__\r\n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 998, in from_pretrained\r\n2023/10/19 17:56:21 ~ File \"/app/huggingface_inference_toolkit/handler.py\"\
    , line 45, in get_inference_handler_either_custom_or_default_handler\r\n2023/10/19\
    \ 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
    , line 584, in __aenter__\r\n2023/10/19 17:56:21 ~ async with self.lifespan_context(app)\
    \ as maybe_state:\r\n2023/10/19 17:56:21 ~ File \"/app/huggingface_inference_toolkit/utils.py\"\
    , line 261, in get_pipeline\r\n2023/10/19 17:56:21 ~ await handler()\r\n2023/10/19\
    \ 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
    , line 705, in pipeline\r\n2023/10/19 17:56:21 ~ await self._router.startup()\r\
    \n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
    , line 705, in lifespan\r\n2023/10/19 17:56:21 ~ Traceback (most recent call last):\r\
    \n2023/10/19 17:56:21 ~ File \"/app/webservice_starlette.py\", line 57, in some_startup_task\r\
    \n2023/10/19 17:56:21 ~ return HuggingFaceHandler(model_dir=model_dir, task=task)\r\
    \n2023/10/19 17:56:21 ~ File \"/opt/conda/lib/python3.9/site-packages/starlette/routing.py\"\
    , line 682, in startup\r\n2023/10/19 17:56:21 ~ Application startup failed. Exiting.\r\
    \n\r\nCan you let me know if there's anything going wrong with my setup? \r\n\r\
    \nThanks!"
  created_at: 2023-10-19 15:01:28+00:00
  edited: false
  hidden: false
  id: 653152d842f892bed1702792
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6a6d922028e8b447b87fca56c00c679.svg
      fullname: T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kopyl
      type: user
    createdAt: '2023-10-25T00:19:33.000Z'
    data:
      edited: false
      editors:
      - kopyl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9807282090187073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6a6d922028e8b447b87fca56c00c679.svg
          fullname: T
          isHf: false
          isPro: false
          name: kopyl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kk-envision&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kk-envision\"\
          >@<span class=\"underline\">kk-envision</span></a></span>\n\n\t</span></span>\
          \ i made a notebook which you can use for the inference.</p>\n<p>I have\
          \ no idea what is \"Inference Endpoints\", but you can make your own API\
          \ with this.</p>\n<p>Let me know if this was helpful.</p>\n"
        raw: '@kk-envision i made a notebook which you can use for the inference.


          I have no idea what is "Inference Endpoints", but you can make your own
          API with this.


          Let me know if this was helpful.'
        updatedAt: '2023-10-25T00:19:33.892Z'
      numEdits: 0
      reactions: []
    id: 65385f1561d44e23f3d62a08
    type: comment
  author: kopyl
  content: '@kk-envision i made a notebook which you can use for the inference.


    I have no idea what is "Inference Endpoints", but you can make your own API with
    this.


    Let me know if this was helpful.'
  created_at: 2023-10-24 23:19:33+00:00
  edited: false
  hidden: false
  id: 65385f1561d44e23f3d62a08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-22T23:01:18.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8209896683692932
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kk-envision&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kk-envision\"\
          >@<span class=\"underline\">kk-envision</span></a></span>\n\n\t</span></span>\
          \ simply put transformers doesnt have mistral_llava support. You have to\
          \ use the bakllava repository</p>\n"
        raw: '@kk-envision simply put transformers doesnt have mistral_llava support.
          You have to use the bakllava repository'
        updatedAt: '2023-11-22T23:01:18.421Z'
      numEdits: 0
      reactions: []
    id: 655e883e48695217d5134e61
    type: comment
  author: YaTharThShaRma999
  content: '@kk-envision simply put transformers doesnt have mistral_llava support.
    You have to use the bakllava repository'
  created_at: 2023-11-22 23:01:18+00:00
  edited: false
  hidden: false
  id: 655e883e48695217d5134e61
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: SkunkworksAI/BakLLaVA-1
repo_type: model
status: open
target_branch: null
title: Inference Endpoints throwing an error
