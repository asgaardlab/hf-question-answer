!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Akimbofmg9
conflicting_files: null
created_at: 2023-12-19 08:04:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7d6a796fa79a30261411bac515bb7722.svg
      fullname: Nilesh J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akimbofmg9
      type: user
    createdAt: '2023-12-19T08:04:18.000Z'
    data:
      edited: false
      editors:
      - Akimbofmg9
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9422653913497925
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7d6a796fa79a30261411bac515bb7722.svg
          fullname: Nilesh J
          isHf: false
          isPro: false
          name: Akimbofmg9
          type: user
        html: '<p>Hello, </p>

          <p>I have a list of api call sequences from emulated malware logs, I want
          to classify the call sequences with their arguments, so I need embeddings.
          I chose this as this model can code.</p>

          <p> I plan on using these embeddings in a Graph transformer, but my question
          is, how do i query get the model to generate embeddings for me. </p>

          <p>Im using the feature extraction pipeline, but the issue is the context
          length, I have strings of API calls more than 3k in count. How would I make
          it work?<br>I''m pretty new to this so please dumb it down a little.<br>Any
          clues? </p>

          '
        raw: "Hello, \r\n\r\nI have a list of api call sequences from emulated malware\
          \ logs, I want to classify the call sequences with their arguments, so I\
          \ need embeddings. I chose this as this model can code.\r\n\r\n I plan on\
          \ using these embeddings in a Graph transformer, but my question is, how\
          \ do i query get the model to generate embeddings for me. \r\n\r\nIm using\
          \ the feature extraction pipeline, but the issue is the context length,\
          \ I have strings of API calls more than 3k in count. How would I make it\
          \ work?\r\nI'm pretty new to this so please dumb it down a little.\r\nAny\
          \ clues? "
        updatedAt: '2023-12-19T08:04:18.318Z'
      numEdits: 0
      reactions: []
    id: 65814e828ecea67c7d7777a9
    type: comment
  author: Akimbofmg9
  content: "Hello, \r\n\r\nI have a list of api call sequences from emulated malware\
    \ logs, I want to classify the call sequences with their arguments, so I need\
    \ embeddings. I chose this as this model can code.\r\n\r\n I plan on using these\
    \ embeddings in a Graph transformer, but my question is, how do i query get the\
    \ model to generate embeddings for me. \r\n\r\nIm using the feature extraction\
    \ pipeline, but the issue is the context length, I have strings of API calls more\
    \ than 3k in count. How would I make it work?\r\nI'm pretty new to this so please\
    \ dumb it down a little.\r\nAny clues? "
  created_at: 2023-12-19 08:04:18+00:00
  edited: false
  hidden: false
  id: 65814e828ecea67c7d7777a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efe0eabd99455ada4da1ff729f632c83.svg
      fullname: "Ren\xE9 Peinl"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rpeinl
      type: user
    createdAt: '2023-12-20T11:12:19.000Z'
    data:
      edited: true
      editors:
      - rpeinl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6497142910957336
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efe0eabd99455ada4da1ff729f632c83.svg
          fullname: "Ren\xE9 Peinl"
          isHf: false
          isPro: false
          name: rpeinl
          type: user
        html: '<p>I''m trying to run the Mixtral models like dolphin here with the
          provided code, but it seems that it is not yet supported in transformers.
          I upgraded to the latest version as suggested (transformers-4.37.0.dev0)
          but it still gives me the following error<br>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566,<br>ValueError(<br>    570     f"Unrecognized
          configuration class {config.<strong>class</strong>} for this kind of AutoModel:
          {cls.<strong>name</strong>}.\n"<br>    571     f"Model type should be one
          of {'', ''.join(c.<strong>name</strong> for c in cls._model_mapping.keys())}."<br>    572
          )<br>...<br>/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py:68,
          in QuantLinear.<strong>init</strong>(self, bits, group_size, infeatures,
          outfeatures, bias, trainable, **kwargs)<br>     66 assert infeatures % 32
          == 0<br>     67 assert infeatures % self.group_size == 0<br>---&gt; 68 assert
          outfeatures % 32 == 0<br>     70 self.register_buffer(<br>     71     ''qweight'',<br>     72     torch.zeros((infeatures
          // 32 * self.bits, outfeatures), dtype=torch.int32)<br>     73 )<br>     74
          self.register_buffer(<br>     75     ''qzeros'',<br>     76     torch.zeros((math.ceil(infeatures
          / self.group_size), outfeatures // 32 * self.bits), dtype=torch.int32)<br>     77
          )</p>

          <p>AssertionError: </p>

          '
        raw: "I'm trying to run the Mixtral models like dolphin here with the provided\
          \ code, but it seems that it is not yet supported in transformers. I upgraded\
          \ to the latest version as suggested (transformers-4.37.0.dev0) but it still\
          \ gives me the following error\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566,\n\
          ValueError(\n    570     f\"Unrecognized configuration class {config.__class__}\
          \ for this kind of AutoModel: {cls.__name__}.\\n\"\n    571     f\"Model\
          \ type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
          \n    572 )\n...\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py:68,\
          \ in QuantLinear.__init__(self, bits, group_size, infeatures, outfeatures,\
          \ bias, trainable, **kwargs)\n     66 assert infeatures % 32 == 0\n    \
          \ 67 assert infeatures % self.group_size == 0\n---> 68 assert outfeatures\
          \ % 32 == 0\n     70 self.register_buffer(\n     71     'qweight',\n   \
          \  72     torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32)\n\
          \     73 )\n     74 self.register_buffer(\n     75     'qzeros',\n     76\
          \     torch.zeros((math.ceil(infeatures / self.group_size), outfeatures\
          \ // 32 * self.bits), dtype=torch.int32)\n     77 )\n\nAssertionError: "
        updatedAt: '2023-12-20T11:12:54.390Z'
      numEdits: 1
      reactions: []
    id: 6582cc13c3fece72a66724cc
    type: comment
  author: rpeinl
  content: "I'm trying to run the Mixtral models like dolphin here with the provided\
    \ code, but it seems that it is not yet supported in transformers. I upgraded\
    \ to the latest version as suggested (transformers-4.37.0.dev0) but it still gives\
    \ me the following error\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566,\n\
    ValueError(\n    570     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\n    571     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \n    572 )\n...\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py:68,\
    \ in QuantLinear.__init__(self, bits, group_size, infeatures, outfeatures, bias,\
    \ trainable, **kwargs)\n     66 assert infeatures % 32 == 0\n     67 assert infeatures\
    \ % self.group_size == 0\n---> 68 assert outfeatures % 32 == 0\n     70 self.register_buffer(\n\
    \     71     'qweight',\n     72     torch.zeros((infeatures // 32 * self.bits,\
    \ outfeatures), dtype=torch.int32)\n     73 )\n     74 self.register_buffer(\n\
    \     75     'qzeros',\n     76     torch.zeros((math.ceil(infeatures / self.group_size),\
    \ outfeatures // 32 * self.bits), dtype=torch.int32)\n     77 )\n\nAssertionError: "
  created_at: 2023-12-20 11:12:19+00:00
  edited: true
  hidden: false
  id: 6582cc13c3fece72a66724cc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/dolphin-2.5-mixtral-8x7b-GPTQ
repo_type: model
status: open
target_branch: null
title: Malware api call sequences embeddings
