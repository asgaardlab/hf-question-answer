!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vijaysb
conflicting_files: null
created_at: 2023-06-04 06:28:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
      fullname: vijay bhadrashetti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijaysb
      type: user
    createdAt: '2023-06-04T07:28:54.000Z'
    data:
      edited: true
      editors:
      - vijaysb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5745809078216553
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
          fullname: vijay bhadrashetti
          isHf: false
          isPro: false
          name: vijaysb
          type: user
        html: "<p>Model trying to load: guanaco-65B.ggmlv3.q4_0.bin<br>Machine: Mac\
          \ m1 max, 64GB RAM</p>\n<hr>\n<h2 id=\"error-in-webui\">Error in WebUI:</h2>\n\
          <p>Traceback (most recent call last): File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 18, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019</p>\n<p>During handling of the\
          \ above exception, another exception occurred:<br>Traceback (most recent\
          \ call last): File \u201C/Users/vij/development/text-generation-webui/server.py\u201D\
          , line 71, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 97, in load_model output = load_func(model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 289, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 22, in sys.exit(-1) SystemExit: -1</p>\n<p>Had the same error previously\
          \ with guanaco-65B-GPTQ, <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ suggested me to use the GGML version to work on Mac, Even if I select\
          \ GGML version, for some reason it is still loading the GPTQ version. Am\
          \ i missing anything?</p>\n"
        raw: "Model trying to load: guanaco-65B.ggmlv3.q4_0.bin\nMachine: Mac m1 max,\
          \ 64GB RAM\n\n-------------------------\nError in WebUI:\n-------------------------\n\
          Traceback (most recent call last): File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 18, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019\n\nDuring handling of the above\
          \ exception, another exception occurred:\nTraceback (most recent call last):\
          \ File \u201C/Users/vij/development/text-generation-webui/server.py\u201D\
          , line 71, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 97, in load_model output = load_func(model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 289, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 22, in sys.exit(-1) SystemExit: -1\n\n\nHad the same error previously\
          \ with guanaco-65B-GPTQ, @TheBloke suggested me to use the GGML version\
          \ to work on Mac, Even if I select GGML version, for some reason it is still\
          \ loading the GPTQ version. Am i missing anything?"
        updatedAt: '2023-06-04T07:30:57.719Z'
      numEdits: 1
      reactions: []
    id: 647c3d361b6ecd15f4b9be72
    type: comment
  author: vijaysb
  content: "Model trying to load: guanaco-65B.ggmlv3.q4_0.bin\nMachine: Mac m1 max,\
    \ 64GB RAM\n\n-------------------------\nError in WebUI:\n-------------------------\n\
    Traceback (most recent call last): File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 18, in import llama_inference_offload ModuleNotFoundError: No module named\
    \ \u2018llama_inference_offload\u2019\n\nDuring handling of the above exception,\
    \ another exception occurred:\nTraceback (most recent call last): File \u201C\
    /Users/vij/development/text-generation-webui/server.py\u201D, line 71, in load_model_wrapper\
    \ shared.model, shared.tokenizer = load_model(shared.model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
    , line 97, in load_model output = load_func(model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
    , line 289, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 22, in sys.exit(-1) SystemExit: -1\n\n\nHad the same error previously with\
    \ guanaco-65B-GPTQ, @TheBloke suggested me to use the GGML version to work on\
    \ Mac, Even if I select GGML version, for some reason it is still loading the\
    \ GPTQ version. Am i missing anything?"
  created_at: 2023-06-04 06:28:54+00:00
  edited: true
  hidden: false
  id: 647c3d361b6ecd15f4b9be72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
      fullname: vijay bhadrashetti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijaysb
      type: user
    createdAt: '2023-06-04T07:41:10.000Z'
    data:
      edited: false
      editors:
      - vijaysb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3493257164955139
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
          fullname: vijay bhadrashetti
          isHf: false
          isPro: false
          name: vijaysb
          type: user
        html: '<p>I think i''m running out of memory too.</p>

          <hr>

          <h2 id="error-in-command-prompt">Error in command prompt:</h2>

          <p>INFO:Loading TheBloke_guanaco-65B-GGML...<br>INFO:llama.cpp weights detected:
          models/TheBloke_guanaco-65B-GGML/guanaco-65B.ggmlv3.q4_1.bin</p>

          <p>INFO:Cache capacity is 0 bytes<br>llama.cpp: loading model from models/TheBloke_guanaco-65B-GGML/guanaco-65B.ggmlv3.q4_1.bin<br>llama_model_load_internal:
          format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab    =
          32000<br>llama_model_load_internal: n_ctx      = 2048<br>llama_model_load_internal:
          n_embd     = 8192<br>llama_model_load_internal: n_mult     = 256<br>llama_model_load_internal:
          n_head     = 64<br>llama_model_load_internal: n_layer    = 80<br>llama_model_load_internal:
          n_rot      = 128<br>llama_model_load_internal: ftype      = 3 (mostly Q4_1)<br>llama_model_load_internal:
          n_ff       = 22016<br>llama_model_load_internal: n_parts    = 1<br>llama_model_load_internal:
          model size = 65B<br>llama_model_load_internal: ggml ctx size =    0.00 MB<br>error
          loading model: llama.cpp: tensor ''layers.1.ffn_norm.weight'' is missing
          from model<br>llama_init_from_file: failed to load model<br>Exception ignored
          in: &lt;function LlamaCppModel.__del__ at 0x16a6b7520&gt;<br>Traceback (most
          recent call last):<br>  File "/Users/vij/development/text-generation-webui/modules/llamacpp_model.py",
          line 23, in <strong>del</strong><br>    self.model.<strong>del</strong>()<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''<br>INFO:Loading TheBloke_guanaco-65B-GGML...<br>ERROR:Failed
          to load GPTQ-for-LLaMa<br>ERROR:See <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a></p>

          '
        raw: "I think i'm running out of memory too.\n\n\n\n-------------------------------------\n\
          Error in command prompt:\n-------------------------------------\n\nINFO:Loading\
          \ TheBloke_guanaco-65B-GGML...\nINFO:llama.cpp weights detected: models/TheBloke_guanaco-65B-GGML/guanaco-65B.ggmlv3.q4_1.bin\n\
          \nINFO:Cache capacity is 0 bytes\nllama.cpp: loading model from models/TheBloke_guanaco-65B-GGML/guanaco-65B.ggmlv3.q4_1.bin\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 8192\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 64\nllama_model_load_internal: n_layer    = 80\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 3 (mostly Q4_1)\n\
          llama_model_load_internal: n_ff       = 22016\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 65B\nllama_model_load_internal:\
          \ ggml ctx size =    0.00 MB\nerror loading model: llama.cpp: tensor 'layers.1.ffn_norm.weight'\
          \ is missing from model\nllama_init_from_file: failed to load model\nException\
          \ ignored in: <function LlamaCppModel.__del__ at 0x16a6b7520>\nTraceback\
          \ (most recent call last):\n  File \"/Users/vij/development/text-generation-webui/modules/llamacpp_model.py\"\
          , line 23, in __del__\n    self.model.__del__()\nAttributeError: 'LlamaCppModel'\
          \ object has no attribute 'model'\nINFO:Loading TheBloke_guanaco-65B-GGML...\n\
          ERROR:Failed to load GPTQ-for-LLaMa\nERROR:See https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md"
        updatedAt: '2023-06-04T07:41:10.130Z'
      numEdits: 0
      reactions: []
    id: 647c4016c788767ab5c4cf40
    type: comment
  author: vijaysb
  content: "I think i'm running out of memory too.\n\n\n\n-------------------------------------\n\
    Error in command prompt:\n-------------------------------------\n\nINFO:Loading\
    \ TheBloke_guanaco-65B-GGML...\nINFO:llama.cpp weights detected: models/TheBloke_guanaco-65B-GGML/guanaco-65B.ggmlv3.q4_1.bin\n\
    \nINFO:Cache capacity is 0 bytes\nllama.cpp: loading model from models/TheBloke_guanaco-65B-GGML/guanaco-65B.ggmlv3.q4_1.bin\n\
    llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
    \ n_embd     = 8192\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 64\nllama_model_load_internal: n_layer    = 80\nllama_model_load_internal:\
    \ n_rot      = 128\nllama_model_load_internal: ftype      = 3 (mostly Q4_1)\n\
    llama_model_load_internal: n_ff       = 22016\nllama_model_load_internal: n_parts\
    \    = 1\nllama_model_load_internal: model size = 65B\nllama_model_load_internal:\
    \ ggml ctx size =    0.00 MB\nerror loading model: llama.cpp: tensor 'layers.1.ffn_norm.weight'\
    \ is missing from model\nllama_init_from_file: failed to load model\nException\
    \ ignored in: <function LlamaCppModel.__del__ at 0x16a6b7520>\nTraceback (most\
    \ recent call last):\n  File \"/Users/vij/development/text-generation-webui/modules/llamacpp_model.py\"\
    , line 23, in __del__\n    self.model.__del__()\nAttributeError: 'LlamaCppModel'\
    \ object has no attribute 'model'\nINFO:Loading TheBloke_guanaco-65B-GGML...\n\
    ERROR:Failed to load GPTQ-for-LLaMa\nERROR:See https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md"
  created_at: 2023-06-04 06:41:10+00:00
  edited: false
  hidden: false
  id: 647c4016c788767ab5c4cf40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T09:49:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.813610315322876
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah you can''t use GPTQ on macOS.</p>

          <p>I imagine you have the parameters set wrong.  You don''t set GPTQ parameters
          for GGML models.  Leave all the GPTQ parameters at "None".  Then text-gen-ui
          will load this model as a GGML model on CPU.</p>

          '
        raw: 'Yeah you can''t use GPTQ on macOS.


          I imagine you have the parameters set wrong.  You don''t set GPTQ parameters
          for GGML models.  Leave all the GPTQ parameters at "None".  Then text-gen-ui
          will load this model as a GGML model on CPU.'
        updatedAt: '2023-06-04T09:49:57.571Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vijaysb
    id: 647c5e451c0644de8d20b8c1
    type: comment
  author: TheBloke
  content: 'Yeah you can''t use GPTQ on macOS.


    I imagine you have the parameters set wrong.  You don''t set GPTQ parameters for
    GGML models.  Leave all the GPTQ parameters at "None".  Then text-gen-ui will
    load this model as a GGML model on CPU.'
  created_at: 2023-06-04 08:49:57+00:00
  edited: false
  hidden: false
  id: 647c5e451c0644de8d20b8c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/guanaco-65B-GGML
repo_type: model
status: open
target_branch: null
title: "ModuleNotFoundError: No module named \u2018llama_inference_offload\u2019 on\
  \ Mac m1 chip"
