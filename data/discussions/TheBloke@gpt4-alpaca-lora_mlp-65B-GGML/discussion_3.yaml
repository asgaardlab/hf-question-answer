!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nicostouch
conflicting_files: null
created_at: 2023-05-15 09:44:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0a890eb2d2f0268a5d32e232d4706b7.svg
      fullname: NST
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicostouch
      type: user
    createdAt: '2023-05-15T10:44:29.000Z'
    data:
      edited: false
      editors:
      - nicostouch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0a890eb2d2f0268a5d32e232d4706b7.svg
          fullname: NST
          isHf: false
          isPro: false
          name: nicostouch
          type: user
        html: '<p>Since llama.cpp using the new GGML format can split between CPU
          and GPU the performance of the q4_0 model offloading 40 layers to an RTX4090
          and running the rest on 79003DX gets just under 2 tokens/s, which is just
          shy of feeling usable. I was wondering if it''s possible to do 3-Bit quantization
          and if the trade-off in perplexity / speed might provide better output than
          the 30B models, while running at decent speed?</p>

          '
        raw: Since llama.cpp using the new GGML format can split between CPU and GPU
          the performance of the q4_0 model offloading 40 layers to an RTX4090 and
          running the rest on 79003DX gets just under 2 tokens/s, which is just shy
          of feeling usable. I was wondering if it's possible to do 3-Bit quantization
          and if the trade-off in perplexity / speed might provide better output than
          the 30B models, while running at decent speed?
        updatedAt: '2023-05-15T10:44:29.121Z'
      numEdits: 0
      reactions: []
    id: 64620d0da1fee75a8c672796
    type: comment
  author: nicostouch
  content: Since llama.cpp using the new GGML format can split between CPU and GPU
    the performance of the q4_0 model offloading 40 layers to an RTX4090 and running
    the rest on 79003DX gets just under 2 tokens/s, which is just shy of feeling usable.
    I was wondering if it's possible to do 3-Bit quantization and if the trade-off
    in perplexity / speed might provide better output than the 30B models, while running
    at decent speed?
  created_at: 2023-05-15 09:44:29+00:00
  edited: false
  hidden: false
  id: 64620d0da1fee75a8c672796
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-15T21:57:32.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah maybe. GPTQ does support 3bit quantisation (for CUDA only,
          not Triton).  I haven''t tested it, and have a feeling that very few people
          have and so there''s quite possibly going to be issues and bugs.</p>

          <p>But I will make a note to try it out with AutoGPTQ sometime soon and
          see how it goes.</p>

          '
        raw: 'Yeah maybe. GPTQ does support 3bit quantisation (for CUDA only, not
          Triton).  I haven''t tested it, and have a feeling that very few people
          have and so there''s quite possibly going to be issues and bugs.


          But I will make a note to try it out with AutoGPTQ sometime soon and see
          how it goes.'
        updatedAt: '2023-05-15T21:57:41.568Z'
      numEdits: 1
      reactions: []
    id: 6462aacc904bbc4cf2e2bc39
    type: comment
  author: TheBloke
  content: 'Yeah maybe. GPTQ does support 3bit quantisation (for CUDA only, not Triton).  I
    haven''t tested it, and have a feeling that very few people have and so there''s
    quite possibly going to be issues and bugs.


    But I will make a note to try it out with AutoGPTQ sometime soon and see how it
    goes.'
  created_at: 2023-05-15 20:57:32+00:00
  edited: true
  hidden: false
  id: 6462aacc904bbc4cf2e2bc39
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/gpt4-alpaca-lora_mlp-65B-GGML
repo_type: model
status: open
target_branch: null
title: 3-Bit quantization?
