!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mirek190
conflicting_files: null
created_at: 2023-05-12 21:46:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-12T22:46:18.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I can say this 65B model  writes  stories much better ( complex
          and long sentences ) than  dromedary-lora-65B-q5_1 ... far more better...<br>Something
          closer to alpaca-lora 65B which is  a bit worse than this model anyway...
          actually reading now what I got from gpt4-alpaca-lora_mlp-65B ...damm this
          is good :D</p>

          <p>Even with the match problems is good.<br>Negative numbers are difficult
          for smaller llama models but here is done ... well.</p>

          <blockquote>

          <p>solve this equation and explain each step 2Y-12=-16<br>The equation is
          already in standard form. We can solve it by adding 12 to both sides of
          the equation, isolating Y on one side, and then applying the inverse operation
          of subtraction. Here are the steps:</p>

          </blockquote>

          <ol>

          <li>Start with 2Y - 12 = -16</li>

          <li>Adding 12 to both sides, we get: 2Y = -4</li>

          <li>Now dividing both sides by 2, we have: Y = -2.</li>

          </ol>

          <p>** Note that the inverse operation of subtraction is addition and the
          inverse operation of multiplication is division. **</p>

          <p>In conclusion:<br> Dromedary-lora-65B is not even worth to keep on my
          SSD  :P<br>Alpaca-lora 65B is better than dromedary-lora-65B and has no
          restrictions at all , zero censorship... REALLY zero. Even some small models
          with uncensored in name has limits....I tried hard ...   For test purposes
          such questions would probably send me straight to jail ... :D<br>Gpt4-alpaca-lora_mlp-65B
          is the best so far but is censored not badly but censored....NSFW works
          at least :P</p>

          <p>I''m happy is something happening with 65B models which are more capable
          in everything plus better reasoning.</p>

          '
        raw: "I can say this 65B model  writes  stories much better ( complex and\
          \ long sentences ) than  dromedary-lora-65B-q5_1 ... far more better...\
          \  \nSomething closer to alpaca-lora 65B which is  a bit worse than this\
          \ model anyway... actually reading now what I got from gpt4-alpaca-lora_mlp-65B\
          \ ...damm this is good :D\n\nEven with the match problems is good.\nNegative\
          \ numbers are difficult for smaller llama models but here is done ... well.\n\
          \n> solve this equation and explain each step 2Y-12=-16\nThe equation is\
          \ already in standard form. We can solve it by adding 12 to both sides of\
          \ the equation, isolating Y on one side, and then applying the inverse operation\
          \ of subtraction. Here are the steps:\n\n1) Start with 2Y - 12 = -16\n2)\
          \ Adding 12 to both sides, we get: 2Y = -4\n3) Now dividing both sides by\
          \ 2, we have: Y = -2.\n\n** Note that the inverse operation of subtraction\
          \ is addition and the inverse operation of multiplication is division. **\n\
          \nIn conclusion:\n Dromedary-lora-65B is not even worth to keep on my SSD\
          \  :P\nAlpaca-lora 65B is better than dromedary-lora-65B and has no restrictions\
          \ at all , zero censorship... REALLY zero. Even some small models with uncensored\
          \ in name has limits....I tried hard ...   For test purposes such questions\
          \ would probably send me straight to jail ... :D\nGpt4-alpaca-lora_mlp-65B\
          \ is the best so far but is censored not badly but censored....NSFW works\
          \ at least :P\n\nI'm happy is something happening with 65B models which\
          \ are more capable in everything plus better reasoning."
        updatedAt: '2023-05-13T11:16:01.757Z'
      numEdits: 6
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Okki
        - creative420
    id: 645ec1ba7c8ed63e83eb19b2
    type: comment
  author: mirek190
  content: "I can say this 65B model  writes  stories much better ( complex and long\
    \ sentences ) than  dromedary-lora-65B-q5_1 ... far more better...  \nSomething\
    \ closer to alpaca-lora 65B which is  a bit worse than this model anyway... actually\
    \ reading now what I got from gpt4-alpaca-lora_mlp-65B ...damm this is good :D\n\
    \nEven with the match problems is good.\nNegative numbers are difficult for smaller\
    \ llama models but here is done ... well.\n\n> solve this equation and explain\
    \ each step 2Y-12=-16\nThe equation is already in standard form. We can solve\
    \ it by adding 12 to both sides of the equation, isolating Y on one side, and\
    \ then applying the inverse operation of subtraction. Here are the steps:\n\n\
    1) Start with 2Y - 12 = -16\n2) Adding 12 to both sides, we get: 2Y = -4\n3) Now\
    \ dividing both sides by 2, we have: Y = -2.\n\n** Note that the inverse operation\
    \ of subtraction is addition and the inverse operation of multiplication is division.\
    \ **\n\nIn conclusion:\n Dromedary-lora-65B is not even worth to keep on my SSD\
    \  :P\nAlpaca-lora 65B is better than dromedary-lora-65B and has no restrictions\
    \ at all , zero censorship... REALLY zero. Even some small models with uncensored\
    \ in name has limits....I tried hard ...   For test purposes such questions would\
    \ probably send me straight to jail ... :D\nGpt4-alpaca-lora_mlp-65B is the best\
    \ so far but is censored not badly but censored....NSFW works at least :P\n\n\
    I'm happy is something happening with 65B models which are more capable in everything\
    \ plus better reasoning."
  created_at: 2023-05-12 21:46:18+00:00
  edited: true
  hidden: false
  id: 645ec1ba7c8ed63e83eb19b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-12T22:48:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>How do you always know I''ve uploaded models before I''ve even told
          anyone? :)</p>

          <p>Anyway, thanks for the feedback! I''ve not even tried it myself yet.  Glad
          to hear it''s good for creative, because Dromedary was awful for that IMHO.</p>

          '
        raw: 'How do you always know I''ve uploaded models before I''ve even told
          anyone? :)


          Anyway, thanks for the feedback! I''ve not even tried it myself yet.  Glad
          to hear it''s good for creative, because Dromedary was awful for that IMHO.'
        updatedAt: '2023-05-12T22:48:05.006Z'
      numEdits: 0
      reactions: []
    id: 645ec2256d343f4bb6152c5b
    type: comment
  author: TheBloke
  content: 'How do you always know I''ve uploaded models before I''ve even told anyone?
    :)


    Anyway, thanks for the feedback! I''ve not even tried it myself yet.  Glad to
    hear it''s good for creative, because Dromedary was awful for that IMHO.'
  created_at: 2023-05-12 21:48:05+00:00
  edited: false
  hidden: false
  id: 645ec2256d343f4bb6152c5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80b38d74d7f44657b8793e8bd512fac0.svg
      fullname: AM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nulled
      type: user
    createdAt: '2023-05-13T05:31:43.000Z'
    data:
      edited: false
      editors:
      - nulled
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80b38d74d7f44657b8793e8bd512fac0.svg
          fullname: AM
          isHf: false
          isPro: false
          name: nulled
          type: user
        html: '<p>Does it work with 32GB RAM, even if super slow?</p>

          '
        raw: Does it work with 32GB RAM, even if super slow?
        updatedAt: '2023-05-13T05:31:43.421Z'
      numEdits: 0
      reactions: []
    id: 645f20bfc4304442f1183de9
    type: comment
  author: nulled
  content: Does it work with 32GB RAM, even if super slow?
  created_at: 2023-05-13 04:31:43+00:00
  edited: false
  hidden: false
  id: 645f20bfc4304442f1183de9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/513eb8a60b47c6ad7a3dbc8230c8c807.svg
      fullname: Okki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Okki
      type: user
    createdAt: '2023-05-13T09:24:56.000Z'
    data:
      edited: false
      editors:
      - Okki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/513eb8a60b47c6ad7a3dbc8230c8c807.svg
          fullname: Okki
          isHf: false
          isPro: false
          name: Okki
          type: user
        html: "<p>I can't say for sure about this model, but usually 65B requires\
          \ ~38Gb ram, however, you'd better ask  <span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mirek190\"\
          >@<span class=\"underline\">mirek190</span></a></span>\n\n\t</span></span>\
          \ .</p>\n"
        raw: I can't say for sure about this model, but usually 65B requires ~38Gb
          ram, however, you'd better ask  @mirek190 .
        updatedAt: '2023-05-13T09:24:56.470Z'
      numEdits: 0
      reactions: []
    id: 645f5768d27fa024aa834ad3
    type: comment
  author: Okki
  content: I can't say for sure about this model, but usually 65B requires ~38Gb ram,
    however, you'd better ask  @mirek190 .
  created_at: 2023-05-13 08:24:56+00:00
  edited: false
  hidden: false
  id: 645f5768d27fa024aa834ad3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd408483dcf2e6c113405476a004b625.svg
      fullname: Jinhyeok Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zenyr
      type: user
    createdAt: '2023-05-13T09:26:48.000Z'
    data:
      edited: false
      editors:
      - zenyr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd408483dcf2e6c113405476a004b625.svg
          fullname: Jinhyeok Lee
          isHf: false
          isPro: false
          name: zenyr
          type: user
        html: '<blockquote>

          <p>Does it work with 32GB RAM, even if super slow?</p>

          </blockquote>

          <p>I''d say with huge amount of swap memory (i.e. your SSD) it is not <em>impossible</em>
          but <strong>implausible</strong> per se.</p>

          '
        raw: '> Does it work with 32GB RAM, even if super slow?


          I''d say with huge amount of swap memory (i.e. your SSD) it is not *impossible*
          but **implausible** per se.'
        updatedAt: '2023-05-13T09:26:48.247Z'
      numEdits: 0
      reactions: []
    id: 645f57d898464f9fcbfdc1b7
    type: comment
  author: zenyr
  content: '> Does it work with 32GB RAM, even if super slow?


    I''d say with huge amount of swap memory (i.e. your SSD) it is not *impossible*
    but **implausible** per se.'
  created_at: 2023-05-13 08:26:48+00:00
  edited: false
  hidden: false
  id: 645f57d898464f9fcbfdc1b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T09:28:05.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can see the RAM requirements in my README</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png"></a></p>

          <p>You technically could load the model with 32GB RAM if  you have enough
          swap space.  But it will be unbelievably, unpleasantly slow.  I would not
          recommend it at all.  It would take tens of minutes to load the model, and
          might take a minute or more to generate a single token.</p>

          <p>Try a 30B model instead.  </p>

          <p>Or if you really want to use 65B, you could try it in the cloud. Microsoft
          Azure and Google Compute both give free credits to new users - $200 from
          Azure, $300 from Google Compute. You can''t get GPU machines for free, but
          you can get decent CPU-only servers, eg a Linux server with 16 cores and
          128GB RAM.  That would be usable with this model.</p>

          '
        raw: "You can see the RAM requirements in my README\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png)\n\
          \nYou technically could load the model with 32GB RAM if  you have enough\
          \ swap space.  But it will be unbelievably, unpleasantly slow.  I would\
          \ not recommend it at all.  It would take tens of minutes to load the model,\
          \ and might take a minute or more to generate a single token.\n\nTry a 30B\
          \ model instead.  \n\nOr if you really want to use 65B, you could try it\
          \ in the cloud. Microsoft Azure and Google Compute both give free credits\
          \ to new users - $200 from Azure, $300 from Google Compute. You can't get\
          \ GPU machines for free, but you can get decent CPU-only servers, eg a Linux\
          \ server with 16 cores and 128GB RAM.  That would be usable with this model."
        updatedAt: '2023-05-13T09:29:10.387Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - zenyr
        - Okki
        - mirek190
    id: 645f58256990e12085232e59
    type: comment
  author: TheBloke
  content: "You can see the RAM requirements in my README\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png)\n\
    \nYou technically could load the model with 32GB RAM if  you have enough swap\
    \ space.  But it will be unbelievably, unpleasantly slow.  I would not recommend\
    \ it at all.  It would take tens of minutes to load the model, and might take\
    \ a minute or more to generate a single token.\n\nTry a 30B model instead.  \n\
    \nOr if you really want to use 65B, you could try it in the cloud. Microsoft Azure\
    \ and Google Compute both give free credits to new users - $200 from Azure, $300\
    \ from Google Compute. You can't get GPU machines for free, but you can get decent\
    \ CPU-only servers, eg a Linux server with 16 cores and 128GB RAM.  That would\
    \ be usable with this model."
  created_at: 2023-05-13 08:28:05+00:00
  edited: true
  hidden: false
  id: 645f58256990e12085232e59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-13T11:12:27.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<blockquote>

          <p>You can see the RAM requirements in my README</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png"></a></p>

          <p>You technically could load the model with 32GB RAM if  you have enough
          swap space.  But it will be unbelievably, unpleasantly slow.  I would not
          recommend it at all.  It would take tens of minutes to load the model, and
          might take a minute or more to generate a single token.</p>

          <p>Try a 30B model instead.  </p>

          <p>Or if you really want to use 65B, you could try it in the cloud. Microsoft
          Azure and Google Compute both give free credits to new users - $200 from
          Azure, $300 from Google Compute. You can''t get GPU machines for free, but
          you can get decent CPU-only servers, eg a Linux server with 16 cores and
          128GB RAM.  That would be usable with this model.</p>

          </blockquote>

          <p>Hello TheBloke</p>

          <p>I''m getting almost 1 token /s on my CPU i9 9900k and 4 channel dd4 3800
          MHz .<br>I''m curious how many tokens are you getting on 2x 4090 with that
          model.</p>

          '
        raw: "> You can see the RAM requirements in my README\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png)\n\
          > \n> You technically could load the model with 32GB RAM if  you have enough\
          \ swap space.  But it will be unbelievably, unpleasantly slow.  I would\
          \ not recommend it at all.  It would take tens of minutes to load the model,\
          \ and might take a minute or more to generate a single token.\n> \n> Try\
          \ a 30B model instead.  \n> \n> Or if you really want to use 65B, you could\
          \ try it in the cloud. Microsoft Azure and Google Compute both give free\
          \ credits to new users - $200 from Azure, $300 from Google Compute. You\
          \ can't get GPU machines for free, but you can get decent CPU-only servers,\
          \ eg a Linux server with 16 cores and 128GB RAM.  That would be usable with\
          \ this model.\n\nHello TheBloke\n\nI'm getting almost 1 token /s on my CPU\
          \ i9 9900k and 4 channel dd4 3800 MHz .\nI'm curious how many tokens are\
          \ you getting on 2x 4090 with that model."
        updatedAt: '2023-05-13T11:12:27.511Z'
      numEdits: 0
      reactions: []
    id: 645f709b6990e1208523d178
    type: comment
  author: mirek190
  content: "> You can see the RAM requirements in my README\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MRZ2bbSslClCqVfG_YEEy.png)\n\
    > \n> You technically could load the model with 32GB RAM if  you have enough swap\
    \ space.  But it will be unbelievably, unpleasantly slow.  I would not recommend\
    \ it at all.  It would take tens of minutes to load the model, and might take\
    \ a minute or more to generate a single token.\n> \n> Try a 30B model instead.\
    \  \n> \n> Or if you really want to use 65B, you could try it in the cloud. Microsoft\
    \ Azure and Google Compute both give free credits to new users - $200 from Azure,\
    \ $300 from Google Compute. You can't get GPU machines for free, but you can get\
    \ decent CPU-only servers, eg a Linux server with 16 cores and 128GB RAM.  That\
    \ would be usable with this model.\n\nHello TheBloke\n\nI'm getting almost 1 token\
    \ /s on my CPU i9 9900k and 4 channel dd4 3800 MHz .\nI'm curious how many tokens\
    \ are you getting on 2x 4090 with that model."
  created_at: 2023-05-13 10:12:27+00:00
  edited: false
  hidden: false
  id: 645f709b6990e1208523d178
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T11:15:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>With the 4bit GPTQ model + 2 x 4090, testing on the Dromedary 65B
          model (but it should be the same for this one), I got:</p>

          <pre><code>Output generated in 95.16 seconds (4.97 tokens/s, 473 tokens,
          context 33, seed 2085900431)

          Output generated in 424.31 seconds (3.54 tokens/s, 1504 tokens, context
          33, seed 1719700952)

          </code></pre>

          <p>That''s with streaming enabled; it''d probably be a bit quicker without
          streaming.</p>

          <p>That second query ran out of memory after 1500 tokens returned. So I
          can''t use the full context size of the model.</p>

          <p>I tried to enable CPU offloading so it wouldn''t run out of memory, but
          I couldn''t get that working. I think maybe there''s some bugs with CPU
          offloading + multiple GPUs, at the moment.</p>

          <p>This will hopefully be fixed in the future, eg once AutoGPTQ is ready.</p>

          '
        raw: 'With the 4bit GPTQ model + 2 x 4090, testing on the Dromedary 65B model
          (but it should be the same for this one), I got:


          ```

          Output generated in 95.16 seconds (4.97 tokens/s, 473 tokens, context 33,
          seed 2085900431)

          Output generated in 424.31 seconds (3.54 tokens/s, 1504 tokens, context
          33, seed 1719700952)

          ```


          That''s with streaming enabled; it''d probably be a bit quicker without
          streaming.


          That second query ran out of memory after 1500 tokens returned. So I can''t
          use the full context size of the model.


          I tried to enable CPU offloading so it wouldn''t run out of memory, but
          I couldn''t get that working. I think maybe there''s some bugs with CPU
          offloading + multiple GPUs, at the moment.


          This will hopefully be fixed in the future, eg once AutoGPTQ is ready.'
        updatedAt: '2023-05-13T11:15:26.005Z'
      numEdits: 0
      reactions: []
    id: 645f714e98464f9fcbfe6d0e
    type: comment
  author: TheBloke
  content: 'With the 4bit GPTQ model + 2 x 4090, testing on the Dromedary 65B model
    (but it should be the same for this one), I got:


    ```

    Output generated in 95.16 seconds (4.97 tokens/s, 473 tokens, context 33, seed
    2085900431)

    Output generated in 424.31 seconds (3.54 tokens/s, 1504 tokens, context 33, seed
    1719700952)

    ```


    That''s with streaming enabled; it''d probably be a bit quicker without streaming.


    That second query ran out of memory after 1500 tokens returned. So I can''t use
    the full context size of the model.


    I tried to enable CPU offloading so it wouldn''t run out of memory, but I couldn''t
    get that working. I think maybe there''s some bugs with CPU offloading + multiple
    GPUs, at the moment.


    This will hopefully be fixed in the future, eg once AutoGPTQ is ready.'
  created_at: 2023-05-13 10:15:26+00:00
  edited: false
  hidden: false
  id: 645f714e98464f9fcbfe6d0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/513eb8a60b47c6ad7a3dbc8230c8c807.svg
      fullname: Okki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Okki
      type: user
    createdAt: '2023-05-13T11:29:16.000Z'
    data:
      edited: false
      editors:
      - Okki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/513eb8a60b47c6ad7a3dbc8230c8c807.svg
          fullname: Okki
          isHf: false
          isPro: false
          name: Okki
          type: user
        html: '<p>I''m interesting what was more bottleneck - ram bandwidth or core
          number?</p>

          '
        raw: I'm interesting what was more bottleneck - ram bandwidth or core number?
        updatedAt: '2023-05-13T11:29:16.947Z'
      numEdits: 0
      reactions: []
    id: 645f748c446a4fa469561a07
    type: comment
  author: Okki
  content: I'm interesting what was more bottleneck - ram bandwidth or core number?
  created_at: 2023-05-13 10:29:16+00:00
  edited: false
  hidden: false
  id: 645f748c446a4fa469561a07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-13T11:37:43.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Hmm 4-5 tokens on GPU most powerful consumer one for the time being
          ... 4x -5x times faster than my ( old ) CPU.<br>Thanks</p>

          '
        raw: 'Hmm 4-5 tokens on GPU most powerful consumer one for the time being
          ... 4x -5x times faster than my ( old ) CPU.

          Thanks'
        updatedAt: '2023-05-13T11:41:59.869Z'
      numEdits: 2
      reactions: []
    id: 645f768798464f9fcbfe9cd5
    type: comment
  author: mirek190
  content: 'Hmm 4-5 tokens on GPU most powerful consumer one for the time being ...
    4x -5x times faster than my ( old ) CPU.

    Thanks'
  created_at: 2023-05-13 10:37:43+00:00
  edited: true
  hidden: false
  id: 645f768798464f9fcbfe9cd5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
      fullname: Jakub Strnad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: creative420
      type: user
    createdAt: '2023-05-14T08:51:39.000Z'
    data:
      edited: true
      editors:
      - creative420
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
          fullname: Jakub Strnad
          isHf: false
          isPro: false
          name: creative420
          type: user
        html: '<p>just about 1 token/s on Ryzen 5900x + 3090ti using the new gpu offloading
          in llama.cpp (oobabooga webui, windows 11, q4_0, --n_gpu_layers 41). Quite
          slow (1t/s) but for coding tasks works absolutely best from all models I''ve
          tried. Maybe I should try it on linux<br>edit: I moved to linux and now
          it "runs" 1.7t/s... not great but already usable</p>

          '
        raw: 'just about 1 token/s on Ryzen 5900x + 3090ti using the new gpu offloading
          in llama.cpp (oobabooga webui, windows 11, q4_0, --n_gpu_layers 41). Quite
          slow (1t/s) but for coding tasks works absolutely best from all models I''ve
          tried. Maybe I should try it on linux

          edit: I moved to linux and now it "runs" 1.7t/s... not great but already
          usable'
        updatedAt: '2023-05-19T19:13:07.381Z'
      numEdits: 2
      reactions: []
    id: 6460a11b567598449e039100
    type: comment
  author: creative420
  content: 'just about 1 token/s on Ryzen 5900x + 3090ti using the new gpu offloading
    in llama.cpp (oobabooga webui, windows 11, q4_0, --n_gpu_layers 41). Quite slow
    (1t/s) but for coding tasks works absolutely best from all models I''ve tried.
    Maybe I should try it on linux

    edit: I moved to linux and now it "runs" 1.7t/s... not great but already usable'
  created_at: 2023-05-14 07:51:39+00:00
  edited: true
  hidden: false
  id: 6460a11b567598449e039100
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-14T15:19:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Today I tested it on an A6000 48GB with llama.cpp''s new offloading
          feature and got 6.86 tokens/s (145.73 ms per run) - pretty cool!</p>

          <p>Testing the GPTQ version, I got 12.46 tokens/s with CUDA AutoGPTQ, and
          6.12 tok/s with Triton AutoGPTQ.  But Triton uses less VRAM and won''t OOM
          in 48GB, where CUDA does at around 1600 tokens returned.</p>

          '
        raw: 'Today I tested it on an A6000 48GB with llama.cpp''s new offloading
          feature and got 6.86 tokens/s (145.73 ms per run) - pretty cool!


          Testing the GPTQ version, I got 12.46 tokens/s with CUDA AutoGPTQ, and 6.12
          tok/s with Triton AutoGPTQ.  But Triton uses less VRAM and won''t OOM in
          48GB, where CUDA does at around 1600 tokens returned.'
        updatedAt: '2023-05-14T15:19:52.685Z'
      numEdits: 0
      reactions: []
    id: 6460fc18232a700f265aad6e
    type: comment
  author: TheBloke
  content: 'Today I tested it on an A6000 48GB with llama.cpp''s new offloading feature
    and got 6.86 tokens/s (145.73 ms per run) - pretty cool!


    Testing the GPTQ version, I got 12.46 tokens/s with CUDA AutoGPTQ, and 6.12 tok/s
    with Triton AutoGPTQ.  But Triton uses less VRAM and won''t OOM in 48GB, where
    CUDA does at around 1600 tokens returned.'
  created_at: 2023-05-14 14:19:52+00:00
  edited: false
  hidden: false
  id: 6460fc18232a700f265aad6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-14T17:28:15.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Impressive!</p>

          '
        raw: Impressive!
        updatedAt: '2023-05-14T17:28:15.109Z'
      numEdits: 0
      reactions: []
    id: 64611a2fbf985b8b4eb721b4
    type: comment
  author: mirek190
  content: Impressive!
  created_at: 2023-05-14 16:28:15+00:00
  edited: false
  hidden: false
  id: 64611a2fbf985b8b4eb721b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b6d58fd2050a61bbb87c9de7e39a4c9c.svg
      fullname: Zorkudia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vulkarie
      type: user
    createdAt: '2023-06-05T15:54:45.000Z'
    data:
      edited: false
      editors:
      - vulkarie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487612843513489
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b6d58fd2050a61bbb87c9de7e39a4c9c.svg
          fullname: Zorkudia
          isHf: false
          isPro: false
          name: vulkarie
          type: user
        html: '<p>Do the requirement stated RAM not VRAM? if so, my 3090 can run the
          model in 5bit as long as I have 64GB RAM?</p>

          '
        raw: Do the requirement stated RAM not VRAM? if so, my 3090 can run the model
          in 5bit as long as I have 64GB RAM?
        updatedAt: '2023-06-05T15:54:45.163Z'
      numEdits: 0
      reactions: []
    id: 647e05455214d172cbbd0f76
    type: comment
  author: vulkarie
  content: Do the requirement stated RAM not VRAM? if so, my 3090 can run the model
    in 5bit as long as I have 64GB RAM?
  created_at: 2023-06-05 14:54:45+00:00
  edited: false
  hidden: false
  id: 647e05455214d172cbbd0f76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-05T16:46:32.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9729167819023132
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>YES</p>

          <p>I have rtx 3090 as well<br>65B q5_1  you can put almost 40 layers on
          gpu. I have 700 ms/t .</p>

          '
        raw: "YES\n\nI have rtx 3090 as well \n65B q5_1  you can put almost 40 layers\
          \ on gpu. I have 700 ms/t ."
        updatedAt: '2023-06-05T16:46:32.336Z'
      numEdits: 0
      reactions: []
    id: 647e116810b7a3b1570a3504
    type: comment
  author: mirek190
  content: "YES\n\nI have rtx 3090 as well \n65B q5_1  you can put almost 40 layers\
    \ on gpu. I have 700 ms/t ."
  created_at: 2023-06-05 15:46:32+00:00
  edited: false
  hidden: false
  id: 647e116810b7a3b1570a3504
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d70ae605c07e368f8a50543ca42e0eba.svg
      fullname: Pablito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boricuapab
      type: user
    createdAt: '2023-06-12T00:54:33.000Z'
    data:
      edited: true
      editors:
      - boricuapab
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9629972577095032
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d70ae605c07e368f8a50543ca42e0eba.svg
          fullname: Pablito
          isHf: false
          isPro: false
          name: boricuapab
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nulled&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nulled\">@<span class=\"\
          underline\">nulled</span></a></span>\n\n\t</span></span> I was able to run\
          \ the q3ks model on 32gb ram with gpu offloading 20 layers, was quite slow\
          \ though, total time was about 21 mins for the response, I let it do it's\
          \ thing while I took a coffee break, but it's great to see I can run a 65b\
          \ model locally</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/-YVmmHxL7ljogdjNBBM0Y.png\"\
          ><img alt=\"gpt4Alpaca65B_21mins.PNG\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/-YVmmHxL7ljogdjNBBM0Y.png\"\
          ></a></p>\n"
        raw: '@nulled I was able to run the q3ks model on 32gb ram with gpu offloading
          20 layers, was quite slow though, total time was about 21 mins for the response,
          I let it do it''s thing while I took a coffee break, but it''s great to
          see I can run a 65b model locally


          ![gpt4Alpaca65B_21mins.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/-YVmmHxL7ljogdjNBBM0Y.png)'
        updatedAt: '2023-06-12T00:55:02.629Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mirek190
    id: 64866cc9a4cf2081f20ec28d
    type: comment
  author: boricuapab
  content: '@nulled I was able to run the q3ks model on 32gb ram with gpu offloading
    20 layers, was quite slow though, total time was about 21 mins for the response,
    I let it do it''s thing while I took a coffee break, but it''s great to see I
    can run a 65b model locally


    ![gpt4Alpaca65B_21mins.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/-YVmmHxL7ljogdjNBBM0Y.png)'
  created_at: 2023-06-11 23:54:33+00:00
  edited: true
  hidden: false
  id: 64866cc9a4cf2081f20ec28d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-12T20:23:28.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.98082035779953
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>At least works ;)<br>Few moth ago even run 7B model was nearly impossible
          on local PC.</p>

          '
        raw: 'At least works ;)

          Few moth ago even run 7B model was nearly impossible on local PC.'
        updatedAt: '2023-06-12T20:23:28.293Z'
      numEdits: 0
      reactions: []
    id: 64877ec0c48667a387efac8d
    type: comment
  author: mirek190
  content: 'At least works ;)

    Few moth ago even run 7B model was nearly impossible on local PC.'
  created_at: 2023-06-12 19:23:28+00:00
  edited: false
  hidden: false
  id: 64877ec0c48667a387efac8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-12T21:00:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.13605381548404694
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Very true!</p>

          '
        raw: Very true!
        updatedAt: '2023-06-12T21:00:40.932Z'
      numEdits: 0
      reactions: []
    id: 64878778eaf65f126138cc30
    type: comment
  author: TheBloke
  content: Very true!
  created_at: 2023-06-12 20:00:40+00:00
  edited: false
  hidden: false
  id: 64878778eaf65f126138cc30
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/gpt4-alpaca-lora_mlp-65B-GGML
repo_type: model
status: open
target_branch: null
title: Opinion about this model.
