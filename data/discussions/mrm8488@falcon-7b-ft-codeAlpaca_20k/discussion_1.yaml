!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hemangjoshi37a
conflicting_files: null
created_at: 2023-08-02 11:57:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664607238774-60910a419a8bcaa437b234a6.jpeg?w=200&h=200&f=face
      fullname: Hemang Joshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hemangjoshi37a
      type: user
    createdAt: '2023-08-02T12:57:02.000Z'
    data:
      edited: false
      editors:
      - hemangjoshi37a
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4292835295200348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664607238774-60910a419a8bcaa437b234a6.jpeg?w=200&h=200&f=face
          fullname: Hemang Joshi
          isHf: false
          isPro: false
          name: hemangjoshi37a
          type: user
        html: "<p>code : </p>\n<pre><code>from transformers import AutoTokenizer,\
          \ AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
          , trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
          , trust_remote_code=True, device='cpu')\nmodel = model.eval()\n</code></pre>\n\
          <p>error : </p>\n<pre><code>Downloading (\u2026)okenizer_config.json: 100%\n\
          180/180 [00:00&lt;00:00, 7.79kB/s]\nDownloading (\u2026)/main/tokenizer.json:\
          \ 100%\n2.73M/2.73M [00:00&lt;00:00, 3.69MB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%\n313/313 [00:00&lt;00:00, 15.8kB/s]\n---------------------------------------------------------------------------\n\
          HTTPError                                 Traceback (most recent call last)\n\
          File ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:261,\
          \ in hf_raise_for_status(response, endpoint_name)\n    260 try:\n--&gt;\
          \ 261     response.raise_for_status()\n    262 except HTTPError as e:\n\n\
          File ~/.local/lib/python3.11/site-packages/requests/models.py:941, in Response.raise_for_status(self)\n\
          \    940 if http_error_msg:\n--&gt; 941     raise HTTPError(http_error_msg,\
          \ response=self)\n\nHTTPError: 404 Client Error: Not Found for url: https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/resolve/main/config.json\n\
          \nThe above exception was the direct cause of the following exception:\n\
          \nEntryNotFoundError                        Traceback (most recent call\
          \ last)\nFile ~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:417,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, use_auth_token, revision, local_files_only, subfolder, repo_type,\
          \ user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash)\n    415 try:\n    416     # Load from URL or cache if already\
          \ cached\n--&gt; 417     resolved_file = hf_hub_download(\n    418     \
          \    path_or_repo_id,\n    419         filename,\n    420         subfolder=None\
          \ if len(subfolder) == 0 else subfolder,\n    421         repo_type=repo_type,\n\
          \    422         revision=revision,\n    423         cache_dir=cache_dir,\n\
          \    424         user_agent=user_agent,\n    425         force_download=force_download,\n\
          \    426         proxies=proxies,\n    427         resume_download=resume_download,\n\
          \    428         use_auth_token=use_auth_token,\n    429         local_files_only=local_files_only,\n\
          \    430     )\n    432 except RepositoryNotFoundError:\n\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118,\
          \ in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n  \
          \  116     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__,\
          \ has_token=has_token, kwargs=kwargs)\n--&gt; 118 return fn(*args, **kwargs)\n\
          \nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195,\
          \ in hf_hub_download(repo_id, filename, subfolder, repo_type, revision,\
          \ library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks,\
          \ user_agent, force_download, force_filename, proxies, etag_timeout, resume_download,\
          \ token, local_files_only, legacy_cache_layout)\n   1194 try:\n-&gt; 1195\
          \     metadata = get_hf_file_metadata(\n   1196         url=url,\n   1197\
          \         token=token,\n   1198         proxies=proxies,\n   1199      \
          \   timeout=etag_timeout,\n   1200     )\n   1201 except EntryNotFoundError\
          \ as http_error:\n   1202     # Cache the non-existence of the file and\
          \ raise\n\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118,\
          \ in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n  \
          \  116     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__,\
          \ has_token=has_token, kwargs=kwargs)\n--&gt; 118 return fn(*args, **kwargs)\n\
          \nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1541,\
          \ in get_hf_file_metadata(url, token, proxies, timeout)\n   1532 r = _request_wrapper(\n\
          \   1533     method=\"HEAD\",\n   1534     url=url,\n   (...)\n   1539 \
          \    timeout=timeout,\n   1540 )\n-&gt; 1541 hf_raise_for_status(r)\n  \
          \ 1543 # Return\n\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:271,\
          \ in hf_raise_for_status(response, endpoint_name)\n    270     message =\
          \ f\"{response.status_code} Client Error.\" + \"\\n\\n\" + f\"Entry Not\
          \ Found for url: {response.url}.\"\n--&gt; 271     raise EntryNotFoundError(message,\
          \ response) from e\n    273 elif error_code == \"GatedRepo\":\n\nEntryNotFoundError:\
          \ 404 Client Error. (Request ID: Root=1-64ca5274-2c220b8f686b175757ff6e2b;3d241865-9500-4c7a-94f3-385feff80967)\n\
          \nEntry Not Found for url: https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/resolve/main/config.json.\n\
          \nDuring handling of the above exception, another exception occurred:\n\n\
          OSError                                   Traceback (most recent call last)\n\
          Cell In[6], line 3\n      1 from transformers import AutoTokenizer, AutoModel\n\
          \      2 tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
          , trust_remote_code=True)\n----&gt; 3 model = AutoModel.from_pretrained(\"\
          mrm8488/falcon-7b-ft-codeAlpaca_20k\", trust_remote_code=True, device='cpu')\n\
          \      4 model = model.eval()\n\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:456,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    453 if kwargs.get(\"torch_dtype\", None) ==\
          \ \"auto\":\n    454     _ = kwargs.pop(\"torch_dtype\")\n--&gt; 456 config,\
          \ kwargs = AutoConfig.from_pretrained(\n    457     pretrained_model_name_or_path,\n\
          \    458     return_unused_kwargs=True,\n    459     trust_remote_code=trust_remote_code,\n\
          \    460     **hub_kwargs,\n    461     **kwargs,\n    462 )\n    464 #\
          \ if torch_dtype=auto was passed here, ensure to pass it on\n    465 if\
          \ kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:944,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    942 kwargs[\"name_or_path\"] = pretrained_model_name_or_path\n    943\
          \ trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n--&gt; 944\
          \ config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n    945 has_remote_code = \"auto_map\" in config_dict and \"\
          AutoConfig\" in config_dict[\"auto_map\"]\n    946 has_local_code = \"model_type\"\
          \ in config_dict and config_dict[\"model_type\"] in CONFIG_MAPPING\n\nFile\
          \ ~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:574,\
          \ in PretrainedConfig.get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\n    572 original_kwargs = copy.deepcopy(kwargs)\n    573 #\
          \ Get config dict associated with the base config file\n--&gt; 574 config_dict,\
          \ kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n\
          \    575 if \"_commit_hash\" in config_dict:\n    576     original_kwargs[\"\
          _commit_hash\"] = config_dict[\"_commit_hash\"]\n\nFile ~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:629,\
          \ in PretrainedConfig._get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\n    625 configuration_file = kwargs.pop(\"_configuration_file\"\
          , CONFIG_NAME)\n    627 try:\n    628     # Load from local folder or from\
          \ cache or download from model Hub and cache\n--&gt; 629     resolved_config_file\
          \ = cached_file(\n    630         pretrained_model_name_or_path,\n    631\
          \         configuration_file,\n    632         cache_dir=cache_dir,\n  \
          \  633         force_download=force_download,\n    634         proxies=proxies,\n\
          \    635         resume_download=resume_download,\n    636         local_files_only=local_files_only,\n\
          \    637         use_auth_token=use_auth_token,\n    638         user_agent=user_agent,\n\
          \    639         revision=revision,\n    640         subfolder=subfolder,\n\
          \    641         _commit_hash=commit_hash,\n    642     )\n    643     commit_hash\
          \ = extract_commit_hash(resolved_config_file, commit_hash)\n    644 except\
          \ EnvironmentError:\n    645     # Raise any environment error raise by\
          \ `cached_file`. It will have a helpful error message adapted to\n    646\
          \     # the original exception.\n\nFile ~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:463,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, use_auth_token, revision, local_files_only, subfolder, repo_type,\
          \ user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash)\n    461     if revision is None:\n    462         revision\
          \ = \"main\"\n--&gt; 463     raise EnvironmentError(\n    464         f\"\
          {path_or_repo_id} does not appear to have a file named {full_filename}.\
          \ Checkout \"\n    465         f\"'https://huggingface.co/{path_or_repo_id}/{revision}'\
          \ for available files.\"\n    466     )\n    467 except HTTPError as err:\n\
          \    468     # First we try to see if we have a cached version (not up to\
          \ date):\n    469     resolved_file = try_to_load_from_cache(path_or_repo_id,\
          \ full_filename, cache_dir=cache_dir, revision=revision)\n\nOSError: mrm8488/falcon-7b-ft-codeAlpaca_20k\
          \ does not appear to have a file named config.json. Checkout 'https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/main'\
          \ for available files.\n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://linktr.ee/hemangjoshi37a\"\
          >https://linktr.ee/hemangjoshi37a</a></p>\n"
        raw: "code : \r\n```\r\nfrom transformers import AutoTokenizer, AutoModel\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
          , trust_remote_code=True)\r\nmodel = AutoModel.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
          , trust_remote_code=True, device='cpu')\r\nmodel = model.eval()\r\n```\r\
          \n\r\nerror : \r\n```\r\nDownloading (\u2026)okenizer_config.json: 100%\r\
          \n180/180 [00:00<00:00, 7.79kB/s]\r\nDownloading (\u2026)/main/tokenizer.json:\
          \ 100%\r\n2.73M/2.73M [00:00<00:00, 3.69MB/s]\r\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%\r\n313/313 [00:00<00:00, 15.8kB/s]\r\n---------------------------------------------------------------------------\r\
          \nHTTPError                                 Traceback (most recent call\
          \ last)\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:261,\
          \ in hf_raise_for_status(response, endpoint_name)\r\n    260 try:\r\n-->\
          \ 261     response.raise_for_status()\r\n    262 except HTTPError as e:\r\
          \n\r\nFile ~/.local/lib/python3.11/site-packages/requests/models.py:941,\
          \ in Response.raise_for_status(self)\r\n    940 if http_error_msg:\r\n-->\
          \ 941     raise HTTPError(http_error_msg, response=self)\r\n\r\nHTTPError:\
          \ 404 Client Error: Not Found for url: https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/resolve/main/config.json\r\
          \n\r\nThe above exception was the direct cause of the following exception:\r\
          \n\r\nEntryNotFoundError                        Traceback (most recent call\
          \ last)\r\nFile ~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:417,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, use_auth_token, revision, local_files_only, subfolder, repo_type,\
          \ user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash)\r\n    415 try:\r\n    416     # Load from URL or cache\
          \ if already cached\r\n--> 417     resolved_file = hf_hub_download(\r\n\
          \    418         path_or_repo_id,\r\n    419         filename,\r\n    420\
          \         subfolder=None if len(subfolder) == 0 else subfolder,\r\n    421\
          \         repo_type=repo_type,\r\n    422         revision=revision,\r\n\
          \    423         cache_dir=cache_dir,\r\n    424         user_agent=user_agent,\r\
          \n    425         force_download=force_download,\r\n    426         proxies=proxies,\r\
          \n    427         resume_download=resume_download,\r\n    428         use_auth_token=use_auth_token,\r\
          \n    429         local_files_only=local_files_only,\r\n    430     )\r\n\
          \    432 except RepositoryNotFoundError:\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118,\
          \ in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\r\n    116\
          \     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
          \ kwargs=kwargs)\r\n--> 118 return fn(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195,\
          \ in hf_hub_download(repo_id, filename, subfolder, repo_type, revision,\
          \ library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks,\
          \ user_agent, force_download, force_filename, proxies, etag_timeout, resume_download,\
          \ token, local_files_only, legacy_cache_layout)\r\n   1194 try:\r\n-> 1195\
          \     metadata = get_hf_file_metadata(\r\n   1196         url=url,\r\n \
          \  1197         token=token,\r\n   1198         proxies=proxies,\r\n   1199\
          \         timeout=etag_timeout,\r\n   1200     )\r\n   1201 except EntryNotFoundError\
          \ as http_error:\r\n   1202     # Cache the non-existence of the file and\
          \ raise\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118,\
          \ in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\r\n    116\
          \     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
          \ kwargs=kwargs)\r\n--> 118 return fn(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1541,\
          \ in get_hf_file_metadata(url, token, proxies, timeout)\r\n   1532 r = _request_wrapper(\r\
          \n   1533     method=\"HEAD\",\r\n   1534     url=url,\r\n   (...)\r\n \
          \  1539     timeout=timeout,\r\n   1540 )\r\n-> 1541 hf_raise_for_status(r)\r\
          \n   1543 # Return\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:271,\
          \ in hf_raise_for_status(response, endpoint_name)\r\n    270     message\
          \ = f\"{response.status_code} Client Error.\" + \"\\n\\n\" + f\"Entry Not\
          \ Found for url: {response.url}.\"\r\n--> 271     raise EntryNotFoundError(message,\
          \ response) from e\r\n    273 elif error_code == \"GatedRepo\":\r\n\r\n\
          EntryNotFoundError: 404 Client Error. (Request ID: Root=1-64ca5274-2c220b8f686b175757ff6e2b;3d241865-9500-4c7a-94f3-385feff80967)\r\
          \n\r\nEntry Not Found for url: https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/resolve/main/config.json.\r\
          \n\r\nDuring handling of the above exception, another exception occurred:\r\
          \n\r\nOSError                                   Traceback (most recent call\
          \ last)\r\nCell In[6], line 3\r\n      1 from transformers import AutoTokenizer,\
          \ AutoModel\r\n      2 tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
          , trust_remote_code=True)\r\n----> 3 model = AutoModel.from_pretrained(\"\
          mrm8488/falcon-7b-ft-codeAlpaca_20k\", trust_remote_code=True, device='cpu')\r\
          \n      4 model = model.eval()\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:456,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n    453 if kwargs.get(\"torch_dtype\", None)\
          \ == \"auto\":\r\n    454     _ = kwargs.pop(\"torch_dtype\")\r\n--> 456\
          \ config, kwargs = AutoConfig.from_pretrained(\r\n    457     pretrained_model_name_or_path,\r\
          \n    458     return_unused_kwargs=True,\r\n    459     trust_remote_code=trust_remote_code,\r\
          \n    460     **hub_kwargs,\r\n    461     **kwargs,\r\n    462 )\r\n  \
          \  464 # if torch_dtype=auto was passed here, ensure to pass it on\r\n \
          \   465 if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\r\n\r\nFile\
          \ ~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:944,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
          \n    942 kwargs[\"name_or_path\"] = pretrained_model_name_or_path\r\n \
          \   943 trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\r\n\
          --> 944 config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\r\n    945 has_remote_code = \"auto_map\" in config_dict and\
          \ \"AutoConfig\" in config_dict[\"auto_map\"]\r\n    946 has_local_code\
          \ = \"model_type\" in config_dict and config_dict[\"model_type\"] in CONFIG_MAPPING\r\
          \n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:574,\
          \ in PretrainedConfig.get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\r\n    572 original_kwargs = copy.deepcopy(kwargs)\r\n    573\
          \ # Get config dict associated with the base config file\r\n--> 574 config_dict,\
          \ kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\
          \n    575 if \"_commit_hash\" in config_dict:\r\n    576     original_kwargs[\"\
          _commit_hash\"] = config_dict[\"_commit_hash\"]\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:629,\
          \ in PretrainedConfig._get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\r\n    625 configuration_file = kwargs.pop(\"_configuration_file\"\
          , CONFIG_NAME)\r\n    627 try:\r\n    628     # Load from local folder or\
          \ from cache or download from model Hub and cache\r\n--> 629     resolved_config_file\
          \ = cached_file(\r\n    630         pretrained_model_name_or_path,\r\n \
          \   631         configuration_file,\r\n    632         cache_dir=cache_dir,\r\
          \n    633         force_download=force_download,\r\n    634         proxies=proxies,\r\
          \n    635         resume_download=resume_download,\r\n    636         local_files_only=local_files_only,\r\
          \n    637         use_auth_token=use_auth_token,\r\n    638         user_agent=user_agent,\r\
          \n    639         revision=revision,\r\n    640         subfolder=subfolder,\r\
          \n    641         _commit_hash=commit_hash,\r\n    642     )\r\n    643\
          \     commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\r\
          \n    644 except EnvironmentError:\r\n    645     # Raise any environment\
          \ error raise by `cached_file`. It will have a helpful error message adapted\
          \ to\r\n    646     # the original exception.\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:463,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, use_auth_token, revision, local_files_only, subfolder, repo_type,\
          \ user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash)\r\n    461     if revision is None:\r\n    462         revision\
          \ = \"main\"\r\n--> 463     raise EnvironmentError(\r\n    464         f\"\
          {path_or_repo_id} does not appear to have a file named {full_filename}.\
          \ Checkout \"\r\n    465         f\"'https://huggingface.co/{path_or_repo_id}/{revision}'\
          \ for available files.\"\r\n    466     )\r\n    467 except HTTPError as\
          \ err:\r\n    468     # First we try to see if we have a cached version\
          \ (not up to date):\r\n    469     resolved_file = try_to_load_from_cache(path_or_repo_id,\
          \ full_filename, cache_dir=cache_dir, revision=revision)\r\n\r\nOSError:\
          \ mrm8488/falcon-7b-ft-codeAlpaca_20k does not appear to have a file named\
          \ config.json. Checkout 'https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/main'\
          \ for available files.\r\n```\r\nhttps://linktr.ee/hemangjoshi37a"
        updatedAt: '2023-08-02T12:57:02.103Z'
      numEdits: 0
      reactions: []
    id: 64ca529e87bfb0fc21e9f099
    type: comment
  author: hemangjoshi37a
  content: "code : \r\n```\r\nfrom transformers import AutoTokenizer, AutoModel\r\n\
    tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
    , trust_remote_code=True)\r\nmodel = AutoModel.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
    , trust_remote_code=True, device='cpu')\r\nmodel = model.eval()\r\n```\r\n\r\n\
    error : \r\n```\r\nDownloading (\u2026)okenizer_config.json: 100%\r\n180/180 [00:00<00:00,\
    \ 7.79kB/s]\r\nDownloading (\u2026)/main/tokenizer.json: 100%\r\n2.73M/2.73M [00:00<00:00,\
    \ 3.69MB/s]\r\nDownloading (\u2026)cial_tokens_map.json: 100%\r\n313/313 [00:00<00:00,\
    \ 15.8kB/s]\r\n---------------------------------------------------------------------------\r\
    \nHTTPError                                 Traceback (most recent call last)\r\
    \nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:261,\
    \ in hf_raise_for_status(response, endpoint_name)\r\n    260 try:\r\n--> 261 \
    \    response.raise_for_status()\r\n    262 except HTTPError as e:\r\n\r\nFile\
    \ ~/.local/lib/python3.11/site-packages/requests/models.py:941, in Response.raise_for_status(self)\r\
    \n    940 if http_error_msg:\r\n--> 941     raise HTTPError(http_error_msg, response=self)\r\
    \n\r\nHTTPError: 404 Client Error: Not Found for url: https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/resolve/main/config.json\r\
    \n\r\nThe above exception was the direct cause of the following exception:\r\n\
    \r\nEntryNotFoundError                        Traceback (most recent call last)\r\
    \nFile ~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:417, in\
    \ cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
    \ proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent,\
    \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
    \ _commit_hash)\r\n    415 try:\r\n    416     # Load from URL or cache if already\
    \ cached\r\n--> 417     resolved_file = hf_hub_download(\r\n    418         path_or_repo_id,\r\
    \n    419         filename,\r\n    420         subfolder=None if len(subfolder)\
    \ == 0 else subfolder,\r\n    421         repo_type=repo_type,\r\n    422    \
    \     revision=revision,\r\n    423         cache_dir=cache_dir,\r\n    424  \
    \       user_agent=user_agent,\r\n    425         force_download=force_download,\r\
    \n    426         proxies=proxies,\r\n    427         resume_download=resume_download,\r\
    \n    428         use_auth_token=use_auth_token,\r\n    429         local_files_only=local_files_only,\r\
    \n    430     )\r\n    432 except RepositoryNotFoundError:\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118,\
    \ in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\r\n    116     kwargs\
    \ = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
    \ kwargs=kwargs)\r\n--> 118 return fn(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195,\
    \ in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name,\
    \ library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download,\
    \ force_filename, proxies, etag_timeout, resume_download, token, local_files_only,\
    \ legacy_cache_layout)\r\n   1194 try:\r\n-> 1195     metadata = get_hf_file_metadata(\r\
    \n   1196         url=url,\r\n   1197         token=token,\r\n   1198        \
    \ proxies=proxies,\r\n   1199         timeout=etag_timeout,\r\n   1200     )\r\
    \n   1201 except EntryNotFoundError as http_error:\r\n   1202     # Cache the\
    \ non-existence of the file and raise\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118,\
    \ in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\r\n    116     kwargs\
    \ = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
    \ kwargs=kwargs)\r\n--> 118 return fn(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1541,\
    \ in get_hf_file_metadata(url, token, proxies, timeout)\r\n   1532 r = _request_wrapper(\r\
    \n   1533     method=\"HEAD\",\r\n   1534     url=url,\r\n   (...)\r\n   1539\
    \     timeout=timeout,\r\n   1540 )\r\n-> 1541 hf_raise_for_status(r)\r\n   1543\
    \ # Return\r\n\r\nFile ~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:271,\
    \ in hf_raise_for_status(response, endpoint_name)\r\n    270     message = f\"\
    {response.status_code} Client Error.\" + \"\\n\\n\" + f\"Entry Not Found for url:\
    \ {response.url}.\"\r\n--> 271     raise EntryNotFoundError(message, response)\
    \ from e\r\n    273 elif error_code == \"GatedRepo\":\r\n\r\nEntryNotFoundError:\
    \ 404 Client Error. (Request ID: Root=1-64ca5274-2c220b8f686b175757ff6e2b;3d241865-9500-4c7a-94f3-385feff80967)\r\
    \n\r\nEntry Not Found for url: https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/resolve/main/config.json.\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nOSError                                   Traceback (most recent call last)\r\
    \nCell In[6], line 3\r\n      1 from transformers import AutoTokenizer, AutoModel\r\
    \n      2 tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
    , trust_remote_code=True)\r\n----> 3 model = AutoModel.from_pretrained(\"mrm8488/falcon-7b-ft-codeAlpaca_20k\"\
    , trust_remote_code=True, device='cpu')\r\n      4 model = model.eval()\r\n\r\n\
    File ~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:456,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n    453 if kwargs.get(\"torch_dtype\", None) == \"auto\":\r\n \
    \   454     _ = kwargs.pop(\"torch_dtype\")\r\n--> 456 config, kwargs = AutoConfig.from_pretrained(\r\
    \n    457     pretrained_model_name_or_path,\r\n    458     return_unused_kwargs=True,\r\
    \n    459     trust_remote_code=trust_remote_code,\r\n    460     **hub_kwargs,\r\
    \n    461     **kwargs,\r\n    462 )\r\n    464 # if torch_dtype=auto was passed\
    \ here, ensure to pass it on\r\n    465 if kwargs_orig.get(\"torch_dtype\", None)\
    \ == \"auto\":\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:944,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n    942 kwargs[\"name_or_path\"] = pretrained_model_name_or_path\r\n    943\
    \ trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\r\n--> 944 config_dict,\
    \ unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\r\n    945 has_remote_code = \"auto_map\" in config_dict and \"AutoConfig\"\
    \ in config_dict[\"auto_map\"]\r\n    946 has_local_code = \"model_type\" in config_dict\
    \ and config_dict[\"model_type\"] in CONFIG_MAPPING\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:574,\
    \ in PretrainedConfig.get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n    572 original_kwargs = copy.deepcopy(kwargs)\r\n    573 # Get config dict\
    \ associated with the base config file\r\n--> 574 config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\r\n    575 if \"_commit_hash\" in config_dict:\r\n    576     original_kwargs[\"\
    _commit_hash\"] = config_dict[\"_commit_hash\"]\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:629,\
    \ in PretrainedConfig._get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n    625 configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME)\r\
    \n    627 try:\r\n    628     # Load from local folder or from cache or download\
    \ from model Hub and cache\r\n--> 629     resolved_config_file = cached_file(\r\
    \n    630         pretrained_model_name_or_path,\r\n    631         configuration_file,\r\
    \n    632         cache_dir=cache_dir,\r\n    633         force_download=force_download,\r\
    \n    634         proxies=proxies,\r\n    635         resume_download=resume_download,\r\
    \n    636         local_files_only=local_files_only,\r\n    637         use_auth_token=use_auth_token,\r\
    \n    638         user_agent=user_agent,\r\n    639         revision=revision,\r\
    \n    640         subfolder=subfolder,\r\n    641         _commit_hash=commit_hash,\r\
    \n    642     )\r\n    643     commit_hash = extract_commit_hash(resolved_config_file,\
    \ commit_hash)\r\n    644 except EnvironmentError:\r\n    645     # Raise any\
    \ environment error raise by `cached_file`. It will have a helpful error message\
    \ adapted to\r\n    646     # the original exception.\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:463,\
    \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
    \ proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent,\
    \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
    \ _commit_hash)\r\n    461     if revision is None:\r\n    462         revision\
    \ = \"main\"\r\n--> 463     raise EnvironmentError(\r\n    464         f\"{path_or_repo_id}\
    \ does not appear to have a file named {full_filename}. Checkout \"\r\n    465\
    \         f\"'https://huggingface.co/{path_or_repo_id}/{revision}' for available\
    \ files.\"\r\n    466     )\r\n    467 except HTTPError as err:\r\n    468   \
    \  # First we try to see if we have a cached version (not up to date):\r\n   \
    \ 469     resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename,\
    \ cache_dir=cache_dir, revision=revision)\r\n\r\nOSError: mrm8488/falcon-7b-ft-codeAlpaca_20k\
    \ does not appear to have a file named config.json. Checkout 'https://huggingface.co/mrm8488/falcon-7b-ft-codeAlpaca_20k/main'\
    \ for available files.\r\n```\r\nhttps://linktr.ee/hemangjoshi37a"
  created_at: 2023-08-02 11:57:02+00:00
  edited: false
  hidden: false
  id: 64ca529e87bfb0fc21e9f099
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mrm8488/falcon-7b-ft-codeAlpaca_20k
repo_type: model
status: open
target_branch: null
title: 'getting this error '
