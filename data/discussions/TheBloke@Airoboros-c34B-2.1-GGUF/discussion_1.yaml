!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ronenzyroff
conflicting_files: null
created_at: 2023-08-28 01:46:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-08-28T02:46:36.000Z'
    data:
      edited: false
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32259997725486755
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: '<p>See my results here: I''m using KoboldCpp-NoCuda Version 1.41 koboldcpp-1.41
          (beta)<br><a href="https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF/discussions/1#64ec09c4c68ddc867b897078">https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF/discussions/1#64ec09c4c68ddc867b897078</a></p>

          '
        raw: "See my results here: I'm using KoboldCpp-NoCuda Version 1.41 koboldcpp-1.41\
          \ (beta)\r\nhttps://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF/discussions/1#64ec09c4c68ddc867b897078"
        updatedAt: '2023-08-28T02:46:36.388Z'
      numEdits: 0
      reactions: []
    id: 64ec0a8c6878d90b0336ad4f
    type: comment
  author: ronenzyroff
  content: "See my results here: I'm using KoboldCpp-NoCuda Version 1.41 koboldcpp-1.41\
    \ (beta)\r\nhttps://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF/discussions/1#64ec09c4c68ddc867b897078"
  created_at: 2023-08-28 01:46:36+00:00
  edited: false
  hidden: false
  id: 64ec0a8c6878d90b0336ad4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-08-29T13:11:40.000Z'
    data:
      edited: true
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7421677708625793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: "<p>Instructions for running using <a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui\"\
          >https://github.com/oobabooga/text-generation-webui</a> on Windows 11 on\
          \ CPU with llama.cpp, with GPU acceleration enabled for the purpose of faster\
          \ prompt ingestion (x26 speed).<br>On my laptop, the model runs at 1.0 token\
          \ per second, but because I'm using the BLAS GPU acceleration, my prompt\
          \ gets processed by llama.cpp at 26 tokens per second, so I don't have to\
          \ wait long until the AI starts responding.</p>\n<h1 id=\"prerequisites\"\
          >Prerequisites</h1>\n<ol>\n<li><p>At least one of:<br>a. An integrated GPU\
          \ with at least 500 megabytes of dedicated VRAM.<br>b. A dedicated AMD GPU\
          \ with at least 4 gigabytes of dedicated VRAM, and AMD ROCm drivers installed\
          \ globally on the system.<br>c. A dedicated Nvidia GPU with at least 4 gigabytes\
          \ of dedicated VRAM, and Nvidia CUDA drivers installed globally on the system.</p>\n\
          </li>\n<li><p>Windows 11 with Git installed</p>\n</li>\n<li><p>Minimum RAM\
          \ (regular RAM, not VRAM)- 64 gigabytes of RAM for the 8-bit quantized model,\
          \ or 32 gigabytes of RAM for the 4-bit quantized model.</p>\n</li>\n</ol>\n\
          <h1 id=\"installation\">Installation</h1>\n<ol>\n<li>Download the zip file\
          \ of the one-click installer for Windows from: <a rel=\"nofollow\" href=\"\
          https://github.com/oobabooga/text-generation-webui\">https://github.com/oobabooga/text-generation-webui</a></li>\n\
          <li>Extract the zip file to <code>C:/ai/oobabooga_windows</code></li>\n\
          <li>Double-click the file <code>C:/ai/oobabooga_windows/start_windows.bat</code>\
          \ (no need to run as admin).</li>\n<li>In the opened CMD window, wait half\
          \ a minute and then choose \"Nvidia GPU\" or based on whatever GPU you have\
          \ on your system. If you're not sure, choose \"Nvidia GPU\" (yes, even for\
          \ an AMD integrated GPU the Nvidia option will still probably work).</li>\n\
          <li>Wait 20 minutes until start_windows.bat finishes installing oobabooga.</li>\n\
          </ol>\n<h1 id=\"configuration\">Configuration</h1>\n<ol>\n<li>Edit the empty\
          \ file: <code>C:/ai/oobabooga_windows/CMD_FLAGS.txt</code> and place the\
          \ text: <code>--listen --model airoboros-c34b-2.1.Q8_0.gguf --loader llamacpp\
          \ --threads 8 --n_ctx 4096</code>. Adjust according to the number of physical\
          \ cores on your CPU in order to get the full power of your CPU, adjust according\
          \ to the model quantization level you're planning to use.</li>\n<li>Copy\
          \ the file: <code>C:/ai/oobabooga_windows/text-generation-webui/settings-template.yaml</code>\
          \ so that you have a new file with identical contents named: <code>C:/ai/oobabooga_windows/text-generation-webui/settings.yaml</code>.\
          \ This allows you to customize the default oobabooga settings.</li>\n<li>There\
          \ are many settings in the file <code>C:/ai/oobabooga_windows/text-generation-webui/settings.yaml</code>.\
          \ Make sure that these very specific options are set to the correct values:</li>\n\
          </ol>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">max_new_tokens:</span>\
          \ <span class=\"hljs-number\">4096</span>\n<span class=\"hljs-attr\">truncation_length:</span>\
          \ <span class=\"hljs-number\">4096</span>\n<span class=\"hljs-attr\">ban_eos_token:</span>\
          \ <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">add_bos_token:</span>\
          \ <span class=\"hljs-literal\">false</span>\n</code></pre>\n<p>Edit: I'm\
          \ not actually sure about that. The correct setting might actually include\
          \ the bos token:</p>\n<pre><code class=\"language-yaml\"><span class=\"\
          hljs-attr\">max_new_tokens:</span> <span class=\"hljs-number\">4096</span>\n\
          <span class=\"hljs-attr\">truncation_length:</span> <span class=\"hljs-number\"\
          >4096</span>\n<span class=\"hljs-attr\">ban_eos_token:</span> <span class=\"\
          hljs-literal\">false</span>\n<span class=\"hljs-attr\">add_bos_token:</span>\
          \ <span class=\"hljs-literal\">true</span>\n</code></pre>\n<p>See my confusion\
          \ regarding the prompt template in this discussion: <a href=\"https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/2#64ee670981e61fdcf76117eb\"\
          >https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/2#64ee670981e61fdcf76117eb</a><br>4.\
          \ Change the contents of the file: <code>C:/ai/oobabooga_windows/text-generation-webui/prompts/QA.txt</code>\
          \ from the default value of:</p>\n<pre><code class=\"language-Python3\"\
          >\"\"\"Common sense questions and answers\n\nQuestion: \nFactual answer:\n\
          \"\"\"\n</code></pre>\n<p>to the value fitting the airoboros-c34b-2.1 model\
          \ prompt template:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-string\">\"\"\"A chat.</span>\n<span class=\"hljs-string\">USER: Can\
          \ you explain the difference between vector and dequeue in C++?</span>\n\
          <span class=\"hljs-string\">ASSISTANT: \"\"\"</span>\n</code></pre>\n<p>Make\
          \ sure to use a text editor that doesn't add that problematic newline at\
          \ the end of the QA.txt text file. Use notepad++ to make the edit, instead\
          \ of regular notepad in order to make sure of the precise prompt template.<br>4.\
          \ Create a folder called: <code>C:/ai/oobabooga_windows/text-generation-webui/models/airoboros-c34b-2.1.Q8_0.gguf</code>.\
          \ Adjust according to the model quantization level that you chose.<br>5.\
          \ Download the model from the huggingface GUI from this repo: <a href=\"\
          https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF\">https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF</a>\
          \ and place it directly inside of the folder that you created in the previous\
          \ step. You only need that 1 *.gguf file, so use the browser to download\
          \ the file. I recommend <code>airoboros-c34b-2.1.Q5_K_M.gguf</code> if you\
          \ have only 32 gigabytes of RAM.</p>\n<h1 id=\"running-the-model\">Running\
          \ the model</h1>\n<ol>\n<li>double-click: <code>C:/ai/oobabooga_windows/cmd_windows.bat</code>\
          \ such that a CMD window opens.</li>\n<li>Run the command: <code>python\
          \ webui.py</code> and if everything works correctly, you should see the\
          \ output:</li>\n</ol>\n<pre><code class=\"language-cmd\"><span class=\"\
          hljs-function\">llm_load_tensors: <span class=\"hljs-title\">mem</span>\
          \ <span class=\"hljs-title\">required</span>  = 34133.87 <span class=\"\
          hljs-title\">MB</span> (+  768.00 <span class=\"hljs-title\">MB</span> <span\
          \ class=\"hljs-title\">per</span> <span class=\"hljs-title\">state</span>)</span>\n\
          <span class=\"hljs-function\"><span class=\"hljs-title\">llm_load_tensors</span>:\
          \ <span class=\"hljs-title\">offloading</span> 0 <span class=\"hljs-title\"\
          >repeating</span> <span class=\"hljs-title\">layers</span> <span class=\"\
          hljs-title\">to</span> <span class=\"hljs-title\">GPU</span></span>\n<span\
          \ class=\"hljs-function\"><span class=\"hljs-title\">llm_load_tensors</span>:\
          \ <span class=\"hljs-title\">offloaded</span> 0/51 <span class=\"hljs-title\"\
          >layers</span> <span class=\"hljs-title\">to</span> <span class=\"hljs-title\"\
          >GPU</span></span>\n<span class=\"hljs-function\"><span class=\"hljs-title\"\
          >llm_load_tensors</span>: <span class=\"hljs-title\">VRAM</span> <span class=\"\
          hljs-title\">used</span>: 0 <span class=\"hljs-title\">MB</span></span>\n\
          <span class=\"hljs-function\">....................................................................................................</span>\n\
          <span class=\"hljs-function\"><span class=\"hljs-title\">llama_new_context_with_model</span>:\
          \ <span class=\"hljs-title\">kv</span> <span class=\"hljs-title\">self</span>\
          \ <span class=\"hljs-title\">size</span>  =  768.00 <span class=\"hljs-title\"\
          >MB</span></span>\n<span class=\"hljs-function\"><span class=\"hljs-title\"\
          >llama_new_context_with_model</span>: <span class=\"hljs-title\">compute</span>\
          \ <span class=\"hljs-title\">buffer</span> <span class=\"hljs-title\">total</span>\
          \ <span class=\"hljs-title\">size</span> =  561.41 <span class=\"hljs-title\"\
          >MB</span></span>\n<span class=\"hljs-function\"><span class=\"hljs-title\"\
          >llama_new_context_with_model</span>: <span class=\"hljs-title\">VRAM</span>\
          \ <span class=\"hljs-title\">scratch</span> <span class=\"hljs-title\">buffer</span>:\
          \ 560.00 <span class=\"hljs-title\">MB</span></span>\n<span class=\"hljs-function\"\
          ><span class=\"hljs-title\">AVX</span> = 1 | <span class=\"hljs-title\"\
          >AVX2</span> = 1 | <span class=\"hljs-title\">AVX512</span> = 0 | <span\
          \ class=\"hljs-title\">AVX512_VBMI</span> = 0 | <span class=\"hljs-title\"\
          >AVX512_VNNI</span> = 0 | <span class=\"hljs-title\">FMA</span> = 1 | <span\
          \ class=\"hljs-title\">NEON</span> = 0 | <span class=\"hljs-title\">ARM_FMA</span>\
          \ = 0 | <span class=\"hljs-title\">F16C</span> = 1 | <span class=\"hljs-title\"\
          >FP16_VA</span> = 0 | <span class=\"hljs-title\">WASM_SIMD</span> = 0 |\
          \ <span class=\"hljs-title\">BLAS</span> = 1 | <span class=\"hljs-title\"\
          >SSE3</span> = 1 | <span class=\"hljs-title\">SSSE3</span> = 0 | <span class=\"\
          hljs-title\">VSX</span> = 0 |</span>\n<span class=\"hljs-function\">2023-08-29\
          \ 14:48:12 <span class=\"hljs-title\">INFO:Loaded</span> <span class=\"\
          hljs-title\">the</span> <span class=\"hljs-title\">model</span> <span class=\"\
          hljs-title\">in</span> 3.86 <span class=\"hljs-title\">seconds</span>.</span>\n\
          <span class=\"hljs-function\"></span>\n<span class=\"hljs-function\">2023-08-29\
          \ 14:48:12 <span class=\"hljs-title\">INFO:Loading</span> <span class=\"\
          hljs-title\">the</span> <span class=\"hljs-title\">extension</span> \"<span\
          \ class=\"hljs-title\">gallery</span>\"...</span>\n<span class=\"hljs-function\"\
          ><span class=\"hljs-title\">Running</span> <span class=\"hljs-title\">on</span>\
          \ <span class=\"hljs-title\">local</span> <span class=\"hljs-title\">URL</span>:\
          \  <span class=\"hljs-title\">http</span>://0.0.0.0:7860</span>\n<span class=\"\
          hljs-function\"></span>\n<span class=\"hljs-function\"><span class=\"hljs-title\"\
          >To</span> <span class=\"hljs-title\">create</span> <span class=\"hljs-title\"\
          >a</span> <span class=\"hljs-title\">public</span> <span class=\"hljs-title\"\
          >link</span>, <span class=\"hljs-title\">set</span> `<span class=\"hljs-title\"\
          >share</span>=<span class=\"hljs-title\">True</span>` <span class=\"hljs-title\"\
          >in</span> `<span class=\"hljs-title\">launch</span>()`.</span>\n</code></pre>\n\
          <h1 id=\"using-the-model\">Using the model</h1>\n<ol>\n<li>Open the Firefox\
          \ web browser on either your local Windows computer, or on a remote compute\
          \ on the same network and enter the IP address: \"localhost:7860\" and press\
          \ ENTER.</li>\n<li>Paste the prompt template in the \"Default\" tab in the\
          \ webui.<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/PCELtY-V9xmuaNwxZx04L.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/PCELtY-V9xmuaNwxZx04L.png\"\
          ></a><br>The prompt template is super sensitive to spaces / newlines.<br>The\
          \ correct prompt template that I saw working is:</li>\n</ol>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-string\">\"\"\"A chat.</span>\n\
          <span class=\"hljs-string\">USER: Tell me a joke.</span>\n<span class=\"\
          hljs-string\">ASSISTANT: \"\"\"</span>\n</code></pre>\n<p>Of course, the\
          \ prompt doesn't include those triple quotation marks.<br>Note the lack\
          \ of a newline at the beginner of the prompt template, note that newline\
          \ after each line, and note that the last line (ASSISTANT) doesn't have\
          \ a newline, it has a single space instead.<br>Multi-turn conversations\
          \ do work, here is the prompt template:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-string\">\"\"\"A chat.</span>\n<span class=\"hljs-string\"\
          >USER: Tell me a joke.</span>\n<span class=\"hljs-string\">ASSISTANT: Why\
          \ don't secrets ever get lost?</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">They always pop up when you least expect them!</span>\n\
          <span class=\"hljs-string\">USER: Explain the joke</span>\n<span class=\"\
          hljs-string\">ASSISTANT: \"\"\"</span>\n</code></pre>\n<p>Of course, the\
          \ prompt doesn't include those triple quotation marks.<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/EV8AxNfZb2baXdJ20wmXE.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/EV8AxNfZb2baXdJ20wmXE.png\"\
          ></a></p>\n<p>Yes, I know that LLMs don't know how to tell jokes yet, but\
          \ it tries its best to explain that non-sensical joke\U0001F61C</p>\n"
        raw: "Instructions for running using https://github.com/oobabooga/text-generation-webui\
          \ on Windows 11 on CPU with llama.cpp, with GPU acceleration enabled for\
          \ the purpose of faster prompt ingestion (x26 speed).\nOn my laptop, the\
          \ model runs at 1.0 token per second, but because I'm using the BLAS GPU\
          \ acceleration, my prompt gets processed by llama.cpp at 26 tokens per second,\
          \ so I don't have to wait long until the AI starts responding.\n\n# Prerequisites\n\
          1. At least one of:\na. An integrated GPU with at least 500 megabytes of\
          \ dedicated VRAM.\nb. A dedicated AMD GPU with at least 4 gigabytes of dedicated\
          \ VRAM, and AMD ROCm drivers installed globally on the system.\nc. A dedicated\
          \ Nvidia GPU with at least 4 gigabytes of dedicated VRAM, and Nvidia CUDA\
          \ drivers installed globally on the system.\n\n2. Windows 11 with Git installed\n\
          \n3. Minimum RAM (regular RAM, not VRAM)- 64 gigabytes of RAM for the 8-bit\
          \ quantized model, or 32 gigabytes of RAM for the 4-bit quantized model.\n\
          \n# Installation\n1. Download the zip file of the one-click installer for\
          \ Windows from: https://github.com/oobabooga/text-generation-webui\n2. Extract\
          \ the zip file to `C:/ai/oobabooga_windows`\n3. Double-click the file `C:/ai/oobabooga_windows/start_windows.bat`\
          \ (no need to run as admin).\n4. In the opened CMD window, wait half a minute\
          \ and then choose \"Nvidia GPU\" or based on whatever GPU you have on your\
          \ system. If you're not sure, choose \"Nvidia GPU\" (yes, even for an AMD\
          \ integrated GPU the Nvidia option will still probably work).\n5. Wait 20\
          \ minutes until start_windows.bat finishes installing oobabooga.\n\n# Configuration\n\
          1. Edit the empty file: `C:/ai/oobabooga_windows/CMD_FLAGS.txt` and place\
          \ the text: `--listen --model airoboros-c34b-2.1.Q8_0.gguf --loader llamacpp\
          \ --threads 8 --n_ctx 4096`. Adjust according to the number of physical\
          \ cores on your CPU in order to get the full power of your CPU, adjust according\
          \ to the model quantization level you're planning to use.\n2. Copy the file:\
          \ `C:/ai/oobabooga_windows/text-generation-webui/settings-template.yaml`\
          \ so that you have a new file with identical contents named: `C:/ai/oobabooga_windows/text-generation-webui/settings.yaml`.\
          \ This allows you to customize the default oobabooga settings.\n3. There\
          \ are many settings in the file `C:/ai/oobabooga_windows/text-generation-webui/settings.yaml`.\
          \ Make sure that these very specific options are set to the correct values:\n\
          ```yaml\nmax_new_tokens: 4096\ntruncation_length: 4096\nban_eos_token: false\n\
          add_bos_token: false\n```\nEdit: I'm not actually sure about that. The correct\
          \ setting might actually include the bos token:\n```yaml\nmax_new_tokens:\
          \ 4096\ntruncation_length: 4096\nban_eos_token: false\nadd_bos_token: true\n\
          ```\nSee my confusion regarding the prompt template in this discussion:\
          \ https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/2#64ee670981e61fdcf76117eb\n\
          4. Change the contents of the file: `C:/ai/oobabooga_windows/text-generation-webui/prompts/QA.txt`\
          \ from the default value of:\n```Python3\n\"\"\"Common sense questions and\
          \ answers\n\nQuestion: \nFactual answer:\n\"\"\"\n```\nto the value fitting\
          \ the airoboros-c34b-2.1 model prompt template:\n```python\n\"\"\"A chat.\n\
          USER: Can you explain the difference between vector and dequeue in C++?\n\
          ASSISTANT: \"\"\"\n```\nMake sure to use a text editor that doesn't add\
          \ that problematic newline at the end of the QA.txt text file. Use notepad++\
          \ to make the edit, instead of regular notepad in order to make sure of\
          \ the precise prompt template.\n4. Create a folder called: `C:/ai/oobabooga_windows/text-generation-webui/models/airoboros-c34b-2.1.Q8_0.gguf`.\
          \ Adjust according to the model quantization level that you chose.\n5. Download\
          \ the model from the huggingface GUI from this repo: https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF\
          \ and place it directly inside of the folder that you created in the previous\
          \ step. You only need that 1 *.gguf file, so use the browser to download\
          \ the file. I recommend `airoboros-c34b-2.1.Q5_K_M.gguf` if you have only\
          \ 32 gigabytes of RAM.\n\n# Running the model\n1. double-click: `C:/ai/oobabooga_windows/cmd_windows.bat`\
          \ such that a CMD window opens.\n2. Run the command: `python webui.py` and\
          \ if everything works correctly, you should see the output:\n```cmd\nllm_load_tensors:\
          \ mem required  = 34133.87 MB (+  768.00 MB per state)\nllm_load_tensors:\
          \ offloading 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/51\
          \ layers to GPU\nllm_load_tensors: VRAM used: 0 MB\n....................................................................................................\n\
          llama_new_context_with_model: kv self size  =  768.00 MB\nllama_new_context_with_model:\
          \ compute buffer total size =  561.41 MB\nllama_new_context_with_model:\
          \ VRAM scratch buffer: 560.00 MB\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX =\
          \ 0 |\n2023-08-29 14:48:12 INFO:Loaded the model in 3.86 seconds.\n\n2023-08-29\
          \ 14:48:12 INFO:Loading the extension \"gallery\"...\nRunning on local URL:\
          \  http://0.0.0.0:7860\n\nTo create a public link, set `share=True` in `launch()`.\n\
          ```\n\n# Using the model\n1. Open the Firefox web browser on either your\
          \ local Windows computer, or on a remote compute on the same network and\
          \ enter the IP address: \"localhost:7860\" and press ENTER.\n2. Paste the\
          \ prompt template in the \"Default\" tab in the webui.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/PCELtY-V9xmuaNwxZx04L.png)\n\
          The prompt template is super sensitive to spaces / newlines.\nThe correct\
          \ prompt template that I saw working is:\n```python\n\"\"\"A chat.\nUSER:\
          \ Tell me a joke.\nASSISTANT: \"\"\"\n```\nOf course, the prompt doesn't\
          \ include those triple quotation marks.\nNote the lack of a newline at the\
          \ beginner of the prompt template, note that newline after each line, and\
          \ note that the last line (ASSISTANT) doesn't have a newline, it has a single\
          \ space instead.\nMulti-turn conversations do work, here is the prompt template:\n\
          ```python\n\"\"\"A chat.\nUSER: Tell me a joke.\nASSISTANT: Why don't secrets\
          \ ever get lost?\n\nThey always pop up when you least expect them!\nUSER:\
          \ Explain the joke\nASSISTANT: \"\"\"\n```\nOf course, the prompt doesn't\
          \ include those triple quotation marks.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/EV8AxNfZb2baXdJ20wmXE.png)\n\
          \nYes, I know that LLMs don't know how to tell jokes yet, but it tries its\
          \ best to explain that non-sensical joke\U0001F61C"
        updatedAt: '2023-08-30T14:32:44.170Z'
      numEdits: 6
      reactions: []
    id: 64edee8c9e9c0fc5f6df5b34
    type: comment
  author: ronenzyroff
  content: "Instructions for running using https://github.com/oobabooga/text-generation-webui\
    \ on Windows 11 on CPU with llama.cpp, with GPU acceleration enabled for the purpose\
    \ of faster prompt ingestion (x26 speed).\nOn my laptop, the model runs at 1.0\
    \ token per second, but because I'm using the BLAS GPU acceleration, my prompt\
    \ gets processed by llama.cpp at 26 tokens per second, so I don't have to wait\
    \ long until the AI starts responding.\n\n# Prerequisites\n1. At least one of:\n\
    a. An integrated GPU with at least 500 megabytes of dedicated VRAM.\nb. A dedicated\
    \ AMD GPU with at least 4 gigabytes of dedicated VRAM, and AMD ROCm drivers installed\
    \ globally on the system.\nc. A dedicated Nvidia GPU with at least 4 gigabytes\
    \ of dedicated VRAM, and Nvidia CUDA drivers installed globally on the system.\n\
    \n2. Windows 11 with Git installed\n\n3. Minimum RAM (regular RAM, not VRAM)-\
    \ 64 gigabytes of RAM for the 8-bit quantized model, or 32 gigabytes of RAM for\
    \ the 4-bit quantized model.\n\n# Installation\n1. Download the zip file of the\
    \ one-click installer for Windows from: https://github.com/oobabooga/text-generation-webui\n\
    2. Extract the zip file to `C:/ai/oobabooga_windows`\n3. Double-click the file\
    \ `C:/ai/oobabooga_windows/start_windows.bat` (no need to run as admin).\n4. In\
    \ the opened CMD window, wait half a minute and then choose \"Nvidia GPU\" or\
    \ based on whatever GPU you have on your system. If you're not sure, choose \"\
    Nvidia GPU\" (yes, even for an AMD integrated GPU the Nvidia option will still\
    \ probably work).\n5. Wait 20 minutes until start_windows.bat finishes installing\
    \ oobabooga.\n\n# Configuration\n1. Edit the empty file: `C:/ai/oobabooga_windows/CMD_FLAGS.txt`\
    \ and place the text: `--listen --model airoboros-c34b-2.1.Q8_0.gguf --loader\
    \ llamacpp --threads 8 --n_ctx 4096`. Adjust according to the number of physical\
    \ cores on your CPU in order to get the full power of your CPU, adjust according\
    \ to the model quantization level you're planning to use.\n2. Copy the file: `C:/ai/oobabooga_windows/text-generation-webui/settings-template.yaml`\
    \ so that you have a new file with identical contents named: `C:/ai/oobabooga_windows/text-generation-webui/settings.yaml`.\
    \ This allows you to customize the default oobabooga settings.\n3. There are many\
    \ settings in the file `C:/ai/oobabooga_windows/text-generation-webui/settings.yaml`.\
    \ Make sure that these very specific options are set to the correct values:\n\
    ```yaml\nmax_new_tokens: 4096\ntruncation_length: 4096\nban_eos_token: false\n\
    add_bos_token: false\n```\nEdit: I'm not actually sure about that. The correct\
    \ setting might actually include the bos token:\n```yaml\nmax_new_tokens: 4096\n\
    truncation_length: 4096\nban_eos_token: false\nadd_bos_token: true\n```\nSee my\
    \ confusion regarding the prompt template in this discussion: https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/2#64ee670981e61fdcf76117eb\n\
    4. Change the contents of the file: `C:/ai/oobabooga_windows/text-generation-webui/prompts/QA.txt`\
    \ from the default value of:\n```Python3\n\"\"\"Common sense questions and answers\n\
    \nQuestion: \nFactual answer:\n\"\"\"\n```\nto the value fitting the airoboros-c34b-2.1\
    \ model prompt template:\n```python\n\"\"\"A chat.\nUSER: Can you explain the\
    \ difference between vector and dequeue in C++?\nASSISTANT: \"\"\"\n```\nMake\
    \ sure to use a text editor that doesn't add that problematic newline at the end\
    \ of the QA.txt text file. Use notepad++ to make the edit, instead of regular\
    \ notepad in order to make sure of the precise prompt template.\n4. Create a folder\
    \ called: `C:/ai/oobabooga_windows/text-generation-webui/models/airoboros-c34b-2.1.Q8_0.gguf`.\
    \ Adjust according to the model quantization level that you chose.\n5. Download\
    \ the model from the huggingface GUI from this repo: https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF\
    \ and place it directly inside of the folder that you created in the previous\
    \ step. You only need that 1 *.gguf file, so use the browser to download the file.\
    \ I recommend `airoboros-c34b-2.1.Q5_K_M.gguf` if you have only 32 gigabytes of\
    \ RAM.\n\n# Running the model\n1. double-click: `C:/ai/oobabooga_windows/cmd_windows.bat`\
    \ such that a CMD window opens.\n2. Run the command: `python webui.py` and if\
    \ everything works correctly, you should see the output:\n```cmd\nllm_load_tensors:\
    \ mem required  = 34133.87 MB (+  768.00 MB per state)\nllm_load_tensors: offloading\
    \ 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/51 layers to GPU\n\
    llm_load_tensors: VRAM used: 0 MB\n....................................................................................................\n\
    llama_new_context_with_model: kv self size  =  768.00 MB\nllama_new_context_with_model:\
    \ compute buffer total size =  561.41 MB\nllama_new_context_with_model: VRAM scratch\
    \ buffer: 560.00 MB\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\n2023-08-29 14:48:12 INFO:Loaded\
    \ the model in 3.86 seconds.\n\n2023-08-29 14:48:12 INFO:Loading the extension\
    \ \"gallery\"...\nRunning on local URL:  http://0.0.0.0:7860\n\nTo create a public\
    \ link, set `share=True` in `launch()`.\n```\n\n# Using the model\n1. Open the\
    \ Firefox web browser on either your local Windows computer, or on a remote compute\
    \ on the same network and enter the IP address: \"localhost:7860\" and press ENTER.\n\
    2. Paste the prompt template in the \"Default\" tab in the webui.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/PCELtY-V9xmuaNwxZx04L.png)\n\
    The prompt template is super sensitive to spaces / newlines.\nThe correct prompt\
    \ template that I saw working is:\n```python\n\"\"\"A chat.\nUSER: Tell me a joke.\n\
    ASSISTANT: \"\"\"\n```\nOf course, the prompt doesn't include those triple quotation\
    \ marks.\nNote the lack of a newline at the beginner of the prompt template, note\
    \ that newline after each line, and note that the last line (ASSISTANT) doesn't\
    \ have a newline, it has a single space instead.\nMulti-turn conversations do\
    \ work, here is the prompt template:\n```python\n\"\"\"A chat.\nUSER: Tell me\
    \ a joke.\nASSISTANT: Why don't secrets ever get lost?\n\nThey always pop up when\
    \ you least expect them!\nUSER: Explain the joke\nASSISTANT: \"\"\"\n```\nOf course,\
    \ the prompt doesn't include those triple quotation marks.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6454215a06728ff79a3685bf/EV8AxNfZb2baXdJ20wmXE.png)\n\
    \nYes, I know that LLMs don't know how to tell jokes yet, but it tries its best\
    \ to explain that non-sensical joke\U0001F61C"
  created_at: 2023-08-29 12:11:40+00:00
  edited: true
  hidden: false
  id: 64edee8c9e9c0fc5f6df5b34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZdU0rWpmK17L7-ECZI3-H.png?w=200&h=200&f=face
      fullname: Dan Simmonds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BingoBird
      type: user
    createdAt: '2023-09-01T05:40:56.000Z'
    data:
      edited: false
      editors:
      - BingoBird
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45182672142982483
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZdU0rWpmK17L7-ECZI3-H.png?w=200&h=200&f=face
          fullname: Dan Simmonds
          isHf: false
          isPro: false
          name: BingoBird
          type: user
        html: '<p>/dl/Projects/Neural/LLM/llama.cpp/./main  -m ./airoboros-c34b-2.1.Q5_K_M.gguf   -f
          ./testprompt.txt<br>main: build = 1083 (c1ac54b)<br>main: seed  = 1693546812<br>ggml_init_cublas:
          found 1 CUDA devices:<br>  Device 0: NVIDIA GeForce GTX 1070, compute capability
          6.1<br>Segmentation fault</p>

          '
        raw: "/dl/Projects/Neural/LLM/llama.cpp/./main  -m ./airoboros-c34b-2.1.Q5_K_M.gguf\
          \   -f ./testprompt.txt\nmain: build = 1083 (c1ac54b)\nmain: seed  = 1693546812\n\
          ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce GTX\
          \ 1070, compute capability 6.1\nSegmentation fault\n"
        updatedAt: '2023-09-01T05:40:56.730Z'
      numEdits: 0
      reactions: []
    id: 64f179688b4dcf22afcc505a
    type: comment
  author: BingoBird
  content: "/dl/Projects/Neural/LLM/llama.cpp/./main  -m ./airoboros-c34b-2.1.Q5_K_M.gguf\
    \   -f ./testprompt.txt\nmain: build = 1083 (c1ac54b)\nmain: seed  = 1693546812\n\
    ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce GTX 1070,\
    \ compute capability 6.1\nSegmentation fault\n"
  created_at: 2023-09-01 04:40:56+00:00
  edited: false
  hidden: false
  id: 64f179688b4dcf22afcc505a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Airoboros-c34B-2.1-GGUF
repo_type: model
status: open
target_branch: null
title: Best open source model for coding (August 2023)
