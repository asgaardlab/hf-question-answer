!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nacs
conflicting_files: []
created_at: 2023-08-29 20:21:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-08-29T21:21:46.000Z'
    data:
      edited: false
      editors:
      - nacs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24593819677829742
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: '<p>Airoboros uses Vicuna style prompt (not Alpaca)</p>

          '
        raw: Airoboros uses Vicuna style prompt (not Alpaca)
        updatedAt: '2023-08-29T21:21:46.644Z'
      numEdits: 0
      reactions: []
    id: 64ee616af9ed75901c0b5047
    type: comment
  author: nacs
  content: Airoboros uses Vicuna style prompt (not Alpaca)
  created_at: 2023-08-29 20:21:46+00:00
  edited: false
  hidden: false
  id: 64ee616af9ed75901c0b5047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-08-29T21:21:47.000Z'
    data:
      oid: e8393d61ec9236075184a66f0e297081b7953561
      parents:
      - 890b9dca7251a0370dfd7b35da6e81e90685c18a
      subject: Fix prompt format in llama.cpp command
    id: 64ee616b0000000000000000
    type: commit
  author: nacs
  created_at: 2023-08-29 20:21:47+00:00
  id: 64ee616b0000000000000000
  oid: e8393d61ec9236075184a66f0e297081b7953561
  summary: Fix prompt format in llama.cpp command
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-29T21:24:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9668474197387695
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>huh, yeah that''s not right. I thought I had that working to put
          in the right prompt template for GGML/GGUF files.  Maybe a regression specific
          to GGUF, I''ll check it.  Thanks for the PR</p>

          '
        raw: huh, yeah that's not right. I thought I had that working to put in the
          right prompt template for GGML/GGUF files.  Maybe a regression specific
          to GGUF, I'll check it.  Thanks for the PR
        updatedAt: '2023-08-29T21:24:06.819Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nacs
      relatedEventId: 64ee61f6962cc529616244bd
    id: 64ee61f6962cc529616244ba
    type: comment
  author: TheBloke
  content: huh, yeah that's not right. I thought I had that working to put in the
    right prompt template for GGML/GGUF files.  Maybe a regression specific to GGUF,
    I'll check it.  Thanks for the PR
  created_at: 2023-08-29 20:24:06+00:00
  edited: false
  hidden: false
  id: 64ee61f6962cc529616244ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-29T21:24:06.000Z'
    data:
      status: merged
    id: 64ee61f6962cc529616244bd
    type: status-change
  author: TheBloke
  created_at: 2023-08-29 20:24:06+00:00
  id: 64ee61f6962cc529616244bd
  new_status: merged
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-08-29T21:45:45.000Z'
    data:
      edited: true
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8588838577270508
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: '<p>That''s not the correct prompt format either.<br>This <a rel="nofollow"
          href="https://chat.openai.com/share/8e98059c-723a-460f-88f4-f010ec996925">https://chat.openai.com/share/8e98059c-723a-460f-88f4-f010ec996925</a>
          analysis of the source code of QLORA tells you what the valid prompt formats
          are.<br>According to the README here <a href="https://huggingface.co/jondurbin/airoboros-c34b-2.1">https://huggingface.co/jondurbin/airoboros-c34b-2.1</a></p>

          <pre><code class="language-Text">Prompt format


          The training code was updated to randomize newline vs space: https://github.com/jondurbin/qlora/blob/main/qlora.py#L559C1-L559C1

          </code></pre>

          <p>Via experimentation and looking at the source code of QLORA, I arrived
          at the conclusion that there''s a prompt format for airoboros-c34b-2.1 that
          works for every situation.</p>

          <p>Look at my post here that mentions that prompt format several times:
          <a href="https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/1#64edee8c9e9c0fc5f6df5b34">https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/1#64edee8c9e9c0fc5f6df5b34</a></p>

          <p>Long story short:</p>

          <pre><code class="language-Python3">"""A chat.

          USER: Can you explain the difference between vector and dequeue in C++?

          ASSISTANT: """

          </code></pre>

          <p>Notice the single space character after the <code>ASSISTANT:</code>,
          and notice the period after the <code>A chat</code>.</p>

          <p>As you can see in ChatGPT-4''s analysis of the source code of QLORA (that
          I linked above),<br>there is no valid prompt format that involves a newline
          character after the <code>ASSISTANT:</code>, it''s always a single space
          after <code>ASSISTANT:</code>.</p>

          <p>A weird thing that I can''t explain: When providing the prompt without
          a space after <code>ASSISTANT:</code>, like this:</p>

          <pre><code class="language-Python3">"""A chat.

          USER: Can you explain the difference between vector and dequeue in C++?

          ASSISTANT:"""

          </code></pre>

          <p>the model always adds a space (but seems to get a little confused).</p>

          <p>Then, when using the (supposedly) correct prompt format:</p>

          <pre><code class="language-Python3">"""A chat.

          USER: Can you explain the difference between vector and dequeue in C++?

          ASSISTANT: """

          </code></pre>

          <p>The model always adds 1 additional space to the response, such that we
          get 2 spaces, like this: <code>ASSISTANT:  </code> (2 spaces).<br>When I
          use the supposedly correct prompt format, the AI model produces high-quality
          responses, but it tends to get confused and sometimes include a double newline
          in its answer.</p>

          <p>What''s also odd is that in Jon Durbin''s README, he doesn''t use any
          space character after <code>ASSISTANT:</code>.</p>

          <p>Note also that oobabooga/text-generation-webui has this setting:<br><code>Add
          the bos_token to the beginning of prompts</code>.</p>

          <p>I don''t know what that setting does, it might be important. I decided
          to turn it off because it''s not part of the prompt format in <code>Airoboros-c34B-2.1</code>,
          as opposed to llama2-chat models in which I understood that the BOS token
          is part of the prompt format.</p>

          '
        raw: 'That''s not the correct prompt format either.

          This https://chat.openai.com/share/8e98059c-723a-460f-88f4-f010ec996925
          analysis of the source code of QLORA tells you what the valid prompt formats
          are.

          According to the README here https://huggingface.co/jondurbin/airoboros-c34b-2.1

          ```Text

          Prompt format


          The training code was updated to randomize newline vs space: https://github.com/jondurbin/qlora/blob/main/qlora.py#L559C1-L559C1

          ```

          Via experimentation and looking at the source code of QLORA, I arrived at
          the conclusion that there''s a prompt format for airoboros-c34b-2.1 that
          works for every situation.


          Look at my post here that mentions that prompt format several times: https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/1#64edee8c9e9c0fc5f6df5b34


          Long story short:

          ```Python3

          """A chat.

          USER: Can you explain the difference between vector and dequeue in C++?

          ASSISTANT: """

          ```

          Notice the single space character after the `ASSISTANT:`, and notice the
          period after the `A chat`.


          As you can see in ChatGPT-4''s analysis of the source code of QLORA (that
          I linked above),

          there is no valid prompt format that involves a newline character after
          the `ASSISTANT:`, it''s always a single space after `ASSISTANT:`.


          A weird thing that I can''t explain: When providing the prompt without a
          space after `ASSISTANT:`, like this:

          ```Python3

          """A chat.

          USER: Can you explain the difference between vector and dequeue in C++?

          ASSISTANT:"""

          ```

          the model always adds a space (but seems to get a little confused).


          Then, when using the (supposedly) correct prompt format:

          ```Python3

          """A chat.

          USER: Can you explain the difference between vector and dequeue in C++?

          ASSISTANT: """

          ```

          The model always adds 1 additional space to the response, such that we get
          2 spaces, like this: `ASSISTANT:  ` (2 spaces).

          When I use the supposedly correct prompt format, the AI model produces high-quality
          responses, but it tends to get confused and sometimes include a double newline
          in its answer.


          What''s also odd is that in Jon Durbin''s README, he doesn''t use any space
          character after `ASSISTANT:`.


          Note also that oobabooga/text-generation-webui has this setting:

          `Add the bos_token to the beginning of prompts`.


          I don''t know what that setting does, it might be important. I decided to
          turn it off because it''s not part of the prompt format in `Airoboros-c34B-2.1`,
          as opposed to llama2-chat models in which I understood that the BOS token
          is part of the prompt format.'
        updatedAt: '2023-08-29T21:55:10.542Z'
      numEdits: 2
      reactions: []
    id: 64ee670981e61fdcf76117eb
    type: comment
  author: ronenzyroff
  content: 'That''s not the correct prompt format either.

    This https://chat.openai.com/share/8e98059c-723a-460f-88f4-f010ec996925 analysis
    of the source code of QLORA tells you what the valid prompt formats are.

    According to the README here https://huggingface.co/jondurbin/airoboros-c34b-2.1

    ```Text

    Prompt format


    The training code was updated to randomize newline vs space: https://github.com/jondurbin/qlora/blob/main/qlora.py#L559C1-L559C1

    ```

    Via experimentation and looking at the source code of QLORA, I arrived at the
    conclusion that there''s a prompt format for airoboros-c34b-2.1 that works for
    every situation.


    Look at my post here that mentions that prompt format several times: https://huggingface.co/TheBloke/Airoboros-c34B-2.1-GGUF/discussions/1#64edee8c9e9c0fc5f6df5b34


    Long story short:

    ```Python3

    """A chat.

    USER: Can you explain the difference between vector and dequeue in C++?

    ASSISTANT: """

    ```

    Notice the single space character after the `ASSISTANT:`, and notice the period
    after the `A chat`.


    As you can see in ChatGPT-4''s analysis of the source code of QLORA (that I linked
    above),

    there is no valid prompt format that involves a newline character after the `ASSISTANT:`,
    it''s always a single space after `ASSISTANT:`.


    A weird thing that I can''t explain: When providing the prompt without a space
    after `ASSISTANT:`, like this:

    ```Python3

    """A chat.

    USER: Can you explain the difference between vector and dequeue in C++?

    ASSISTANT:"""

    ```

    the model always adds a space (but seems to get a little confused).


    Then, when using the (supposedly) correct prompt format:

    ```Python3

    """A chat.

    USER: Can you explain the difference between vector and dequeue in C++?

    ASSISTANT: """

    ```

    The model always adds 1 additional space to the response, such that we get 2 spaces,
    like this: `ASSISTANT:  ` (2 spaces).

    When I use the supposedly correct prompt format, the AI model produces high-quality
    responses, but it tends to get confused and sometimes include a double newline
    in its answer.


    What''s also odd is that in Jon Durbin''s README, he doesn''t use any space character
    after `ASSISTANT:`.


    Note also that oobabooga/text-generation-webui has this setting:

    `Add the bos_token to the beginning of prompts`.


    I don''t know what that setting does, it might be important. I decided to turn
    it off because it''s not part of the prompt format in `Airoboros-c34B-2.1`, as
    opposed to llama2-chat models in which I understood that the BOS token is part
    of the prompt format.'
  created_at: 2023-08-29 20:45:45+00:00
  edited: true
  hidden: false
  id: 64ee670981e61fdcf76117eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-29T21:47:28.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9425278306007385
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah that''s true - there shouldn''t be a newline after ASSISTANT:</p>

          <p>I''ve fixed the issue in my GGUF and GGML code that was causing the prompt
          templates not to merge into the llama.cpp command properly and will roll
          it out to these repos shortly</p>

          '
        raw: 'Yeah that''s true - there shouldn''t be a newline after ASSISTANT:


          I''ve fixed the issue in my GGUF and GGML code that was causing the prompt
          templates not to merge into the llama.cpp command properly and will roll
          it out to these repos shortly'
        updatedAt: '2023-08-29T21:47:28.997Z'
      numEdits: 0
      reactions: []
    id: 64ee6770d46736be6de8a5a9
    type: comment
  author: TheBloke
  content: 'Yeah that''s true - there shouldn''t be a newline after ASSISTANT:


    I''ve fixed the issue in my GGUF and GGML code that was causing the prompt templates
    not to merge into the llama.cpp command properly and will roll it out to these
    repos shortly'
  created_at: 2023-08-29 20:47:28+00:00
  edited: false
  hidden: false
  id: 64ee6770d46736be6de8a5a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-08-29T22:05:39.000Z'
    data:
      edited: false
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9289509057998657
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ I don't quite understand what you mean. I've recently added information\
          \ to my previous reply, and I mentioned 3 issues altogether:</p>\n<ol>\n\
          <li>No newline allowed after <code>ASSISTANT:</code></li>\n<li>Period required\
          \ after  <code>A chat</code></li>\n<li>Weird tendency for the model to add\
          \ an extra space character after <code>ASSISTANT:</code></li>\n</ol>\n"
        raw: 'Hi @TheBloke I don''t quite understand what you mean. I''ve recently
          added information to my previous reply, and I mentioned 3 issues altogether:

          1. No newline allowed after `ASSISTANT:`

          2. Period required after  `A chat`

          3. Weird tendency for the model to add an extra space character after `ASSISTANT:`'
        updatedAt: '2023-08-29T22:05:39.853Z'
      numEdits: 0
      reactions: []
    id: 64ee6bb3886255716b7f92b8
    type: comment
  author: ronenzyroff
  content: 'Hi @TheBloke I don''t quite understand what you mean. I''ve recently added
    information to my previous reply, and I mentioned 3 issues altogether:

    1. No newline allowed after `ASSISTANT:`

    2. Period required after  `A chat`

    3. Weird tendency for the model to add an extra space character after `ASSISTANT:`'
  created_at: 2023-08-29 21:05:39+00:00
  edited: false
  hidden: false
  id: 64ee6bb3886255716b7f92b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-29T22:29:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861895442008972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, I was saying that I will be updating the README for the correct
          prompt template as per the source README.</p>

          <p>However Jon has just told me that the C34B model had not been correctly
          updated in his repo.  So the updated model I uploaded today -second upload
          of this model - was not actually updated at all and was just the same as
          the first one, which is known to have various bugs.</p>

          <p>So I am now doing it again, for the third time.  This time it should
          finally work better - and then maybe the stated prompt template will work
          OK.</p>

          <p>Check back in an hour or so for the newly updated GGUFs, and in ~3 hours
          for updated GPTQs.</p>

          '
        raw: 'Yes, I was saying that I will be updating the README for the correct
          prompt template as per the source README.


          However Jon has just told me that the C34B model had not been correctly
          updated in his repo.  So the updated model I uploaded today -second upload
          of this model - was not actually updated at all and was just the same as
          the first one, which is known to have various bugs.


          So I am now doing it again, for the third time.  This time it should finally
          work better - and then maybe the stated prompt template will work OK.


          Check back in an hour or so for the newly updated GGUFs, and in ~3 hours
          for updated GPTQs.'
        updatedAt: '2023-08-29T22:29:42.555Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nacs
    id: 64ee715612f6864885f3e7d8
    type: comment
  author: TheBloke
  content: 'Yes, I was saying that I will be updating the README for the correct prompt
    template as per the source README.


    However Jon has just told me that the C34B model had not been correctly updated
    in his repo.  So the updated model I uploaded today -second upload of this model
    - was not actually updated at all and was just the same as the first one, which
    is known to have various bugs.


    So I am now doing it again, for the third time.  This time it should finally work
    better - and then maybe the stated prompt template will work OK.


    Check back in an hour or so for the newly updated GGUFs, and in ~3 hours for updated
    GPTQs.'
  created_at: 2023-08-29 21:29:42+00:00
  edited: false
  hidden: false
  id: 64ee715612f6864885f3e7d8
  type: comment
is_pull_request: true
merge_commit_oid: 8efe6576899adc16329be68ea9c4b6e2b0ca3174
num: 2
repo_id: TheBloke/Airoboros-c34B-2.1-GGUF
repo_type: model
status: merged
target_branch: refs/heads/main
title: Fix prompt format in llama.cpp command
