!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kopyl
conflicting_files: null
created_at: 2023-11-26 15:28:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6a6d922028e8b447b87fca56c00c679.svg
      fullname: T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kopyl
      type: user
    createdAt: '2023-11-26T15:28:24.000Z'
    data:
      edited: false
      editors:
      - kopyl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8023565411567688
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6a6d922028e8b447b87fca56c00c679.svg
          fullname: T
          isHf: false
          isPro: false
          name: kopyl
          type: user
        html: '<ol>

          <li>Why does this model require more resources than original one? <a href="https://huggingface.co/kandinsky-community/kandinsky-3">https://huggingface.co/kandinsky-community/kandinsky-3</a></li>

          <li>Why do you download <code>kandinsky3.pt</code> and <code>movq.pt</code>
          each time instead of loading from hf cache directory?</li>

          <li>Where do you store <code>kandinsky3.pt</code> and <code>movq.pt</code>?</li>

          <li>How to run the inference on RTX 4090? I was barely able to load it adding
          more RAM, but the inference does not work, even distribute on 2 RTX 4090
          like this:</li>

          </ol>

          <pre><code>import torch

          from accelerate import PartialState

          from diffusers import DiffusionPipeline



          distributed_state = PartialState()

          distributed_state.num_processes = 2


          t2i_pipe = get_T2I_pipeline(distributed_state.device, fp16=True)

          </code></pre>

          <p>Inference on A100 is horribly slow...</p>

          '
        raw: "1. Why does this model require more resources than original one? https://huggingface.co/kandinsky-community/kandinsky-3\r\
          \n2. Why do you download `kandinsky3.pt` and `movq.pt` each time instead\
          \ of loading from hf cache directory?\r\n3. Where do you store `kandinsky3.pt`\
          \ and `movq.pt`?\r\n4. How to run the inference on RTX 4090? I was barely\
          \ able to load it adding more RAM, but the inference does not work, even\
          \ distribute on 2 RTX 4090 like this:\r\n\r\n```\r\nimport torch\r\nfrom\
          \ accelerate import PartialState\r\nfrom diffusers import DiffusionPipeline\r\
          \n\r\n\r\ndistributed_state = PartialState()\r\ndistributed_state.num_processes\
          \ = 2\r\n\r\nt2i_pipe = get_T2I_pipeline(distributed_state.device, fp16=True)\r\
          \n````\r\n\r\nInference on A100 is horribly slow..."
        updatedAt: '2023-11-26T15:28:24.930Z'
      numEdits: 0
      reactions: []
    id: 656364186ef2a1d0f979da5e
    type: comment
  author: kopyl
  content: "1. Why does this model require more resources than original one? https://huggingface.co/kandinsky-community/kandinsky-3\r\
    \n2. Why do you download `kandinsky3.pt` and `movq.pt` each time instead of loading\
    \ from hf cache directory?\r\n3. Where do you store `kandinsky3.pt` and `movq.pt`?\r\
    \n4. How to run the inference on RTX 4090? I was barely able to load it adding\
    \ more RAM, but the inference does not work, even distribute on 2 RTX 4090 like\
    \ this:\r\n\r\n```\r\nimport torch\r\nfrom accelerate import PartialState\r\n\
    from diffusers import DiffusionPipeline\r\n\r\n\r\ndistributed_state = PartialState()\r\
    \ndistributed_state.num_processes = 2\r\n\r\nt2i_pipe = get_T2I_pipeline(distributed_state.device,\
    \ fp16=True)\r\n````\r\n\r\nInference on A100 is horribly slow..."
  created_at: 2023-11-26 15:28:24+00:00
  edited: false
  hidden: false
  id: 656364186ef2a1d0f979da5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671594780986-6340427077fd972573e6c398.jpeg?w=200&h=200&f=face
      fullname: Alexandr German
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrsteyk
      type: user
    createdAt: '2023-11-28T16:33:58.000Z'
    data:
      edited: true
      editors:
      - mrsteyk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9331333637237549
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671594780986-6340427077fd972573e6c398.jpeg?w=200&h=200&f=face
          fullname: Alexandr German
          isHf: false
          isPro: false
          name: mrsteyk
          type: user
        html: '<p>I was able to run it on 3060 12gb by changing T5Model to T5EncoderModel
          with 4bit stuff (making sure compute dtype is bfloat16 because original
          T5 checkpoints didn''t really like fp16) and loading models sequentially,
          as in: first load an encoder, encode text, unload and clear cache, load
          unet, inference, unload and clear cache, load movqgan and decode. But I
          think NF4 is biting me in the ass changing embeddings a bit too much for
          this model. It''s not that good to begin with...</p>

          <p>P.S. loading T5Encoder takes like 30-40s, while encoding is done in 2s.
          UNet constructor is 10s and loading is 7. MoVQGAN eats almost all my VRAM
          decoding 1024x1024.</p>

          '
        raw: 'I was able to run it on 3060 12gb by changing T5Model to T5EncoderModel
          with 4bit stuff (making sure compute dtype is bfloat16 because original
          T5 checkpoints didn''t really like fp16) and loading models sequentially,
          as in: first load an encoder, encode text, unload and clear cache, load
          unet, inference, unload and clear cache, load movqgan and decode. But I
          think NF4 is biting me in the ass changing embeddings a bit too much for
          this model. It''s not that good to begin with...


          P.S. loading T5Encoder takes like 30-40s, while encoding is done in 2s.
          UNet constructor is 10s and loading is 7. MoVQGAN eats almost all my VRAM
          decoding 1024x1024.'
        updatedAt: '2023-11-28T16:35:06.760Z'
      numEdits: 1
      reactions: []
    id: 65661676dd9e04e4cc87ae35
    type: comment
  author: mrsteyk
  content: 'I was able to run it on 3060 12gb by changing T5Model to T5EncoderModel
    with 4bit stuff (making sure compute dtype is bfloat16 because original T5 checkpoints
    didn''t really like fp16) and loading models sequentially, as in: first load an
    encoder, encode text, unload and clear cache, load unet, inference, unload and
    clear cache, load movqgan and decode. But I think NF4 is biting me in the ass
    changing embeddings a bit too much for this model. It''s not that good to begin
    with...


    P.S. loading T5Encoder takes like 30-40s, while encoding is done in 2s. UNet constructor
    is 10s and loading is 7. MoVQGAN eats almost all my VRAM decoding 1024x1024.'
  created_at: 2023-11-28 16:33:58+00:00
  edited: true
  hidden: false
  id: 65661676dd9e04e4cc87ae35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6a6d922028e8b447b87fca56c00c679.svg
      fullname: T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kopyl
      type: user
    createdAt: '2023-11-29T13:02:26.000Z'
    data:
      edited: false
      editors:
      - kopyl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9441795349121094
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6a6d922028e8b447b87fca56c00c679.svg
          fullname: T
          isHf: false
          isPro: false
          name: kopyl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mrsteyk&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mrsteyk\">@<span class=\"\
          underline\">mrsteyk</span></a></span>\n\n\t</span></span> thank you very\
          \ much. Can I please ask you how the quality of generation is doing compared\
          \ to the original encoder?</p>\n"
        raw: '@mrsteyk thank you very much. Can I please ask you how the quality of
          generation is doing compared to the original encoder?'
        updatedAt: '2023-11-29T13:02:26.995Z'
      numEdits: 0
      reactions: []
    id: 656736620ac92a7da96ed41e
    type: comment
  author: kopyl
  content: '@mrsteyk thank you very much. Can I please ask you how the quality of
    generation is doing compared to the original encoder?'
  created_at: 2023-11-29 13:02:26+00:00
  edited: false
  hidden: false
  id: 656736620ac92a7da96ed41e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671594780986-6340427077fd972573e6c398.jpeg?w=200&h=200&f=face
      fullname: Alexandr German
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrsteyk
      type: user
    createdAt: '2023-11-30T04:07:07.000Z'
    data:
      edited: false
      editors:
      - mrsteyk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9759860634803772
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671594780986-6340427077fd972573e6c398.jpeg?w=200&h=200&f=face
          fullname: Alexandr German
          isHf: false
          isPro: false
          name: mrsteyk
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kopyl&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kopyl\">@<span class=\"\
          underline\">kopyl</span></a></span>\n\n\t</span></span> I can't run the\
          \ original encoder, so can't say but model is the exact same. There were\
          \ similar tests for DeepFloyd's IF (ImagenFree) and it did change the outputs\
          \ (4 vs 6 vs 16). From my experience, this model is either undertrained\
          \ or NF4 ruins the understanding. There also might be a \"bug\" of using\
          \ fp16 for the encoder during training, which would also change the embeddings.</p>\n"
        raw: '@kopyl I can''t run the original encoder, so can''t say but model is
          the exact same. There were similar tests for DeepFloyd''s IF (ImagenFree)
          and it did change the outputs (4 vs 6 vs 16). From my experience, this model
          is either undertrained or NF4 ruins the understanding. There also might
          be a "bug" of using fp16 for the encoder during training, which would also
          change the embeddings.'
        updatedAt: '2023-11-30T04:07:07.316Z'
      numEdits: 0
      reactions: []
    id: 65680a6b5025f8e01b28feb1
    type: comment
  author: mrsteyk
  content: '@kopyl I can''t run the original encoder, so can''t say but model is the
    exact same. There were similar tests for DeepFloyd''s IF (ImagenFree) and it did
    change the outputs (4 vs 6 vs 16). From my experience, this model is either undertrained
    or NF4 ruins the understanding. There also might be a "bug" of using fp16 for
    the encoder during training, which would also change the embeddings.'
  created_at: 2023-11-30 04:07:07+00:00
  edited: false
  hidden: false
  id: 65680a6b5025f8e01b28feb1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ai-forever/Kandinsky3.0
repo_type: model
status: open
target_branch: null
title: A few questions
