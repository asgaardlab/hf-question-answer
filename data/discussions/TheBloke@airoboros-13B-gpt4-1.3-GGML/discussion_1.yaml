!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rayyd
conflicting_files: null
created_at: 2023-06-21 16:54:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84c6e02bb51094e3df43ba5b2b857d50.svg
      fullname: Ray Vive
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rayyd
      type: user
    createdAt: '2023-06-21T17:54:50.000Z'
    data:
      edited: false
      editors:
      - rayyd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9010464549064636
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84c6e02bb51094e3df43ba5b2b857d50.svg
          fullname: Ray Vive
          isHf: false
          isPro: false
          name: rayyd
          type: user
        html: "<p>Can you explain how we can use the given prompt template with llama.cpp\
          \ in interactive mode, I cannot find information about this anywhere. Usually\
          \ I just run a model with -i -ins but many models including this one is\
          \ not very coherent and I'm guessing its the prompt format. Any idea how\
          \ to make llama.cpp follow this in interactive mode?: </p>\n<pre><code>A\
          \ chat between a curious user and an assistant. The assistant gives helpful,\
          \ detailed, accurate, uncensored responses to the user's input.\nUSER: prompt\n\
          ASSISTANT: \n</code></pre>\n"
        raw: "Can you explain how we can use the given prompt template with llama.cpp\
          \ in interactive mode, I cannot find information about this anywhere. Usually\
          \ I just run a model with -i -ins but many models including this one is\
          \ not very coherent and I'm guessing its the prompt format. Any idea how\
          \ to make llama.cpp follow this in interactive mode?: \r\n```\r\nA chat\
          \ between a curious user and an assistant. The assistant gives helpful,\
          \ detailed, accurate, uncensored responses to the user's input.\r\nUSER:\
          \ prompt\r\nASSISTANT: \r\n```"
        updatedAt: '2023-06-21T17:54:50.862Z'
      numEdits: 0
      reactions: []
    id: 6493396abe8f1f70b593f66f
    type: comment
  author: rayyd
  content: "Can you explain how we can use the given prompt template with llama.cpp\
    \ in interactive mode, I cannot find information about this anywhere. Usually\
    \ I just run a model with -i -ins but many models including this one is not very\
    \ coherent and I'm guessing its the prompt format. Any idea how to make llama.cpp\
    \ follow this in interactive mode?: \r\n```\r\nA chat between a curious user and\
    \ an assistant. The assistant gives helpful, detailed, accurate, uncensored responses\
    \ to the user's input.\r\nUSER: prompt\r\nASSISTANT: \r\n```"
  created_at: 2023-06-21 16:54:50+00:00
  edited: false
  hidden: false
  id: 6493396abe8f1f70b593f66f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T21:59:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9451767802238464
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There''s --in-prefix and --in-suffix that might be what you want?  <a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/discussions/1980#discussioncomment-6265342">https://github.com/ggerganov/llama.cpp/discussions/1980#discussioncomment-6265342</a></p>

          <p>I''ve not tried it myself but it looks like that''s what it''s for</p>

          '
        raw: 'There''s --in-prefix and --in-suffix that might be what you want?  https://github.com/ggerganov/llama.cpp/discussions/1980#discussioncomment-6265342


          I''ve not tried it myself but it looks like that''s what it''s for'
        updatedAt: '2023-06-23T21:59:06.725Z'
      numEdits: 0
      reactions: []
    id: 649615aab8d4efc75bf53641
    type: comment
  author: TheBloke
  content: 'There''s --in-prefix and --in-suffix that might be what you want?  https://github.com/ggerganov/llama.cpp/discussions/1980#discussioncomment-6265342


    I''ve not tried it myself but it looks like that''s what it''s for'
  created_at: 2023-06-23 20:59:06+00:00
  edited: false
  hidden: false
  id: 649615aab8d4efc75bf53641
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84c6e02bb51094e3df43ba5b2b857d50.svg
      fullname: Ray Vive
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rayyd
      type: user
    createdAt: '2023-06-25T07:16:43.000Z'
    data:
      edited: false
      editors:
      - rayyd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9573874473571777
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84c6e02bb51094e3df43ba5b2b857d50.svg
          fullname: Ray Vive
          isHf: false
          isPro: false
          name: rayyd
          type: user
        html: '<p>I started using it and it definitely gives better results with models
          like guanaco and  airoboros and more coherent chat.<br>I''m note sure why
          there isn''t more information about it though, also some templates have
          three prompt turns like user,input,output, not sure how that works with
          llama.cpp.</p>

          '
        raw: 'I started using it and it definitely gives better results with models
          like guanaco and  airoboros and more coherent chat.

          I''m note sure why there isn''t more information about it though, also some
          templates have three prompt turns like user,input,output, not sure how that
          works with llama.cpp.'
        updatedAt: '2023-06-25T07:16:43.617Z'
      numEdits: 0
      reactions: []
    id: 6497e9db3e191333af12e948
    type: comment
  author: rayyd
  content: 'I started using it and it definitely gives better results with models
    like guanaco and  airoboros and more coherent chat.

    I''m note sure why there isn''t more information about it though, also some templates
    have three prompt turns like user,input,output, not sure how that works with llama.cpp.'
  created_at: 2023-06-25 06:16:43+00:00
  edited: false
  hidden: false
  id: 6497e9db3e191333af12e948
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/airoboros-13B-gpt4-1.3-GGML
repo_type: model
status: open
target_branch: null
title: Prompt template with llama.cpp in interactive mode
