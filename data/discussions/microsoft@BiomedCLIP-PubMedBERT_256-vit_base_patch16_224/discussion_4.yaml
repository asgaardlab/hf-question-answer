!!python/object:huggingface_hub.community.DiscussionWithDetails
author: seemorebricks
conflicting_files: null
created_at: 2023-04-14 23:09:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef301f291e12d9327446d6b5c3db2403.svg
      fullname: Seemore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seemorebricks
      type: user
    createdAt: '2023-04-15T00:09:42.000Z'
    data:
      edited: true
      editors:
      - seemorebricks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef301f291e12d9327446d6b5c3db2403.svg
          fullname: Seemore
          isHf: false
          isPro: false
          name: seemorebricks
          type: user
        html: "<p>Hello,</p>\n<p>I see this from your paper in regards to implementing\
          \ VQA and/or captioning:</p>\n<p>'''<br>We utilize the METER (Dou et al.,\
          \ 2022) framework to facilitate our experiments on visual ques\x02tion answering\
          \ (VQA). It formulates the VQA task as a classification task. The core module\
          \ of METER is a transformer-based co-attention multimodal fusion module\
          \ that produces cross-modal representations over the image and text encodings,\
          \ which are then fed to a classifier for predicting the final answer.<br>'''</p>\n\
          <p>Is there some source code for this task that is available? Apologies,\
          \ I'm new to the field so it's not quite intuitive to me.</p>\n"
        raw: "Hello,\n\nI see this from your paper in regards to implementing VQA\
          \ and/or captioning:\n\n'''\nWe utilize the METER (Dou et al., 2022) framework\
          \ to facilitate our experiments on visual ques\x02tion answering (VQA).\
          \ It formulates the VQA task as a classification task. The core module of\
          \ METER is a transformer-based co-attention multimodal fusion module that\
          \ produces cross-modal representations over the image and text encodings,\
          \ which are then fed to a classifier for predicting the final answer.\n\
          '''\n\nIs there some source code for this task that is available? Apologies,\
          \ I'm new to the field so it's not quite intuitive to me."
        updatedAt: '2023-04-15T01:33:29.941Z'
      numEdits: 1
      reactions: []
    id: 6439eb467433c50280237df2
    type: comment
  author: seemorebricks
  content: "Hello,\n\nI see this from your paper in regards to implementing VQA and/or\
    \ captioning:\n\n'''\nWe utilize the METER (Dou et al., 2022) framework to facilitate\
    \ our experiments on visual ques\x02tion answering (VQA). It formulates the VQA\
    \ task as a classification task. The core module of METER is a transformer-based\
    \ co-attention multimodal fusion module that produces cross-modal representations\
    \ over the image and text encodings, which are then fed to a classifier for predicting\
    \ the final answer.\n'''\n\nIs there some source code for this task that is available?\
    \ Apologies, I'm new to the field so it's not quite intuitive to me."
  created_at: 2023-04-14 23:09:42+00:00
  edited: true
  hidden: false
  id: 6439eb467433c50280237df2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57a6dfd5b71f7d1fad5257989fdf9840.svg
      fullname: Yanbo Xu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: YanboXu
      type: user
    createdAt: '2023-04-17T19:25:10.000Z'
    data:
      edited: false
      editors:
      - YanboXu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57a6dfd5b71f7d1fad5257989fdf9840.svg
          fullname: Yanbo Xu
          isHf: false
          isPro: false
          name: YanboXu
          type: user
        html: '<p>Please refer to the METER package at <a rel="nofollow" href="https://github.com/zdou0830/METER">https://github.com/zdou0830/METER</a>.</p>

          '
        raw: Please refer to the METER package at https://github.com/zdou0830/METER.
        updatedAt: '2023-04-17T19:25:10.897Z'
      numEdits: 0
      reactions: []
    id: 643d9d16aa3c5728ed650a4c
    type: comment
  author: YanboXu
  content: Please refer to the METER package at https://github.com/zdou0830/METER.
  created_at: 2023-04-17 18:25:10+00:00
  edited: false
  hidden: false
  id: 643d9d16aa3c5728ed650a4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg
      fullname: Sheng Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: shengz
      type: user
    createdAt: '2023-04-20T22:56:09.000Z'
    data:
      status: closed
    id: 6441c309a839ee80331e563d
    type: status-change
  author: shengz
  created_at: 2023-04-20 21:56:09+00:00
  id: 6441c309a839ee80331e563d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224
repo_type: model
status: closed
target_branch: null
title: How to try VQA
