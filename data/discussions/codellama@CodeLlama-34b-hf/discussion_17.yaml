!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hk11
conflicting_files: null
created_at: 2023-10-07 07:41:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0754f9fd5faa1c2d3ef5eb8d69cc3ea.svg
      fullname: Himanshu Kapoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hk11
      type: user
    createdAt: '2023-10-07T08:41:38.000Z'
    data:
      edited: true
      editors:
      - hk11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4934314787387848
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0754f9fd5faa1c2d3ef5eb8d69cc3ea.svg
          fullname: Himanshu Kapoor
          isHf: false
          isPro: false
          name: hk11
          type: user
        html: '<p>Does CodeLlama supports LoRA ? What should be the target model ?</p>

          <p><code>config = LoraConfig(     r=16,     lora_alpha=32     target_modules=
          ?,     lora_dropout=0.05,     bias="none",     task_type="CAUSAL_LM" )</code></p>

          '
        raw: "Does CodeLlama supports LoRA ? What should be the target model ?\n\n\
          `config = LoraConfig(\n    r=16,\n    lora_alpha=32\n    target_modules=\
          \ ?,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\
          \n)`\n"
        updatedAt: '2023-10-07T08:41:52.575Z'
      numEdits: 1
      reactions: []
    id: 652119c2fe5881ad35b9840e
    type: comment
  author: hk11
  content: "Does CodeLlama supports LoRA ? What should be the target model ?\n\n`config\
    \ = LoraConfig(\n    r=16,\n    lora_alpha=32\n    target_modules= ?,\n    lora_dropout=0.05,\n\
    \    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)`\n"
  created_at: 2023-10-07 07:41:38+00:00
  edited: true
  hidden: false
  id: 652119c2fe5881ad35b9840e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0754f9fd5faa1c2d3ef5eb8d69cc3ea.svg
      fullname: Himanshu Kapoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hk11
      type: user
    createdAt: '2023-10-07T08:59:36.000Z'
    data:
      edited: true
      editors:
      - hk11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6338312029838562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0754f9fd5faa1c2d3ef5eb8d69cc3ea.svg
          fullname: Himanshu Kapoor
          isHf: false
          isPro: false
          name: hk11
          type: user
        html: '<p>i went ahead with , removing <code>target_modules</code> from the
          LoraConfig</p>

          <p><code>config = LoraConfig( r=16, lora_alpha=32 , lora_dropout=0.05, bias="none",
          task_type="CAUSAL_LM" )</code></p>

          <p>when i printed the trainable parameters</p>

          <p><code>trainable params: 19660800 || all params: 33763631104 || trainable%:
          0.05823070373989121</code></p>

          <p>is this is the right configuration to go for fine-tunning ?</p>

          '
        raw: 'i went ahead with , removing `target_modules` from the LoraConfig


          `config = LoraConfig( r=16, lora_alpha=32 , lora_dropout=0.05, bias="none",
          task_type="CAUSAL_LM" )`


          when i printed the trainable parameters


          `trainable params: 19660800 || all params: 33763631104 || trainable%: 0.05823070373989121`


          is this is the right configuration to go for fine-tunning ?


          '
        updatedAt: '2023-10-07T08:59:49.158Z'
      numEdits: 1
      reactions: []
    id: 65211df85b57ffc322967786
    type: comment
  author: hk11
  content: 'i went ahead with , removing `target_modules` from the LoraConfig


    `config = LoraConfig( r=16, lora_alpha=32 , lora_dropout=0.05, bias="none", task_type="CAUSAL_LM"
    )`


    when i printed the trainable parameters


    `trainable params: 19660800 || all params: 33763631104 || trainable%: 0.05823070373989121`


    is this is the right configuration to go for fine-tunning ?


    '
  created_at: 2023-10-07 07:59:36+00:00
  edited: true
  hidden: false
  id: 65211df85b57ffc322967786
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: codellama/CodeLlama-34b-hf
repo_type: model
status: open
target_branch: null
title: CodeLlama and LoRA
