!!python/object:huggingface_hub.community.DiscussionWithDetails
author: huluwahaha
conflicting_files: null
created_at: 2023-10-25 22:24:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/943ab8a01b16d06dff0033c7b579616e.svg
      fullname: huluwa haha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huluwahaha
      type: user
    createdAt: '2023-10-25T23:24:46.000Z'
    data:
      edited: false
      editors:
      - huluwahaha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40317806601524353
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/943ab8a01b16d06dff0033c7b579616e.svg
          fullname: huluwa haha
          isHf: false
          isPro: false
          name: huluwahaha
          type: user
        html: '<p>here is a max_length<br>transformers.pipeline(<br>        "text-generation",<br>        model=_hf_model_dir,<br>        torch_dtype=torch.float16,<br>        device_map="auto",<br>    )(_content,<br>        do_sample=True,<br>        top_k=5,<br>        num_return_sequences=1,<br>        eos_token_id=_hf_tokenizer.eos_token_id,<br>        max_length=1000,<br>        temperature=0.2,<br>        top_p=0.95,<br>    )</p>

          '
        raw: "here is a max_length\r\ntransformers.pipeline(\r\n        \"text-generation\"\
          ,\r\n        model=_hf_model_dir,\r\n        torch_dtype=torch.float16,\r\
          \n        device_map=\"auto\",\r\n    )(_content,\r\n        do_sample=True,\r\
          \n        top_k=5,\r\n        num_return_sequences=1,\r\n        eos_token_id=_hf_tokenizer.eos_token_id,\r\
          \n        max_length=1000,\r\n        temperature=0.2,\r\n        top_p=0.95,\r\
          \n    )"
        updatedAt: '2023-10-25T23:24:46.863Z'
      numEdits: 0
      reactions: []
    id: 6539a3bef940c8a035a6a660
    type: comment
  author: huluwahaha
  content: "here is a max_length\r\ntransformers.pipeline(\r\n        \"text-generation\"\
    ,\r\n        model=_hf_model_dir,\r\n        torch_dtype=torch.float16,\r\n  \
    \      device_map=\"auto\",\r\n    )(_content,\r\n        do_sample=True,\r\n\
    \        top_k=5,\r\n        num_return_sequences=1,\r\n        eos_token_id=_hf_tokenizer.eos_token_id,\r\
    \n        max_length=1000,\r\n        temperature=0.2,\r\n        top_p=0.95,\r\
    \n    )"
  created_at: 2023-10-25 22:24:46+00:00
  edited: false
  hidden: false
  id: 6539a3bef940c8a035a6a660
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: codellama/CodeLlama-34b-hf
repo_type: model
status: open
target_branch: null
title: "what's the meaning of max_length\uFF1F"
