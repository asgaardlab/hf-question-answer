!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Zihao
conflicting_files: null
created_at: 2023-05-23 03:52:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/18b60e3d1f06de80d3501d1f2a80a596.svg
      fullname: Meng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zihao
      type: user
    createdAt: '2023-05-23T04:52:21.000Z'
    data:
      edited: false
      editors:
      - Zihao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/18b60e3d1f06de80d3501d1f2a80a596.svg
          fullname: Meng
          isHf: false
          isPro: false
          name: Zihao
          type: user
        html: '<p>Compared with the "e5-base" model, what is the main update in this
          "e5-base-v2" version? </p>

          '
        raw: 'Compared with the "e5-base" model, what is the main update in this "e5-base-v2"
          version? '
        updatedAt: '2023-05-23T04:52:21.641Z'
      numEdits: 0
      reactions: []
    id: 646c468531968a60a023d19f
    type: comment
  author: Zihao
  content: 'Compared with the "e5-base" model, what is the main update in this "e5-base-v2"
    version? '
  created_at: 2023-05-23 03:52:21+00:00
  edited: false
  hidden: false
  id: 646c468531968a60a023d19f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2023-05-23T07:52:24.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: '<p>Nothing new, v2 models are simply pre-trained on larger quantity
          and more diverse text pair datasets.</p>

          '
        raw: Nothing new, v2 models are simply pre-trained on larger quantity and
          more diverse text pair datasets.
        updatedAt: '2023-05-23T07:52:24.078Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Zihao
    id: 646c70b82c29b8753ace1df1
    type: comment
  author: intfloat
  content: Nothing new, v2 models are simply pre-trained on larger quantity and more
    diverse text pair datasets.
  created_at: 2023-05-23 06:52:24+00:00
  edited: false
  hidden: false
  id: 646c70b82c29b8753ace1df1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604912003181-noauth.jpeg?w=200&h=200&f=face
      fullname: Jo Kristian Bergum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bergum
      type: user
    createdAt: '2023-05-26T16:39:59.000Z'
    data:
      edited: true
      editors:
      - bergum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604912003181-noauth.jpeg?w=200&h=200&f=face
          fullname: Jo Kristian Bergum
          isHf: false
          isPro: false
          name: bergum
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;intfloat&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/intfloat\"\
          >@<span class=\"underline\">intfloat</span></a></span>\n\n\t</span></span>,\
          \ does this repo have the unsupervised weights (Table 1 in the paper), or\
          \ the weights from after without fine-tuning on MS Marco/BEIR (Table 2)?\
          \ <a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2212.03533.pdf\">paper</a></p>\n"
        raw: Hi @intfloat, does this repo have the unsupervised weights (Table 1 in
          the paper), or the weights from after without fine-tuning on MS Marco/BEIR
          (Table 2)? [paper](https://arxiv.org/pdf/2212.03533.pdf)
        updatedAt: '2023-05-26T16:41:14.733Z'
      numEdits: 2
      reactions: []
    id: 6470e0df1f0e7ee7fb1e6766
    type: comment
  author: bergum
  content: Hi @intfloat, does this repo have the unsupervised weights (Table 1 in
    the paper), or the weights from after without fine-tuning on MS Marco/BEIR (Table
    2)? [paper](https://arxiv.org/pdf/2212.03533.pdf)
  created_at: 2023-05-26 15:39:59+00:00
  edited: true
  hidden: false
  id: 6470e0df1f0e7ee7fb1e6766
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2023-05-27T01:51:25.000Z'
    data:
      edited: false
      editors:
      - intfloat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bergum&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bergum\">@<span class=\"\
          underline\">bergum</span></a></span>\n\n\t</span></span> We do not have\
          \ plans to release its unsupervised weights. Embedding models without supervised\
          \ fine-tuning do not perform very well and are not suitable for out-of-box\
          \ use cases. If you'd like to fine-tune from the unsupervised ones, you\
          \ can build upon <a href=\"https://huggingface.co/intfloat/e5-base-unsupervised\"\
          >https://huggingface.co/intfloat/e5-base-unsupervised</a> (also has small\
          \ and large versions)</p>\n"
        raw: '@bergum We do not have plans to release its unsupervised weights. Embedding
          models without supervised fine-tuning do not perform very well and are not
          suitable for out-of-box use cases. If you''d like to fine-tune from the
          unsupervised ones, you can build upon https://huggingface.co/intfloat/e5-base-unsupervised
          (also has small and large versions)'
        updatedAt: '2023-05-27T01:51:25.701Z'
      numEdits: 0
      reactions: []
    id: 6471621da4fe3fa9f13389fa
    type: comment
  author: intfloat
  content: '@bergum We do not have plans to release its unsupervised weights. Embedding
    models without supervised fine-tuning do not perform very well and are not suitable
    for out-of-box use cases. If you''d like to fine-tune from the unsupervised ones,
    you can build upon https://huggingface.co/intfloat/e5-base-unsupervised (also
    has small and large versions)'
  created_at: 2023-05-27 00:51:25+00:00
  edited: false
  hidden: false
  id: 6471621da4fe3fa9f13389fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604912003181-noauth.jpeg?w=200&h=200&f=face
      fullname: Jo Kristian Bergum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bergum
      type: user
    createdAt: '2023-05-30T11:54:45.000Z'
    data:
      edited: true
      editors:
      - bergum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604912003181-noauth.jpeg?w=200&h=200&f=face
          fullname: Jo Kristian Bergum
          isHf: false
          isPro: false
          name: bergum
          type: user
        html: "<p>Thanks for confirming, <span data-props=\"{&quot;user&quot;:&quot;intfloat&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/intfloat\"\
          >@<span class=\"underline\">intfloat</span></a></span>\n\n\t</span></span>.\
          \ </p>\n<p>I'm asking because I can't reproduce the BEIR results reported\
          \ in the paper or close to it. This could be explained if, by mistake, the\
          \ wrong weights were uploaded.</p>\n<p>With e5-base-v2 on TREC-COVID, I\
          \ get 0.69633 ndcg_at_10, which is off compared to the .79 reported in the\
          \ paper (which is a very good result for a dense model on TREC-COVID). </p>\n\
          <p>Edit:<br><em>It should be noted that this is on CPU; I haven't tested\
          \ this on GPU yet, and only tested TREC-COVID.</em></p>\n<pre><code>python3\
          \ mteb_beir_eval.py --model-name-or-path intfloat/e5-base-v2\n...\n[2023-05-30\
          \ 01:16:30,748 INFO] Evaluation for TRECCOVID on test took 89452.90 seconds\n\
          [2023-05-30 01:16:30,748 INFO] Scores: {'ndcg_at_1': 0.75, 'ndcg_at_3':\
          \ 0.74397, 'ndcg_at_5': 0.73222, 'ndcg_at_10': 0.69633, 'ndcg_at_100': 0.52017,\
          \ 'ndcg_at_1000': 0.48872, 'map_at_1': 0.00215, 'map_at_3': 0.00602, 'map_at_5':\
          \ 0.00968, 'map_at_10': 0.01753, 'map_at_100': 0.09263, 'map_at_1000': 0.23437,\
          \ 'recall_at_1': 0.00215, 'recall_at_3': 0.0065, 'recall_at_5': 0.01057,\
          \ 'recall_at_10': 0.01961, 'recall_at_100': 0.12825, 'recall_at_1000': 0.46435,\
          \ 'precision_at_1': 0.84, 'precision_at_3': 0.8, 'precision_at_5': 0.784,\
          \ 'precision_at_10': 0.74, 'precision_at_100': 0.5326, 'precision_at_1000':\
          \ 0.21844, 'mrr_at_1': 0.84, 'mrr_at_3': 0.91333, 'mrr_at_5': 0.91333, 'mrr_at_10':\
          \ 0.91333, 'mrr_at_100': 0.91333, 'mrr_at_1000': 0.91333, 'evaluation_time':\
          \ 89452.9}\n</code></pre>\n"
        raw: "Thanks for confirming, @intfloat. \n\nI'm asking because I can't reproduce\
          \ the BEIR results reported in the paper or close to it. This could be explained\
          \ if, by mistake, the wrong weights were uploaded.\n\nWith e5-base-v2 on\
          \ TREC-COVID, I get 0.69633 ndcg_at_10, which is off compared to the .79\
          \ reported in the paper (which is a very good result for a dense model on\
          \ TREC-COVID). \n\n\nEdit:\n_It should be noted that this is on CPU; I haven't\
          \ tested this on GPU yet, and only tested TREC-COVID._\n\n```\npython3 mteb_beir_eval.py\
          \ --model-name-or-path intfloat/e5-base-v2\n...\n[2023-05-30 01:16:30,748\
          \ INFO] Evaluation for TRECCOVID on test took 89452.90 seconds\n[2023-05-30\
          \ 01:16:30,748 INFO] Scores: {'ndcg_at_1': 0.75, 'ndcg_at_3': 0.74397, 'ndcg_at_5':\
          \ 0.73222, 'ndcg_at_10': 0.69633, 'ndcg_at_100': 0.52017, 'ndcg_at_1000':\
          \ 0.48872, 'map_at_1': 0.00215, 'map_at_3': 0.00602, 'map_at_5': 0.00968,\
          \ 'map_at_10': 0.01753, 'map_at_100': 0.09263, 'map_at_1000': 0.23437, 'recall_at_1':\
          \ 0.00215, 'recall_at_3': 0.0065, 'recall_at_5': 0.01057, 'recall_at_10':\
          \ 0.01961, 'recall_at_100': 0.12825, 'recall_at_1000': 0.46435, 'precision_at_1':\
          \ 0.84, 'precision_at_3': 0.8, 'precision_at_5': 0.784, 'precision_at_10':\
          \ 0.74, 'precision_at_100': 0.5326, 'precision_at_1000': 0.21844, 'mrr_at_1':\
          \ 0.84, 'mrr_at_3': 0.91333, 'mrr_at_5': 0.91333, 'mrr_at_10': 0.91333,\
          \ 'mrr_at_100': 0.91333, 'mrr_at_1000': 0.91333, 'evaluation_time': 89452.9}\n\
          ```"
        updatedAt: '2023-05-30T11:58:02.170Z'
      numEdits: 1
      reactions: []
    id: 6475e405e9b57ce0caa75ae8
    type: comment
  author: bergum
  content: "Thanks for confirming, @intfloat. \n\nI'm asking because I can't reproduce\
    \ the BEIR results reported in the paper or close to it. This could be explained\
    \ if, by mistake, the wrong weights were uploaded.\n\nWith e5-base-v2 on TREC-COVID,\
    \ I get 0.69633 ndcg_at_10, which is off compared to the .79 reported in the paper\
    \ (which is a very good result for a dense model on TREC-COVID). \n\n\nEdit:\n\
    _It should be noted that this is on CPU; I haven't tested this on GPU yet, and\
    \ only tested TREC-COVID._\n\n```\npython3 mteb_beir_eval.py --model-name-or-path\
    \ intfloat/e5-base-v2\n...\n[2023-05-30 01:16:30,748 INFO] Evaluation for TRECCOVID\
    \ on test took 89452.90 seconds\n[2023-05-30 01:16:30,748 INFO] Scores: {'ndcg_at_1':\
    \ 0.75, 'ndcg_at_3': 0.74397, 'ndcg_at_5': 0.73222, 'ndcg_at_10': 0.69633, 'ndcg_at_100':\
    \ 0.52017, 'ndcg_at_1000': 0.48872, 'map_at_1': 0.00215, 'map_at_3': 0.00602,\
    \ 'map_at_5': 0.00968, 'map_at_10': 0.01753, 'map_at_100': 0.09263, 'map_at_1000':\
    \ 0.23437, 'recall_at_1': 0.00215, 'recall_at_3': 0.0065, 'recall_at_5': 0.01057,\
    \ 'recall_at_10': 0.01961, 'recall_at_100': 0.12825, 'recall_at_1000': 0.46435,\
    \ 'precision_at_1': 0.84, 'precision_at_3': 0.8, 'precision_at_5': 0.784, 'precision_at_10':\
    \ 0.74, 'precision_at_100': 0.5326, 'precision_at_1000': 0.21844, 'mrr_at_1':\
    \ 0.84, 'mrr_at_3': 0.91333, 'mrr_at_5': 0.91333, 'mrr_at_10': 0.91333, 'mrr_at_100':\
    \ 0.91333, 'mrr_at_1000': 0.91333, 'evaluation_time': 89452.9}\n```"
  created_at: 2023-05-30 10:54:45+00:00
  edited: true
  hidden: false
  id: 6475e405e9b57ce0caa75ae8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
      fullname: Liang Wang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: intfloat
      type: user
    createdAt: '2023-05-30T13:24:37.000Z'
    data:
      edited: true
      editors:
      - intfloat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1ee74c2dbe349a6ec9843a1599d281.svg
          fullname: Liang Wang
          isHf: false
          isPro: false
          name: intfloat
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bergum&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bergum\">@<span class=\"\
          underline\">bergum</span></a></span>\n\n\t</span></span> The results in\
          \ the paper correspond to <a href=\"https://huggingface.co/intfloat/e5-base\"\
          >https://huggingface.co/intfloat/e5-base</a> instead of v2 models.</p>\n\
          <p>Your results are consistent with ours, which you can check at \"Evaluation\
          \ results\" part of <a href=\"https://huggingface.co/intfloat/e5-base-v2\"\
          >https://huggingface.co/intfloat/e5-base-v2</a>  Note that software version\
          \ and hardware could cause very minor differences.</p>\n<p>By the way, the\
          \ TREC COVID dataset is very small and has large performance fluctuations\
          \ when fine-tuning with different random seeds. We mainly focus on the average\
          \ results across all BEIR datasets.</p>\n"
        raw: '@bergum The results in the paper correspond to https://huggingface.co/intfloat/e5-base
          instead of v2 models.


          Your results are consistent with ours, which you can check at "Evaluation
          results" part of https://huggingface.co/intfloat/e5-base-v2  Note that software
          version and hardware could cause very minor differences.


          By the way, the TREC COVID dataset is very small and has large performance
          fluctuations when fine-tuning with different random seeds. We mainly focus
          on the average results across all BEIR datasets.'
        updatedAt: '2023-05-30T13:27:13.785Z'
      numEdits: 1
      reactions: []
    id: 6475f91509e77322633a9352
    type: comment
  author: intfloat
  content: '@bergum The results in the paper correspond to https://huggingface.co/intfloat/e5-base
    instead of v2 models.


    Your results are consistent with ours, which you can check at "Evaluation results"
    part of https://huggingface.co/intfloat/e5-base-v2  Note that software version
    and hardware could cause very minor differences.


    By the way, the TREC COVID dataset is very small and has large performance fluctuations
    when fine-tuning with different random seeds. We mainly focus on the average results
    across all BEIR datasets.'
  created_at: 2023-05-30 12:24:37+00:00
  edited: true
  hidden: false
  id: 6475f91509e77322633a9352
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604912003181-noauth.jpeg?w=200&h=200&f=face
      fullname: Jo Kristian Bergum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bergum
      type: user
    createdAt: '2023-05-30T14:21:19.000Z'
    data:
      edited: false
      editors:
      - bergum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604912003181-noauth.jpeg?w=200&h=200&f=face
          fullname: Jo Kristian Bergum
          isHf: false
          isPro: false
          name: bergum
          type: user
        html: "<p>Perfect, <span data-props=\"{&quot;user&quot;:&quot;intfloat&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/intfloat\"\
          >@<span class=\"underline\">intfloat</span></a></span>\n\n\t</span></span>.\
          \ Thank you for your time explaining this. I wrongly assumed v1 and v2 would\
          \ be similar. I see now that the self-reported ndcg_at_10 is 69.596, which\
          \ is close and easily explained. Thank you for publishing this work, and\
          \ for making it easy to reproduce!</p>\n"
        raw: Perfect, @intfloat. Thank you for your time explaining this. I wrongly
          assumed v1 and v2 would be similar. I see now that the self-reported ndcg_at_10
          is 69.596, which is close and easily explained. Thank you for publishing
          this work, and for making it easy to reproduce!
        updatedAt: '2023-05-30T14:21:19.719Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - intfloat
        - GordonM
    id: 6476065f808893fdc9d1e5a6
    type: comment
  author: bergum
  content: Perfect, @intfloat. Thank you for your time explaining this. I wrongly
    assumed v1 and v2 would be similar. I see now that the self-reported ndcg_at_10
    is 69.596, which is close and easily explained. Thank you for publishing this
    work, and for making it easy to reproduce!
  created_at: 2023-05-30 13:21:19+00:00
  edited: false
  hidden: false
  id: 6476065f808893fdc9d1e5a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: intfloat/e5-base-v2
repo_type: model
status: open
target_branch: null
title: 'Compared with the "e5-base" model, what is the main update in this "e5-base-v2"
  version? '
