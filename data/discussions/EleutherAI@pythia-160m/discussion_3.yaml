!!python/object:huggingface_hub.community.DiscussionWithDetails
author: prshnthrv
conflicting_files: null
created_at: 2023-08-02 10:56:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7d71d098ff5d36dd0d61719f2cceacab.svg
      fullname: Prashanth Viswanath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prshnthrv
      type: user
    createdAt: '2023-08-02T11:56:20.000Z'
    data:
      edited: false
      editors:
      - prshnthrv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9398524165153503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7d71d098ff5d36dd0d61719f2cceacab.svg
          fullname: Prashanth Viswanath
          isHf: false
          isPro: false
          name: prshnthrv
          type: user
        html: "<p>Hi,<br>I evaluated the Pythia-160m and Pythia-160m-v0 on the Pile\
          \ test group 0 and I see PPL scores of 12.92 (v1) vs 11.80 (v0).<br>Also,\
          \ looking at some stats of the output logits on a portion of the pile training\
          \ data, logits mean is 0.0909 (v1) vs 0.0157 (v0). </p>\n<p>I notice from\
          \ the changelogs on EleutherAI/Pythia github:</p>\n<p>Changelog<br>[April\
          \ 3, 2023] We have released a new version of all Pythia models, with the\
          \ following changes to our training procedure:</p>\n<ol>\n<li>All model\
          \ sizes are now trained with uniform batch size of 2M tokens. Previously,\
          \ the models of size 160M, 410M, and 1.4B parameters were trained with batch\
          \ sizes of 4M tokens.</li>\n<li>We added checkpoints at initialization (step\
          \ 0) and steps {1,2,4,8,16,32,64, 128,256,512} in addition to every 1000\
          \ training steps.</li>\n<li>Flash Attention was used in the new retrained\
          \ suite. Empirically, this seems to have effected the dynamic range of model\
          \ outputs in some cases, which we are investigating further.</li>\n<li>We\
          \ remedied a minor inconsistency that existed in the original suite: all\
          \ models of size 2.8B parameters or smaller had a learning rate (LR) schedule\
          \ which decayed to a minimum LR of 10% the starting LR rate, but the 6.9B\
          \ and 12B models all used an LR schedule which decayed to a minimum LR of\
          \ 0. In the redone training runs, we rectified this inconsistency: all models\
          \ now were trained with LR decaying to a minimum of 0.1\xD7 their maximum\
          \ LR.<br>the new EleutherAI/pythia-1b is trained with bf16, because in fp16\
          \ the model corrupted due to loss spikes late in training.</li>\n</ol>\n\
          <p>Could the (3) above be the cause for this? Could you please elaborate\
          \ more on the issue listed in (3)?</p>\n"
        raw: "Hi,\r\nI evaluated the Pythia-160m and Pythia-160m-v0 on the Pile test\
          \ group 0 and I see PPL scores of 12.92 (v1) vs 11.80 (v0).\r\nAlso, looking\
          \ at some stats of the output logits on a portion of the pile training data,\
          \ logits mean is 0.0909 (v1) vs 0.0157 (v0). \r\n\r\nI notice from the changelogs\
          \ on EleutherAI/Pythia github:\r\n\r\nChangelog\r\n[April 3, 2023] We have\
          \ released a new version of all Pythia models, with the following changes\
          \ to our training procedure:\r\n\r\n1. All model sizes are now trained with\
          \ uniform batch size of 2M tokens. Previously, the models of size 160M,\
          \ 410M, and 1.4B parameters were trained with batch sizes of 4M tokens.\r\
          \n2. We added checkpoints at initialization (step 0) and steps {1,2,4,8,16,32,64,\
          \ 128,256,512} in addition to every 1000 training steps.\r\n3. Flash Attention\
          \ was used in the new retrained suite. Empirically, this seems to have effected\
          \ the dynamic range of model outputs in some cases, which we are investigating\
          \ further.\r\n4. We remedied a minor inconsistency that existed in the original\
          \ suite: all models of size 2.8B parameters or smaller had a learning rate\
          \ (LR) schedule which decayed to a minimum LR of 10% the starting LR rate,\
          \ but the 6.9B and 12B models all used an LR schedule which decayed to a\
          \ minimum LR of 0. In the redone training runs, we rectified this inconsistency:\
          \ all models now were trained with LR decaying to a minimum of 0.1\xD7 their\
          \ maximum LR.\r\nthe new EleutherAI/pythia-1b is trained with bf16, because\
          \ in fp16 the model corrupted due to loss spikes late in training.\r\n\r\
          \nCould the (3) above be the cause for this? Could you please elaborate\
          \ more on the issue listed in (3)?"
        updatedAt: '2023-08-02T11:56:20.369Z'
      numEdits: 0
      reactions: []
    id: 64ca4464942890af932942ff
    type: comment
  author: prshnthrv
  content: "Hi,\r\nI evaluated the Pythia-160m and Pythia-160m-v0 on the Pile test\
    \ group 0 and I see PPL scores of 12.92 (v1) vs 11.80 (v0).\r\nAlso, looking at\
    \ some stats of the output logits on a portion of the pile training data, logits\
    \ mean is 0.0909 (v1) vs 0.0157 (v0). \r\n\r\nI notice from the changelogs on\
    \ EleutherAI/Pythia github:\r\n\r\nChangelog\r\n[April 3, 2023] We have released\
    \ a new version of all Pythia models, with the following changes to our training\
    \ procedure:\r\n\r\n1. All model sizes are now trained with uniform batch size\
    \ of 2M tokens. Previously, the models of size 160M, 410M, and 1.4B parameters\
    \ were trained with batch sizes of 4M tokens.\r\n2. We added checkpoints at initialization\
    \ (step 0) and steps {1,2,4,8,16,32,64, 128,256,512} in addition to every 1000\
    \ training steps.\r\n3. Flash Attention was used in the new retrained suite. Empirically,\
    \ this seems to have effected the dynamic range of model outputs in some cases,\
    \ which we are investigating further.\r\n4. We remedied a minor inconsistency\
    \ that existed in the original suite: all models of size 2.8B parameters or smaller\
    \ had a learning rate (LR) schedule which decayed to a minimum LR of 10% the starting\
    \ LR rate, but the 6.9B and 12B models all used an LR schedule which decayed to\
    \ a minimum LR of 0. In the redone training runs, we rectified this inconsistency:\
    \ all models now were trained with LR decaying to a minimum of 0.1\xD7 their maximum\
    \ LR.\r\nthe new EleutherAI/pythia-1b is trained with bf16, because in fp16 the\
    \ model corrupted due to loss spikes late in training.\r\n\r\nCould the (3) above\
    \ be the cause for this? Could you please elaborate more on the issue listed in\
    \ (3)?"
  created_at: 2023-08-02 10:56:20+00:00
  edited: false
  hidden: false
  id: 64ca4464942890af932942ff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: EleutherAI/pythia-160m
repo_type: model
status: open
target_branch: null
title: Pythia-160m vs Pythia-160m-v0
