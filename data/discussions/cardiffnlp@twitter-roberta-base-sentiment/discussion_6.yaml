!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mmustafaicer
conflicting_files: null
created_at: 2023-01-27 18:28:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661885600300-noauth.jpeg?w=200&h=200&f=face
      fullname: Mehmet Mustafa Icer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmustafaicer
      type: user
    createdAt: '2023-01-27T18:28:44.000Z'
    data:
      edited: true
      editors:
      - mmustafaicer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661885600300-noauth.jpeg?w=200&h=200&f=face
          fullname: Mehmet Mustafa Icer
          isHf: false
          isPro: false
          name: mmustafaicer
          type: user
        html: '<p>This is not per se a code fix question. It is more like a production
          environment compute instance capacity problem. I have created an endpoint
          using this model on Azure with <strong>STANDARD_DS4_V2</strong>  (8 cores,
          28 GB RAM, 56 GB disk) to score texts coming in batches. It is production
          environment of a call center. So you can imagine how many of rows of transcript
          is flowing (streaming data).</p>

          <p>My question is: What type of compute instances you guys use for this
          model in production environment. It is ~400 MB PyTorch model. <strong>For
          inference, do you guys use CPU or GPU instance?</strong> Would it matter
          in inference as well? I know it is a big difference in training. But is
          it the same with the inference.</p>

          <p>I observe at monitoring tab of Azure endpoints. I can see that endpoint
          is struggling with incoming data although auto-scaling is enabled. Any experience
          of running this model in production environment? Which instance types you
          guys are using compute optimized? memory optimized?</p>

          '
        raw: 'This is not per se a code fix question. It is more like a production
          environment compute instance capacity problem. I have created an endpoint
          using this model on Azure with **STANDARD_DS4_V2**  (8 cores, 28 GB RAM,
          56 GB disk) to score texts coming in batches. It is production environment
          of a call center. So you can imagine how many of rows of transcript is flowing
          (streaming data).


          My question is: What type of compute instances you guys use for this model
          in production environment. It is ~400 MB PyTorch model. **For inference,
          do you guys use CPU or GPU instance?** Would it matter in inference as well?
          I know it is a big difference in training. But is it the same with the inference.


          I observe at monitoring tab of Azure endpoints. I can see that endpoint
          is struggling with incoming data although auto-scaling is enabled. Any experience
          of running this model in production environment? Which instance types you
          guys are using compute optimized? memory optimized?'
        updatedAt: '2023-01-27T18:31:37.394Z'
      numEdits: 1
      reactions: []
    id: 63d417dcff1384ce6c610cc9
    type: comment
  author: mmustafaicer
  content: 'This is not per se a code fix question. It is more like a production environment
    compute instance capacity problem. I have created an endpoint using this model
    on Azure with **STANDARD_DS4_V2**  (8 cores, 28 GB RAM, 56 GB disk) to score texts
    coming in batches. It is production environment of a call center. So you can imagine
    how many of rows of transcript is flowing (streaming data).


    My question is: What type of compute instances you guys use for this model in
    production environment. It is ~400 MB PyTorch model. **For inference, do you guys
    use CPU or GPU instance?** Would it matter in inference as well? I know it is
    a big difference in training. But is it the same with the inference.


    I observe at monitoring tab of Azure endpoints. I can see that endpoint is struggling
    with incoming data although auto-scaling is enabled. Any experience of running
    this model in production environment? Which instance types you guys are using
    compute optimized? memory optimized?'
  created_at: 2023-01-27 18:28:44+00:00
  edited: true
  hidden: false
  id: 63d417dcff1384ce6c610cc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661885600300-noauth.jpeg?w=200&h=200&f=face
      fullname: Mehmet Mustafa Icer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmustafaicer
      type: user
    createdAt: '2023-10-19T17:51:00.000Z'
    data:
      status: closed
    id: 65316c84c8da3465f469b5ab
    type: status-change
  author: mmustafaicer
  created_at: 2023-10-19 16:51:00+00:00
  id: 65316c84c8da3465f469b5ab
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: cardiffnlp/twitter-roberta-base-sentiment
repo_type: model
status: closed
target_branch: null
title: CPU or GPU to run PyTorch model on Azure?
