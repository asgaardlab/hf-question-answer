!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abdullahalzubaer
conflicting_files: null
created_at: 2023-05-04 15:30:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/616c9a876858449cffdf7ee6/LuATMV6NiOpY74xKr_0l1.jpeg?w=200&h=200&f=face
      fullname: Abdullah Al Zubaer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdullahalzubaer
      type: user
    createdAt: '2023-05-04T16:30:03.000Z'
    data:
      edited: false
      editors:
      - abdullahalzubaer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/616c9a876858449cffdf7ee6/LuATMV6NiOpY74xKr_0l1.jpeg?w=200&h=200&f=face
          fullname: Abdullah Al Zubaer
          isHf: false
          isPro: false
          name: abdullahalzubaer
          type: user
        html: "<p>First, thanks a lot for this adapter, I was looking for it a while\
          \ and came across this. </p>\n<p>I was wondering if you can kindly provide\
          \ (or any pointer of how to do it) code for inference?  I am trying to do\
          \ inference and this is my code so far</p>\n<pre><code>BASE_MODEL = \"malteos/bloom-6b4-clp-german\"\
          \n\nmodel_bloom= BloomForCausalLM.from_pretrained(\n    BASE_MODEL,\n  \
          \  load_in_8bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"\
          auto\",\n)\n \ntokenizer_bloom= AutoTokenizer.from_pretrained(BASE_MODEL)\n\
          model_peft = PeftModel.from_pretrained(model_bloom, \"asprenger/bloom-6b4-clp-german-instruct-lora\"\
          )\n</code></pre>\n<p>The last line of the code throws this error </p>\n\
          <pre><code>RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\n\
          \        size mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight:\
          \ \ncopying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([16, \n4096]).\n        size mismatch\
          \ for base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight:\
          \ \ncopying a param with shape torch.Size([8192, 16, 1]) from checkpoint,\
          \ the shape in current model is \ntorch.Size([12288, 16]).\n</code></pre>\n\
          <p>The error continues up to h.29</p>\n<p>Thank you in advance :) </p>\n"
        raw: "First, thanks a lot for this adapter, I was looking for it a while and\
          \ came across this. \r\n\r\nI was wondering if you can kindly provide (or\
          \ any pointer of how to do it) code for inference?  I am trying to do inference\
          \ and this is my code so far\r\n\r\n```\r\nBASE_MODEL = \"malteos/bloom-6b4-clp-german\"\
          \r\n\r\nmodel_bloom= BloomForCausalLM.from_pretrained(\r\n    BASE_MODEL,\r\
          \n    load_in_8bit=True,\r\n    torch_dtype=torch.float16,\r\n    device_map=\"\
          auto\",\r\n)\r\n \r\ntokenizer_bloom= AutoTokenizer.from_pretrained(BASE_MODEL)\r\
          \nmodel_peft = PeftModel.from_pretrained(model_bloom, \"asprenger/bloom-6b4-clp-german-instruct-lora\"\
          )\r\n```\r\n\r\nThe last line of the code throws this error \r\n```\r\n\
          RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\r\
          \n        size mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight:\
          \ \r\ncopying a param with shape torch.Size([32, 4096]) from checkpoint,\
          \ the shape in current model is torch.Size([16, \r\n4096]).\r\n        size\
          \ mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight:\
          \ \r\ncopying a param with shape torch.Size([8192, 16, 1]) from checkpoint,\
          \ the shape in current model is \r\ntorch.Size([12288, 16]).\r\n```\r\n\
          The error continues up to h.29\r\n\r\nThank you in advance :) "
        updatedAt: '2023-05-04T16:30:03.248Z'
      numEdits: 0
      reactions: []
    id: 6453dd8b9dcddd329ae6052b
    type: comment
  author: abdullahalzubaer
  content: "First, thanks a lot for this adapter, I was looking for it a while and\
    \ came across this. \r\n\r\nI was wondering if you can kindly provide (or any\
    \ pointer of how to do it) code for inference?  I am trying to do inference and\
    \ this is my code so far\r\n\r\n```\r\nBASE_MODEL = \"malteos/bloom-6b4-clp-german\"\
    \r\n\r\nmodel_bloom= BloomForCausalLM.from_pretrained(\r\n    BASE_MODEL,\r\n\
    \    load_in_8bit=True,\r\n    torch_dtype=torch.float16,\r\n    device_map=\"\
    auto\",\r\n)\r\n \r\ntokenizer_bloom= AutoTokenizer.from_pretrained(BASE_MODEL)\r\
    \nmodel_peft = PeftModel.from_pretrained(model_bloom, \"asprenger/bloom-6b4-clp-german-instruct-lora\"\
    )\r\n```\r\n\r\nThe last line of the code throws this error \r\n```\r\nRuntimeError:\
    \ Error(s) in loading state_dict for PeftModelForCausalLM:\r\n        size mismatch\
    \ for base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight:\
    \ \r\ncopying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([16, \r\n4096]).\r\n        size mismatch for\
    \ base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight:\
    \ \r\ncopying a param with shape torch.Size([8192, 16, 1]) from checkpoint, the\
    \ shape in current model is \r\ntorch.Size([12288, 16]).\r\n```\r\nThe error continues\
    \ up to h.29\r\n\r\nThank you in advance :) "
  created_at: 2023-05-04 15:30:03+00:00
  edited: false
  hidden: false
  id: 6453dd8b9dcddd329ae6052b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/616c9a876858449cffdf7ee6/LuATMV6NiOpY74xKr_0l1.jpeg?w=200&h=200&f=face
      fullname: Abdullah Al Zubaer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdullahalzubaer
      type: user
    createdAt: '2023-05-05T00:20:01.000Z'
    data:
      edited: false
      editors:
      - abdullahalzubaer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/616c9a876858449cffdf7ee6/LuATMV6NiOpY74xKr_0l1.jpeg?w=200&h=200&f=face
          fullname: Abdullah Al Zubaer
          isHf: false
          isPro: false
          name: abdullahalzubaer
          type: user
        html: '<p>Update:</p>

          <p>The issue was solved by installing peft 0.2.0, there were some breaking
          changes with the newer version.</p>

          <p>Reference:</p>

          <ul>

          <li><a rel="nofollow" href="https://github.com/linhduongtuan/BLOOM-LORA/issues/5">https://github.com/linhduongtuan/BLOOM-LORA/issues/5</a></li>

          <li><a rel="nofollow" href="https://github.com/huggingface/peft/issues/276">https://github.com/huggingface/peft/issues/276</a></li>

          </ul>

          <p>I will close the issue since I think the problem has been resolved.</p>

          '
        raw: 'Update:


          The issue was solved by installing peft 0.2.0, there were some breaking
          changes with the newer version.


          Reference:

          - https://github.com/linhduongtuan/BLOOM-LORA/issues/5

          - https://github.com/huggingface/peft/issues/276


          I will close the issue since I think the problem has been resolved.'
        updatedAt: '2023-05-05T00:20:01.779Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64544bb1c4cbe32fbe2b4adc
    id: 64544bb1c4cbe32fbe2b4adb
    type: comment
  author: abdullahalzubaer
  content: 'Update:


    The issue was solved by installing peft 0.2.0, there were some breaking changes
    with the newer version.


    Reference:

    - https://github.com/linhduongtuan/BLOOM-LORA/issues/5

    - https://github.com/huggingface/peft/issues/276


    I will close the issue since I think the problem has been resolved.'
  created_at: 2023-05-04 23:20:01+00:00
  edited: false
  hidden: false
  id: 64544bb1c4cbe32fbe2b4adb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/616c9a876858449cffdf7ee6/LuATMV6NiOpY74xKr_0l1.jpeg?w=200&h=200&f=face
      fullname: Abdullah Al Zubaer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdullahalzubaer
      type: user
    createdAt: '2023-05-05T00:20:01.000Z'
    data:
      status: closed
    id: 64544bb1c4cbe32fbe2b4adc
    type: status-change
  author: abdullahalzubaer
  created_at: 2023-05-04 23:20:01+00:00
  id: 64544bb1c4cbe32fbe2b4adc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: asprenger/bloom-6b4-clp-german-instruct-lora
repo_type: model
status: closed
target_branch: null
title: Request for inference code.
