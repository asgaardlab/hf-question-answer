!!python/object:huggingface_hub.community.DiscussionWithDetails
author: c4lliope
conflicting_files: []
created_at: 2023-12-26 22:48:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7cf3bb1b6bb744e5aa8e625385e25bf1.svg
      fullname: Calliope Youngblood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: c4lliope
      type: user
    createdAt: '2023-12-26T22:48:03.000Z'
    data:
      edited: false
      editors:
      - c4lliope
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4125911593437195
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7cf3bb1b6bb744e5aa8e625385e25bf1.svg
          fullname: Calliope Youngblood
          isHf: false
          isPro: false
          name: c4lliope
          type: user
        html: '<p>Made using:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"facebook/mbart-large-50"</span>)

          <span class="hljs-keyword">assert</span> tokenizer.is_fast

          tokenizer.save_pretrained(<span class="hljs-string">"..."</span>)

          </code></pre>

          <p>Also added on main model.</p>

          '
        raw: 'Made using:


          ```python

          from transformers import AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50")

          assert tokenizer.is_fast

          tokenizer.save_pretrained("...")

          ```


          Also added on main model.'
        updatedAt: '2023-12-26T22:48:03.064Z'
      numEdits: 0
      reactions: []
    id: 658b58235208d1ca0e27b6ad
    type: comment
  author: c4lliope
  content: 'Made using:


    ```python

    from transformers import AutoTokenizer


    tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50")

    assert tokenizer.is_fast

    tokenizer.save_pretrained("...")

    ```


    Also added on main model.'
  created_at: 2023-12-26 22:48:03+00:00
  edited: false
  hidden: false
  id: 658b58235208d1ca0e27b6ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/7cf3bb1b6bb744e5aa8e625385e25bf1.svg
      fullname: Calliope Youngblood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: c4lliope
      type: user
    createdAt: '2023-12-26T22:48:03.000Z'
    data:
      oid: 151b8e7638b50d805e263b0451ea17c9e4754251
      parents:
      - f7d0dd48dd4256e8172c58fe0edd57be948a0c62
      subject: Upload tokenizer.json
    id: 658b58230000000000000000
    type: commit
  author: c4lliope
  created_at: 2023-12-26 22:48:03+00:00
  id: 658b58230000000000000000
  oid: 151b8e7638b50d805e263b0451ea17c9e4754251
  summary: Upload tokenizer.json
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 1
repo_id: DrishtiSharma/mbart-large-50-en-es-translation-lr-1e-05-weight-decay-0.0001
repo_type: model
status: open
target_branch: refs/heads/main
title: Upload tokenizer.json
