!!python/object:huggingface_hub.community.DiscussionWithDetails
author: willowill5
conflicting_files: null
created_at: 2023-11-15 22:13:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62acc1f1eda69b28bb64c39d/tmMMhkSyHc6HXDyhE7-jj.jpeg?w=200&h=200&f=face
      fullname: Will Reynolds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: willowill5
      type: user
    createdAt: '2023-11-15T22:13:05.000Z'
    data:
      edited: false
      editors:
      - willowill5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43301376700401306
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62acc1f1eda69b28bb64c39d/tmMMhkSyHc6HXDyhE7-jj.jpeg?w=200&h=200&f=face
          fullname: Will Reynolds
          isHf: false
          isPro: false
          name: willowill5
          type: user
        html: '<p>Weird bug with this model or vLLM. But the original hf4 model loads
          fine on 24 GB but OOMs with AWQ / vLLM.</p>

          <p>python -m vllm.entrypoints.api_server --model TheBloke/zephyr-7B-alpha-AWQ
          --quantization awq --dtype float16</p>

          '
        raw: "Weird bug with this model or vLLM. But the original hf4 model loads\
          \ fine on 24 GB but OOMs with AWQ / vLLM.\r\n\r\npython -m vllm.entrypoints.api_server\
          \ --model TheBloke/zephyr-7B-alpha-AWQ --quantization awq --dtype float16"
        updatedAt: '2023-11-15T22:13:05.711Z'
      numEdits: 0
      reactions: []
    id: 6555427148093c3f6e39615e
    type: comment
  author: willowill5
  content: "Weird bug with this model or vLLM. But the original hf4 model loads fine\
    \ on 24 GB but OOMs with AWQ / vLLM.\r\n\r\npython -m vllm.entrypoints.api_server\
    \ --model TheBloke/zephyr-7B-alpha-AWQ --quantization awq --dtype float16"
  created_at: 2023-11-15 22:13:05+00:00
  edited: false
  hidden: false
  id: 6555427148093c3f6e39615e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/zephyr-7B-alpha-AWQ
repo_type: model
status: open
target_branch: null
title: OOM on RTX 3090 with vLLM
