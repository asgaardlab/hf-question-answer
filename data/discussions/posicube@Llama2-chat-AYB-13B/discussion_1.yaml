!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-10-15 03:58:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-10-15T04:58:40.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968444287776947
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>The standard LLM tests are handy, but severely limited since they
          need to be easily and objectively graded (e.g. multiple choice tests).</p>

          <p>I put LLMs through a set of tricky and varied tests, which includes largely
          overlooked things like pop culture, and grade them on degree of correctness.
          For example, which character did Meg Ryan play in Joe versus the Volcano,
          which is actually 3 different characters, so an LLM gets 1 point for a right
          name, and up to 3 points if it returns all 3.</p>

          <p>Anyways, your AYT LLM performed notably better than any other Llama 2
          13b in my testing, or Mistral 7b for that matter. However, this AYB version
          started failing randomly and miserably, and it seems to be primarily caused
          by misfiring alignment.</p>

          <p>For example, questions that asked about characters from movies or shows
          were treated like real people simply because they contained a celebrity
          name as a hint for finding the correct answer. Or it wasn''t even the prompt.
          The LLM''s response would type something that trigger an alignment response
          mid-sentence.</p>

          <p>In short, AYT went from being the best 13b Llama 2 LLM by far on my testing,
          to being below the pack with AYB.</p>

          '
        raw: "The standard LLM tests are handy, but severely limited since they need\
          \ to be easily and objectively graded (e.g. multiple choice tests).\r\n\r\
          \nI put LLMs through a set of tricky and varied tests, which includes largely\
          \ overlooked things like pop culture, and grade them on degree of correctness.\
          \ For example, which character did Meg Ryan play in Joe versus the Volcano,\
          \ which is actually 3 different characters, so an LLM gets 1 point for a\
          \ right name, and up to 3 points if it returns all 3.\r\n\r\nAnyways, your\
          \ AYT LLM performed notably better than any other Llama 2 13b in my testing,\
          \ or Mistral 7b for that matter. However, this AYB version started failing\
          \ randomly and miserably, and it seems to be primarily caused by misfiring\
          \ alignment.\r\n\r\nFor example, questions that asked about characters from\
          \ movies or shows were treated like real people simply because they contained\
          \ a celebrity name as a hint for finding the correct answer. Or it wasn't\
          \ even the prompt. The LLM's response would type something that trigger\
          \ an alignment response mid-sentence.\r\n\r\nIn short, AYT went from being\
          \ the best 13b Llama 2 LLM by far on my testing, to being below the pack\
          \ with AYB."
        updatedAt: '2023-10-15T04:58:40.614Z'
      numEdits: 0
      reactions: []
    id: 652b718072280df4263bca31
    type: comment
  author: Phil337
  content: "The standard LLM tests are handy, but severely limited since they need\
    \ to be easily and objectively graded (e.g. multiple choice tests).\r\n\r\nI put\
    \ LLMs through a set of tricky and varied tests, which includes largely overlooked\
    \ things like pop culture, and grade them on degree of correctness. For example,\
    \ which character did Meg Ryan play in Joe versus the Volcano, which is actually\
    \ 3 different characters, so an LLM gets 1 point for a right name, and up to 3\
    \ points if it returns all 3.\r\n\r\nAnyways, your AYT LLM performed notably better\
    \ than any other Llama 2 13b in my testing, or Mistral 7b for that matter. However,\
    \ this AYB version started failing randomly and miserably, and it seems to be\
    \ primarily caused by misfiring alignment.\r\n\r\nFor example, questions that\
    \ asked about characters from movies or shows were treated like real people simply\
    \ because they contained a celebrity name as a hint for finding the correct answer.\
    \ Or it wasn't even the prompt. The LLM's response would type something that trigger\
    \ an alignment response mid-sentence.\r\n\r\nIn short, AYT went from being the\
    \ best 13b Llama 2 LLM by far on my testing, to being below the pack with AYB."
  created_at: 2023-10-15 03:58:40+00:00
  edited: false
  hidden: false
  id: 652b718072280df4263bca31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb857e1d2c9ff3305a296be09a8afcaa.svg
      fullname: jhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jjaaaww
      type: user
    createdAt: '2023-10-17T06:25:45.000Z'
    data:
      edited: false
      editors:
      - jjaaaww
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9765677452087402
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb857e1d2c9ff3305a296be09a8afcaa.svg
          fullname: jhang
          isHf: false
          isPro: false
          name: jjaaaww
          type: user
        html: '<p>Thanks for sharing your analysis. We are deveploping models considering
          many issues but I think this leaderboard isn''t the best way to measure
          LLM''s performance in practical aspects.<br>So, we''re trying to make several
          models to match the standard of leaderboard and common people''s.</p>

          '
        raw: "Thanks for sharing your analysis. We are deveploping models considering\
          \ many issues but I think this leaderboard isn't the best way to measure\
          \ LLM's performance in practical aspects. \nSo, we're trying to make several\
          \ models to match the standard of leaderboard and common people's."
        updatedAt: '2023-10-17T06:25:45.776Z'
      numEdits: 0
      reactions: []
    id: 652e28e91d12768fff037103
    type: comment
  author: jjaaaww
  content: "Thanks for sharing your analysis. We are deveploping models considering\
    \ many issues but I think this leaderboard isn't the best way to measure LLM's\
    \ performance in practical aspects. \nSo, we're trying to make several models\
    \ to match the standard of leaderboard and common people's."
  created_at: 2023-10-17 05:25:45+00:00
  edited: false
  hidden: false
  id: 652e28e91d12768fff037103
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: posicube/Llama2-chat-AYB-13B
repo_type: model
status: open
target_branch: null
title: Alignment Issues
