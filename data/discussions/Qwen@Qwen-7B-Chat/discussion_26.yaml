!!python/object:huggingface_hub.community.DiscussionWithDetails
author: komninos
conflicting_files: null
created_at: 2023-08-08 19:51:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c8feb5547f59248fab60f2/ZntsvMfGXQh3hryJcG5Nc.png?w=200&h=200&f=face
      fullname: Komninos Chatzipapas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komninos
      type: user
    createdAt: '2023-08-08T20:51:45.000Z'
    data:
      edited: false
      editors:
      - komninos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7252909541130066
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c8feb5547f59248fab60f2/ZntsvMfGXQh3hryJcG5Nc.png?w=200&h=200&f=face
          fullname: Komninos Chatzipapas
          isHf: false
          isPro: false
          name: komninos
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64c8feb5547f59248fab60f2/Ce1hVqjtfGTgPG4D7uwN6.png"><img
          alt="CleanShot 2023-08-08 at 23.48.47@2x.png" src="https://cdn-uploads.huggingface.co/production/uploads/64c8feb5547f59248fab60f2/Ce1hVqjtfGTgPG4D7uwN6.png"></a></p>

          <p>I''m prompting the model using ChatML on <code>text-generation-webui</code>.
          Honestly getting pretty good results but the model starts outputting gibberish
          after the assistant response. Instead of the gibberish, I expected to get
          an <code>&lt;|imend|&gt;</code> token.</p>

          <p>The gibberish keeps getting generated until it hits the <code>max_new_tokens</code>
          token limit. Not sure how to deal with this.</p>

          '
        raw: "![CleanShot 2023-08-08 at 23.48.47@2x.png](https://cdn-uploads.huggingface.co/production/uploads/64c8feb5547f59248fab60f2/Ce1hVqjtfGTgPG4D7uwN6.png)\r\
          \n\r\nI'm prompting the model using ChatML on `text-generation-webui`. Honestly\
          \ getting pretty good results but the model starts outputting gibberish\
          \ after the assistant response. Instead of the gibberish, I expected to\
          \ get an `<|imend|>` token.\r\n\r\nThe gibberish keeps getting generated\
          \ until it hits the `max_new_tokens` token limit. Not sure how to deal with\
          \ this."
        updatedAt: '2023-08-08T20:51:45.335Z'
      numEdits: 0
      reactions: []
    id: 64d2aae150310d7ad38a3b91
    type: comment
  author: komninos
  content: "![CleanShot 2023-08-08 at 23.48.47@2x.png](https://cdn-uploads.huggingface.co/production/uploads/64c8feb5547f59248fab60f2/Ce1hVqjtfGTgPG4D7uwN6.png)\r\
    \n\r\nI'm prompting the model using ChatML on `text-generation-webui`. Honestly\
    \ getting pretty good results but the model starts outputting gibberish after\
    \ the assistant response. Instead of the gibberish, I expected to get an `<|imend|>`\
    \ token.\r\n\r\nThe gibberish keeps getting generated until it hits the `max_new_tokens`\
    \ token limit. Not sure how to deal with this."
  created_at: 2023-08-08 19:51:45+00:00
  edited: false
  hidden: false
  id: 64d2aae150310d7ad38a3b91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
      fullname: jklj077
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jklj077
      type: user
    createdAt: '2023-08-09T03:23:33.000Z'
    data:
      edited: false
      editors:
      - jklj077
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7309508919715881
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
          fullname: jklj077
          isHf: false
          isPro: false
          name: jklj077
          type: user
        html: '<p>Hi,  I am not familiar with <code>text-generation-webui</code>,
          but let me try.</p>

          <ol>

          <li>The <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/shared.py#L51">default</a>
          for <code>skip_special_tokens</code> is True, which means you may never
          encounter special tokens in the UI even if they are generated. It appears
          to me that one could set this to False in the UI. If it is possible and
          you can see <code>&lt;|im_end|&gt;</code> being generated, it most likely
          is a configuration issue that the stopping criteria are not properly configured.
          </li>

          <li>The stopping criteria in this package seem to be configured only through
          <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/text_generation.py#L55">stop_strings</a>,
          which can be set using the UI with the <code>custom_stopping_strings</code>.
          (There are some default ones <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/chat.py#L156">here</a>).
          This should include <code>&lt;|im_end|&gt;</code>, <code>&lt;|im_start|&gt;</code>,
          and <code>&lt;|endoftext|&gt;</code> with <code>skip_special_tokens</code>
          set to False.</li>

          </ol>

          <p>Please let me know if this helps. Thanks.</p>

          '
        raw: "Hi,  I am not familiar with `text-generation-webui`, but let me try.\n\
          \n1. The [default](https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/shared.py#L51)\
          \ for `skip_special_tokens` is True, which means you may never encounter\
          \ special tokens in the UI even if they are generated. It appears to me\
          \ that one could set this to False in the UI. If it is possible and you\
          \ can see `<|im_end|>` being generated, it most likely is a configuration\
          \ issue that the stopping criteria are not properly configured. \n2. The\
          \ stopping criteria in this package seem to be configured only through [stop_strings](https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/text_generation.py#L55),\
          \ which can be set using the UI with the `custom_stopping_strings`. (There\
          \ are some default ones [here](https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/chat.py#L156)).\
          \ This should include `<|im_end|>`, `<|im_start|>`, and `<|endoftext|>`\
          \ with `skip_special_tokens` set to False.\n\nPlease let me know if this\
          \ helps. Thanks."
        updatedAt: '2023-08-09T03:23:33.388Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - komninos
    id: 64d306b50f17d186414fe550
    type: comment
  author: jklj077
  content: "Hi,  I am not familiar with `text-generation-webui`, but let me try.\n\
    \n1. The [default](https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/shared.py#L51)\
    \ for `skip_special_tokens` is True, which means you may never encounter special\
    \ tokens in the UI even if they are generated. It appears to me that one could\
    \ set this to False in the UI. If it is possible and you can see `<|im_end|>`\
    \ being generated, it most likely is a configuration issue that the stopping criteria\
    \ are not properly configured. \n2. The stopping criteria in this package seem\
    \ to be configured only through [stop_strings](https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/text_generation.py#L55),\
    \ which can be set using the UI with the `custom_stopping_strings`. (There are\
    \ some default ones [here](https://github.com/oobabooga/text-generation-webui/blob/f4caaf337afda85236e3963c22042e2581597424/modules/chat.py#L156)).\
    \ This should include `<|im_end|>`, `<|im_start|>`, and `<|endoftext|>` with `skip_special_tokens`\
    \ set to False.\n\nPlease let me know if this helps. Thanks."
  created_at: 2023-08-09 02:23:33+00:00
  edited: false
  hidden: false
  id: 64d306b50f17d186414fe550
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e936eb4f6fc749a0b8eff5e983cb1dc1.svg
      fullname: silent chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hackersilentchen
      type: user
    createdAt: '2023-08-10T01:20:44.000Z'
    data:
      edited: false
      editors:
      - hackersilentchen
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.7959991693496704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e936eb4f6fc749a0b8eff5e983cb1dc1.svg
          fullname: silent chen
          isHf: false
          isPro: false
          name: hackersilentchen
          type: user
        html: "<p>\u5DF2\u7ECF\u5728\u5FAE\u4FE1\u7FA4\u91CC\u8D34\u7ED9\u5B98\u65B9\
          \u4EBA\u5458\u770B\u5230\u8FC7\uFF0C\u5B98\u65B9\u53EF\u80FD\u4E0D\u4F1A\
          \u4E13\u95E8\u9002\u914Dtext-generation-webui\uFF0C\u8981\u4E0D\u53BBtext-generation-webui\u63D0\
          \u4E2Aissue\uFF1F</p>\n"
        raw: "\u5DF2\u7ECF\u5728\u5FAE\u4FE1\u7FA4\u91CC\u8D34\u7ED9\u5B98\u65B9\u4EBA\
          \u5458\u770B\u5230\u8FC7\uFF0C\u5B98\u65B9\u53EF\u80FD\u4E0D\u4F1A\u4E13\
          \u95E8\u9002\u914Dtext-generation-webui\uFF0C\u8981\u4E0D\u53BBtext-generation-webui\u63D0\
          \u4E2Aissue\uFF1F"
        updatedAt: '2023-08-10T01:20:44.171Z'
      numEdits: 0
      reactions: []
    id: 64d43b6cf8082bf19b8c606f
    type: comment
  author: hackersilentchen
  content: "\u5DF2\u7ECF\u5728\u5FAE\u4FE1\u7FA4\u91CC\u8D34\u7ED9\u5B98\u65B9\u4EBA\
    \u5458\u770B\u5230\u8FC7\uFF0C\u5B98\u65B9\u53EF\u80FD\u4E0D\u4F1A\u4E13\u95E8\
    \u9002\u914Dtext-generation-webui\uFF0C\u8981\u4E0D\u53BBtext-generation-webui\u63D0\
    \u4E2Aissue\uFF1F"
  created_at: 2023-08-10 00:20:44+00:00
  edited: false
  hidden: false
  id: 64d43b6cf8082bf19b8c606f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c8feb5547f59248fab60f2/ZntsvMfGXQh3hryJcG5Nc.png?w=200&h=200&f=face
      fullname: Komninos Chatzipapas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: komninos
      type: user
    createdAt: '2023-08-11T16:57:33.000Z'
    data:
      status: closed
    id: 64d6687d79d99b870039afb2
    type: status-change
  author: komninos
  created_at: 2023-08-11 15:57:33+00:00
  id: 64d6687d79d99b870039afb2
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: Qwen/Qwen-7B-Chat
repo_type: model
status: closed
target_branch: null
title: Model outputs gibberish after assistant response
