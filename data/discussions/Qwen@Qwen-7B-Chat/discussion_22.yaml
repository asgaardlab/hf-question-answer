!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cheshire94
conflicting_files: null
created_at: 2023-08-07 08:15:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6250563d6b9676db94a224e05e42ed3b.svg
      fullname: Shangming Cai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cheshire94
      type: user
    createdAt: '2023-08-07T09:15:47.000Z'
    data:
      edited: false
      editors:
      - Cheshire94
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195669293403625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6250563d6b9676db94a224e05e42ed3b.svg
          fullname: Shangming Cai
          isHf: false
          isPro: false
          name: Cheshire94
          type: user
        html: '<p>I see that the default max_new_tokens in the configuration file
          is 512. May I ask how long the context length that Qwen currently supports?</p>

          '
        raw: I see that the default max_new_tokens in the configuration file is 512.
          May I ask how long the context length that Qwen currently supports?
        updatedAt: '2023-08-07T09:15:47.512Z'
      numEdits: 0
      reactions: []
    id: 64d0b643d8d0927372d7ec2d
    type: comment
  author: Cheshire94
  content: I see that the default max_new_tokens in the configuration file is 512.
    May I ask how long the context length that Qwen currently supports?
  created_at: 2023-08-07 08:15:47+00:00
  edited: false
  hidden: false
  id: 64d0b643d8d0927372d7ec2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644682834896-620760a26e3b7210c2ff1943.jpeg?w=200&h=200&f=face
      fullname: Junyang Lin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JustinLin610
      type: user
    createdAt: '2023-08-09T08:54:43.000Z'
    data:
      edited: false
      editors:
      - JustinLin610
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9875598549842834
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644682834896-620760a26e3b7210c2ff1943.jpeg?w=200&h=200&f=face
          fullname: Junyang Lin
          isHf: false
          isPro: false
          name: JustinLin610
          type: user
        html: '<p>8K is supported. 16K Qwen does not perform well, and we will figure
          it out. </p>

          '
        raw: '8K is supported. 16K Qwen does not perform well, and we will figure
          it out. '
        updatedAt: '2023-08-09T08:54:43.862Z'
      numEdits: 0
      reactions: []
    id: 64d35453919f6f802e570421
    type: comment
  author: JustinLin610
  content: '8K is supported. 16K Qwen does not perform well, and we will figure it
    out. '
  created_at: 2023-08-09 07:54:43+00:00
  edited: false
  hidden: false
  id: 64d35453919f6f802e570421
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
      fullname: An Yang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yangapku
      type: user
    createdAt: '2023-08-16T03:21:23.000Z'
    data:
      status: closed
    id: 64dc40b300b80a024c850485
    type: status-change
  author: yangapku
  created_at: 2023-08-16 02:21:23+00:00
  id: 64dc40b300b80a024c850485
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
      fullname: An Yang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yangapku
      type: user
    createdAt: '2023-08-16T03:25:54.000Z'
    data:
      edited: false
      editors:
      - yangapku
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8401375412940979
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644733028938-62088594a5943c8a8fc94560.png?w=200&h=200&f=face
          fullname: An Yang
          isHf: false
          isPro: false
          name: yangapku
          type: user
        html: '<p>Currently up to 8K sequence length is supported and you can find
          the corresponding specifications in both <code>tokenizer_config.json</code>
          (the <code>model_max_length</code> key) and <code>config.json</code> (the
          <code>max_position_embeddings </code> key). These specifications in config
          files will raise warnings in the tokenization process and model forwarding
          respectively if there is sequence longer than 8192.</p>

          '
        raw: Currently up to 8K sequence length is supported and you can find the
          corresponding specifications in both `tokenizer_config.json` (the `model_max_length`
          key) and `config.json` (the `max_position_embeddings ` key). These specifications
          in config files will raise warnings in the tokenization process and model
          forwarding respectively if there is sequence longer than 8192.
        updatedAt: '2023-08-16T03:25:54.304Z'
      numEdits: 0
      reactions: []
    id: 64dc41c2176ca2c9a2e07336
    type: comment
  author: yangapku
  content: Currently up to 8K sequence length is supported and you can find the corresponding
    specifications in both `tokenizer_config.json` (the `model_max_length` key) and
    `config.json` (the `max_position_embeddings ` key). These specifications in config
    files will raise warnings in the tokenization process and model forwarding respectively
    if there is sequence longer than 8192.
  created_at: 2023-08-16 02:25:54+00:00
  edited: false
  hidden: false
  id: 64dc41c2176ca2c9a2e07336
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: Qwen/Qwen-7B-Chat
repo_type: model
status: closed
target_branch: null
title: Does Qwen support 16k context, what is the best config for max_new_tokens?
