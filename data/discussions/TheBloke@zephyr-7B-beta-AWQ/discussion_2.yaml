!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cfrancois7
conflicting_files: null
created_at: 2023-11-21 11:14:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f77db6a9e661efd4ac375489a267ab67.svg
      fullname: Cyril Francois
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cfrancois7
      type: user
    createdAt: '2023-11-21T11:14:48.000Z'
    data:
      edited: true
      editors:
      - cfrancois7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8982648253440857
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f77db6a9e661efd4ac375489a267ab67.svg
          fullname: Cyril Francois
          isHf: false
          isPro: false
          name: cfrancois7
          type: user
        html: '<p>I have only a RTX 3070 with 8Go VRAM.</p>

          <p>When I execute your code for AutoAWQ, it works well on my computer.<br>I
          succeed to manage the size of my max tokens.<br>I run with around 7Go of
          RAM.</p>

          <p>But when I want to test vLLM, the script want 14Go of GPU RAM allocation,
          and crash.<br>I do not succeed to change the max tokens size.</p>

          '
        raw: 'I have only a RTX 3070 with 8Go VRAM.


          When I execute your code for AutoAWQ, it works well on my computer.

          I succeed to manage the size of my max tokens.

          I run with around 7Go of RAM.


          But when I want to test vLLM, the script want 14Go of GPU RAM allocation,
          and crash.

          I do not succeed to change the max tokens size.'
        updatedAt: '2023-11-21T16:16:44.573Z'
      numEdits: 1
      reactions: []
    id: 655c912848320febfc47f7c3
    type: comment
  author: cfrancois7
  content: 'I have only a RTX 3070 with 8Go VRAM.


    When I execute your code for AutoAWQ, it works well on my computer.

    I succeed to manage the size of my max tokens.

    I run with around 7Go of RAM.


    But when I want to test vLLM, the script want 14Go of GPU RAM allocation, and
    crash.

    I do not succeed to change the max tokens size.'
  created_at: 2023-11-21 11:14:48+00:00
  edited: true
  hidden: false
  id: 655c912848320febfc47f7c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62acc1f1eda69b28bb64c39d/tmMMhkSyHc6HXDyhE7-jj.jpeg?w=200&h=200&f=face
      fullname: Will Reynolds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: willowill5
      type: user
    createdAt: '2023-12-04T22:33:38.000Z'
    data:
      edited: true
      editors:
      - willowill5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5353551506996155
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62acc1f1eda69b28bb64c39d/tmMMhkSyHc6HXDyhE7-jj.jpeg?w=200&h=200&f=face
          fullname: Will Reynolds
          isHf: false
          isPro: false
          name: willowill5
          type: user
        html: '<p>Try with --max-model-len 512</p>

          '
        raw: Try with --max-model-len 512
        updatedAt: '2023-12-04T22:40:12.497Z'
      numEdits: 1
      reactions: []
    id: 656e53c20f21e03bc87f4fe9
    type: comment
  author: willowill5
  content: Try with --max-model-len 512
  created_at: 2023-12-04 22:33:38+00:00
  edited: true
  hidden: false
  id: 656e53c20f21e03bc87f4fe9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f77db6a9e661efd4ac375489a267ab67.svg
      fullname: Cyril Francois
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cfrancois7
      type: user
    createdAt: '2023-12-08T21:27:16.000Z'
    data:
      edited: false
      editors:
      - cfrancois7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5027610063552856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f77db6a9e661efd4ac375489a267ab67.svg
          fullname: Cyril Francois
          isHf: false
          isPro: false
          name: cfrancois7
          type: user
        html: "<p>I reinstall and test with : </p>\n<pre><code>llm = LLM(\n    model=\"\
          TheBloke/zephyr-7B-beta-AWQ\", \n    quantization=\"awq\",\n    dtype=\"\
          auto\",\n    max_model_len=512,\n   gpu_memory_utilization=0.8\n)\n</code></pre>\n\
          <p>And it works.</p>\n"
        raw: "I reinstall and test with : \n```\nllm = LLM(\n    model=\"TheBloke/zephyr-7B-beta-AWQ\"\
          , \n    quantization=\"awq\",\n    dtype=\"auto\",\n    max_model_len=512,\n\
          \   gpu_memory_utilization=0.8\n)\n```\nAnd it works."
        updatedAt: '2023-12-08T21:27:16.444Z'
      numEdits: 0
      reactions: []
    id: 65738a34015c459f1b5db923
    type: comment
  author: cfrancois7
  content: "I reinstall and test with : \n```\nllm = LLM(\n    model=\"TheBloke/zephyr-7B-beta-AWQ\"\
    , \n    quantization=\"awq\",\n    dtype=\"auto\",\n    max_model_len=512,\n \
    \  gpu_memory_utilization=0.8\n)\n```\nAnd it works."
  created_at: 2023-12-08 21:27:16+00:00
  edited: false
  hidden: false
  id: 65738a34015c459f1b5db923
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/zephyr-7B-beta-AWQ
repo_type: model
status: open
target_branch: null
title: vLLM out of memory
