!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yasmolin
conflicting_files: null
created_at: 2023-11-13 13:56:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5cd1aebb812d2ab4302d783bbe2424c7.svg
      fullname: Yaro Allin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yasmolin
      type: user
    createdAt: '2023-11-13T13:56:24.000Z'
    data:
      edited: false
      editors:
      - yasmolin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9646139144897461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5cd1aebb812d2ab4302d783bbe2424c7.svg
          fullname: Yaro Allin
          isHf: false
          isPro: false
          name: yasmolin
          type: user
        html: '<p>Apparently there is no way to finetune this AWQ beauty and it''s
          for inference only?</p>

          '
        raw: Apparently there is no way to finetune this AWQ beauty and it's for inference
          only?
        updatedAt: '2023-11-13T13:56:24.497Z'
      numEdits: 0
      reactions: []
    id: 65522b08d9c8ceffd66c0a83
    type: comment
  author: yasmolin
  content: Apparently there is no way to finetune this AWQ beauty and it's for inference
    only?
  created_at: 2023-11-13 13:56:24+00:00
  edited: false
  hidden: false
  id: 65522b08d9c8ceffd66c0a83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-13T14:08:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9409417510032654
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Correct, I'm not aware of support for training on AWQ models at\
          \ this time. For training with quantization your options are BitsandBytes\
          \ (ie qLoRA), or GPTQ.  For training I recommend the <a rel=\"nofollow\"\
          \ href=\"https://github.com/OpenAccess-AI-Collective/axolotl\">Axolotl training\
          \ framework</a>, which supports both qLoRA and training of GPTQ models.</p>\n\
          <p>Now that Transformers supports AWQ, it's theoretically possible that\
          \ PEFT training support could come in the future.</p>\n<p>Tagging <span\
          \ data-props=\"{&quot;user&quot;:&quot;casperhansen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/casperhansen\">@<span\
          \ class=\"underline\">casperhansen</span></a></span>\n\n\t</span></span>\
          \ (author of AutoAWQ) and <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ (Hugging Face staff, responsible for the Transformers AWQ and GPTQ integration)\
          \ to make them aware of this request.</p>\n"
        raw: 'Correct, I''m not aware of support for training on AWQ models at this
          time. For training with quantization your options are BitsandBytes (ie qLoRA),
          or GPTQ.  For training I recommend the [Axolotl training framework](https://github.com/OpenAccess-AI-Collective/axolotl),
          which supports both qLoRA and training of GPTQ models.


          Now that Transformers supports AWQ, it''s theoretically possible that PEFT
          training support could come in the future.


          Tagging @casperhansen (author of AutoAWQ) and @ybelkada (Hugging Face staff,
          responsible for the Transformers AWQ and GPTQ integration) to make them
          aware of this request.'
        updatedAt: '2023-11-13T14:08:58.966Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - yasmolin
        - Benson
        - HuiZeng
        - dafraile
        - tazik-bronco
        - chansawman
    id: 65522dfa844b41012522b37c
    type: comment
  author: TheBloke
  content: 'Correct, I''m not aware of support for training on AWQ models at this
    time. For training with quantization your options are BitsandBytes (ie qLoRA),
    or GPTQ.  For training I recommend the [Axolotl training framework](https://github.com/OpenAccess-AI-Collective/axolotl),
    which supports both qLoRA and training of GPTQ models.


    Now that Transformers supports AWQ, it''s theoretically possible that PEFT training
    support could come in the future.


    Tagging @casperhansen (author of AutoAWQ) and @ybelkada (Hugging Face staff, responsible
    for the Transformers AWQ and GPTQ integration) to make them aware of this request.'
  created_at: 2023-11-13 14:08:58+00:00
  edited: false
  hidden: false
  id: 65522dfa844b41012522b37c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0c9185347d53a1f7393346ae3e58635.svg
      fullname: casperhansen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: casperhansen
      type: user
    createdAt: '2023-11-13T14:18:49.000Z'
    data:
      edited: false
      editors:
      - casperhansen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9635080099105835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0c9185347d53a1f7393346ae3e58635.svg
          fullname: casperhansen
          isHf: false
          isPro: false
          name: casperhansen
          type: user
        html: '<p>Generally, if you are on a tight budget, I would recommend to train
          quantized models with QLoRA, merge the adapter to base model, then quantize
          to your preferred quant, e.g. AWQ. </p>

          <p>AWQ is not compatible with PEFT yet, and I am not deep enough into the
          subject of training with quantized models to tell you if AWQ would be better
          than GPTQ in that scenario. </p>

          '
        raw: "Generally, if you are on a tight budget, I would recommend to train\
          \ quantized models with QLoRA, merge the adapter to base model, then quantize\
          \ to your preferred quant, e.g. AWQ. \n\nAWQ is not compatible with PEFT\
          \ yet, and I am not deep enough into the subject of training with quantized\
          \ models to tell you if AWQ would be better than GPTQ in that scenario. "
        updatedAt: '2023-11-13T14:18:49.147Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - ybelkada
        - yasmolin
        - choarama
    id: 65523049de3a8a88194fd75b
    type: comment
  author: casperhansen
  content: "Generally, if you are on a tight budget, I would recommend to train quantized\
    \ models with QLoRA, merge the adapter to base model, then quantize to your preferred\
    \ quant, e.g. AWQ. \n\nAWQ is not compatible with PEFT yet, and I am not deep\
    \ enough into the subject of training with quantized models to tell you if AWQ\
    \ would be better than GPTQ in that scenario. "
  created_at: 2023-11-13 14:18:49+00:00
  edited: false
  hidden: false
  id: 65523049de3a8a88194fd75b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-13T17:25:27.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8261402249336243
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Indeed I second what <span data-props=\"{&quot;user&quot;:&quot;casperhansen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/casperhansen\"\
          >@<span class=\"underline\">casperhansen</span></a></span>\n\n\t</span></span>\
          \ said, the recommended workflow is<br>1- Fine-tune the base model using\
          \ QLoRA on your target domain<br>2- Further quantize it with AWQ / GPTQ\
          \ using tools such as autoawq<br>3- Deploy the AWQ/GPTQ for faster inference<br>You\
          \ can read more about it here: <a href=\"https://huggingface.co/blog/overview-quantization-transformers\"\
          >https://huggingface.co/blog/overview-quantization-transformers</a></p>\n"
        raw: "Indeed I second what @casperhansen said, the recommended workflow is\
          \ \n1- Fine-tune the base model using QLoRA on your target domain\n2- Further\
          \ quantize it with AWQ / GPTQ using tools such as autoawq\n3- Deploy the\
          \ AWQ/GPTQ for faster inference\nYou can read more about it here: https://huggingface.co/blog/overview-quantization-transformers"
        updatedAt: '2023-11-13T17:25:27.779Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - yasmolin
        - wizmak
        - choarama
    id: 65525c07764637dae071e5ef
    type: comment
  author: ybelkada
  content: "Indeed I second what @casperhansen said, the recommended workflow is \n\
    1- Fine-tune the base model using QLoRA on your target domain\n2- Further quantize\
    \ it with AWQ / GPTQ using tools such as autoawq\n3- Deploy the AWQ/GPTQ for faster\
    \ inference\nYou can read more about it here: https://huggingface.co/blog/overview-quantization-transformers"
  created_at: 2023-11-13 17:25:27+00:00
  edited: false
  hidden: false
  id: 65525c07764637dae071e5ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2882dc5ca893ab2113420b163855541.svg
      fullname: Matt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mph
      type: user
    createdAt: '2023-12-01T18:19:06.000Z'
    data:
      edited: false
      editors:
      - mph
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9605867266654968
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2882dc5ca893ab2113420b163855541.svg
          fullname: Matt
          isHf: false
          isPro: false
          name: mph
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;casperhansen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/casperhansen\"\
          >@<span class=\"underline\">casperhansen</span></a></span>\n\n\t</span></span>\
          \ I'm looking forward to the day when AWQ and PEFT play nicely together.\
          \ I'm currently creating several GPTQ-LoRA adapters, one for each of my\
          \ tasks. That way, I can keep just one GPTQ base model in VRAM at all times\
          \ and then enable one adapter at a time, depending on where I am in my pipeline.\
          \ Obviously, I would prefer to be doing this with the superior AWQ method.\
          \ \U0001F642</p>\n"
        raw: "@casperhansen I'm looking forward to the day when AWQ and PEFT play\
          \ nicely together. I'm currently creating several GPTQ-LoRA adapters, one\
          \ for each of my tasks. That way, I can keep just one GPTQ base model in\
          \ VRAM at all times and then enable one adapter at a time, depending on\
          \ where I am in my pipeline. Obviously, I would prefer to be doing this\
          \ with the superior AWQ method. \U0001F642"
        updatedAt: '2023-12-01T18:19:06.665Z'
      numEdits: 0
      reactions: []
    id: 656a239ae0ff1cebe9049a31
    type: comment
  author: mph
  content: "@casperhansen I'm looking forward to the day when AWQ and PEFT play nicely\
    \ together. I'm currently creating several GPTQ-LoRA adapters, one for each of\
    \ my tasks. That way, I can keep just one GPTQ base model in VRAM at all times\
    \ and then enable one adapter at a time, depending on where I am in my pipeline.\
    \ Obviously, I would prefer to be doing this with the superior AWQ method. \U0001F642"
  created_at: 2023-12-01 18:19:06+00:00
  edited: false
  hidden: false
  id: 656a239ae0ff1cebe9049a31
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/zephyr-7B-beta-AWQ
repo_type: model
status: open
target_branch: null
title: Is there a way to finetune these AWQ beasts?
