!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tarruda
conflicting_files: null
created_at: 2023-12-04 17:17:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
      fullname: Thiago de Arruda Padilha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarruda
      type: user
    createdAt: '2023-12-04T17:17:43.000Z'
    data:
      edited: false
      editors:
      - tarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9374447464942932
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
          fullname: Thiago de Arruda Padilha
          isHf: false
          isPro: false
          name: tarruda
          type: user
        html: '<p>Accoding to <a rel="nofollow" href="https://twitter.com/Teknium1/status/1731409499595194679">https://twitter.com/Teknium1/status/1731409499595194679</a>,
          this model is not behaving as expected (I was really looking forward to
          trying it! :( ).</p>

          <p>As a dummy LLM user which doesn''t undertand what you''re doing here
          I have a question: Instead of adding vision directly to the LLM, why not
          train it to emit a function call whenever the user prompts for an image
          to be analyzed?</p>

          <p>For example, let''s say we''re building a ChatGPT clone which allows
          the user to upload images, instead of embedding directly the image URL or
          base64 in the conversation (which theoretically consumes context), the app
          could add an uuid  of the image and embed something like this in the conversation:</p>

          <pre><code>[image:68ffdd0b-2e14-4a0c-a57f-e713164d1271]


          What is in this image?

          </code></pre>

          <p>The uuid is a way for the application to locate the image in a database,
          filesystem or URL (assuming the user uploaded the image into the conversation,
          the app would have stored it somewhere). When the LLM sees something like
          this, generate a function call (with uuid as parameter) which the app can
          forward to a vision encoder model or external API and then inject the results
          back in the conversation.</p>

          '
        raw: "Accoding to https://twitter.com/Teknium1/status/1731409499595194679,\
          \ this model is not behaving as expected (I was really looking forward to\
          \ trying it! :( ).\r\n\r\nAs a dummy LLM user which doesn't undertand what\
          \ you're doing here I have a question: Instead of adding vision directly\
          \ to the LLM, why not train it to emit a function call whenever the user\
          \ prompts for an image to be analyzed?\r\n\r\nFor example, let's say we're\
          \ building a ChatGPT clone which allows the user to upload images, instead\
          \ of embedding directly the image URL or base64 in the conversation (which\
          \ theoretically consumes context), the app could add an uuid  of the image\
          \ and embed something like this in the conversation:\r\n\r\n```\r\n[image:68ffdd0b-2e14-4a0c-a57f-e713164d1271]\r\
          \n\r\nWhat is in this image?\r\n```\r\n\r\nThe uuid is a way for the application\
          \ to locate the image in a database, filesystem or URL (assuming the user\
          \ uploaded the image into the conversation, the app would have stored it\
          \ somewhere). When the LLM sees something like this, generate a function\
          \ call (with uuid as parameter) which the app can forward to a vision encoder\
          \ model or external API and then inject the results back in the conversation."
        updatedAt: '2023-12-04T17:17:43.120Z'
      numEdits: 0
      reactions: []
    id: 656e09b7efd0eea7c5f1bef6
    type: comment
  author: tarruda
  content: "Accoding to https://twitter.com/Teknium1/status/1731409499595194679, this\
    \ model is not behaving as expected (I was really looking forward to trying it!\
    \ :( ).\r\n\r\nAs a dummy LLM user which doesn't undertand what you're doing here\
    \ I have a question: Instead of adding vision directly to the LLM, why not train\
    \ it to emit a function call whenever the user prompts for an image to be analyzed?\r\
    \n\r\nFor example, let's say we're building a ChatGPT clone which allows the user\
    \ to upload images, instead of embedding directly the image URL or base64 in the\
    \ conversation (which theoretically consumes context), the app could add an uuid\
    \  of the image and embed something like this in the conversation:\r\n\r\n```\r\
    \n[image:68ffdd0b-2e14-4a0c-a57f-e713164d1271]\r\n\r\nWhat is in this image?\r\
    \n```\r\n\r\nThe uuid is a way for the application to locate the image in a database,\
    \ filesystem or URL (assuming the user uploaded the image into the conversation,\
    \ the app would have stored it somewhere). When the LLM sees something like this,\
    \ generate a function call (with uuid as parameter) which the app can forward\
    \ to a vision encoder model or external API and then inject the results back in\
    \ the conversation."
  created_at: 2023-12-04 17:17:43+00:00
  edited: false
  hidden: false
  id: 656e09b7efd0eea7c5f1bef6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64137e2150358a805203cbac/w9RQx8Q07UvgFyIZ3ce_k.jpeg?w=200&h=200&f=face
      fullname: Jade
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: euclaise
      type: user
    createdAt: '2023-12-04T17:33:09.000Z'
    data:
      edited: true
      editors:
      - euclaise
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9316043853759766
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64137e2150358a805203cbac/w9RQx8Q07UvgFyIZ3ce_k.jpeg?w=200&h=200&f=face
          fullname: Jade
          isHf: false
          isPro: false
          name: euclaise
          type: user
        html: '<p>The vision encoder model ultimately has to emit embeddings (pseudo-text)
          for the model to interpret. Unless I''m unaware of some unique point of
          this model, the image isn''t embedded as a URL or base64 -- it''s embedded
          as image encoder tokens.</p>

          '
        raw: The vision encoder model ultimately has to emit embeddings (pseudo-text)
          for the model to interpret. Unless I'm unaware of some unique point of this
          model, the image isn't embedded as a URL or base64 -- it's embedded as image
          encoder tokens.
        updatedAt: '2023-12-04T17:33:25.616Z'
      numEdits: 1
      reactions: []
    id: 656e0d559ced9d5ff5ace743
    type: comment
  author: euclaise
  content: The vision encoder model ultimately has to emit embeddings (pseudo-text)
    for the model to interpret. Unless I'm unaware of some unique point of this model,
    the image isn't embedded as a URL or base64 -- it's embedded as image encoder
    tokens.
  created_at: 2023-12-04 17:33:09+00:00
  edited: true
  hidden: false
  id: 656e0d559ced9d5ff5ace743
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
      fullname: Thiago de Arruda Padilha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarruda
      type: user
    createdAt: '2023-12-04T19:46:33.000Z'
    data:
      edited: false
      editors:
      - tarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8836169838905334
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
          fullname: Thiago de Arruda Padilha
          isHf: false
          isPro: false
          name: tarruda
          type: user
        html: '<blockquote>

          <p>The vision encoder model ultimately has to emit embeddings (pseudo-text)
          for the model to interpret. Unless I''m unaware of some unique point of
          this model, the image isn''t embedded as a URL or base64 -- it''s embedded
          as image encoder tokens.</p>

          </blockquote>

          <p>I see. Wouldn''t it be possible to implement Vision by having a function
          call that generates a detailed text description for the LLM, which uses
          the description to answer any questions the user might have about the image?
          Here''s an example of what I meant: <a href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef">https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef</a></p>

          '
        raw: '> The vision encoder model ultimately has to emit embeddings (pseudo-text)
          for the model to interpret. Unless I''m unaware of some unique point of
          this model, the image isn''t embedded as a URL or base64 -- it''s embedded
          as image encoder tokens.


          I see. Wouldn''t it be possible to implement Vision by having a function
          call that generates a detailed text description for the LLM, which uses
          the description to answer any questions the user might have about the image?
          Here''s an example of what I meant: https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef'
        updatedAt: '2023-12-04T19:46:33.795Z'
      numEdits: 0
      reactions: []
    id: 656e2c990bbc114fe667447d
    type: comment
  author: tarruda
  content: '> The vision encoder model ultimately has to emit embeddings (pseudo-text)
    for the model to interpret. Unless I''m unaware of some unique point of this model,
    the image isn''t embedded as a URL or base64 -- it''s embedded as image encoder
    tokens.


    I see. Wouldn''t it be possible to implement Vision by having a function call
    that generates a detailed text description for the LLM, which uses the description
    to answer any questions the user might have about the image? Here''s an example
    of what I meant: https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef'
  created_at: 2023-12-04 19:46:33+00:00
  edited: false
  hidden: false
  id: 656e2c990bbc114fe667447d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64137e2150358a805203cbac/w9RQx8Q07UvgFyIZ3ce_k.jpeg?w=200&h=200&f=face
      fullname: Jade
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: euclaise
      type: user
    createdAt: '2023-12-04T19:47:40.000Z'
    data:
      edited: false
      editors:
      - euclaise
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8930245041847229
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64137e2150358a805203cbac/w9RQx8Q07UvgFyIZ3ce_k.jpeg?w=200&h=200&f=face
          fullname: Jade
          isHf: false
          isPro: false
          name: euclaise
          type: user
        html: '<blockquote>

          <blockquote>

          <p>The vision encoder model ultimately has to emit embeddings (pseudo-text)
          for the model to interpret. Unless I''m unaware of some unique point of
          this model, the image isn''t embedded as a URL or base64 -- it''s embedded
          as image encoder tokens.</p>

          </blockquote>

          <p>I see. Wouldn''t it be possible to implement Vision by having a function
          call that generates a detailed text description for the LLM, which uses
          the description to answer any questions the user might have about the image?
          Here''s an example of what I meant: <a href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef">https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef</a></p>

          </blockquote>

          <p>I believe that is how Google Bard''s vision works - it gets a detailed
          description from Lens, and uses that</p>

          '
        raw: "> > The vision encoder model ultimately has to emit embeddings (pseudo-text)\
          \ for the model to interpret. Unless I'm unaware of some unique point of\
          \ this model, the image isn't embedded as a URL or base64 -- it's embedded\
          \ as image encoder tokens.\n> \n> I see. Wouldn't it be possible to implement\
          \ Vision by having a function call that generates a detailed text description\
          \ for the LLM, which uses the description to answer any questions the user\
          \ might have about the image? Here's an example of what I meant: https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef\n\
          \nI believe that is how Google Bard's vision works - it gets a detailed\
          \ description from Lens, and uses that"
        updatedAt: '2023-12-04T19:47:40.531Z'
      numEdits: 0
      reactions: []
    id: 656e2cdc996819a828f35124
    type: comment
  author: euclaise
  content: "> > The vision encoder model ultimately has to emit embeddings (pseudo-text)\
    \ for the model to interpret. Unless I'm unaware of some unique point of this\
    \ model, the image isn't embedded as a URL or base64 -- it's embedded as image\
    \ encoder tokens.\n> \n> I see. Wouldn't it be possible to implement Vision by\
    \ having a function call that generates a detailed text description for the LLM,\
    \ which uses the description to answer any questions the user might have about\
    \ the image? Here's an example of what I meant: https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/discussions/3#656e177f85562996982218ef\n\
    \nI believe that is how Google Bard's vision works - it gets a detailed description\
    \ from Lens, and uses that"
  created_at: 2023-12-04 19:47:40+00:00
  edited: false
  hidden: false
  id: 656e2cdc996819a828f35124
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-05T14:59:43.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.982044517993927
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>yeah so its pretty accurate but it cant really get the full meaning
          of the image. </p>

          '
        raw: 'yeah so its pretty accurate but it cant really get the full meaning
          of the image. '
        updatedAt: '2023-12-05T14:59:43.864Z'
      numEdits: 0
      reactions: []
    id: 656f3adfd23698221e09c278
    type: comment
  author: YaTharThShaRma999
  content: 'yeah so its pretty accurate but it cant really get the full meaning of
    the image. '
  created_at: 2023-12-05 14:59:43+00:00
  edited: false
  hidden: false
  id: 656f3adfd23698221e09c278
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b86328c76e635315401c7b609d861fc.svg
      fullname: Quan Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: qnguyen3
      type: user
    createdAt: '2023-12-05T19:25:04.000Z'
    data:
      edited: false
      editors:
      - qnguyen3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9642425179481506
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b86328c76e635315401c7b609d861fc.svg
          fullname: Quan Nguyen
          isHf: false
          isPro: false
          name: qnguyen3
          type: user
        html: '<p>LLaVA data was generated like that. They feed GPT-4 the caption
          and metadata of the images then tell GPT-4 to generate a conversation out
          of those. This is a great way to get more data but also very vulnerable
          to hallucination</p>

          '
        raw: LLaVA data was generated like that. They feed GPT-4 the caption and metadata
          of the images then tell GPT-4 to generate a conversation out of those. This
          is a great way to get more data but also very vulnerable to hallucination
        updatedAt: '2023-12-05T19:25:04.236Z'
      numEdits: 0
      reactions: []
    id: 656f7910990fa7ae20234e49
    type: comment
  author: qnguyen3
  content: LLaVA data was generated like that. They feed GPT-4 the caption and metadata
    of the images then tell GPT-4 to generate a conversation out of those. This is
    a great way to get more data but also very vulnerable to hallucination
  created_at: 2023-12-05 19:25:04+00:00
  edited: false
  hidden: false
  id: 656f7910990fa7ae20234e49
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: NousResearch/Nous-Hermes-2-Vision-Alpha
repo_type: model
status: open
target_branch: null
title: Implement vision with function calling?
