!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zhimakaimenhf
conflicting_files: null
created_at: 2023-09-22 08:16:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7453a5915c2a8153302d6986fcbfba44.svg
      fullname: yan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhimakaimenhf
      type: user
    createdAt: '2023-09-22T09:16:21.000Z'
    data:
      edited: false
      editors:
      - zhimakaimenhf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5427166819572449
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7453a5915c2a8153302d6986fcbfba44.svg
          fullname: yan
          isHf: false
          isPro: false
          name: zhimakaimenhf
          type: user
        html: "<p>I build whisper.cpp with following command line:</p>\n<p>sudo WHISPER_CUBLAS=1\
          \ WHISPER_CLBLAST=1 make -j8</p>\n<p>Then I run the following line:./main\
          \ -m models/ggml-base.bin -f samples/jfk.wav</p>\n<p>got result:</p>\n<p>whisper_init_from_file_no_state:\
          \ loading model from 'models/ggml-base.bin'<br>whisper_model_load: loading\
          \ model<br>whisper_model_load: n_vocab       = 51865<br>whisper_model_load:\
          \ n_audio_ctx   = 1500<br>whisper_model_load: n_audio_state = 512<br>whisper_model_load:\
          \ n_audio_head  = 8<br>whisper_model_load: n_audio_layer = 6<br>whisper_model_load:\
          \ n_text_ctx    = 448<br>whisper_model_load: n_text_state  = 512<br>whisper_model_load:\
          \ n_text_head   = 8<br>whisper_model_load: n_text_layer  = 6<br>whisper_model_load:\
          \ n_mels        = 80<br>whisper_model_load: ftype         = 1<br>whisper_model_load:\
          \ qntvr         = 0<br>whisper_model_load: type          = 2<br>whisper_model_load:\
          \ adding 1608 extra tokens<br>whisper_model_load: model ctx     =  140.66\
          \ MB<br>ggml_init_cublas: found 1 CUDA devices:<br>  Device 0: NVIDIA GeForce\
          \ RTX 3060, compute capability 8.6<br>whisper_model_load: model size   \
          \ =  140.54 MB<br>whisper_init_state: kv self size  =    5.25 MB<br>whisper_init_state:\
          \ kv cross size =   17.58 MB<br>whisper_init_state: compute buffer (conv)\
          \   =   14.10 MB<br>whisper_init_state: compute buffer (encode) =   81.85\
          \ MB<br>whisper_init_state: compute buffer (cross)  =    4.40 MB<br>whisper_init_state:\
          \ compute buffer (decode) =   24.61 MB</p>\n<p>system_info: n_threads =\
          \ 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA\
          \ = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 |\
          \ SSE3 = 1 | SSSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 0 | </p>\n<p>main:\
          \ processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1\
          \ processors, lang = en, task = transcribe, timestamps = 1 ...</p>\n<p>[00:00:00.000\
          \ --&gt; 00:00:07.600]   And so my fellow Americans, ask not what your country\
          \ can do for you,<br>[00:00:07.600 --&gt; 00:00:10.600]   ask what you can\
          \ do for your country.</p>\n<p>whisper_print_timings:     load time =  \
          \ 856.69 ms<br>whisper_print_timings:     fallbacks =   0 p /   0 h<br>whisper_print_timings:\
          \      mel time =     9.35 ms<br>whisper_print_timings:   sample time =\
          \     9.88 ms /    29 runs (    0.34 ms per run)<br>whisper_print_timings:\
          \   encode time =   361.23 ms /     1 runs (  361.23 ms per run)<br>whisper_print_timings:\
          \   decode time =   138.41 ms /    28 runs (    4.94 ms per run)<br>whisper_print_timings:\
          \   prompt time =     6.21 ms /     1 runs (    6.21 ms per run)<br>whisper_print_timings:\
          \    total time =  1426.51 ms</p>\n<p>on python project whisper jax, same\
          \ model, same wave file, only takes 0.137s</p>\n<p>whisper jax works on\
          \ jax, but only 1 3060 there, it can't earn a lot from pmap(parallel computing\uFF09\
          </p>\n<p>Any idea?</p>\n"
        raw: "I build whisper.cpp with following command line:\r\n\r\nsudo WHISPER_CUBLAS=1\
          \ WHISPER_CLBLAST=1 make -j8\r\n\r\nThen I run the following line:./main\
          \ -m models/ggml-base.bin -f samples/jfk.wav\r\n\r\ngot result:\r\n\r\n\
          whisper_init_from_file_no_state: loading model from 'models/ggml-base.bin'\r\
          \nwhisper_model_load: loading model\r\nwhisper_model_load: n_vocab     \
          \  = 51865\r\nwhisper_model_load: n_audio_ctx   = 1500\r\nwhisper_model_load:\
          \ n_audio_state = 512\r\nwhisper_model_load: n_audio_head  = 8\r\nwhisper_model_load:\
          \ n_audio_layer = 6\r\nwhisper_model_load: n_text_ctx    = 448\r\nwhisper_model_load:\
          \ n_text_state  = 512\r\nwhisper_model_load: n_text_head   = 8\r\nwhisper_model_load:\
          \ n_text_layer  = 6\r\nwhisper_model_load: n_mels        = 80\r\nwhisper_model_load:\
          \ ftype         = 1\r\nwhisper_model_load: qntvr         = 0\r\nwhisper_model_load:\
          \ type          = 2\r\nwhisper_model_load: adding 1608 extra tokens\r\n\
          whisper_model_load: model ctx     =  140.66 MB\r\nggml_init_cublas: found\
          \ 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3060, compute capability\
          \ 8.6\r\nwhisper_model_load: model size    =  140.54 MB\r\nwhisper_init_state:\
          \ kv self size  =    5.25 MB\r\nwhisper_init_state: kv cross size =   17.58\
          \ MB\r\nwhisper_init_state: compute buffer (conv)   =   14.10 MB\r\nwhisper_init_state:\
          \ compute buffer (encode) =   81.85 MB\r\nwhisper_init_state: compute buffer\
          \ (cross)  =    4.40 MB\r\nwhisper_init_state: compute buffer (decode) =\
          \   24.61 MB\r\n\r\nsystem_info: n_threads = 4 / 12 | AVX = 1 | AVX2 = 1\
          \ | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1\
          \ | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX\
          \ = 0 | COREML = 0 | OPENVINO = 0 | \r\n\r\nmain: processing 'samples/jfk.wav'\
          \ (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task =\
          \ transcribe, timestamps = 1 ...\r\n\r\n\r\n[00:00:00.000 --> 00:00:07.600]\
          \   And so my fellow Americans, ask not what your country can do for you,\r\
          \n[00:00:07.600 --> 00:00:10.600]   ask what you can do for your country.\r\
          \n\r\n\r\nwhisper_print_timings:     load time =   856.69 ms\r\nwhisper_print_timings:\
          \     fallbacks =   0 p /   0 h\r\nwhisper_print_timings:      mel time\
          \ =     9.35 ms\r\nwhisper_print_timings:   sample time =     9.88 ms /\
          \    29 runs (    0.34 ms per run)\r\nwhisper_print_timings:   encode time\
          \ =   361.23 ms /     1 runs (  361.23 ms per run)\r\nwhisper_print_timings:\
          \   decode time =   138.41 ms /    28 runs (    4.94 ms per run)\r\nwhisper_print_timings:\
          \   prompt time =     6.21 ms /     1 runs (    6.21 ms per run)\r\nwhisper_print_timings:\
          \    total time =  1426.51 ms\r\n\r\non python project whisper jax, same\
          \ model, same wave file, only takes 0.137s\r\n\r\nwhisper jax works on jax,\
          \ but only 1 3060 there, it can't earn a lot from pmap(parallel computing\uFF09\
          \r\n\r\nAny idea?\r\n\r\n"
        updatedAt: '2023-09-22T09:16:21.500Z'
      numEdits: 0
      reactions: []
    id: 650d5b6503e1ec1fc224d9cb
    type: comment
  author: zhimakaimenhf
  content: "I build whisper.cpp with following command line:\r\n\r\nsudo WHISPER_CUBLAS=1\
    \ WHISPER_CLBLAST=1 make -j8\r\n\r\nThen I run the following line:./main -m models/ggml-base.bin\
    \ -f samples/jfk.wav\r\n\r\ngot result:\r\n\r\nwhisper_init_from_file_no_state:\
    \ loading model from 'models/ggml-base.bin'\r\nwhisper_model_load: loading model\r\
    \nwhisper_model_load: n_vocab       = 51865\r\nwhisper_model_load: n_audio_ctx\
    \   = 1500\r\nwhisper_model_load: n_audio_state = 512\r\nwhisper_model_load: n_audio_head\
    \  = 8\r\nwhisper_model_load: n_audio_layer = 6\r\nwhisper_model_load: n_text_ctx\
    \    = 448\r\nwhisper_model_load: n_text_state  = 512\r\nwhisper_model_load: n_text_head\
    \   = 8\r\nwhisper_model_load: n_text_layer  = 6\r\nwhisper_model_load: n_mels\
    \        = 80\r\nwhisper_model_load: ftype         = 1\r\nwhisper_model_load:\
    \ qntvr         = 0\r\nwhisper_model_load: type          = 2\r\nwhisper_model_load:\
    \ adding 1608 extra tokens\r\nwhisper_model_load: model ctx     =  140.66 MB\r\
    \nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3060,\
    \ compute capability 8.6\r\nwhisper_model_load: model size    =  140.54 MB\r\n\
    whisper_init_state: kv self size  =    5.25 MB\r\nwhisper_init_state: kv cross\
    \ size =   17.58 MB\r\nwhisper_init_state: compute buffer (conv)   =   14.10 MB\r\
    \nwhisper_init_state: compute buffer (encode) =   81.85 MB\r\nwhisper_init_state:\
    \ compute buffer (cross)  =    4.40 MB\r\nwhisper_init_state: compute buffer (decode)\
    \ =   24.61 MB\r\n\r\nsystem_info: n_threads = 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512\
    \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0\
    \ | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO\
    \ = 0 | \r\n\r\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec),\
    \ 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\r\n\
    \r\n\r\n[00:00:00.000 --> 00:00:07.600]   And so my fellow Americans, ask not\
    \ what your country can do for you,\r\n[00:00:07.600 --> 00:00:10.600]   ask what\
    \ you can do for your country.\r\n\r\n\r\nwhisper_print_timings:     load time\
    \ =   856.69 ms\r\nwhisper_print_timings:     fallbacks =   0 p /   0 h\r\nwhisper_print_timings:\
    \      mel time =     9.35 ms\r\nwhisper_print_timings:   sample time =     9.88\
    \ ms /    29 runs (    0.34 ms per run)\r\nwhisper_print_timings:   encode time\
    \ =   361.23 ms /     1 runs (  361.23 ms per run)\r\nwhisper_print_timings: \
    \  decode time =   138.41 ms /    28 runs (    4.94 ms per run)\r\nwhisper_print_timings:\
    \   prompt time =     6.21 ms /     1 runs (    6.21 ms per run)\r\nwhisper_print_timings:\
    \    total time =  1426.51 ms\r\n\r\non python project whisper jax, same model,\
    \ same wave file, only takes 0.137s\r\n\r\nwhisper jax works on jax, but only\
    \ 1 3060 there, it can't earn a lot from pmap(parallel computing\uFF09\r\n\r\n\
    Any idea?\r\n\r\n"
  created_at: 2023-09-22 08:16:21+00:00
  edited: false
  hidden: false
  id: 650d5b6503e1ec1fc224d9cb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: ggerganov/whisper.cpp
repo_type: model
status: open
target_branch: null
title: whisper.cpp is much slower than whisper jax?
