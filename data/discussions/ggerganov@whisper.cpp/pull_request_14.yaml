!!python/object:huggingface_hub.community.DiscussionWithDetails
author: solaoi
conflicting_files: []
created_at: 2023-11-25 07:14:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0e805cef7927e6f29856ab81dd15332.svg
      fullname: Kodai Aoyama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: solaoi
      type: user
    createdAt: '2023-11-25T07:14:45.000Z'
    data:
      edited: false
      editors:
      - solaoi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45962023735046387
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0e805cef7927e6f29856ab81dd15332.svg
          fullname: Kodai Aoyama
          isHf: false
          isPro: false
          name: solaoi
          type: user
        html: "<p>I have uploaded the updated large-v3 CoreML model.<br><a rel=\"\
          nofollow\" href=\"https://github.com/ggerganov/whisper.cpp/issues/1437#issuecomment-1807608344\"\
          >https://github.com/ggerganov/whisper.cpp/issues/1437#issuecomment-1807608344</a></p>\n\
          <p>It seems to be working properly according to the operation check logs.</p>\n\
          <blockquote>\n<p>whisper_init_state: loading Core ML model from 'models/ggml-large-v3-encoder.mlmodelc'</p>\n\
          </blockquote>\n<pre><code>./main -m models/ggml-large-v3.bin -f samples/jfk.wav\n\
          whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-large-v3.bin'\n\
          whisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51866\n\
          whisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state\
          \ = 1280\nwhisper_model_load: n_audio_head  = 20\nwhisper_model_load: n_audio_layer\
          \ = 32\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state\
          \  = 1280\nwhisper_model_load: n_text_head   = 20\nwhisper_model_load: n_text_layer\
          \  = 32\nwhisper_model_load: n_mels        = 128\nwhisper_model_load: ftype\
          \         = 1\nwhisper_model_load: qntvr         = 0\nwhisper_model_load:\
          \ type          = 5 (large v3)\nwhisper_model_load: adding 1609 extra tokens\n\
          whisper_model_load: n_langs       = 100\nwhisper_backend_init: using Metal\
          \ backend\nggml_metal_init: allocating\nggml_metal_init: found device: Apple\
          \ M2\nggml_metal_init: picking default device: Apple M2\nggml_metal_init:\
          \ default.metallib not found, loading from source\nggml_metal_init: loading\
          \ '/Users/solaoi/Projects/solaoi/whisper.cpp/ggml-metal.metal'\nggml_metal_init:\
          \ GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8\
          \ (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
          \ recommendedMaxWorkingSetSize  = 17179.89 MB\nggml_metal_init: maxTransferRate\
          \               = built-in GPU\nggml_metal_add_buffer: allocated 'backend\
          \         ' buffer, size =  3117.88 MB, ( 3118.40 / 17179.89)\nwhisper_model_load:\
          \    Metal buffer size =  3117.87 MB\nwhisper_model_load: model size   \
          \ = 3117.39 MB\nwhisper_backend_init: using Metal backend\nggml_metal_init:\
          \ allocating\nggml_metal_init: found device: Apple M2\nggml_metal_init:\
          \ picking default device: Apple M2\nggml_metal_init: default.metallib not\
          \ found, loading from source\nggml_metal_init: loading '/Users/solaoi/Projects/solaoi/whisper.cpp/ggml-metal.metal'\n\
          ggml_metal_init: GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8\
          \ (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
          \ recommendedMaxWorkingSetSize  = 17179.89 MB\nggml_metal_init: maxTransferRate\
          \               = built-in GPU\nggml_metal_add_buffer: allocated 'backend\
          \         ' buffer, size =   220.20 MB, ( 3338.60 / 17179.89)\nwhisper_init_state:\
          \ kv self size  =  220.20 MB\nggml_metal_add_buffer: allocated 'backend\
          \         ' buffer, size =   245.76 MB, ( 3584.36 / 17179.89)\nwhisper_init_state:\
          \ kv cross size =  245.76 MB\nwhisper_init_state: loading Core ML model\
          \ from 'models/ggml-large-v3-encoder.mlmodelc'\nwhisper_init_state: first\
          \ run on a device may take a while ...\nwhisper_init_state: Core ML model\
          \ loaded\nggml_metal_add_buffer: allocated 'backend         ' buffer, size\
          \ =     0.02 MB, ( 3593.88 / 17179.89)\nwhisper_init_state: compute buffer\
          \ (conv)   =   10.85 MB\nggml_metal_add_buffer: allocated 'backend     \
          \    ' buffer, size =     0.02 MB, ( 3593.90 / 17179.89)\nwhisper_init_state:\
          \ compute buffer (cross)  =    9.32 MB\nggml_metal_add_buffer: allocated\
          \ 'backend         ' buffer, size =     0.02 MB, ( 3593.91 / 17179.89)\n\
          whisper_init_state: compute buffer (decode) =   99.17 MB\nggml_metal_add_buffer:\
          \ allocated 'backend         ' buffer, size =     9.22 MB, ( 3603.14 / 17179.89)\n\
          ggml_metal_add_buffer: allocated 'backend         ' buffer, size =     7.68\
          \ MB, ( 3610.82 / 17179.89)\nggml_metal_add_buffer: allocated 'backend \
          \        ' buffer, size =    97.53 MB, ( 3708.35 / 17179.89)\n\nsystem_info:\
          \ n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON =\
          \ 1 | ARM_FMA = 1 | METAL = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 |\
          \ BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | CUDA = 0 | COREML = 1 | OPENVINO\
          \ = 0 | \n\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec),\
          \ 4 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe,\
          \ timestamps = 1 ...\n\n\n[00:00:00.300 --&gt; 00:00:09.000]   And so, my\
          \ fellow Americans, ask not what your country can do for you, ask what you\n\
          [00:00:09.000 --&gt; 00:00:11.000]   can do for your country.\n\n\nwhisper_print_timings:\
          \     load time =   984.30 ms\nwhisper_print_timings:     fallbacks =  \
          \ 0 p /   0 h\nwhisper_print_timings:      mel time =     7.56 ms\nwhisper_print_timings:\
          \   sample time =    81.95 ms /   148 runs (    0.55 ms per run)\nwhisper_print_timings:\
          \   encode time =  1999.00 ms /     1 runs ( 1999.00 ms per run)\nwhisper_print_timings:\
          \   decode time =     0.00 ms /     1 runs (    0.00 ms per run)\nwhisper_print_timings:\
          \   batchd time =  1377.15 ms /   146 runs (    9.43 ms per run)\nwhisper_print_timings:\
          \   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\nwhisper_print_timings:\
          \    total time =  5798.28 ms\nggml_metal_free: deallocating\nggml_metal_free:\
          \ deallocating\n</code></pre>\n"
        raw: "I have uploaded the updated large-v3 CoreML model.\nhttps://github.com/ggerganov/whisper.cpp/issues/1437#issuecomment-1807608344\n\
          \nIt seems to be working properly according to the operation check logs.\n\
          > whisper_init_state: loading Core ML model from 'models/ggml-large-v3-encoder.mlmodelc'\n\
          ```\n./main -m models/ggml-large-v3.bin -f samples/jfk.wav\nwhisper_init_from_file_with_params_no_state:\
          \ loading model from 'models/ggml-large-v3.bin'\nwhisper_model_load: loading\
          \ model\nwhisper_model_load: n_vocab       = 51866\nwhisper_model_load:\
          \ n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 1280\nwhisper_model_load:\
          \ n_audio_head  = 20\nwhisper_model_load: n_audio_layer = 32\nwhisper_model_load:\
          \ n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 1280\nwhisper_model_load:\
          \ n_text_head   = 20\nwhisper_model_load: n_text_layer  = 32\nwhisper_model_load:\
          \ n_mels        = 128\nwhisper_model_load: ftype         = 1\nwhisper_model_load:\
          \ qntvr         = 0\nwhisper_model_load: type          = 5 (large v3)\n\
          whisper_model_load: adding 1609 extra tokens\nwhisper_model_load: n_langs\
          \       = 100\nwhisper_backend_init: using Metal backend\nggml_metal_init:\
          \ allocating\nggml_metal_init: found device: Apple M2\nggml_metal_init:\
          \ picking default device: Apple M2\nggml_metal_init: default.metallib not\
          \ found, loading from source\nggml_metal_init: loading '/Users/solaoi/Projects/solaoi/whisper.cpp/ggml-metal.metal'\n\
          ggml_metal_init: GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8\
          \ (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
          \ recommendedMaxWorkingSetSize  = 17179.89 MB\nggml_metal_init: maxTransferRate\
          \               = built-in GPU\nggml_metal_add_buffer: allocated 'backend\
          \         ' buffer, size =  3117.88 MB, ( 3118.40 / 17179.89)\nwhisper_model_load:\
          \    Metal buffer size =  3117.87 MB\nwhisper_model_load: model size   \
          \ = 3117.39 MB\nwhisper_backend_init: using Metal backend\nggml_metal_init:\
          \ allocating\nggml_metal_init: found device: Apple M2\nggml_metal_init:\
          \ picking default device: Apple M2\nggml_metal_init: default.metallib not\
          \ found, loading from source\nggml_metal_init: loading '/Users/solaoi/Projects/solaoi/whisper.cpp/ggml-metal.metal'\n\
          ggml_metal_init: GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8\
          \ (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
          \ recommendedMaxWorkingSetSize  = 17179.89 MB\nggml_metal_init: maxTransferRate\
          \               = built-in GPU\nggml_metal_add_buffer: allocated 'backend\
          \         ' buffer, size =   220.20 MB, ( 3338.60 / 17179.89)\nwhisper_init_state:\
          \ kv self size  =  220.20 MB\nggml_metal_add_buffer: allocated 'backend\
          \         ' buffer, size =   245.76 MB, ( 3584.36 / 17179.89)\nwhisper_init_state:\
          \ kv cross size =  245.76 MB\nwhisper_init_state: loading Core ML model\
          \ from 'models/ggml-large-v3-encoder.mlmodelc'\nwhisper_init_state: first\
          \ run on a device may take a while ...\nwhisper_init_state: Core ML model\
          \ loaded\nggml_metal_add_buffer: allocated 'backend         ' buffer, size\
          \ =     0.02 MB, ( 3593.88 / 17179.89)\nwhisper_init_state: compute buffer\
          \ (conv)   =   10.85 MB\nggml_metal_add_buffer: allocated 'backend     \
          \    ' buffer, size =     0.02 MB, ( 3593.90 / 17179.89)\nwhisper_init_state:\
          \ compute buffer (cross)  =    9.32 MB\nggml_metal_add_buffer: allocated\
          \ 'backend         ' buffer, size =     0.02 MB, ( 3593.91 / 17179.89)\n\
          whisper_init_state: compute buffer (decode) =   99.17 MB\nggml_metal_add_buffer:\
          \ allocated 'backend         ' buffer, size =     9.22 MB, ( 3603.14 / 17179.89)\n\
          ggml_metal_add_buffer: allocated 'backend         ' buffer, size =     7.68\
          \ MB, ( 3610.82 / 17179.89)\nggml_metal_add_buffer: allocated 'backend \
          \        ' buffer, size =    97.53 MB, ( 3708.35 / 17179.89)\n\nsystem_info:\
          \ n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON =\
          \ 1 | ARM_FMA = 1 | METAL = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 |\
          \ BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | CUDA = 0 | COREML = 1 | OPENVINO\
          \ = 0 | \n\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec),\
          \ 4 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe,\
          \ timestamps = 1 ...\n\n\n[00:00:00.300 --> 00:00:09.000]   And so, my fellow\
          \ Americans, ask not what your country can do for you, ask what you\n[00:00:09.000\
          \ --> 00:00:11.000]   can do for your country.\n\n\nwhisper_print_timings:\
          \     load time =   984.30 ms\nwhisper_print_timings:     fallbacks =  \
          \ 0 p /   0 h\nwhisper_print_timings:      mel time =     7.56 ms\nwhisper_print_timings:\
          \   sample time =    81.95 ms /   148 runs (    0.55 ms per run)\nwhisper_print_timings:\
          \   encode time =  1999.00 ms /     1 runs ( 1999.00 ms per run)\nwhisper_print_timings:\
          \   decode time =     0.00 ms /     1 runs (    0.00 ms per run)\nwhisper_print_timings:\
          \   batchd time =  1377.15 ms /   146 runs (    9.43 ms per run)\nwhisper_print_timings:\
          \   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\nwhisper_print_timings:\
          \    total time =  5798.28 ms\nggml_metal_free: deallocating\nggml_metal_free:\
          \ deallocating\n```"
        updatedAt: '2023-11-25T07:14:45.707Z'
      numEdits: 0
      reactions: []
    id: 65619ee57065690d7f295b93
    type: comment
  author: solaoi
  content: "I have uploaded the updated large-v3 CoreML model.\nhttps://github.com/ggerganov/whisper.cpp/issues/1437#issuecomment-1807608344\n\
    \nIt seems to be working properly according to the operation check logs.\n> whisper_init_state:\
    \ loading Core ML model from 'models/ggml-large-v3-encoder.mlmodelc'\n```\n./main\
    \ -m models/ggml-large-v3.bin -f samples/jfk.wav\nwhisper_init_from_file_with_params_no_state:\
    \ loading model from 'models/ggml-large-v3.bin'\nwhisper_model_load: loading model\n\
    whisper_model_load: n_vocab       = 51866\nwhisper_model_load: n_audio_ctx   =\
    \ 1500\nwhisper_model_load: n_audio_state = 1280\nwhisper_model_load: n_audio_head\
    \  = 20\nwhisper_model_load: n_audio_layer = 32\nwhisper_model_load: n_text_ctx\
    \    = 448\nwhisper_model_load: n_text_state  = 1280\nwhisper_model_load: n_text_head\
    \   = 20\nwhisper_model_load: n_text_layer  = 32\nwhisper_model_load: n_mels \
    \       = 128\nwhisper_model_load: ftype         = 1\nwhisper_model_load: qntvr\
    \         = 0\nwhisper_model_load: type          = 5 (large v3)\nwhisper_model_load:\
    \ adding 1609 extra tokens\nwhisper_model_load: n_langs       = 100\nwhisper_backend_init:\
    \ using Metal backend\nggml_metal_init: allocating\nggml_metal_init: found device:\
    \ Apple M2\nggml_metal_init: picking default device: Apple M2\nggml_metal_init:\
    \ default.metallib not found, loading from source\nggml_metal_init: loading '/Users/solaoi/Projects/solaoi/whisper.cpp/ggml-metal.metal'\n\
    ggml_metal_init: GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8\
    \ (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
    \ recommendedMaxWorkingSetSize  = 17179.89 MB\nggml_metal_init: maxTransferRate\
    \               = built-in GPU\nggml_metal_add_buffer: allocated 'backend    \
    \     ' buffer, size =  3117.88 MB, ( 3118.40 / 17179.89)\nwhisper_model_load:\
    \    Metal buffer size =  3117.87 MB\nwhisper_model_load: model size    = 3117.39\
    \ MB\nwhisper_backend_init: using Metal backend\nggml_metal_init: allocating\n\
    ggml_metal_init: found device: Apple M2\nggml_metal_init: picking default device:\
    \ Apple M2\nggml_metal_init: default.metallib not found, loading from source\n\
    ggml_metal_init: loading '/Users/solaoi/Projects/solaoi/whisper.cpp/ggml-metal.metal'\n\
    ggml_metal_init: GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8\
    \ (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
    \ recommendedMaxWorkingSetSize  = 17179.89 MB\nggml_metal_init: maxTransferRate\
    \               = built-in GPU\nggml_metal_add_buffer: allocated 'backend    \
    \     ' buffer, size =   220.20 MB, ( 3338.60 / 17179.89)\nwhisper_init_state:\
    \ kv self size  =  220.20 MB\nggml_metal_add_buffer: allocated 'backend      \
    \   ' buffer, size =   245.76 MB, ( 3584.36 / 17179.89)\nwhisper_init_state: kv\
    \ cross size =  245.76 MB\nwhisper_init_state: loading Core ML model from 'models/ggml-large-v3-encoder.mlmodelc'\n\
    whisper_init_state: first run on a device may take a while ...\nwhisper_init_state:\
    \ Core ML model loaded\nggml_metal_add_buffer: allocated 'backend         ' buffer,\
    \ size =     0.02 MB, ( 3593.88 / 17179.89)\nwhisper_init_state: compute buffer\
    \ (conv)   =   10.85 MB\nggml_metal_add_buffer: allocated 'backend         ' buffer,\
    \ size =     0.02 MB, ( 3593.90 / 17179.89)\nwhisper_init_state: compute buffer\
    \ (cross)  =    9.32 MB\nggml_metal_add_buffer: allocated 'backend         ' buffer,\
    \ size =     0.02 MB, ( 3593.91 / 17179.89)\nwhisper_init_state: compute buffer\
    \ (decode) =   99.17 MB\nggml_metal_add_buffer: allocated 'backend         ' buffer,\
    \ size =     9.22 MB, ( 3603.14 / 17179.89)\nggml_metal_add_buffer: allocated\
    \ 'backend         ' buffer, size =     7.68 MB, ( 3610.82 / 17179.89)\nggml_metal_add_buffer:\
    \ allocated 'backend         ' buffer, size =    97.53 MB, ( 3708.35 / 17179.89)\n\
    \nsystem_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 |\
    \ NEON = 1 | ARM_FMA = 1 | METAL = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0\
    \ | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | CUDA = 0 | COREML = 1 | OPENVINO\
    \ = 0 | \n\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads,\
    \ 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps\
    \ = 1 ...\n\n\n[00:00:00.300 --> 00:00:09.000]   And so, my fellow Americans,\
    \ ask not what your country can do for you, ask what you\n[00:00:09.000 --> 00:00:11.000]\
    \   can do for your country.\n\n\nwhisper_print_timings:     load time =   984.30\
    \ ms\nwhisper_print_timings:     fallbacks =   0 p /   0 h\nwhisper_print_timings:\
    \      mel time =     7.56 ms\nwhisper_print_timings:   sample time =    81.95\
    \ ms /   148 runs (    0.55 ms per run)\nwhisper_print_timings:   encode time\
    \ =  1999.00 ms /     1 runs ( 1999.00 ms per run)\nwhisper_print_timings:   decode\
    \ time =     0.00 ms /     1 runs (    0.00 ms per run)\nwhisper_print_timings:\
    \   batchd time =  1377.15 ms /   146 runs (    9.43 ms per run)\nwhisper_print_timings:\
    \   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\nwhisper_print_timings:\
    \    total time =  5798.28 ms\nggml_metal_free: deallocating\nggml_metal_free:\
    \ deallocating\n```"
  created_at: 2023-11-25 07:14:45+00:00
  edited: false
  hidden: false
  id: 65619ee57065690d7f295b93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/a0e805cef7927e6f29856ab81dd15332.svg
      fullname: Kodai Aoyama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: solaoi
      type: user
    createdAt: '2023-11-25T07:14:46.000Z'
    data:
      oid: b5cab3558fada0be4a1fde9312cc9e8dca034759
      parents:
      - 362722b3fdcd2300b58a8286933ead1c48619667
      subject: Include compressed versions of the CoreML versions of large-v3 model.
    id: 65619ee60000000000000000
    type: commit
  author: solaoi
  created_at: 2023-11-25 07:14:46+00:00
  id: 65619ee60000000000000000
  oid: b5cab3558fada0be4a1fde9312cc9e8dca034759
  summary: Include compressed versions of the CoreML versions of large-v3 model.
  type: commit
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/a0e805cef7927e6f29856ab81dd15332.svg
      fullname: Kodai Aoyama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: solaoi
      type: user
    createdAt: '2023-11-25T07:26:32.000Z'
    data:
      from: Include compressed versions of the CoreML versions of large-v3 model.
      to: Include compressed versions of the CoreML version of large-v3 model.
    id: 6561a1a85b395bcbf173860a
    type: title-change
  author: solaoi
  created_at: 2023-11-25 07:26:32+00:00
  id: 6561a1a85b395bcbf173860a
  new_title: Include compressed versions of the CoreML version of large-v3 model.
  old_title: Include compressed versions of the CoreML versions of large-v3 model.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/a0e805cef7927e6f29856ab81dd15332.svg
      fullname: Kodai Aoyama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: solaoi
      type: user
    createdAt: '2023-11-25T07:27:02.000Z'
    data:
      from: Include compressed versions of the CoreML version of large-v3 model.
      to: Include compressed version of the CoreML version of large-v3 model.
    id: 6561a1c679911cb9fa86db4d
    type: title-change
  author: solaoi
  created_at: 2023-11-25 07:27:02+00:00
  id: 6561a1c679911cb9fa86db4d
  new_title: Include compressed version of the CoreML version of large-v3 model.
  old_title: Include compressed versions of the CoreML version of large-v3 model.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/625a6cd91acd8921c72ed02eaa78c746.svg
      fullname: Georgi Gerganov
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ggerganov
      type: user
    createdAt: '2023-11-25T09:00:05.000Z'
    data:
      status: merged
    id: 6561b79513386101848bf2a3
    type: status-change
  author: ggerganov
  created_at: 2023-11-25 09:00:05+00:00
  id: 6561b79513386101848bf2a3
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 13ce9e800d8d6aeab537db9f6d1aa62d3e540c4c
num: 14
repo_id: ggerganov/whisper.cpp
repo_type: model
status: merged
target_branch: refs/heads/main
title: Include compressed version of the CoreML version of large-v3 model.
