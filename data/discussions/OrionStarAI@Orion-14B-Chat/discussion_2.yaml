!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kiranr
conflicting_files: null
created_at: 2024-01-22 17:03:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56ccef10a8426d7160ef3586a771bd63.svg
      fullname: Kiran Kamble
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kiranr
      type: user
    createdAt: '2024-01-22T17:03:43.000Z'
    data:
      edited: false
      editors:
      - kiranr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8974564075469971
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56ccef10a8426d7160ef3586a771bd63.svg
          fullname: Kiran Kamble
          isHf: false
          isPro: false
          name: kiranr
          type: user
        html: '<p>Can you provide the weights in llama format? Or maybe a script to
          do the conversion ourselves?</p>

          '
        raw: Can you provide the weights in llama format? Or maybe a script to do
          the conversion ourselves?
        updatedAt: '2024-01-22T17:03:43.460Z'
      numEdits: 0
      reactions: []
    id: 65ae9fef0214b35f1bd5f8b1
    type: comment
  author: kiranr
  content: Can you provide the weights in llama format? Or maybe a script to do the
    conversion ourselves?
  created_at: 2024-01-22 17:03:43+00:00
  edited: false
  hidden: false
  id: 65ae9fef0214b35f1bd5f8b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c58c9bcedd54d67835cdfa4baba9778.svg
      fullname: lee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sharp
      type: user
    createdAt: '2024-01-24T05:09:23.000Z'
    data:
      edited: true
      editors:
      - sharp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7760888934135437
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c58c9bcedd54d67835cdfa4baba9778.svg
          fullname: lee
          isHf: false
          isPro: false
          name: sharp
          type: user
        html: '<blockquote>

          <p>Can you provide the weights in llama format? Or maybe a script to do
          the conversion ourselves?</p>

          </blockquote>

          <p>Hey bro, the Orion-14B models are slightly different with llama, so we
          could not provide the weights in llama format.<br>In fact, you can just
          download the Orion-14B-  (/path/Orion-14B-Chat for example) and change the
          model-loading way like bellow</p>

          <p>from<br><code>   model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
          **kwargs)</code></p>

          <p>to<br><code>  model = AutoModelForCausalLM.from_pretrained("/path/Orion-14B-Chat",  trust_remote_code=True,
          **kwargs)</code></p>

          <p>Just have a try and enjoy!</p>

          '
        raw: "> Can you provide the weights in llama format? Or maybe a script to\
          \ do the conversion ourselves?\n\nHey bro, the Orion-14B models are slightly\
          \ different with llama, so we could not provide the weights in llama format.\n\
          In fact, you can just download the Orion-14B-  (/path/Orion-14B-Chat for\
          \ example) and change the model-loading way like bellow\n\nfrom\n`\n   model\
          \ = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"\
          , **kwargs)\n`\n \nto\n`\n  model = AutoModelForCausalLM.from_pretrained(\"\
          /path/Orion-14B-Chat\",  trust_remote_code=True, **kwargs)\n`\n   \nJust\
          \ have a try and enjoy!\n\n"
        updatedAt: '2024-01-24T05:10:23.635Z'
      numEdits: 1
      reactions: []
    id: 65b09b837c11edbf6e522108
    type: comment
  author: sharp
  content: "> Can you provide the weights in llama format? Or maybe a script to do\
    \ the conversion ourselves?\n\nHey bro, the Orion-14B models are slightly different\
    \ with llama, so we could not provide the weights in llama format.\nIn fact, you\
    \ can just download the Orion-14B-  (/path/Orion-14B-Chat for example) and change\
    \ the model-loading way like bellow\n\nfrom\n`\n   model = AutoModelForCausalLM.from_pretrained(\"\
    meta-llama/Llama-2-7b-chat-hf\", **kwargs)\n`\n \nto\n`\n  model = AutoModelForCausalLM.from_pretrained(\"\
    /path/Orion-14B-Chat\",  trust_remote_code=True, **kwargs)\n`\n   \nJust have\
    \ a try and enjoy!\n\n"
  created_at: 2024-01-24 05:09:23+00:00
  edited: true
  hidden: false
  id: 65b09b837c11edbf6e522108
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56ccef10a8426d7160ef3586a771bd63.svg
      fullname: Kiran Kamble
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kiranr
      type: user
    createdAt: '2024-01-24T17:58:55.000Z'
    data:
      edited: false
      editors:
      - kiranr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8628447651863098
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56ccef10a8426d7160ef3586a771bd63.svg
          fullname: Kiran Kamble
          isHf: false
          isPro: false
          name: kiranr
          type: user
        html: '<p>I want to use <code>LlamaForCausalLM</code> to load up the model,
          instead of <code>OrionForCausalLM</code> which requires an additional step
          of using <code>trust_remote_code=True</code>. I also plan to run the model
          using  vllm. By transferring the model weights into <code>LlamaForCausalLM</code>,
          I can make it work smoothly with vllm.</p>

          '
        raw: I want to use `LlamaForCausalLM` to load up the model, instead of `OrionForCausalLM`
          which requires an additional step of using `trust_remote_code=True`. I also
          plan to run the model using  vllm. By transferring the model weights into
          `LlamaForCausalLM`, I can make it work smoothly with vllm.
        updatedAt: '2024-01-24T17:58:55.739Z'
      numEdits: 0
      reactions: []
    id: 65b14fdf3e6dd574ece21f66
    type: comment
  author: kiranr
  content: I want to use `LlamaForCausalLM` to load up the model, instead of `OrionForCausalLM`
    which requires an additional step of using `trust_remote_code=True`. I also plan
    to run the model using  vllm. By transferring the model weights into `LlamaForCausalLM`,
    I can make it work smoothly with vllm.
  created_at: 2024-01-24 17:58:55+00:00
  edited: false
  hidden: false
  id: 65b14fdf3e6dd574ece21f66
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OrionStarAI/Orion-14B-Chat
repo_type: model
status: open
target_branch: null
title: weights in llama format
