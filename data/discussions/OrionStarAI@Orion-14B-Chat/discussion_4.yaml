!!python/object:huggingface_hub.community.DiscussionWithDetails
author: J22
conflicting_files: null
created_at: 2024-01-24 13:43:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: J22
      type: user
    createdAt: '2024-01-24T13:43:46.000Z'
    data:
      edited: false
      editors:
      - J22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9149215221405029
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: J22
          type: user
        html: "<pre><code class=\"language-python\">        <span class=\"hljs-comment\"\
          ># In PEFT, usually we cast the layer norms in float32 for training stability\
          \ reasons</span>\n        <span class=\"hljs-comment\"># therefore the input\
          \ hidden states gets silently casted in float32. Hence, we need</span>\n\
          \        <span class=\"hljs-comment\"># cast them back in float16 just to\
          \ be sure everything works as expected.</span>\n        <span class=\"hljs-comment\"\
          ># This might slowdown training &amp; inference so it is recommended to\
          \ not cast the LayerNorms</span>\n        <span class=\"hljs-comment\">#\
          \ in fp32. (LlamaRMSNorm handles it correctly)</span>\n</code></pre>\n"
        raw: "```python\r\n        # In PEFT, usually we cast the layer norms in float32\
          \ for training stability reasons\r\n        # therefore the input hidden\
          \ states gets silently casted in float32. Hence, we need\r\n        # cast\
          \ them back in float16 just to be sure everything works as expected.\r\n\
          \        # This might slowdown training & inference so it is recommended\
          \ to not cast the LayerNorms\r\n        # in fp32. (LlamaRMSNorm handles\
          \ it correctly)\r\n```"
        updatedAt: '2024-01-24T13:43:46.914Z'
      numEdits: 0
      reactions: []
    id: 65b11412f346fb4c5de0d505
    type: comment
  author: J22
  content: "```python\r\n        # In PEFT, usually we cast the layer norms in float32\
    \ for training stability reasons\r\n        # therefore the input hidden states\
    \ gets silently casted in float32. Hence, we need\r\n        # cast them back\
    \ in float16 just to be sure everything works as expected.\r\n        # This might\
    \ slowdown training & inference so it is recommended to not cast the LayerNorms\r\
    \n        # in fp32. (LlamaRMSNorm handles it correctly)\r\n```"
  created_at: 2024-01-24 13:43:46+00:00
  edited: false
  hidden: false
  id: 65b11412f346fb4c5de0d505
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: OrionStarAI/Orion-14B-Chat
repo_type: model
status: open
target_branch: null
title: some text are not renamed to Orion
