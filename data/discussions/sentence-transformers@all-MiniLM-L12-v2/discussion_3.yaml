!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ColinKhan
conflicting_files: null
created_at: 2023-06-16 06:54:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/816eef60f097f91534556c729321136b.svg
      fullname: Colin Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ColinKhan
      type: user
    createdAt: '2023-06-16T07:54:03.000Z'
    data:
      edited: false
      editors:
      - ColinKhan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611653685569763
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/816eef60f097f91534556c729321136b.svg
          fullname: Colin Zhang
          isHf: false
          isPro: false
          name: ColinKhan
          type: user
        html: '<p>Hi!<br>I''ve been using this model trying to perform vector search.<br>Recently
          i notice the default max sequence length of it is 128, while on the page
          it says max sequence length is 256.<br>However on the lower part of the
          page, it says the model was trained with 128 token length.<br>So i''m not
          sure if it''s ok to increase token length to 256,  will this decrease the
          quality of vector because hyper parameters were trained with 128 token length?<br>And
          since it''s in sentence-transformer library, max_sequence_length can even
          be set as 512. Can I also do this for this model?</p>

          <p>Thanks!</p>

          '
        raw: "Hi!\r\nI've been using this model trying to perform vector search. \r\
          \nRecently i notice the default max sequence length of it is 128, while\
          \ on the page it says max sequence length is 256. \r\nHowever on the lower\
          \ part of the page, it says the model was trained with 128 token length.\
          \ \r\nSo i'm not sure if it's ok to increase token length to 256,  will\
          \ this decrease the quality of vector because hyper parameters were trained\
          \ with 128 token length?\r\nAnd since it's in sentence-transformer library,\
          \ max_sequence_length can even be set as 512. Can I also do this for this\
          \ model?\r\n\r\nThanks!"
        updatedAt: '2023-06-16T07:54:03.922Z'
      numEdits: 0
      reactions: []
    id: 648c151bbf712e7d779d612a
    type: comment
  author: ColinKhan
  content: "Hi!\r\nI've been using this model trying to perform vector search. \r\n\
    Recently i notice the default max sequence length of it is 128, while on the page\
    \ it says max sequence length is 256. \r\nHowever on the lower part of the page,\
    \ it says the model was trained with 128 token length. \r\nSo i'm not sure if\
    \ it's ok to increase token length to 256,  will this decrease the quality of\
    \ vector because hyper parameters were trained with 128 token length?\r\nAnd since\
    \ it's in sentence-transformer library, max_sequence_length can even be set as\
    \ 512. Can I also do this for this model?\r\n\r\nThanks!"
  created_at: 2023-06-16 06:54:03+00:00
  edited: false
  hidden: false
  id: 648c151bbf712e7d779d612a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: sentence-transformers/all-MiniLM-L12-v2
repo_type: model
status: open
target_branch: null
title: max token length question
