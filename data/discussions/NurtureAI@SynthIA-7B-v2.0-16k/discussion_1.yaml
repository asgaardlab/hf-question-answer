!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dyoung
conflicting_files: null
created_at: 2023-11-16 23:11:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-11-16T23:11:12.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9728532433509827
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>Hello,</p>

          <p>I was looking through the model card and in the quick use code example
          I noticed in the generate_text function something that caught my curiosity.<br>After
          the tokenization of the input prompt intended to be going to the model in
          GPU("tokens = tokenizer.encode(instruction)"),  the tokens are recast as
          longTensors (64-bit signed interreges tensors recast at "tokens = torch.LongTensor(tokens).unsqueeze(0)").<br>I''ve
          not seen a lot of others doing this with what I''ve seen so far in my Ai
          journey. I was curious as to what the reasoning why. I can speculate several
          reasons why. I figure it wouldn''t hurt if I ask directly. I''ll also be
          looking online. As well as checking with migtissera who posted the non 16k
          version of the model. (I''ve noticed the recast was done in that models
          card as well.)<br>If you can, could you point me at any material I can look
          at that further supports why it''s smart to recast a tensor before sending
          off to the GPU, that would be appreciated. If you can''t or do not want
          to, that is understandable.</p>

          <p>Thank you for your time.</p>

          '
        raw: "Hello,\r\n\r\nI was looking through the model card and in the quick\
          \ use code example I noticed in the generate_text function something that\
          \ caught my curiosity.\r\nAfter the tokenization of the input prompt intended\
          \ to be going to the model in GPU(\"tokens = tokenizer.encode(instruction)\"\
          ),  the tokens are recast as longTensors (64-bit signed interreges tensors\
          \ recast at \"tokens = torch.LongTensor(tokens).unsqueeze(0)\").\r\nI've\
          \ not seen a lot of others doing this with what I've seen so far in my Ai\
          \ journey. I was curious as to what the reasoning why. I can speculate several\
          \ reasons why. I figure it wouldn't hurt if I ask directly. I'll also be\
          \ looking online. As well as checking with migtissera who posted the non\
          \ 16k version of the model. (I've noticed the recast was done in that models\
          \ card as well.)\r\nIf you can, could you point me at any material I can\
          \ look at that further supports why it's smart to recast a tensor before\
          \ sending off to the GPU, that would be appreciated. If you can't or do\
          \ not want to, that is understandable.\r\n\r\nThank you for your time."
        updatedAt: '2023-11-16T23:11:12.856Z'
      numEdits: 0
      reactions: []
    id: 6556a1909dfd3ef87340ec7b
    type: comment
  author: dyoung
  content: "Hello,\r\n\r\nI was looking through the model card and in the quick use\
    \ code example I noticed in the generate_text function something that caught my\
    \ curiosity.\r\nAfter the tokenization of the input prompt intended to be going\
    \ to the model in GPU(\"tokens = tokenizer.encode(instruction)\"),  the tokens\
    \ are recast as longTensors (64-bit signed interreges tensors recast at \"tokens\
    \ = torch.LongTensor(tokens).unsqueeze(0)\").\r\nI've not seen a lot of others\
    \ doing this with what I've seen so far in my Ai journey. I was curious as to\
    \ what the reasoning why. I can speculate several reasons why. I figure it wouldn't\
    \ hurt if I ask directly. I'll also be looking online. As well as checking with\
    \ migtissera who posted the non 16k version of the model. (I've noticed the recast\
    \ was done in that models card as well.)\r\nIf you can, could you point me at\
    \ any material I can look at that further supports why it's smart to recast a\
    \ tensor before sending off to the GPU, that would be appreciated. If you can't\
    \ or do not want to, that is understandable.\r\n\r\nThank you for your time."
  created_at: 2023-11-16 23:11:12+00:00
  edited: false
  hidden: false
  id: 6556a1909dfd3ef87340ec7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-11-16T23:34:42.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40159857273101807
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p> <a href="https://huggingface.co/migtissera/SynthIA-7B-v2.0/discussions/1#6556a1922de0aee1a11119b7">https://huggingface.co/migtissera/SynthIA-7B-v2.0/discussions/1#6556a1922de0aee1a11119b7</a></p>

          '
        raw: ' https://huggingface.co/migtissera/SynthIA-7B-v2.0/discussions/1#6556a1922de0aee1a11119b7'
        updatedAt: '2023-11-16T23:34:42.200Z'
      numEdits: 0
      reactions: []
    id: 6556a71255c514dc591cbfde
    type: comment
  author: dyoung
  content: ' https://huggingface.co/migtissera/SynthIA-7B-v2.0/discussions/1#6556a1922de0aee1a11119b7'
  created_at: 2023-11-16 23:34:42+00:00
  edited: false
  hidden: false
  id: 6556a71255c514dc591cbfde
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NurtureAI/SynthIA-7B-v2.0-16k
repo_type: model
status: open
target_branch: null
title: torch.longTensor recast
