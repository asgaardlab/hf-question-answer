!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Q4234
conflicting_files: null
created_at: 2023-07-26 07:59:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ef64ce50bba82dcc1c64e88ab0d1a17.svg
      fullname: Q4234
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Q4234
      type: user
    createdAt: '2023-07-26T08:59:01.000Z'
    data:
      edited: false
      editors:
      - Q4234
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4513942301273346
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ef64ce50bba82dcc1c64e88ab0d1a17.svg
          fullname: Q4234
          isHf: false
          isPro: false
          name: Q4234
          type: user
        html: '<p>File /usr/local/lib/python3.9/dist-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File /usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py:300,
          in LlamaAttention.forward(self, hidden_states, attention_mask, position_ids,
          past_key_value, output_attentions, use_cache)<br>    297 key_slices = self.k_proj.weight.split(key_value_slicing,
          dim=0)<br>    298 value_slices = self.v_proj.weight.split(key_value_slicing,
          dim=0)<br>--&gt; 300 query_states = [F.linear(hidden_states, query_slices[i])
          for i in range(self.config.pretraining_tp)]<br>    301 query_states = torch.cat(query_states,
          dim=-1)<br>    303 key_states = [F.linear(hidden_states, key_slices[i])
          for i in range(self.config.pretraining_tp)]</p>

          <p>File /usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py:300,
          in (.0)<br>    297 key_slices = self.k_proj.weight.split(key_value_slicing,
          dim=0)<br>    298 value_slices = self.v_proj.weight.split(key_value_slicing,
          dim=0)<br>--&gt; 300 query_states = [F.linear(hidden_states, query_slices[i])
          for i in range(self.config.pretraining_tp)]<br>    301 query_states = torch.cat(query_states,
          dim=-1)<br>    303 key_states = [F.linear(hidden_states, key_slices[i])
          for i in range(self.config.pretraining_tp)]</p>

          <p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (69x5120 and
          1x2560)</p>

          '
        raw: "\r\nFile /usr/local/lib/python3.9/dist-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    163\
          \         output = old_forward(*args, **kwargs)\r\n    164 else:\r\n-->\
          \ 165     output = old_forward(*args, **kwargs)\r\n    166 return module._hf_hook.post_forward(module,\
          \ output)\r\n\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py:300,\
          \ in LlamaAttention.forward(self, hidden_states, attention_mask, position_ids,\
          \ past_key_value, output_attentions, use_cache)\r\n    297 key_slices =\
          \ self.k_proj.weight.split(key_value_slicing, dim=0)\r\n    298 value_slices\
          \ = self.v_proj.weight.split(key_value_slicing, dim=0)\r\n--> 300 query_states\
          \ = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\r\
          \n    301 query_states = torch.cat(query_states, dim=-1)\r\n    303 key_states\
          \ = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\r\
          \n\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py:300,\
          \ in <listcomp>(.0)\r\n    297 key_slices = self.k_proj.weight.split(key_value_slicing,\
          \ dim=0)\r\n    298 value_slices = self.v_proj.weight.split(key_value_slicing,\
          \ dim=0)\r\n--> 300 query_states = [F.linear(hidden_states, query_slices[i])\
          \ for i in range(self.config.pretraining_tp)]\r\n    301 query_states =\
          \ torch.cat(query_states, dim=-1)\r\n    303 key_states = [F.linear(hidden_states,\
          \ key_slices[i]) for i in range(self.config.pretraining_tp)]\r\n\r\nRuntimeError:\
          \ mat1 and mat2 shapes cannot be multiplied (69x5120 and 1x2560)"
        updatedAt: '2023-07-26T08:59:01.975Z'
      numEdits: 0
      reactions: []
    id: 64c0e055e175dd56a57620ac
    type: comment
  author: Q4234
  content: "\r\nFile /usr/local/lib/python3.9/dist-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    163     \
    \    output = old_forward(*args, **kwargs)\r\n    164 else:\r\n--> 165     output\
    \ = old_forward(*args, **kwargs)\r\n    166 return module._hf_hook.post_forward(module,\
    \ output)\r\n\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py:300,\
    \ in LlamaAttention.forward(self, hidden_states, attention_mask, position_ids,\
    \ past_key_value, output_attentions, use_cache)\r\n    297 key_slices = self.k_proj.weight.split(key_value_slicing,\
    \ dim=0)\r\n    298 value_slices = self.v_proj.weight.split(key_value_slicing,\
    \ dim=0)\r\n--> 300 query_states = [F.linear(hidden_states, query_slices[i]) for\
    \ i in range(self.config.pretraining_tp)]\r\n    301 query_states = torch.cat(query_states,\
    \ dim=-1)\r\n    303 key_states = [F.linear(hidden_states, key_slices[i]) for\
    \ i in range(self.config.pretraining_tp)]\r\n\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py:300,\
    \ in <listcomp>(.0)\r\n    297 key_slices = self.k_proj.weight.split(key_value_slicing,\
    \ dim=0)\r\n    298 value_slices = self.v_proj.weight.split(key_value_slicing,\
    \ dim=0)\r\n--> 300 query_states = [F.linear(hidden_states, query_slices[i]) for\
    \ i in range(self.config.pretraining_tp)]\r\n    301 query_states = torch.cat(query_states,\
    \ dim=-1)\r\n    303 key_states = [F.linear(hidden_states, key_slices[i]) for\
    \ i in range(self.config.pretraining_tp)]\r\n\r\nRuntimeError: mat1 and mat2 shapes\
    \ cannot be multiplied (69x5120 and 1x2560)"
  created_at: 2023-07-26 07:59:01+00:00
  edited: false
  hidden: false
  id: 64c0e055e175dd56a57620ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6314285f2c7ffdd9f50564f9/opyywBUbsF-yOpw75V5Ko.jpeg?w=200&h=200&f=face
      fullname: NightWolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nigh8w0lf
      type: user
    createdAt: '2023-07-26T09:12:41.000Z'
    data:
      edited: false
      editors:
      - nigh8w0lf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37780359387397766
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6314285f2c7ffdd9f50564f9/opyywBUbsF-yOpw75V5Ko.jpeg?w=200&h=200&f=face
          fullname: NightWolf
          isHf: false
          isPro: false
          name: nigh8w0lf
          type: user
        html: '<p>Got the same shape mismatch error , using latest transformers&gt;=4.31.0</p>

          '
        raw: 'Got the same shape mismatch error , using latest transformers>=4.31.0

          '
        updatedAt: '2023-07-26T09:12:41.906Z'
      numEdits: 0
      reactions: []
    id: 64c0e389abe5f855b7a569b4
    type: comment
  author: nigh8w0lf
  content: 'Got the same shape mismatch error , using latest transformers>=4.31.0

    '
  created_at: 2023-07-26 08:12:41+00:00
  edited: false
  hidden: false
  id: 64c0e389abe5f855b7a569b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ef64ce50bba82dcc1c64e88ab0d1a17.svg
      fullname: Q4234
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Q4234
      type: user
    createdAt: '2023-07-26T12:50:12.000Z'
    data:
      edited: false
      editors:
      - Q4234
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9262169003486633
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ef64ce50bba82dcc1c64e88ab0d1a17.svg
          fullname: Q4234
          isHf: false
          isPro: false
          name: Q4234
          type: user
        html: '<p>I guess setting the ctxlen is missing in the code ... no idea how
          to do this but the code for SuperCOT/SuperHOT did do this...</p>

          '
        raw: I guess setting the ctxlen is missing in the code ... no idea how to
          do this but the code for SuperCOT/SuperHOT did do this...
        updatedAt: '2023-07-26T12:50:12.190Z'
      numEdits: 0
      reactions: []
    id: 64c11684a2ec8cb2f58f4339
    type: comment
  author: Q4234
  content: I guess setting the ctxlen is missing in the code ... no idea how to do
    this but the code for SuperCOT/SuperHOT did do this...
  created_at: 2023-07-26 11:50:12+00:00
  edited: false
  hidden: false
  id: 64c11684a2ec8cb2f58f4339
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
      fullname: Thorold Tronrud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ttronrud
      type: user
    createdAt: '2023-07-26T16:23:11.000Z'
    data:
      edited: false
      editors:
      - ttronrud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8939615488052368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
          fullname: Thorold Tronrud
          isHf: false
          isPro: false
          name: ttronrud
          type: user
        html: '<p>They need to set pretraining_tp to 1 for it to work with quantized
          models. You can set the value in the local copy of the model''s config and
          that''ll fix the problem.<br><a rel="nofollow" href="https://github.com/facebookresearch/llama/issues/423#issuecomment-1643387661">https://github.com/facebookresearch/llama/issues/423#issuecomment-1643387661</a></p>

          '
        raw: 'They need to set pretraining_tp to 1 for it to work with quantized models.
          You can set the value in the local copy of the model''s config and that''ll
          fix the problem.

          https://github.com/facebookresearch/llama/issues/423#issuecomment-1643387661'
        updatedAt: '2023-07-26T16:23:11.438Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - nigh8w0lf
        - Q4234
    id: 64c1486fc9ed8d21e646f80f
    type: comment
  author: ttronrud
  content: 'They need to set pretraining_tp to 1 for it to work with quantized models.
    You can set the value in the local copy of the model''s config and that''ll fix
    the problem.

    https://github.com/facebookresearch/llama/issues/423#issuecomment-1643387661'
  created_at: 2023-07-26 15:23:11+00:00
  edited: false
  hidden: false
  id: 64c1486fc9ed8d21e646f80f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b888106cf60e8c3d00dc86/lNCiIc424q60PC2D33vxN.jpeg?w=200&h=200&f=face
      fullname: "Andreas K\xF6pf"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: andreaskoepf
      type: user
    createdAt: '2023-07-27T11:06:15.000Z'
    data:
      edited: false
      editors:
      - andreaskoepf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8713731169700623
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b888106cf60e8c3d00dc86/lNCiIc424q60PC2D33vxN.jpeg?w=200&h=200&f=face
          fullname: "Andreas K\xF6pf"
          isHf: false
          isPro: false
          name: andreaskoepf
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ttronrud&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ttronrud\">@<span class=\"\
          underline\">ttronrud</span></a></span>\n\n\t</span></span> thanks for reporting.\
          \ We added pretraining_tp to the config.</p>\n"
        raw: '@ttronrud thanks for reporting. We added pretraining_tp to the config.'
        updatedAt: '2023-07-27T11:06:15.176Z'
      numEdits: 0
      reactions: []
    id: 64c24fa73499e4f3a2fc2c55
    type: comment
  author: andreaskoepf
  content: '@ttronrud thanks for reporting. We added pretraining_tp to the config.'
  created_at: 2023-07-27 10:06:15+00:00
  edited: false
  hidden: false
  id: 64c24fa73499e4f3a2fc2c55
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: OpenAssistant/llama2-13b-orca-8k-3319
repo_type: model
status: open
target_branch: null
title: example code is not working
