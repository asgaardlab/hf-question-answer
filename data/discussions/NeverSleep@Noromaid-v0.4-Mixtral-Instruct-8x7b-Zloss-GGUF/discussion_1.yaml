!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ProphetOfBostrom
conflicting_files: null
created_at: 2024-01-10 04:52:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2024-01-10T04:52:00.000Z'
    data:
      edited: true
      editors:
      - ProphetOfBostrom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9571949243545532
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
          fullname: Albion Perfidia
          isHf: false
          isPro: false
          name: ProphetOfBostrom
          type: user
        html: '<p>What''s the word on k-quants? I never found any mention of an issue
          on the llama.cpp github (or koboldcpp for that matter) - and you''re publishing
          them (Q4_K_M) now.<br>What was the story there? Bug with llama.cpp''s inference
          or bug with the GGUF quantisation in the first place?<br>...was there a
          bug at all?<br>Just being picky because my download speeds are not fast.
          I love the work you guys do and I don''t want some scuffed nibbles* tainting
          your delightful transformers.<br>p.s. did you keep the original output tensors?
          if you did I might get the Q8_0 (isn''t this also technically a k-quant
          now?) and re-quantize it down to Q4_K_S or Q3_K_L. 24 GB sweet spot right?</p>

          <p>maybe I should just bite the bullet and see if I can QUIP# or HQQ one
          of these? I''m pretty confident that those would be an improvement on Q2_K
          at least. Contrastive search from the native Transformers support <em>hugely
          improves RP output quality</em>. Exllama2 and llama.cpp both seem uninterested
          in supporting this sampling mode, which means that AWQ and GPTQ (and BnB)
          are now actually offering better quality generation than the cool kid quants
          (IMO - but try contrastive search if you haven''t!)</p>

          <ul>

          <li><a rel="nofollow" href="https://en.wikipedia.org/wiki/Nibble">https://en.wikipedia.org/wiki/Nibble</a></li>

          </ul>

          '
        raw: "What's the word on k-quants? I never found any mention of an issue on\
          \ the llama.cpp github (or koboldcpp for that matter) - and you're publishing\
          \ them (Q4_K_M) now. \nWhat was the story there? Bug with llama.cpp's inference\
          \ or bug with the GGUF quantisation in the first place? \n...was there a\
          \ bug at all?\nJust being picky because my download speeds are not fast.\
          \ I love the work you guys do and I don't want some scuffed nibbles* tainting\
          \ your delightful transformers.\np.s. did you keep the original output tensors?\
          \ if you did I might get the Q8_0 (isn't this also technically a k-quant\
          \ now?) and re-quantize it down to Q4_K_S or Q3_K_L. 24 GB sweet spot right?\n\
          \nmaybe I should just bite the bullet and see if I can QUIP# or HQQ one\
          \ of these? I'm pretty confident that those would be an improvement on Q2_K\
          \ at least. Contrastive search from the native Transformers support *hugely\
          \ improves RP output quality*. Exllama2 and llama.cpp both seem uninterested\
          \ in supporting this sampling mode, which means that AWQ and GPTQ (and BnB)\
          \ are now actually offering better quality generation than the cool kid\
          \ quants (IMO - but try contrastive search if you haven't!)\n\n* https://en.wikipedia.org/wiki/Nibble"
        updatedAt: '2024-01-10T05:14:13.730Z'
      numEdits: 2
      reactions: []
    id: 659e2270912b75e2c36c2f23
    type: comment
  author: ProphetOfBostrom
  content: "What's the word on k-quants? I never found any mention of an issue on\
    \ the llama.cpp github (or koboldcpp for that matter) - and you're publishing\
    \ them (Q4_K_M) now. \nWhat was the story there? Bug with llama.cpp's inference\
    \ or bug with the GGUF quantisation in the first place? \n...was there a bug at\
    \ all?\nJust being picky because my download speeds are not fast. I love the work\
    \ you guys do and I don't want some scuffed nibbles* tainting your delightful\
    \ transformers.\np.s. did you keep the original output tensors? if you did I might\
    \ get the Q8_0 (isn't this also technically a k-quant now?) and re-quantize it\
    \ down to Q4_K_S or Q3_K_L. 24 GB sweet spot right?\n\nmaybe I should just bite\
    \ the bullet and see if I can QUIP# or HQQ one of these? I'm pretty confident\
    \ that those would be an improvement on Q2_K at least. Contrastive search from\
    \ the native Transformers support *hugely improves RP output quality*. Exllama2\
    \ and llama.cpp both seem uninterested in supporting this sampling mode, which\
    \ means that AWQ and GPTQ (and BnB) are now actually offering better quality generation\
    \ than the cool kid quants (IMO - but try contrastive search if you haven't!)\n\
    \n* https://en.wikipedia.org/wiki/Nibble"
  created_at: 2024-01-10 04:52:00+00:00
  edited: true
  hidden: false
  id: 659e2270912b75e2c36c2f23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Undi95
      type: user
    createdAt: '2024-01-10T15:53:50.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9734955430030823
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Well I''m a little lost, some say it''s fixed, other say it isn''t.<br>For
          me, it''s just that MoE take quantization very badly, so if you want to
          be sure, stay on Q4_0, Q5_0 or Q8_0</p>

          '
        raw: 'Well I''m a little lost, some say it''s fixed, other say it isn''t.

          For me, it''s just that MoE take quantization very badly, so if you want
          to be sure, stay on Q4_0, Q5_0 or Q8_0'
        updatedAt: '2024-01-10T15:53:50.858Z'
      numEdits: 0
      reactions: []
    id: 659ebd8e67f72a688ebddf38
    type: comment
  author: Undi95
  content: 'Well I''m a little lost, some say it''s fixed, other say it isn''t.

    For me, it''s just that MoE take quantization very badly, so if you want to be
    sure, stay on Q4_0, Q5_0 or Q8_0'
  created_at: 2024-01-10 15:53:50+00:00
  edited: false
  hidden: false
  id: 659ebd8e67f72a688ebddf38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2024-01-10T21:15:52.000Z'
    data:
      edited: true
      editors:
      - ProphetOfBostrom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9612647891044617
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
          fullname: Albion Perfidia
          isHf: false
          isPro: false
          name: ProphetOfBostrom
          type: user
        html: '<p>Thanks for getting back to me Undi - what you''re describing has
          in principle been documented with HQQ already (bottom of this reply) and
          now I''m interested.<br>I''m gonna go ahead and pull the full HF model (unlimited
          LTE ain''t quick - but it''s unlimited!) and fiddle with numerous programs
          all called "quantize". Curiosity is currently killing this cat.<br>I''ll
          go ahead and use Noromaid as a surrogate for base mixtral primarily because
          I am controlled not by a living human soul, but by sin and linear rectification.
          </p>

          <p>Hopefully I''ll have something a worthwhile to contribute (perhaps a
          strong 2 bit quant!) But I probably won''t - so if the person reading this
          thinking "noromaid 2 bit HQQ sounds cool" - do it yourself because I''m
          probably not gonna deliver.<br>if I don''t, I may just upload a (properly)
          compressed tar/archive of the .safetensors, they should be super compressible
          and they''ll definitely save someone at least <em>some</em> time. I''ll
          provide checksums because I''m not a silly billy.<br>Needless to say: now
          I''ve made many promises - expect nothing. You will never see me again.
          Goodbye.</p>

          <hr>

          <p>I get that this is probably not much use to you but it''s relevant and
          maybe someone googling this might find it more insightful some day. It''s
          worth following those github links a few levels because there''s clever
          people who''ve actually performed experiments and recorded their results
          like good data scientists (we''re all data scientists here, right, Neversleep?)<br>For
          the 2 bit HQQ quantization of mixtral, the initial all-2-bit (I think? maths
          is hard) quantization was weak, but this change seems to retain a much stronger
          model:</p>

          <blockquote>

          <p>More specifically, the attention layers are quantized to 4-bit and the
          experts are quantized to 2-bit. This model should perform a lot better compared
          to the all 2-bit model for a slight increase in model size (18.2GB vs. 18GB).</p>

          </blockquote>

          <p>as seen:<br><a href="https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ">https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ</a></p>

          '
        raw: "Thanks for getting back to me Undi - what you're describing has in principle\
          \ been documented with HQQ already (bottom of this reply) and now I'm interested.\n\
          I'm gonna go ahead and pull the full HF model (unlimited LTE ain't quick\
          \ - but it's unlimited!) and fiddle with numerous programs all called \"\
          quantize\". Curiosity is currently killing this cat.\nI'll go ahead and\
          \ use Noromaid as a surrogate for base mixtral primarily because I am controlled\
          \ not by a living human soul, but by sin and linear rectification. \n\n\
          Hopefully I'll have something a worthwhile to contribute (perhaps a strong\
          \ 2 bit quant!) But I probably won't - so if the person reading this thinking\
          \ \"noromaid 2 bit HQQ sounds cool\" - do it yourself because I'm probably\
          \ not gonna deliver.\nif I don't, I may just upload a (properly) compressed\
          \ tar/archive of the .safetensors, they should be super compressible and\
          \ they'll definitely save someone at least *some* time. I'll provide checksums\
          \ because I'm not a silly billy.\nNeedless to say: now I've made many promises\
          \ - expect nothing. You will never see me again. Goodbye.\n----\nI get that\
          \ this is probably not much use to you but it's relevant and maybe someone\
          \ googling this might find it more insightful some day. It's worth following\
          \ those github links a few levels because there's clever people who've actually\
          \ performed experiments and recorded their results like good data scientists\
          \ (we're all data scientists here, right, Neversleep?)\nFor the 2 bit HQQ\
          \ quantization of mixtral, the initial all-2-bit (I think? maths is hard)\
          \ quantization was weak, but this change seems to retain a much stronger\
          \ model:\n\n> More specifically, the attention layers are quantized to 4-bit\
          \ and the experts are quantized to 2-bit. This model should perform a lot\
          \ better compared to the all 2-bit model for a slight increase in model\
          \ size (18.2GB vs. 18GB).\n\nas seen:\nhttps://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ\n"
        updatedAt: '2024-01-10T21:45:31.355Z'
      numEdits: 2
      reactions: []
    id: 659f090844a230e92caa62c1
    type: comment
  author: ProphetOfBostrom
  content: "Thanks for getting back to me Undi - what you're describing has in principle\
    \ been documented with HQQ already (bottom of this reply) and now I'm interested.\n\
    I'm gonna go ahead and pull the full HF model (unlimited LTE ain't quick - but\
    \ it's unlimited!) and fiddle with numerous programs all called \"quantize\".\
    \ Curiosity is currently killing this cat.\nI'll go ahead and use Noromaid as\
    \ a surrogate for base mixtral primarily because I am controlled not by a living\
    \ human soul, but by sin and linear rectification. \n\nHopefully I'll have something\
    \ a worthwhile to contribute (perhaps a strong 2 bit quant!) But I probably won't\
    \ - so if the person reading this thinking \"noromaid 2 bit HQQ sounds cool\"\
    \ - do it yourself because I'm probably not gonna deliver.\nif I don't, I may\
    \ just upload a (properly) compressed tar/archive of the .safetensors, they should\
    \ be super compressible and they'll definitely save someone at least *some* time.\
    \ I'll provide checksums because I'm not a silly billy.\nNeedless to say: now\
    \ I've made many promises - expect nothing. You will never see me again. Goodbye.\n\
    ----\nI get that this is probably not much use to you but it's relevant and maybe\
    \ someone googling this might find it more insightful some day. It's worth following\
    \ those github links a few levels because there's clever people who've actually\
    \ performed experiments and recorded their results like good data scientists (we're\
    \ all data scientists here, right, Neversleep?)\nFor the 2 bit HQQ quantization\
    \ of mixtral, the initial all-2-bit (I think? maths is hard) quantization was\
    \ weak, but this change seems to retain a much stronger model:\n\n> More specifically,\
    \ the attention layers are quantized to 4-bit and the experts are quantized to\
    \ 2-bit. This model should perform a lot better compared to the all 2-bit model\
    \ for a slight increase in model size (18.2GB vs. 18GB).\n\nas seen:\nhttps://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ\n"
  created_at: 2024-01-10 21:15:52+00:00
  edited: true
  hidden: false
  id: 659f090844a230e92caa62c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2024-01-10T22:13:53.000Z'
    data:
      edited: true
      editors:
      - ProphetOfBostrom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9118514060974121
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
          fullname: Albion Perfidia
          isHf: false
          isPro: false
          name: ProphetOfBostrom
          type: user
        html: '<p>Low effort to test idea which could be the whole problem. Probably
          not but very quick to test... if you have the quants all ready.<br>What
          kind of issues do you have? Is it just behaving like it''s lost too much
          precision or is it actually malfunctioning and producing complete gibberish?
          The latter might be a heisenbug involving either/both the tokeniser or how
          quantize(.exe, if you insist) may be different when llama.cpp as a whole
          is compiled with additional flags (like cublas) </p>

          <p>A description of the issue you had (or better, just some example bad
          output) would be invaluable to whoever''s gonna figure this out. It probably
          won''t be me but I''ll try.</p>

          <p>Have you tried using one your bad quants with the llamacpp_HF loader
          in texgen_webui? Use your tokeniser files from your merge repo (not the
          stock llama one that tgwui provides) then compare to the llama.cpp loader
          next to it.<br>Pure speculation, but I''ve seen reports of k-quants and
          busted llama.cpp tokenizer output before so worth a few minutes testing
          side-by-side right? Set a fixed seed on load (17, always use 17 for fixed
          seeds. don''t ask, do google.) for the same .gguf and my understanding is
          that they shouldn''t diverge? Certainly not in a way that makes you think
          "broken". If they do, you''ve found a real bug! (I''ve seen some reports
          that a small divergence can happen with the fast tokeniser even for a full
          fat transformer - but it''s not a case of one being more or less correct
          than the other.)<br>I''d do it myself but I''ve committed the next 90gb
          of my life to downloading 16 bit files I intend to delete 14 bits from.</p>

          '
        raw: "Low effort to test idea which could be the whole problem. Probably not\
          \ but very quick to test... if you have the quants all ready.\nWhat kind\
          \ of issues do you have? Is it just behaving like it's lost too much precision\
          \ or is it actually malfunctioning and producing complete gibberish? The\
          \ latter might be a heisenbug involving either/both the tokeniser or how\
          \ quantize(.exe, if you insist) may be different when llama.cpp as a whole\
          \ is compiled with additional flags (like cublas) \n\nA description of the\
          \ issue you had (or better, just some example bad output) would be invaluable\
          \ to whoever's gonna figure this out. It probably won't be me but I'll try.\n\
          \nHave you tried using one your bad quants with the llamacpp_HF loader in\
          \ texgen_webui? Use your tokeniser files from your merge repo (not the stock\
          \ llama one that tgwui provides) then compare to the llama.cpp loader next\
          \ to it. \nPure speculation, but I've seen reports of k-quants and busted\
          \ llama.cpp tokenizer output before so worth a few minutes testing side-by-side\
          \ right? Set a fixed seed on load (17, always use 17 for fixed seeds. don't\
          \ ask, do google.) for the same .gguf and my understanding is that they\
          \ shouldn't diverge? Certainly not in a way that makes you think \"broken\"\
          . If they do, you've found a real bug! (I've seen some reports that a small\
          \ divergence can happen with the fast tokeniser even for a full fat transformer\
          \ - but it's not a case of one being more or less correct than the other.)\n\
          I'd do it myself but I've committed the next 90gb of my life to downloading\
          \ 16 bit files I intend to delete 14 bits from."
        updatedAt: '2024-01-10T22:22:26.490Z'
      numEdits: 4
      reactions: []
    id: 659f16a124e3a2aa7243d250
    type: comment
  author: ProphetOfBostrom
  content: "Low effort to test idea which could be the whole problem. Probably not\
    \ but very quick to test... if you have the quants all ready.\nWhat kind of issues\
    \ do you have? Is it just behaving like it's lost too much precision or is it\
    \ actually malfunctioning and producing complete gibberish? The latter might be\
    \ a heisenbug involving either/both the tokeniser or how quantize(.exe, if you\
    \ insist) may be different when llama.cpp as a whole is compiled with additional\
    \ flags (like cublas) \n\nA description of the issue you had (or better, just\
    \ some example bad output) would be invaluable to whoever's gonna figure this\
    \ out. It probably won't be me but I'll try.\n\nHave you tried using one your\
    \ bad quants with the llamacpp_HF loader in texgen_webui? Use your tokeniser files\
    \ from your merge repo (not the stock llama one that tgwui provides) then compare\
    \ to the llama.cpp loader next to it. \nPure speculation, but I've seen reports\
    \ of k-quants and busted llama.cpp tokenizer output before so worth a few minutes\
    \ testing side-by-side right? Set a fixed seed on load (17, always use 17 for\
    \ fixed seeds. don't ask, do google.) for the same .gguf and my understanding\
    \ is that they shouldn't diverge? Certainly not in a way that makes you think\
    \ \"broken\". If they do, you've found a real bug! (I've seen some reports that\
    \ a small divergence can happen with the fast tokeniser even for a full fat transformer\
    \ - but it's not a case of one being more or less correct than the other.)\nI'd\
    \ do it myself but I've committed the next 90gb of my life to downloading 16 bit\
    \ files I intend to delete 14 bits from."
  created_at: 2024-01-10 22:13:53+00:00
  edited: true
  hidden: false
  id: 659f16a124e3a2aa7243d250
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2024-01-13T23:52:39.000Z'
    data:
      edited: false
      editors:
      - ProphetOfBostrom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3284616768360138
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
          fullname: Albion Perfidia
          isHf: false
          isPro: false
          name: ProphetOfBostrom
          type: user
        html: '<p>Huh.<br><a href="https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ">https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ</a><br>:)</p>

          '
        raw: 'Huh.

          https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ

          :)'
        updatedAt: '2024-01-13T23:52:39.501Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a322472548c41ad9b06327
    id: 65a322472548c41ad9b06324
    type: comment
  author: ProphetOfBostrom
  content: 'Huh.

    https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ

    :)'
  created_at: 2024-01-13 23:52:39+00:00
  edited: false
  hidden: false
  id: 65a322472548c41ad9b06324
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2024-01-13T23:52:39.000Z'
    data:
      status: closed
    id: 65a322472548c41ad9b06327
    type: status-change
  author: ProphetOfBostrom
  created_at: 2024-01-13 23:52:39+00:00
  id: 65a322472548c41ad9b06327
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-GGUF
repo_type: model
status: closed
target_branch: null
title: K-quantisation
