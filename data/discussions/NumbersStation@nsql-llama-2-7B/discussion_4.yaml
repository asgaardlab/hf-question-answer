!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rishisaraf11
conflicting_files: null
created_at: 2023-08-17 13:47:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69ad038d409e3e4916b185aea67f9a11.svg
      fullname: Rishi Saraf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rishisaraf11
      type: user
    createdAt: '2023-08-17T14:47:34.000Z'
    data:
      edited: false
      editors:
      - rishisaraf11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8774431943893433
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69ad038d409e3e4916b185aea67f9a11.svg
          fullname: Rishi Saraf
          isHf: false
          isPro: false
          name: rishisaraf11
          type: user
        html: '<p>I am getting the below error in Cloudwatch. We are trying to deploy
          it in ml.g5.2xlarge instance. Any resolution for this or we need to deploy
          it in bigger instance.</p>

          <p>torch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed
          memory. (out of memory)<br>Currently allocated     : 20.61 GiB<br>Requested               :
          172.00 MiB<br>Device limit            : 22.20 GiB<br>Free (according to
          CUDA): 15.12 MiB<br>PyTorch limit (set by user-supplied memory fraction)<br>                        :
          22.20 GiB<br>The above exception was the direct cause of the following exception:</p>

          '
        raw: "I am getting the below error in Cloudwatch. We are trying to deploy\
          \ it in ml.g5.2xlarge instance. Any resolution for this or we need to deploy\
          \ it in bigger instance.\r\n\r\ntorch.cuda.OutOfMemoryError: Allocation\
          \ on device 0 would exceed allowed memory. (out of memory)\r\nCurrently\
          \ allocated     : 20.61 GiB\r\nRequested               : 172.00 MiB\r\n\
          Device limit            : 22.20 GiB\r\nFree (according to CUDA): 15.12 MiB\r\
          \nPyTorch limit (set by user-supplied memory fraction)\r\n             \
          \           : 22.20 GiB\r\nThe above exception was the direct cause of the\
          \ following exception:"
        updatedAt: '2023-08-17T14:47:34.165Z'
      numEdits: 0
      reactions: []
    id: 64de33062aeee6796926dab7
    type: comment
  author: rishisaraf11
  content: "I am getting the below error in Cloudwatch. We are trying to deploy it\
    \ in ml.g5.2xlarge instance. Any resolution for this or we need to deploy it in\
    \ bigger instance.\r\n\r\ntorch.cuda.OutOfMemoryError: Allocation on device 0\
    \ would exceed allowed memory. (out of memory)\r\nCurrently allocated     : 20.61\
    \ GiB\r\nRequested               : 172.00 MiB\r\nDevice limit            : 22.20\
    \ GiB\r\nFree (according to CUDA): 15.12 MiB\r\nPyTorch limit (set by user-supplied\
    \ memory fraction)\r\n                        : 22.20 GiB\r\nThe above exception\
    \ was the direct cause of the following exception:"
  created_at: 2023-08-17 13:47:34+00:00
  edited: false
  hidden: false
  id: 64de33062aeee6796926dab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-18T00:15:44.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7116429209709167
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>The model can be deployed on <code>g5.xlarge</code> with <code>torch.bfloat16</code>.</p>

          '
        raw: The model can be deployed on `g5.xlarge` with `torch.bfloat16`.
        updatedAt: '2023-08-18T00:15:44.868Z'
      numEdits: 0
      reactions: []
    id: 64deb8302be7a57fa258c4b0
    type: comment
  author: senwu
  content: The model can be deployed on `g5.xlarge` with `torch.bfloat16`.
  created_at: 2023-08-17 23:15:44+00:00
  edited: false
  hidden: false
  id: 64deb8302be7a57fa258c4b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69ad038d409e3e4916b185aea67f9a11.svg
      fullname: Rishi Saraf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rishisaraf11
      type: user
    createdAt: '2023-08-18T04:10:02.000Z'
    data:
      edited: false
      editors:
      - rishisaraf11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5966995358467102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69ad038d409e3e4916b185aea67f9a11.svg
          fullname: Rishi Saraf
          isHf: false
          isPro: false
          name: rishisaraf11
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;senwu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/senwu\"\
          >@<span class=\"underline\">senwu</span></a></span>\n\n\t</span></span>\
          \ . Can you please tell me how to give torch.bfloat16. configuration in\
          \ the deployment script. Sorry, I am new to this and don't know many of\
          \ these configs. Below is the deployment script I am using</p>\n<pre><code\
          \ class=\"language-import\">import sagemaker\nimport boto3\nfrom sagemaker.huggingface\
          \ import HuggingFaceModel, get_huggingface_llm_image_uri\n\ntry:\n    role\
          \ = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n\
          \    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20230723T133694')['Role']['Arn']\n\
          \n# Hub Model configuration. https://huggingface.co/models\nhub = {\n  \
          \  'HF_MODEL_ID':'NumbersStation/nsql-llama-2-7B',\n    'SM_NUM_GPUS': json.dumps(1)\n\
          }\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          \    image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"\
          0.9.3\"),\n    env=hub,\n    role=role, \n)\n\n# deploy model to SageMaker\
          \ Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
          \    instance_type=\"ml.g5.2xlarge\",\n    container_startup_health_check_timeout=300,\n\
          )\n\npredictor.predict({\n    \"inputs\": \"Can you please let us know more\
          \ details about your \",\n})```\n</code></pre>\n"
        raw: "Thanks @senwu . Can you please tell me how to give torch.bfloat16. configuration\
          \ in the deployment script. Sorry, I am new to this and don't know many\
          \ of these configs. Below is the deployment script I am using\n\n```import\
          \ json\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import\
          \ HuggingFaceModel, get_huggingface_llm_image_uri\n\ntry:\n\trole = sagemaker.get_execution_role()\n\
          except ValueError:\n\tiam = boto3.client('iam')\n\trole = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20230723T133694')['Role']['Arn']\n\
          \n# Hub Model configuration. https://huggingface.co/models\nhub = {\n\t\
          'HF_MODEL_ID':'NumbersStation/nsql-llama-2-7B',\n\t'SM_NUM_GPUS': json.dumps(1)\n\
          }\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          \timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"0.9.3\"\
          ),\n\tenv=hub,\n\trole=role, \n)\n\n# deploy model to SageMaker Inference\n\
          predictor = huggingface_model.deploy(\n\tinitial_instance_count=1,\n\tinstance_type=\"\
          ml.g5.2xlarge\",\n\tcontainer_startup_health_check_timeout=300,\n)\n\npredictor.predict({\n\
          \t\"inputs\": \"Can you please let us know more details about your \",\n\
          })```\n"
        updatedAt: '2023-08-18T04:10:02.501Z'
      numEdits: 0
      reactions: []
    id: 64deef1a2be7a57fa260344d
    type: comment
  author: rishisaraf11
  content: "Thanks @senwu . Can you please tell me how to give torch.bfloat16. configuration\
    \ in the deployment script. Sorry, I am new to this and don't know many of these\
    \ configs. Below is the deployment script I am using\n\n```import json\nimport\
    \ sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel,\
    \ get_huggingface_llm_image_uri\n\ntry:\n\trole = sagemaker.get_execution_role()\n\
    except ValueError:\n\tiam = boto3.client('iam')\n\trole = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20230723T133694')['Role']['Arn']\n\
    \n# Hub Model configuration. https://huggingface.co/models\nhub = {\n\t'HF_MODEL_ID':'NumbersStation/nsql-llama-2-7B',\n\
    \t'SM_NUM_GPUS': json.dumps(1)\n}\n\n# create Hugging Face Model Class\nhuggingface_model\
    \ = HuggingFaceModel(\n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\"\
    ,version=\"0.9.3\"),\n\tenv=hub,\n\trole=role, \n)\n\n# deploy model to SageMaker\
    \ Inference\npredictor = huggingface_model.deploy(\n\tinitial_instance_count=1,\n\
    \tinstance_type=\"ml.g5.2xlarge\",\n\tcontainer_startup_health_check_timeout=300,\n\
    )\n\npredictor.predict({\n\t\"inputs\": \"Can you please let us know more details\
    \ about your \",\n})```\n"
  created_at: 2023-08-18 03:10:02+00:00
  edited: false
  hidden: false
  id: 64deef1a2be7a57fa260344d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-19T01:28:18.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9631897807121277
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;rishisaraf11&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rishisaraf11\"\
          >@<span class=\"underline\">rishisaraf11</span></a></span>\n\n\t</span></span>\uFF0C\
          </p>\n<p>We haven't used Sagemaker to deploy the model and from the doc\
          \ it doesn't seem like there is much flexibility. The model prefers <code>torch.bfloat16</code>\
          \ but you can still use other dtype.</p>\n"
        raw: "Hi @rishisaraf11\uFF0C\n\nWe haven't used Sagemaker to deploy the model\
          \ and from the doc it doesn't seem like there is much flexibility. The model\
          \ prefers `torch.bfloat16` but you can still use other dtype."
        updatedAt: '2023-08-19T01:28:18.451Z'
      numEdits: 0
      reactions: []
    id: 64e01ab259b152302d1efc7c
    type: comment
  author: senwu
  content: "Hi @rishisaraf11\uFF0C\n\nWe haven't used Sagemaker to deploy the model\
    \ and from the doc it doesn't seem like there is much flexibility. The model prefers\
    \ `torch.bfloat16` but you can still use other dtype."
  created_at: 2023-08-19 00:28:18+00:00
  edited: false
  hidden: false
  id: 64e01ab259b152302d1efc7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BFQ7vI_g-kA0C9HkyfDnf.jpeg?w=200&h=200&f=face
      fullname: Arvind Shelke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arviii
      type: user
    createdAt: '2023-08-19T05:31:04.000Z'
    data:
      edited: false
      editors:
      - arviii
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.568810224533081
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BFQ7vI_g-kA0C9HkyfDnf.jpeg?w=200&h=200&f=face
          fullname: Arvind Shelke
          isHf: false
          isPro: false
          name: arviii
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;senwu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/senwu\">@<span class=\"\
          underline\">senwu</span></a></span>\n\n\t</span></span> </p>\n<h4 id=\"\
          i-tried-different-variations-of-passing-sm_framework_params-into-env-for-huggingfacemodel-class-in-the-script-shared-by-rishisaraf11--but-no-luck\"\
          >I tried different variations of passing <code>SM_FRAMEWORK_PARAMS</code>\
          \ into env for <code>HuggingFaceModel</code> class in the script shared\
          \ by <span data-props=\"{&quot;user&quot;:&quot;rishisaraf11&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rishisaraf11\">@<span\
          \ class=\"underline\">rishisaraf11</span></a></span>\n\n\t</span></span>\
          \  but no luck</h4>\n<p>hub = {<br>    'HF_MODEL_ID': 'NumbersStation/nsql-llama-2-7B',<br>\
          \    'SM_NUM_GPUS': json.dumps(1),<br>    'SM_FRAMEWORK_PARAMS': \"{'torch_dtype':\
          \ 'bfloat16'}\"<br>}</p>\n<p>#create Hugging Face Model Class<br>huggingface_model\
          \ = HuggingFaceModel(<br>    image_uri=get_huggingface_llm_image_uri(\"\
          huggingface\", version=\"0.9.3\"),<br>    env=hub,<br>    role=role,<br>)</p>\n\
          <p>#deploy model to SageMaker Inference<br>predictor = huggingface_model.deploy(<br>\
          \    initial_instance_count=1,<br>    instance_type=\"ml.g5.2xlarge\",<br>\
          \    container_startup_health_check_timeout=300,<br>)</p>\n"
        raw: "Hi @senwu \n\n#### I tried different variations of passing `SM_FRAMEWORK_PARAMS`\
          \ into env for `HuggingFaceModel` class in the script shared by @rishisaraf11\
          \  but no luck\n\nhub = {\n    'HF_MODEL_ID': 'NumbersStation/nsql-llama-2-7B',\n\
          \    'SM_NUM_GPUS': json.dumps(1),\n    'SM_FRAMEWORK_PARAMS': \"{'torch_dtype':\
          \ 'bfloat16'}\"\n}\n\n#create Hugging Face Model Class\nhuggingface_model\
          \ = HuggingFaceModel(\n    image_uri=get_huggingface_llm_image_uri(\"huggingface\"\
          , version=\"0.9.3\"),\n    env=hub,\n    role=role,\n)\n\n#deploy model\
          \ to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
          \    instance_type=\"ml.g5.2xlarge\",\n    container_startup_health_check_timeout=300,\n\
          )"
        updatedAt: '2023-08-19T05:31:04.403Z'
      numEdits: 0
      reactions: []
    id: 64e0539875fae2212e6dc065
    type: comment
  author: arviii
  content: "Hi @senwu \n\n#### I tried different variations of passing `SM_FRAMEWORK_PARAMS`\
    \ into env for `HuggingFaceModel` class in the script shared by @rishisaraf11\
    \  but no luck\n\nhub = {\n    'HF_MODEL_ID': 'NumbersStation/nsql-llama-2-7B',\n\
    \    'SM_NUM_GPUS': json.dumps(1),\n    'SM_FRAMEWORK_PARAMS': \"{'torch_dtype':\
    \ 'bfloat16'}\"\n}\n\n#create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
    \    image_uri=get_huggingface_llm_image_uri(\"huggingface\", version=\"0.9.3\"\
    ),\n    env=hub,\n    role=role,\n)\n\n#deploy model to SageMaker Inference\n\
    predictor = huggingface_model.deploy(\n    initial_instance_count=1,\n    instance_type=\"\
    ml.g5.2xlarge\",\n    container_startup_health_check_timeout=300,\n)"
  created_at: 2023-08-19 04:31:04+00:00
  edited: false
  hidden: false
  id: 64e0539875fae2212e6dc065
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-25T20:05:10.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9134967923164368
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>It seems like sagemaker doesn''t have full transformer support yet.
          You can use the default config for the model as well.</p>

          <p>You can also use <code>g5.2xlarge </code> machine or <code>low_cpu_mem_usage=True</code>
          from <a href="https://huggingface.co/docs/transformers/main_classes/model">https://huggingface.co/docs/transformers/main_classes/model</a>
          to reduce the RAM usage when loading the model.</p>

          '
        raw: 'It seems like sagemaker doesn''t have full transformer support yet.
          You can use the default config for the model as well.


          You can also use `g5.2xlarge ` machine or `low_cpu_mem_usage=True` from
          https://huggingface.co/docs/transformers/main_classes/model to reduce the
          RAM usage when loading the model.'
        updatedAt: '2023-08-25T20:05:10.785Z'
      numEdits: 0
      reactions: []
    id: 64e90976f09eea562e05bb32
    type: comment
  author: senwu
  content: 'It seems like sagemaker doesn''t have full transformer support yet. You
    can use the default config for the model as well.


    You can also use `g5.2xlarge ` machine or `low_cpu_mem_usage=True` from https://huggingface.co/docs/transformers/main_classes/model
    to reduce the RAM usage when loading the model.'
  created_at: 2023-08-25 19:05:10+00:00
  edited: false
  hidden: false
  id: 64e90976f09eea562e05bb32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BFQ7vI_g-kA0C9HkyfDnf.jpeg?w=200&h=200&f=face
      fullname: Arvind Shelke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arviii
      type: user
    createdAt: '2023-08-28T06:36:10.000Z'
    data:
      edited: false
      editors:
      - arviii
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8521674275398254
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BFQ7vI_g-kA0C9HkyfDnf.jpeg?w=200&h=200&f=face
          fullname: Arvind Shelke
          isHf: false
          isPro: false
          name: arviii
          type: user
        html: "<h3 id=\"thank-you-for-the-reply-senwu\">Thank you for the reply <span\
          \ data-props=\"{&quot;user&quot;:&quot;senwu&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/senwu\">@<span class=\"underline\"\
          >senwu</span></a></span>\n\n\t</span></span></h3>\n<h4 id=\"problem-seems-with-the-overflow-of-gpu-vram-which-is-222-gbs\"\
          >Problem seems with the overflow of GPU VRAM which is <code>~22.2 GB's</code></h4>\n\
          <p>for <code>ml.g5.2xlarge</code> which has Nvidia A10g 24 GB GPU.</p>\n\
          <h3 id=\"error-sagemaker-deployment-failed-due-to-memory-error\">Error:\
          \ Sagemaker deployment failed due to memory error</h3>\n<pre><code>torch.cuda.OutOfMemoryError:\
          \ Allocation on device 0 would exceed allowed memory. (out of memory)\n\
          Currently allocated : 20.61 GiB\nRequested : 172.00 MiB\nDevice limit :\
          \ 22.20 GiB\nFree (according to CUDA): 15.12 MiB\nPyTorch limit (set by\
          \ user-supplied memory fraction)\n: 22.20 GiB\n</code></pre>\n"
        raw: "### Thank you for the reply @senwu \n#### Problem seems with the overflow\
          \ of GPU VRAM which is `~22.2 GB's`\nfor `ml.g5.2xlarge` which has Nvidia\
          \ A10g 24 GB GPU.\n\n### Error: Sagemaker deployment failed due to memory\
          \ error\n```\ntorch.cuda.OutOfMemoryError: Allocation on device 0 would\
          \ exceed allowed memory. (out of memory)\nCurrently allocated : 20.61 GiB\n\
          Requested : 172.00 MiB\nDevice limit : 22.20 GiB\nFree (according to CUDA):\
          \ 15.12 MiB\nPyTorch limit (set by user-supplied memory fraction)\n: 22.20\
          \ GiB\n```"
        updatedAt: '2023-08-28T06:36:10.038Z'
      numEdits: 0
      reactions: []
    id: 64ec405aa4b2985194092073
    type: comment
  author: arviii
  content: "### Thank you for the reply @senwu \n#### Problem seems with the overflow\
    \ of GPU VRAM which is `~22.2 GB's`\nfor `ml.g5.2xlarge` which has Nvidia A10g\
    \ 24 GB GPU.\n\n### Error: Sagemaker deployment failed due to memory error\n```\n\
    torch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed memory.\
    \ (out of memory)\nCurrently allocated : 20.61 GiB\nRequested : 172.00 MiB\nDevice\
    \ limit : 22.20 GiB\nFree (according to CUDA): 15.12 MiB\nPyTorch limit (set by\
    \ user-supplied memory fraction)\n: 22.20 GiB\n```"
  created_at: 2023-08-28 05:36:10+00:00
  edited: false
  hidden: false
  id: 64ec405aa4b2985194092073
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-28T22:52:37.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7552071809768677
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>To <code>torch.float32</code> version of the model it requires around
          26G VRAM. We will adjust the default model type this week.</p>

          '
        raw: To `torch.float32` version of the model it requires around 26G VRAM.
          We will adjust the default model type this week.
        updatedAt: '2023-08-28T22:52:37.958Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - arviii
    id: 64ed25358d7034fff7ae8c2b
    type: comment
  author: senwu
  content: To `torch.float32` version of the model it requires around 26G VRAM. We
    will adjust the default model type this week.
  created_at: 2023-08-28 21:52:37+00:00
  edited: false
  hidden: false
  id: 64ed25358d7034fff7ae8c2b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: NumbersStation/nsql-llama-2-7B
repo_type: model
status: open
target_branch: null
title: Sagemaker Deployment Failing in ml.g5.2xlarge instance
