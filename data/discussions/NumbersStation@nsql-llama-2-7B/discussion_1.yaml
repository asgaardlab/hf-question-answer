!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dudub
conflicting_files: null
created_at: 2023-08-02 17:16:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
      fullname: Dudu Butbul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dudub
      type: user
    createdAt: '2023-08-02T18:16:34.000Z'
    data:
      edited: false
      editors:
      - dudub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3916081488132477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
          fullname: Dudu Butbul
          isHf: false
          isPro: false
          name: dudub
          type: user
        html: '<p>Hi, I cloned this model into my local machine and tried to use it
          using the following manifest command:<br>python3 -m manifest.api.app <br>    --model_type
          huggingface <br>    --model_generation_type llama-text-generation <br>    --model_name_or_path
          nsql-llama-2-7B <br>    --device 0</p>

          <p>but I''m getting this exception:</p>

          <p>Traceback (most recent call last):<br>  File "", line 198, in _run_module_as_main<br>  File
          "", line 88, in _run_code<br>  File "/lib/python3.11/site-packages/manifest/api/app.py",
          line 301, in <br>    main()<br>  File "/lib/python3.11/site-packages/manifest/api/app.py",
          line 148, in main<br>    model = MODEL_CONSTRUCTORS[model_type](<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/lib/python3.11/site-packages/manifest/api/models/huggingface.py", line
          474, in __init__<br>    tokenizer = LlamaTokenizer.from_pretrained(self.model_name)<br>                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/lib/python3.11/site-packages/transformers/tokenization_utils_base.py",
          line 1825, in from_pretrained<br>    return cls._from_pretrained(<br>           ^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/lib/python3.11/site-packages/transformers/tokenization_utils_base.py",
          line 1988, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py",
          line 96, in <strong>init</strong><br>    self.sp_model.Load(vocab_file)<br>  File
          "/lib/python3.11/site-packages/sentencepiece/<strong>init</strong>.py",
          line 905, in Load<br>    return self.LoadFromFile(model_file)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/lib/python3.11/site-packages/sentencepiece/<strong>init</strong>.py",
          line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>RuntimeError:
          Internal: /Users/runner/work/sentencepiece/sentencepiece/src/sentencepiece_processor.cc(1102)
          [model_proto-&gt;ParseFromArray(serialized.data(), serialized.size())]</p>

          <p>Would appreciate your help here</p>

          '
        raw: "Hi, I cloned this model into my local machine and tried to use it using\
          \ the following manifest command:\r\npython3 -m manifest.api.app \\\r\n\
          \    --model_type huggingface \\\r\n    --model_generation_type llama-text-generation\
          \ \\\r\n    --model_name_or_path nsql-llama-2-7B \\\r\n    --device 0\r\n\
          \r\nbut I'm getting this exception:\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File\
          \ \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/lib/python3.11/site-packages/manifest/api/app.py\"\
          , line 301, in <module>\r\n    main()\r\n  File \"/lib/python3.11/site-packages/manifest/api/app.py\"\
          , line 148, in main\r\n    model = MODEL_CONSTRUCTORS[model_type](\r\n \
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lib/python3.11/site-packages/manifest/api/models/huggingface.py\"\
          , line 474, in __init__\r\n    tokenizer = LlamaTokenizer.from_pretrained(self.model_name)\r\
          \n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
          \ \"/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1825, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1988, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"\
          /lib/python3.11/site-packages/sentencepiece/__init__.py\", line 905, in\
          \ Load\r\n    return self.LoadFromFile(model_file)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/lib/python3.11/site-packages/sentencepiece/__init__.py\", line\
          \ 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nRuntimeError: Internal: /Users/runner/work/sentencepiece/sentencepiece/src/sentencepiece_processor.cc(1102)\
          \ [model_proto->ParseFromArray(serialized.data(), serialized.size())]\r\n\
          \r\nWould appreciate your help here"
        updatedAt: '2023-08-02T18:16:34.665Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eladtsur
    id: 64ca9d82b704b851477ac457
    type: comment
  author: dudub
  content: "Hi, I cloned this model into my local machine and tried to use it using\
    \ the following manifest command:\r\npython3 -m manifest.api.app \\\r\n    --model_type\
    \ huggingface \\\r\n    --model_generation_type llama-text-generation \\\r\n \
    \   --model_name_or_path nsql-llama-2-7B \\\r\n    --device 0\r\n\r\nbut I'm getting\
    \ this exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen\
    \ runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line\
    \ 88, in _run_code\r\n  File \"/lib/python3.11/site-packages/manifest/api/app.py\"\
    , line 301, in <module>\r\n    main()\r\n  File \"/lib/python3.11/site-packages/manifest/api/app.py\"\
    , line 148, in main\r\n    model = MODEL_CONSTRUCTORS[model_type](\r\n       \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lib/python3.11/site-packages/manifest/api/models/huggingface.py\"\
    , line 474, in __init__\r\n    tokenizer = LlamaTokenizer.from_pretrained(self.model_name)\r\
    \n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
    /lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line\
    \ 1825, in from_pretrained\r\n    return cls._from_pretrained(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1988, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"/lib/python3.11/site-packages/sentencepiece/__init__.py\"\
    , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n          \
    \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lib/python3.11/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \nRuntimeError: Internal: /Users/runner/work/sentencepiece/sentencepiece/src/sentencepiece_processor.cc(1102)\
    \ [model_proto->ParseFromArray(serialized.data(), serialized.size())]\r\n\r\n\
    Would appreciate your help here"
  created_at: 2023-08-02 17:16:34+00:00
  edited: false
  hidden: false
  id: 64ca9d82b704b851477ac457
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-02T20:06:11.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5952017307281494
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;dudub&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dudub\">@<span class=\"\
          underline\">dudub</span></a></span>\n\n\t</span></span>,</p>\n<p>Can you\
          \ try this cmd?</p>\n<pre><code>python3 -m manifest.api.app \\\n    --model_type\
          \ huggingface \\\n    --model_generation_type text-generation \\\n    --model_name_or_path\
          \ NumbersStation/nsql-llama-2-7B \\\n    --device 0\n</code></pre>\n<p>The\
          \ current version manifest uses an old class to load llama based model and\
          \ we will update it in the future manifest release. Thanks!</p>\n"
        raw: "Hi @dudub,\n\nCan you try this cmd?\n```\npython3 -m manifest.api.app\
          \ \\\n    --model_type huggingface \\\n    --model_generation_type text-generation\
          \ \\\n    --model_name_or_path NumbersStation/nsql-llama-2-7B \\\n    --device\
          \ 0\n```\n\nThe current version manifest uses an old class to load llama\
          \ based model and we will update it in the future manifest release. Thanks!"
        updatedAt: '2023-08-02T20:06:11.143Z'
      numEdits: 0
      reactions: []
    id: 64cab7337846f146bd320110
    type: comment
  author: senwu
  content: "Hi @dudub,\n\nCan you try this cmd?\n```\npython3 -m manifest.api.app\
    \ \\\n    --model_type huggingface \\\n    --model_generation_type text-generation\
    \ \\\n    --model_name_or_path NumbersStation/nsql-llama-2-7B \\\n    --device\
    \ 0\n```\n\nThe current version manifest uses an old class to load llama based\
    \ model and we will update it in the future manifest release. Thanks!"
  created_at: 2023-08-02 19:06:11+00:00
  edited: false
  hidden: false
  id: 64cab7337846f146bd320110
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
      fullname: Dudu Butbul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dudub
      type: user
    createdAt: '2023-08-02T20:39:09.000Z'
    data:
      edited: false
      editors:
      - dudub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7287055850028992
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
          fullname: Dudu Butbul
          isHf: false
          isPro: false
          name: dudub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;senwu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/senwu\">@<span class=\"\
          underline\">senwu</span></a></span>\n\n\t</span></span><br>Thanks for the\
          \ quick answer!<br>Yes I ran it and looks like a server is up and running\
          \ but now I'm getting this error while I'm trying to communicate with the\
          \ LLM using LangChain:</p>\n<ul>\n<li>Running on all addresses (0.0.0.0)</li>\n\
          <li>Running on <a rel=\"nofollow\" href=\"http://127.0.0.1:5002\">http://127.0.0.1:5002</a></li>\n\
          <li>Running on <a rel=\"nofollow\" href=\"http://192.168.1.108:5002\">http://192.168.1.108:5002</a><br>Press\
          \ CTRL+C to quit<br>127.0.0.1 - - [02/Aug/2023 23:37:28] \"POST /params\
          \ HTTP/1.1\" 200 -<br>127.0.0.1 - - [02/Aug/2023 23:37:28] \"POST /params\
          \ HTTP/1.1\" 200 -<br>The following <code>model_kwargs</code> are not used\
          \ by the model: ['token_type_ids'] (note: typos in the generate arguments\
          \ will also show up in this list)<br>127.0.0.1 - - [02/Aug/2023 23:37:28]\
          \ \"POST /completions HTTP/1.1\" 400 -</li>\n</ul>\n<p>That's how I configured\
          \ it in the LangChain side:<br>manifest = Manifest(<br>        client_name=\"\
          huggingface\",<br>        client_connection=\"<a rel=\"nofollow\" href=\"\
          http://127.0.0.1:5002&quot;\">http://127.0.0.1:5002\"</a><br>    )</p>\n\
          <pre><code>local_llm = ManifestWrapper(\n    client=manifest, \n    llm_kwargs={\"\
          temperature\": 0.0, \"max_tokens\": 1024}, \n    verbose=True\n)\n</code></pre>\n\
          <p>and used the llm in a SQL agent.</p>\n"
        raw: "@senwu \nThanks for the quick answer!\nYes I ran it and looks like a\
          \ server is up and running but now I'm getting this error while I'm trying\
          \ to communicate with the LLM using LangChain:\n\n * Running on all addresses\
          \ (0.0.0.0)\n * Running on http://127.0.0.1:5002\n * Running on http://192.168.1.108:5002\n\
          Press CTRL+C to quit\n127.0.0.1 - - [02/Aug/2023 23:37:28] \"POST /params\
          \ HTTP/1.1\" 200 -\n127.0.0.1 - - [02/Aug/2023 23:37:28] \"POST /params\
          \ HTTP/1.1\" 200 -\nThe following `model_kwargs` are not used by the model:\
          \ ['token_type_ids'] (note: typos in the generate arguments will also show\
          \ up in this list)\n127.0.0.1 - - [02/Aug/2023 23:37:28] \"POST /completions\
          \ HTTP/1.1\" 400 -\n\nThat's how I configured it in the LangChain side:\n\
          manifest = Manifest(\n        client_name=\"huggingface\", \n        client_connection=\"\
          http://127.0.0.1:5002\"\n    )\n\n    local_llm = ManifestWrapper(\n   \
          \     client=manifest, \n        llm_kwargs={\"temperature\": 0.0, \"max_tokens\"\
          : 1024}, \n        verbose=True\n    )\n\nand used the llm in a SQL agent."
        updatedAt: '2023-08-02T20:39:09.005Z'
      numEdits: 0
      reactions: []
    id: 64cabeed91726743360df519
    type: comment
  author: dudub
  content: "@senwu \nThanks for the quick answer!\nYes I ran it and looks like a server\
    \ is up and running but now I'm getting this error while I'm trying to communicate\
    \ with the LLM using LangChain:\n\n * Running on all addresses (0.0.0.0)\n * Running\
    \ on http://127.0.0.1:5002\n * Running on http://192.168.1.108:5002\nPress CTRL+C\
    \ to quit\n127.0.0.1 - - [02/Aug/2023 23:37:28] \"POST /params HTTP/1.1\" 200\
    \ -\n127.0.0.1 - - [02/Aug/2023 23:37:28] \"POST /params HTTP/1.1\" 200 -\nThe\
    \ following `model_kwargs` are not used by the model: ['token_type_ids'] (note:\
    \ typos in the generate arguments will also show up in this list)\n127.0.0.1 -\
    \ - [02/Aug/2023 23:37:28] \"POST /completions HTTP/1.1\" 400 -\n\nThat's how\
    \ I configured it in the LangChain side:\nmanifest = Manifest(\n        client_name=\"\
    huggingface\", \n        client_connection=\"http://127.0.0.1:5002\"\n    )\n\n\
    \    local_llm = ManifestWrapper(\n        client=manifest, \n        llm_kwargs={\"\
    temperature\": 0.0, \"max_tokens\": 1024}, \n        verbose=True\n    )\n\nand\
    \ used the llm in a SQL agent."
  created_at: 2023-08-02 19:39:09+00:00
  edited: false
  hidden: false
  id: 64cabeed91726743360df519
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-02T22:00:56.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8244166374206543
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;dudub&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dudub\">@<span class=\"\
          underline\">dudub</span></a></span>\n\n\t</span></span>,</p>\n<p>I am not\
          \ very familiar with LangCahin but from the error message above it seems\
          \ like LangChain sends the wrong arguments to manifest. Can you double-check\
          \ the argument? </p>\n"
        raw: 'Hi @dudub,


          I am not very familiar with LangCahin but from the error message above it
          seems like LangChain sends the wrong arguments to manifest. Can you double-check
          the argument? '
        updatedAt: '2023-08-02T22:00:56.847Z'
      numEdits: 0
      reactions: []
    id: 64cad2187a4f2363570f7243
    type: comment
  author: senwu
  content: 'Hi @dudub,


    I am not very familiar with LangCahin but from the error message above it seems
    like LangChain sends the wrong arguments to manifest. Can you double-check the
    argument? '
  created_at: 2023-08-02 21:00:56+00:00
  edited: false
  hidden: false
  id: 64cad2187a4f2363570f7243
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
      fullname: Dudu Butbul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dudub
      type: user
    createdAt: '2023-08-03T08:03:12.000Z'
    data:
      edited: true
      editors:
      - dudub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7017067670822144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
          fullname: Dudu Butbul
          isHf: false
          isPro: false
          name: dudub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;senwu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/senwu\">@<span class=\"\
          underline\">senwu</span></a></span>\n\n\t</span></span><br>I think it's\
          \ not an issue with LangChain but Manifest cause I'm getting the same error\
          \ we a simple POST request using Postman:</p>\n<p>curl --location '<a rel=\"\
          nofollow\" href=\"http://127.0.0.1:5002/completions'\">http://127.0.0.1:5002/completions'</a>\
          \ <br>--header 'Content-Type: application/json' <br>--data '{<br>    \"\
          prompt\": \"Hello World\",<br>    \"max_tokens\": 1024,<br>    \"temperature\"\
          : 0.0,<br>    \"repetition_penalty\": 1,<br>    \"top_k\": 50,<br>    \"\
          top_p\": 10,<br>    \"do_sample\": \"True\",<br>    \"n\": 1,<br>    \"\
          max_new_tokens\": 1024<br>}'</p>\n<p>I think it's something related to the\
          \ Manifest server/transformers.<br><a href=\"https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226/discussions/2\"\
          >https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226/discussions/2</a></p>\n\
          <p>are you familiar with another way to run this model locally besides Manifest?</p>\n"
        raw: "@senwu \nI think it's not an issue with LangChain but Manifest cause\
          \ I'm getting the same error we a simple POST request using Postman:\n\n\
          curl --location 'http://127.0.0.1:5002/completions' \\\n--header 'Content-Type:\
          \ application/json' \\\n--data '{\n    \"prompt\": \"Hello World\",\n  \
          \  \"max_tokens\": 1024,\n    \"temperature\": 0.0,\n    \"repetition_penalty\"\
          : 1,\n    \"top_k\": 50,\n    \"top_p\": 10,\n    \"do_sample\": \"True\"\
          ,\n    \"n\": 1,\n    \"max_new_tokens\": 1024\n}'\n\nI think it's something\
          \ related to the Manifest server/transformers.\nhttps://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226/discussions/2\n\
          \nare you familiar with another way to run this model locally besides Manifest?"
        updatedAt: '2023-08-03T08:04:14.952Z'
      numEdits: 1
      reactions: []
    id: 64cb5f4087bfb0fc210dad2e
    type: comment
  author: dudub
  content: "@senwu \nI think it's not an issue with LangChain but Manifest cause I'm\
    \ getting the same error we a simple POST request using Postman:\n\ncurl --location\
    \ 'http://127.0.0.1:5002/completions' \\\n--header 'Content-Type: application/json'\
    \ \\\n--data '{\n    \"prompt\": \"Hello World\",\n    \"max_tokens\": 1024,\n\
    \    \"temperature\": 0.0,\n    \"repetition_penalty\": 1,\n    \"top_k\": 50,\n\
    \    \"top_p\": 10,\n    \"do_sample\": \"True\",\n    \"n\": 1,\n    \"max_new_tokens\"\
    : 1024\n}'\n\nI think it's something related to the Manifest server/transformers.\n\
    https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226/discussions/2\n\n\
    are you familiar with another way to run this model locally besides Manifest?"
  created_at: 2023-08-03 07:03:12+00:00
  edited: true
  hidden: false
  id: 64cb5f4087bfb0fc210dad2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-03T17:13:59.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3085349202156067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;dudub&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dudub\">@<span class=\"\
          underline\">dudub</span></a></span>\n\n\t</span></span>,</p>\n<p>Which transformers\
          \ version are you using? We are using <code>transformer 4.31.0</code> and\
          \ it works.</p>\n<pre><code>FLASK_PORT=7000 python3 -m manifest.api.app\
          \ --model_type huggingface --model_generation_type text-generation --model_name_or_path\
          \ NumbersStation/nsql-llama-2-7B --device 0\nModel Name: NumbersStation/nsql-llama-2-7B\
          \ Model Path: NumbersStation/nsql-llama-2-7B\nLoading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:20&lt;00:00, \
          \ 6.87s/it]\nLoaded Model DType torch.float32\nUsings max_length: 4096\n\
          \ * Serving Flask app 'app'\n * Debug mode: off\nWARNING: This is a development\
          \ server. Do not use it in a production deployment. Use a production WSGI\
          \ server instead.\n * Running on all addresses (0.0.0.0)\n * Running on\
          \ http://127.0.0.1:7000\n * Running on http://38.99.106.21:7000\nPress CTRL+C\
          \ to quit\n100.110.106.20 - - [03/Aug/2023 10:10:29] \"POST /completions\
          \ HTTP/1.1\" 200 -\n</code></pre>\n<pre><code>curl --location 'http://127.0.0.1:7000/completions'\
          \ --header 'Content-Type: application/json' --data '{\n\"prompt\": \"Hello\
          \ World\",\n\"max_tokens\": 1024,\n\"temperature\": 0.0,\n\"repetition_penalty\"\
          : 1,\n\"top_k\": 50,\n\"top_p\": 10,\n\"do_sample\": \"True\",\n\"n\": 1,\n\
          \"max_new_tokens\": 1024\n}'\n{\"id\": \"0b13e512-4d03-4d97-8f90-e97d5479ead2\"\
          , \"object\": \"text_completion\", \"created\": 1691082629, \"model\": \"\
          flask_model\", \"choices\": [{\"text\": \"\\n\", \"logprob\": -2.203536033630371,\
          \ \"tokens\": [13, 2], \"token_logprobs\": [-1.6638275384902954, -0.5397084951400757]}]}\n\
          </code></pre>\n"
        raw: "Hi @dudub,\n\nWhich transformers version are you using? We are using\
          \ `transformer 4.31.0` and it works.\n\n```\nFLASK_PORT=7000 python3 -m\
          \ manifest.api.app --model_type huggingface --model_generation_type text-generation\
          \ --model_name_or_path NumbersStation/nsql-llama-2-7B --device 0\nModel\
          \ Name: NumbersStation/nsql-llama-2-7B Model Path: NumbersStation/nsql-llama-2-7B\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 3/3 [00:20<00:00,  6.87s/it]\nLoaded Model DType torch.float32\nUsings\
          \ max_length: 4096\n * Serving Flask app 'app'\n * Debug mode: off\nWARNING:\
          \ This is a development server. Do not use it in a production deployment.\
          \ Use a production WSGI server instead.\n * Running on all addresses (0.0.0.0)\n\
          \ * Running on http://127.0.0.1:7000\n * Running on http://38.99.106.21:7000\n\
          Press CTRL+C to quit\n100.110.106.20 - - [03/Aug/2023 10:10:29] \"POST /completions\
          \ HTTP/1.1\" 200 -\n```\n\n```\ncurl --location 'http://127.0.0.1:7000/completions'\
          \ --header 'Content-Type: application/json' --data '{\n\"prompt\": \"Hello\
          \ World\",\n\"max_tokens\": 1024,\n\"temperature\": 0.0,\n\"repetition_penalty\"\
          : 1,\n\"top_k\": 50,\n\"top_p\": 10,\n\"do_sample\": \"True\",\n\"n\": 1,\n\
          \"max_new_tokens\": 1024\n}'\n{\"id\": \"0b13e512-4d03-4d97-8f90-e97d5479ead2\"\
          , \"object\": \"text_completion\", \"created\": 1691082629, \"model\": \"\
          flask_model\", \"choices\": [{\"text\": \"\\n\", \"logprob\": -2.203536033630371,\
          \ \"tokens\": [13, 2], \"token_logprobs\": [-1.6638275384902954, -0.5397084951400757]}]}\n\
          ```"
        updatedAt: '2023-08-03T17:13:59.472Z'
      numEdits: 0
      reactions: []
    id: 64cbe05779d99f9e7a4a1c29
    type: comment
  author: senwu
  content: "Hi @dudub,\n\nWhich transformers version are you using? We are using `transformer\
    \ 4.31.0` and it works.\n\n```\nFLASK_PORT=7000 python3 -m manifest.api.app --model_type\
    \ huggingface --model_generation_type text-generation --model_name_or_path NumbersStation/nsql-llama-2-7B\
    \ --device 0\nModel Name: NumbersStation/nsql-llama-2-7B Model Path: NumbersStation/nsql-llama-2-7B\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 3/3 [00:20<00:00,  6.87s/it]\nLoaded Model DType torch.float32\nUsings max_length:\
    \ 4096\n * Serving Flask app 'app'\n * Debug mode: off\nWARNING: This is a development\
    \ server. Do not use it in a production deployment. Use a production WSGI server\
    \ instead.\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:7000\n\
    \ * Running on http://38.99.106.21:7000\nPress CTRL+C to quit\n100.110.106.20\
    \ - - [03/Aug/2023 10:10:29] \"POST /completions HTTP/1.1\" 200 -\n```\n\n```\n\
    curl --location 'http://127.0.0.1:7000/completions' --header 'Content-Type: application/json'\
    \ --data '{\n\"prompt\": \"Hello World\",\n\"max_tokens\": 1024,\n\"temperature\"\
    : 0.0,\n\"repetition_penalty\": 1,\n\"top_k\": 50,\n\"top_p\": 10,\n\"do_sample\"\
    : \"True\",\n\"n\": 1,\n\"max_new_tokens\": 1024\n}'\n{\"id\": \"0b13e512-4d03-4d97-8f90-e97d5479ead2\"\
    , \"object\": \"text_completion\", \"created\": 1691082629, \"model\": \"flask_model\"\
    , \"choices\": [{\"text\": \"\\n\", \"logprob\": -2.203536033630371, \"tokens\"\
    : [13, 2], \"token_logprobs\": [-1.6638275384902954, -0.5397084951400757]}]}\n\
    ```"
  created_at: 2023-08-03 16:13:59+00:00
  edited: false
  hidden: false
  id: 64cbe05779d99f9e7a4a1c29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
      fullname: Dudu Butbul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dudub
      type: user
    createdAt: '2023-08-03T20:29:00.000Z'
    data:
      edited: false
      editors:
      - dudub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8741825819015503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3be5319808671a41fae751930eb53d.svg
          fullname: Dudu Butbul
          isHf: false
          isPro: false
          name: dudub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;senwu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/senwu\">@<span class=\"\
          underline\">senwu</span></a></span>\n\n\t</span></span><br>Thanks again\
          \ for your help, It indeed was that issue and now the error is gone but\
          \ now Im facing a new one...<br>can you tell me on what machine are you\
          \ running it?<br>I'm trying to run it on my local Macbook Pro M1 Pro but\
          \ getting the following error:<br>\"addmm_impl_cpu_\" not implemented for\
          \ 'Half'</p>\n<p>maybe I can even do it and need to deploy it on a remote\
          \ machine? (for now, it's just for testing and playground of course)</p>\n\
          <p>That's my log when the server starts up:</p>\n<p>[2023-08-03 23:19:50,322]\
          \ [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\
          \ to cuda (auto detect)<br>Model Name: nsql-llama-2-7B Model Path: nsql-llama-2-7B<br>Loading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 3/3 [00:29&lt;00:00,  9.86s/it]<br>Loaded Model DType torch.float16<br>Usings\
          \ max_length: 4096</p>\n<ul>\n<li>Serving Flask app 'app'</li>\n<li>Debug\
          \ mode: off</li>\n</ul>\n"
        raw: "@senwu \nThanks again for your help, It indeed was that issue and now\
          \ the error is gone but now Im facing a new one...\ncan you tell me on what\
          \ machine are you running it?\nI'm trying to run it on my local Macbook\
          \ Pro M1 Pro but getting the following error:\n\"addmm_impl_cpu_\" not implemented\
          \ for 'Half'\n\nmaybe I can even do it and need to deploy it on a remote\
          \ machine? (for now, it's just for testing and playground of course)\n\n\
          That's my log when the server starts up:\n\n[2023-08-03 23:19:50,322] [INFO]\
          \ [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda\
          \ (auto detect)\nModel Name: nsql-llama-2-7B Model Path: nsql-llama-2-7B\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 3/3 [00:29<00:00,  9.86s/it]\nLoaded Model DType torch.float16\n\
          Usings max_length: 4096\n * Serving Flask app 'app'\n * Debug mode: off"
        updatedAt: '2023-08-03T20:29:00.142Z'
      numEdits: 0
      reactions: []
    id: 64cc0e0c8174e45ae06bf7f8
    type: comment
  author: dudub
  content: "@senwu \nThanks again for your help, It indeed was that issue and now\
    \ the error is gone but now Im facing a new one...\ncan you tell me on what machine\
    \ are you running it?\nI'm trying to run it on my local Macbook Pro M1 Pro but\
    \ getting the following error:\n\"addmm_impl_cpu_\" not implemented for 'Half'\n\
    \nmaybe I can even do it and need to deploy it on a remote machine? (for now,\
    \ it's just for testing and playground of course)\n\nThat's my log when the server\
    \ starts up:\n\n[2023-08-03 23:19:50,322] [INFO] [real_accelerator.py:110:get_accelerator]\
    \ Setting ds_accelerator to cuda (auto detect)\nModel Name: nsql-llama-2-7B Model\
    \ Path: nsql-llama-2-7B\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:29<00:00,  9.86s/it]\nLoaded Model\
    \ DType torch.float16\nUsings max_length: 4096\n * Serving Flask app 'app'\n *\
    \ Debug mode: off"
  created_at: 2023-08-03 19:29:00+00:00
  edited: false
  hidden: false
  id: 64cc0e0c8174e45ae06bf7f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-03T22:29:25.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8041698336601257
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>We''ve tested the model on Ubuntu 20.04.</p>

          '
        raw: We've tested the model on Ubuntu 20.04.
        updatedAt: '2023-08-03T22:29:25.436Z'
      numEdits: 0
      reactions: []
    id: 64cc2a45a81988d0735f399d
    type: comment
  author: senwu
  content: We've tested the model on Ubuntu 20.04.
  created_at: 2023-08-03 21:29:25+00:00
  edited: false
  hidden: false
  id: 64cc2a45a81988d0735f399d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a8189ce487d1859871a544678cc4872.svg
      fullname: AGomes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ditchtech
      type: user
    createdAt: '2023-08-10T01:53:46.000Z'
    data:
      edited: false
      editors:
      - ditchtech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8964257836341858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a8189ce487d1859871a544678cc4872.svg
          fullname: AGomes
          isHf: false
          isPro: false
          name: ditchtech
          type: user
        html: '<p>Any luck dudub on using a local database query language? I get the
          best response with OpenAI models, but trying to replicate with private LLM
          setup.</p>

          '
        raw: Any luck dudub on using a local database query language? I get the best
          response with OpenAI models, but trying to replicate with private LLM setup.
        updatedAt: '2023-08-10T01:53:46.431Z'
      numEdits: 0
      reactions: []
    id: 64d4432a23996fa867770ee1
    type: comment
  author: ditchtech
  content: Any luck dudub on using a local database query language? I get the best
    response with OpenAI models, but trying to replicate with private LLM setup.
  created_at: 2023-08-10 00:53:46+00:00
  edited: false
  hidden: false
  id: 64d4432a23996fa867770ee1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-16T19:13:03.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7650753855705261
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>We''ve provided some tutorials about the local database query generation
          <a rel="nofollow" href="https://github.com/NumbersStationAI/NSQL/tree/main/examples">here</a>
          and you can also <a rel="nofollow" href="https://github.com/tobymao/sqlglot">SQLGlot</a>
          to convert the generated query into the dialect you want.</p>

          '
        raw: We've provided some tutorials about the local database query generation
          [here](https://github.com/NumbersStationAI/NSQL/tree/main/examples) and
          you can also [SQLGlot](https://github.com/tobymao/sqlglot) to convert the
          generated query into the dialect you want.
        updatedAt: '2023-08-16T19:13:03.078Z'
      numEdits: 0
      reactions: []
    id: 64dd1fbf4e2c7863c31a443e
    type: comment
  author: senwu
  content: We've provided some tutorials about the local database query generation
    [here](https://github.com/NumbersStationAI/NSQL/tree/main/examples) and you can
    also [SQLGlot](https://github.com/tobymao/sqlglot) to convert the generated query
    into the dialect you want.
  created_at: 2023-08-16 18:13:03+00:00
  edited: false
  hidden: false
  id: 64dd1fbf4e2c7863c31a443e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-25T17:32:17.000Z'
    data:
      status: closed
    id: 64e8e5a1a1e1a36f96e05d58
    type: status-change
  author: senwu
  created_at: 2023-08-25 16:32:17+00:00
  id: 64e8e5a1a1e1a36f96e05d58
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NumbersStation/nsql-llama-2-7B
repo_type: model
status: closed
target_branch: null
title: Encounter an exception while trying to run it using manifest
