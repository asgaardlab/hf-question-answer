!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-05-10 00:54:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-10T01:54:20.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>As a reminder is use Oobabooga when possible and llama.cpp when
          I have too. </p>

          <p>I don''t really need an in depth answer but I tried to google it and
          nothing made sense to me. Is the Triton model better for Oobabooga?<br>I
          even read some of your other discussion including things like "true sequential"
          + "groupsize 128" and "act order" </p>

          <p>Act order is an improvement on how the USER and AI talk? Like were it
          will stop less or answer it''s own questions less or something no that I
          have there issues at present?<br>Here is my current argument if I run the
          Triton model that has these new functions should I change my arguments?<br>"call
          python server.py --auto-devices --chat --model reeducator_vicuna-13b-cocktail
          --wbits 4 --groupsize 128"<br>Thanks.</p>

          '
        raw: "As a reminder is use Oobabooga when possible and llama.cpp when I have\
          \ too. \n\nI don't really need an in depth answer but I tried to google\
          \ it and nothing made sense to me. Is the Triton model better for Oobabooga?\
          \ \nI even read some of your other discussion including things like \"true\
          \ sequential\" + \"groupsize 128\" and \"act order\" \n\nAct order is an\
          \ improvement on how the USER and AI talk? Like were it will stop less or\
          \ answer it's own questions less or something no that I have there issues\
          \ at present? \nHere is my current argument if I run the Triton model that\
          \ has these new functions should I change my arguments? \n\"call python\
          \ server.py --auto-devices --chat --model reeducator_vicuna-13b-cocktail\
          \ --wbits 4 --groupsize 128\"\nThanks."
        updatedAt: '2023-05-10T01:59:34.472Z'
      numEdits: 3
      reactions: []
    id: 645af94cc266796265bc1aa5
    type: comment
  author: Goldenblood56
  content: "As a reminder is use Oobabooga when possible and llama.cpp when I have\
    \ too. \n\nI don't really need an in depth answer but I tried to google it and\
    \ nothing made sense to me. Is the Triton model better for Oobabooga? \nI even\
    \ read some of your other discussion including things like \"true sequential\"\
    \ + \"groupsize 128\" and \"act order\" \n\nAct order is an improvement on how\
    \ the USER and AI talk? Like were it will stop less or answer it's own questions\
    \ less or something no that I have there issues at present? \nHere is my current\
    \ argument if I run the Triton model that has these new functions should I change\
    \ my arguments? \n\"call python server.py --auto-devices --chat --model reeducator_vicuna-13b-cocktail\
    \ --wbits 4 --groupsize 128\"\nThanks."
  created_at: 2023-05-10 00:54:20+00:00
  edited: true
  hidden: false
  id: 645af94cc266796265bc1aa5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-10T18:43:39.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p>Personally I'm not familiar if there are any benefits of using one\
          \ over the other, since I don't use GPU inference myself at this point...\
          \ Maybe <span data-props=\"{&quot;user&quot;:&quot;TheYuriLover&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheYuriLover\"\
          >@<span class=\"underline\">TheYuriLover</span></a></span>\n\n\t</span></span>\
          \ knows about about it since he requested the triton conversion? Or somehow\
          \ who knows more might fill in here.</p>\n"
        raw: Personally I'm not familiar if there are any benefits of using one over
          the other, since I don't use GPU inference myself at this point... Maybe
          @TheYuriLover knows about about it since he requested the triton conversion?
          Or somehow who knows more might fill in here.
        updatedAt: '2023-05-10T18:43:39.736Z'
      numEdits: 0
      reactions: []
    id: 645be5dbc971fbab7422b076
    type: comment
  author: reeducator
  content: Personally I'm not familiar if there are any benefits of using one over
    the other, since I don't use GPU inference myself at this point... Maybe @TheYuriLover
    knows about about it since he requested the triton conversion? Or somehow who
    knows more might fill in here.
  created_at: 2023-05-10 17:43:39+00:00
  edited: false
  hidden: false
  id: 645be5dbc971fbab7422b076
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-10T19:02:47.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: "<p>Thanks I'm just happy I'm not the only one. I will try asking chat\
          \ GPT because google itself did not help me. To technical. lol And then\
          \ maybe reach out to <span data-props=\"{&quot;user&quot;:&quot;TheYuriLover&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheYuriLover\"\
          >@<span class=\"underline\">TheYuriLover</span></a></span>\n\n\t</span></span>\
          \ or hope that he sees this.</p>\n"
        raw: Thanks I'm just happy I'm not the only one. I will try asking chat GPT
          because google itself did not help me. To technical. lol And then maybe
          reach out to @TheYuriLover or hope that he sees this.
        updatedAt: '2023-05-10T19:02:47.880Z'
      numEdits: 0
      reactions: []
    id: 645bea57b396a40a849339c8
    type: comment
  author: Goldenblood56
  content: Thanks I'm just happy I'm not the only one. I will try asking chat GPT
    because google itself did not help me. To technical. lol And then maybe reach
    out to @TheYuriLover or hope that he sees this.
  created_at: 2023-05-10 18:02:47+00:00
  edited: false
  hidden: false
  id: 645bea57b396a40a849339c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-05-10T19:04:55.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>The thing is that only triton makes the groupsize 128 + act order
          work on ooba''s webui, cuda can''t or is too slow for it.<br>Having the
          act_order is an advantage because it gives a better quantization quality,
          the perplexity is closer to f16 if you add act_order</p>

          <p>Look the readame: <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda">https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda</a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/72M7ukpm78XuFgYlUp6vS.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/72M7ukpm78XuFgYlUp6vS.png"></a></p>

          '
        raw: 'The thing is that only triton makes the groupsize 128 + act order work
          on ooba''s webui, cuda can''t or is too slow for it.

          Having the act_order is an advantage because it gives a better quantization
          quality, the perplexity is closer to f16 if you add act_order


          Look the readame: https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/72M7ukpm78XuFgYlUp6vS.png)'
        updatedAt: '2023-05-10T19:06:02.850Z'
      numEdits: 1
      reactions: []
    id: 645bead7b396a40a84933e54
    type: comment
  author: TheYuriLover
  content: 'The thing is that only triton makes the groupsize 128 + act order work
    on ooba''s webui, cuda can''t or is too slow for it.

    Having the act_order is an advantage because it gives a better quantization quality,
    the perplexity is closer to f16 if you add act_order


    Look the readame: https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/72M7ukpm78XuFgYlUp6vS.png)'
  created_at: 2023-05-10 18:04:55+00:00
  edited: true
  hidden: false
  id: 645bead7b396a40a84933e54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-10T19:42:28.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks Yuri I copy and pasted our conversation here into chat GTP
          including what you said. And then asked a few questions. I think I kind
          of get it between you and chat GTP. </p>

          <p>I have one last question should I add --act-order and --true-sequential
          to my arguments? </p>

          <p>This is what chat GTP said about Cuda vs Triton. Not sure how much of
          that is true and there may be more to it. But I think I at least get it.
          It''s kind of like OpenGL VS Vulkan etc. Pros and cons to each I guess.<br>"Yes,
          CUDA and Triton are both software frameworks used for GPU-accelerated deep
          learning inference.</p>

          <p>CUDA is a software platform developed by NVIDIA that enables developers
          to use NVIDIA GPUs to accelerate compute-intensive applications. It includes
          libraries, tools, and APIs for developing and deploying deep learning models
          on NVIDIA GPUs.</p>

          <p>Triton, on the other hand, is a deep learning inference server developed
          by NVIDIA that provides a flexible and efficient way to deploy and serve
          deep learning models. It supports multiple deep learning frameworks and
          allows users to deploy models on any GPU or CPU in their data center or
          cloud environment.</p>

          <p>Both frameworks have their own advantages and disadvantages, and the
          choice of which to use may depend on factors such as the specific use case,
          the available hardware, and the expertise of the development team."</p>

          '
        raw: "Thanks Yuri I copy and pasted our conversation here into chat GTP including\
          \ what you said. And then asked a few questions. I think I kind of get it\
          \ between you and chat GTP. \n\nI have one last question should I add --act-order\
          \ and --true-sequential to my arguments? \n\nThis is what chat GTP said\
          \ about Cuda vs Triton. Not sure how much of that is true and there may\
          \ be more to it. But I think I at least get it. It's kind of like OpenGL\
          \ VS Vulkan etc. Pros and cons to each I guess. \n\"Yes, CUDA and Triton\
          \ are both software frameworks used for GPU-accelerated deep learning inference.\n\
          \nCUDA is a software platform developed by NVIDIA that enables developers\
          \ to use NVIDIA GPUs to accelerate compute-intensive applications. It includes\
          \ libraries, tools, and APIs for developing and deploying deep learning\
          \ models on NVIDIA GPUs.\n\nTriton, on the other hand, is a deep learning\
          \ inference server developed by NVIDIA that provides a flexible and efficient\
          \ way to deploy and serve deep learning models. It supports multiple deep\
          \ learning frameworks and allows users to deploy models on any GPU or CPU\
          \ in their data center or cloud environment.\n\nBoth frameworks have their\
          \ own advantages and disadvantages, and the choice of which to use may depend\
          \ on factors such as the specific use case, the available hardware, and\
          \ the expertise of the development team.\""
        updatedAt: '2023-05-10T19:42:28.797Z'
      numEdits: 0
      reactions: []
    id: 645bf3a4b396a40a84938f85
    type: comment
  author: Goldenblood56
  content: "Thanks Yuri I copy and pasted our conversation here into chat GTP including\
    \ what you said. And then asked a few questions. I think I kind of get it between\
    \ you and chat GTP. \n\nI have one last question should I add --act-order and\
    \ --true-sequential to my arguments? \n\nThis is what chat GTP said about Cuda\
    \ vs Triton. Not sure how much of that is true and there may be more to it. But\
    \ I think I at least get it. It's kind of like OpenGL VS Vulkan etc. Pros and\
    \ cons to each I guess. \n\"Yes, CUDA and Triton are both software frameworks\
    \ used for GPU-accelerated deep learning inference.\n\nCUDA is a software platform\
    \ developed by NVIDIA that enables developers to use NVIDIA GPUs to accelerate\
    \ compute-intensive applications. It includes libraries, tools, and APIs for developing\
    \ and deploying deep learning models on NVIDIA GPUs.\n\nTriton, on the other hand,\
    \ is a deep learning inference server developed by NVIDIA that provides a flexible\
    \ and efficient way to deploy and serve deep learning models. It supports multiple\
    \ deep learning frameworks and allows users to deploy models on any GPU or CPU\
    \ in their data center or cloud environment.\n\nBoth frameworks have their own\
    \ advantages and disadvantages, and the choice of which to use may depend on factors\
    \ such as the specific use case, the available hardware, and the expertise of\
    \ the development team.\""
  created_at: 2023-05-10 18:42:28+00:00
  edited: false
  hidden: false
  id: 645bf3a4b396a40a84938f85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-05-10T19:48:14.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<blockquote>

          <p>I have one last question should I add --act-order and --true-sequential
          to my arguments?</p>

          </blockquote>

          <p>You don''t need to do anything, the act_order and true_sequential are
          implemented on the triton quantized model, just run it normally and you''re
          good to go</p>

          '
        raw: '>I have one last question should I add --act-order and --true-sequential
          to my arguments?


          You don''t need to do anything, the act_order and true_sequential are implemented
          on the triton quantized model, just run it normally and you''re good to
          go'
        updatedAt: '2023-05-10T19:48:14.187Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Goldenblood56
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Goldenblood56
    id: 645bf4feb396a40a84939c6b
    type: comment
  author: TheYuriLover
  content: '>I have one last question should I add --act-order and --true-sequential
    to my arguments?


    You don''t need to do anything, the act_order and true_sequential are implemented
    on the triton quantized model, just run it normally and you''re good to go'
  created_at: 2023-05-10 18:48:14+00:00
  edited: false
  hidden: false
  id: 645bf4feb396a40a84939c6b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: reeducator/vicuna-13b-cocktail
repo_type: model
status: open
target_branch: null
title: 'Sorry Triton VS Cuda what is the difference? '
