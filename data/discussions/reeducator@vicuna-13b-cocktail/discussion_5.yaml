!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deleted
conflicting_files: null
created_at: 2023-05-10 07:38:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-10T08:38:57.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: "<p><a href=\"https://huggingface.co/ehartford/WizardLM-7B-Uncensored/discussions/10\"\
          >https://huggingface.co/ehartford/WizardLM-7B-Uncensored/discussions/10</a></p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/reeducator\">@<span class=\"\
          underline\">reeducator</span></a></span>\n\n\t</span></span> Just linking\
          \ this thread from Henk about overtraining. Some extra reading direction\
          \ for model stickiness assuming you haven't been messing with dials and\
          \ knobs already around these things.</p>\n"
        raw: "https://huggingface.co/ehartford/WizardLM-7B-Uncensored/discussions/10\r\
          \n\r\n@reeducator Just linking this thread from Henk about overtraining.\
          \ Some extra reading direction for model stickiness assuming you haven't\
          \ been messing with dials and knobs already around these things."
        updatedAt: '2023-05-10T08:38:57.460Z'
      numEdits: 0
      reactions: []
    id: 645b5821bccccb946f800a35
    type: comment
  author: deleted
  content: "https://huggingface.co/ehartford/WizardLM-7B-Uncensored/discussions/10\r\
    \n\r\n@reeducator Just linking this thread from Henk about overtraining. Some\
    \ extra reading direction for model stickiness assuming you haven't been messing\
    \ with dials and knobs already around these things."
  created_at: 2023-05-10 07:38:57+00:00
  edited: false
  hidden: false
  id: 645b5821bccccb946f800a35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-10T14:56:19.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Thanks @gozfarb. Our hyperparams are still default from the vicuna
          branch. Learning rate is 2e-5 with cosine scheduling, i.e. the LR goes orders
          of magnitude lower towards the end of the training. We might keep it as
          it is for now, unless someone points out that the model we have here suffers
          from similar issues. For Vicuna I can''t lower it much, since the training
          already takes quite some time, but if necessary, there''s still room for
          longer training periods.</p>

          '
        raw: Thanks @gozfarb. Our hyperparams are still default from the vicuna branch.
          Learning rate is 2e-5 with cosine scheduling, i.e. the LR goes orders of
          magnitude lower towards the end of the training. We might keep it as it
          is for now, unless someone points out that the model we have here suffers
          from similar issues. For Vicuna I can't lower it much, since the training
          already takes quite some time, but if necessary, there's still room for
          longer training periods.
        updatedAt: '2023-05-10T14:56:19.325Z'
      numEdits: 0
      reactions: []
    id: 645bb0932c76efd4c666650b
    type: comment
  author: reeducator
  content: Thanks @gozfarb. Our hyperparams are still default from the vicuna branch.
    Learning rate is 2e-5 with cosine scheduling, i.e. the LR goes orders of magnitude
    lower towards the end of the training. We might keep it as it is for now, unless
    someone points out that the model we have here suffers from similar issues. For
    Vicuna I can't lower it much, since the training already takes quite some time,
    but if necessary, there's still room for longer training periods.
  created_at: 2023-05-10 13:56:19+00:00
  edited: false
  hidden: false
  id: 645bb0932c76efd4c666650b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    createdAt: '2023-05-13T07:01:47.000Z'
    data:
      from: The Stickiness Promblem
      to: The Stickiness Problem
    id: 645f35db98464f9fcbfcbdcc
    type: title-change
  author: deleted
  created_at: 2023-05-13 06:01:47+00:00
  id: 645f35db98464f9fcbfcbdcc
  new_title: The Stickiness Problem
  old_title: The Stickiness Promblem
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: reeducator/vicuna-13b-cocktail
repo_type: model
status: open
target_branch: null
title: The Stickiness Problem
