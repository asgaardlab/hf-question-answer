!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-05-10 19:11:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-10T20:11:48.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>I tried in chat and instruct mode. Double checked my other models
          still work fine. I updated "Ooba"  </p>

          <p>Here is how I installed it.<br>I copy and pasted my "reeducator_vicuna-13b-cocktail"
          folder. Renamed the copy to "reeducator_vicuna-16b-cocktail_Tri"<br>put
          the Triton model in there and removed the former model so only the new one
          is there. </p>

          <p>Ran it with command line "call python server.py --auto-devices --chat
          --model reeducator_vicuna-16b-cocktail_Tri --wbits 4 --groupsize 128" same
          as before. It''s giving me gibberish? If anyone has any ideas please let
          me know. </p>

          <p>Here is what I suspect is the issue.</p>

          <ol>

          <li>I need to install or update sort sort of addon for Ooba something that
          does not install or update with git pull. </li>

          <li>The files that are in "Files and Versions"  tokenizer.model are not
          for the Triton model? Or there is some missing step? </li>

          <li>Something is off with my arguments?</li>

          </ol>

          <p>If no on here knows I will likely seek assistance on Ooba''s discussion
          link.  Thank you.</p>

          '
        raw: "I tried in chat and instruct mode. Double checked my other models still\
          \ work fine. I updated \"Ooba\"  \n\nHere is how I installed it. \nI copy\
          \ and pasted my \"reeducator_vicuna-13b-cocktail\" folder. Renamed the copy\
          \ to \"reeducator_vicuna-16b-cocktail_Tri\" \nput the Triton model in there\
          \ and removed the former model so only the new one is there. \n\nRan it\
          \ with command line \"call python server.py --auto-devices --chat --model\
          \ reeducator_vicuna-16b-cocktail_Tri --wbits 4 --groupsize 128\" same as\
          \ before. It's giving me gibberish? If anyone has any ideas please let me\
          \ know. \n\nHere is what I suspect is the issue.\n1. I need to install or\
          \ update sort sort of addon for Ooba something that does not install or\
          \ update with git pull. \n2. The files that are in \"Files and Versions\"\
          \  tokenizer.model are not for the Triton model? Or there is some missing\
          \ step? \n3. Something is off with my arguments? \n\nIf no on here knows\
          \ I will likely seek assistance on Ooba's discussion link.  Thank you."
        updatedAt: '2023-05-10T20:13:47.585Z'
      numEdits: 3
      reactions: []
    id: 645bfa84bccccb946f83e01c
    type: comment
  author: Goldenblood56
  content: "I tried in chat and instruct mode. Double checked my other models still\
    \ work fine. I updated \"Ooba\"  \n\nHere is how I installed it. \nI copy and\
    \ pasted my \"reeducator_vicuna-13b-cocktail\" folder. Renamed the copy to \"\
    reeducator_vicuna-16b-cocktail_Tri\" \nput the Triton model in there and removed\
    \ the former model so only the new one is there. \n\nRan it with command line\
    \ \"call python server.py --auto-devices --chat --model reeducator_vicuna-16b-cocktail_Tri\
    \ --wbits 4 --groupsize 128\" same as before. It's giving me gibberish? If anyone\
    \ has any ideas please let me know. \n\nHere is what I suspect is the issue.\n\
    1. I need to install or update sort sort of addon for Ooba something that does\
    \ not install or update with git pull. \n2. The files that are in \"Files and\
    \ Versions\"  tokenizer.model are not for the Triton model? Or there is some missing\
    \ step? \n3. Something is off with my arguments? \n\nIf no on here knows I will\
    \ likely seek assistance on Ooba's discussion link.  Thank you."
  created_at: 2023-05-10 19:11:48+00:00
  edited: true
  hidden: false
  id: 645bfa84bccccb946f83e01c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-11T13:59:18.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Have you actually switched to a Triton branch in your repositories/GPTQ-for-LLaMa
          ?</p>

          <p>This is the Triton branch: <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a>
          as per <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a></p>

          '
        raw: 'Have you actually switched to a Triton branch in your repositories/GPTQ-for-LLaMa
          ?


          This is the Triton branch: https://github.com/qwopqwop200/GPTQ-for-LLaMa
          as per https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md'
        updatedAt: '2023-05-11T13:59:18.962Z'
      numEdits: 0
      reactions: []
    id: 645cf4b6afa77201e2fa8ae8
    type: comment
  author: mancub
  content: 'Have you actually switched to a Triton branch in your repositories/GPTQ-for-LLaMa
    ?


    This is the Triton branch: https://github.com/qwopqwop200/GPTQ-for-LLaMa as per
    https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md'
  created_at: 2023-05-11 12:59:18+00:00
  edited: false
  hidden: false
  id: 645cf4b6afa77201e2fa8ae8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-11T16:09:59.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<blockquote>

          <p>Have you actually switched to a Triton branch in your repositories/GPTQ-for-LLaMa
          ?</p>

          <p>This is the Triton branch: <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a>
          as per <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a></p>

          </blockquote>

          <p>Thank you mancub.  That must be my issue. Since I''m a windows user I
          also need WSL it seems. Whatever that is. I may or may not try to set this
          up. It seems a little complicated and I can never find like a step by step
          guide on these pages. I usually don''t understand more than half of what
          I''m actually doing. So I will read though all of this another time. I like
          finding like a Youtube video of someone just walking us though it. That''s
          how I figured out Stable Diffusion and most of this AI stuff. Just following
          Youtube guides. But I don''t think any of the AI channels have setup this
          yet. Thanks again I''m used to figuring it out as I go so I will get around
          to it eventually.</p>

          '
        raw: "> Have you actually switched to a Triton branch in your repositories/GPTQ-for-LLaMa\
          \ ?\n> \n> This is the Triton branch: https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ as per https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md\n\
          \nThank you mancub.  That must be my issue. Since I'm a windows user I also\
          \ need WSL it seems. Whatever that is. I may or may not try to set this\
          \ up. It seems a little complicated and I can never find like a step by\
          \ step guide on these pages. I usually don't understand more than half of\
          \ what I'm actually doing. So I will read though all of this another time.\
          \ I like finding like a Youtube video of someone just walking us though\
          \ it. That's how I figured out Stable Diffusion and most of this AI stuff.\
          \ Just following Youtube guides. But I don't think any of the AI channels\
          \ have setup this yet. Thanks again I'm used to figuring it out as I go\
          \ so I will get around to it eventually."
        updatedAt: '2023-05-11T16:12:02.280Z'
      numEdits: 1
      reactions: []
    id: 645d13578ce4443cae713a33
    type: comment
  author: Goldenblood56
  content: "> Have you actually switched to a Triton branch in your repositories/GPTQ-for-LLaMa\
    \ ?\n> \n> This is the Triton branch: https://github.com/qwopqwop200/GPTQ-for-LLaMa\
    \ as per https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md\n\
    \nThank you mancub.  That must be my issue. Since I'm a windows user I also need\
    \ WSL it seems. Whatever that is. I may or may not try to set this up. It seems\
    \ a little complicated and I can never find like a step by step guide on these\
    \ pages. I usually don't understand more than half of what I'm actually doing.\
    \ So I will read though all of this another time. I like finding like a Youtube\
    \ video of someone just walking us though it. That's how I figured out Stable\
    \ Diffusion and most of this AI stuff. Just following Youtube guides. But I don't\
    \ think any of the AI channels have setup this yet. Thanks again I'm used to figuring\
    \ it out as I go so I will get around to it eventually."
  created_at: 2023-05-11 15:09:59+00:00
  edited: true
  hidden: false
  id: 645d13578ce4443cae713a33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-12T00:38:08.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I''m the same way, figuring out as I go. Can''t be bothered watching
          instructional YT videos though because they are too long and I only need
          a tldr; most of the time.</p>

          <p>I am running WSL and have no issues. Matter a fact I interchange between
          CUDA and Triton branches for oobabooga, depending on the model I use.</p>

          <p>Setting WSL up is not hard and definitely worth the effort because, like
          you, I tried running in native Windows at first and it was such a pain to
          get everything compiled and working. I gave up after spending couple of
          days trying to compile BitsAndBytes and make it work. Went ahead and installed
          Ubuntu in WSL. Already had Debian running for other things so the basic
          setup was there.</p>

          <p>If you start from <a rel="nofollow" href="https://learn.microsoft.com/en-us/windows/wsl/install">https://learn.microsoft.com/en-us/windows/wsl/install</a>
          or <a rel="nofollow" href="https://learn.microsoft.com/en-us/windows/wsl/install-manual">https://learn.microsoft.com/en-us/windows/wsl/install-manual</a>
          you will be up and running with WSL in not time.</p>

          '
        raw: 'I''m the same way, figuring out as I go. Can''t be bothered watching
          instructional YT videos though because they are too long and I only need
          a tldr; most of the time.


          I am running WSL and have no issues. Matter a fact I interchange between
          CUDA and Triton branches for oobabooga, depending on the model I use.


          Setting WSL up is not hard and definitely worth the effort because, like
          you, I tried running in native Windows at first and it was such a pain to
          get everything compiled and working. I gave up after spending couple of
          days trying to compile BitsAndBytes and make it work. Went ahead and installed
          Ubuntu in WSL. Already had Debian running for other things so the basic
          setup was there.


          If you start from https://learn.microsoft.com/en-us/windows/wsl/install
          or https://learn.microsoft.com/en-us/windows/wsl/install-manual you will
          be up and running with WSL in not time.'
        updatedAt: '2023-05-12T00:38:08.179Z'
      numEdits: 0
      reactions: []
    id: 645d8a708a63155046dfa778
    type: comment
  author: mancub
  content: 'I''m the same way, figuring out as I go. Can''t be bothered watching instructional
    YT videos though because they are too long and I only need a tldr; most of the
    time.


    I am running WSL and have no issues. Matter a fact I interchange between CUDA
    and Triton branches for oobabooga, depending on the model I use.


    Setting WSL up is not hard and definitely worth the effort because, like you,
    I tried running in native Windows at first and it was such a pain to get everything
    compiled and working. I gave up after spending couple of days trying to compile
    BitsAndBytes and make it work. Went ahead and installed Ubuntu in WSL. Already
    had Debian running for other things so the basic setup was there.


    If you start from https://learn.microsoft.com/en-us/windows/wsl/install or https://learn.microsoft.com/en-us/windows/wsl/install-manual
    you will be up and running with WSL in not time.'
  created_at: 2023-05-11 23:38:08+00:00
  edited: false
  hidden: false
  id: 645d8a708a63155046dfa778
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: reeducator/vicuna-13b-cocktail
repo_type: model
status: open
target_branch: null
title: 'Great triton-4bit-128g.safetensors is giving me giberish? '
