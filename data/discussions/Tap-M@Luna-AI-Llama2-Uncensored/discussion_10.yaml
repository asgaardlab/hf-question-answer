!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-09-08 12:01:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-09-08T13:01:14.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772180914878845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I asked an innocuous question about who was Alan Harper''s ex-wife
          on Two and a Half Men and it gave me attitude about respecting the private
          lives of celebrities. I asked no remotely inappropriate questions by this
          point, including about any real-life person, and just asked questions about
          TV shows to test the model.  Curious, since this didn''t happen with the
          original Llama, I went online and found out Meta pre-censored the model
          so it can''t be uncensored. For example, you can ask it how to break into
          a car, and it won''t refuse, but will only say use a rock. And if you ask
          about if this or that celebrity hooked up it won''t have an answer even
          though it''s common knowledge and answered by even unfiltered Llama 1. This
          is also obviously why the hallucination rate skyrockets whenever you ask
          something that remotely relates to a contentious subject matter, for example,
          due to an accidental misinterpretation because a word has multiple meanings,
          or in the example above, talking about the fictional lives of TV characters
          in a way that can be perceived as celebrity gossip.</p>

          '
        raw: I asked an innocuous question about who was Alan Harper's ex-wife on
          Two and a Half Men and it gave me attitude about respecting the private
          lives of celebrities. I asked no remotely inappropriate questions by this
          point, including about any real-life person, and just asked questions about
          TV shows to test the model.  Curious, since this didn't happen with the
          original Llama, I went online and found out Meta pre-censored the model
          so it can't be uncensored. For example, you can ask it how to break into
          a car, and it won't refuse, but will only say use a rock. And if you ask
          about if this or that celebrity hooked up it won't have an answer even though
          it's common knowledge and answered by even unfiltered Llama 1. This is also
          obviously why the hallucination rate skyrockets whenever you ask something
          that remotely relates to a contentious subject matter, for example, due
          to an accidental misinterpretation because a word has multiple meanings,
          or in the example above, talking about the fictional lives of TV characters
          in a way that can be perceived as celebrity gossip.
        updatedAt: '2023-09-08T13:01:14.023Z'
      numEdits: 0
      reactions: []
    id: 64fb1b1ad75293f417d3d464
    type: comment
  author: Phil337
  content: I asked an innocuous question about who was Alan Harper's ex-wife on Two
    and a Half Men and it gave me attitude about respecting the private lives of celebrities.
    I asked no remotely inappropriate questions by this point, including about any
    real-life person, and just asked questions about TV shows to test the model.  Curious,
    since this didn't happen with the original Llama, I went online and found out
    Meta pre-censored the model so it can't be uncensored. For example, you can ask
    it how to break into a car, and it won't refuse, but will only say use a rock.
    And if you ask about if this or that celebrity hooked up it won't have an answer
    even though it's common knowledge and answered by even unfiltered Llama 1. This
    is also obviously why the hallucination rate skyrockets whenever you ask something
    that remotely relates to a contentious subject matter, for example, due to an
    accidental misinterpretation because a word has multiple meanings, or in the example
    above, talking about the fictional lives of TV characters in a way that can be
    perceived as celebrity gossip.
  created_at: 2023-09-08 12:01:14+00:00
  edited: false
  hidden: false
  id: 64fb1b1ad75293f417d3d464
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: Tap-M/Luna-AI-Llama2-Uncensored
repo_type: model
status: open
target_branch: null
title: Not Uncensored Because Meta Pre-Filtered
