!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jackboot
conflicting_files: null
created_at: 2023-12-28 18:02:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2023-12-28T18:02:45.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9527864456176758
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>The model itself I think is 45.1g, will it squeeze into 48gb with
          the full 4096 if you have flash attention? I''m not planning on roping it
          but if it requires 64 or 56 for at least full ctx I''ll grab the smaller
          one.</p>

          '
        raw: The model itself I think is 45.1g, will it squeeze into 48gb with the
          full 4096 if you have flash attention? I'm not planning on roping it but
          if it requires 64 or 56 for at least full ctx I'll grab the smaller one.
        updatedAt: '2023-12-28T18:02:45.873Z'
      numEdits: 0
      reactions: []
    id: 658db845a9ba8a30cd51c3d4
    type: comment
  author: jackboot
  content: The model itself I think is 45.1g, will it squeeze into 48gb with the full
    4096 if you have flash attention? I'm not planning on roping it but if it requires
    64 or 56 for at least full ctx I'll grab the smaller one.
  created_at: 2023-12-28 18:02:45+00:00
  edited: false
  hidden: false
  id: 658db845a9ba8a30cd51c3d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/55b60a97e677e7249611fdb4195d63df.svg
      fullname: hrtdind
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hrtdind
      type: user
    createdAt: '2023-12-28T22:06:09.000Z'
    data:
      edited: false
      editors:
      - hrtdind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6861556172370911
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/55b60a97e677e7249611fdb4195d63df.svg
          fullname: hrtdind
          isHf: false
          isPro: false
          name: hrtdind
          type: user
        html: '<p>With FP8 KV cache and 8192 context size, it fit in A6000, taking
          47gb out of 48gb.</p>

          '
        raw: With FP8 KV cache and 8192 context size, it fit in A6000, taking 47gb
          out of 48gb.
        updatedAt: '2023-12-28T22:06:09.974Z'
      numEdits: 0
      reactions: []
    id: 658df15189f1ff04630ba084
    type: comment
  author: hrtdind
  content: With FP8 KV cache and 8192 context size, it fit in A6000, taking 47gb out
    of 48gb.
  created_at: 2023-12-28 22:06:09+00:00
  edited: false
  hidden: false
  id: 658df15189f1ff04630ba084
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2023-12-28T22:12:30.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.882095992565155
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>thanks.. hope the overhead doesn''t  get me on 2x24 but i''m gonna
          queue it up to d/l overnight</p>

          '
        raw: thanks.. hope the overhead doesn't  get me on 2x24 but i'm gonna queue
          it up to d/l overnight
        updatedAt: '2023-12-28T22:12:30.604Z'
      numEdits: 0
      reactions: []
    id: 658df2cedfca9fad61ef0ddd
    type: comment
  author: jackboot
  content: thanks.. hope the overhead doesn't  get me on 2x24 but i'm gonna queue
    it up to d/l overnight
  created_at: 2023-12-28 22:12:30+00:00
  edited: false
  hidden: false
  id: 658df2cedfca9fad61ef0ddd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-29T04:22:03.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9375578165054321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>It''s a very tight fit. I have to use GPU split: <code>21.75,23</code>
          to load with fp16 cache and default 4k context. I have nothing else running
          on either 3090 card; running headless with no desktop or apps at all other
          than ooba.  The model seems reasonably good even compared to other 70Bs
          even at 4.65bpw.</p>

          '
        raw: 'It''s a very tight fit. I have to use GPU split: `21.75,23` to load
          with fp16 cache and default 4k context. I have nothing else running on either
          3090 card; running headless with no desktop or apps at all other than ooba.  The
          model seems reasonably good even compared to other 70Bs even at 4.65bpw.'
        updatedAt: '2023-12-29T04:22:03.142Z'
      numEdits: 0
      reactions: []
    id: 658e496b9e16fa7510fac69f
    type: comment
  author: LoneStriker
  content: 'It''s a very tight fit. I have to use GPU split: `21.75,23` to load with
    fp16 cache and default 4k context. I have nothing else running on either 3090
    card; running headless with no desktop or apps at all other than ooba.  The model
    seems reasonably good even compared to other 70Bs even at 4.65bpw.'
  created_at: 2023-12-29 04:22:03+00:00
  edited: false
  hidden: false
  id: 658e496b9e16fa7510fac69f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2023-12-29T11:57:18.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9541901350021362
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>I just loaded it in tabby, auto split lets you squeeze more than
          in textgen, i''m not sure why. I haven''t tried 8k but for 4k I have a 98/94
          split. I think I can fit more context crazy enough. Initial impression of
          model is good.</p>

          '
        raw: I just loaded it in tabby, auto split lets you squeeze more than in textgen,
          i'm not sure why. I haven't tried 8k but for 4k I have a 98/94 split. I
          think I can fit more context crazy enough. Initial impression of model is
          good.
        updatedAt: '2023-12-29T11:57:18.121Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 658eb41e754092f6b14e67cb
    type: comment
  author: jackboot
  content: I just loaded it in tabby, auto split lets you squeeze more than in textgen,
    i'm not sure why. I haven't tried 8k but for 4k I have a 98/94 split. I think
    I can fit more context crazy enough. Initial impression of model is good.
  created_at: 2023-12-29 11:57:18+00:00
  edited: false
  hidden: false
  id: 658eb41e754092f6b14e67cb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Aurora-Nights-103B-v1.0-3.5bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Will it squeeze into 48?
