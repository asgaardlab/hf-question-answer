!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HemanthSai7
conflicting_files: null
created_at: 2023-10-26 13:03:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
      fullname: Hemanth Sai Garladinne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HemanthSai7
      type: user
    createdAt: '2023-10-26T14:03:59.000Z'
    data:
      edited: false
      editors:
      - HemanthSai7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9715341329574585
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
          fullname: Hemanth Sai Garladinne
          isHf: false
          isPro: false
          name: HemanthSai7
          type: user
        html: '<p>The results are so funny. I wanted to know how you trained the SDXL
          model on the custom dataset. I''m a beginner in training LLMs and want to
          learn. Could you please share some resources since I''m planning to build
          a project similar to this?</p>

          '
        raw: The results are so funny. I wanted to know how you trained the SDXL model
          on the custom dataset. I'm a beginner in training LLMs and want to learn.
          Could you please share some resources since I'm planning to build a project
          similar to this?
        updatedAt: '2023-10-26T14:03:59.673Z'
      numEdits: 0
      reactions: []
    id: 653a71cfcda0df188d52670f
    type: comment
  author: HemanthSai7
  content: The results are so funny. I wanted to know how you trained the SDXL model
    on the custom dataset. I'm a beginner in training LLMs and want to learn. Could
    you please share some resources since I'm planning to build a project similar
    to this?
  created_at: 2023-10-26 13:03:59+00:00
  edited: false
  hidden: false
  id: 653a71cfcda0df188d52670f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiUqKWBWPjwdDKzuJb6El.png?w=200&h=200&f=face
      fullname: John Hastings
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flobbit
      type: user
    createdAt: '2023-10-26T14:47:10.000Z'
    data:
      edited: true
      editors:
      - flobbit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9598722457885742
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiUqKWBWPjwdDKzuJb6El.png?w=200&h=200&f=face
          fullname: John Hastings
          isHf: false
          isPro: false
          name: flobbit
          type: user
        html: '<p>I had a hard time figuring out how to do this as the instructions
          are pretty scarce or incomplete.  I''m away from my computer that I used
          to train the model, but let me share a screenshot of my directory structure
          tonight along with some tips.  The directory structure is almost the key
          thing along with the captioning in the directory containing the files.  Once
          you have that set up, you can use kohya for the training.  It''s pretty
          straighforward. I used a local GPU.  My original intent was to play with
          small LLMs and I''ve been sidetracked with images!!!</p>

          '
        raw: I had a hard time figuring out how to do this as the instructions are
          pretty scarce or incomplete.  I'm away from my computer that I used to train
          the model, but let me share a screenshot of my directory structure tonight
          along with some tips.  The directory structure is almost the key thing along
          with the captioning in the directory containing the files.  Once you have
          that set up, you can use kohya for the training.  It's pretty straighforward.
          I used a local GPU.  My original intent was to play with small LLMs and
          I've been sidetracked with images!!!
        updatedAt: '2023-10-26T14:47:56.202Z'
      numEdits: 1
      reactions: []
    id: 653a7bee4d105d9696b1bce1
    type: comment
  author: flobbit
  content: I had a hard time figuring out how to do this as the instructions are pretty
    scarce or incomplete.  I'm away from my computer that I used to train the model,
    but let me share a screenshot of my directory structure tonight along with some
    tips.  The directory structure is almost the key thing along with the captioning
    in the directory containing the files.  Once you have that set up, you can use
    kohya for the training.  It's pretty straighforward. I used a local GPU.  My original
    intent was to play with small LLMs and I've been sidetracked with images!!!
  created_at: 2023-10-26 13:47:10+00:00
  edited: true
  hidden: false
  id: 653a7bee4d105d9696b1bce1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
      fullname: Hemanth Sai Garladinne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HemanthSai7
      type: user
    createdAt: '2023-10-26T15:15:28.000Z'
    data:
      edited: false
      editors:
      - HemanthSai7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9779742956161499
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
          fullname: Hemanth Sai Garladinne
          isHf: false
          isPro: false
          name: HemanthSai7
          type: user
        html: '<p>Thank you so much. It would be really helpful. Did you create the
          dataset from scratch or was the dataset available?</p>

          '
        raw: Thank you so much. It would be really helpful. Did you create the dataset
          from scratch or was the dataset available?
        updatedAt: '2023-10-26T15:15:28.304Z'
      numEdits: 0
      reactions: []
    id: 653a82901e1140b0c7de03e1
    type: comment
  author: HemanthSai7
  content: Thank you so much. It would be really helpful. Did you create the dataset
    from scratch or was the dataset available?
  created_at: 2023-10-26 14:15:28+00:00
  edited: false
  hidden: false
  id: 653a82901e1140b0c7de03e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiUqKWBWPjwdDKzuJb6El.png?w=200&h=200&f=face
      fullname: John Hastings
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flobbit
      type: user
    createdAt: '2023-10-26T15:25:22.000Z'
    data:
      edited: false
      editors:
      - flobbit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9766045212745667
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiUqKWBWPjwdDKzuJb6El.png?w=200&h=200&f=face
          fullname: John Hastings
          isHf: false
          isPro: false
          name: flobbit
          type: user
        html: '<p>I trained from images posted to the net.   It''s doesn''t take a
          huge number of images.  I''ll try to do a screenshot of my directory structure
          and I think that will show how many images I used.</p>

          '
        raw: I trained from images posted to the net.   It's doesn't take a huge number
          of images.  I'll try to do a screenshot of my directory structure and I
          think that will show how many images I used.
        updatedAt: '2023-10-26T15:25:22.578Z'
      numEdits: 0
      reactions: []
    id: 653a84e2e81b2b0dfb769cf9
    type: comment
  author: flobbit
  content: I trained from images posted to the net.   It's doesn't take a huge number
    of images.  I'll try to do a screenshot of my directory structure and I think
    that will show how many images I used.
  created_at: 2023-10-26 14:25:22+00:00
  edited: false
  hidden: false
  id: 653a84e2e81b2b0dfb769cf9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
      fullname: Hemanth Sai Garladinne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HemanthSai7
      type: user
    createdAt: '2023-10-26T15:32:27.000Z'
    data:
      edited: false
      editors:
      - HemanthSai7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.816265344619751
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
          fullname: Hemanth Sai Garladinne
          isHf: false
          isPro: false
          name: HemanthSai7
          type: user
        html: '<p>Oh okay, thank you!</p>

          '
        raw: Oh okay, thank you!
        updatedAt: '2023-10-26T15:32:27.050Z'
      numEdits: 0
      reactions: []
    id: 653a868beacd31a26d683fc7
    type: comment
  author: HemanthSai7
  content: Oh okay, thank you!
  created_at: 2023-10-26 14:32:27+00:00
  edited: false
  hidden: false
  id: 653a868beacd31a26d683fc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiUqKWBWPjwdDKzuJb6El.png?w=200&h=200&f=face
      fullname: John Hastings
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flobbit
      type: user
    createdAt: '2023-10-26T22:02:12.000Z'
    data:
      edited: false
      editors:
      - flobbit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9540983438491821
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiUqKWBWPjwdDKzuJb6El.png?w=200&h=200&f=face
          fullname: John Hastings
          isHf: false
          isPro: false
          name: flobbit
          type: user
        html: '<p>Here are some further details.  I hope this helps.  I used kohya
          to train. Specifically the Lora tab and not the Dreambooth tab.   I have
          uploaded my kohya settings file so you can look at that. You can load it
          into Kohya and it should give you something to start with.</p>

          <p>Here is my directory structure.  The number in front of the one directory
          is important. You can play around with that to see what works best for you.  I''ve
          used different numbers there.  2 or 3 or...  Some people set this based
          on some calculations (like 1500 divided by the number of training images)
          but I don''t know that it always works that way. Looks like I had 243 images
          to train this model but you might not need nearly so many depending on what
          you are training.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/AaS5wjWjYCmtZdlcAxZPB.png"><img
          alt="dirstructure.png" src="https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/AaS5wjWjYCmtZdlcAxZPB.png"></a><br>I
          used WD14 captioning on this dataset.  You can go through the files and
          make sure the captions make sense.  I think I was hurrying on this one and
          didn''t do that... and the results were good enough so I called it good.
          In the captioning settings, you can specify a prefix and I used the following
          (I''m using a completely weird trigger word so that SD can learn a new class
          and not rely on what it thinks it knows about an existing class that it
          has already been trained on): m0nstrcrz, caricature,</p>

          <p>Run it and see what you get.  If you see things repeatedly that you don''t
          want, you can also specify things you don''t want to see in the captions.  There
          is a windows program that allows you to scroll through your captions, but
          I am on linux so I usually just use command line to fiddle with things.  You
          can also use Blip Captioning if you want more English style captions.  I
          have been using more of those lately but I''m not sure what is best.  Here
          is a sample of a WD14 caption for this model:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/51xi6opUbjbTDjH072DUC.png"><img
          alt="captions.png" src="https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/51xi6opUbjbTDjH072DUC.png"></a><br>When
          you run the training, you can also have it generate sample images periodically,
          e.g., after each epoch or after X number of steps.  I usually do it after
          every one or two epochs depending on how long they take.  This shows me
          if I''m getting something reasonable.  Under the Lora tab/ Parameters/ Samples
          you can specific that, and give some prompts to generate images.  </p>

          <p>My settings might be running the training for a lot of epochs.  It depends
          on the model, but too many can create a model that causes SD to become really
          bad at a lot of things.  So, it''s a balancing act.  Just enough training
          to get it to learn your concept well enough (and play well with other models
          too) without destroying the rest of stable diffusion.</p>

          '
        raw: "Here are some further details.  I hope this helps.  I used kohya to\
          \ train. Specifically the Lora tab and not the Dreambooth tab.   I have\
          \ uploaded my kohya settings file so you can look at that. You can load\
          \ it into Kohya and it should give you something to start with.\n\nHere\
          \ is my directory structure.  The number in front of the one directory is\
          \ important. You can play around with that to see what works best for you.\
          \  I've used different numbers there.  2 or 3 or...  Some people set this\
          \ based on some calculations (like 1500 divided by the number of training\
          \ images) but I don't know that it always works that way. Looks like I had\
          \ 243 images to train this model but you might not need nearly so many depending\
          \ on what you are training.\n\n![dirstructure.png](https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/AaS5wjWjYCmtZdlcAxZPB.png)\n\
          I used WD14 captioning on this dataset.  You can go through the files and\
          \ make sure the captions make sense.  I think I was hurrying on this one\
          \ and didn't do that... and the results were good enough so I called it\
          \ good. In the captioning settings, you can specify a prefix and I used\
          \ the following (I'm using a completely weird trigger word so that SD can\
          \ learn a new class and not rely on what it thinks it knows about an existing\
          \ class that it has already been trained on): m0nstrcrz, caricature,\n\n\
          Run it and see what you get.  If you see things repeatedly that you don't\
          \ want, you can also specify things you don't want to see in the captions.\
          \  There is a windows program that allows you to scroll through your captions,\
          \ but I am on linux so I usually just use command line to fiddle with things.\
          \  You can also use Blip Captioning if you want more English style captions.\
          \  I have been using more of those lately but I'm not sure what is best.\
          \  Here is a sample of a WD14 caption for this model:\n![captions.png](https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/51xi6opUbjbTDjH072DUC.png)\n\
          When you run the training, you can also have it generate sample images periodically,\
          \ e.g., after each epoch or after X number of steps.  I usually do it after\
          \ every one or two epochs depending on how long they take.  This shows me\
          \ if I'm getting something reasonable.  Under the Lora tab/ Parameters/\
          \ Samples you can specific that, and give some prompts to generate images.\
          \  \n\nMy settings might be running the training for a lot of epochs.  It\
          \ depends on the model, but too many can create a model that causes SD to\
          \ become really bad at a lot of things.  So, it's a balancing act.  Just\
          \ enough training to get it to learn your concept well enough (and play\
          \ well with other models too) without destroying the rest of stable diffusion."
        updatedAt: '2023-10-26T22:02:12.214Z'
      numEdits: 0
      reactions: []
    id: 653ae1e4d4e4e0fd6c90705c
    type: comment
  author: flobbit
  content: "Here are some further details.  I hope this helps.  I used kohya to train.\
    \ Specifically the Lora tab and not the Dreambooth tab.   I have uploaded my kohya\
    \ settings file so you can look at that. You can load it into Kohya and it should\
    \ give you something to start with.\n\nHere is my directory structure.  The number\
    \ in front of the one directory is important. You can play around with that to\
    \ see what works best for you.  I've used different numbers there.  2 or 3 or...\
    \  Some people set this based on some calculations (like 1500 divided by the number\
    \ of training images) but I don't know that it always works that way. Looks like\
    \ I had 243 images to train this model but you might not need nearly so many depending\
    \ on what you are training.\n\n![dirstructure.png](https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/AaS5wjWjYCmtZdlcAxZPB.png)\n\
    I used WD14 captioning on this dataset.  You can go through the files and make\
    \ sure the captions make sense.  I think I was hurrying on this one and didn't\
    \ do that... and the results were good enough so I called it good. In the captioning\
    \ settings, you can specify a prefix and I used the following (I'm using a completely\
    \ weird trigger word so that SD can learn a new class and not rely on what it\
    \ thinks it knows about an existing class that it has already been trained on):\
    \ m0nstrcrz, caricature,\n\nRun it and see what you get.  If you see things repeatedly\
    \ that you don't want, you can also specify things you don't want to see in the\
    \ captions.  There is a windows program that allows you to scroll through your\
    \ captions, but I am on linux so I usually just use command line to fiddle with\
    \ things.  You can also use Blip Captioning if you want more English style captions.\
    \  I have been using more of those lately but I'm not sure what is best.  Here\
    \ is a sample of a WD14 caption for this model:\n![captions.png](https://cdn-uploads.huggingface.co/production/uploads/647f60634da748fc4abd8efc/51xi6opUbjbTDjH072DUC.png)\n\
    When you run the training, you can also have it generate sample images periodically,\
    \ e.g., after each epoch or after X number of steps.  I usually do it after every\
    \ one or two epochs depending on how long they take.  This shows me if I'm getting\
    \ something reasonable.  Under the Lora tab/ Parameters/ Samples you can specific\
    \ that, and give some prompts to generate images.  \n\nMy settings might be running\
    \ the training for a lot of epochs.  It depends on the model, but too many can\
    \ create a model that causes SD to become really bad at a lot of things.  So,\
    \ it's a balancing act.  Just enough training to get it to learn your concept\
    \ well enough (and play well with other models too) without destroying the rest\
    \ of stable diffusion."
  created_at: 2023-10-26 21:02:12+00:00
  edited: false
  hidden: false
  id: 653ae1e4d4e4e0fd6c90705c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
      fullname: Hemanth Sai Garladinne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HemanthSai7
      type: user
    createdAt: '2023-10-27T06:00:43.000Z'
    data:
      edited: false
      editors:
      - HemanthSai7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.955639123916626
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6350d53289def14ad21e0f68/4lsL98mXUQ20mLzUkU82y.png?w=200&h=200&f=face
          fullname: Hemanth Sai Garladinne
          isHf: false
          isPro: false
          name: HemanthSai7
          type: user
        html: "<p>Lots of information to process to\U0001F605. I'll start with this\
          \ and will let you know if I have any further questions. Thank you so much.</p>\n"
        raw: "Lots of information to process to\U0001F605. I'll start with this and\
          \ will let you know if I have any further questions. Thank you so much."
        updatedAt: '2023-10-27T06:00:43.911Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flobbit
    id: 653b520b9388c917d0f3d260
    type: comment
  author: HemanthSai7
  content: "Lots of information to process to\U0001F605. I'll start with this and\
    \ will let you know if I have any further questions. Thank you so much."
  created_at: 2023-10-27 05:00:43+00:00
  edited: false
  hidden: false
  id: 653b520b9388c917d0f3d260
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flobbit/monster-cars-sdxl-lora
repo_type: model
status: open
target_branch: null
title: Methodologies and Dataset
