!!python/object:huggingface_hub.community.DiscussionWithDetails
author: terilias
conflicting_files: null
created_at: 2024-01-10 10:58:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
      fullname: Terzis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terilias
      type: user
    createdAt: '2024-01-10T10:58:08.000Z'
    data:
      edited: false
      editors:
      - terilias
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8558209538459778
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
          fullname: Terzis
          isHf: false
          isPro: false
          name: terilias
          type: user
        html: '<p>Hello and thanks for publishing such great models!</p>

          <p>I''m using "bge-reranker-large" to rerank some text chunks after retrieving
          them from a RAG pipeline using "bge-large-en-v1.5". I am encountering the
          following error multiple times, indicating a token limit exceed: {''error'':
          ''Input validation error: inputs must have less than 512 tokens. Given:
          556'', ''error_type'': ''Validation''}.</p>

          <p>I am using the tokenizer from "bge-large-en-v1.5" for creating the chunks.
          However, I noticed that the "bge-reranker-large" tokenizer works differently,
          and some chunks appear to have a length &gt; 512 tokens with the reranker''s
          tokenizer, whereas their length is &lt;512 with the embedding''s tokenizer.
          Consequently, I decided to use the reranker''s tokenizer for chunk creation
          to avoid exceeding the limit. Despite this, I still encounter the same error.
          So I thought that I need to count the total tokens from both chunk and query.</p>

          <p>So, my question is: is the 512 tokens limit for each chunk or for the
          chunk-query pair? Do I need to limit the total length of the chunk and query
          tokens to 512 or just the length of the chunk?</p>

          '
        raw: "Hello and thanks for publishing such great models!\r\n\r\nI'm using\
          \ \"bge-reranker-large\" to rerank some text chunks after retrieving them\
          \ from a RAG pipeline using \"bge-large-en-v1.5\". I am encountering the\
          \ following error multiple times, indicating a token limit exceed: {'error':\
          \ 'Input validation error: inputs must have less than 512 tokens. Given:\
          \ 556', 'error_type': 'Validation'}.\r\n\r\nI am using the tokenizer from\
          \ \"bge-large-en-v1.5\" for creating the chunks. However, I noticed that\
          \ the \"bge-reranker-large\" tokenizer works differently, and some chunks\
          \ appear to have a length > 512 tokens with the reranker's tokenizer, whereas\
          \ their length is <512 with the embedding's tokenizer. Consequently, I decided\
          \ to use the reranker's tokenizer for chunk creation to avoid exceeding\
          \ the limit. Despite this, I still encounter the same error. So I thought\
          \ that I need to count the total tokens from both chunk and query.\r\n\r\
          \nSo, my question is: is the 512 tokens limit for each chunk or for the\
          \ chunk-query pair? Do I need to limit the total length of the chunk and\
          \ query tokens to 512 or just the length of the chunk?"
        updatedAt: '2024-01-10T10:58:08.480Z'
      numEdits: 0
      reactions: []
    id: 659e78400ae7608987cd65aa
    type: comment
  author: terilias
  content: "Hello and thanks for publishing such great models!\r\n\r\nI'm using \"\
    bge-reranker-large\" to rerank some text chunks after retrieving them from a RAG\
    \ pipeline using \"bge-large-en-v1.5\". I am encountering the following error\
    \ multiple times, indicating a token limit exceed: {'error': 'Input validation\
    \ error: inputs must have less than 512 tokens. Given: 556', 'error_type': 'Validation'}.\r\
    \n\r\nI am using the tokenizer from \"bge-large-en-v1.5\" for creating the chunks.\
    \ However, I noticed that the \"bge-reranker-large\" tokenizer works differently,\
    \ and some chunks appear to have a length > 512 tokens with the reranker's tokenizer,\
    \ whereas their length is <512 with the embedding's tokenizer. Consequently, I\
    \ decided to use the reranker's tokenizer for chunk creation to avoid exceeding\
    \ the limit. Despite this, I still encounter the same error. So I thought that\
    \ I need to count the total tokens from both chunk and query.\r\n\r\nSo, my question\
    \ is: is the 512 tokens limit for each chunk or for the chunk-query pair? Do I\
    \ need to limit the total length of the chunk and query tokens to 512 or just\
    \ the length of the chunk?"
  created_at: 2024-01-10 10:58:08+00:00
  edited: false
  hidden: false
  id: 659e78400ae7608987cd65aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0675d05a52192ee14e9ab1633353956.svg
      fullname: Xiao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Shitao
      type: user
    createdAt: '2024-01-10T11:28:56.000Z'
    data:
      edited: false
      editors:
      - Shitao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8265238404273987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0675d05a52192ee14e9ab1633353956.svg
          fullname: Xiao
          isHf: false
          isPro: false
          name: Shitao
          type: user
        html: '<p>Thanks for your interest in our work!<br>For reranker model, query
          and passage are concatenated and inputted into the model together. Therefore,
          you need to limit the total length of "query+chunk".<br>If you use the tokenizer
          from transformers library, you can set <code>truncation=True, max_length=512</code>
          to truncate the input text.</p>

          '
        raw: "Thanks for your interest in our work!\nFor reranker model, query and\
          \ passage are concatenated and inputted into the model together. Therefore,\
          \ you need to limit the total length of \"query+chunk\". \nIf you use the\
          \ tokenizer from transformers library, you can set `truncation=True, max_length=512`\
          \ to truncate the input text."
        updatedAt: '2024-01-10T11:28:56.864Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - terilias
    id: 659e7f78b5c950aab14dad2c
    type: comment
  author: Shitao
  content: "Thanks for your interest in our work!\nFor reranker model, query and passage\
    \ are concatenated and inputted into the model together. Therefore, you need to\
    \ limit the total length of \"query+chunk\". \nIf you use the tokenizer from transformers\
    \ library, you can set `truncation=True, max_length=512` to truncate the input\
    \ text."
  created_at: 2024-01-10 11:28:56+00:00
  edited: false
  hidden: false
  id: 659e7f78b5c950aab14dad2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
      fullname: Terzis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terilias
      type: user
    createdAt: '2024-01-10T12:32:12.000Z'
    data:
      edited: false
      editors:
      - terilias
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8224248290061951
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
          fullname: Terzis
          isHf: false
          isPro: false
          name: terilias
          type: user
        html: '<p>Thank you for the immediate response!</p>

          '
        raw: Thank you for the immediate response!
        updatedAt: '2024-01-10T12:32:12.975Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659e8e4d94150198b6b29887
    id: 659e8e4c94150198b6b29885
    type: comment
  author: terilias
  content: Thank you for the immediate response!
  created_at: 2024-01-10 12:32:12+00:00
  edited: false
  hidden: false
  id: 659e8e4c94150198b6b29885
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
      fullname: Terzis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terilias
      type: user
    createdAt: '2024-01-10T12:32:13.000Z'
    data:
      status: closed
    id: 659e8e4d94150198b6b29887
    type: status-change
  author: terilias
  created_at: 2024-01-10 12:32:13+00:00
  id: 659e8e4d94150198b6b29887
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: BAAI/bge-reranker-large
repo_type: model
status: closed
target_branch: null
title: About Sequence Length on Reranker model
