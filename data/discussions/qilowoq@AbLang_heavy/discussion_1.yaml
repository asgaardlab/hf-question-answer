!!python/object:huggingface_hub.community.DiscussionWithDetails
author: demharters
conflicting_files: null
created_at: 2023-05-30 08:58:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/673e0cbc05b80ed1390e27b4fcd3837d.svg
      fullname: Samuel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: demharters
      type: user
    createdAt: '2023-05-30T09:58:28.000Z'
    data:
      edited: true
      editors:
      - demharters
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/673e0cbc05b80ed1390e27b4fcd3837d.svg
          fullname: Samuel
          isHf: false
          isPro: false
          name: demharters
          type: user
        html: '<p>There seems to be a discrepancy between vocab length and embedding
          size. Any ideas why?</p>

          <p>```# Load ablang model and tokenizer<br>model_name=''qilowoq/AbLang_heavy''<br>tokenizer
          = AutoTokenizer.from_pretrained(model_name, revision=''c451857'')<br>model
          = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True,
          revision = ''c451857'')</p>

          <p>embedding_size = model.roberta.embeddings.word_embeddings.weight.size(0)<br>print(f"Embedding
          size: {embedding_size}")</p>

          <p>vocab_size = tokenizer.vocab_size<br>print(f"Vocabulary size: {vocab_size}")```</p>

          <p>"Embedding size: 24<br>Vocabulary size: 25"</p>

          '
        raw: 'There seems to be a discrepancy between vocab length and embedding size.
          Any ideas why?


          ```# Load ablang model and tokenizer

          model_name=''qilowoq/AbLang_heavy''

          tokenizer = AutoTokenizer.from_pretrained(model_name, revision=''c451857'')

          model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True,
          revision = ''c451857'')


          embedding_size = model.roberta.embeddings.word_embeddings.weight.size(0)

          print(f"Embedding size: {embedding_size}")


          vocab_size = tokenizer.vocab_size

          print(f"Vocabulary size: {vocab_size}")```


          "Embedding size: 24

          Vocabulary size: 25"'
        updatedAt: '2023-05-30T11:30:17.599Z'
      numEdits: 2
      reactions: []
    id: 6475c8c4f84258330f5a1de8
    type: comment
  author: demharters
  content: 'There seems to be a discrepancy between vocab length and embedding size.
    Any ideas why?


    ```# Load ablang model and tokenizer

    model_name=''qilowoq/AbLang_heavy''

    tokenizer = AutoTokenizer.from_pretrained(model_name, revision=''c451857'')

    model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True,
    revision = ''c451857'')


    embedding_size = model.roberta.embeddings.word_embeddings.weight.size(0)

    print(f"Embedding size: {embedding_size}")


    vocab_size = tokenizer.vocab_size

    print(f"Vocabulary size: {vocab_size}")```


    "Embedding size: 24

    Vocabulary size: 25"'
  created_at: 2023-05-30 08:58:28+00:00
  edited: true
  hidden: false
  id: 6475c8c4f84258330f5a1de8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679157663990-noauth.jpeg?w=200&h=200&f=face
      fullname: Oleg Dmitriev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: qilowoq
      type: user
    createdAt: '2023-05-30T14:15:36.000Z'
    data:
      edited: false
      editors:
      - qilowoq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679157663990-noauth.jpeg?w=200&h=200&f=face
          fullname: Oleg Dmitriev
          isHf: false
          isPro: false
          name: qilowoq
          type: user
        html: '<p>Yes. That was intentional.</p>

          <p>Tokenizer needs [UNK] token, but there were no such token in original
          model. So [UNK] token is 25th token. It would not affect model unless there
          is unknown animoacid in sequence.</p>

          '
        raw: 'Yes. That was intentional.


          Tokenizer needs [UNK] token, but there were no such token in original model.
          So [UNK] token is 25th token. It would not affect model unless there is
          unknown animoacid in sequence.'
        updatedAt: '2023-05-30T14:15:36.805Z'
      numEdits: 0
      reactions: []
    id: 64760508c7e6f8e9fec943fc
    type: comment
  author: qilowoq
  content: 'Yes. That was intentional.


    Tokenizer needs [UNK] token, but there were no such token in original model. So
    [UNK] token is 25th token. It would not affect model unless there is unknown animoacid
    in sequence.'
  created_at: 2023-05-30 13:15:36+00:00
  edited: false
  hidden: false
  id: 64760508c7e6f8e9fec943fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/673e0cbc05b80ed1390e27b4fcd3837d.svg
      fullname: Samuel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: demharters
      type: user
    createdAt: '2023-05-30T14:40:08.000Z'
    data:
      edited: false
      editors:
      - demharters
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/673e0cbc05b80ed1390e27b4fcd3837d.svg
          fullname: Samuel
          isHf: false
          isPro: false
          name: demharters
          type: user
        html: '<p>It''s just that I got an error when starting fine tuning due to
          the discrepancy. Thanks for clarifying.</p>

          '
        raw: It's just that I got an error when starting fine tuning due to the discrepancy.
          Thanks for clarifying.
        updatedAt: '2023-05-30T14:40:08.568Z'
      numEdits: 0
      reactions: []
    id: 64760ac8400d8a9ea630abad
    type: comment
  author: demharters
  content: It's just that I got an error when starting fine tuning due to the discrepancy.
    Thanks for clarifying.
  created_at: 2023-05-30 13:40:08+00:00
  edited: false
  hidden: false
  id: 64760ac8400d8a9ea630abad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679157663990-noauth.jpeg?w=200&h=200&f=face
      fullname: Oleg Dmitriev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: qilowoq
      type: user
    createdAt: '2023-05-31T14:51:06.000Z'
    data:
      status: closed
    id: 64775edabb7681ad67062017
    type: status-change
  author: qilowoq
  created_at: 2023-05-31 13:51:06+00:00
  id: 64775edabb7681ad67062017
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: qilowoq/AbLang_heavy
repo_type: model
status: closed
target_branch: null
title: Different size between tokenizer vocab and embedding
