!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mutiann
conflicting_files: null
created_at: 2022-08-26 08:12:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-08-26T09:12:36.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>Hello Sanchit,</p>

          <p>I''m here again :) Now trying to add BART into the model.</p>

          <p>While now my model basically lingers around WER~14%, and I find that
          the most common errors are from long sentences where the model tends to
          skip long segments when generating the transcript.</p>

          <p>May I ask if you have any idea about that?</p>

          <p>Thanks!</p>

          '
        raw: "Hello Sanchit,\r\n\r\nI'm here again :) Now trying to add BART into\
          \ the model.\r\n\r\nWhile now my model basically lingers around WER~14%,\
          \ and I find that the most common errors are from long sentences where the\
          \ model tends to skip long segments when generating the transcript.\r\n\r\
          \nMay I ask if you have any idea about that?\r\n\r\nThanks!"
        updatedAt: '2022-08-26T09:12:36.814Z'
      numEdits: 0
      reactions: []
    id: 63088e84e3246e2be8874384
    type: comment
  author: mutiann
  content: "Hello Sanchit,\r\n\r\nI'm here again :) Now trying to add BART into the\
    \ model.\r\n\r\nWhile now my model basically lingers around WER~14%, and I find\
    \ that the most common errors are from long sentences where the model tends to\
    \ skip long segments when generating the transcript.\r\n\r\nMay I ask if you have\
    \ any idea about that?\r\n\r\nThanks!"
  created_at: 2022-08-26 08:12:36+00:00
  edited: false
  hidden: false
  id: 63088e84e3246e2be8874384
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-09-05T16:43:05.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mutiann&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mutiann\"\
          >@<span class=\"underline\">mutiann</span></a></span>\n\n\t</span></span>!\
          \ </p>\n<p>In my experience, running a wandb sweep over the generation hyper-parameters\
          \ (<code>num_beams</code>, <a href=\"https://huggingface.co/docs/transformers/v4.21.3/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.length_penalty\"\
          ><code>length_penalty</code></a>) works well! This blog explains generation\
          \ very succinctly: <a href=\"https://huggingface.co/blog/how-to-generate\"\
          >https://huggingface.co/blog/how-to-generate</a></p>\n<p>Sweeping over <code>length_penalty</code>:\
          \ <a rel=\"nofollow\" href=\"https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40?workspace=user-sanchit-gandhi\"\
          >https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40?workspace=user-sanchit-gandhi</a><br>You\
          \ can find the sweep config here: <a rel=\"nofollow\" href=\"https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40/overview?workspace=user-sanchit-gandhi\"\
          >https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40/overview?workspace=user-sanchit-gandhi</a></p>\n\
          <p>You can play around with the sweep config to sweep over <code>num_beams</code>\
          \ as well, or fix this to a value &gt; 5 for adequate results</p>\n"
        raw: "Hey @mutiann! \n\nIn my experience, running a wandb sweep over the generation\
          \ hyper-parameters (`num_beams`, [`length_penalty`](https://huggingface.co/docs/transformers/v4.21.3/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.length_penalty))\
          \ works well! This blog explains generation very succinctly: https://huggingface.co/blog/how-to-generate\n\
          \nSweeping over `length_penalty`: https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40?workspace=user-sanchit-gandhi\n\
          You can find the sweep config here: https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40/overview?workspace=user-sanchit-gandhi\n\
          \nYou can play around with the sweep config to sweep over `num_beams` as\
          \ well, or fix this to a value > 5 for adequate results"
        updatedAt: '2022-09-05T16:43:05.768Z'
      numEdits: 0
      reactions: []
    id: 63162719f5e32157c51d0035
    type: comment
  author: sanchit-gandhi
  content: "Hey @mutiann! \n\nIn my experience, running a wandb sweep over the generation\
    \ hyper-parameters (`num_beams`, [`length_penalty`](https://huggingface.co/docs/transformers/v4.21.3/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.length_penalty))\
    \ works well! This blog explains generation very succinctly: https://huggingface.co/blog/how-to-generate\n\
    \nSweeping over `length_penalty`: https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40?workspace=user-sanchit-gandhi\n\
    You can find the sweep config here: https://wandb.ai/sanchit-gandhi/tedlium/sweeps/186lxl40/overview?workspace=user-sanchit-gandhi\n\
    \nYou can play around with the sweep config to sweep over `num_beams` as well,\
    \ or fix this to a value > 5 for adequate results"
  created_at: 2022-09-05 15:43:05+00:00
  edited: false
  hidden: false
  id: 63162719f5e32157c51d0035
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-09-05T17:01:13.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Another example sweep! One that I''m currently running for LibriSpeech:
          <a rel="nofollow" href="https://wandb.ai/sanchit-gandhi/librispeech-960h-dev/sweeps/po1rn2fe?workspace=user-sanchit-gandhi">https://wandb.ai/sanchit-gandhi/librispeech-960h-dev/sweeps/po1rn2fe?workspace=user-sanchit-gandhi</a></p>

          '
        raw: 'Another example sweep! One that I''m currently running for LibriSpeech:
          https://wandb.ai/sanchit-gandhi/librispeech-960h-dev/sweeps/po1rn2fe?workspace=user-sanchit-gandhi'
        updatedAt: '2022-09-05T17:01:13.966Z'
      numEdits: 0
      reactions: []
    id: 63162b59550b00d37db7b5d5
    type: comment
  author: sanchit-gandhi
  content: 'Another example sweep! One that I''m currently running for LibriSpeech:
    https://wandb.ai/sanchit-gandhi/librispeech-960h-dev/sweeps/po1rn2fe?workspace=user-sanchit-gandhi'
  created_at: 2022-09-05 16:01:13+00:00
  edited: false
  hidden: false
  id: 63162b59550b00d37db7b5d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-09-07T09:28:18.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>Thank you so much! Unfortunately, the optimal generation hyperparameter
          only gives ~0.1% performance increase :(</p>

          <p>I tried to mimic your training recipe and could only reach 10.1% WER
          at 50k steps. Continuing training will help it to reach 9.5% at 100k, and
          by normalizing the loss not across utterances but across tokens (i.e. longer
          utterances getting higher impacts) it will reach 10.0% at 50k and 8.5% at
          100k, but I''m certain that you can reach better results if you do these
          on your models as well, so this is not the point. Also your WER curve is
          quite surprising (around 18% from 10k to 40k steps, and then suddenly improved
          to 9.0% at 50k steps), while my models are not (they are like 12% at 40k
          steps). So I guess that there must be some special wit :) Do you have any
          idea?</p>

          <p>Thanks in advance!</p>

          '
        raw: 'Thank you so much! Unfortunately, the optimal generation hyperparameter
          only gives ~0.1% performance increase :(


          I tried to mimic your training recipe and could only reach 10.1% WER at
          50k steps. Continuing training will help it to reach 9.5% at 100k, and by
          normalizing the loss not across utterances but across tokens (i.e. longer
          utterances getting higher impacts) it will reach 10.0% at 50k and 8.5% at
          100k, but I''m certain that you can reach better results if you do these
          on your models as well, so this is not the point. Also your WER curve is
          quite surprising (around 18% from 10k to 40k steps, and then suddenly improved
          to 9.0% at 50k steps), while my models are not (they are like 12% at 40k
          steps). So I guess that there must be some special wit :) Do you have any
          idea?


          Thanks in advance!'
        updatedAt: '2022-09-07T09:28:18.917Z'
      numEdits: 0
      reactions: []
    id: 63186432e24c5d66f164cab9
    type: comment
  author: mutiann
  content: 'Thank you so much! Unfortunately, the optimal generation hyperparameter
    only gives ~0.1% performance increase :(


    I tried to mimic your training recipe and could only reach 10.1% WER at 50k steps.
    Continuing training will help it to reach 9.5% at 100k, and by normalizing the
    loss not across utterances but across tokens (i.e. longer utterances getting higher
    impacts) it will reach 10.0% at 50k and 8.5% at 100k, but I''m certain that you
    can reach better results if you do these on your models as well, so this is not
    the point. Also your WER curve is quite surprising (around 18% from 10k to 40k
    steps, and then suddenly improved to 9.0% at 50k steps), while my models are not
    (they are like 12% at 40k steps). So I guess that there must be some special wit
    :) Do you have any idea?


    Thanks in advance!'
  created_at: 2022-09-07 08:28:18+00:00
  edited: false
  hidden: false
  id: 63186432e24c5d66f164cab9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-09-07T13:19:48.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mutiann&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mutiann\"\
          >@<span class=\"underline\">mutiann</span></a></span>\n\n\t</span></span>,\
          \ replying in-line to your comments!</p>\n<blockquote>\n<p>Unfortunately,\
          \ the optimal generation hyperparameter only gives ~0.1% performance increase\
          \ :(</p>\n</blockquote>\n<p>That suggests the model is indeed under-trained!</p>\n\
          <blockquote>\n<p> by normalizing the loss not across utterances but across\
          \ tokens (i.e. longer utterances getting higher impacts) it will reach 10.0%\
          \ at 50k and 8.5% at 100k</p>\n</blockquote>\n<p>In the training loop, we\
          \ normalise the CE loss by the number of non-padded tokens: <a rel=\"nofollow\"\
          \ href=\"https://github.com/sanchit-gandhi/seq2seq-speech/blob/cd25022f24677835f8bd26f528e0e444b80702c6/run_flax_speech_recognition_seq2seq.py#L1265-L1267\"\
          >https://github.com/sanchit-gandhi/seq2seq-speech/blob/cd25022f24677835f8bd26f528e0e444b80702c6/run_flax_speech_recognition_seq2seq.py#L1265-L1267</a><br>Therefore,\
          \ the loss is weighted equally on a token-by-token basis. Did you weight\
          \ longer utterances more heavily?</p>\n<blockquote>\n<p>around 18% from\
          \ 10k to 40k steps, and then suddenly improved to 9.0% at 50k steps</p>\n\
          </blockquote>\n<p>With regards to the sharp drop-off, this is because for\
          \ the first 40k steps I evaluate with greedy, and then for the last evaluation/prediction\
          \ steps I evaluate with beam search!</p>\n<p>As you correctly pointed out\
          \ on <a rel=\"nofollow\" href=\"https://github.com/sanchit-gandhi/seq2seq-speech/issues/77\"\
          >https://github.com/sanchit-gandhi/seq2seq-speech/issues/77</a>, we should\
          \ <strong>not</strong> filter the eval/test samples by our audio/text length\
          \ criterion! I re-ran training without this filtering step, and evaluated\
          \ using beam search at each evaluation step (so no sharp drop-off). The\
          \ results are here: <a rel=\"nofollow\" href=\"https://wandb.ai/sanchit-gandhi/tedlium/runs/2pefefil?workspace=user-sanchit-gandhi\"\
          >https://wandb.ai/sanchit-gandhi/tedlium/runs/2pefefil?workspace=user-sanchit-gandhi</a></p>\n\
          <p>I achieved an eval WER of 12.1% after 50k train steps, and a test WER\
          \ of 7.0%. Optimising the generation HPs, I get an eval WER of 11.8% and\
          \ a test WER of 7.0% <a rel=\"nofollow\" href=\"https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/mqmy3sfo?workspace=user-sanchit-gandhi\"\
          >https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/mqmy3sfo?workspace=user-sanchit-gandhi</a></p>\n\
          <p>Are your quoted training results with or without the eval sample filtering?</p>\n"
        raw: 'Hey @mutiann, replying in-line to your comments!


          > Unfortunately, the optimal generation hyperparameter only gives ~0.1%
          performance increase :(


          That suggests the model is indeed under-trained!


          >  by normalizing the loss not across utterances but across tokens (i.e.
          longer utterances getting higher impacts) it will reach 10.0% at 50k and
          8.5% at 100k


          In the training loop, we normalise the CE loss by the number of non-padded
          tokens: https://github.com/sanchit-gandhi/seq2seq-speech/blob/cd25022f24677835f8bd26f528e0e444b80702c6/run_flax_speech_recognition_seq2seq.py#L1265-L1267

          Therefore, the loss is weighted equally on a token-by-token basis. Did you
          weight longer utterances more heavily?


          > around 18% from 10k to 40k steps, and then suddenly improved to 9.0% at
          50k steps


          With regards to the sharp drop-off, this is because for the first 40k steps
          I evaluate with greedy, and then for the last evaluation/prediction steps
          I evaluate with beam search!


          As you correctly pointed out on https://github.com/sanchit-gandhi/seq2seq-speech/issues/77,
          we should **not** filter the eval/test samples by our audio/text length
          criterion! I re-ran training without this filtering step, and evaluated
          using beam search at each evaluation step (so no sharp drop-off). The results
          are here: https://wandb.ai/sanchit-gandhi/tedlium/runs/2pefefil?workspace=user-sanchit-gandhi


          I achieved an eval WER of 12.1% after 50k train steps, and a test WER of
          7.0%. Optimising the generation HPs, I get an eval WER of 11.8% and a test
          WER of 7.0% https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/mqmy3sfo?workspace=user-sanchit-gandhi


          Are your quoted training results with or without the eval sample filtering?'
        updatedAt: '2022-09-07T13:19:48.572Z'
      numEdits: 0
      reactions: []
    id: 63189a7457a5f5d7ce21172e
    type: comment
  author: sanchit-gandhi
  content: 'Hey @mutiann, replying in-line to your comments!


    > Unfortunately, the optimal generation hyperparameter only gives ~0.1% performance
    increase :(


    That suggests the model is indeed under-trained!


    >  by normalizing the loss not across utterances but across tokens (i.e. longer
    utterances getting higher impacts) it will reach 10.0% at 50k and 8.5% at 100k


    In the training loop, we normalise the CE loss by the number of non-padded tokens:
    https://github.com/sanchit-gandhi/seq2seq-speech/blob/cd25022f24677835f8bd26f528e0e444b80702c6/run_flax_speech_recognition_seq2seq.py#L1265-L1267

    Therefore, the loss is weighted equally on a token-by-token basis. Did you weight
    longer utterances more heavily?


    > around 18% from 10k to 40k steps, and then suddenly improved to 9.0% at 50k
    steps


    With regards to the sharp drop-off, this is because for the first 40k steps I
    evaluate with greedy, and then for the last evaluation/prediction steps I evaluate
    with beam search!


    As you correctly pointed out on https://github.com/sanchit-gandhi/seq2seq-speech/issues/77,
    we should **not** filter the eval/test samples by our audio/text length criterion!
    I re-ran training without this filtering step, and evaluated using beam search
    at each evaluation step (so no sharp drop-off). The results are here: https://wandb.ai/sanchit-gandhi/tedlium/runs/2pefefil?workspace=user-sanchit-gandhi


    I achieved an eval WER of 12.1% after 50k train steps, and a test WER of 7.0%.
    Optimising the generation HPs, I get an eval WER of 11.8% and a test WER of 7.0%
    https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/mqmy3sfo?workspace=user-sanchit-gandhi


    Are your quoted training results with or without the eval sample filtering?'
  created_at: 2022-09-07 12:19:48+00:00
  edited: false
  hidden: false
  id: 63189a7457a5f5d7ce21172e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-09-07T15:00:55.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>Thank you for your reply!</p>

          <p>I''ve been evaluating models without filtering. An eval WER of around
          12% is roughly equal to my results. So actually there isn''t any gap and
          we are on the same ground :) The only mystery (or challenge) is the gap
          between BART and CTC now.</p>

          <p>As for normalization, previously I normalize them by utterances, so longer
          utterances will actually be underweighed. This won''t make any difference
          at 50k steps, but the curve becomes different after that and leads to 1.5%
          WER gap later, compared to normalizing by tokens as both of us do now. Maybe
          weight longer utterances more heavily will even help it more, since most
          errors are from skipping in longer utterances.</p>

          '
        raw: 'Thank you for your reply!


          I''ve been evaluating models without filtering. An eval WER of around 12%
          is roughly equal to my results. So actually there isn''t any gap and we
          are on the same ground :) The only mystery (or challenge) is the gap between
          BART and CTC now.


          As for normalization, previously I normalize them by utterances, so longer
          utterances will actually be underweighed. This won''t make any difference
          at 50k steps, but the curve becomes different after that and leads to 1.5%
          WER gap later, compared to normalizing by tokens as both of us do now. Maybe
          weight longer utterances more heavily will even help it more, since most
          errors are from skipping in longer utterances.'
        updatedAt: '2022-09-07T15:00:55.184Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6318b22769ef5467e18ef3d0
    id: 6318b22769ef5467e18ef3cf
    type: comment
  author: mutiann
  content: 'Thank you for your reply!


    I''ve been evaluating models without filtering. An eval WER of around 12% is roughly
    equal to my results. So actually there isn''t any gap and we are on the same ground
    :) The only mystery (or challenge) is the gap between BART and CTC now.


    As for normalization, previously I normalize them by utterances, so longer utterances
    will actually be underweighed. This won''t make any difference at 50k steps, but
    the curve becomes different after that and leads to 1.5% WER gap later, compared
    to normalizing by tokens as both of us do now. Maybe weight longer utterances
    more heavily will even help it more, since most errors are from skipping in longer
    utterances.'
  created_at: 2022-09-07 14:00:55+00:00
  edited: false
  hidden: false
  id: 6318b22769ef5467e18ef3cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-09-07T15:00:55.000Z'
    data:
      status: closed
    id: 6318b22769ef5467e18ef3d0
    type: status-change
  author: mutiann
  created_at: 2022-09-07 14:00:55+00:00
  id: 6318b22769ef5467e18ef3d0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-09-07T15:22:02.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<blockquote>

          <p>The only mystery (or challenge) is the gap between BART and CTC now.</p>

          </blockquote>

          <p>The Seq2Seq model (Wav2Vec2-2-BART) is able to leverage the knowledge
          of the target text domain gained by the BART model during pre-training (massive
          amounts of text data!). This greatly helps it with phonetic error corrections
          (<em>foebe</em> -&gt; <em>Phoebe</em>). CTC is still very much susceptible
          to these errors, basing its predictions almost entirely on the audio inputs.</p>

          <p>You can boot-strap your CTC results by incorporating a language model
          (LM) and performing LM boosted beam-search. In doing so, you can correct
          for phonetic errors and receive a 25-30% reduction in WER as a result, bringing
          it close (if not better) to Seq2Seq performance! Refer to this (fantastic)
          blog for a tutorial on n-grams with Transformers: <a href="https://huggingface.co/blog/wav2vec2-with-ngram">https://huggingface.co/blog/wav2vec2-with-ngram</a></p>

          <p>You can train an n-gram LM using this PR: <a rel="nofollow" href="https://github.com/sanchit-gandhi/seq2seq-speech/pull/83/files">https://github.com/sanchit-gandhi/seq2seq-speech/pull/83/files</a></p>

          <p>You can evaluate with an n-gram using this script: <a rel="nofollow"
          href="https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc_ngram.py">https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc_ngram.py</a></p>

          <p>All the best!</p>

          <p>Sanchit</p>

          '
        raw: '> The only mystery (or challenge) is the gap between BART and CTC now.


          The Seq2Seq model (Wav2Vec2-2-BART) is able to leverage the knowledge of
          the target text domain gained by the BART model during pre-training (massive
          amounts of text data!). This greatly helps it with phonetic error corrections
          (_foebe_ -> _Phoebe_). CTC is still very much susceptible to these errors,
          basing its predictions almost entirely on the audio inputs.


          You can boot-strap your CTC results by incorporating a language model (LM)
          and performing LM boosted beam-search. In doing so, you can correct for
          phonetic errors and receive a 25-30% reduction in WER as a result, bringing
          it close (if not better) to Seq2Seq performance! Refer to this (fantastic)
          blog for a tutorial on n-grams with Transformers: https://huggingface.co/blog/wav2vec2-with-ngram


          You can train an n-gram LM using this PR: https://github.com/sanchit-gandhi/seq2seq-speech/pull/83/files


          You can evaluate with an n-gram using this script: https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc_ngram.py


          All the best!


          Sanchit'
        updatedAt: '2022-09-07T15:22:02.946Z'
      numEdits: 0
      reactions: []
    id: 6318b71a2f7c7c940f11fe74
    type: comment
  author: sanchit-gandhi
  content: '> The only mystery (or challenge) is the gap between BART and CTC now.


    The Seq2Seq model (Wav2Vec2-2-BART) is able to leverage the knowledge of the target
    text domain gained by the BART model during pre-training (massive amounts of text
    data!). This greatly helps it with phonetic error corrections (_foebe_ -> _Phoebe_).
    CTC is still very much susceptible to these errors, basing its predictions almost
    entirely on the audio inputs.


    You can boot-strap your CTC results by incorporating a language model (LM) and
    performing LM boosted beam-search. In doing so, you can correct for phonetic errors
    and receive a 25-30% reduction in WER as a result, bringing it close (if not better)
    to Seq2Seq performance! Refer to this (fantastic) blog for a tutorial on n-grams
    with Transformers: https://huggingface.co/blog/wav2vec2-with-ngram


    You can train an n-gram LM using this PR: https://github.com/sanchit-gandhi/seq2seq-speech/pull/83/files


    You can evaluate with an n-gram using this script: https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc_ngram.py


    All the best!


    Sanchit'
  created_at: 2022-09-07 14:22:02+00:00
  edited: false
  hidden: false
  id: 6318b71a2f7c7c940f11fe74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-09-08T09:00:15.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>Yes indeed, the LM definitely helps, so we could expect that BART
          will usually improve the performance. The mystery I mentioned is why in
          both of our experiments the Wav2Vec2 + BART models have higher WER than
          Wav2Vec2 + CTC. BART seems to be very unstable in generating long transcripts
          and often skips part of the transcript.</p>

          <p>Thanks a lot!</p>

          '
        raw: 'Yes indeed, the LM definitely helps, so we could expect that BART will
          usually improve the performance. The mystery I mentioned is why in both
          of our experiments the Wav2Vec2 + BART models have higher WER than Wav2Vec2
          + CTC. BART seems to be very unstable in generating long transcripts and
          often skips part of the transcript.


          Thanks a lot!'
        updatedAt: '2022-09-08T09:00:15.130Z'
      numEdits: 0
      reactions: []
    id: 6319af1f307cb811990429ee
    type: comment
  author: mutiann
  content: 'Yes indeed, the LM definitely helps, so we could expect that BART will
    usually improve the performance. The mystery I mentioned is why in both of our
    experiments the Wav2Vec2 + BART models have higher WER than Wav2Vec2 + CTC. BART
    seems to be very unstable in generating long transcripts and often skips part
    of the transcript.


    Thanks a lot!'
  created_at: 2022-09-08 08:00:15+00:00
  edited: false
  hidden: false
  id: 6319af1f307cb811990429ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-09-09T14:44:31.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Ah I see! Yes that''s an interesting one - removing the filtering
          constraint on the eval dataset reduced my Seq2Seq eval WER by several percentage
          points. Interesting to hear that you''ve found Seq2Seq to fare poorly on
          long transcripts, this aligns with the numbers I got from evaluation! The
          weighting of the BART decoder must play quite heavily and try to ''correct''
          these long transcriptions. Curious to hear if you''ve looked into remedying
          this problem! I''ve tried to tackle it with the generation HPs: <a rel="nofollow"
          href="https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/p5e3l8rw?workspace=user-sanchit-gandhi">https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/p5e3l8rw?workspace=user-sanchit-gandhi</a><br>Still
          a way off CTC on the eval set!</p>

          <p>I''ve actually found Seq2Seq to outperform vanilla CTC on the test set
          - 7.0% vs 8.2%. CTC + n-gram will almost certainly close the gap.</p>

          '
        raw: 'Ah I see! Yes that''s an interesting one - removing the filtering constraint
          on the eval dataset reduced my Seq2Seq eval WER by several percentage points.
          Interesting to hear that you''ve found Seq2Seq to fare poorly on long transcripts,
          this aligns with the numbers I got from evaluation! The weighting of the
          BART decoder must play quite heavily and try to ''correct'' these long transcriptions.
          Curious to hear if you''ve looked into remedying this problem! I''ve tried
          to tackle it with the generation HPs: https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/p5e3l8rw?workspace=user-sanchit-gandhi

          Still a way off CTC on the eval set!


          I''ve actually found Seq2Seq to outperform vanilla CTC on the test set -
          7.0% vs 8.2%. CTC + n-gram will almost certainly close the gap.'
        updatedAt: '2022-09-09T14:44:57.290Z'
      numEdits: 1
      reactions: []
    id: 631b514f9edb6d320a0be3d8
    type: comment
  author: sanchit-gandhi
  content: 'Ah I see! Yes that''s an interesting one - removing the filtering constraint
    on the eval dataset reduced my Seq2Seq eval WER by several percentage points.
    Interesting to hear that you''ve found Seq2Seq to fare poorly on long transcripts,
    this aligns with the numbers I got from evaluation! The weighting of the BART
    decoder must play quite heavily and try to ''correct'' these long transcriptions.
    Curious to hear if you''ve looked into remedying this problem! I''ve tried to
    tackle it with the generation HPs: https://wandb.ai/sanchit-gandhi/tedlium-dev/sweeps/p5e3l8rw?workspace=user-sanchit-gandhi

    Still a way off CTC on the eval set!


    I''ve actually found Seq2Seq to outperform vanilla CTC on the test set - 7.0%
    vs 8.2%. CTC + n-gram will almost certainly close the gap.'
  created_at: 2022-09-09 13:44:31+00:00
  edited: true
  hidden: false
  id: 631b514f9edb6d320a0be3d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-09-12T11:42:11.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>I don''t have much idea to tackle the issue at the moment...Of course
          there are many tricks that can be tried on but I doubt if they are generally
          applicable. Anyway I''m trying to investigate if there is any better approach.</p>

          '
        raw: I don't have much idea to tackle the issue at the moment...Of course
          there are many tricks that can be tried on but I doubt if they are generally
          applicable. Anyway I'm trying to investigate if there is any better approach.
        updatedAt: '2022-09-12T11:42:11.010Z'
      numEdits: 0
      reactions: []
    id: 631f1b13124782a19ef87027
    type: comment
  author: mutiann
  content: I don't have much idea to tackle the issue at the moment...Of course there
    are many tricks that can be tried on but I doubt if they are generally applicable.
    Anyway I'm trying to investigate if there is any better approach.
  created_at: 2022-09-12 10:42:11+00:00
  edited: false
  hidden: false
  id: 631f1b13124782a19ef87027
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: sanchit-gandhi/wav2vec2-2-bart-large-tedlium
repo_type: model
status: closed
target_branch: null
title: Model training
