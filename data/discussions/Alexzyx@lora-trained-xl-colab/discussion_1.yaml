!!python/object:huggingface_hub.community.DiscussionWithDetails
author: patrickvonplaten
conflicting_files: null
created_at: 2023-10-10 08:08:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-10-10T09:08:44.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3664532005786896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>When loading the state dict of this file, one can see that the weights\
          \ are broken.</p>\n<p>E.g. doing the following:</p>\n<pre><code class=\"\
          language-py\"><span class=\"hljs-keyword\">from</span> safetensors.torch\
          \ <span class=\"hljs-keyword\">import</span> load_file\n\nsd = load_file(<span\
          \ class=\"hljs-string\">\"./pytorch_lora_weights.safetensors\"</span>)\n\
          <span class=\"hljs-built_in\">print</span>(sd.values)\n</code></pre>\n<p>shows:</p>\n\
          <pre><code>...\n        [ 0.4089, -0.4093,  0.0417,  ...,  0.3376, -0.0011,\
          \ -0.3596]]), tensor([[nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n\
          \        [nan, nan, nan, nan],\n        ...,\n        [nan, nan, nan, nan],\n\
          \        [nan, nan, nan, nan],\n        [nan, nan, nan, nan]]), tensor([[nan,\
          \ nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan,\
          \ nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan,\
          \ nan,  ..., nan, nan, nan]]), tensor([[nan, nan, nan, nan],\n        [nan,\
          \ nan, nan, nan],\n        [nan, nan, nan, nan],\n        ...,\n       \
          \ [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan,\
          \ nan, nan]]), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan,\
          \ nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan,\
          \ nan],\n        [nan, nan, nan,  ..., nan, nan, nan]]), tensor([[nan, nan,\
          \ nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n\
          \        ...,\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n\
          \        [nan, nan, nan, nan]]), tensor([[ 0.3282, -0.1082,  0.1383,  ...,\
          \ -0.2528, -0.2138, -0.3022],\n        [ 0.2439,  0.0952,  0.1135,  ...,\
          \  0.2367, -0.1400,  0.0796],\n        [-0.1357,  0.2351, -0.4367,  ...,\
          \ -0.5447, -0.3476, -0.0175],\n        [-0.2212,  0.2084, -0.5737,  ...,\
          \ -0.0218, -0.1498,  0.6869]]), tensor([[nan, nan, nan, nan],\n        [nan,\
          \ nan, nan, nan],\n        [nan, nan, nan, nan],\n        ...,\n       \
          \ [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan,\
          \ nan, nan]])])\n</code></pre>\n<p>E.g. for the key: <code>sd['unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.processor.to_v_lora.up.weight']</code></p>\n"
        raw: "When loading the state dict of this file, one can see that the weights\
          \ are broken.\r\n\r\nE.g. doing the following:\r\n```py\r\nfrom safetensors.torch\
          \ import load_file\r\n\r\nsd = load_file(\"./pytorch_lora_weights.safetensors\"\
          )\r\nprint(sd.values)\r\n```\r\n\r\nshows:\r\n\r\n```\r\n...\r\n       \
          \ [ 0.4089, -0.4093,  0.0417,  ...,  0.3376, -0.0011, -0.3596]]), tensor([[nan,\
          \ nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan,\
          \ nan, nan],\r\n        ...,\r\n        [nan, nan, nan, nan],\r\n      \
          \  [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan]]), tensor([[nan,\
          \ nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan,\
          \ nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n      \
          \  [nan, nan, nan,  ..., nan, nan, nan]]), tensor([[nan, nan, nan, nan],\r\
          \n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n   \
          \     ...,\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\
          \n        [nan, nan, nan, nan]]), tensor([[nan, nan, nan,  ..., nan, nan,\
          \ nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan,\
          \ nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan,\
          \ nan, nan]]), tensor([[nan, nan, nan, nan],\r\n        [nan, nan, nan,\
          \ nan],\r\n        [nan, nan, nan, nan],\r\n        ...,\r\n        [nan,\
          \ nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan,\
          \ nan, nan]]), tensor([[ 0.3282, -0.1082,  0.1383,  ..., -0.2528, -0.2138,\
          \ -0.3022],\r\n        [ 0.2439,  0.0952,  0.1135,  ...,  0.2367, -0.1400,\
          \  0.0796],\r\n        [-0.1357,  0.2351, -0.4367,  ..., -0.5447, -0.3476,\
          \ -0.0175],\r\n        [-0.2212,  0.2084, -0.5737,  ..., -0.0218, -0.1498,\
          \  0.6869]]), tensor([[nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\
          \n        [nan, nan, nan, nan],\r\n        ...,\r\n        [nan, nan, nan,\
          \ nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan]])])\r\
          \n```\r\n\r\nE.g. for the key: `sd['unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.processor.to_v_lora.up.weight']`"
        updatedAt: '2023-10-10T09:08:44.952Z'
      numEdits: 0
      reactions: []
    id: 6525149ca6c9cbd7e92ba6af
    type: comment
  author: patrickvonplaten
  content: "When loading the state dict of this file, one can see that the weights\
    \ are broken.\r\n\r\nE.g. doing the following:\r\n```py\r\nfrom safetensors.torch\
    \ import load_file\r\n\r\nsd = load_file(\"./pytorch_lora_weights.safetensors\"\
    )\r\nprint(sd.values)\r\n```\r\n\r\nshows:\r\n\r\n```\r\n...\r\n        [ 0.4089,\
    \ -0.4093,  0.0417,  ...,  0.3376, -0.0011, -0.3596]]), tensor([[nan, nan, nan,\
    \ nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n\
    \        ...,\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\
    \n        [nan, nan, nan, nan]]), tensor([[nan, nan, nan,  ..., nan, nan, nan],\r\
    \n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ...,\
    \ nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan]]), tensor([[nan,\
    \ nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan,\
    \ nan],\r\n        ...,\r\n        [nan, nan, nan, nan],\r\n        [nan, nan,\
    \ nan, nan],\r\n        [nan, nan, nan, nan]]), tensor([[nan, nan, nan,  ...,\
    \ nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n       \
    \ [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan,\
    \ nan, nan]]), tensor([[nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\
    \n        [nan, nan, nan, nan],\r\n        ...,\r\n        [nan, nan, nan, nan],\r\
    \n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan]]), tensor([[ 0.3282,\
    \ -0.1082,  0.1383,  ..., -0.2528, -0.2138, -0.3022],\r\n        [ 0.2439,  0.0952,\
    \  0.1135,  ...,  0.2367, -0.1400,  0.0796],\r\n        [-0.1357,  0.2351, -0.4367,\
    \  ..., -0.5447, -0.3476, -0.0175],\r\n        [-0.2212,  0.2084, -0.5737,  ...,\
    \ -0.0218, -0.1498,  0.6869]]), tensor([[nan, nan, nan, nan],\r\n        [nan,\
    \ nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        ...,\r\n       \
    \ [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan,\
    \ nan, nan]])])\r\n```\r\n\r\nE.g. for the key: `sd['unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.processor.to_v_lora.up.weight']`"
  created_at: 2023-10-10 08:08:44+00:00
  edited: false
  hidden: false
  id: 6525149ca6c9cbd7e92ba6af
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Alexzyx/lora-trained-xl-colab
repo_type: model
status: open
target_branch: null
title: Weights are broken
