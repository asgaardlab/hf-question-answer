!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hthomas
conflicting_files: null
created_at: 2023-01-11 09:18:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57c8c09e6aa18f322d6f59d83dcd36d8.svg
      fullname: Hugo THOMAS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hthomas
      type: user
    createdAt: '2023-01-11T09:18:33.000Z'
    data:
      edited: false
      editors:
      - hthomas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57c8c09e6aa18f322d6f59d83dcd36d8.svg
          fullname: Hugo THOMAS
          isHf: false
          isPro: false
          name: hthomas
          type: user
        html: '<p>Whenever I try to load any of your mLuke models with ###.from_pretrained("name_of_the_model")
          (I tried with ### as AutoModel, LukeModel, LukeForMaskedLM...), every single
          one of the model''s weights get ignored, and so I end up with a randomly
          initialized model.<br>I tried on several machines and versions and always
          get this issue, so I suppose that it is a configuration file issue.</p>

          <p>Thank you for any help you can offer !</p>

          '
        raw: "Whenever I try to load any of your mLuke models with ###.from_pretrained(\"\
          name_of_the_model\") (I tried with ### as AutoModel, LukeModel, LukeForMaskedLM...),\
          \ every single one of the model's weights get ignored, and so I end up with\
          \ a randomly initialized model.\r\nI tried on several machines and versions\
          \ and always get this issue, so I suppose that it is a configuration file\
          \ issue.\r\n\r\nThank you for any help you can offer !"
        updatedAt: '2023-01-11T09:18:33.265Z'
      numEdits: 0
      reactions: []
    id: 63be7ee94a2beec6555e512b
    type: comment
  author: hthomas
  content: "Whenever I try to load any of your mLuke models with ###.from_pretrained(\"\
    name_of_the_model\") (I tried with ### as AutoModel, LukeModel, LukeForMaskedLM...),\
    \ every single one of the model's weights get ignored, and so I end up with a\
    \ randomly initialized model.\r\nI tried on several machines and versions and\
    \ always get this issue, so I suppose that it is a configuration file issue.\r\
    \n\r\nThank you for any help you can offer !"
  created_at: 2023-01-11 09:18:33+00:00
  edited: false
  hidden: false
  id: 63be7ee94a2beec6555e512b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
      fullname: Ryokan Ri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ryo0634
      type: user
    createdAt: '2023-01-11T09:35:25.000Z'
    data:
      edited: true
      editors:
      - ryo0634
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
          fullname: Ryokan Ri
          isHf: false
          isPro: false
          name: ryo0634
          type: user
        html: '<p>Hi, thank you for showing an interest in our model.</p>

          <p>I think that you are seeing a bunch of weights with the prefixes <code>e2w_</code>,
          <code>e2e_</code>, <code>w2e_</code> are not loaded.<br>This is actually
          an expected behavior when <code>use_entity_aware_attention</code> is set
          to ''false'' in the model config.<br>These weights are only used for entity
          aware attention, so those weights should be ignored if the model does not
          have entity_aware_attention.</p>

          <p>The detail of entity aware attention is described in the monolingual
          LUKE paper (<a rel="nofollow" href="https://aclanthology.org/2020.emnlp-main.523/">https://aclanthology.org/2020.emnlp-main.523/</a>).<br>We''ve
          found entity aware attention does not improve performance in cross-lingual
          transfer settings, so turned it off by default.</p>

          '
        raw: 'Hi, thank you for showing an interest in our model.


          I think that you are seeing a bunch of weights with the prefixes `e2w_`,
          `e2e_`, `w2e_` are not loaded.

          This is actually an expected behavior when `use_entity_aware_attention`
          is set to ''false'' in the model config.

          These weights are only used for entity aware attention, so those weights
          should be ignored if the model does not have entity_aware_attention.


          The detail of entity aware attention is described in the monolingual LUKE
          paper (https://aclanthology.org/2020.emnlp-main.523/).

          We''ve found entity aware attention does not improve performance in cross-lingual
          transfer settings, so turned it off by default.'
        updatedAt: '2023-01-11T09:37:38.964Z'
      numEdits: 2
      reactions: []
    id: 63be82ddc26a8a4d713e4eff
    type: comment
  author: ryo0634
  content: 'Hi, thank you for showing an interest in our model.


    I think that you are seeing a bunch of weights with the prefixes `e2w_`, `e2e_`,
    `w2e_` are not loaded.

    This is actually an expected behavior when `use_entity_aware_attention` is set
    to ''false'' in the model config.

    These weights are only used for entity aware attention, so those weights should
    be ignored if the model does not have entity_aware_attention.


    The detail of entity aware attention is described in the monolingual LUKE paper
    (https://aclanthology.org/2020.emnlp-main.523/).

    We''ve found entity aware attention does not improve performance in cross-lingual
    transfer settings, so turned it off by default.'
  created_at: 2023-01-11 09:35:25+00:00
  edited: true
  hidden: false
  id: 63be82ddc26a8a4d713e4eff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57c8c09e6aa18f322d6f59d83dcd36d8.svg
      fullname: Hugo THOMAS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hthomas
      type: user
    createdAt: '2023-01-11T10:17:42.000Z'
    data:
      edited: false
      editors:
      - hthomas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57c8c09e6aa18f322d6f59d83dcd36d8.svg
          fullname: Hugo THOMAS
          isHf: false
          isPro: false
          name: hthomas
          type: user
        html: '<p>Hi,<br>Thank you very much for your quick response.</p>

          <p>Indeed the weights which do not load are those related to entity-aware
          attention. I think that I had mistaken entity-aware attention for entity
          specific embeddings, which are two very different things. </p>

          <p>Thank you for your work !</p>

          '
        raw: "Hi,\nThank you very much for your quick response.\n\nIndeed the weights\
          \ which do not load are those related to entity-aware attention. I think\
          \ that I had mistaken entity-aware attention for entity specific embeddings,\
          \ which are two very different things. \n\nThank you for your work !"
        updatedAt: '2023-01-11T10:17:42.179Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ryo0634
      relatedEventId: 63be8cc6c26a8a4d713ee08b
    id: 63be8cc6c26a8a4d713ee08a
    type: comment
  author: hthomas
  content: "Hi,\nThank you very much for your quick response.\n\nIndeed the weights\
    \ which do not load are those related to entity-aware attention. I think that\
    \ I had mistaken entity-aware attention for entity specific embeddings, which\
    \ are two very different things. \n\nThank you for your work !"
  created_at: 2023-01-11 10:17:42+00:00
  edited: false
  hidden: false
  id: 63be8cc6c26a8a4d713ee08a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/57c8c09e6aa18f322d6f59d83dcd36d8.svg
      fullname: Hugo THOMAS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hthomas
      type: user
    createdAt: '2023-01-11T10:17:42.000Z'
    data:
      status: closed
    id: 63be8cc6c26a8a4d713ee08b
    type: status-change
  author: hthomas
  created_at: 2023-01-11 10:17:42+00:00
  id: 63be8cc6c26a8a4d713ee08b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: studio-ousia/mluke-base
repo_type: model
status: closed
target_branch: null
title: issue with from_pretrained
