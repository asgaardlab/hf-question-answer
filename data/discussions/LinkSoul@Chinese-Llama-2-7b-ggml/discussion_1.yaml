!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aldennX
conflicting_files: null
created_at: 2023-08-06 09:39:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T10:39:03.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24009597301483154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "C:\Users\Administrator\text-generation-webui\server.py",
          line 68, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "C:\Users\Administrator\text-generation-webui\modules\models.py",
          line 78, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "C:\Users\Administrator\text-generation-webui\modules\models.py", line 232,
          in llamacpp_loader<br>    from modules.llamacpp_model import LlamaCppModel<br>  File
          "C:\Users\Administrator\text-generation-webui\modules\llamacpp_model.py",
          line 11, in <br>    import llama_cpp<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp_<em>init</em>_.py",
          line 1, in <br>    from .llama_cpp import *<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 1292, in <br>    llama_backend_init(c_bool(False))<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 403, in llama_backend_init<br>    return _lib.llama_backend_init(numa)<br>OSError:
          [WinError -1073741795] Windows Error 0xc000001d</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\\
          text-generation-webui\\server.py\", line 68, in load_model_wrapper\r\n \
          \   shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\models.py\"\
          , line 78, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\models.py\"\
          , line 232, in llamacpp_loader\r\n    from modules.llamacpp_model import\
          \ LlamaCppModel\r\n  File \"C:\\Users\\Administrator\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 11, in <module>\r\n    import llama_cpp\r\
          \n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp\\__init__.py\", line 1, in <module>\r\n    from .llama_cpp import\
          \ *\r\n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp\\llama_cpp.py\", line 1292, in <module>\r\n    llama_backend_init(c_bool(False))\r\
          \n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp\\llama_cpp.py\", line 403, in llama_backend_init\r\n    return\
          \ _lib.llama_backend_init(numa)\r\nOSError: [WinError -1073741795] Windows\
          \ Error 0xc000001d"
        updatedAt: '2023-08-06T10:39:03.284Z'
      numEdits: 0
      reactions: []
    id: 64cf78477a7305c589235e7c
    type: comment
  author: aldennX
  content: "Traceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\\
    text-generation-webui\\server.py\", line 68, in load_model_wrapper\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\r\n  File \"C:\\Users\\\
    Administrator\\text-generation-webui\\modules\\models.py\", line 78, in load_model\r\
    \n    output = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\Administrator\\\
    text-generation-webui\\modules\\models.py\", line 232, in llamacpp_loader\r\n\
    \    from modules.llamacpp_model import LlamaCppModel\r\n  File \"C:\\Users\\\
    Administrator\\text-generation-webui\\modules\\llamacpp_model.py\", line 11, in\
    \ <module>\r\n    import llama_cpp\r\n  File \"C:\\Users\\Administrator\\.conda\\\
    envs\\textgen\\lib\\site-packages\\llama_cpp\\__init__.py\", line 1, in <module>\r\
    \n    from .llama_cpp import *\r\n  File \"C:\\Users\\Administrator\\.conda\\\
    envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\", line 1292, in <module>\r\
    \n    llama_backend_init(c_bool(False))\r\n  File \"C:\\Users\\Administrator\\\
    .conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\", line 403,\
    \ in llama_backend_init\r\n    return _lib.llama_backend_init(numa)\r\nOSError:\
    \ [WinError -1073741795] Windows Error 0xc000001d"
  created_at: 2023-08-06 09:39:03+00:00
  edited: false
  hidden: false
  id: 64cf78477a7305c589235e7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25765c077ea5b12c87615ec5351d595f.svg
      fullname: chenshake
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenshake
      type: user
    createdAt: '2023-12-12T11:08:00.000Z'
    data:
      edited: false
      editors:
      - chenshake
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.5849711298942566
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25765c077ea5b12c87615ec5351d595f.svg
          fullname: chenshake
          isHf: false
          isPro: false
          name: chenshake
          type: user
        html: "<p>ggml\u7684\u683C\u5F0F\uFF0Cllama_cpp\u5DF2\u7ECF\u4E0D\u652F\u6301\
          \uFF0C\u9700\u8981ggnf</p>\n"
        raw: "ggml\u7684\u683C\u5F0F\uFF0Cllama_cpp\u5DF2\u7ECF\u4E0D\u652F\u6301\uFF0C\
          \u9700\u8981ggnf"
        updatedAt: '2023-12-12T11:08:00.355Z'
      numEdits: 0
      reactions: []
    id: 65783f1073080b490cce96de
    type: comment
  author: chenshake
  content: "ggml\u7684\u683C\u5F0F\uFF0Cllama_cpp\u5DF2\u7ECF\u4E0D\u652F\u6301\uFF0C\
    \u9700\u8981ggnf"
  created_at: 2023-12-12 11:08:00+00:00
  edited: false
  hidden: false
  id: 65783f1073080b490cce96de
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LinkSoul/Chinese-Llama-2-7b-ggml
repo_type: model
status: open
target_branch: null
title: "\u9009\u62E9\u540E\u62A5\u9519"
