!!python/object:huggingface_hub.community.DiscussionWithDetails
author: qeternity
conflicting_files: null
created_at: 2023-06-17 15:43:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6351bdcc21c37db00cbb1c9d/kSE7O99Du60DaBayOYSaR.jpeg?w=200&h=200&f=face
      fullname: qeternity
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: qeternity
      type: user
    createdAt: '2023-06-17T16:43:27.000Z'
    data:
      edited: false
      editors:
      - qeternity
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989024817943573
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6351bdcc21c37db00cbb1c9d/kSE7O99Du60DaBayOYSaR.jpeg?w=200&h=200&f=face
          fullname: qeternity
          isHf: false
          isPro: false
          name: qeternity
          type: user
        html: '<p>Why did you delete the safetensors version? It makes it substantially
          less convenient to clone now.</p>

          '
        raw: Why did you delete the safetensors version? It makes it substantially
          less convenient to clone now.
        updatedAt: '2023-06-17T16:43:27.434Z'
      numEdits: 0
      reactions: []
    id: 648de2af8a66ab9239aa8baa
    type: comment
  author: qeternity
  content: Why did you delete the safetensors version? It makes it substantially less
    convenient to clone now.
  created_at: 2023-06-17 15:43:27+00:00
  edited: false
  hidden: false
  id: 648de2af8a66ab9239aa8baa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-17T18:50:25.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8051956295967102
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I moved it to its own branch because It uses group_size + desc_act
          and therefore either produces gibberish, or runs extremely slow, for the
          majority of users.  Also having two models in one repo doesn''t work well
          now that there''s <code>quantize_config.json</code> for AutoGPTQ.</p>

          <p>It''s not gone, it''s just in a separate branch.  Just as easy to clone
          as it was before.</p>

          <p>If using git, then do:</p>

          <pre><code>git clone -b actorder https://huggingface.co/TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g

          </code></pre>

          <p>If using text-generation-webui to download in the UI, type:</p>

          <pre><code>TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g:actorder

          </code></pre>

          <p>And if using text-generation-webui''s <code>download-model.py</code>
          on the command line, do:</p>

          <pre><code>python download-model.py TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g
          --branch actorder

          </code></pre>

          '
        raw: 'I moved it to its own branch because It uses group_size + desc_act and
          therefore either produces gibberish, or runs extremely slow, for the majority
          of users.  Also having two models in one repo doesn''t work well now that
          there''s `quantize_config.json` for AutoGPTQ.


          It''s not gone, it''s just in a separate branch.  Just as easy to clone
          as it was before.


          If using git, then do:

          ```

          git clone -b actorder https://huggingface.co/TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g

          ```


          If using text-generation-webui to download in the UI, type:

          ```

          TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g:actorder

          ```


          And if using text-generation-webui''s `download-model.py` on the command
          line, do:

          ```

          python download-model.py TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g --branch
          actorder

          ```

          '
        updatedAt: '2023-06-17T18:55:37.445Z'
      numEdits: 1
      reactions: []
    id: 648e0071a10a0cc8d1f65765
    type: comment
  author: TheBloke
  content: 'I moved it to its own branch because It uses group_size + desc_act and
    therefore either produces gibberish, or runs extremely slow, for the majority
    of users.  Also having two models in one repo doesn''t work well now that there''s
    `quantize_config.json` for AutoGPTQ.


    It''s not gone, it''s just in a separate branch.  Just as easy to clone as it
    was before.


    If using git, then do:

    ```

    git clone -b actorder https://huggingface.co/TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g

    ```


    If using text-generation-webui to download in the UI, type:

    ```

    TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g:actorder

    ```


    And if using text-generation-webui''s `download-model.py` on the command line,
    do:

    ```

    python download-model.py TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g --branch actorder

    ```

    '
  created_at: 2023-06-17 17:50:25+00:00
  edited: true
  hidden: false
  id: 648e0071a10a0cc8d1f65765
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6351bdcc21c37db00cbb1c9d/kSE7O99Du60DaBayOYSaR.jpeg?w=200&h=200&f=face
      fullname: qeternity
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: qeternity
      type: user
    createdAt: '2023-06-17T18:54:03.000Z'
    data:
      edited: false
      editors:
      - qeternity
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9437362551689148
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6351bdcc21c37db00cbb1c9d/kSE7O99Du60DaBayOYSaR.jpeg?w=200&h=200&f=face
          fullname: qeternity
          isHf: false
          isPro: false
          name: qeternity
          type: user
        html: '<p>Ah right, apologies I missed that. Didn''t even bother checking
          other branches on HF. FWIW on exllama, the perf difference is very minimal.</p>

          <p>Anyway, thanks for all your work! We''re Patreons :)</p>

          '
        raw: 'Ah right, apologies I missed that. Didn''t even bother checking other
          branches on HF. FWIW on exllama, the perf difference is very minimal.


          Anyway, thanks for all your work! We''re Patreons :)'
        updatedAt: '2023-06-17T18:54:03.224Z'
      numEdits: 0
      reactions: []
    id: 648e014bc20f218a8cb99955
    type: comment
  author: qeternity
  content: 'Ah right, apologies I missed that. Didn''t even bother checking other
    branches on HF. FWIW on exllama, the perf difference is very minimal.


    Anyway, thanks for all your work! We''re Patreons :)'
  created_at: 2023-06-17 17:54:03+00:00
  edited: false
  hidden: false
  id: 648e014bc20f218a8cb99955
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-17T19:01:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9860151410102844
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No worries - I really should have mentioned all that in the README!   I''ve
          been planning to revamp these older repos but I''ve not had time yet. So
          I''ve just been tidying up issues like this as people raise them but haven''t
          dealt with them properly yet.</p>

          <p>And yeah fair enough, exllama is amazing. Now I''ve learned quite how
          good it is I''m planning to start adding more GPTQ choice, like having an
          act_order + desc_act model for every GPTQ repo.</p>

          <p>Out of interest, what is it that you like about Vicuna 1.1 compared to
          all the models that have come out since? Wizard-Vicuna-Uncensored, WizardLM-Uncensored,
          Nous-Hermes, etc?  I''d rather thought that it had been superseded by now.</p>

          <p>Thanks very much for your support!</p>

          '
        raw: 'No worries - I really should have mentioned all that in the README!   I''ve
          been planning to revamp these older repos but I''ve not had time yet. So
          I''ve just been tidying up issues like this as people raise them but haven''t
          dealt with them properly yet.


          And yeah fair enough, exllama is amazing. Now I''ve learned quite how good
          it is I''m planning to start adding more GPTQ choice, like having an act_order
          + desc_act model for every GPTQ repo.


          Out of interest, what is it that you like about Vicuna 1.1 compared to all
          the models that have come out since? Wizard-Vicuna-Uncensored, WizardLM-Uncensored,
          Nous-Hermes, etc?  I''d rather thought that it had been superseded by now.


          Thanks very much for your support!'
        updatedAt: '2023-06-17T19:01:49.628Z'
      numEdits: 0
      reactions: []
    id: 648e031d439443b085407b78
    type: comment
  author: TheBloke
  content: 'No worries - I really should have mentioned all that in the README!   I''ve
    been planning to revamp these older repos but I''ve not had time yet. So I''ve
    just been tidying up issues like this as people raise them but haven''t dealt
    with them properly yet.


    And yeah fair enough, exllama is amazing. Now I''ve learned quite how good it
    is I''m planning to start adding more GPTQ choice, like having an act_order +
    desc_act model for every GPTQ repo.


    Out of interest, what is it that you like about Vicuna 1.1 compared to all the
    models that have come out since? Wizard-Vicuna-Uncensored, WizardLM-Uncensored,
    Nous-Hermes, etc?  I''d rather thought that it had been superseded by now.


    Thanks very much for your support!'
  created_at: 2023-06-17 18:01:49+00:00
  edited: false
  hidden: false
  id: 648e031d439443b085407b78
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/vicuna-7B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: safetensors
