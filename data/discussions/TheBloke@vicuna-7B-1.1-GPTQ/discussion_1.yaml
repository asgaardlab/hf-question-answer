!!python/object:huggingface_hub.community.DiscussionWithDetails
author: steppi
conflicting_files: null
created_at: 2023-04-16 22:20:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c926d36490d659a9ceb28e20d4b6216.svg
      fullname: Stefano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: steppi
      type: user
    createdAt: '2023-04-16T23:20:11.000Z'
    data:
      edited: false
      editors:
      - steppi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c926d36490d659a9ceb28e20d4b6216.svg
          fullname: Stefano
          isHf: false
          isPro: false
          name: steppi
          type: user
        html: '<p>Hi, since I updated fastchat to version 0.2.2 I can no longer make
          the 4-bit GPTQ work because I get this error:</p>

          <p>python3 -m fastchat.serve.cli --model-path models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g
          --wbits 4 --groupsize 128<br>usage: cli.py [-h] [--model-path MODEL_PATH]
          [--device {cpu,cuda,mps}] [--num-gpus NUM_GPUS] [--load-8bit]<br>              [--conv-template
          CONV_TEMPLATE] [--temperature TEMPERATURE] [--max-new-tokens MAX_NEW_TOKENS]
          [--style {simple,rich}]<br>              [--debug]<br>cli.py: error: unrecognized
          arguments: --wbits 4 --groupsize 128 </p>

          <p>How can I fix this? Thank you bye!</p>

          '
        raw: "Hi, since I updated fastchat to version 0.2.2 I can no longer make the\
          \ 4-bit GPTQ work because I get this error:\r\n\r\npython3 -m fastchat.serve.cli\
          \ --model-path models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g --wbits 4 --groupsize\
          \ 128\r\nusage: cli.py [-h] [--model-path MODEL_PATH] [--device {cpu,cuda,mps}]\
          \ [--num-gpus NUM_GPUS] [--load-8bit]\r\n              [--conv-template\
          \ CONV_TEMPLATE] [--temperature TEMPERATURE] [--max-new-tokens MAX_NEW_TOKENS]\
          \ [--style {simple,rich}]\r\n              [--debug]\r\ncli.py: error: unrecognized\
          \ arguments: --wbits 4 --groupsize 128 \r\n\r\nHow can I fix this? Thank\
          \ you bye!\r\n\r\n"
        updatedAt: '2023-04-16T23:20:11.862Z'
      numEdits: 0
      reactions: []
    id: 643c82abb409fef15e0be9bc
    type: comment
  author: steppi
  content: "Hi, since I updated fastchat to version 0.2.2 I can no longer make the\
    \ 4-bit GPTQ work because I get this error:\r\n\r\npython3 -m fastchat.serve.cli\
    \ --model-path models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g --wbits 4 --groupsize\
    \ 128\r\nusage: cli.py [-h] [--model-path MODEL_PATH] [--device {cpu,cuda,mps}]\
    \ [--num-gpus NUM_GPUS] [--load-8bit]\r\n              [--conv-template CONV_TEMPLATE]\
    \ [--temperature TEMPERATURE] [--max-new-tokens MAX_NEW_TOKENS] [--style {simple,rich}]\r\
    \n              [--debug]\r\ncli.py: error: unrecognized arguments: --wbits 4\
    \ --groupsize 128 \r\n\r\nHow can I fix this? Thank you bye!\r\n\r\n"
  created_at: 2023-04-16 22:20:11+00:00
  edited: false
  hidden: false
  id: 643c82abb409fef15e0be9bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T23:33:02.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m confused. When has FastChat ever supported GPTQ?  I didn''t
          know it did. And I can''t see any recent commits that would affect this.</p>

          <p>It''s <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui"><code>text-generation-webui</code></a>
          that supports GPTQ with arguments <code>--wbits 4 --groupsize 128</code>.</p>

          <p>I think you might be confusing the two pieces of software?</p>

          '
        raw: 'I''m confused. When has FastChat ever supported GPTQ?  I didn''t know
          it did. And I can''t see any recent commits that would affect this.


          It''s [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui)
          that supports GPTQ with arguments `--wbits 4 --groupsize 128`.


          I think you might be confusing the two pieces of software?'
        updatedAt: '2023-04-16T23:33:14.459Z'
      numEdits: 1
      reactions: []
    id: 643c85ae5ff72e5a4ea22327
    type: comment
  author: TheBloke
  content: 'I''m confused. When has FastChat ever supported GPTQ?  I didn''t know
    it did. And I can''t see any recent commits that would affect this.


    It''s [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui)
    that supports GPTQ with arguments `--wbits 4 --groupsize 128`.


    I think you might be confusing the two pieces of software?'
  created_at: 2023-04-16 22:33:02+00:00
  edited: true
  hidden: false
  id: 643c85ae5ff72e5a4ea22327
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c926d36490d659a9ceb28e20d4b6216.svg
      fullname: Stefano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: steppi
      type: user
    createdAt: '2023-04-16T23:41:08.000Z'
    data:
      edited: false
      editors:
      - steppi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c926d36490d659a9ceb28e20d4b6216.svg
          fullname: Stefano
          isHf: false
          isPro: false
          name: steppi
          type: user
        html: '<p>You''re right, but with the 0.1 fi FastChat version it was enough
          to enter a directory called "repository" where inside it was enough to do
          git clone of GPTQ for LLAMA, then launch the cuda setup and you were able
          to get GPTQ to go with FastChat, which I find much better than text-generation.</p>

          <p>Below I paste a guide taken from Medium to do what I described above.</p>

          <p>The problem is that with version 0.2.2 I can''t do it anymore. The advantage
          is that FastChat + the 4bit model = super speed! If you try this GitHub
          <a rel="nofollow" href="https://github.com/thisserand/FastChat.git">https://github.com/thisserand/FastChat.git</a></p>

          <p>You can still install FastChat 0.1</p>

          <p>Medium: <a rel="nofollow" href="https://medium.com/@martin-thissen/vicuna-13b-best-free-chatgpt-alternative-according-to-gpt-4-tutorial-gpu-ec6eb513a717">https://medium.com/@martin-thissen/vicuna-13b-best-free-chatgpt-alternative-according-to-gpt-4-tutorial-gpu-ec6eb513a717</a></p>

          '
        raw: 'You''re right, but with the 0.1 fi FastChat version it was enough to
          enter a directory called "repository" where inside it was enough to do git
          clone of GPTQ for LLAMA, then launch the cuda setup and you were able to
          get GPTQ to go with FastChat, which I find much better than text-generation.


          Below I paste a guide taken from Medium to do what I described above.


          The problem is that with version 0.2.2 I can''t do it anymore. The advantage
          is that FastChat + the 4bit model = super speed! If you try this GitHub
          https://github.com/thisserand/FastChat.git


          You can still install FastChat 0.1


          Medium: https://medium.com/@martin-thissen/vicuna-13b-best-free-chatgpt-alternative-according-to-gpt-4-tutorial-gpu-ec6eb513a717'
        updatedAt: '2023-04-16T23:41:08.583Z'
      numEdits: 0
      reactions: []
    id: 643c87942168686700420c3c
    type: comment
  author: steppi
  content: 'You''re right, but with the 0.1 fi FastChat version it was enough to enter
    a directory called "repository" where inside it was enough to do git clone of
    GPTQ for LLAMA, then launch the cuda setup and you were able to get GPTQ to go
    with FastChat, which I find much better than text-generation.


    Below I paste a guide taken from Medium to do what I described above.


    The problem is that with version 0.2.2 I can''t do it anymore. The advantage is
    that FastChat + the 4bit model = super speed! If you try this GitHub https://github.com/thisserand/FastChat.git


    You can still install FastChat 0.1


    Medium: https://medium.com/@martin-thissen/vicuna-13b-best-free-chatgpt-alternative-according-to-gpt-4-tutorial-gpu-ec6eb513a717'
  created_at: 2023-04-16 22:41:08+00:00
  edited: false
  hidden: false
  id: 643c87942168686700420c3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T23:42:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh, interesting. I suppose they must have borrowed some code from
          text-generation-webui then.  I''d no idea that was possible.  Sounds like
          their recent rewrite must have removed the ability.</p>

          <p>Why not just run text-generation-webui instead?</p>

          '
        raw: 'Oh, interesting. I suppose they must have borrowed some code from text-generation-webui
          then.  I''d no idea that was possible.  Sounds like their recent rewrite
          must have removed the ability.


          Why not just run text-generation-webui instead?'
        updatedAt: '2023-04-16T23:42:51.704Z'
      numEdits: 0
      reactions: []
    id: 643c87fbd9a06e038df3a41f
    type: comment
  author: TheBloke
  content: 'Oh, interesting. I suppose they must have borrowed some code from text-generation-webui
    then.  I''d no idea that was possible.  Sounds like their recent rewrite must
    have removed the ability.


    Why not just run text-generation-webui instead?'
  created_at: 2023-04-16 22:42:51+00:00
  edited: false
  hidden: false
  id: 643c87fbd9a06e038df3a41f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c926d36490d659a9ceb28e20d4b6216.svg
      fullname: Stefano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: steppi
      type: user
    createdAt: '2023-04-16T23:46:45.000Z'
    data:
      edited: false
      editors:
      - steppi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c926d36490d659a9ceb28e20d4b6216.svg
          fullname: Stefano
          isHf: false
          isPro: false
          name: steppi
          type: user
        html: '<p>Because I''ve tried both systems and I believe that FastChat is
          much faster and more accurate in the answers. I know it depends on the model,
          but I''ve done hundreds of tests and I''m sure FastChat is more orderly
          and accurate in the answers. Now I''m downloading your TheBloke/vicuna-13B-1.1-HF
          model that I can run without the 4bit. I''ll tell you in a few minutes...</p>

          '
        raw: Because I've tried both systems and I believe that FastChat is much faster
          and more accurate in the answers. I know it depends on the model, but I've
          done hundreds of tests and I'm sure FastChat is more orderly and accurate
          in the answers. Now I'm downloading your TheBloke/vicuna-13B-1.1-HF model
          that I can run without the 4bit. I'll tell you in a few minutes...
        updatedAt: '2023-04-16T23:46:45.485Z'
      numEdits: 0
      reactions: []
    id: 643c88e52eeda4c05156c617
    type: comment
  author: steppi
  content: Because I've tried both systems and I believe that FastChat is much faster
    and more accurate in the answers. I know it depends on the model, but I've done
    hundreds of tests and I'm sure FastChat is more orderly and accurate in the answers.
    Now I'm downloading your TheBloke/vicuna-13B-1.1-HF model that I can run without
    the 4bit. I'll tell you in a few minutes...
  created_at: 2023-04-16 22:46:45+00:00
  edited: false
  hidden: false
  id: 643c88e52eeda4c05156c617
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T00:00:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I just read the article you linked and now I understand. Martin
          Thissen made his own repo, merging GPTQ-for-LLaMa into FastChat.  So you
          weren''t using Fastchat, you were using a fork of FastChat.  If  you wanted
          this to continue to work, you''d need to wait for Martin to update for FastChat
          0.2.2 - or update the code yourself.</p>

          <p>It''s good to hear FastChat is faster. But I can''t see how it could
          possibly be more accurate, because they were literally using the same code
          for inference. With Martin''s fork, it just calls GPTQ-for-LLaMA to do the
          inference, which is the same as what text-generation-webui does.</p>

          <p>What prompts were you using? One explanation for a perceived increase
          in accuracy could be that FastChat automatically applied a suitable prompt
          template, but you''ve not used that same template when using text-generation-webui.  When
          I query Vicuna, I use this prompt format:</p>

          <pre><code>Below is an instruction that describes a task. Write a response
          that appropriately completes the request.

          ### Instruction:

          prompt goes here

          ### Response:"

          </code></pre>

          <p>text-generation-webui has a feature where you can define a template and
          then save it to be used for each request. It''s in the bottom left of the
          inference UI.</p>

          <p>I''ll close this now as it''s not related to my file.  I hope you manage
          to get it working OK for your needs.</p>

          '
        raw: 'OK I just read the article you linked and now I understand. Martin Thissen
          made his own repo, merging GPTQ-for-LLaMa into FastChat.  So you weren''t
          using Fastchat, you were using a fork of FastChat.  If  you wanted this
          to continue to work, you''d need to wait for Martin to update for FastChat
          0.2.2 - or update the code yourself.


          It''s good to hear FastChat is faster. But I can''t see how it could possibly
          be more accurate, because they were literally using the same code for inference.
          With Martin''s fork, it just calls GPTQ-for-LLaMA to do the inference, which
          is the same as what text-generation-webui does.


          What prompts were you using? One explanation for a perceived increase in
          accuracy could be that FastChat automatically applied a suitable prompt
          template, but you''ve not used that same template when using text-generation-webui.  When
          I query Vicuna, I use this prompt format:

          ```

          Below is an instruction that describes a task. Write a response that appropriately
          completes the request.

          ### Instruction:

          prompt goes here

          ### Response:"

          ```


          text-generation-webui has a feature where you can define a template and
          then save it to be used for each request. It''s in the bottom left of the
          inference UI.


          I''ll close this now as it''s not related to my file.  I hope you manage
          to get it working OK for your needs.'
        updatedAt: '2023-04-17T00:00:59.566Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - steppi
      relatedEventId: 643c8c3b21686867004222ec
    id: 643c8c3b21686867004222eb
    type: comment
  author: TheBloke
  content: 'OK I just read the article you linked and now I understand. Martin Thissen
    made his own repo, merging GPTQ-for-LLaMa into FastChat.  So you weren''t using
    Fastchat, you were using a fork of FastChat.  If  you wanted this to continue
    to work, you''d need to wait for Martin to update for FastChat 0.2.2 - or update
    the code yourself.


    It''s good to hear FastChat is faster. But I can''t see how it could possibly
    be more accurate, because they were literally using the same code for inference.
    With Martin''s fork, it just calls GPTQ-for-LLaMA to do the inference, which is
    the same as what text-generation-webui does.


    What prompts were you using? One explanation for a perceived increase in accuracy
    could be that FastChat automatically applied a suitable prompt template, but you''ve
    not used that same template when using text-generation-webui.  When I query Vicuna,
    I use this prompt format:

    ```

    Below is an instruction that describes a task. Write a response that appropriately
    completes the request.

    ### Instruction:

    prompt goes here

    ### Response:"

    ```


    text-generation-webui has a feature where you can define a template and then save
    it to be used for each request. It''s in the bottom left of the inference UI.


    I''ll close this now as it''s not related to my file.  I hope you manage to get
    it working OK for your needs.'
  created_at: 2023-04-16 23:00:59+00:00
  edited: false
  hidden: false
  id: 643c8c3b21686867004222eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T00:00:59.000Z'
    data:
      status: closed
    id: 643c8c3b21686867004222ec
    type: status-change
  author: TheBloke
  created_at: 2023-04-16 23:00:59+00:00
  id: 643c8c3b21686867004222ec
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/vicuna-7B-1.1-GPTQ
repo_type: model
status: closed
target_branch: null
title: FastChat - error
