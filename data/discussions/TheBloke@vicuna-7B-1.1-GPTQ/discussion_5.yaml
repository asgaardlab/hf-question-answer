!!python/object:huggingface_hub.community.DiscussionWithDetails
author: enginee
conflicting_files: null
created_at: 2023-05-21 21:44:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e8447962115e4ec94997c0ebcf92d5.svg
      fullname: engineer gaming
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: enginee
      type: user
    createdAt: '2023-05-21T22:44:49.000Z'
    data:
      edited: false
      editors:
      - enginee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e8447962115e4ec94997c0ebcf92d5.svg
          fullname: engineer gaming
          isHf: false
          isPro: false
          name: enginee
          type: user
        html: '<p>i have this error:</p>

          <p>OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index
          or flax_model.msgpack found in directory models\TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g.</p>

          '
        raw: "i have this error:\r\n\r\nOSError: Error no file named pytorch_model.bin,\
          \ tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory\
          \ models\\TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g."
        updatedAt: '2023-05-21T22:44:49.982Z'
      numEdits: 0
      reactions: []
    id: 646a9ee135c7a57f936b7932
    type: comment
  author: enginee
  content: "i have this error:\r\n\r\nOSError: Error no file named pytorch_model.bin,\
    \ tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory models\\\
    TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g."
  created_at: 2023-05-21 21:44:49+00:00
  edited: false
  hidden: false
  id: 646a9ee135c7a57f936b7932
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T23:12:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please check the README again - you need to apply the GPTQ parameters.</p>

          '
        raw: Please check the README again - you need to apply the GPTQ parameters.
        updatedAt: '2023-05-23T23:12:51.478Z'
      numEdits: 0
      reactions: []
    id: 646d4873acc13867a13a694d
    type: comment
  author: TheBloke
  content: Please check the README again - you need to apply the GPTQ parameters.
  created_at: 2023-05-23 22:12:51+00:00
  edited: false
  hidden: false
  id: 646d4873acc13867a13a694d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
      fullname: F Muntean
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fmuntean
      type: user
    createdAt: '2023-06-10T15:44:58.000Z'
    data:
      edited: false
      editors:
      - fmuntean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7837720513343811
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
          fullname: F Muntean
          isHf: false
          isPro: false
          name: fmuntean
          type: user
        html: '<p>OSError:  model.safetensors or model.safetensors.index.json and
          thus cannot be loaded with <code>safetensors</code>. Please make sure that
          the model has been saved with <code>safe_serialization=True</code> or do
          not set <code>use_safetensors=True</code>. </p>

          <p>  File "C:\GIT\localGPT\run_localGPT.py", line 28, in load_model<br>    model
          = AutoGPTQForCausalLM.from_pretrained(model_id,quantize_config, device="cuda",use_safetensors=True)</p>

          <p>a Python example on how to load this model in GPU would be appreciated.</p>

          '
        raw: "OSError:  model.safetensors or model.safetensors.index.json and thus\
          \ cannot be loaded with `safetensors`. Please make sure that the model has\
          \ been saved with `safe_serialization=True` or do not set `use_safetensors=True`.\
          \ \n\n  File \"C:\\GIT\\localGPT\\run_localGPT.py\", line 28, in load_model\n\
          \    model = AutoGPTQForCausalLM.from_pretrained(model_id,quantize_config,\
          \ device=\"cuda\",use_safetensors=True)\n\na Python example on how to load\
          \ this model in GPU would be appreciated."
        updatedAt: '2023-06-10T15:44:58.869Z'
      numEdits: 0
      reactions: []
    id: 64849a7a410825bba8e8d674
    type: comment
  author: fmuntean
  content: "OSError:  model.safetensors or model.safetensors.index.json and thus cannot\
    \ be loaded with `safetensors`. Please make sure that the model has been saved\
    \ with `safe_serialization=True` or do not set `use_safetensors=True`. \n\n  File\
    \ \"C:\\GIT\\localGPT\\run_localGPT.py\", line 28, in load_model\n    model =\
    \ AutoGPTQForCausalLM.from_pretrained(model_id,quantize_config, device=\"cuda\"\
    ,use_safetensors=True)\n\na Python example on how to load this model in GPU would\
    \ be appreciated."
  created_at: 2023-06-10 14:44:58+00:00
  edited: false
  hidden: false
  id: 64849a7a410825bba8e8d674
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T15:49:36.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23760776221752167
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>set <code>use_safetensors=False</code> for this model. It''s one
          of the very few that I didn''t save in safetensors</p>

          <p>Or use one of the newer, better models like WizardLM-13B-Uncensored-GPTQ
          or Nous-Hermes-13B-GPTQ</p>

          '
        raw: 'set `use_safetensors=False` for this model. It''s one of the very few
          that I didn''t save in safetensors


          Or use one of the newer, better models like WizardLM-13B-Uncensored-GPTQ
          or Nous-Hermes-13B-GPTQ'
        updatedAt: '2023-06-10T15:50:08.916Z'
      numEdits: 1
      reactions: []
    id: 64849b905d8d3eeb85c2976c
    type: comment
  author: TheBloke
  content: 'set `use_safetensors=False` for this model. It''s one of the very few
    that I didn''t save in safetensors


    Or use one of the newer, better models like WizardLM-13B-Uncensored-GPTQ or Nous-Hermes-13B-GPTQ'
  created_at: 2023-06-10 14:49:36+00:00
  edited: true
  hidden: false
  id: 64849b905d8d3eeb85c2976c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
      fullname: F Muntean
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fmuntean
      type: user
    createdAt: '2023-06-10T19:55:48.000Z'
    data:
      edited: true
      editors:
      - fmuntean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8122394680976868
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
          fullname: F Muntean
          isHf: false
          isPro: false
          name: fmuntean
          type: user
        html: '<blockquote>

          <p>set <code>use_safetensors=False</code> for this model. It''s one of the
          very few that I didn''t save in safetensors</p>

          <p>Or use one of the newer, better models like WizardLM-13B-Uncensored-GPTQ
          or Nous-Hermes-13B-GPTQ</p>

          </blockquote>

          <p>Is still not working:<br>TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g does not
          appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or
          flax_model.msgpack.</p>

          <p>This is the code that I am using:<br>from auto_gptq import AutoGPTQForCausalLM,
          BaseQuantizeConfig<br>model_id = "TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g"</p>

          <p>quantize_config = BaseQuantizeConfig(<br>                bits=4,  # quantize
          model to 4-bit<br>                group_size=128,  # it is recommended to
          set the value to 128<br>                desc_act=False,  # set to False
          can significantly speed up inference but the perplexity may slightly bad<br>            )</p>

          <p>model = AutoGPTQForCausalLM.from_pretrained(model_id,quantize_config,
          device="cuda",use_safetensors=False)</p>

          <p>13B is too big for my GPU. I also tried: TheBloke/wizardLM-7B-GPTQ and
          get the same error.</p>

          '
        raw: "> set `use_safetensors=False` for this model. It's one of the very few\
          \ that I didn't save in safetensors\n> \n> Or use one of the newer, better\
          \ models like WizardLM-13B-Uncensored-GPTQ or Nous-Hermes-13B-GPTQ\n\nIs\
          \ still not working:\nTheBloke/vicuna-7B-1.1-GPTQ-4bit-128g does not appear\
          \ to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\
          \nThis is the code that I am using:\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nmodel_id = \"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"\
          \n\nquantize_config = BaseQuantizeConfig(\n                bits=4,  # quantize\
          \ model to 4-bit\n                group_size=128,  # it is recommended to\
          \ set the value to 128\n                desc_act=False,  # set to False\
          \ can significantly speed up inference but the perplexity may slightly bad\
          \ \n            )\n\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id,quantize_config,\
          \ device=\"cuda\",use_safetensors=False)\n\n13B is too big for my GPU. I\
          \ also tried: TheBloke/wizardLM-7B-GPTQ and get the same error."
        updatedAt: '2023-06-10T19:58:27.939Z'
      numEdits: 1
      reactions: []
    id: 6484d5446bc1cfb5dc18499e
    type: comment
  author: fmuntean
  content: "> set `use_safetensors=False` for this model. It's one of the very few\
    \ that I didn't save in safetensors\n> \n> Or use one of the newer, better models\
    \ like WizardLM-13B-Uncensored-GPTQ or Nous-Hermes-13B-GPTQ\n\nIs still not working:\n\
    TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g does not appear to have a file named pytorch_model.bin,\
    \ tf_model.h5, model.ckpt or flax_model.msgpack.\n\nThis is the code that I am\
    \ using:\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nmodel_id\
    \ = \"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"\n\nquantize_config = BaseQuantizeConfig(\n\
    \                bits=4,  # quantize model to 4-bit\n                group_size=128,\
    \  # it is recommended to set the value to 128\n                desc_act=False,\
    \  # set to False can significantly speed up inference but the perplexity may\
    \ slightly bad \n            )\n\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id,quantize_config,\
    \ device=\"cuda\",use_safetensors=False)\n\n13B is too big for my GPU. I also\
    \ tried: TheBloke/wizardLM-7B-GPTQ and get the same error."
  created_at: 2023-06-10 18:55:48+00:00
  edited: true
  hidden: false
  id: 6484d5446bc1cfb5dc18499e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T20:04:47.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7325669527053833
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I just added a quantize_config.json so you don''t need to pass the
          <code>quantize_config</code>.  But you do need to pass <code>model_basename</code></p>

          <p>Try this:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          auto_gptq <span class="hljs-keyword">import</span> AutoGPTQForCausalLM

          model_id = <span class="hljs-string">"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g"</span>

          model_basename=<span class="hljs-string">"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"</span>


          model = AutoGPTQForCausalLM.from_pretrained(model_id,device=<span class="hljs-string">"cuda:0"</span>,use_safetensors=<span
          class="hljs-literal">False</span>, quantize_config=<span class="hljs-literal">None</span>,
          model_basename=model_basename)

          </code></pre>

          <p>That''s also the issue if you''re trying models like WizardLM-7B-Uncensored
          (which is a better model than this one.)  You need to pass <code>model_basename=</code>
          and the name of the file without the extension.  So this file is called
          <code>vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt</code> so we pass <code>model_basename="vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"</code></p>

          '
        raw: 'I just added a quantize_config.json so you don''t need to pass the `quantize_config`.  But
          you do need to pass `model_basename`


          Try this:


          ```python

          from auto_gptq import AutoGPTQForCausalLM

          model_id = "TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g"

          model_basename="vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"


          model = AutoGPTQForCausalLM.from_pretrained(model_id,device="cuda:0",use_safetensors=False,
          quantize_config=None, model_basename=model_basename)

          ```


          That''s also the issue if you''re trying models like WizardLM-7B-Uncensored
          (which is a better model than this one.)  You need to pass `model_basename=`
          and the name of the file without the extension.  So this file is called
          `vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt` so we pass `model_basename="vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"`'
        updatedAt: '2023-06-10T20:12:49.347Z'
      numEdits: 1
      reactions: []
    id: 6484d75f5816864bb47a7fc3
    type: comment
  author: TheBloke
  content: 'I just added a quantize_config.json so you don''t need to pass the `quantize_config`.  But
    you do need to pass `model_basename`


    Try this:


    ```python

    from auto_gptq import AutoGPTQForCausalLM

    model_id = "TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g"

    model_basename="vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"


    model = AutoGPTQForCausalLM.from_pretrained(model_id,device="cuda:0",use_safetensors=False,
    quantize_config=None, model_basename=model_basename)

    ```


    That''s also the issue if you''re trying models like WizardLM-7B-Uncensored (which
    is a better model than this one.)  You need to pass `model_basename=` and the
    name of the file without the extension.  So this file is called `vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt`
    so we pass `model_basename="vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"`'
  created_at: 2023-06-10 19:04:47+00:00
  edited: true
  hidden: false
  id: 6484d75f5816864bb47a7fc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
      fullname: F Muntean
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fmuntean
      type: user
    createdAt: '2023-06-10T20:17:53.000Z'
    data:
      edited: true
      editors:
      - fmuntean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7230173945426941
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
          fullname: F Muntean
          isHf: false
          isPro: false
          name: fmuntean
          type: user
        html: '<blockquote>

          <p>I just added a quantize_config.json so you don''t need to pass the <code>quantize_config</code>.  But
          you do need to pass <code>model_basename</code></p>

          <p>Try this:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          auto_gptq <span class="hljs-keyword">import</span> AutoGPTQForCausalLM

          model_id = <span class="hljs-string">"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g"</span>

          model_basename=<span class="hljs-string">"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"</span>


          model = AutoGPTQForCausalLM.from_pretrained(model_id,device=<span class="hljs-string">"cuda:0"</span>,use_safetensors=<span
          class="hljs-literal">False</span>, quantize_config=<span class="hljs-literal">None</span>,
          model_basename=model_basename)

          </code></pre>

          <p>That''s also the issue if you''re trying models like WizardLM-7B-Uncensored
          (which is a better model than this one.)  You need to pass <code>model_basename=</code>
          and the name of the file without the extension.  So this file is called
          <code>vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt</code> so we pass <code>model_basename="vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order"</code></p>

          </blockquote>

          <p>still getting the same error:<br>TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g
          does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt
          or flax_model.msgpack.</p>

          <p>I have the module auto-gptq v0.2.2 ; bitsandbytes v0.39.0; transformers
          v4.29.2; torch v2.1.0</p>

          '
        raw: "> I just added a quantize_config.json so you don't need to pass the\
          \ `quantize_config`.  But you do need to pass `model_basename`\n> \n> Try\
          \ this:\n> \n> ```python\n> from auto_gptq import AutoGPTQForCausalLM\n\
          > model_id = \"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"\n> model_basename=\"\
          vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"\n> \n> model = AutoGPTQForCausalLM.from_pretrained(model_id,device=\"\
          cuda:0\",use_safetensors=False, quantize_config=None, model_basename=model_basename)\n\
          > ```\n> \n> That's also the issue if you're trying models like WizardLM-7B-Uncensored\
          \ (which is a better model than this one.)  You need to pass `model_basename=`\
          \ and the name of the file without the extension.  So this file is called\
          \ `vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt` so we pass `model_basename=\"\
          vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"`\n\nstill getting the same error:\n\
          TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g does not appear to have a file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\
          I have the module auto-gptq v0.2.2 ; bitsandbytes v0.39.0; transformers\
          \ v4.29.2; torch v2.1.0"
        updatedAt: '2023-06-10T20:22:36.057Z'
      numEdits: 1
      reactions: []
    id: 6484da7104474ca2133dc227
    type: comment
  author: fmuntean
  content: "> I just added a quantize_config.json so you don't need to pass the `quantize_config`.\
    \  But you do need to pass `model_basename`\n> \n> Try this:\n> \n> ```python\n\
    > from auto_gptq import AutoGPTQForCausalLM\n> model_id = \"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"\
    \n> model_basename=\"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"\n> \n> model\
    \ = AutoGPTQForCausalLM.from_pretrained(model_id,device=\"cuda:0\",use_safetensors=False,\
    \ quantize_config=None, model_basename=model_basename)\n> ```\n> \n> That's also\
    \ the issue if you're trying models like WizardLM-7B-Uncensored (which is a better\
    \ model than this one.)  You need to pass `model_basename=` and the name of the\
    \ file without the extension.  So this file is called `vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt`\
    \ so we pass `model_basename=\"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"`\n\n\
    still getting the same error:\nTheBloke/vicuna-7B-1.1-GPTQ-4bit-128g does not\
    \ appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\
    \nI have the module auto-gptq v0.2.2 ; bitsandbytes v0.39.0; transformers v4.29.2;\
    \ torch v2.1.0"
  created_at: 2023-06-10 19:17:53+00:00
  edited: true
  hidden: false
  id: 6484da7104474ca2133dc227
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T20:53:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5644248127937317
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>ohhh, I didn't spot before that you were using <code>.from_pretrained()</code>.\
          \  It should be <code>.from_quantized()</code>.  Also this model didn't\
          \ have a fast tokenizer, which I have now added.</p>\n<p>This code works:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ auto_gptq <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, logging, pipeline\n\nmodel_id = <span class=\"\
          hljs-string\">\"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"</span>\nmodel_basename=<span\
          \ class=\"hljs-string\">\"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"</span>\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_id,device=<span class=\"\
          hljs-string\">\"cuda:0\"</span>,use_safetensors=<span class=\"hljs-literal\"\
          >False</span>, quantize_config=<span class=\"hljs-literal\">None</span>,\
          \ model_basename=model_basename)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ use_fast=<span class=\"hljs-literal\">True</span>)\n\nprompt = <span class=\"\
          hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span class=\"\
          hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n\n<span class=\"hljs-comment\"># Inference\
          \ can also be done using transformers' pipeline</span>\n\n<span class=\"\
          hljs-comment\"># Prevent printing spurious transformers error when using\
          \ pipeline with AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\
          \n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"*** Pipeline:\"</span>)\npipe = pipeline(\n    <span class=\"hljs-string\"\
          >\"text-generation\"</span>,\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=<span class=\"hljs-number\">512</span>,\n    temperature=<span\
          \ class=\"hljs-number\">0.7</span>,\n    top_p=<span class=\"hljs-number\"\
          >0.95</span>,\n    repetition_penalty=<span class=\"hljs-number\">1.15</span>\n\
          )\n\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n"
        raw: "ohhh, I didn't spot before that you were using `.from_pretrained()`.\
          \  It should be `.from_quantized()`.  Also this model didn't have a fast\
          \ tokenizer, which I have now added.\n\nThis code works:\n```python\nfrom\
          \ auto_gptq import AutoGPTQForCausalLM\nfrom transformers import AutoTokenizer,\
          \ logging, pipeline\n\nmodel_id = \"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"\
          \nmodel_basename=\"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"\n\nmodel\
          \ = AutoGPTQForCausalLM.from_quantized(model_id,device=\"cuda:0\",use_safetensors=False,\
          \ quantize_config=None, model_basename=model_basename)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
          \ Human: {prompt}\n### Assistant:'''\n\nprint(\"\\n\\n*** Generate:\")\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n\n# Prevent printing spurious transformers error\
          \ when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n```"
        updatedAt: '2023-06-10T20:53:53.049Z'
      numEdits: 0
      reactions: []
    id: 6484e2e1e6d3a544f90884c6
    type: comment
  author: TheBloke
  content: "ohhh, I didn't spot before that you were using `.from_pretrained()`. \
    \ It should be `.from_quantized()`.  Also this model didn't have a fast tokenizer,\
    \ which I have now added.\n\nThis code works:\n```python\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM\nfrom transformers import AutoTokenizer, logging, pipeline\n\
    \nmodel_id = \"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"\nmodel_basename=\"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"\
    \n\nmodel = AutoGPTQForCausalLM.from_quantized(model_id,device=\"cuda:0\",use_safetensors=False,\
    \ quantize_config=None, model_basename=model_basename)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
    \ use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human:\
    \ {prompt}\n### Assistant:'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids =\
    \ tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n\
    # Inference can also be done using transformers' pipeline\n\n# Prevent printing\
    \ spurious transformers error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n   \
    \ top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    ```"
  created_at: 2023-06-10 19:53:53+00:00
  edited: false
  hidden: false
  id: 6484e2e1e6d3a544f90884c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
      fullname: F Muntean
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fmuntean
      type: user
    createdAt: '2023-06-10T21:07:01.000Z'
    data:
      edited: false
      editors:
      - fmuntean
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9840022921562195
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a0468400bb0686e4b8616f36594c56f.svg
          fullname: F Muntean
          isHf: false
          isPro: false
          name: fmuntean
          type: user
        html: '<p>Than you very much for your help. The above code works perfectly
          and is very easy to follow and understand. Is good to have a full example.</p>

          '
        raw: Than you very much for your help. The above code works perfectly and
          is very easy to follow and understand. Is good to have a full example.
        updatedAt: '2023-06-10T21:07:01.887Z'
      numEdits: 0
      reactions: []
    id: 6484e5f55ef9702190585754
    type: comment
  author: fmuntean
  content: Than you very much for your help. The above code works perfectly and is
    very easy to follow and understand. Is good to have a full example.
  created_at: 2023-06-10 20:07:01+00:00
  edited: false
  hidden: false
  id: 6484e5f55ef9702190585754
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/vicuna-7B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: I need help I don't know what to do
