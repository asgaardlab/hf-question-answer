!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vmajor
conflicting_files: null
created_at: 2023-04-27 03:21:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-27T04:21:46.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>13B model seems to work well (although I am trying to diagnose its
          seemingly random refusal to process inputs), but when attempting to use
          the 7B model I get this error:</p>

          <p>return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)<br>RuntimeError:
          Internal: src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),
          serialized.size())]</p>

          <p>Path to 7B tokenizer is correctly set. Both the model and tokenizer are
          locally hosted.</p>

          '
        raw: "13B model seems to work well (although I am trying to diagnose its seemingly\
          \ random refusal to process inputs), but when attempting to use the 7B model\
          \ I get this error:\r\n\r\nreturn _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())]\r\n\r\nPath to 7B tokenizer is correctly set. Both\
          \ the model and tokenizer are locally hosted."
        updatedAt: '2023-04-27T04:21:46.698Z'
      numEdits: 0
      reactions: []
    id: 6449f85adf4e6cb7eaf04611
    type: comment
  author: vmajor
  content: "13B model seems to work well (although I am trying to diagnose its seemingly\
    \ random refusal to process inputs), but when attempting to use the 7B model I\
    \ get this error:\r\n\r\nreturn _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())]\r\n\r\nPath to 7B tokenizer is correctly set. Both the model\
    \ and tokenizer are locally hosted."
  created_at: 2023-04-27 03:21:46+00:00
  edited: false
  hidden: false
  id: 6449f85adf4e6cb7eaf04611
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-27T05:43:24.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>Never mind. I had downloaded the pointer file, not the actual tokenizer.model.</p>

          '
        raw: Never mind. I had downloaded the pointer file, not the actual tokenizer.model.
        updatedAt: '2023-04-27T05:43:24.079Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644a0b7c1af713976c3cc8db
    id: 644a0b7c1af713976c3cc8da
    type: comment
  author: vmajor
  content: Never mind. I had downloaded the pointer file, not the actual tokenizer.model.
  created_at: 2023-04-27 04:43:24+00:00
  edited: false
  hidden: false
  id: 644a0b7c1af713976c3cc8da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-27T05:43:24.000Z'
    data:
      status: closed
    id: 644a0b7c1af713976c3cc8db
    type: status-change
  author: vmajor
  created_at: 2023-04-27 04:43:24+00:00
  id: 644a0b7c1af713976c3cc8db
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-27T07:19:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Glad you got it sorted.</p>

          <blockquote>

          <p>13B model seems to work well (although I am trying to diagnose its seemingly
          random refusal to process inputs)</p>

          </blockquote>

          <p>If using text-generation-webui, re-download and update to the latest
          version.  There was a bug introduced in the last couple of days that caused
          Vicuna models to stop generating text very early.  It''s been fixed in the
          last 12 hours.</p>

          '
        raw: 'Glad you got it sorted.


          > 13B model seems to work well (although I am trying to diagnose its seemingly
          random refusal to process inputs)


          If using text-generation-webui, re-download and update to the latest version.  There
          was a bug introduced in the last couple of days that caused Vicuna models
          to stop generating text very early.  It''s been fixed in the last 12 hours.'
        updatedAt: '2023-04-27T07:19:54.307Z'
      numEdits: 0
      reactions: []
    id: 644a221a97fcd34670864ae4
    type: comment
  author: TheBloke
  content: 'Glad you got it sorted.


    > 13B model seems to work well (although I am trying to diagnose its seemingly
    random refusal to process inputs)


    If using text-generation-webui, re-download and update to the latest version.  There
    was a bug introduced in the last couple of days that caused Vicuna models to stop
    generating text very early.  It''s been fixed in the last 12 hours.'
  created_at: 2023-04-27 06:19:54+00:00
  edited: false
  hidden: false
  id: 644a221a97fcd34670864ae4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-28T04:33:54.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>I have a standalone python program based on GPTQ-for-LLaMa. It works
          very well, as well as the Alpaca-65b for my needs, but it only works reliable
          if provided with the prompts using input(), it seemingly randomly refuses
          to process prompts given to it programmatically, inside a python loop. After
          two days of trying to make it obey the law I gave up and now have several
          smaller python programs that are driven by a bash script. Other models (every
          7B and 13B GPTQ compatible model that I found on HuggingFace) that I tried
          do not have this problem, but their output quality is not good enough. I
          will try Alpacino 4bit.safetensors now.</p>

          <p>Also, do you know what may be causing the error with my python loop?
          I really ran out of ideas on what to check. Even inserting a conditional
          loop and giving it time, or changing seed on each failure to generate just
          made the model not give me any output at all... almost as if it is self
          aware and extremely stubborn.</p>

          '
        raw: 'I have a standalone python program based on GPTQ-for-LLaMa. It works
          very well, as well as the Alpaca-65b for my needs, but it only works reliable
          if provided with the prompts using input(), it seemingly randomly refuses
          to process prompts given to it programmatically, inside a python loop. After
          two days of trying to make it obey the law I gave up and now have several
          smaller python programs that are driven by a bash script. Other models (every
          7B and 13B GPTQ compatible model that I found on HuggingFace) that I tried
          do not have this problem, but their output quality is not good enough. I
          will try Alpacino 4bit.safetensors now.


          Also, do you know what may be causing the error with my python loop? I really
          ran out of ideas on what to check. Even inserting a conditional loop and
          giving it time, or changing seed on each failure to generate just made the
          model not give me any output at all... almost as if it is self aware and
          extremely stubborn.'
        updatedAt: '2023-04-28T04:33:54.285Z'
      numEdits: 0
      reactions: []
    id: 644b4cb2d4483bfaa07933b0
    type: comment
  author: vmajor
  content: 'I have a standalone python program based on GPTQ-for-LLaMa. It works very
    well, as well as the Alpaca-65b for my needs, but it only works reliable if provided
    with the prompts using input(), it seemingly randomly refuses to process prompts
    given to it programmatically, inside a python loop. After two days of trying to
    make it obey the law I gave up and now have several smaller python programs that
    are driven by a bash script. Other models (every 7B and 13B GPTQ compatible model
    that I found on HuggingFace) that I tried do not have this problem, but their
    output quality is not good enough. I will try Alpacino 4bit.safetensors now.


    Also, do you know what may be causing the error with my python loop? I really
    ran out of ideas on what to check. Even inserting a conditional loop and giving
    it time, or changing seed on each failure to generate just made the model not
    give me any output at all... almost as if it is self aware and extremely stubborn.'
  created_at: 2023-04-28 03:33:54+00:00
  edited: false
  hidden: false
  id: 644b4cb2d4483bfaa07933b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T07:03:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Not really sure. Show the Python code?</p>

          '
        raw: Not really sure. Show the Python code?
        updatedAt: '2023-04-28T07:03:17.414Z'
      numEdits: 0
      reactions: []
    id: 644b6fb5cc032814c011a047
    type: comment
  author: TheBloke
  content: Not really sure. Show the Python code?
  created_at: 2023-04-28 06:03:17+00:00
  edited: false
  hidden: false
  id: 644b6fb5cc032814c011a047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-28T07:33:09.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: "<pre><code>import torch\nimport torch.nn as nn\nimport quant\nfrom\
          \ gptq import GPTQ\nfrom utils import find_layers, DEV, set_seed, get_wikitext2,\
          \ get_ptb, get_c4, get_ptb_new, get_c4_new, get_loaders\nimport transformers\n\
          from transformers import AutoTokenizer\nimport argparse\nimport warnings\n\
          \n# Suppress warnings from the specified modules\nwarnings.filterwarnings(\"\
          ignore\", module=\"safetensors\")\nwarnings.filterwarnings(\"ignore\", module=\"\
          torch\")\n\ndef get_llama(model):\n\n    def skip(*args, **kwargs):\n  \
          \      pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_\
          \ = skip\n    torch.nn.init.normal_ = skip\n    from transformers import\
          \ LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
          \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef\
          \ load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True,\
          \ warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM\n\
          \    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args,\
          \ **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n\
          \    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n\
          \    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
          \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
          \    torch.set_default_dtype(torch.float)\n    if eval:\n        model =\
          \ model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n\
          \        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
          \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model\
          \ ...')\n    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch\
          \ import load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
          \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n      \
          \  quant.autotune_warmup_linear(model, transpose=not (eval))\n        if\
          \ eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
          \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef\
          \ run_llama_inference(\n    model_path,\n    wbits=4,\n    groupsize=-1,\n\
          \    load_path=\"\",\n    text=\"\",\n    min_length=10,\n    max_length=1024,\n\
          \    top_p=0.7,\n    temperature=0.8,\n    device=0,\n):\n\n    if load_path:\n\
          \        model = load_quant(model_path, load_path, wbits, groupsize)\n \
          \   else:\n        model = get_llama(model_path)\n        model.eval()\n\
          \n    model.to(DEV)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=False)\n    input_ids = tokenizer.encode(text, return_tensors=\"\
          pt\").to(DEV)\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n\
          \            input_ids,\n            do_sample=True,\n            min_length=min_length,\n\
          \            max_length=max_length,\n            top_p=top_p,\n        \
          \    temperature=temperature,\n        )\n    return tokenizer.decode([el.item()\
          \ for el in generated_ids[0]])\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"\
          Summarize an article using Vicuna.\")\n    parser.add_argument('--text',\
          \ required=True, help='The text to summarize.')\n    args = parser.parse_args()\n\
          \n    model_path = \"~/models/Vicuna-13B-quantized-128g\"\n    load_path\
          \ = \"~/models/Vicuna-13B-quantized-128g/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\"\
          \n    wbits = 4\n    groupsize = 128\n\n    output = run_llama_inference(\n\
          \        model_path,\n        wbits=wbits,\n        groupsize=groupsize,\n\
          \        load_path=load_path,\n        text=args.text,\n    )\n\n    with\
          \ open(\"output.txt\", \"a\", encoding=\"utf-8\") as f:\n        f.write(f\"\
          {args.text}\\n{output}\\n\")\n\n    print(f\"Output: {output}\")\n\nif __name__\
          \ == \"__main__\":\n    main()\n</code></pre>\n"
        raw: "```\nimport torch\nimport torch.nn as nn\nimport quant\nfrom gptq import\
          \ GPTQ\nfrom utils import find_layers, DEV, set_seed, get_wikitext2, get_ptb,\
          \ get_c4, get_ptb_new, get_c4_new, get_loaders\nimport transformers\nfrom\
          \ transformers import AutoTokenizer\nimport argparse\nimport warnings\n\n\
          # Suppress warnings from the specified modules\nwarnings.filterwarnings(\"\
          ignore\", module=\"safetensors\")\nwarnings.filterwarnings(\"ignore\", module=\"\
          torch\")\n\ndef get_llama(model):\n\n    def skip(*args, **kwargs):\n  \
          \      pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_\
          \ = skip\n    torch.nn.init.normal_ = skip\n    from transformers import\
          \ LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
          \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef\
          \ load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True,\
          \ warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM\n\
          \    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args,\
          \ **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n\
          \    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n\
          \    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
          \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
          \    torch.set_default_dtype(torch.float)\n    if eval:\n        model =\
          \ model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n\
          \        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
          \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model\
          \ ...')\n    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch\
          \ import load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
          \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n      \
          \  quant.autotune_warmup_linear(model, transpose=not (eval))\n        if\
          \ eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
          \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef\
          \ run_llama_inference(\n    model_path,\n    wbits=4,\n    groupsize=-1,\n\
          \    load_path=\"\",\n    text=\"\",\n    min_length=10,\n    max_length=1024,\n\
          \    top_p=0.7,\n    temperature=0.8,\n    device=0,\n):\n\n    if load_path:\n\
          \        model = load_quant(model_path, load_path, wbits, groupsize)\n \
          \   else:\n        model = get_llama(model_path)\n        model.eval()\n\
          \n    model.to(DEV)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=False)\n    input_ids = tokenizer.encode(text, return_tensors=\"\
          pt\").to(DEV)\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n\
          \            input_ids,\n            do_sample=True,\n            min_length=min_length,\n\
          \            max_length=max_length,\n            top_p=top_p,\n        \
          \    temperature=temperature,\n        )\n    return tokenizer.decode([el.item()\
          \ for el in generated_ids[0]])\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"\
          Summarize an article using Vicuna.\")\n    parser.add_argument('--text',\
          \ required=True, help='The text to summarize.')\n    args = parser.parse_args()\n\
          \n    model_path = \"~/models/Vicuna-13B-quantized-128g\"\n    load_path\
          \ = \"~/models/Vicuna-13B-quantized-128g/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\"\
          \n    wbits = 4\n    groupsize = 128\n\n    output = run_llama_inference(\n\
          \        model_path,\n        wbits=wbits,\n        groupsize=groupsize,\n\
          \        load_path=load_path,\n        text=args.text,\n    )\n\n    with\
          \ open(\"output.txt\", \"a\", encoding=\"utf-8\") as f:\n        f.write(f\"\
          {args.text}\\n{output}\\n\")\n\n    print(f\"Output: {output}\")\n\nif __name__\
          \ == \"__main__\":\n    main()\n\n```"
        updatedAt: '2023-04-28T07:33:09.529Z'
      numEdits: 0
      reactions: []
    id: 644b76b513af0231ecc499b7
    type: comment
  author: vmajor
  content: "```\nimport torch\nimport torch.nn as nn\nimport quant\nfrom gptq import\
    \ GPTQ\nfrom utils import find_layers, DEV, set_seed, get_wikitext2, get_ptb,\
    \ get_c4, get_ptb_new, get_c4_new, get_loaders\nimport transformers\nfrom transformers\
    \ import AutoTokenizer\nimport argparse\nimport warnings\n\n# Suppress warnings\
    \ from the specified modules\nwarnings.filterwarnings(\"ignore\", module=\"safetensors\"\
    )\nwarnings.filterwarnings(\"ignore\", module=\"torch\")\n\ndef get_llama(model):\n\
    \n    def skip(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_\
    \ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n\
    \    from transformers import LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
    \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef load_quant(model,\
    \ checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n\
    \    from transformers import LlamaConfig, LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\
    \n    def noop(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_\
    \ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\
    \n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
    \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
    \    torch.set_default_dtype(torch.float)\n    if eval:\n        model = model.eval()\n\
    \    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name\
    \ in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
    \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model ...')\n\
    \    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch import\
    \ load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
    \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
    \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n        quant.autotune_warmup_linear(model,\
    \ transpose=not (eval))\n        if eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
    \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef run_llama_inference(\n\
    \    model_path,\n    wbits=4,\n    groupsize=-1,\n    load_path=\"\",\n    text=\"\
    \",\n    min_length=10,\n    max_length=1024,\n    top_p=0.7,\n    temperature=0.8,\n\
    \    device=0,\n):\n\n    if load_path:\n        model = load_quant(model_path,\
    \ load_path, wbits, groupsize)\n    else:\n        model = get_llama(model_path)\n\
    \        model.eval()\n\n    model.to(DEV)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ use_fast=False)\n    input_ids = tokenizer.encode(text, return_tensors=\"pt\"\
    ).to(DEV)\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n\
    \            input_ids,\n            do_sample=True,\n            min_length=min_length,\n\
    \            max_length=max_length,\n            top_p=top_p,\n            temperature=temperature,\n\
    \        )\n    return tokenizer.decode([el.item() for el in generated_ids[0]])\n\
    \ndef main():\n    parser = argparse.ArgumentParser(description=\"Summarize an\
    \ article using Vicuna.\")\n    parser.add_argument('--text', required=True, help='The\
    \ text to summarize.')\n    args = parser.parse_args()\n\n    model_path = \"\
    ~/models/Vicuna-13B-quantized-128g\"\n    load_path = \"~/models/Vicuna-13B-quantized-128g/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\"\
    \n    wbits = 4\n    groupsize = 128\n\n    output = run_llama_inference(\n  \
    \      model_path,\n        wbits=wbits,\n        groupsize=groupsize,\n     \
    \   load_path=load_path,\n        text=args.text,\n    )\n\n    with open(\"output.txt\"\
    , \"a\", encoding=\"utf-8\") as f:\n        f.write(f\"{args.text}\\n{output}\\\
    n\")\n\n    print(f\"Output: {output}\")\n\nif __name__ == \"__main__\":\n   \
    \ main()\n\n```"
  created_at: 2023-04-28 06:33:09+00:00
  edited: false
  hidden: false
  id: 644b76b513af0231ecc499b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T08:07:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Nice code! I like that.</p>\n<p>OK so using this code I think I\
          \ finally diagnosed an issue that's been bugging me as well. When you say\
          \ \"refuses to process inputs\" do you mean it stops generating really soon?\
          \  If so then I noticed that as well.</p>\n<p>My standard test prompt is:</p>\n\
          <pre><code>Below is an instruction that describes a task. Write a response\
          \ that appropriately completes the request.\n### Instruction:\nWrite a story\
          \ about llamas\n### Response:\n</code></pre>\n<p>And my Vicuna 1.1 7B and\
          \ 13B would consistently answer with: <code>Once upon a time, in a land\
          \ far, far away, there lived a herd of llama</code> and then just stop there.</p>\n\
          <p>I think I just figured it out! I had pad_token : -1 in config.json for\
          \ some reason. And it should have been pad_token: 0.</p>\n<p>I've fixed\
          \ that in the repos and tested again with your code and now it's reliably\
          \ answering correctly:</p>\n<pre><code>root@9f5e0b1e927a:~/gptq-llama# python\
          \ do_inf.py --text \"Below is an instruction that describes a task. Write\
          \ a response that appropriately completes the request.\n### Instruction:\n\
          Write a story about llamas\n### Response:\"\nLoading model ...\nFound 3\
          \ unique KN Linear values.\nWarming up autotune cache ...\n100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:39&lt;00:00,\
          \  3.28s/it]\nFound 1 unique fused mlp KN values.\nWarming up autotune cache\
          \ ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 12/12 [00:15&lt;00:00,  1.30s/it]\nDone.\nOutput: &lt;s&gt; Below is an\
          \ instruction that describes a task. Write a response that appropriately\
          \ completes the request.\n### Instruction:\nWrite a story about llamas\n\
          ### Response:\nOnce upon a time, in a land far, far away, there lived a\
          \ group of llamas. They lived in a beautiful valley surrounded by towering\
          \ mountains. The llamas were happy and content, roaming freely through the\
          \ lush green fields and forests.\n\nOne day, a group of explorers came to\
          \ the valley. They were fascinated by the llamas and wanted to learn more\
          \ about them. The llamas, in turn, were intrigued by the explorers and their\
          \ strange clothing and tools.\n\nAs the days passed, the llamas and the\
          \ explorers became good friends. They learned from each other and taught\
          \ each other their ways of life. The llamas showed the explorers how to\
          \ survive in the harsh climate, while the explorers taught the llamas about\
          \ their culture and civilization.\n\nOne day, the explorers had to leave\
          \ the valley and return to their own land. The llamas were sad to see them\
          \ go, but they knew that they would always have a special place in their\
          \ hearts. The llamas continued to roam freely in the valley, always remembering\
          \ their friends from the land far, far away.\n\nThe end.&lt;/s&gt;\nroot@9f5e0b1e927a:~/gptq-llama#\n\
          </code></pre>\n<p>Please re-download <code>config.json</code> and test again\
          \ with your code and let me know.</p>\n"
        raw: "Nice code! I like that.\n\nOK so using this code I think I finally diagnosed\
          \ an issue that's been bugging me as well. When you say \"refuses to process\
          \ inputs\" do you mean it stops generating really soon?  If so then I noticed\
          \ that as well.\n\nMy standard test prompt is:\n\n```\nBelow is an instruction\
          \ that describes a task. Write a response that appropriately completes the\
          \ request.\n### Instruction:\nWrite a story about llamas\n### Response:\n\
          ```\n\nAnd my Vicuna 1.1 7B and 13B would consistently answer with: `Once\
          \ upon a time, in a land far, far away, there lived a herd of llama` and\
          \ then just stop there.\n\nI think I just figured it out! I had pad_token\
          \ : -1 in config.json for some reason. And it should have been pad_token:\
          \ 0.\n\nI've fixed that in the repos and tested again with your code and\
          \ now it's reliably answering correctly:\n```\nroot@9f5e0b1e927a:~/gptq-llama#\
          \ python do_inf.py --text \"Below is an instruction that describes a task.\
          \ Write a response that appropriately completes the request.\n### Instruction:\n\
          Write a story about llamas\n### Response:\"\nLoading model ...\nFound 3\
          \ unique KN Linear values.\nWarming up autotune cache ...\n100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:39<00:00,  3.28s/it]\n\
          Found 1 unique fused mlp KN values.\nWarming up autotune cache ...\n100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:15<00:00,\
          \  1.30s/it]\nDone.\nOutput: <s> Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\n###\
          \ Instruction:\nWrite a story about llamas\n### Response:\nOnce upon a time,\
          \ in a land far, far away, there lived a group of llamas. They lived in\
          \ a beautiful valley surrounded by towering mountains. The llamas were happy\
          \ and content, roaming freely through the lush green fields and forests.\n\
          \nOne day, a group of explorers came to the valley. They were fascinated\
          \ by the llamas and wanted to learn more about them. The llamas, in turn,\
          \ were intrigued by the explorers and their strange clothing and tools.\n\
          \nAs the days passed, the llamas and the explorers became good friends.\
          \ They learned from each other and taught each other their ways of life.\
          \ The llamas showed the explorers how to survive in the harsh climate, while\
          \ the explorers taught the llamas about their culture and civilization.\n\
          \nOne day, the explorers had to leave the valley and return to their own\
          \ land. The llamas were sad to see them go, but they knew that they would\
          \ always have a special place in their hearts. The llamas continued to roam\
          \ freely in the valley, always remembering their friends from the land far,\
          \ far away.\n\nThe end.</s>\nroot@9f5e0b1e927a:~/gptq-llama#\n```\n\nPlease\
          \ re-download `config.json` and test again with your code and let me know."
        updatedAt: '2023-04-28T08:07:05.248Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vmajor
    id: 644b7ea9db3a59aba083a1ba
    type: comment
  author: TheBloke
  content: "Nice code! I like that.\n\nOK so using this code I think I finally diagnosed\
    \ an issue that's been bugging me as well. When you say \"refuses to process inputs\"\
    \ do you mean it stops generating really soon?  If so then I noticed that as well.\n\
    \nMy standard test prompt is:\n\n```\nBelow is an instruction that describes a\
    \ task. Write a response that appropriately completes the request.\n### Instruction:\n\
    Write a story about llamas\n### Response:\n```\n\nAnd my Vicuna 1.1 7B and 13B\
    \ would consistently answer with: `Once upon a time, in a land far, far away,\
    \ there lived a herd of llama` and then just stop there.\n\nI think I just figured\
    \ it out! I had pad_token : -1 in config.json for some reason. And it should have\
    \ been pad_token: 0.\n\nI've fixed that in the repos and tested again with your\
    \ code and now it's reliably answering correctly:\n```\nroot@9f5e0b1e927a:~/gptq-llama#\
    \ python do_inf.py --text \"Below is an instruction that describes a task. Write\
    \ a response that appropriately completes the request.\n### Instruction:\nWrite\
    \ a story about llamas\n### Response:\"\nLoading model ...\nFound 3 unique KN\
    \ Linear values.\nWarming up autotune cache ...\n100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 12/12 [00:39<00:00,  3.28s/it]\nFound 1 unique fused mlp KN\
    \ values.\nWarming up autotune cache ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 12/12 [00:15<00:00,  1.30s/it]\nDone.\nOutput: <s> Below is an instruction\
    \ that describes a task. Write a response that appropriately completes the request.\n\
    ### Instruction:\nWrite a story about llamas\n### Response:\nOnce upon a time,\
    \ in a land far, far away, there lived a group of llamas. They lived in a beautiful\
    \ valley surrounded by towering mountains. The llamas were happy and content,\
    \ roaming freely through the lush green fields and forests.\n\nOne day, a group\
    \ of explorers came to the valley. They were fascinated by the llamas and wanted\
    \ to learn more about them. The llamas, in turn, were intrigued by the explorers\
    \ and their strange clothing and tools.\n\nAs the days passed, the llamas and\
    \ the explorers became good friends. They learned from each other and taught each\
    \ other their ways of life. The llamas showed the explorers how to survive in\
    \ the harsh climate, while the explorers taught the llamas about their culture\
    \ and civilization.\n\nOne day, the explorers had to leave the valley and return\
    \ to their own land. The llamas were sad to see them go, but they knew that they\
    \ would always have a special place in their hearts. The llamas continued to roam\
    \ freely in the valley, always remembering their friends from the land far, far\
    \ away.\n\nThe end.</s>\nroot@9f5e0b1e927a:~/gptq-llama#\n```\n\nPlease re-download\
    \ `config.json` and test again with your code and let me know."
  created_at: 2023-04-28 07:07:05+00:00
  edited: false
  hidden: false
  id: 644b7ea9db3a59aba083a1ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-28T08:43:09.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: "<p>OK I will try that! Quantizing my own Alpacino 13b at the moment\
          \ so my workstation is going to be a little busy for a while. And I realized\
          \ that I shared the working code, the one that takes a single input() and\
          \ that I am now using as a part of that bash script that I mentioned. Let\
          \ me carefully look at the code that does not work and give you that :)\
          \ I tried all kinds of stuff in here. \"Prewarming\" the model, a while\
          \ loop, nothing works, and feel free to remove both and observe the model\
          \ still not working reliably. When it summarises it does it really well,\
          \ but mostly it returns empty output and just skips ahead. The dependencies\
          \ are all pip installable:</p>\n<pre><code>import torch\nimport torch.nn\
          \ as nn\nimport quant\nfrom gptq import GPTQ\nfrom utils import find_layers,\
          \ DEV, set_seed, get_wikitext2, get_ptb, get_c4, get_ptb_new, get_c4_new,\
          \ get_loaders\nimport transformers\nfrom transformers import AutoTokenizer\n\
          import csv\nimport FinNews as fn\nimport requests\nfrom bs4 import BeautifulSoup\n\
          import argparse\nimport time\nfrom utils import set_seed\nimport random\n\
          \ndef get_llama(model):\n\n    def skip(*args, **kwargs):\n        pass\n\
          \n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_\
          \ = skip\n    torch.nn.init.normal_ = skip\n    from transformers import\
          \ LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
          \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef\
          \ load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True,\
          \ warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM\n\
          \    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args,\
          \ **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n\
          \    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n\
          \    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
          \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
          \    torch.set_default_dtype(torch.float)\n    if eval:\n        model =\
          \ model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n\
          \        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
          \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model\
          \ ...')\n    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch\
          \ import load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
          \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n      \
          \  quant.autotune_warmup_linear(model, transpose=not (eval))\n        if\
          \ eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
          \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef\
          \ run_llama_inference(\n    model,\n    tokenizer,\n    wbits=4,\n    groupsize=-1,\n\
          \    texts=[],\n    min_length=10,\n    max_length=2048,\n    top_p=0.7,\n\
          \    temperature=0.8,\n    device=0,\n):\n    model = model.to(DEV)\n\n\
          \    # Dummy generation for warm-up\n    dummy_input = tokenizer.encode(\"\
          Dummy input for warm-up\", return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n\
          \        _ = model.generate(dummy_input)\n\n    answers = []\n\n    for\
          \ text in texts:\n        input_ids = tokenizer.encode(text, return_tensors=\"\
          pt\").to(DEV)\n\n        answer = \"\"\n        attempts = 0\n        max_attempts\
          \ = 5\n\n        while attempts &lt; max_attempts:\n            with torch.no_grad():\n\
          \                generated_ids = model.generate(\n                    input_ids,\n\
          \                    do_sample=True,\n                    min_length=min_length,\n\
          \                    max_length=max_length,\n                    top_p=top_p,\n\
          \                    temperature=temperature\n                )\n\n    \
          \        output = tokenizer.decode([el.item() for el in generated_ids[0]])\n\
          \            parts = output.split(\"Answer:\")\n\n            if len(parts)\
          \ == 2:\n                answer = parts[1].strip()\n                if len(answer)\
          \ &gt; 10:  # Check if the answer has more than 10 characters\n        \
          \            break\n\n            attempts += 1\n            sleep_time\
          \ = random.uniform(0.1, 0.5)  # Random sleep time between 0.1 and 0.5 seconds\n\
          \            time.sleep(sleep_time)\n\n        answers.append(answer)\n\n\
          \    return answers\n\ndef load_processed_articles(file_name):\n    processed_articles\
          \ = set()\n    try:\n        with open(file_name, 'r') as file:\n      \
          \      for line in file:\n                processed_articles.add(line.strip())\n\
          \    except FileNotFoundError:\n        pass\n    return processed_articles\n\
          \ndef save_processed_articles(file_name, processed_articles):\n    with\
          \ open(file_name, 'w') as file:\n        for article in processed_articles:\n\
          \            file.write(f\"{article}\\n\")\n\n\ndef get_first_paragraphs(url):\n\
          \    response = requests.get(url)\n    soup = BeautifulSoup(response.text,\
          \ 'html.parser')\n\n    # Find the main content of the article using CSS\
          \ selector\n    paragraphs = soup.select('div.group &gt; p')\n\n    if paragraphs:\n\
          \        extracted_text = []\n        for p in paragraphs[:5]:  # Change\
          \ this to 5 to get the first five paragraphs\n            text = p.get_text()\
          \  # Remove the strip=True parameter\n            extracted_text.append(text)\n\
          \        return ' '.join(extracted_text)\n    return \"\"\n\nif __name__\
          \ == \"__main__\":\n    model_path = \"~/models/Vicuna-13B-quantized-128g\"\
          \n    load_path = \"~/models/Vicuna-13B-quantized-128g/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\"\
          \n    wbits = 4\n    groupsize = 128\n\n    # Load the model\n    model\
          \ = load_quant(model_path, load_path, wbits, groupsize)\n    tokenizer =\
          \ AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n    # Fetch\
          \ news articles from CNBC using FinNews library\n    cnbc_feed = fn.CNBC(topics=['finance',\
          \ 'earnings'])\n    cnbc_news = cnbc_feed.get_news()\n\n    # Load processed\
          \ articles\n    processed_articles_file = \"processed_articles.txt\"\n \
          \   processed_articles = load_processed_articles(processed_articles_file)\n\
          \n    # Prepare the texts for inference\n    texts = []\n    articles_to_process\
          \ = []\n\n    for article in cnbc_news:\n        article_id = article['id']\n\
          \n        if article_id in processed_articles:\n            print(f\"Article\
          \ {article_id} already processed\")\n            continue\n\n        url\
          \ = article['link']\n        first_paragraphs = get_first_paragraphs(url)\n\
          \        if first_paragraphs:\n            title = article['title']\n  \
          \          summary_prompt = \"Question: Please provide a concise summary\
          \ of the following news article, capturing the key information and stating\
          \ company ticker symbols, and government entity abbreviations, whenever\
          \ possible: \"\n            texts.append(summary_prompt + title + \". \"\
          \ + first_paragraphs + \" Answer: \")\n            articles_to_process.append(article)\n\
          \        else:\n            print(f\"Could not extract content from {url}\"\
          )\n\n    # Run the inference for all texts\n    summaries = run_llama_inference(\n\
          \        model,\n        tokenizer,\n        wbits=wbits,\n        groupsize=groupsize,\n\
          \        texts=texts,\n        min_length=10,\n        max_length=1024,\n\
          \        top_p=0.7,\n        temperature=0.8,\n    )\n\n    # Write the\
          \ results to the CSV file\n    with open('cnbc_news_summaries.csv', 'w',\
          \ newline='', encoding='utf-8') as csvfile:\n        # Create a CSV writer\
          \ object\n        csv_writer = csv.writer(csvfile)\n\n        # Write the\
          \ header row\n        csv_writer.writerow(['ID', 'Date', 'Title', 'Summary'])\n\
          \n        for idx, summary in enumerate(summaries):\n            article\
          \ = articles_to_process[idx]\n            article_id = article['id']\n \
          \           title = article['title']\n            print(\"Title: \", title)\n\
          \n            if summary:\n                # Write the row to the CSV file\n\
          \                csv_writer.writerow([article_id, article['published'],\
          \ title, summary])\n\n                processed_articles.add(article_id)\n\
          \n                # Clear past attentions and hidden states\n          \
          \      if hasattr(model, 'past'):\n                    del model.past\n\
          \                torch.cuda.empty_cache()\n\n            else:\n       \
          \         # Print an error message if there is no answer in the output\n\
          \                print(\"No answer found in the output.\")\n\n    save_processed_articles(processed_articles_file,\
          \ processed_articles)\n</code></pre>\n"
        raw: "OK I will try that! Quantizing my own Alpacino 13b at the moment so\
          \ my workstation is going to be a little busy for a while. And I realized\
          \ that I shared the working code, the one that takes a single input() and\
          \ that I am now using as a part of that bash script that I mentioned. Let\
          \ me carefully look at the code that does not work and give you that :)\
          \ I tried all kinds of stuff in here. \"Prewarming\" the model, a while\
          \ loop, nothing works, and feel free to remove both and observe the model\
          \ still not working reliably. When it summarises it does it really well,\
          \ but mostly it returns empty output and just skips ahead. The dependencies\
          \ are all pip installable:\n\n```\nimport torch\nimport torch.nn as nn\n\
          import quant\nfrom gptq import GPTQ\nfrom utils import find_layers, DEV,\
          \ set_seed, get_wikitext2, get_ptb, get_c4, get_ptb_new, get_c4_new, get_loaders\n\
          import transformers\nfrom transformers import AutoTokenizer\nimport csv\n\
          import FinNews as fn\nimport requests\nfrom bs4 import BeautifulSoup\nimport\
          \ argparse\nimport time\nfrom utils import set_seed\nimport random\n\ndef\
          \ get_llama(model):\n\n    def skip(*args, **kwargs):\n        pass\n\n\
          \    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_ =\
          \ skip\n    torch.nn.init.normal_ = skip\n    from transformers import LlamaForCausalLM\n\
          \    model = LlamaForCausalLM.from_pretrained(model, torch_dtype='auto')\n\
          \    model.seqlen = 2048\n    return model\n\n\ndef load_quant(model, checkpoint,\
          \ wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n\
          \    from transformers import LlamaConfig, LlamaForCausalLM\n    config\
          \ = LlamaConfig.from_pretrained(model)\n\n    def noop(*args, **kwargs):\n\
          \        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n    torch.nn.init.uniform_\
          \ = noop\n    torch.nn.init.normal_ = noop\n\n    torch.set_default_dtype(torch.half)\n\
          \    transformers.modeling_utils._init_weights = False\n    torch.set_default_dtype(torch.half)\n\
          \    model = LlamaForCausalLM(config)\n    torch.set_default_dtype(torch.float)\n\
          \    if eval:\n        model = model.eval()\n    layers = find_layers(model)\n\
          \    for name in ['lm_head']:\n        if name in layers:\n            del\
          \ layers[name]\n    quant.make_quant_linear(model, layers, wbits, groupsize)\n\
          \n    del layers\n\n    print('Loading model ...')\n    if checkpoint.endswith('.safetensors'):\n\
          \        from safetensors.torch import load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
          \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n      \
          \  quant.autotune_warmup_linear(model, transpose=not (eval))\n        if\
          \ eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
          \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef\
          \ run_llama_inference(\n    model,\n    tokenizer,\n    wbits=4,\n    groupsize=-1,\n\
          \    texts=[],\n    min_length=10,\n    max_length=2048,\n    top_p=0.7,\n\
          \    temperature=0.8,\n    device=0,\n):\n    model = model.to(DEV)\n\n\
          \    # Dummy generation for warm-up\n    dummy_input = tokenizer.encode(\"\
          Dummy input for warm-up\", return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n\
          \        _ = model.generate(dummy_input)\n\n    answers = []\n\n    for\
          \ text in texts:\n        input_ids = tokenizer.encode(text, return_tensors=\"\
          pt\").to(DEV)\n\n        answer = \"\"\n        attempts = 0\n        max_attempts\
          \ = 5\n\n        while attempts < max_attempts:\n            with torch.no_grad():\n\
          \                generated_ids = model.generate(\n                    input_ids,\n\
          \                    do_sample=True,\n                    min_length=min_length,\n\
          \                    max_length=max_length,\n                    top_p=top_p,\n\
          \                    temperature=temperature\n                )\n\n    \
          \        output = tokenizer.decode([el.item() for el in generated_ids[0]])\n\
          \            parts = output.split(\"Answer:\")\n\n            if len(parts)\
          \ == 2:\n                answer = parts[1].strip()\n                if len(answer)\
          \ > 10:  # Check if the answer has more than 10 characters\n           \
          \         break\n\n            attempts += 1\n            sleep_time = random.uniform(0.1,\
          \ 0.5)  # Random sleep time between 0.1 and 0.5 seconds\n            time.sleep(sleep_time)\n\
          \n        answers.append(answer)\n\n    return answers\n\ndef load_processed_articles(file_name):\n\
          \    processed_articles = set()\n    try:\n        with open(file_name,\
          \ 'r') as file:\n            for line in file:\n                processed_articles.add(line.strip())\n\
          \    except FileNotFoundError:\n        pass\n    return processed_articles\n\
          \ndef save_processed_articles(file_name, processed_articles):\n    with\
          \ open(file_name, 'w') as file:\n        for article in processed_articles:\n\
          \            file.write(f\"{article}\\n\")\n\n\ndef get_first_paragraphs(url):\n\
          \    response = requests.get(url)\n    soup = BeautifulSoup(response.text,\
          \ 'html.parser')\n\n    # Find the main content of the article using CSS\
          \ selector\n    paragraphs = soup.select('div.group > p')\n\n    if paragraphs:\n\
          \        extracted_text = []\n        for p in paragraphs[:5]:  # Change\
          \ this to 5 to get the first five paragraphs\n            text = p.get_text()\
          \  # Remove the strip=True parameter\n            extracted_text.append(text)\n\
          \        return ' '.join(extracted_text)\n    return \"\"\n\nif __name__\
          \ == \"__main__\":\n    model_path = \"~/models/Vicuna-13B-quantized-128g\"\
          \n    load_path = \"~/models/Vicuna-13B-quantized-128g/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\"\
          \n    wbits = 4\n    groupsize = 128\n\n    # Load the model\n    model\
          \ = load_quant(model_path, load_path, wbits, groupsize)\n    tokenizer =\
          \ AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n    # Fetch\
          \ news articles from CNBC using FinNews library\n    cnbc_feed = fn.CNBC(topics=['finance',\
          \ 'earnings'])\n    cnbc_news = cnbc_feed.get_news()\n\n    # Load processed\
          \ articles\n    processed_articles_file = \"processed_articles.txt\"\n \
          \   processed_articles = load_processed_articles(processed_articles_file)\n\
          \n    # Prepare the texts for inference\n    texts = []\n    articles_to_process\
          \ = []\n\n    for article in cnbc_news:\n        article_id = article['id']\n\
          \n        if article_id in processed_articles:\n            print(f\"Article\
          \ {article_id} already processed\")\n            continue\n\n        url\
          \ = article['link']\n        first_paragraphs = get_first_paragraphs(url)\n\
          \        if first_paragraphs:\n            title = article['title']\n  \
          \          summary_prompt = \"Question: Please provide a concise summary\
          \ of the following news article, capturing the key information and stating\
          \ company ticker symbols, and government entity abbreviations, whenever\
          \ possible: \"\n            texts.append(summary_prompt + title + \". \"\
          \ + first_paragraphs + \" Answer: \")\n            articles_to_process.append(article)\n\
          \        else:\n            print(f\"Could not extract content from {url}\"\
          )\n\n    # Run the inference for all texts\n    summaries = run_llama_inference(\n\
          \        model,\n        tokenizer,\n        wbits=wbits,\n        groupsize=groupsize,\n\
          \        texts=texts,\n        min_length=10,\n        max_length=1024,\n\
          \        top_p=0.7,\n        temperature=0.8,\n    )\n\n    # Write the\
          \ results to the CSV file\n    with open('cnbc_news_summaries.csv', 'w',\
          \ newline='', encoding='utf-8') as csvfile:\n        # Create a CSV writer\
          \ object\n        csv_writer = csv.writer(csvfile)\n\n        # Write the\
          \ header row\n        csv_writer.writerow(['ID', 'Date', 'Title', 'Summary'])\n\
          \n        for idx, summary in enumerate(summaries):\n            article\
          \ = articles_to_process[idx]\n            article_id = article['id']\n \
          \           title = article['title']\n            print(\"Title: \", title)\n\
          \n            if summary:\n                # Write the row to the CSV file\n\
          \                csv_writer.writerow([article_id, article['published'],\
          \ title, summary])\n\n                processed_articles.add(article_id)\n\
          \n                # Clear past attentions and hidden states\n          \
          \      if hasattr(model, 'past'):\n                    del model.past\n\
          \                torch.cuda.empty_cache()\n\n            else:\n       \
          \         # Print an error message if there is no answer in the output\n\
          \                print(\"No answer found in the output.\")\n\n    save_processed_articles(processed_articles_file,\
          \ processed_articles)\n```"
        updatedAt: '2023-04-28T08:43:32.364Z'
      numEdits: 1
      reactions: []
    id: 644b871db64fb3f65f59ed86
    type: comment
  author: vmajor
  content: "OK I will try that! Quantizing my own Alpacino 13b at the moment so my\
    \ workstation is going to be a little busy for a while. And I realized that I\
    \ shared the working code, the one that takes a single input() and that I am now\
    \ using as a part of that bash script that I mentioned. Let me carefully look\
    \ at the code that does not work and give you that :) I tried all kinds of stuff\
    \ in here. \"Prewarming\" the model, a while loop, nothing works, and feel free\
    \ to remove both and observe the model still not working reliably. When it summarises\
    \ it does it really well, but mostly it returns empty output and just skips ahead.\
    \ The dependencies are all pip installable:\n\n```\nimport torch\nimport torch.nn\
    \ as nn\nimport quant\nfrom gptq import GPTQ\nfrom utils import find_layers, DEV,\
    \ set_seed, get_wikitext2, get_ptb, get_c4, get_ptb_new, get_c4_new, get_loaders\n\
    import transformers\nfrom transformers import AutoTokenizer\nimport csv\nimport\
    \ FinNews as fn\nimport requests\nfrom bs4 import BeautifulSoup\nimport argparse\n\
    import time\nfrom utils import set_seed\nimport random\n\ndef get_llama(model):\n\
    \n    def skip(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_\
    \ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n\
    \    from transformers import LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
    \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef load_quant(model,\
    \ checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n\
    \    from transformers import LlamaConfig, LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\
    \n    def noop(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_\
    \ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\
    \n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
    \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
    \    torch.set_default_dtype(torch.float)\n    if eval:\n        model = model.eval()\n\
    \    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name\
    \ in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
    \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model ...')\n\
    \    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch import\
    \ load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
    \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
    \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n        quant.autotune_warmup_linear(model,\
    \ transpose=not (eval))\n        if eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
    \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef run_llama_inference(\n\
    \    model,\n    tokenizer,\n    wbits=4,\n    groupsize=-1,\n    texts=[],\n\
    \    min_length=10,\n    max_length=2048,\n    top_p=0.7,\n    temperature=0.8,\n\
    \    device=0,\n):\n    model = model.to(DEV)\n\n    # Dummy generation for warm-up\n\
    \    dummy_input = tokenizer.encode(\"Dummy input for warm-up\", return_tensors=\"\
    pt\").to(device)\n    with torch.no_grad():\n        _ = model.generate(dummy_input)\n\
    \n    answers = []\n\n    for text in texts:\n        input_ids = tokenizer.encode(text,\
    \ return_tensors=\"pt\").to(DEV)\n\n        answer = \"\"\n        attempts =\
    \ 0\n        max_attempts = 5\n\n        while attempts < max_attempts:\n    \
    \        with torch.no_grad():\n                generated_ids = model.generate(\n\
    \                    input_ids,\n                    do_sample=True,\n       \
    \             min_length=min_length,\n                    max_length=max_length,\n\
    \                    top_p=top_p,\n                    temperature=temperature\n\
    \                )\n\n            output = tokenizer.decode([el.item() for el\
    \ in generated_ids[0]])\n            parts = output.split(\"Answer:\")\n\n   \
    \         if len(parts) == 2:\n                answer = parts[1].strip()\n   \
    \             if len(answer) > 10:  # Check if the answer has more than 10 characters\n\
    \                    break\n\n            attempts += 1\n            sleep_time\
    \ = random.uniform(0.1, 0.5)  # Random sleep time between 0.1 and 0.5 seconds\n\
    \            time.sleep(sleep_time)\n\n        answers.append(answer)\n\n    return\
    \ answers\n\ndef load_processed_articles(file_name):\n    processed_articles =\
    \ set()\n    try:\n        with open(file_name, 'r') as file:\n            for\
    \ line in file:\n                processed_articles.add(line.strip())\n    except\
    \ FileNotFoundError:\n        pass\n    return processed_articles\n\ndef save_processed_articles(file_name,\
    \ processed_articles):\n    with open(file_name, 'w') as file:\n        for article\
    \ in processed_articles:\n            file.write(f\"{article}\\n\")\n\n\ndef get_first_paragraphs(url):\n\
    \    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\
    \n    # Find the main content of the article using CSS selector\n    paragraphs\
    \ = soup.select('div.group > p')\n\n    if paragraphs:\n        extracted_text\
    \ = []\n        for p in paragraphs[:5]:  # Change this to 5 to get the first\
    \ five paragraphs\n            text = p.get_text()  # Remove the strip=True parameter\n\
    \            extracted_text.append(text)\n        return ' '.join(extracted_text)\n\
    \    return \"\"\n\nif __name__ == \"__main__\":\n    model_path = \"~/models/Vicuna-13B-quantized-128g\"\
    \n    load_path = \"~/models/Vicuna-13B-quantized-128g/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\"\
    \n    wbits = 4\n    groupsize = 128\n\n    # Load the model\n    model = load_quant(model_path,\
    \ load_path, wbits, groupsize)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ use_fast=False)\n\n    # Fetch news articles from CNBC using FinNews library\n\
    \    cnbc_feed = fn.CNBC(topics=['finance', 'earnings'])\n    cnbc_news = cnbc_feed.get_news()\n\
    \n    # Load processed articles\n    processed_articles_file = \"processed_articles.txt\"\
    \n    processed_articles = load_processed_articles(processed_articles_file)\n\n\
    \    # Prepare the texts for inference\n    texts = []\n    articles_to_process\
    \ = []\n\n    for article in cnbc_news:\n        article_id = article['id']\n\n\
    \        if article_id in processed_articles:\n            print(f\"Article {article_id}\
    \ already processed\")\n            continue\n\n        url = article['link']\n\
    \        first_paragraphs = get_first_paragraphs(url)\n        if first_paragraphs:\n\
    \            title = article['title']\n            summary_prompt = \"Question:\
    \ Please provide a concise summary of the following news article, capturing the\
    \ key information and stating company ticker symbols, and government entity abbreviations,\
    \ whenever possible: \"\n            texts.append(summary_prompt + title + \"\
    . \" + first_paragraphs + \" Answer: \")\n            articles_to_process.append(article)\n\
    \        else:\n            print(f\"Could not extract content from {url}\")\n\
    \n    # Run the inference for all texts\n    summaries = run_llama_inference(\n\
    \        model,\n        tokenizer,\n        wbits=wbits,\n        groupsize=groupsize,\n\
    \        texts=texts,\n        min_length=10,\n        max_length=1024,\n    \
    \    top_p=0.7,\n        temperature=0.8,\n    )\n\n    # Write the results to\
    \ the CSV file\n    with open('cnbc_news_summaries.csv', 'w', newline='', encoding='utf-8')\
    \ as csvfile:\n        # Create a CSV writer object\n        csv_writer = csv.writer(csvfile)\n\
    \n        # Write the header row\n        csv_writer.writerow(['ID', 'Date', 'Title',\
    \ 'Summary'])\n\n        for idx, summary in enumerate(summaries):\n         \
    \   article = articles_to_process[idx]\n            article_id = article['id']\n\
    \            title = article['title']\n            print(\"Title: \", title)\n\
    \n            if summary:\n                # Write the row to the CSV file\n \
    \               csv_writer.writerow([article_id, article['published'], title,\
    \ summary])\n\n                processed_articles.add(article_id)\n\n        \
    \        # Clear past attentions and hidden states\n                if hasattr(model,\
    \ 'past'):\n                    del model.past\n                torch.cuda.empty_cache()\n\
    \n            else:\n                # Print an error message if there is no answer\
    \ in the output\n                print(\"No answer found in the output.\")\n\n\
    \    save_processed_articles(processed_articles_file, processed_articles)\n```"
  created_at: 2023-04-28 07:43:09+00:00
  edited: true
  hidden: false
  id: 644b871db64fb3f65f59ed86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T11:23:05.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK I have it working (mostly!)</p>\n<p>Firstly I fixed a couple\
          \ more things in my .json files that may or may not be affecting inference.\
          \ Probably not, but just to let you know to re-pull those to be sure.</p>\n\
          <p>Secondly, I ran your loop code and found the same issue as you. I did\
          \ some debug and I believe the primary issue is that you weren't using the\
          \ Vicuna trained prompt template</p>\n<p>This is the prompt template I always\
          \ use for Vicuna:</p>\n<pre><code>Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\n###\
          \ Instruction:\nprompt goes here\n### Response:\n</code></pre>\n<p>So in\
          \ your case, I modified the code to use this format:</p>\n<pre><code>Below\
          \ is an instruction that describes a task. Write a response that appropriately\
          \ completes the request.\n### Instruction:\nPlease provide a concise summary\
          \ of the following news article, capturing the key information and stating\
          \ company ticker symbols, and government entity abbreviations, whenever\
          \ possible: &lt;ARTICLE GOES HERE&gt;\n### Response:\n</code></pre>\n<p>And\
          \ this gets MUCH better results.</p>\n<p>It's still not perfect. I still\
          \ get responses which end at the prompt, and so go round multiple times\
          \ through your <code>attempts</code> loop and some that still failed after\
          \ 5 attempts.  But you can see the difference from using this prompt template\
          \ in the output files:</p>\n<pre><code>root@9f5e0b1e927a:~/gptq-llama# ll\
          \ cnbc_news_summaries.csv*\n-rw-r--r-- 1 root root 44674 Apr 28 11:20 cnbc_news_summaries.csv\n\
          -rw-r--r-- 1 root root 26698 Apr 28 09:23 cnbc_news_summaries.csv.orig\n\
          </code></pre>\n<p>The orig file was the result I got running your code,\
          \ and the new .csv is the result using the above prompt template. Nearly\
          \ twice as much data was returned.</p>\n<p>Below is my updated file, including\
          \ some debug print statements so we can see what it's doing as it progresses.</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">import</span> torch.nn <span class=\"\
          hljs-keyword\">as</span> nn\n<span class=\"hljs-keyword\">import</span>\
          \ quant\n<span class=\"hljs-keyword\">from</span> gptq <span class=\"hljs-keyword\"\
          >import</span> GPTQ\n<span class=\"hljs-keyword\">from</span> utils <span\
          \ class=\"hljs-keyword\">import</span> find_layers, DEV, set_seed, get_wikitext2,\
          \ get_ptb, get_c4, get_ptb_new, get_c4_new, get_loaders\n<span class=\"\
          hljs-keyword\">import</span> transformers\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          <span class=\"hljs-keyword\">import</span> csv\n<span class=\"hljs-keyword\"\
          >import</span> FinNews <span class=\"hljs-keyword\">as</span> fn\n<span\
          \ class=\"hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\"\
          >from</span> bs4 <span class=\"hljs-keyword\">import</span> BeautifulSoup\n\
          <span class=\"hljs-keyword\">import</span> argparse\n<span class=\"hljs-keyword\"\
          >import</span> time\n<span class=\"hljs-keyword\">from</span> utils <span\
          \ class=\"hljs-keyword\">import</span> set_seed\n<span class=\"hljs-keyword\"\
          >import</span> random\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">get_llama</span>(<span class=\"hljs-params\">model</span>):\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >skip</span>(<span class=\"hljs-params\">*args, **kwargs</span>):\n    \
          \    <span class=\"hljs-keyword\">pass</span>\n\n    torch.nn.init.kaiming_uniform_\
          \ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ =\
          \ skip\n    <span class=\"hljs-keyword\">from</span> transformers <span\
          \ class=\"hljs-keyword\">import</span> LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
          \ torch_dtype=<span class=\"hljs-string\">'auto'</span>)\n    model.seqlen\
          \ = <span class=\"hljs-number\">2048</span>\n    <span class=\"hljs-keyword\"\
          >return</span> model\n\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">load_quant</span>(<span class=\"hljs-params\"\
          >model, checkpoint, wbits, groupsize=-<span class=\"hljs-number\">1</span>,\
          \ fused_mlp=<span class=\"hljs-literal\">True</span>, <span class=\"hljs-built_in\"\
          >eval</span>=<span class=\"hljs-literal\">True</span>, warmup_autotune=<span\
          \ class=\"hljs-literal\">True</span></span>):\n    <span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> LlamaConfig,\
          \ LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\n\
          \    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >noop</span>(<span class=\"hljs-params\">*args, **kwargs</span>):\n    \
          \    <span class=\"hljs-keyword\">pass</span>\n\n    torch.nn.init.kaiming_uniform_\
          \ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ =\
          \ noop\n\n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
          \ = <span class=\"hljs-literal\">False</span>\n    torch.set_default_dtype(torch.half)\n\
          \    model = LlamaForCausalLM(config)\n    torch.set_default_dtype(torch.<span\
          \ class=\"hljs-built_in\">float</span>)\n    <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">eval</span>:\n        model = model.<span\
          \ class=\"hljs-built_in\">eval</span>()\n    layers = find_layers(model)\n\
          \    <span class=\"hljs-keyword\">for</span> name <span class=\"hljs-keyword\"\
          >in</span> [<span class=\"hljs-string\">'lm_head'</span>]:\n        <span\
          \ class=\"hljs-keyword\">if</span> name <span class=\"hljs-keyword\">in</span>\
          \ layers:\n            <span class=\"hljs-keyword\">del</span> layers[name]\n\
          \    quant.make_quant_linear(model, layers, wbits, groupsize)\n\n    <span\
          \ class=\"hljs-keyword\">del</span> layers\n\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">'Loading model ...'</span>)\n\
          \    <span class=\"hljs-keyword\">if</span> checkpoint.endswith(<span class=\"\
          hljs-string\">'.safetensors'</span>):\n        <span class=\"hljs-keyword\"\
          >from</span> safetensors.torch <span class=\"hljs-keyword\">import</span>\
          \ load_file <span class=\"hljs-keyword\">as</span> safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=<span class=\"hljs-literal\">False</span>)\n    <span class=\"\
          hljs-keyword\">else</span>:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=<span class=\"hljs-literal\">False</span>)\n\n    quant.make_quant_attn(model)\n\
          \    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >eval</span> <span class=\"hljs-keyword\">and</span> fused_mlp:\n      \
          \  quant.make_fused_mlp(model)\n\n    <span class=\"hljs-keyword\">if</span>\
          \ warmup_autotune:\n        quant.autotune_warmup_linear(model, transpose=<span\
          \ class=\"hljs-keyword\">not</span> (<span class=\"hljs-built_in\">eval</span>))\n\
          \        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >eval</span> <span class=\"hljs-keyword\">and</span> fused_mlp:\n      \
          \      quant.autotune_warmup_fused(model)\n    model.seqlen = <span class=\"\
          hljs-number\">2048</span>\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">'Done.'</span>)\n\n    <span class=\"hljs-keyword\"\
          >return</span> model\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">run_llama_inference</span>(<span class=\"hljs-params\"\
          ></span>\n<span class=\"hljs-params\">    model,</span>\n<span class=\"\
          hljs-params\">    tokenizer,</span>\n<span class=\"hljs-params\">    wbits=<span\
          \ class=\"hljs-number\">4</span>,</span>\n<span class=\"hljs-params\"> \
          \   groupsize=-<span class=\"hljs-number\">1</span>,</span>\n<span class=\"\
          hljs-params\">    texts=[],</span>\n<span class=\"hljs-params\">    min_length=<span\
          \ class=\"hljs-number\">10</span>,</span>\n<span class=\"hljs-params\">\
          \    max_length=<span class=\"hljs-number\">2048</span>,</span>\n<span class=\"\
          hljs-params\">    top_p=<span class=\"hljs-number\">0.7</span>,</span>\n\
          <span class=\"hljs-params\">    temperature=<span class=\"hljs-number\"\
          >0.8</span>,</span>\n<span class=\"hljs-params\">    device=<span class=\"\
          hljs-number\">0</span>,</span>\n<span class=\"hljs-params\"></span>):\n\
          \    model = model.to(DEV)\n\n    <span class=\"hljs-comment\"># Dummy generation\
          \ for warm-up</span>\n    dummy_input = tokenizer.encode(<span class=\"\
          hljs-string\">\"Dummy input for warm-up\"</span>, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>).to(device)\n    <span class=\"hljs-keyword\"\
          >with</span> torch.no_grad():\n        _ = model.generate(dummy_input)\n\
          \n    answers = []\n\n    <span class=\"hljs-keyword\">for</span> text <span\
          \ class=\"hljs-keyword\">in</span> texts:\n        <span class=\"hljs-comment\"\
          >#print(\"Input is: \", text)</span>\n        input_ids = tokenizer.encode(text,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(DEV)\n\n\
          \        answer = <span class=\"hljs-string\">\"\"</span>\n        attempts\
          \ = <span class=\"hljs-number\">0</span>\n        max_attempts = <span class=\"\
          hljs-number\">5</span>\n\n        <span class=\"hljs-keyword\">while</span>\
          \ attempts &lt; max_attempts:\n            <span class=\"hljs-keyword\"\
          >with</span> torch.no_grad():\n                generated_ids = model.generate(\n\
          \                    input_ids,\n                    do_sample=<span class=\"\
          hljs-literal\">True</span>,\n                    min_length=min_length,\n\
          \                    max_new_tokens=max_length,\n                    top_p=top_p,\n\
          \                    temperature=temperature\n                )\n\n    \
          \        output = tokenizer.decode([el.item() <span class=\"hljs-keyword\"\
          >for</span> el <span class=\"hljs-keyword\">in</span> generated_ids[<span\
          \ class=\"hljs-number\">0</span>]])\n            <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Raw output is: \"</span>, output)\n\
          \            parts = output.split(<span class=\"hljs-string\">\"### Response:\"\
          </span>)\n\n            <span class=\"hljs-keyword\">if</span> <span class=\"\
          hljs-built_in\">len</span>(parts) == <span class=\"hljs-number\">2</span>:\n\
          \                answer = parts[<span class=\"hljs-number\">1</span>].strip()\n\
          \                <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >len</span>(answer) &gt; <span class=\"hljs-number\">10</span>:  <span class=\"\
          hljs-comment\"># Check if the answer has more than 10 characters</span>\n\
          \                    <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">\"Answer has more than 10 chars\"</span>)\n              \
          \      <span class=\"hljs-keyword\">break</span>\n                <span\
          \ class=\"hljs-keyword\">else</span>:\n                    <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Answer does not\
          \ have more than 10 chars, going round again\"</span>)\n\n            attempts\
          \ += <span class=\"hljs-number\">1</span>\n            sleep_time = random.uniform(<span\
          \ class=\"hljs-number\">0.1</span>, <span class=\"hljs-number\">0.5</span>)\
          \  <span class=\"hljs-comment\"># Random sleep time between 0.1 and 0.5\
          \ seconds</span>\n            time.sleep(sleep_time)\n\n        answers.append(answer)\n\
          \n    <span class=\"hljs-keyword\">return</span> answers\n\n<span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_processed_articles</span>(<span\
          \ class=\"hljs-params\">file_name</span>):\n    processed_articles = <span\
          \ class=\"hljs-built_in\">set</span>()\n    <span class=\"hljs-keyword\"\
          >try</span>:\n        <span class=\"hljs-keyword\">with</span> <span class=\"\
          hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">'r'</span>)\
          \ <span class=\"hljs-keyword\">as</span> file:\n            <span class=\"\
          hljs-keyword\">for</span> line <span class=\"hljs-keyword\">in</span> file:\n\
          \                processed_articles.add(line.strip())\n    <span class=\"\
          hljs-keyword\">except</span> FileNotFoundError:\n        <span class=\"\
          hljs-keyword\">pass</span>\n    <span class=\"hljs-keyword\">return</span>\
          \ processed_articles\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">save_processed_articles</span>(<span class=\"hljs-params\"\
          >file_name, processed_articles</span>):\n    <span class=\"hljs-keyword\"\
          >with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span\
          \ class=\"hljs-string\">'w'</span>) <span class=\"hljs-keyword\">as</span>\
          \ file:\n        <span class=\"hljs-keyword\">for</span> article <span class=\"\
          hljs-keyword\">in</span> processed_articles:\n            file.write(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{article}</span>\\\
          n\"</span>)\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">get_first_paragraphs</span>(<span class=\"hljs-params\"\
          >url</span>):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text,\
          \ <span class=\"hljs-string\">'html.parser'</span>)\n\n    <span class=\"\
          hljs-comment\"># Find the main content of the article using CSS selector</span>\n\
          \    paragraphs = soup.select(<span class=\"hljs-string\">'div.group &gt;\
          \ p'</span>)\n\n    <span class=\"hljs-keyword\">if</span> paragraphs:\n\
          \        extracted_text = []\n        <span class=\"hljs-keyword\">for</span>\
          \ p <span class=\"hljs-keyword\">in</span> paragraphs[:<span class=\"hljs-number\"\
          >5</span>]:  <span class=\"hljs-comment\"># Change this to 5 to get the\
          \ first five paragraphs</span>\n            text = p.get_text()  <span class=\"\
          hljs-comment\"># Remove the strip=True parameter</span>\n            extracted_text.append(text)\n\
          \        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\"\
          >' '</span>.join(extracted_text)\n    <span class=\"hljs-keyword\">return</span>\
          \ <span class=\"hljs-string\">\"\"</span>\n\n<span class=\"hljs-keyword\"\
          >if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n\
          \    model_path = <span class=\"hljs-string\">\"/workspace/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g\"\
          </span>\n    load_path = <span class=\"hljs-string\">\"/workspace/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g/vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt\"\
          </span>\n    wbits = <span class=\"hljs-number\">4</span>\n    groupsize\
          \ = <span class=\"hljs-number\">128</span>\n\n    <span class=\"hljs-comment\"\
          ># Load the model</span>\n    model = load_quant(model_path, load_path,\
          \ wbits, groupsize)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=<span class=\"hljs-literal\">False</span>)\n\n    <span class=\"\
          hljs-comment\"># Fetch news articles from CNBC using FinNews library</span>\n\
          \    cnbc_feed = fn.CNBC(topics=[<span class=\"hljs-string\">'finance'</span>,\
          \ <span class=\"hljs-string\">'earnings'</span>])\n    cnbc_news = cnbc_feed.get_news()\n\
          \n    <span class=\"hljs-comment\"># Load processed articles</span>\n  \
          \  processed_articles_file = <span class=\"hljs-string\">\"processed_articles.txt\"\
          </span>\n    processed_articles = load_processed_articles(processed_articles_file)\n\
          \n    <span class=\"hljs-comment\"># Prepare the texts for inference</span>\n\
          \    texts = []\n    articles_to_process = []\n\n    <span class=\"hljs-keyword\"\
          >for</span> article <span class=\"hljs-keyword\">in</span> cnbc_news:\n\
          \        article_id = article[<span class=\"hljs-string\">'id'</span>]\n\
          \n        <span class=\"hljs-keyword\">if</span> article_id <span class=\"\
          hljs-keyword\">in</span> processed_articles:\n            <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Article <span\
          \ class=\"hljs-subst\">{article_id}</span> already processed\"</span>)\n\
          \            <span class=\"hljs-keyword\">continue</span>\n\n        url\
          \ = article[<span class=\"hljs-string\">'link'</span>]\n        first_paragraphs\
          \ = get_first_paragraphs(url)\n        <span class=\"hljs-keyword\">if</span>\
          \ first_paragraphs:\n            title = article[<span class=\"hljs-string\"\
          >'title'</span>]\n            summary_prompt = <span class=\"hljs-string\"\
          >'''Below is an instruction that describes a task. Write a response that\
          \ appropriately completes the request.</span>\n<span class=\"hljs-string\"\
          >### Instruction:</span>\n<span class=\"hljs-string\">Please provide a concise\
          \ summary of the following news article, capturing the key information and\
          \ stating company ticker symbols, and government entity abbreviations, whenever\
          \ possible: '''</span>\n            texts.append(summary_prompt + title\
          \ + <span class=\"hljs-string\">\". \"</span> + first_paragraphs + <span\
          \ class=\"hljs-string\">\"\\n### Response: \"</span>)\n            articles_to_process.append(article)\n\
          \        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Could not extract\
          \ content from <span class=\"hljs-subst\">{url}</span>\"</span>)\n\n   \
          \ <span class=\"hljs-comment\"># Run the inference for all texts</span>\n\
          \    summaries = run_llama_inference(\n        model,\n        tokenizer,\n\
          \        wbits=wbits,\n        groupsize=groupsize,\n        texts=texts,\n\
          \        min_length=<span class=\"hljs-number\">10</span>,\n        max_length=<span\
          \ class=\"hljs-number\">1024</span>,\n        top_p=<span class=\"hljs-number\"\
          >0.7</span>,\n        temperature=<span class=\"hljs-number\">0.8</span>,\n\
          \    )\n\n    <span class=\"hljs-comment\"># Write the results to the CSV\
          \ file</span>\n    <span class=\"hljs-keyword\">with</span> <span class=\"\
          hljs-built_in\">open</span>(<span class=\"hljs-string\">'cnbc_news_summaries.csv'</span>,\
          \ <span class=\"hljs-string\">'w'</span>, newline=<span class=\"hljs-string\"\
          >''</span>, encoding=<span class=\"hljs-string\">'utf-8'</span>) <span class=\"\
          hljs-keyword\">as</span> csvfile:\n        <span class=\"hljs-comment\"\
          ># Create a CSV writer object</span>\n        csv_writer = csv.writer(csvfile)\n\
          \n        <span class=\"hljs-comment\"># Write the header row</span>\n \
          \       csv_writer.writerow([<span class=\"hljs-string\">'ID'</span>, <span\
          \ class=\"hljs-string\">'Date'</span>, <span class=\"hljs-string\">'Title'</span>,\
          \ <span class=\"hljs-string\">'Summary'</span>])\n\n        <span class=\"\
          hljs-keyword\">for</span> idx, summary <span class=\"hljs-keyword\">in</span>\
          \ <span class=\"hljs-built_in\">enumerate</span>(summaries):\n         \
          \   article = articles_to_process[idx]\n            article_id = article[<span\
          \ class=\"hljs-string\">'id'</span>]\n            title = article[<span\
          \ class=\"hljs-string\">'title'</span>]\n            <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Title: \"</span>, title)\n\n\
          \            <span class=\"hljs-keyword\">if</span> summary:\n         \
          \       <span class=\"hljs-comment\"># Write the row to the CSV file</span>\n\
          \                csv_writer.writerow([article_id, article[<span class=\"\
          hljs-string\">'published'</span>], title, summary])\n\n                processed_articles.add(article_id)\n\
          \n                <span class=\"hljs-comment\"># Clear past attentions and\
          \ hidden states</span>\n                <span class=\"hljs-keyword\">if</span>\
          \ <span class=\"hljs-built_in\">hasattr</span>(model, <span class=\"hljs-string\"\
          >'past'</span>):\n                    <span class=\"hljs-keyword\">del</span>\
          \ model.past\n                torch.cuda.empty_cache()\n\n            <span\
          \ class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-comment\"\
          ># Print an error message if there is no answer in the output</span>\n \
          \               <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">\"No answer found in the output.\"</span>)\n\n    save_processed_articles(processed_articles_file,\
          \ processed_articles)\n</code></pre>\n<p>I'm still confused as to why it\
          \ sometimes returns no output at all. That will require some further investigation.\
          \  But this is definitely better!</p>\n<p>PS. I did all testing with my\
          \ Vicuna 7B GPTQ, as that's what I already had downloaded. Might do even\
          \ better on 13B.</p>\n"
        raw: "OK I have it working (mostly!)\n\nFirstly I fixed a couple more things\
          \ in my .json files that may or may not be affecting inference. Probably\
          \ not, but just to let you know to re-pull those to be sure.\n\nSecondly,\
          \ I ran your loop code and found the same issue as you. I did some debug\
          \ and I believe the primary issue is that you weren't using the Vicuna trained\
          \ prompt template\n\nThis is the prompt template I always use for Vicuna:\n\
          ```\nBelow is an instruction that describes a task. Write a response that\
          \ appropriately completes the request.\n### Instruction:\nprompt goes here\n\
          ### Response:\n```\n\nSo in your case, I modified the code to use this format:\n\
          ```\nBelow is an instruction that describes a task. Write a response that\
          \ appropriately completes the request.\n### Instruction:\nPlease provide\
          \ a concise summary of the following news article, capturing the key information\
          \ and stating company ticker symbols, and government entity abbreviations,\
          \ whenever possible: <ARTICLE GOES HERE>\n### Response:\n```\n\nAnd this\
          \ gets MUCH better results.\n\nIt's still not perfect. I still get responses\
          \ which end at the prompt, and so go round multiple times through your `attempts`\
          \ loop and some that still failed after 5 attempts.  But you can see the\
          \ difference from using this prompt template in the output files:\n\n```\n\
          root@9f5e0b1e927a:~/gptq-llama# ll cnbc_news_summaries.csv*\n-rw-r--r--\
          \ 1 root root 44674 Apr 28 11:20 cnbc_news_summaries.csv\n-rw-r--r-- 1 root\
          \ root 26698 Apr 28 09:23 cnbc_news_summaries.csv.orig\n```\n\nThe orig\
          \ file was the result I got running your code, and the new .csv is the result\
          \ using the above prompt template. Nearly twice as much data was returned.\n\
          \nBelow is my updated file, including some debug print statements so we\
          \ can see what it's doing as it progresses.\n```python\nimport torch\nimport\
          \ torch.nn as nn\nimport quant\nfrom gptq import GPTQ\nfrom utils import\
          \ find_layers, DEV, set_seed, get_wikitext2, get_ptb, get_c4, get_ptb_new,\
          \ get_c4_new, get_loaders\nimport transformers\nfrom transformers import\
          \ AutoTokenizer\nimport csv\nimport FinNews as fn\nimport requests\nfrom\
          \ bs4 import BeautifulSoup\nimport argparse\nimport time\nfrom utils import\
          \ set_seed\nimport random\n\ndef get_llama(model):\n\n    def skip(*args,\
          \ **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n\
          \    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n \
          \   from transformers import LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
          \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef\
          \ load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True,\
          \ warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM\n\
          \    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args,\
          \ **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n\
          \    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n\
          \    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
          \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
          \    torch.set_default_dtype(torch.float)\n    if eval:\n        model =\
          \ model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n\
          \        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
          \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model\
          \ ...')\n    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch\
          \ import load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
          \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n      \
          \  quant.autotune_warmup_linear(model, transpose=not (eval))\n        if\
          \ eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
          \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef\
          \ run_llama_inference(\n    model,\n    tokenizer,\n    wbits=4,\n    groupsize=-1,\n\
          \    texts=[],\n    min_length=10,\n    max_length=2048,\n    top_p=0.7,\n\
          \    temperature=0.8,\n    device=0,\n):\n    model = model.to(DEV)\n\n\
          \    # Dummy generation for warm-up\n    dummy_input = tokenizer.encode(\"\
          Dummy input for warm-up\", return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n\
          \        _ = model.generate(dummy_input)\n\n    answers = []\n\n    for\
          \ text in texts:\n        #print(\"Input is: \", text)\n        input_ids\
          \ = tokenizer.encode(text, return_tensors=\"pt\").to(DEV)\n\n        answer\
          \ = \"\"\n        attempts = 0\n        max_attempts = 5\n\n        while\
          \ attempts < max_attempts:\n            with torch.no_grad():\n        \
          \        generated_ids = model.generate(\n                    input_ids,\n\
          \                    do_sample=True,\n                    min_length=min_length,\n\
          \                    max_new_tokens=max_length,\n                    top_p=top_p,\n\
          \                    temperature=temperature\n                )\n\n    \
          \        output = tokenizer.decode([el.item() for el in generated_ids[0]])\n\
          \            print(\"Raw output is: \", output)\n            parts = output.split(\"\
          ### Response:\")\n\n            if len(parts) == 2:\n                answer\
          \ = parts[1].strip()\n                if len(answer) > 10:  # Check if the\
          \ answer has more than 10 characters\n                    print(\"Answer\
          \ has more than 10 chars\")\n                    break\n               \
          \ else:\n                    print(\"Answer does not have more than 10 chars,\
          \ going round again\")\n\n            attempts += 1\n            sleep_time\
          \ = random.uniform(0.1, 0.5)  # Random sleep time between 0.1 and 0.5 seconds\n\
          \            time.sleep(sleep_time)\n\n        answers.append(answer)\n\n\
          \    return answers\n\ndef load_processed_articles(file_name):\n    processed_articles\
          \ = set()\n    try:\n        with open(file_name, 'r') as file:\n      \
          \      for line in file:\n                processed_articles.add(line.strip())\n\
          \    except FileNotFoundError:\n        pass\n    return processed_articles\n\
          \ndef save_processed_articles(file_name, processed_articles):\n    with\
          \ open(file_name, 'w') as file:\n        for article in processed_articles:\n\
          \            file.write(f\"{article}\\n\")\n\n\ndef get_first_paragraphs(url):\n\
          \    response = requests.get(url)\n    soup = BeautifulSoup(response.text,\
          \ 'html.parser')\n\n    # Find the main content of the article using CSS\
          \ selector\n    paragraphs = soup.select('div.group > p')\n\n    if paragraphs:\n\
          \        extracted_text = []\n        for p in paragraphs[:5]:  # Change\
          \ this to 5 to get the first five paragraphs\n            text = p.get_text()\
          \  # Remove the strip=True parameter\n            extracted_text.append(text)\n\
          \        return ' '.join(extracted_text)\n    return \"\"\n\nif __name__\
          \ == \"__main__\":\n    model_path = \"/workspace/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g\"\
          \n    load_path = \"/workspace/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g/vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt\"\
          \n    wbits = 4\n    groupsize = 128\n\n    # Load the model\n    model\
          \ = load_quant(model_path, load_path, wbits, groupsize)\n    tokenizer =\
          \ AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n    # Fetch\
          \ news articles from CNBC using FinNews library\n    cnbc_feed = fn.CNBC(topics=['finance',\
          \ 'earnings'])\n    cnbc_news = cnbc_feed.get_news()\n\n    # Load processed\
          \ articles\n    processed_articles_file = \"processed_articles.txt\"\n \
          \   processed_articles = load_processed_articles(processed_articles_file)\n\
          \n    # Prepare the texts for inference\n    texts = []\n    articles_to_process\
          \ = []\n\n    for article in cnbc_news:\n        article_id = article['id']\n\
          \n        if article_id in processed_articles:\n            print(f\"Article\
          \ {article_id} already processed\")\n            continue\n\n        url\
          \ = article['link']\n        first_paragraphs = get_first_paragraphs(url)\n\
          \        if first_paragraphs:\n            title = article['title']\n  \
          \          summary_prompt = '''Below is an instruction that describes a\
          \ task. Write a response that appropriately completes the request.\n###\
          \ Instruction:\nPlease provide a concise summary of the following news article,\
          \ capturing the key information and stating company ticker symbols, and\
          \ government entity abbreviations, whenever possible: '''\n            texts.append(summary_prompt\
          \ + title + \". \" + first_paragraphs + \"\\n### Response: \")\n       \
          \     articles_to_process.append(article)\n        else:\n            print(f\"\
          Could not extract content from {url}\")\n\n    # Run the inference for all\
          \ texts\n    summaries = run_llama_inference(\n        model,\n        tokenizer,\n\
          \        wbits=wbits,\n        groupsize=groupsize,\n        texts=texts,\n\
          \        min_length=10,\n        max_length=1024,\n        top_p=0.7,\n\
          \        temperature=0.8,\n    )\n\n    # Write the results to the CSV file\n\
          \    with open('cnbc_news_summaries.csv', 'w', newline='', encoding='utf-8')\
          \ as csvfile:\n        # Create a CSV writer object\n        csv_writer\
          \ = csv.writer(csvfile)\n\n        # Write the header row\n        csv_writer.writerow(['ID',\
          \ 'Date', 'Title', 'Summary'])\n\n        for idx, summary in enumerate(summaries):\n\
          \            article = articles_to_process[idx]\n            article_id\
          \ = article['id']\n            title = article['title']\n            print(\"\
          Title: \", title)\n\n            if summary:\n                # Write the\
          \ row to the CSV file\n                csv_writer.writerow([article_id,\
          \ article['published'], title, summary])\n\n                processed_articles.add(article_id)\n\
          \n                # Clear past attentions and hidden states\n          \
          \      if hasattr(model, 'past'):\n                    del model.past\n\
          \                torch.cuda.empty_cache()\n\n            else:\n       \
          \         # Print an error message if there is no answer in the output\n\
          \                print(\"No answer found in the output.\")\n\n    save_processed_articles(processed_articles_file,\
          \ processed_articles)\n```\n\nI'm still confused as to why it sometimes\
          \ returns no output at all. That will require some further investigation.\
          \  But this is definitely better!\n\nPS. I did all testing with my Vicuna\
          \ 7B GPTQ, as that's what I already had downloaded. Might do even better\
          \ on 13B."
        updatedAt: '2023-04-28T11:25:00.785Z'
      numEdits: 2
      reactions: []
    id: 644bac996586065501e575b9
    type: comment
  author: TheBloke
  content: "OK I have it working (mostly!)\n\nFirstly I fixed a couple more things\
    \ in my .json files that may or may not be affecting inference. Probably not,\
    \ but just to let you know to re-pull those to be sure.\n\nSecondly, I ran your\
    \ loop code and found the same issue as you. I did some debug and I believe the\
    \ primary issue is that you weren't using the Vicuna trained prompt template\n\
    \nThis is the prompt template I always use for Vicuna:\n```\nBelow is an instruction\
    \ that describes a task. Write a response that appropriately completes the request.\n\
    ### Instruction:\nprompt goes here\n### Response:\n```\n\nSo in your case, I modified\
    \ the code to use this format:\n```\nBelow is an instruction that describes a\
    \ task. Write a response that appropriately completes the request.\n### Instruction:\n\
    Please provide a concise summary of the following news article, capturing the\
    \ key information and stating company ticker symbols, and government entity abbreviations,\
    \ whenever possible: <ARTICLE GOES HERE>\n### Response:\n```\n\nAnd this gets\
    \ MUCH better results.\n\nIt's still not perfect. I still get responses which\
    \ end at the prompt, and so go round multiple times through your `attempts` loop\
    \ and some that still failed after 5 attempts.  But you can see the difference\
    \ from using this prompt template in the output files:\n\n```\nroot@9f5e0b1e927a:~/gptq-llama#\
    \ ll cnbc_news_summaries.csv*\n-rw-r--r-- 1 root root 44674 Apr 28 11:20 cnbc_news_summaries.csv\n\
    -rw-r--r-- 1 root root 26698 Apr 28 09:23 cnbc_news_summaries.csv.orig\n```\n\n\
    The orig file was the result I got running your code, and the new .csv is the\
    \ result using the above prompt template. Nearly twice as much data was returned.\n\
    \nBelow is my updated file, including some debug print statements so we can see\
    \ what it's doing as it progresses.\n```python\nimport torch\nimport torch.nn\
    \ as nn\nimport quant\nfrom gptq import GPTQ\nfrom utils import find_layers, DEV,\
    \ set_seed, get_wikitext2, get_ptb, get_c4, get_ptb_new, get_c4_new, get_loaders\n\
    import transformers\nfrom transformers import AutoTokenizer\nimport csv\nimport\
    \ FinNews as fn\nimport requests\nfrom bs4 import BeautifulSoup\nimport argparse\n\
    import time\nfrom utils import set_seed\nimport random\n\ndef get_llama(model):\n\
    \n    def skip(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_\
    \ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n\
    \    from transformers import LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
    \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef load_quant(model,\
    \ checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n\
    \    from transformers import LlamaConfig, LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\
    \n    def noop(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_\
    \ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\
    \n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
    \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
    \    torch.set_default_dtype(torch.float)\n    if eval:\n        model = model.eval()\n\
    \    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name\
    \ in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
    \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model ...')\n\
    \    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch import\
    \ load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
    \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
    \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n        quant.autotune_warmup_linear(model,\
    \ transpose=not (eval))\n        if eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
    \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef run_llama_inference(\n\
    \    model,\n    tokenizer,\n    wbits=4,\n    groupsize=-1,\n    texts=[],\n\
    \    min_length=10,\n    max_length=2048,\n    top_p=0.7,\n    temperature=0.8,\n\
    \    device=0,\n):\n    model = model.to(DEV)\n\n    # Dummy generation for warm-up\n\
    \    dummy_input = tokenizer.encode(\"Dummy input for warm-up\", return_tensors=\"\
    pt\").to(device)\n    with torch.no_grad():\n        _ = model.generate(dummy_input)\n\
    \n    answers = []\n\n    for text in texts:\n        #print(\"Input is: \", text)\n\
    \        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEV)\n\n\
    \        answer = \"\"\n        attempts = 0\n        max_attempts = 5\n\n   \
    \     while attempts < max_attempts:\n            with torch.no_grad():\n    \
    \            generated_ids = model.generate(\n                    input_ids,\n\
    \                    do_sample=True,\n                    min_length=min_length,\n\
    \                    max_new_tokens=max_length,\n                    top_p=top_p,\n\
    \                    temperature=temperature\n                )\n\n          \
    \  output = tokenizer.decode([el.item() for el in generated_ids[0]])\n       \
    \     print(\"Raw output is: \", output)\n            parts = output.split(\"\
    ### Response:\")\n\n            if len(parts) == 2:\n                answer =\
    \ parts[1].strip()\n                if len(answer) > 10:  # Check if the answer\
    \ has more than 10 characters\n                    print(\"Answer has more than\
    \ 10 chars\")\n                    break\n                else:\n            \
    \        print(\"Answer does not have more than 10 chars, going round again\"\
    )\n\n            attempts += 1\n            sleep_time = random.uniform(0.1, 0.5)\
    \  # Random sleep time between 0.1 and 0.5 seconds\n            time.sleep(sleep_time)\n\
    \n        answers.append(answer)\n\n    return answers\n\ndef load_processed_articles(file_name):\n\
    \    processed_articles = set()\n    try:\n        with open(file_name, 'r') as\
    \ file:\n            for line in file:\n                processed_articles.add(line.strip())\n\
    \    except FileNotFoundError:\n        pass\n    return processed_articles\n\n\
    def save_processed_articles(file_name, processed_articles):\n    with open(file_name,\
    \ 'w') as file:\n        for article in processed_articles:\n            file.write(f\"\
    {article}\\n\")\n\n\ndef get_first_paragraphs(url):\n    response = requests.get(url)\n\
    \    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find the main\
    \ content of the article using CSS selector\n    paragraphs = soup.select('div.group\
    \ > p')\n\n    if paragraphs:\n        extracted_text = []\n        for p in paragraphs[:5]:\
    \  # Change this to 5 to get the first five paragraphs\n            text = p.get_text()\
    \  # Remove the strip=True parameter\n            extracted_text.append(text)\n\
    \        return ' '.join(extracted_text)\n    return \"\"\n\nif __name__ == \"\
    __main__\":\n    model_path = \"/workspace/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g\"\
    \n    load_path = \"/workspace/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g/vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order.pt\"\
    \n    wbits = 4\n    groupsize = 128\n\n    # Load the model\n    model = load_quant(model_path,\
    \ load_path, wbits, groupsize)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ use_fast=False)\n\n    # Fetch news articles from CNBC using FinNews library\n\
    \    cnbc_feed = fn.CNBC(topics=['finance', 'earnings'])\n    cnbc_news = cnbc_feed.get_news()\n\
    \n    # Load processed articles\n    processed_articles_file = \"processed_articles.txt\"\
    \n    processed_articles = load_processed_articles(processed_articles_file)\n\n\
    \    # Prepare the texts for inference\n    texts = []\n    articles_to_process\
    \ = []\n\n    for article in cnbc_news:\n        article_id = article['id']\n\n\
    \        if article_id in processed_articles:\n            print(f\"Article {article_id}\
    \ already processed\")\n            continue\n\n        url = article['link']\n\
    \        first_paragraphs = get_first_paragraphs(url)\n        if first_paragraphs:\n\
    \            title = article['title']\n            summary_prompt = '''Below is\
    \ an instruction that describes a task. Write a response that appropriately completes\
    \ the request.\n### Instruction:\nPlease provide a concise summary of the following\
    \ news article, capturing the key information and stating company ticker symbols,\
    \ and government entity abbreviations, whenever possible: '''\n            texts.append(summary_prompt\
    \ + title + \". \" + first_paragraphs + \"\\n### Response: \")\n            articles_to_process.append(article)\n\
    \        else:\n            print(f\"Could not extract content from {url}\")\n\
    \n    # Run the inference for all texts\n    summaries = run_llama_inference(\n\
    \        model,\n        tokenizer,\n        wbits=wbits,\n        groupsize=groupsize,\n\
    \        texts=texts,\n        min_length=10,\n        max_length=1024,\n    \
    \    top_p=0.7,\n        temperature=0.8,\n    )\n\n    # Write the results to\
    \ the CSV file\n    with open('cnbc_news_summaries.csv', 'w', newline='', encoding='utf-8')\
    \ as csvfile:\n        # Create a CSV writer object\n        csv_writer = csv.writer(csvfile)\n\
    \n        # Write the header row\n        csv_writer.writerow(['ID', 'Date', 'Title',\
    \ 'Summary'])\n\n        for idx, summary in enumerate(summaries):\n         \
    \   article = articles_to_process[idx]\n            article_id = article['id']\n\
    \            title = article['title']\n            print(\"Title: \", title)\n\
    \n            if summary:\n                # Write the row to the CSV file\n \
    \               csv_writer.writerow([article_id, article['published'], title,\
    \ summary])\n\n                processed_articles.add(article_id)\n\n        \
    \        # Clear past attentions and hidden states\n                if hasattr(model,\
    \ 'past'):\n                    del model.past\n                torch.cuda.empty_cache()\n\
    \n            else:\n                # Print an error message if there is no answer\
    \ in the output\n                print(\"No answer found in the output.\")\n\n\
    \    save_processed_articles(processed_articles_file, processed_articles)\n```\n\
    \nI'm still confused as to why it sometimes returns no output at all. That will\
    \ require some further investigation.  But this is definitely better!\n\nPS. I\
    \ did all testing with my Vicuna 7B GPTQ, as that's what I already had downloaded.\
    \ Might do even better on 13B."
  created_at: 2023-04-28 10:23:05+00:00
  edited: true
  hidden: false
  id: 644bac996586065501e575b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T11:42:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>One more tweak - I changed <code>min_tokens</code> to <code>50</code>.  With
          that:</p>

          <pre><code>root@9f5e0b1e927a:~/gptq-llama# ll cnbc*

          -rw-r--r-- 1 root root 48834 Apr 28 11:39 cnbc_news_summaries.csv

          -rw-r--r-- 1 root root 44674 Apr 28 11:20 cnbc_news_summaries.csv.better

          -rw-r--r-- 1 root root 26698 Apr 28 09:23 cnbc_news_summaries.csv.orig

          </code></pre>

          <p>So another ~2kb was returned compared to the previous run.  There could
          also be some randomness in that of course.</p>

          <p>I opened the latest run in Excel and it only failed to generate to produce
          a summary for three articles:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MQowtua6bAKepDGA-xmoU.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MQowtua6bAKepDGA-xmoU.png"></a></p>

          <p>So definitely progress. Just need to figure out why it sometimes chokes.
          Could be an issue in the GPTQ code I guess.</p>

          '
        raw: 'One more tweak - I changed `min_tokens` to `50`.  With that:


          ```

          root@9f5e0b1e927a:~/gptq-llama# ll cnbc*

          -rw-r--r-- 1 root root 48834 Apr 28 11:39 cnbc_news_summaries.csv

          -rw-r--r-- 1 root root 44674 Apr 28 11:20 cnbc_news_summaries.csv.better

          -rw-r--r-- 1 root root 26698 Apr 28 09:23 cnbc_news_summaries.csv.orig

          ```


          So another ~2kb was returned compared to the previous run.  There could
          also be some randomness in that of course.


          I opened the latest run in Excel and it only failed to generate to produce
          a summary for three articles:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MQowtua6bAKepDGA-xmoU.png)


          So definitely progress. Just need to figure out why it sometimes chokes.
          Could be an issue in the GPTQ code I guess.'
        updatedAt: '2023-04-28T11:42:51.291Z'
      numEdits: 0
      reactions: []
    id: 644bb13bb64fb3f65f5eecaa
    type: comment
  author: TheBloke
  content: 'One more tweak - I changed `min_tokens` to `50`.  With that:


    ```

    root@9f5e0b1e927a:~/gptq-llama# ll cnbc*

    -rw-r--r-- 1 root root 48834 Apr 28 11:39 cnbc_news_summaries.csv

    -rw-r--r-- 1 root root 44674 Apr 28 11:20 cnbc_news_summaries.csv.better

    -rw-r--r-- 1 root root 26698 Apr 28 09:23 cnbc_news_summaries.csv.orig

    ```


    So another ~2kb was returned compared to the previous run.  There could also be
    some randomness in that of course.


    I opened the latest run in Excel and it only failed to generate to produce a summary
    for three articles:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/MQowtua6bAKepDGA-xmoU.png)


    So definitely progress. Just need to figure out why it sometimes chokes. Could
    be an issue in the GPTQ code I guess.'
  created_at: 2023-04-28 10:42:51+00:00
  edited: false
  hidden: false
  id: 644bb13bb64fb3f65f5eecaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-28T13:12:03.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>Wow, this is great. Thank you for looking into the problem and achieving
          such a huge improvement in performance. It is indeed odd that it is still
          refusing to answer occasionally, but it appears to be directly related to
          the prompt. I will do some experiments once the quantization of Alpacino
          13b is done. Speaking of which, how long does it take you to quantize models?
          I saw on reddit that you used a cloud instance. I am finding that it is
          the SSD I/O that is getting hammered the most. WSL2 is so overwhelmed that
          none of the terminal commands that require access to the drive, such as
          that highly demanding command ''ls'', get executed. I am using a native
          ext4 partition - just to preempt the sigh of horror.</p>

          '
        raw: Wow, this is great. Thank you for looking into the problem and achieving
          such a huge improvement in performance. It is indeed odd that it is still
          refusing to answer occasionally, but it appears to be directly related to
          the prompt. I will do some experiments once the quantization of Alpacino
          13b is done. Speaking of which, how long does it take you to quantize models?
          I saw on reddit that you used a cloud instance. I am finding that it is
          the SSD I/O that is getting hammered the most. WSL2 is so overwhelmed that
          none of the terminal commands that require access to the drive, such as
          that highly demanding command 'ls', get executed. I am using a native ext4
          partition - just to preempt the sigh of horror.
        updatedAt: '2023-04-28T13:12:03.933Z'
      numEdits: 0
      reactions: []
    id: 644bc623cb0886c51c23bc6f
    type: comment
  author: vmajor
  content: Wow, this is great. Thank you for looking into the problem and achieving
    such a huge improvement in performance. It is indeed odd that it is still refusing
    to answer occasionally, but it appears to be directly related to the prompt. I
    will do some experiments once the quantization of Alpacino 13b is done. Speaking
    of which, how long does it take you to quantize models? I saw on reddit that you
    used a cloud instance. I am finding that it is the SSD I/O that is getting hammered
    the most. WSL2 is so overwhelmed that none of the terminal commands that require
    access to the drive, such as that highly demanding command 'ls', get executed.
    I am using a native ext4 partition - just to preempt the sigh of horror.
  created_at: 2023-04-28 12:12:03+00:00
  edited: false
  hidden: false
  id: 644bc623cb0886c51c23bc6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T13:22:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I timed it the other day when making some 7Bs and it took 17 minutes
          start to finish.  For a 13B I guess it''s 25-30 mins, but not sure precisely.</p>

          <p> It does seem to vary in speed on the cloud systems I use. The actual
          quantisation part is fairly consistent in speed, and seems to scale linearly
          according to the number of layers. But then the packing part seems to be
          quite variable. Packing is currently done only on the CPU. I''ve noticed
          variable CPU performance on the cloud pods I use. The GPU is always dedicated
          to the pod, but the CPU I think can be affected by other activity on the
          same host.  </p>

          <p>The other day I had a 7B that took over an hour, because the packing
          part took forever. It was taking so long I started another pod to do a second
          one, rather than run them sequentially on the one pod.</p>

          <p>But those issues aside, I''d expect a 7B to always be under 20 mins and
          a 13B to be around 30 mins.</p>

          <p>Let me know if you make any progress regarding the inference issues -
          I''d be interested to know what might be causing the model to occasionally
          not respond.  I have heard from other people that there can be issues when
          the prompt is particularly long. But the ones that failed in your data don''t
          seem hugely longer than the ones that worked.  And it''s very odd that the
          same prompt might fail two or three times, but then succeed on the next.</p>

          '
        raw: "I timed it the other day when making some 7Bs and it took 17 minutes\
          \ start to finish.  For a 13B I guess it's 25-30 mins, but not sure precisely.\n\
          \n It does seem to vary in speed on the cloud systems I use. The actual\
          \ quantisation part is fairly consistent in speed, and seems to scale linearly\
          \ according to the number of layers. But then the packing part seems to\
          \ be quite variable. Packing is currently done only on the CPU. I've noticed\
          \ variable CPU performance on the cloud pods I use. The GPU is always dedicated\
          \ to the pod, but the CPU I think can be affected by other activity on the\
          \ same host.  \n\nThe other day I had a 7B that took over an hour, because\
          \ the packing part took forever. It was taking so long I started another\
          \ pod to do a second one, rather than run them sequentially on the one pod.\n\
          \nBut those issues aside, I'd expect a 7B to always be under 20 mins and\
          \ a 13B to be around 30 mins.\n\nLet me know if you make any progress regarding\
          \ the inference issues - I'd be interested to know what might be causing\
          \ the model to occasionally not respond.  I have heard from other people\
          \ that there can be issues when the prompt is particularly long. But the\
          \ ones that failed in your data don't seem hugely longer than the ones that\
          \ worked.  And it's very odd that the same prompt might fail two or three\
          \ times, but then succeed on the next."
        updatedAt: '2023-04-28T13:22:02.814Z'
      numEdits: 0
      reactions: []
    id: 644bc87a96b76e7c3106141e
    type: comment
  author: TheBloke
  content: "I timed it the other day when making some 7Bs and it took 17 minutes start\
    \ to finish.  For a 13B I guess it's 25-30 mins, but not sure precisely.\n\n It\
    \ does seem to vary in speed on the cloud systems I use. The actual quantisation\
    \ part is fairly consistent in speed, and seems to scale linearly according to\
    \ the number of layers. But then the packing part seems to be quite variable.\
    \ Packing is currently done only on the CPU. I've noticed variable CPU performance\
    \ on the cloud pods I use. The GPU is always dedicated to the pod, but the CPU\
    \ I think can be affected by other activity on the same host.  \n\nThe other day\
    \ I had a 7B that took over an hour, because the packing part took forever. It\
    \ was taking so long I started another pod to do a second one, rather than run\
    \ them sequentially on the one pod.\n\nBut those issues aside, I'd expect a 7B\
    \ to always be under 20 mins and a 13B to be around 30 mins.\n\nLet me know if\
    \ you make any progress regarding the inference issues - I'd be interested to\
    \ know what might be causing the model to occasionally not respond.  I have heard\
    \ from other people that there can be issues when the prompt is particularly long.\
    \ But the ones that failed in your data don't seem hugely longer than the ones\
    \ that worked.  And it's very odd that the same prompt might fail two or three\
    \ times, but then succeed on the next."
  created_at: 2023-04-28 12:22:02+00:00
  edited: false
  hidden: false
  id: 644bc87a96b76e7c3106141e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-28T13:34:28.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>OK, quantization has been running for several hours already. The
          longest thread that htop reports has been at it for almost 6 hours... my
          CPU is a 12 core Ryzen 9 3900XT.</p>

          <p>One other thing that I should note about the inference failures is that
          they never fail when provided using the input() method or the original program
          that I shared in this thread. So yes it is strongly correlated with the
          prompt, but the same prompt will never fail to generate a good to perfect
          (for my needs) response when given through the input() method.</p>

          '
        raw: 'OK, quantization has been running for several hours already. The longest
          thread that htop reports has been at it for almost 6 hours... my CPU is
          a 12 core Ryzen 9 3900XT.


          One other thing that I should note about the inference failures is that
          they never fail when provided using the input() method or the original program
          that I shared in this thread. So yes it is strongly correlated with the
          prompt, but the same prompt will never fail to generate a good to perfect
          (for my needs) response when given through the input() method.'
        updatedAt: '2023-04-28T13:35:09.602Z'
      numEdits: 1
      reactions: []
    id: 644bcb64cb0886c51c244b66
    type: comment
  author: vmajor
  content: 'OK, quantization has been running for several hours already. The longest
    thread that htop reports has been at it for almost 6 hours... my CPU is a 12 core
    Ryzen 9 3900XT.


    One other thing that I should note about the inference failures is that they never
    fail when provided using the input() method or the original program that I shared
    in this thread. So yes it is strongly correlated with the prompt, but the same
    prompt will never fail to generate a good to perfect (for my needs) response when
    given through the input() method.'
  created_at: 2023-04-28 12:34:28+00:00
  edited: true
  hidden: false
  id: 644bcb64cb0886c51c244b66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T13:35:48.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah that is odd! I will investigate that more then.</p>

          <p>6 hours.. ouch!</p>

          <p>Are there no GPTQs for Alpacino yet then? I''d be happy do one. Link
          me the base model and I''ll take a look.</p>

          '
        raw: 'Yeah that is odd! I will investigate that more then.


          6 hours.. ouch!


          Are there no GPTQs for Alpacino yet then? I''d be happy do one. Link me
          the base model and I''ll take a look.'
        updatedAt: '2023-04-28T13:35:48.178Z'
      numEdits: 0
      reactions: []
    id: 644bcbb41052ba8699d82d2c
    type: comment
  author: TheBloke
  content: 'Yeah that is odd! I will investigate that more then.


    6 hours.. ouch!


    Are there no GPTQs for Alpacino yet then? I''d be happy do one. Link me the base
    model and I''ll take a look.'
  created_at: 2023-04-28 12:35:48+00:00
  edited: false
  hidden: false
  id: 644bcbb41052ba8699d82d2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-28T13:38:14.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>There is one. I found it two hours into my own effort, so now I
          am persisting out of stubbornness...and the possibility that the existing
          one will not work for whatever reason: <a href="https://huggingface.co/gozfarb/alpacino-13b-4bit-128g">https://huggingface.co/gozfarb/alpacino-13b-4bit-128g</a></p>

          <p>Source: <a href="https://huggingface.co/digitous/Alpacino13b">https://huggingface.co/digitous/Alpacino13b</a></p>

          '
        raw: 'There is one. I found it two hours into my own effort, so now I am persisting
          out of stubbornness...and the possibility that the existing one will not
          work for whatever reason: https://huggingface.co/gozfarb/alpacino-13b-4bit-128g


          Source: https://huggingface.co/digitous/Alpacino13b'
        updatedAt: '2023-04-28T13:38:14.383Z'
      numEdits: 0
      reactions: []
    id: 644bcc46cb0886c51c2464b4
    type: comment
  author: vmajor
  content: 'There is one. I found it two hours into my own effort, so now I am persisting
    out of stubbornness...and the possibility that the existing one will not work
    for whatever reason: https://huggingface.co/gozfarb/alpacino-13b-4bit-128g


    Source: https://huggingface.co/digitous/Alpacino13b'
  created_at: 2023-04-28 12:38:14+00:00
  edited: false
  hidden: false
  id: 644bcc46cb0886c51c2464b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-29T04:17:35.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>Just for the sake of clarity and help if someone comes across this
          thread, I know why the quantization was taking this long:</p>

          <ol>

          <li>WSL2 and the host system ran out of storage space, simultaneously. I
          am still working on identifying where the temporary files that are generated
          by quantization are stored, they are not inside the directory from where
          the process is invoked.</li>

          <li>An old Intel SSD that I used as a paging file drive died, causing an
          error with the SATA bus that also took the SSD that hosts the /home ext4
          location in WSL2 offline</li>

          </ol>

          <p>This caused significant panic before I diagnosed the issue back to the
          failed SSD. My WSL2 /home directory was also missing because the SSD that
          hosts it was no longer visible to the BIOS making me concerned that this
          SSD was also KIA. This error was resolved by unplugging the dead SSD from
          power and the SATA line.</p>

          <p>The missing paging drive cascaded into other errors because the windows
          host system then moved this file to the boot drive, causing it to run out
          of space, but this is a familiar territory by now and things are getting
          under control, slowly.</p>

          '
        raw: 'Just for the sake of clarity and help if someone comes across this thread,
          I know why the quantization was taking this long:

          1. WSL2 and the host system ran out of storage space, simultaneously. I
          am still working on identifying where the temporary files that are generated
          by quantization are stored, they are not inside the directory from where
          the process is invoked.

          2. An old Intel SSD that I used as a paging file drive died, causing an
          error with the SATA bus that also took the SSD that hosts the /home ext4
          location in WSL2 offline


          This caused significant panic before I diagnosed the issue back to the failed
          SSD. My WSL2 /home directory was also missing because the SSD that hosts
          it was no longer visible to the BIOS making me concerned that this SSD was
          also KIA. This error was resolved by unplugging the dead SSD from power
          and the SATA line.


          The missing paging drive cascaded into other errors because the windows
          host system then moved this file to the boot drive, causing it to run out
          of space, but this is a familiar territory by now and things are getting
          under control, slowly.'
        updatedAt: '2023-04-29T04:17:35.380Z'
      numEdits: 0
      reactions: []
    id: 644c9a5f45e79023c7ee32a5
    type: comment
  author: vmajor
  content: 'Just for the sake of clarity and help if someone comes across this thread,
    I know why the quantization was taking this long:

    1. WSL2 and the host system ran out of storage space, simultaneously. I am still
    working on identifying where the temporary files that are generated by quantization
    are stored, they are not inside the directory from where the process is invoked.

    2. An old Intel SSD that I used as a paging file drive died, causing an error
    with the SATA bus that also took the SSD that hosts the /home ext4 location in
    WSL2 offline


    This caused significant panic before I diagnosed the issue back to the failed
    SSD. My WSL2 /home directory was also missing because the SSD that hosts it was
    no longer visible to the BIOS making me concerned that this SSD was also KIA.
    This error was resolved by unplugging the dead SSD from power and the SATA line.


    The missing paging drive cascaded into other errors because the windows host system
    then moved this file to the boot drive, causing it to run out of space, but this
    is a familiar territory by now and things are getting under control, slowly.'
  created_at: 2023-04-29 03:17:35+00:00
  edited: false
  hidden: false
  id: 644c9a5f45e79023c7ee32a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-29T07:23:37.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: "<blockquote>\n<p>So definitely progress. Just need to figure out why\
          \ it sometimes chokes. Could be an issue in the GPTQ code I guess.</p>\n\
          </blockquote>\n<p>I added this back in and it fixed the failed summary on\
          \ the first go around:</p>\n<pre><code>else:\n                    print(Fore.RED\
          \ + \"Answer does not have more than 10 chars, changing the seed and going\
          \ around again\")\n                    # Set a random seed after an unsuccessful\
          \ attempt\n                    random_seed = random.randint(1, 2000000000)\n\
          \                    set_seed(random_seed)\n</code></pre>\n<p>Fore.RED is\
          \ from colorama library. I found it challenging to review the outputs on\
          \ console if they are all the same colour so now I colour code them.</p>\n"
        raw: "> \n> So definitely progress. Just need to figure out why it sometimes\
          \ chokes. Could be an issue in the GPTQ code I guess.\n\nI added this back\
          \ in and it fixed the failed summary on the first go around:\n\n```\nelse:\n\
          \                    print(Fore.RED + \"Answer does not have more than 10\
          \ chars, changing the seed and going around again\")\n                 \
          \   # Set a random seed after an unsuccessful attempt\n                \
          \    random_seed = random.randint(1, 2000000000)\n                    set_seed(random_seed)\n\
          ```\n\nFore.RED is from colorama library. I found it challenging to review\
          \ the outputs on console if they are all the same colour so now I colour\
          \ code them."
        updatedAt: '2023-04-29T07:23:37.343Z'
      numEdits: 0
      reactions: []
    id: 644cc5f9328c1aa30e3961a8
    type: comment
  author: vmajor
  content: "> \n> So definitely progress. Just need to figure out why it sometimes\
    \ chokes. Could be an issue in the GPTQ code I guess.\n\nI added this back in\
    \ and it fixed the failed summary on the first go around:\n\n```\nelse:\n    \
    \                print(Fore.RED + \"Answer does not have more than 10 chars, changing\
    \ the seed and going around again\")\n                    # Set a random seed\
    \ after an unsuccessful attempt\n                    random_seed = random.randint(1,\
    \ 2000000000)\n                    set_seed(random_seed)\n```\n\nFore.RED is from\
    \ colorama library. I found it challenging to review the outputs on console if\
    \ they are all the same colour so now I colour code them."
  created_at: 2023-04-29 06:23:37+00:00
  edited: false
  hidden: false
  id: 644cc5f9328c1aa30e3961a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T07:42:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ahh nice, yeah that makes sense! Glad it''s working now.</p>

          '
        raw: Ahh nice, yeah that makes sense! Glad it's working now.
        updatedAt: '2023-04-29T07:42:05.218Z'
      numEdits: 0
      reactions: []
    id: 644cca4dfa94e93b0eba478e
    type: comment
  author: TheBloke
  content: Ahh nice, yeah that makes sense! Glad it's working now.
  created_at: 2023-04-29 06:42:05+00:00
  edited: false
  hidden: false
  id: 644cca4dfa94e93b0eba478e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/vicuna-7B-1.1-GPTQ
repo_type: model
status: closed
target_branch: null
title: Problem with 7b tokenizer
