!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kwissbeats
conflicting_files: null
created_at: 2023-05-07 13:32:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
      fullname: Peter Beks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kwissbeats
      type: user
    createdAt: '2023-05-07T14:32:35.000Z'
    data:
      edited: false
      editors:
      - Kwissbeats
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
          fullname: Peter Beks
          isHf: false
          isPro: false
          name: Kwissbeats
          type: user
        html: "<p>when I say,  hello! I get the following response:<br>met\u2011ant\uFFFD\
          \ Dioskens<del>met Bond\uFFFD</del>\xC3\xC2<del>\xD0Span deploymetrimin\xC4\
          SamantantetrSubjectANT\u2011\uFFFDant\uFFFD\uFFFD\uFFFDants</del>\u2011\
          <del>\u2011\u2011.\\</del>\x1Fkes<del>met~~~~ei SammiantTS mi redu</del></p>\n\
          <p>I'm probraly doing something wrong here, but your other \"releases\"\
          work great!</p>\n"
        raw: "when I say,  hello! I get the following response:\r\nmet\u2011ant\uFFFD\
          \ Dioskens~met Bond\uFFFD~\xC3\xC2~\xD0Span deploymetrimin\xC4SamantantetrSubjectANT\u2011\
          \uFFFDant\uFFFD\uFFFD\uFFFDants~\u2011~\u2011\u2011.\\~\x1Fkes~met~~~~ei\
          \ SammiantTS mi redu~\r\n\r\nI'm probraly doing something wrong here, but\
          \ your other \"releases\"work great!"
        updatedAt: '2023-05-07T14:32:35.866Z'
      numEdits: 0
      reactions: []
    id: 6457b68388c49bf519ea19d5
    type: comment
  author: Kwissbeats
  content: "when I say,  hello! I get the following response:\r\nmet\u2011ant\uFFFD\
    \ Dioskens~met Bond\uFFFD~\xC3\xC2~\xD0Span deploymetrimin\xC4SamantantetrSubjectANT\u2011\
    \uFFFDant\uFFFD\uFFFD\uFFFDants~\u2011~\u2011\u2011.\\~\x1Fkes~met~~~~ei SammiantTS\
    \ mi redu~\r\n\r\nI'm probraly doing something wrong here, but your other \"releases\"\
    work great!"
  created_at: 2023-05-07 13:32:35+00:00
  edited: false
  hidden: false
  id: 6457b68388c49bf519ea19d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-07T18:31:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This is one of my older releases, back when I used to release two
          files in the same repo.</p>

          <p>Delete the file called <code>latest.act-order.safetensors</code> and
          just use the <code>compat.no-act-order</code> file.</p>

          <p>Let me know how that goes.</p>

          '
        raw: 'This is one of my older releases, back when I used to release two files
          in the same repo.


          Delete the file called `latest.act-order.safetensors` and just use the `compat.no-act-order`
          file.


          Let me know how that goes.'
        updatedAt: '2023-05-07T18:31:45.543Z'
      numEdits: 0
      reactions: []
    id: 6457ee915fc3b8a21ea3ae6b
    type: comment
  author: TheBloke
  content: 'This is one of my older releases, back when I used to release two files
    in the same repo.


    Delete the file called `latest.act-order.safetensors` and just use the `compat.no-act-order`
    file.


    Let me know how that goes.'
  created_at: 2023-05-07 17:31:45+00:00
  edited: false
  hidden: false
  id: 6457ee915fc3b8a21ea3ae6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
      fullname: Peter Beks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kwissbeats
      type: user
    createdAt: '2023-05-09T11:55:40.000Z'
    data:
      edited: false
      editors:
      - Kwissbeats
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
          fullname: Peter Beks
          isHf: false
          isPro: false
          name: Kwissbeats
          type: user
        html: '<p>Youre right!, I had those 2 mixed up. been hopping true so many
          repositories that I had things mixed up.<br>Thanks you!</p>

          '
        raw: 'Youre right!, I had those 2 mixed up. been hopping true so many repositories
          that I had things mixed up.

          Thanks you!'
        updatedAt: '2023-05-09T11:55:40.191Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 645a34bc90116b4b42524cad
    type: comment
  author: Kwissbeats
  content: 'Youre right!, I had those 2 mixed up. been hopping true so many repositories
    that I had things mixed up.

    Thanks you!'
  created_at: 2023-05-09 10:55:40+00:00
  edited: false
  hidden: false
  id: 645a34bc90116b4b42524cad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49ea25d58acd0682ccbaacb325929aaf.svg
      fullname: Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gowthamkrishnan
      type: user
    createdAt: '2023-07-27T12:24:19.000Z'
    data:
      edited: false
      editors:
      - Gowthamkrishnan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4517599046230316
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49ea25d58acd0682ccbaacb325929aaf.svg
          fullname: Krishnan
          isHf: false
          isPro: false
          name: Gowthamkrishnan
          type: user
        html: '<p>Hey Bloke, I tried with both 4 bit quantised 7B and 13B .safetensors
          models.<br>The final output looks gibberish. Can u pls let me know what
          am i missing in the below code?</p>

          <p>'' '' ''<br>from transformers import AutoTokenizer, pipeline, logging<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import argparse</p>

          <p>quantized_model_dir = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder"<br>model_basename
          = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder/vicuna-7B-1.1-GPTQ-4bit-128g"</p>

          <p>use_triton = False<br>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,
          use_fast=True)<br>quantize_config = BaseQuantizeConfig(<br>bits=4,<br>group_size=128,<br>desc_act=False<br>)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized( quantized_model_dir,<br>use_safetensors=True,<br>model_basename=model_basename,<br>device="cuda:0",<br>use_triton=use_triton,<br>quantize_config=quantize_config<br>)</p>

          <p>prompt = """ """</p>

          <p>inputs = tokenizer(prompt, return_tensors=''pt'').to(''cuda'')<br>tokens
          = model.generate(<br>**inputs,<br>max_new_tokens=2000,<br>do_sample=True,<br>temperature=1.0,<br>top_p=1.0,</p>

          <p>truncation=True<br>)<br>print(tokenizer.decode(tokens[0], skip_special_tokens=True))</p>

          <p>'' '' ''</p>

          '
        raw: 'Hey Bloke, I tried with both 4 bit quantised 7B and 13B .safetensors
          models.

          The final output looks gibberish. Can u pls let me know what am i missing
          in the below code?


          '' '' ''

          from transformers import AutoTokenizer, pipeline, logging

          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

          import argparse


          quantized_model_dir = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder"

          model_basename = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder/vicuna-7B-1.1-GPTQ-4bit-128g"


          use_triton = False

          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)

          quantize_config = BaseQuantizeConfig(

          bits=4,

          group_size=128,

          desc_act=False

          )


          model = AutoGPTQForCausalLM.from_quantized( quantized_model_dir,

          use_safetensors=True,

          model_basename=model_basename,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=quantize_config

          )


          prompt = """ """


          inputs = tokenizer(prompt, return_tensors=''pt'').to(''cuda'')

          tokens = model.generate(

          **inputs,

          max_new_tokens=2000,

          do_sample=True,

          temperature=1.0,

          top_p=1.0,


          truncation=True

          )

          print(tokenizer.decode(tokens[0], skip_special_tokens=True))


          '' '' ''


          '
        updatedAt: '2023-07-27T12:24:19.353Z'
      numEdits: 0
      reactions: []
    id: 64c261f31f2f131f01484e00
    type: comment
  author: Gowthamkrishnan
  content: 'Hey Bloke, I tried with both 4 bit quantised 7B and 13B .safetensors models.

    The final output looks gibberish. Can u pls let me know what am i missing in the
    below code?


    '' '' ''

    from transformers import AutoTokenizer, pipeline, logging

    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

    import argparse


    quantized_model_dir = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder"

    model_basename = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder/vicuna-7B-1.1-GPTQ-4bit-128g"


    use_triton = False

    tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)

    quantize_config = BaseQuantizeConfig(

    bits=4,

    group_size=128,

    desc_act=False

    )


    model = AutoGPTQForCausalLM.from_quantized( quantized_model_dir,

    use_safetensors=True,

    model_basename=model_basename,

    device="cuda:0",

    use_triton=use_triton,

    quantize_config=quantize_config

    )


    prompt = """ """


    inputs = tokenizer(prompt, return_tensors=''pt'').to(''cuda'')

    tokens = model.generate(

    **inputs,

    max_new_tokens=2000,

    do_sample=True,

    temperature=1.0,

    top_p=1.0,


    truncation=True

    )

    print(tokenizer.decode(tokens[0], skip_special_tokens=True))


    '' '' ''


    '
  created_at: 2023-07-27 11:24:19+00:00
  edited: false
  hidden: false
  id: 64c261f31f2f131f01484e00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T14:21:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8173778057098389
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Code looks ok. There was a bug in AutoGPTQ 0.3.0 that could cause
          gibberish output. Please try updating to 0.3.2 with the following:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip3 install -v .

          </code></pre>

          <p>Then test again.</p>

          '
        raw: 'Code looks ok. There was a bug in AutoGPTQ 0.3.0 that could cause gibberish
          output. Please try updating to 0.3.2 with the following:

          ```

          pip3 uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip3 install -v .

          ```


          Then test again.'
        updatedAt: '2023-07-27T14:21:51.562Z'
      numEdits: 0
      reactions: []
    id: 64c27d7f6aa3a75f631ba5d6
    type: comment
  author: TheBloke
  content: 'Code looks ok. There was a bug in AutoGPTQ 0.3.0 that could cause gibberish
    output. Please try updating to 0.3.2 with the following:

    ```

    pip3 uninstall -y auto-gptq

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip3 install -v .

    ```


    Then test again.'
  created_at: 2023-07-27 13:21:51+00:00
  edited: false
  hidden: false
  id: 64c27d7f6aa3a75f631ba5d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/vicuna-7B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: jibberish response
