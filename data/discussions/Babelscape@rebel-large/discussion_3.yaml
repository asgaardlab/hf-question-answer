!!python/object:huggingface_hub.community.DiscussionWithDetails
author: f1amigo
conflicting_files: null
created_at: 2023-01-18 08:10:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49d4685c39ab9fcb8b7b793996629285.svg
      fullname: Chen Weiyi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: f1amigo
      type: user
    createdAt: '2023-01-18T08:10:52.000Z'
    data:
      edited: false
      editors:
      - f1amigo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49d4685c39ab9fcb8b7b793996629285.svg
          fullname: Chen Weiyi
          isHf: false
          isPro: false
          name: f1amigo
          type: user
        html: '<p>I''m quite new to this, so bear with me.</p>

          <p>I want to use this model on texts that are not from Wikipedia, and in
          fact, the texts are not articles at all. However, from what I understand,
          this model seemed to use Wikipedia as its knowledge base. So is it possible
          to train and use the model for non-Wikipedia text inputs? So far, the results
          I got from testing using Wikipedia articles have been great, and I would
          like to extend this to other types of texts as well.</p>

          '
        raw: "I'm quite new to this, so bear with me.\r\n\r\nI want to use this model\
          \ on texts that are not from Wikipedia, and in fact, the texts are not articles\
          \ at all. However, from what I understand, this model seemed to use Wikipedia\
          \ as its knowledge base. So is it possible to train and use the model for\
          \ non-Wikipedia text inputs? So far, the results I got from testing using\
          \ Wikipedia articles have been great, and I would like to extend this to\
          \ other types of texts as well."
        updatedAt: '2023-01-18T08:10:52.183Z'
      numEdits: 0
      reactions: []
    id: 63c7a98c08d61ef2b678d293
    type: comment
  author: f1amigo
  content: "I'm quite new to this, so bear with me.\r\n\r\nI want to use this model\
    \ on texts that are not from Wikipedia, and in fact, the texts are not articles\
    \ at all. However, from what I understand, this model seemed to use Wikipedia\
    \ as its knowledge base. So is it possible to train and use the model for non-Wikipedia\
    \ text inputs? So far, the results I got from testing using Wikipedia articles\
    \ have been great, and I would like to extend this to other types of texts as\
    \ well."
  created_at: 2023-01-18 08:10:52+00:00
  edited: false
  hidden: false
  id: 63c7a98c08d61ef2b678d293
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04e7545d08220171a0ad3e/M0dWlakxnLakQlZl5iOq6.jpeg?w=200&h=200&f=face
      fullname: Pere-Lluis Huguet Cabot
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PereLluis13
      type: user
    createdAt: '2023-01-23T10:31:56.000Z'
    data:
      edited: false
      editors:
      - PereLluis13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04e7545d08220171a0ad3e/M0dWlakxnLakQlZl5iOq6.jpeg?w=200&h=200&f=face
          fullname: Pere-Lluis Huguet Cabot
          isHf: false
          isPro: false
          name: PereLluis13
          type: user
        html: '<p>Hi, welcome to the ever-changing NLP field :)</p>

          <p>Indeed, REBEL was trained on a lot of Wikipedia texts. This means that
          it will work better on text that is similar to Wikipedia, especially to
          the text before Table of Contents, since that is what was used.</p>

          <p>So, how the model will work on Out-of-Domain (OOD, is how we refer to
          text from a different source/nature than the one used at train time) can
          really depend on how different that text is. </p>

          <p>I would first try REBEL on that text and see how the output looks. Then,
          if it is not that good, I suggest you fine-tune the model with text that
          is more similar to yours. This requires a bit more work. First, you would
          need to annotate some text with the relations that you are interested in
          extracting. Then you need to fine-tune on top of REBEL with your annotations.
          Be aware that when fine-tuning on top of REBEL you may face catastrophic
          forgetting, which means that the model "forgets" what it was first trained
          on (i.e., to extract the relation types you see using REBEL). So you either
          cover all the relation types you want to extract on your new annotation
          or you can mix the data with the  (<a href="https://huggingface.co/datasets/Babelscape/rebel-dataset">REBEL
          dataset</a>). </p>

          <p>I know this is quite a lot, but you can check the <a rel="nofollow" href="https://github.com/Babelscape/rebel">REBEL
          github</a> for more details on training. Also, if by any chance you are
          interested in news text, there is a dataset (NYT) that covers 25 relation
          types, which may cover your needs. On the GitHub page, you can find how
          to finetune REBEL on NYT, but if you don''t figure that out, I can find
          a way to share with you a trained checkpoint.</p>

          <p>Hope this helps, and sorry if some of these were redundant for you, I
          figured if you said you were new to this, it was better to assume not.</p>

          '
        raw: "Hi, welcome to the ever-changing NLP field :)\n\nIndeed, REBEL was trained\
          \ on a lot of Wikipedia texts. This means that it will work better on text\
          \ that is similar to Wikipedia, especially to the text before Table of Contents,\
          \ since that is what was used.\n\nSo, how the model will work on Out-of-Domain\
          \ (OOD, is how we refer to text from a different source/nature than the\
          \ one used at train time) can really depend on how different that text is.\
          \ \n\nI would first try REBEL on that text and see how the output looks.\
          \ Then, if it is not that good, I suggest you fine-tune the model with text\
          \ that is more similar to yours. This requires a bit more work. First, you\
          \ would need to annotate some text with the relations that you are interested\
          \ in extracting. Then you need to fine-tune on top of REBEL with your annotations.\
          \ Be aware that when fine-tuning on top of REBEL you may face catastrophic\
          \ forgetting, which means that the model \"forgets\" what it was first trained\
          \ on (i.e., to extract the relation types you see using REBEL). So you either\
          \ cover all the relation types you want to extract on your new annotation\
          \ or you can mix the data with the  ([REBEL dataset](https://huggingface.co/datasets/Babelscape/rebel-dataset)).\
          \ \n\nI know this is quite a lot, but you can check the [REBEL github](https://github.com/Babelscape/rebel)\
          \ for more details on training. Also, if by any chance you are interested\
          \ in news text, there is a dataset (NYT) that covers 25 relation types,\
          \ which may cover your needs. On the GitHub page, you can find how to finetune\
          \ REBEL on NYT, but if you don't figure that out, I can find a way to share\
          \ with you a trained checkpoint.\n\nHope this helps, and sorry if some of\
          \ these were redundant for you, I figured if you said you were new to this,\
          \ it was better to assume not."
        updatedAt: '2023-01-23T10:31:56.347Z'
      numEdits: 0
      reactions: []
    id: 63ce621c1378c94f84bd0118
    type: comment
  author: PereLluis13
  content: "Hi, welcome to the ever-changing NLP field :)\n\nIndeed, REBEL was trained\
    \ on a lot of Wikipedia texts. This means that it will work better on text that\
    \ is similar to Wikipedia, especially to the text before Table of Contents, since\
    \ that is what was used.\n\nSo, how the model will work on Out-of-Domain (OOD,\
    \ is how we refer to text from a different source/nature than the one used at\
    \ train time) can really depend on how different that text is. \n\nI would first\
    \ try REBEL on that text and see how the output looks. Then, if it is not that\
    \ good, I suggest you fine-tune the model with text that is more similar to yours.\
    \ This requires a bit more work. First, you would need to annotate some text with\
    \ the relations that you are interested in extracting. Then you need to fine-tune\
    \ on top of REBEL with your annotations. Be aware that when fine-tuning on top\
    \ of REBEL you may face catastrophic forgetting, which means that the model \"\
    forgets\" what it was first trained on (i.e., to extract the relation types you\
    \ see using REBEL). So you either cover all the relation types you want to extract\
    \ on your new annotation or you can mix the data with the  ([REBEL dataset](https://huggingface.co/datasets/Babelscape/rebel-dataset)).\
    \ \n\nI know this is quite a lot, but you can check the [REBEL github](https://github.com/Babelscape/rebel)\
    \ for more details on training. Also, if by any chance you are interested in news\
    \ text, there is a dataset (NYT) that covers 25 relation types, which may cover\
    \ your needs. On the GitHub page, you can find how to finetune REBEL on NYT, but\
    \ if you don't figure that out, I can find a way to share with you a trained checkpoint.\n\
    \nHope this helps, and sorry if some of these were redundant for you, I figured\
    \ if you said you were new to this, it was better to assume not."
  created_at: 2023-01-23 10:31:56+00:00
  edited: false
  hidden: false
  id: 63ce621c1378c94f84bd0118
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04e7545d08220171a0ad3e/M0dWlakxnLakQlZl5iOq6.jpeg?w=200&h=200&f=face
      fullname: Pere-Lluis Huguet Cabot
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PereLluis13
      type: user
    createdAt: '2023-01-31T10:31:40.000Z'
    data:
      status: closed
    id: 63d8ee0c649b0868ef2e4e46
    type: status-change
  author: PereLluis13
  created_at: 2023-01-31 10:31:40+00:00
  id: 63d8ee0c649b0868ef2e4e46
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Babelscape/rebel-large
repo_type: model
status: closed
target_branch: null
title: Input Limitations
