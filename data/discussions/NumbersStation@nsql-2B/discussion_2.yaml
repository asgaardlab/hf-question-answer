!!python/object:huggingface_hub.community.DiscussionWithDetails
author: arviii
conflicting_files: null
created_at: 2023-08-28 05:15:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BFQ7vI_g-kA0C9HkyfDnf.jpeg?w=200&h=200&f=face
      fullname: Arvind Shelke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arviii
      type: user
    createdAt: '2023-08-28T06:15:35.000Z'
    data:
      edited: true
      editors:
      - arviii
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7905141711235046
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BFQ7vI_g-kA0C9HkyfDnf.jpeg?w=200&h=200&f=face
          fullname: Arvind Shelke
          isHf: false
          isPro: false
          name: arviii
          type: user
        html: "<h2 id=\"hey-i-am-trying-to-deploy-a-model-on-a-cpu-instancemlm52xlarge-on-sagemaker-but-it-overflows-the-storage-and-best-way-to-resolve-this-might-be-to-mount-a-storage-volume-ebs-i-suppose\"\
          >Hey, I am trying to deploy a model on a CPU instance(<code>ml.m5.2xlarge</code>)\
          \ on sagemaker, but it overflows the storage and best way to resolve this\
          \ might be to mount a storage volume (EBS I suppose)</h2>\n<h4 id=\"to-do-so-ideally-should-pass-volume_size80-in-huggingface_modeldeploy-parameters-but-it-doesnt-seem-to-work-in-my-case-and-it-still-throws-same-error-about-storage-running-out\"\
          >To do so, ideally should pass <code>volume_size=80</code> in <code>huggingface_model.deploy</code>\
          \ parameters. But it doesn't seem to work in my case and it still throws\
          \ same error about storage running out.</h4>\n<p>Model: <code>https://huggingface.co/NumbersStation/nsql-llama-2-7B</code><br>Instance:\
          \ <code>ml.m5.2xlarge</code> (it works perfectly fine on <code>ml.g5.2xlarge</code>)</p>\n\
          <pre><code>error: \"Error: Download \nError safetensors_rust.SafetensorError:\
          \ Error while serializing: IoError(Os { code: 28, kind: StorageFull, message:\
          \ \"\"No space left on device\"\" })\"\ncode: predictor = huggingface_model.deploy(\n\
          \    initial_instance_count=1,\n    # instance_type=\"ml.g5.2xlarge\",\n\
          \    instance_type=\"ml.m5.2xlarge\",\n    container_startup_health_check_timeout=300,\n\
          \    volume_size=80,\n)\n</code></pre>\n<h4 id=\"rest-code-is-just-fine-as-it-gets-deployed-successfully-on-mlg52xlarge\"\
          >rest code is just fine as it gets deployed successfully on ml.g5.2xlarge</h4>\n"
        raw: "## Hey, I am trying to deploy a model on a CPU instance(`ml.m5.2xlarge`)\
          \ on sagemaker, but it overflows the storage and best way to resolve this\
          \ might be to mount a storage volume (EBS I suppose)\n#### To do so, ideally\
          \ should pass `volume_size=80` in `huggingface_model.deploy` parameters.\
          \ But it doesn't seem to work in my case and it still throws same error\
          \ about storage running out.\n\nModel: `https://huggingface.co/NumbersStation/nsql-llama-2-7B`\n\
          Instance: `ml.m5.2xlarge` (it works perfectly fine on `ml.g5.2xlarge`)\n\
          \n```\nerror: \"Error: Download \nError safetensors_rust.SafetensorError:\
          \ Error while serializing: IoError(Os { code: 28, kind: StorageFull, message:\
          \ \"\"No space left on device\"\" })\"\ncode: predictor = huggingface_model.deploy(\n\
          \    initial_instance_count=1,\n    # instance_type=\"ml.g5.2xlarge\",\n\
          \    instance_type=\"ml.m5.2xlarge\",\n    container_startup_health_check_timeout=300,\n\
          \    volume_size=80,\n)\n```\n#### rest code is just fine as it gets deployed\
          \ successfully on ml.g5.2xlarge"
        updatedAt: '2023-08-29T09:18:59.405Z'
      numEdits: 2
      reactions: []
    id: 64ec3b87345cf9d8c4e1d285
    type: comment
  author: arviii
  content: "## Hey, I am trying to deploy a model on a CPU instance(`ml.m5.2xlarge`)\
    \ on sagemaker, but it overflows the storage and best way to resolve this might\
    \ be to mount a storage volume (EBS I suppose)\n#### To do so, ideally should\
    \ pass `volume_size=80` in `huggingface_model.deploy` parameters. But it doesn't\
    \ seem to work in my case and it still throws same error about storage running\
    \ out.\n\nModel: `https://huggingface.co/NumbersStation/nsql-llama-2-7B`\nInstance:\
    \ `ml.m5.2xlarge` (it works perfectly fine on `ml.g5.2xlarge`)\n\n```\nerror:\
    \ \"Error: Download \nError safetensors_rust.SafetensorError: Error while serializing:\
    \ IoError(Os { code: 28, kind: StorageFull, message: \"\"No space left on device\"\
    \" })\"\ncode: predictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
    \    # instance_type=\"ml.g5.2xlarge\",\n    instance_type=\"ml.m5.2xlarge\",\n\
    \    container_startup_health_check_timeout=300,\n    volume_size=80,\n)\n```\n\
    #### rest code is just fine as it gets deployed successfully on ml.g5.2xlarge"
  created_at: 2023-08-28 05:15:35+00:00
  edited: true
  hidden: false
  id: 64ec3b87345cf9d8c4e1d285
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-08-28T22:55:19.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9081476926803589
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>Thank you for sharing this information! It will be helpful for others
          who are interested in deploying on Sagemaker.</p>

          '
        raw: Thank you for sharing this information! It will be helpful for others
          who are interested in deploying on Sagemaker.
        updatedAt: '2023-08-28T22:55:19.126Z'
      numEdits: 0
      reactions: []
    id: 64ed25d7f2d40fb56a828eee
    type: comment
  author: senwu
  content: Thank you for sharing this information! It will be helpful for others who
    are interested in deploying on Sagemaker.
  created_at: 2023-08-28 21:55:19+00:00
  edited: false
  hidden: false
  id: 64ed25d7f2d40fb56a828eee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NumbersStation/nsql-2B
repo_type: model
status: open
target_branch: null
title: Does it run on a CPU instance in sagemaker (ml.m5.2xlarge)?
