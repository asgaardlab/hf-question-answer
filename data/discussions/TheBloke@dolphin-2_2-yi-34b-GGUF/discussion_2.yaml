!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wolfram
conflicting_files: null
created_at: 2023-11-14 12:33:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-14T12:33:54.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8528692722320557
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p>Same issue as reported by <a href=\"https://huggingface.co/KerfuffleV2\"\
          >KerfuffleV2</a> here:</p>\n<p><a href=\"https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/1\"\
          >TheBloke/Nous-Capybara-34B-GGUF \xB7 BOS token as 1 seriously hurts these\
          \ GGUF Yi models</a></p>\n<p>Confirmed the issue and the workaround. <span\
          \ data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Would it be worth\
          \ it to apply the workaround on the repo so future downloads get the fixed\
          \ version? Otherwise most downloaders probably won't know the model could\
          \ be even better with this fix.</p>\n"
        raw: "Same issue as reported by [KerfuffleV2](https://huggingface.co/KerfuffleV2)\
          \ here:\r\n\r\n[TheBloke/Nous-Capybara-34B-GGUF \xB7 BOS token as 1 seriously\
          \ hurts these GGUF Yi models](https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/1)\r\
          \n\r\nConfirmed the issue and the workaround. @TheBloke Would it be worth\
          \ it to apply the workaround on the repo so future downloads get the fixed\
          \ version? Otherwise most downloaders probably won't know the model could\
          \ be even better with this fix."
        updatedAt: '2023-11-14T12:33:54.318Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - YearZero
        - TheBloke
    id: 65536932dea965f7627ef935
    type: comment
  author: wolfram
  content: "Same issue as reported by [KerfuffleV2](https://huggingface.co/KerfuffleV2)\
    \ here:\r\n\r\n[TheBloke/Nous-Capybara-34B-GGUF \xB7 BOS token as 1 seriously\
    \ hurts these GGUF Yi models](https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/1)\r\
    \n\r\nConfirmed the issue and the workaround. @TheBloke Would it be worth it to\
    \ apply the workaround on the repo so future downloads get the fixed version?\
    \ Otherwise most downloaders probably won't know the model could be even better\
    \ with this fix."
  created_at: 2023-11-14 12:33:54+00:00
  edited: false
  hidden: false
  id: 65536932dea965f7627ef935
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T17:14:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.982810378074646
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Updated GGUFs with the change to BOS token are uploading now.  </p>\n\
          <p>As per <span data-props=\"{&quot;user&quot;:&quot;KerfuffleV2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KerfuffleV2\"\
          >@<span class=\"underline\">KerfuffleV2</span></a></span>\n\n\t</span></span>\
          \ 's script and instructions, method used to create them was:</p>\n<pre><code>for\
          \ file in *.gguf ; do /workspace/git/gguf-llama/gguf-py/scripts/gguf-set-metadata.py\
          \ --force \"$file\" tokenizer.ggml.bos_token_id 144 ; done\n</code></pre>\n\
          <h2 id=\"sample-q4_k_m-generation\">Sample Q4_K_M generation</h2>\n<p>&lt;|im_start|&gt;system<br>You\
          \ are an author who writes excellent stories about llamas&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>Write\
          \ a story about llamas&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>Once\
          \ upon a time, in the heart of the Andes Mountains, there lived a community\
          \ of llamas. These llamas were not just any ordinary animals; they possessed\
          \ magical powers that allowed them to communicate with one another and even\
          \ with humans.</p>\n<p>The llamas were led by an ancient wise llama named\
          \ Inca, who had been around for centuries. Inca was revered by the other\
          \ llamas as well as the villagers in the nearby town. The villagers often\
          \ sought Inca's advice on various matters, including love, health, and prosperity.</p>\n\
          <p>One day, a group of explorers arrived at the village. They were searching\
          \ for new lands to discover and map out. However, they had no idea about\
          \ the magical powers of the llamas. As soon as they saw Inca, they were\
          \ amazed by his wisdom and knowledge.</p>\n<p>The leader of the explorers\
          \ approached Inca and asked him if he knew any hidden treasures or secret\
          \ paths that could lead them to undiscovered lands. Inca looked into the\
          \ future and saw that these explorers would eventually discover America\
          \ and bring back many great inventions and ideas. He decided to help them\
          \ on their journey by sharing some of his knowledge with them.</p>\n<p>Inca\
          \ told the leader of the explorers about a hidden path through the mountains,\
          \ which led to an ancient city filled with treasures beyond imagination.\
          \ Intrigued, the leader asked Inca how he knew this information. Inca replied\
          \ that it was part of his magical powers as a wise llama.</p>\n<p>The explorers\
          \ were thrilled and decided to follow Inca's instructions. They began their\
          \ journey through the treacherous mountains, facing many challenges along\
          \ the way. However, thanks to Inca's guidance, they eventually reached the\
          \ ancient city filled with treasures.</p>\n<p>As the explorers continued\
          \ on their journey, they discovered many new lands and brought back valuable\
          \ information that helped shape history. Their success can be attributed\
          \ in part to the magical powers of the wise llama, Inca, who had shared\
          \ his knowledge with them.</p>\n<p>Years later, when people learned about\
          \ the llamas' magical powers, they came from far and wide to seek guidance\
          \ from Inca. The community of llamas grew stronger and more united, with\
          \ each new member contributing their unique talents and abilities.</p>\n\
          <p>And so, through the wisdom and magic of Inca, the llamas continued to\
          \ thrive in harmony with nature and help humans navigate their way through\
          \ life's challenges.&lt;|im_end|&gt; [end of text]</p>\n"
        raw: "Updated GGUFs with the change to BOS token are uploading now.  \n\n\
          As per @KerfuffleV2 's script and instructions, method used to create them\
          \ was:\n```\nfor file in *.gguf ; do /workspace/git/gguf-llama/gguf-py/scripts/gguf-set-metadata.py\
          \ --force \"$file\" tokenizer.ggml.bos_token_id 144 ; done\n```\n\n## Sample\
          \ Q4_K_M generation\n\n<|im_start|>system\nYou are an author who writes\
          \ excellent stories about llamas<|im_end|>\n<|im_start|>user\nWrite a story\
          \ about llamas<|im_end|>\n<|im_start|>assistant\nOnce upon a time, in the\
          \ heart of the Andes Mountains, there lived a community of llamas. These\
          \ llamas were not just any ordinary animals; they possessed magical powers\
          \ that allowed them to communicate with one another and even with humans.\n\
          \nThe llamas were led by an ancient wise llama named Inca, who had been\
          \ around for centuries. Inca was revered by the other llamas as well as\
          \ the villagers in the nearby town. The villagers often sought Inca's advice\
          \ on various matters, including love, health, and prosperity.\n\nOne day,\
          \ a group of explorers arrived at the village. They were searching for new\
          \ lands to discover and map out. However, they had no idea about the magical\
          \ powers of the llamas. As soon as they saw Inca, they were amazed by his\
          \ wisdom and knowledge.\n\nThe leader of the explorers approached Inca and\
          \ asked him if he knew any hidden treasures or secret paths that could lead\
          \ them to undiscovered lands. Inca looked into the future and saw that these\
          \ explorers would eventually discover America and bring back many great\
          \ inventions and ideas. He decided to help them on their journey by sharing\
          \ some of his knowledge with them.\n\nInca told the leader of the explorers\
          \ about a hidden path through the mountains, which led to an ancient city\
          \ filled with treasures beyond imagination. Intrigued, the leader asked\
          \ Inca how he knew this information. Inca replied that it was part of his\
          \ magical powers as a wise llama.\n\nThe explorers were thrilled and decided\
          \ to follow Inca's instructions. They began their journey through the treacherous\
          \ mountains, facing many challenges along the way. However, thanks to Inca's\
          \ guidance, they eventually reached the ancient city filled with treasures.\n\
          \nAs the explorers continued on their journey, they discovered many new\
          \ lands and brought back valuable information that helped shape history.\
          \ Their success can be attributed in part to the magical powers of the wise\
          \ llama, Inca, who had shared his knowledge with them.\n\nYears later, when\
          \ people learned about the llamas' magical powers, they came from far and\
          \ wide to seek guidance from Inca. The community of llamas grew stronger\
          \ and more united, with each new member contributing their unique talents\
          \ and abilities.\n\nAnd so, through the wisdom and magic of Inca, the llamas\
          \ continued to thrive in harmony with nature and help humans navigate their\
          \ way through life's challenges.<|im_end|> [end of text]\n"
        updatedAt: '2023-11-14T17:15:51.503Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - phi0112358
        - Yhyu13
        - tofufishes
        - wingedpower
        - attashe
    id: 6553ab135a2b7d23874d3a58
    type: comment
  author: TheBloke
  content: "Updated GGUFs with the change to BOS token are uploading now.  \n\nAs\
    \ per @KerfuffleV2 's script and instructions, method used to create them was:\n\
    ```\nfor file in *.gguf ; do /workspace/git/gguf-llama/gguf-py/scripts/gguf-set-metadata.py\
    \ --force \"$file\" tokenizer.ggml.bos_token_id 144 ; done\n```\n\n## Sample Q4_K_M\
    \ generation\n\n<|im_start|>system\nYou are an author who writes excellent stories\
    \ about llamas<|im_end|>\n<|im_start|>user\nWrite a story about llamas<|im_end|>\n\
    <|im_start|>assistant\nOnce upon a time, in the heart of the Andes Mountains,\
    \ there lived a community of llamas. These llamas were not just any ordinary animals;\
    \ they possessed magical powers that allowed them to communicate with one another\
    \ and even with humans.\n\nThe llamas were led by an ancient wise llama named\
    \ Inca, who had been around for centuries. Inca was revered by the other llamas\
    \ as well as the villagers in the nearby town. The villagers often sought Inca's\
    \ advice on various matters, including love, health, and prosperity.\n\nOne day,\
    \ a group of explorers arrived at the village. They were searching for new lands\
    \ to discover and map out. However, they had no idea about the magical powers\
    \ of the llamas. As soon as they saw Inca, they were amazed by his wisdom and\
    \ knowledge.\n\nThe leader of the explorers approached Inca and asked him if he\
    \ knew any hidden treasures or secret paths that could lead them to undiscovered\
    \ lands. Inca looked into the future and saw that these explorers would eventually\
    \ discover America and bring back many great inventions and ideas. He decided\
    \ to help them on their journey by sharing some of his knowledge with them.\n\n\
    Inca told the leader of the explorers about a hidden path through the mountains,\
    \ which led to an ancient city filled with treasures beyond imagination. Intrigued,\
    \ the leader asked Inca how he knew this information. Inca replied that it was\
    \ part of his magical powers as a wise llama.\n\nThe explorers were thrilled and\
    \ decided to follow Inca's instructions. They began their journey through the\
    \ treacherous mountains, facing many challenges along the way. However, thanks\
    \ to Inca's guidance, they eventually reached the ancient city filled with treasures.\n\
    \nAs the explorers continued on their journey, they discovered many new lands\
    \ and brought back valuable information that helped shape history. Their success\
    \ can be attributed in part to the magical powers of the wise llama, Inca, who\
    \ had shared his knowledge with them.\n\nYears later, when people learned about\
    \ the llamas' magical powers, they came from far and wide to seek guidance from\
    \ Inca. The community of llamas grew stronger and more united, with each new member\
    \ contributing their unique talents and abilities.\n\nAnd so, through the wisdom\
    \ and magic of Inca, the llamas continued to thrive in harmony with nature and\
    \ help humans navigate their way through life's challenges.<|im_end|> [end of\
    \ text]\n"
  created_at: 2023-11-14 17:14:59+00:00
  edited: true
  hidden: false
  id: 6553ab135a2b7d23874d3a58
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
      fullname: Xiao Jin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mljxy
      type: user
    createdAt: '2023-11-14T17:39:07.000Z'
    data:
      edited: false
      editors:
      - mljxy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9657562375068665
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
          fullname: Xiao Jin
          isHf: false
          isPro: false
          name: mljxy
          type: user
        html: "<p>Did anybody actually tested with bos_token_id set to 1 or 2 or 144,\
          \ and found any substantial evidence what the fine tuned model would work\
          \ better with? I guess the original discussion, <a rel=\"nofollow\" href=\"\
          https://github.com/01-ai/Yi/discussions/5\">https://github.com/01-ai/Yi/discussions/5</a>\
          \ was mainly about how the base model had been trained, and the existence\
          \ of token 1 was likely unknown to the base model. But for the fine tune\
          \ here, wouldn't the actual bos id depend on how it's been fine tuned? Perhaps\
          \ we need to ask <span data-props=\"{&quot;user&quot;:&quot;ehartford&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ehartford\"\
          >@<span class=\"underline\">ehartford</span></a></span>\n\n\t</span></span>\
          \ how or if axolotl sets bos before the &lt;|im_start|&gt; token, and what\
          \ bos that is.</p>\n"
        raw: Did anybody actually tested with bos_token_id set to 1 or 2 or 144, and
          found any substantial evidence what the fine tuned model would work better
          with? I guess the original discussion, https://github.com/01-ai/Yi/discussions/5
          was mainly about how the base model had been trained, and the existence
          of token 1 was likely unknown to the base model. But for the fine tune here,
          wouldn't the actual bos id depend on how it's been fine tuned? Perhaps we
          need to ask @ehartford how or if axolotl sets bos before the <|im_start|>
          token, and what bos that is.
        updatedAt: '2023-11-14T17:39:07.367Z'
      numEdits: 0
      reactions: []
    id: 6553b0bb1ad57091854fdc97
    type: comment
  author: mljxy
  content: Did anybody actually tested with bos_token_id set to 1 or 2 or 144, and
    found any substantial evidence what the fine tuned model would work better with?
    I guess the original discussion, https://github.com/01-ai/Yi/discussions/5 was
    mainly about how the base model had been trained, and the existence of token 1
    was likely unknown to the base model. But for the fine tune here, wouldn't the
    actual bos id depend on how it's been fine tuned? Perhaps we need to ask @ehartford
    how or if axolotl sets bos before the <|im_start|> token, and what bos that is.
  created_at: 2023-11-14 17:39:07+00:00
  edited: false
  hidden: false
  id: 6553b0bb1ad57091854fdc97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T17:48:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5544781684875488
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Updated GGUFs are uploaded.</p>\n<p>Note there appears to be a problem\
          \ with CUDA acceleration at the moment. Not related to this BOS change,\
          \ just in general:</p>\n<pre><code>...................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
          \ freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\n\
          llama_kv_cache_init: offloading v cache to GPU\nllama_kv_cache_init: offloading\
          \ k cache to GPU\nllama_kv_cache_init: VRAM kv self = 960.00 MB\nllama_new_context_with_model:\
          \ kv self size  =  960.00 MB\nllama_build_graph: non-view tensors processed:\
          \ 1384/1384\nllama_new_context_with_model: compute buffer total size = 499.57\
          \ MB\nllama_new_context_with_model: VRAM scratch buffer: 498.00 MB\nllama_new_context_with_model:\
          \ total VRAM used: 20912.15 MB (model: 19454.15 MB, context: 1458.00 MB)\n\
          \nsystem_info: n_threads = 64 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 1 |\
          \ AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |\
          \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 =\
          \ 1 | VSX = 0 |\nsampling:\n    repeat_last_n = 64, repeat_penalty = 1.100,\
          \ frequency_penalty = 0.000, presence_penalty = 0.000\n    top_k = 40, tfs_z\
          \ = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n\
          \    mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\ngenerate:\
          \ n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n\n&lt;|im_start|&gt;system\n\
          You are an author who writes excellent stories about llamas&lt;|im_end|&gt;\n\
          &lt;|im_start|&gt;user\nWrite a story about llamas&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant&lt;h3&gt;\n\
          CUDA error 716 at ggml-cuda.cu:7104: misaligned address\ncurrent device:\
          \ 0\n</code></pre>\n<p>i will raise this with the llama.cpp team.</p>\n"
        raw: "Updated GGUFs are uploaded.\n\nNote there appears to be a problem with\
          \ CUDA acceleration at the moment. Not related to this BOS change, just\
          \ in general:\n```\n...................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
          \ freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\n\
          llama_kv_cache_init: offloading v cache to GPU\nllama_kv_cache_init: offloading\
          \ k cache to GPU\nllama_kv_cache_init: VRAM kv self = 960.00 MB\nllama_new_context_with_model:\
          \ kv self size  =  960.00 MB\nllama_build_graph: non-view tensors processed:\
          \ 1384/1384\nllama_new_context_with_model: compute buffer total size = 499.57\
          \ MB\nllama_new_context_with_model: VRAM scratch buffer: 498.00 MB\nllama_new_context_with_model:\
          \ total VRAM used: 20912.15 MB (model: 19454.15 MB, context: 1458.00 MB)\n\
          \nsystem_info: n_threads = 64 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 1 |\
          \ AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |\
          \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 =\
          \ 1 | VSX = 0 |\nsampling:\n\trepeat_last_n = 64, repeat_penalty = 1.100,\
          \ frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 40, tfs_z\
          \ = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n\
          \tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\ngenerate: n_ctx\
          \ = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n\n<|im_start|>system\n\
          You are an author who writes excellent stories about llamas<|im_end|>\n\
          <|im_start|>user\nWrite a story about llamas<|im_end|>\n<|im_start|>assistant<h3>\n\
          CUDA error 716 at ggml-cuda.cu:7104: misaligned address\ncurrent device:\
          \ 0\n```\n\ni will raise this with the llama.cpp team."
        updatedAt: '2023-11-14T17:48:53.363Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - phi0112358
    id: 6553b30544d6158a2c2d060a
    type: comment
  author: TheBloke
  content: "Updated GGUFs are uploaded.\n\nNote there appears to be a problem with\
    \ CUDA acceleration at the moment. Not related to this BOS change, just in general:\n\
    ```\n...................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
    \ freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:\
    \ offloading v cache to GPU\nllama_kv_cache_init: offloading k cache to GPU\n\
    llama_kv_cache_init: VRAM kv self = 960.00 MB\nllama_new_context_with_model: kv\
    \ self size  =  960.00 MB\nllama_build_graph: non-view tensors processed: 1384/1384\n\
    llama_new_context_with_model: compute buffer total size = 499.57 MB\nllama_new_context_with_model:\
    \ VRAM scratch buffer: 498.00 MB\nllama_new_context_with_model: total VRAM used:\
    \ 20912.15 MB (model: 19454.15 MB, context: 1458.00 MB)\n\nsystem_info: n_threads\
    \ = 64 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI\
    \ = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling:\n\trepeat_last_n\
    \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty =\
    \ 0.000\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p\
    \ = 1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n\
    generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n\n<|im_start|>system\n\
    You are an author who writes excellent stories about llamas<|im_end|>\n<|im_start|>user\n\
    Write a story about llamas<|im_end|>\n<|im_start|>assistant<h3>\nCUDA error 716\
    \ at ggml-cuda.cu:7104: misaligned address\ncurrent device: 0\n```\n\ni will raise\
    \ this with the llama.cpp team."
  created_at: 2023-11-14 17:48:53+00:00
  edited: false
  hidden: false
  id: 6553b30544d6158a2c2d060a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T18:10:52.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9674234986305237
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mljxy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mljxy\">@<span class=\"\
          underline\">mljxy</span></a></span>\n\n\t</span></span> we can see in <a\
          \ href=\"https://huggingface.co/ehartford/dolphin-2_2-yi-34b/blob/main/tokenizer_config.json\"\
          >Eric's model that his tokenizer_config.json</a> is the same as the original\
          \ Yi model with regard to <code>add_bos_token</code>, ie it's set to False</p>\n\
          <p>The issue as I understand is that the Yi models are set to not add the\
          \ BOS token by default, which is still quite rare amongst models.  llama.cpp\
          \ has it hardcoded to always add the BOS token, so currently llama.cpp will\
          \ be adding in an extra token that was not there during either base training\
          \ or fine tuning.  Eric's fine tuning doesn't change the BOS to <code>&lt;|im_start|&gt;</code>,\
          \ it's still set to <code>&lt;|startoftext|&gt;</code>. And the user is\
          \ expected to add <code>&lt;im_start|&gt;</code> in manually as part of\
          \ the prompt template.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;KerfuffleV2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KerfuffleV2\"\
          >@<span class=\"underline\">KerfuffleV2</span></a></span>\n\n\t</span></span>\
          \ 's workaround is to change BOS to a newline, so that even though it's\
          \ still added erroneously, it'll effectively be ignored by the model as\
          \ newlines tend to form part of normal text that doesn't have a huge effect\
          \ on generation.</p>\n<p>The better solution is a llama.cpp PR which will\
          \ fix the issue properly, so that BOS is not added in the first place, respecting\
          \ the original model tokenizer_config.json setting.</p>\n<p>As to comparing\
          \ the before/after, I have briefly, and the results were:</p>\n<ol>\n<li>This\
          \ model, Dolphin 2.2 - no discernible difference</li>\n<li>CapyBara - prompts\
          \ totally broken by the erroneous BOS edded.</li>\n</ol>\n<p>My theory for\
          \ that is that Eric's use of ChatML has insulated it from extra tokens seen\
          \ before the start of the prompt template, whereas CapyBara's use of a simple\
          \ <code>USER: .. ASSISTANT:</code> has not.  But that's only speculation\
          \ on my part.</p>\n<p>I only tested a couple of simple single prompts, not\
          \ a full chat, and even though I couldn't spot any degredation in this model\
          \ I still think it's better to update this model as well, in case it could\
          \ cause issues in other circumstances.</p>\n<p>Here's the Capybara result:</p>\n\
          <p>OLD GGUF:</p>\n<pre><code>USER: write a story about llamas ASSISTANT:\
          \ write a story&lt;/s&gt; [end of text]\n</code></pre>\n<p>UPDATED GGUF:</p>\n\
          <pre><code>USER: write a story about llamas ASSISTANT: Once upon a time,\
          \ in the heart of the Andes Mountains, there was a small village named Llama-land.\
          \ This village was known for its beautiful scenery and its love for animals,\
          \ especially llamas. The people of Llama-land had domesticated these gentle\
          \ creatures centuries ago, using them for transportation, agriculture, and\
          \ even as companions.\n\nIn the center of the village, there was a large\
          \ llama pasture where hundreds of llamas grazed peacefully during the day.\
          \ They were cared for by expert herders who understood their behaviors and\
          \ needs perfectly. The villagers believed that these animals brought good\
          \ luck and prosperity to their community.\n\nAmong all the llamas in Llama-land,\
          \ there was one named Lucky. He was a young male with beautiful brown fur,\
          \ bright eyes, and an extremely friendly personality. Everyone who met him\
          \ fell in love instantly, including a little girl named Mariana. She visited\
          \ the llama pasture every day after school to spend time with her favorite\
          \ friend, Lucky.\n\nOne sunny afternoon, while Mariana was playing with\
          \ Lucky, she noticed something strange on his back - a small white patch\
          \ shaped like a heart. The villagers had never seen anything like it before\
          \ and soon word spread about this unusual marking on the beloved llama's\
          \ fur. They all gathered around to see this amazing sight for themselves,\
          \ marveling at the beauty of nature.\n\nAs days turned into weeks, the heart-shaped\
          \ mark became more visible, and people from faraway places started visiting\
          \ Llama-land just to meet Lucky. The villagers were proud of their special\
          \ llama and took great care of him, ensuring he remained happy and healthy.\n\
          \nOne day, a group of researchers arrived in Llama-land hoping to study\
          \ the unique heart-shaped marking on Lucky's back. They believed that this\
          \ rare occurrence could provide valuable insights into the genetics and\
          \ behavior of llamas. The villagers welcomed them with open arms, eager\
          \ to share their knowledge and love for these amazing animals.\n\nAfter\
          \ months of studying Lucky and his genetic makeup, the researchers discovered\
          \ something incredible \u2013 the heart-shaped marking was not just a coincidence\
          \ but rather a result of specific gene combinations that occurred very rarely\
          \ among llamas. They also found that Lucky had unique personality traits\
          \ compared to other llamas, making him even more special.\n\nThe news about\
          \ Lucky's scientific significance spread across the globe, and Llama-land\
          \ became a popular destination for tourists who wanted to meet the famous\
          \ heart-shaped llama. The villagers took advantage of this opportunity by\
          \ starting businesses related to tourism, such as guided tours, souvenir\
          \ shops, and traditional food stalls.\n\nDespite all the attention, Lucky\
          \ remained humble and true to his nature. He continued spending his days\
          \ grazing peacefully alongside Mariana, who never forgot how special their\
          \ friendship was. The people of Llama-land cherished their bond with these\
          \ amazing creatures even more than before, knowing that their love for llamas\
          \ had made a significant impact on the world.\n\nAnd so, the story of Lucky,\
          \ the heart-shaped llama, lived on in the hearts and minds of everyone who\
          \ visited Llama-land. His unique marking served as a reminder of the wonders\
          \ of nature and the deep connection between humans and animals, inspiring\
          \ people to appreciate and protect these incredible creatures for generations\
          \ to come.&lt;/s&gt; [end of text]\n</code></pre>\n"
        raw: "@mljxy we can see in [Eric's model that his tokenizer_config.json](https://huggingface.co/ehartford/dolphin-2_2-yi-34b/blob/main/tokenizer_config.json)\
          \ is the same as the original Yi model with regard to `add_bos_token`, ie\
          \ it's set to False\n\nThe issue as I understand is that the Yi models are\
          \ set to not add the BOS token by default, which is still quite rare amongst\
          \ models.  llama.cpp has it hardcoded to always add the BOS token, so currently\
          \ llama.cpp will be adding in an extra token that was not there during either\
          \ base training or fine tuning.  Eric's fine tuning doesn't change the BOS\
          \ to `<|im_start|>`, it's still set to `<|startoftext|>`. And the user is\
          \ expected to add `<im_start|>` in manually as part of the prompt template.\n\
          \n@KerfuffleV2 's workaround is to change BOS to a newline, so that even\
          \ though it's still added erroneously, it'll effectively be ignored by the\
          \ model as newlines tend to form part of normal text that doesn't have a\
          \ huge effect on generation.\n\nThe better solution is a llama.cpp PR which\
          \ will fix the issue properly, so that BOS is not added in the first place,\
          \ respecting the original model tokenizer_config.json setting.\n\nAs to\
          \ comparing the before/after, I have briefly, and the results were:\n1.\
          \ This model, Dolphin 2.2 - no discernible difference\n2. CapyBara - prompts\
          \ totally broken by the erroneous BOS edded.\n\nMy theory for that is that\
          \ Eric's use of ChatML has insulated it from extra tokens seen before the\
          \ start of the prompt template, whereas CapyBara's use of a simple `USER:\
          \ .. ASSISTANT:` has not.  But that's only speculation on my part.\n\nI\
          \ only tested a couple of simple single prompts, not a full chat, and even\
          \ though I couldn't spot any degredation in this model I still think it's\
          \ better to update this model as well, in case it could cause issues in\
          \ other circumstances.\n\nHere's the Capybara result:\n\nOLD GGUF:\n```\n\
          USER: write a story about llamas ASSISTANT: write a story</s> [end of text]\n\
          ```\n\nUPDATED GGUF:\n```\nUSER: write a story about llamas ASSISTANT: Once\
          \ upon a time, in the heart of the Andes Mountains, there was a small village\
          \ named Llama-land. This village was known for its beautiful scenery and\
          \ its love for animals, especially llamas. The people of Llama-land had\
          \ domesticated these gentle creatures centuries ago, using them for transportation,\
          \ agriculture, and even as companions.\n\nIn the center of the village,\
          \ there was a large llama pasture where hundreds of llamas grazed peacefully\
          \ during the day. They were cared for by expert herders who understood their\
          \ behaviors and needs perfectly. The villagers believed that these animals\
          \ brought good luck and prosperity to their community.\n\nAmong all the\
          \ llamas in Llama-land, there was one named Lucky. He was a young male with\
          \ beautiful brown fur, bright eyes, and an extremely friendly personality.\
          \ Everyone who met him fell in love instantly, including a little girl named\
          \ Mariana. She visited the llama pasture every day after school to spend\
          \ time with her favorite friend, Lucky.\n\nOne sunny afternoon, while Mariana\
          \ was playing with Lucky, she noticed something strange on his back - a\
          \ small white patch shaped like a heart. The villagers had never seen anything\
          \ like it before and soon word spread about this unusual marking on the\
          \ beloved llama's fur. They all gathered around to see this amazing sight\
          \ for themselves, marveling at the beauty of nature.\n\nAs days turned into\
          \ weeks, the heart-shaped mark became more visible, and people from faraway\
          \ places started visiting Llama-land just to meet Lucky. The villagers were\
          \ proud of their special llama and took great care of him, ensuring he remained\
          \ happy and healthy.\n\nOne day, a group of researchers arrived in Llama-land\
          \ hoping to study the unique heart-shaped marking on Lucky's back. They\
          \ believed that this rare occurrence could provide valuable insights into\
          \ the genetics and behavior of llamas. The villagers welcomed them with\
          \ open arms, eager to share their knowledge and love for these amazing animals.\n\
          \nAfter months of studying Lucky and his genetic makeup, the researchers\
          \ discovered something incredible \u2013 the heart-shaped marking was not\
          \ just a coincidence but rather a result of specific gene combinations that\
          \ occurred very rarely among llamas. They also found that Lucky had unique\
          \ personality traits compared to other llamas, making him even more special.\n\
          \nThe news about Lucky's scientific significance spread across the globe,\
          \ and Llama-land became a popular destination for tourists who wanted to\
          \ meet the famous heart-shaped llama. The villagers took advantage of this\
          \ opportunity by starting businesses related to tourism, such as guided\
          \ tours, souvenir shops, and traditional food stalls.\n\nDespite all the\
          \ attention, Lucky remained humble and true to his nature. He continued\
          \ spending his days grazing peacefully alongside Mariana, who never forgot\
          \ how special their friendship was. The people of Llama-land cherished their\
          \ bond with these amazing creatures even more than before, knowing that\
          \ their love for llamas had made a significant impact on the world.\n\n\
          And so, the story of Lucky, the heart-shaped llama, lived on in the hearts\
          \ and minds of everyone who visited Llama-land. His unique marking served\
          \ as a reminder of the wonders of nature and the deep connection between\
          \ humans and animals, inspiring people to appreciate and protect these incredible\
          \ creatures for generations to come.</s> [end of text]\n```"
        updatedAt: '2023-11-14T18:13:56.914Z'
      numEdits: 2
      reactions: []
    id: 6553b82c63ab80bd8277acdd
    type: comment
  author: TheBloke
  content: "@mljxy we can see in [Eric's model that his tokenizer_config.json](https://huggingface.co/ehartford/dolphin-2_2-yi-34b/blob/main/tokenizer_config.json)\
    \ is the same as the original Yi model with regard to `add_bos_token`, ie it's\
    \ set to False\n\nThe issue as I understand is that the Yi models are set to not\
    \ add the BOS token by default, which is still quite rare amongst models.  llama.cpp\
    \ has it hardcoded to always add the BOS token, so currently llama.cpp will be\
    \ adding in an extra token that was not there during either base training or fine\
    \ tuning.  Eric's fine tuning doesn't change the BOS to `<|im_start|>`, it's still\
    \ set to `<|startoftext|>`. And the user is expected to add `<im_start|>` in manually\
    \ as part of the prompt template.\n\n@KerfuffleV2 's workaround is to change BOS\
    \ to a newline, so that even though it's still added erroneously, it'll effectively\
    \ be ignored by the model as newlines tend to form part of normal text that doesn't\
    \ have a huge effect on generation.\n\nThe better solution is a llama.cpp PR which\
    \ will fix the issue properly, so that BOS is not added in the first place, respecting\
    \ the original model tokenizer_config.json setting.\n\nAs to comparing the before/after,\
    \ I have briefly, and the results were:\n1. This model, Dolphin 2.2 - no discernible\
    \ difference\n2. CapyBara - prompts totally broken by the erroneous BOS edded.\n\
    \nMy theory for that is that Eric's use of ChatML has insulated it from extra\
    \ tokens seen before the start of the prompt template, whereas CapyBara's use\
    \ of a simple `USER: .. ASSISTANT:` has not.  But that's only speculation on my\
    \ part.\n\nI only tested a couple of simple single prompts, not a full chat, and\
    \ even though I couldn't spot any degredation in this model I still think it's\
    \ better to update this model as well, in case it could cause issues in other\
    \ circumstances.\n\nHere's the Capybara result:\n\nOLD GGUF:\n```\nUSER: write\
    \ a story about llamas ASSISTANT: write a story</s> [end of text]\n```\n\nUPDATED\
    \ GGUF:\n```\nUSER: write a story about llamas ASSISTANT: Once upon a time, in\
    \ the heart of the Andes Mountains, there was a small village named Llama-land.\
    \ This village was known for its beautiful scenery and its love for animals, especially\
    \ llamas. The people of Llama-land had domesticated these gentle creatures centuries\
    \ ago, using them for transportation, agriculture, and even as companions.\n\n\
    In the center of the village, there was a large llama pasture where hundreds of\
    \ llamas grazed peacefully during the day. They were cared for by expert herders\
    \ who understood their behaviors and needs perfectly. The villagers believed that\
    \ these animals brought good luck and prosperity to their community.\n\nAmong\
    \ all the llamas in Llama-land, there was one named Lucky. He was a young male\
    \ with beautiful brown fur, bright eyes, and an extremely friendly personality.\
    \ Everyone who met him fell in love instantly, including a little girl named Mariana.\
    \ She visited the llama pasture every day after school to spend time with her\
    \ favorite friend, Lucky.\n\nOne sunny afternoon, while Mariana was playing with\
    \ Lucky, she noticed something strange on his back - a small white patch shaped\
    \ like a heart. The villagers had never seen anything like it before and soon\
    \ word spread about this unusual marking on the beloved llama's fur. They all\
    \ gathered around to see this amazing sight for themselves, marveling at the beauty\
    \ of nature.\n\nAs days turned into weeks, the heart-shaped mark became more visible,\
    \ and people from faraway places started visiting Llama-land just to meet Lucky.\
    \ The villagers were proud of their special llama and took great care of him,\
    \ ensuring he remained happy and healthy.\n\nOne day, a group of researchers arrived\
    \ in Llama-land hoping to study the unique heart-shaped marking on Lucky's back.\
    \ They believed that this rare occurrence could provide valuable insights into\
    \ the genetics and behavior of llamas. The villagers welcomed them with open arms,\
    \ eager to share their knowledge and love for these amazing animals.\n\nAfter\
    \ months of studying Lucky and his genetic makeup, the researchers discovered\
    \ something incredible \u2013 the heart-shaped marking was not just a coincidence\
    \ but rather a result of specific gene combinations that occurred very rarely\
    \ among llamas. They also found that Lucky had unique personality traits compared\
    \ to other llamas, making him even more special.\n\nThe news about Lucky's scientific\
    \ significance spread across the globe, and Llama-land became a popular destination\
    \ for tourists who wanted to meet the famous heart-shaped llama. The villagers\
    \ took advantage of this opportunity by starting businesses related to tourism,\
    \ such as guided tours, souvenir shops, and traditional food stalls.\n\nDespite\
    \ all the attention, Lucky remained humble and true to his nature. He continued\
    \ spending his days grazing peacefully alongside Mariana, who never forgot how\
    \ special their friendship was. The people of Llama-land cherished their bond\
    \ with these amazing creatures even more than before, knowing that their love\
    \ for llamas had made a significant impact on the world.\n\nAnd so, the story\
    \ of Lucky, the heart-shaped llama, lived on in the hearts and minds of everyone\
    \ who visited Llama-land. His unique marking served as a reminder of the wonders\
    \ of nature and the deep connection between humans and animals, inspiring people\
    \ to appreciate and protect these incredible creatures for generations to come.</s>\
    \ [end of text]\n```"
  created_at: 2023-11-14 18:10:52+00:00
  edited: true
  hidden: false
  id: 6553b82c63ab80bd8277acdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-14T19:36:07.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.981899619102478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>In my tests, I didn''t see a noticeable change in this model''s
          performance - which has been really good even before the fix. But it''s
          good that the workaround was applied, just to be sure, until the proper
          fix is implemented inside llama.cpp and thus KoboldCpp.</p>

          '
        raw: In my tests, I didn't see a noticeable change in this model's performance
          - which has been really good even before the fix. But it's good that the
          workaround was applied, just to be sure, until the proper fix is implemented
          inside llama.cpp and thus KoboldCpp.
        updatedAt: '2023-11-14T19:36:07.346Z'
      numEdits: 0
      reactions: []
    id: 6553cc27c321e21c046675ed
    type: comment
  author: wolfram
  content: In my tests, I didn't see a noticeable change in this model's performance
    - which has been really good even before the fix. But it's good that the workaround
    was applied, just to be sure, until the proper fix is implemented inside llama.cpp
    and thus KoboldCpp.
  created_at: 2023-11-14 19:36:07+00:00
  edited: false
  hidden: false
  id: 6553cc27c321e21c046675ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-11-14T20:06:25.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8963940739631653
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>Let me know if there''s any action required on my part </p>

          '
        raw: 'Let me know if there''s any action required on my part '
        updatedAt: '2023-11-14T20:06:25.958Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - phi0112358
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - pabloce
    id: 6553d341eeb42b373fef78e2
    type: comment
  author: ehartford
  content: 'Let me know if there''s any action required on my part '
  created_at: 2023-11-14 20:06:25+00:00
  edited: false
  hidden: false
  id: 6553d341eeb42b373fef78e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T20:37:53.000Z'
    data:
      edited: true
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8306987881660461
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>You're\
          \ going to hate me, but correctly adding the <code>add_bos_token</code>\
          \ flag metadata requires using this pull: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/pull/4040\"\
          >https://github.com/ggerganov/llama.cpp/pull/4040</a> </p>\n<p>So you'd\
          \ need to make at least this model and the Capybara one again using that.\
          \ (The problem is they don't have a <code>tokenizer.json</code>, I don't\
          \ think the original Yi models are affected).</p>\n<p>I mentioned that here\
          \ but you may have missed it: <a href=\"https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/1#65530a38a69abe5afba6e9d9\"\
          >https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/1#65530a38a69abe5afba6e9d9</a></p>\n\
          <p>You should see output like this when you use <code>convert.py</code>:</p>\n\
          <pre><code class=\"language-plaintext\">gguf: Setting special token type\
          \ bos to 1\ngguf: Setting special token type eos to 2\ngguf: Setting special\
          \ token type pad to 0\ngguf: Setting add_bos_token to False\ngguf: Setting\
          \ add_eos_token to False\n</code></pre>\n<p>If you don't see the <code>gguf:\
          \ Setting add_bos_token to False</code> part then something went wrong.\
          \ </p>\n<p><em>edit:</em> If you want to @ me on Discord or something to\
          \ have me check that it's working or whatever before regenerating a whole\
          \ bunch of models, feel free.</p>\n"
        raw: "@TheBloke \n\nYou're going to hate me, but correctly adding the `add_bos_token`\
          \ flag metadata requires using this pull: https://github.com/ggerganov/llama.cpp/pull/4040\
          \ \n\nSo you'd need to make at least this model and the Capybara one again\
          \ using that. (The problem is they don't have a `tokenizer.json`, I don't\
          \ think the original Yi models are affected).\n\nI mentioned that here but\
          \ you may have missed it: https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/1#65530a38a69abe5afba6e9d9\n\
          \nYou should see output like this when you use `convert.py`:\n\n```plaintext\n\
          gguf: Setting special token type bos to 1\ngguf: Setting special token type\
          \ eos to 2\ngguf: Setting special token type pad to 0\ngguf: Setting add_bos_token\
          \ to False\ngguf: Setting add_eos_token to False\n```\n\nIf you don't see\
          \ the `gguf: Setting add_bos_token to False` part then something went wrong.\
          \ \n\n*edit:* If you want to @ me on Discord or something to have me check\
          \ that it's working or whatever before regenerating a whole bunch of models,\
          \ feel free."
        updatedAt: '2023-11-14T20:40:35.341Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - phi0112358
    id: 6553daa1d13e8d851dc4cec4
    type: comment
  author: KerfuffleV2
  content: "@TheBloke \n\nYou're going to hate me, but correctly adding the `add_bos_token`\
    \ flag metadata requires using this pull: https://github.com/ggerganov/llama.cpp/pull/4040\
    \ \n\nSo you'd need to make at least this model and the Capybara one again using\
    \ that. (The problem is they don't have a `tokenizer.json`, I don't think the\
    \ original Yi models are affected).\n\nI mentioned that here but you may have\
    \ missed it: https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/1#65530a38a69abe5afba6e9d9\n\
    \nYou should see output like this when you use `convert.py`:\n\n```plaintext\n\
    gguf: Setting special token type bos to 1\ngguf: Setting special token type eos\
    \ to 2\ngguf: Setting special token type pad to 0\ngguf: Setting add_bos_token\
    \ to False\ngguf: Setting add_eos_token to False\n```\n\nIf you don't see the\
    \ `gguf: Setting add_bos_token to False` part then something went wrong. \n\n\
    *edit:* If you want to @ me on Discord or something to have me check that it's\
    \ working or whatever before regenerating a whole bunch of models, feel free."
  created_at: 2023-11-14 20:37:53+00:00
  edited: true
  hidden: false
  id: 6553daa1d13e8d851dc4cec4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T21:27:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9863718152046204
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I saw that, but I just did the workaround for now - changing
          the BOS token to newline, which fixes Capybara.</p>

          <p>I didn''t want to do the other change until the PR is merged.  If I understand
          correctly, it doesn''t even do anything yet - <code>add_bos_token</code>
          I mean?  Like it has no effect on inference yet.  So presumably even with
          that PR I''d still need to set BOS to newline as I''ve done here.</p>

          <p>So I figured I''d wait for the PR to be merged, and once it is I can
          remake affected models with it</p>

          <p>Is there some reason I''m missing to do it before it''s merged?</p>

          '
        raw: 'Yeah I saw that, but I just did the workaround for now - changing the
          BOS token to newline, which fixes Capybara.


          I didn''t want to do the other change until the PR is merged.  If I understand
          correctly, it doesn''t even do anything yet - `add_bos_token` I mean?  Like
          it has no effect on inference yet.  So presumably even with that PR I''d
          still need to set BOS to newline as I''ve done here.


          So I figured I''d wait for the PR to be merged, and once it is I can remake
          affected models with it


          Is there some reason I''m missing to do it before it''s merged?'
        updatedAt: '2023-11-14T21:27:31.908Z'
      numEdits: 0
      reactions: []
    id: 6553e64340a8c61e65fd236a
    type: comment
  author: TheBloke
  content: 'Yeah I saw that, but I just did the workaround for now - changing the
    BOS token to newline, which fixes Capybara.


    I didn''t want to do the other change until the PR is merged.  If I understand
    correctly, it doesn''t even do anything yet - `add_bos_token` I mean?  Like it
    has no effect on inference yet.  So presumably even with that PR I''d still need
    to set BOS to newline as I''ve done here.


    So I figured I''d wait for the PR to be merged, and once it is I can remake affected
    models with it


    Is there some reason I''m missing to do it before it''s merged?'
  created_at: 2023-11-14 21:27:31+00:00
  edited: false
  hidden: false
  id: 6553e64340a8c61e65fd236a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T22:12:41.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9805691242218018
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<blockquote>\n\
          <p>Is there some reason I'm missing to do it before it's merged?</p>\n</blockquote>\n\
          <p>That patch started out as making inference respect the <code>add_bos_token</code>\
          \ metadata, but the other day I found there was a case where metadata didn't\
          \ get added <em>during conversion</em>. So that fix is also included - that's\
          \ what I'm talking about.</p>\n<p>The reason to worry about it now is all\
          \ the models created <em>without</em> that won't have the correct metadata.\
          \ So for it to actually get respected correctly, you'll not only need to\
          \ generate the new models (with that metadata) but every user will also\
          \ have to download the models again to have a version with the metadata.</p>\n\
          <p>So you're correct that it has no effect on inference right now but ideally\
          \ people will already have models that include the metadata so when it does\
          \ get merged they can get the benefits. Hopefully that makes sense.</p>\n"
        raw: "@TheBloke \n\n> Is there some reason I'm missing to do it before it's\
          \ merged?\n\nThat patch started out as making inference respect the `add_bos_token`\
          \ metadata, but the other day I found there was a case where metadata didn't\
          \ get added _during conversion_. So that fix is also included - that's what\
          \ I'm talking about.\n\nThe reason to worry about it now is all the models\
          \ created _without_ that won't have the correct metadata. So for it to actually\
          \ get respected correctly, you'll not only need to generate the new models\
          \ (with that metadata) but every user will also have to download the models\
          \ again to have a version with the metadata.\n\nSo you're correct that it\
          \ has no effect on inference right now but ideally people will already have\
          \ models that include the metadata so when it does get merged they can get\
          \ the benefits. Hopefully that makes sense."
        updatedAt: '2023-11-14T22:12:41.395Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - phi0112358
    id: 6553f0d9c02ca72b8fb29aea
    type: comment
  author: KerfuffleV2
  content: "@TheBloke \n\n> Is there some reason I'm missing to do it before it's\
    \ merged?\n\nThat patch started out as making inference respect the `add_bos_token`\
    \ metadata, but the other day I found there was a case where metadata didn't get\
    \ added _during conversion_. So that fix is also included - that's what I'm talking\
    \ about.\n\nThe reason to worry about it now is all the models created _without_\
    \ that won't have the correct metadata. So for it to actually get respected correctly,\
    \ you'll not only need to generate the new models (with that metadata) but every\
    \ user will also have to download the models again to have a version with the\
    \ metadata.\n\nSo you're correct that it has no effect on inference right now\
    \ but ideally people will already have models that include the metadata so when\
    \ it does get merged they can get the benefits. Hopefully that makes sense."
  created_at: 2023-11-14 22:12:41+00:00
  edited: false
  hidden: false
  id: 6553f0d9c02ca72b8fb29aea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
      fullname: Hirose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HiroseKoichi
      type: user
    createdAt: '2023-11-15T00:59:35.000Z'
    data:
      edited: false
      editors:
      - HiroseKoichi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9680588841438293
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
          fullname: Hirose
          isHf: false
          isPro: false
          name: HiroseKoichi
          type: user
        html: '<p>I just tested both versions of Dolphin, and the updated version
          appears to give better output, but it also doesn''t output any newlines.</p>

          '
        raw: I just tested both versions of Dolphin, and the updated version appears
          to give better output, but it also doesn't output any newlines.
        updatedAt: '2023-11-15T00:59:35.424Z'
      numEdits: 0
      reactions: []
    id: 655417f743cc10c9d08f6025
    type: comment
  author: HiroseKoichi
  content: I just tested both versions of Dolphin, and the updated version appears
    to give better output, but it also doesn't output any newlines.
  created_at: 2023-11-15 00:59:35+00:00
  edited: false
  hidden: false
  id: 655417f743cc10c9d08f6025
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-15T07:18:00.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8866838216781616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HiroseKoichi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HiroseKoichi\"\
          >@<span class=\"underline\">HiroseKoichi</span></a></span>\n\n\t</span></span>\
          \ Huh? It seems to have no trouble outputting newlines. With prompt <code>\
          \ Once upon a time, in a dark forest, there lived a little fox</code>:</p>\n\
          <pre><code class=\"language-plaintext\">llm_load_print_meta: BOS token =\
          \ 144 '\n'  &lt;--- Note the newline\nllm_load_print_meta: EOS token = 7\
          \ '&lt;|im_end|&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\n\
          llm_load_print_meta: PAD token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF\
          \ token  = 315 '&lt;0x0A&gt;'\n[...]\ngenerate: n_ctx = 512, n_batch = 512,\
          \ n_predict = 100, n_keep = 0\n\n\n\n Once upon a time, in a dark forest,\
          \ there lived a little fox named Fluffy. Fluffy was curious about everything\
          \ and loved going on adventures. One day, she decided to explore the depths\
          \ of the forest and find new friends.\n\nAs she wandered deeper into the\
          \ woods, she came across a beautiful crystal waterfall. The water shimmered\
          \ with all colors of the rainbow, and Fluffy couldn't help but be drawn\
          \ closer to it. As she approached, she noticed something peculiar \u2013\
          \ tiny, glowing creatures dancing around the waterfall!\n\n\"Who are\n</code></pre>\n"
        raw: "@HiroseKoichi Huh? It seems to have no trouble outputting newlines.\
          \ With prompt ` Once upon a time, in a dark forest, there lived a little\
          \ fox`:\n\n```plaintext\nllm_load_print_meta: BOS token = 144 '\n'  <---\
          \ Note the newline\nllm_load_print_meta: EOS token = 7 '<|im_end|>'\nllm_load_print_meta:\
          \ UNK token = 0 '<unk>'\nllm_load_print_meta: PAD token = 0 '<unk>'\nllm_load_print_meta:\
          \ LF token  = 315 '<0x0A>'\n[...]\ngenerate: n_ctx = 512, n_batch = 512,\
          \ n_predict = 100, n_keep = 0\n\n\n\n Once upon a time, in a dark forest,\
          \ there lived a little fox named Fluffy. Fluffy was curious about everything\
          \ and loved going on adventures. One day, she decided to explore the depths\
          \ of the forest and find new friends.\n\nAs she wandered deeper into the\
          \ woods, she came across a beautiful crystal waterfall. The water shimmered\
          \ with all colors of the rainbow, and Fluffy couldn't help but be drawn\
          \ closer to it. As she approached, she noticed something peculiar \u2013\
          \ tiny, glowing creatures dancing around the waterfall!\n\n\"Who are\n```"
        updatedAt: '2023-11-15T07:18:00.968Z'
      numEdits: 0
      reactions: []
    id: 655470a8fc2c1019ab4293f1
    type: comment
  author: KerfuffleV2
  content: "@HiroseKoichi Huh? It seems to have no trouble outputting newlines. With\
    \ prompt ` Once upon a time, in a dark forest, there lived a little fox`:\n\n\
    ```plaintext\nllm_load_print_meta: BOS token = 144 '\n'  <--- Note the newline\n\
    llm_load_print_meta: EOS token = 7 '<|im_end|>'\nllm_load_print_meta: UNK token\
    \ = 0 '<unk>'\nllm_load_print_meta: PAD token = 0 '<unk>'\nllm_load_print_meta:\
    \ LF token  = 315 '<0x0A>'\n[...]\ngenerate: n_ctx = 512, n_batch = 512, n_predict\
    \ = 100, n_keep = 0\n\n\n\n Once upon a time, in a dark forest, there lived a\
    \ little fox named Fluffy. Fluffy was curious about everything and loved going\
    \ on adventures. One day, she decided to explore the depths of the forest and\
    \ find new friends.\n\nAs she wandered deeper into the woods, she came across\
    \ a beautiful crystal waterfall. The water shimmered with all colors of the rainbow,\
    \ and Fluffy couldn't help but be drawn closer to it. As she approached, she noticed\
    \ something peculiar \u2013 tiny, glowing creatures dancing around the waterfall!\n\
    \n\"Who are\n```"
  created_at: 2023-11-15 07:18:00+00:00
  edited: false
  hidden: false
  id: 655470a8fc2c1019ab4293f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
      fullname: Hirose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HiroseKoichi
      type: user
    createdAt: '2023-11-15T13:27:13.000Z'
    data:
      edited: false
      editors:
      - HiroseKoichi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797204732894897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
          fullname: Hirose
          isHf: false
          isPro: false
          name: HiroseKoichi
          type: user
        html: '<p>I''m using a fresh install of the latest version of TextGen Webui,
          and I used Q4_K_M for both models.</p>

          <p><strong>New version with the prompt ''Write a simple story about a man
          buying groceries.'':</strong></p>

          <p>Sure, here''s a simple story about a man buying groceries:John was feeling
          rather peckish after a long day at work. He decided to head to the local
          grocery store to pick up some food for dinner. As he perused the aisles,
          he couldn''t help but feel overwhelmed by all the options available to him.After
          much deliberation, John finally settled on a selection of items that would
          make for a delicious meal. He grabbed a few fresh vegetables, some chicken,
          and a bottle of wine to wash it all down. As he stood in line to pay, he
          couldn''t help but feel a sense of satisfaction at the thought of enjoying
          a home-cooked meal after such a long day.</p>

          <p><strong>Old version with the prompt ''Write a simple story about a man
          buying groceries.'':</strong></p>

          <p>Sure, here''s a simple story about a man buying groceries:</p>

          <p>John was walking through the aisles of his local grocery store, pushing
          his cart filled with food items he needed for the week. He picked up some
          fresh produce, including apples, bananas, and avocados. He also stocked
          up on grains, pasta, and canned goods.</p>

          <p>As he made his way to the checkout counter, John noticed that the line
          was quite long. He patiently waited his turn, taking a moment to look at
          the items in his cart and make sure he hadn''t forgotten anything. When
          it was finally his turn, John unloaded his groceries onto the conveyor belt
          and the cashier scanned each item.</p>

          '
        raw: 'I''m using a fresh install of the latest version of TextGen Webui, and
          I used Q4_K_M for both models.


          **New version with the prompt ''Write a simple story about a man buying
          groceries.'':**


          Sure, here''s a simple story about a man buying groceries:John was feeling
          rather peckish after a long day at work. He decided to head to the local
          grocery store to pick up some food for dinner. As he perused the aisles,
          he couldn''t help but feel overwhelmed by all the options available to him.After
          much deliberation, John finally settled on a selection of items that would
          make for a delicious meal. He grabbed a few fresh vegetables, some chicken,
          and a bottle of wine to wash it all down. As he stood in line to pay, he
          couldn''t help but feel a sense of satisfaction at the thought of enjoying
          a home-cooked meal after such a long day.


          **Old version with the prompt ''Write a simple story about a man buying
          groceries.'':**


          Sure, here''s a simple story about a man buying groceries:


          John was walking through the aisles of his local grocery store, pushing
          his cart filled with food items he needed for the week. He picked up some
          fresh produce, including apples, bananas, and avocados. He also stocked
          up on grains, pasta, and canned goods.


          As he made his way to the checkout counter, John noticed that the line was
          quite long. He patiently waited his turn, taking a moment to look at the
          items in his cart and make sure he hadn''t forgotten anything. When it was
          finally his turn, John unloaded his groceries onto the conveyor belt and
          the cashier scanned each item.'
        updatedAt: '2023-11-15T13:27:13.963Z'
      numEdits: 0
      reactions: []
    id: 6554c7314e5c205097df0abf
    type: comment
  author: HiroseKoichi
  content: 'I''m using a fresh install of the latest version of TextGen Webui, and
    I used Q4_K_M for both models.


    **New version with the prompt ''Write a simple story about a man buying groceries.'':**


    Sure, here''s a simple story about a man buying groceries:John was feeling rather
    peckish after a long day at work. He decided to head to the local grocery store
    to pick up some food for dinner. As he perused the aisles, he couldn''t help but
    feel overwhelmed by all the options available to him.After much deliberation,
    John finally settled on a selection of items that would make for a delicious meal.
    He grabbed a few fresh vegetables, some chicken, and a bottle of wine to wash
    it all down. As he stood in line to pay, he couldn''t help but feel a sense of
    satisfaction at the thought of enjoying a home-cooked meal after such a long day.


    **Old version with the prompt ''Write a simple story about a man buying groceries.'':**


    Sure, here''s a simple story about a man buying groceries:


    John was walking through the aisles of his local grocery store, pushing his cart
    filled with food items he needed for the week. He picked up some fresh produce,
    including apples, bananas, and avocados. He also stocked up on grains, pasta,
    and canned goods.


    As he made his way to the checkout counter, John noticed that the line was quite
    long. He patiently waited his turn, taking a moment to look at the items in his
    cart and make sure he hadn''t forgotten anything. When it was finally his turn,
    John unloaded his groceries onto the conveyor belt and the cashier scanned each
    item.'
  created_at: 2023-11-15 13:27:13+00:00
  edited: false
  hidden: false
  id: 6554c7314e5c205097df0abf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-15T13:38:07.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992491602897644
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I can''t recreate that issue with llama.cpp:</p>

          <h2 id="test1">Test1:</h2>

          <pre><code>&lt;|im_start|&gt;system

          You are an author who writes stories&lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          Write a simple story about a man buying groceries.&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          John was a middle-aged man with a wife and two children, living in the suburbs
          of New York City. Every Friday evening, it was his responsibility to go
          to the local supermarket and buy groceries for the weekend and the upcoming
          week.


          As John made his way through the aisles of the grocery store, he noticed
          how busy it was. He could see that there were many people shopping this
          time of day, as they probably had the same idea as him - wanting to get
          their errands done before the weekend began.


          John picked up his usual items: milk, bread, eggs, and juice for the kids.
          As he continued down each aisle, he made a mental list of everything else
          his family might need. He grabbed some fruits, vegetables, and meats for
          dinner, as well as snacks and cereals for breakfast. He also added some
          cleaning supplies to his cart, knowing that they were running low on laundry
          detergent and dishwasher pods.


          As John neared the checkout lines, he noticed that there was a long waiting
          time due to the busy hour. He patiently stood in line with his cart full
          of groceries while observing all the other shoppers around him. It struck
          him how different everyone''s shopping experience was - some people had
          only a few items, while others seemed to be restocking their entire pantry.


          Finally, it was John''s turn at the checkout counter. The cashier greeted
          him with a friendly smile and began scanning his items. As he paid for his
          groceries, he couldn''t help but feel a sense of accomplishment and satisfaction
          knowing that his family would have everything they needed for the upcoming
          week.&lt;|im_end|&gt; [end of text]

          </code></pre>

          <h2 id="test2">Test2:</h2>

          <pre><code>&lt;|im_start|&gt;system

          You are an author who writes stories&lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          Write a simple story about a man buying groceries.&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          John was a middle-aged man with a passion for cooking. Every weekend, he
          would visit the local grocery store to buy fresh ingredients for his elaborate
          meals. This particular Saturday morning, as he pushed his cart through the
          aisles filled with colorful produce and neatly stacked packages, John couldn''t
          help but feel excited about what culinary delights he would create that
          evening.


          First on his list was fresh vegetables. He picked up crisp lettuce leaves,
          juicy tomatoes, and tender bell peppers - all perfect for his famous salad.
          Next, he moved to the meat section where he carefully selected a succulent
          chicken breast, which would be cooked until it had just enough crunch on
          the outside while staying tender and juicy inside.


          As John continued shopping, he couldn''t ignore the sweet smell of baking
          that wafted from the bakery section. He decided to treat himself and picked
          up a warm loaf of crusty bread and a few buttery croissants for dessert.
          Finally, he made his way over to the spice aisle, adding herbs and spices
          that would give his dishes an extra burst of flavor - garlic powder, paprika,
          cayenne pepper, and rosemary.


          With bags loaded with ingredients, John headed home eager to start cooking.
          He knew that tonight''s dinner would be one to remember, not only because
          of the delicious food but also because it brought him joy in creating something
          special for those he loved. And so, as he began chopping vegetables and
          searing chicken, John smiled, knowing that every ingredient and step taken
          was an essential part of crafting a truly unforgettable meal.&lt;|im_end|&gt;
          [end of text]

          </code></pre>

          <h2 id="test3">Test3:</h2>

          <pre><code>&lt;|im_start|&gt;system

          You are an author who writes stories&lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          Write a simple story about a man buying groceries.&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          John was a busy man, always running from one thing to the next. One day
          he realized that his fridge was empty and decided it was time to buy groceries.
          He made a list of all the items he needed: milk, bread, eggs, fruits, vegetables,
          and some snacks for his kids.


          As John walked into the store, he noticed how crowded it was with people
          pushing their carts around. He took a deep breath and began collecting everything
          on his list. First, he found himself in front of the dairy section where
          he picked up two jugs of milk and four loaves of bread because you can never
          have too much bread at home!


          Next, he moved onto the produce aisle where colorful fruits and vegetables
          filled every corner. He carefully selected apples, bananas, carrots, broccoli,
          and lettuce for his family''s meals this week. Finally, he made sure to
          grab some cookies and chips for those late-night cravings that often sneak
          up on them.


          John loaded all these items into his cart and headed towards the checkout
          lines. There were so many people waiting in line, but John patiently waited
          his turn. Once it was finally his turn, he unloaded all of his groceries
          onto the conveyor belt and watched as they went through the barcode scanner.


          After paying for his purchases, John loaded everything into his car and
          drove back home with a full cart. He couldn''t wait to see the look on his
          kids'' faces when they saw all the delicious treats he had brought back!&lt;|im_end|&gt;
          [end of text]

          </code></pre>

          <p>Apparently in Dolphin, men are always called John :)</p>

          <h2 id="test4">Test4:</h2>

          <pre><code>&lt;|im_start|&gt;system

          You are an author who writes stories&lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          Write a long story about a woman buying shopping.&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          Once upon a time, in a quaint little village nestled between the rolling
          hills and the bustling city, there lived a woman named Margaret. Margaret
          was known for her generosity, kindness, and warm smile that could brighten
          anyone''s day. She owned a small but thriving flower shop at the heart of
          the village, where she created beautiful arrangements to celebrate life''s
          special moments.


          One morning, as the sun began to rise, casting a golden hue on the cobblestone
          streets, Margaret decided it was time to restock her beloved flower shop.
          With a list of fresh flowers and supplies in hand, she embarked on her shopping
          journey. As she approached the entrance of the village market, she couldn''t
          help but notice that something seemed different.


          The marketplace was alive with activity - vendors called out their specials,
          children played games, and the air smelled like freshly baked bread. Margaret
          felt a sense of excitement building inside her as she began to explore the
          market. She passed by colorful stalls filled with ripe fruits, plump vegetables,
          and succulent meats. The aromas wafted through the air, making her stomach
          growl with anticipation.


          As she continued walking, she stumbled upon a small, unassuming stall that
          caught her eye. It was lined with delicate fabrics in shades of ivory and
          gold, and displayed an array of handcrafted jewelry, each piece more exquisite
          than the last. Margaret''s heart skipped a beat as she approached the stall.
          She had always been fascinated by artistry, and these unique creations were
          no exception.


          The artist behind the stall was a middle-aged woman with salt-and-pepper
          hair and kind eyes that sparkled like the morning dew. Her name was Eleanor,
          and she greeted Margaret warmly. They exchanged pleasantries before delving
          into their shared love for creativity and beauty.


          As Margaret perused the intricate designs, her hands touched a delicate
          necklace of gold beads adorned with tiny opalescent flowers. It felt as
          though the piece had been crafted just for her. She couldn''t help but ask
          Eleanor about it, who revealed that it was an heirloom from her grandmother,
          passed down through generations.


          Margaret knew in her heart that she needed to have this necklace. With a
          smile, she asked Eleanor if it was available for purchase. The artist hesitated,
          but after a moment''s pause, she agreed to sell the precious piece to Margaret
          for a modest sum.


          Overwhelmed with joy and gratitude, Margaret placed the necklace around
          her neck, feeling its warmth and energy pulsate through her body. She knew
          that this treasure would always hold special meaning in her life.


          As their conversation continued, Eleanor confided that she was planning
          to close her stall and retire soon. She had been searching for someone who
          could carry on her grandmother''s legacy of artistry, and Margaret''s genuine
          love for the craft made her stand out as the perfect candidate.


          Margaret hesitated at first, unsure if she could handle such a responsibility.
          But upon seeing the pride in Eleanor''s eyes and the beauty of her creations,
          she couldn''t deny her own passion and drive to learn more about this captivating
          art form. With a smile, they shook hands, sealing their newfound partnership.


          The days turned into weeks, and soon enough, Margaret found herself immersed
          in the world of artistry. She spent long hours learning from Eleanor, absorbing
          every detail about the history, materials, and techniques used to create
          such stunning masterpieces. They worked side by side, forging not only an
          unbreakable bond but also a beautiful collection of unique pieces that captured
          their shared passion for beauty.


          As the season changed, so did Margaret''s life. Her flower shop began to
          incorporate Eleanor''s exquisite jewelry and accessories, becoming a haven
          for both the artistry of nature and the artistry of human hands. The village
          flourished as word spread about this one-of-a-kind establishment, drawing
          in customers from far and wide.


          Through it all, Margaret never forgot the simple shopping trip that changed
          her life forever. And as she stood in her now thriving business, surrounded
          by vibrant flowers and intricate artistry, she couldn''t help but smile
          at the serendipity of it all. Little did she know that this seemingly ordinary
          shopping excursion would be the catalyst for a new chapter in both her own
          life and the lives of countless others who found solace, beauty, and inspiration
          within those precious walls.&lt;|im_end|&gt; [end of text]

          </code></pre>

          '
        raw: 'I can''t recreate that issue with llama.cpp:


          ## Test1:

          ```

          <|im_start|>system

          You are an author who writes stories<|im_end|>

          <|im_start|>user

          Write a simple story about a man buying groceries.<|im_end|>

          <|im_start|>assistant

          John was a middle-aged man with a wife and two children, living in the suburbs
          of New York City. Every Friday evening, it was his responsibility to go
          to the local supermarket and buy groceries for the weekend and the upcoming
          week.


          As John made his way through the aisles of the grocery store, he noticed
          how busy it was. He could see that there were many people shopping this
          time of day, as they probably had the same idea as him - wanting to get
          their errands done before the weekend began.


          John picked up his usual items: milk, bread, eggs, and juice for the kids.
          As he continued down each aisle, he made a mental list of everything else
          his family might need. He grabbed some fruits, vegetables, and meats for
          dinner, as well as snacks and cereals for breakfast. He also added some
          cleaning supplies to his cart, knowing that they were running low on laundry
          detergent and dishwasher pods.


          As John neared the checkout lines, he noticed that there was a long waiting
          time due to the busy hour. He patiently stood in line with his cart full
          of groceries while observing all the other shoppers around him. It struck
          him how different everyone''s shopping experience was - some people had
          only a few items, while others seemed to be restocking their entire pantry.


          Finally, it was John''s turn at the checkout counter. The cashier greeted
          him with a friendly smile and began scanning his items. As he paid for his
          groceries, he couldn''t help but feel a sense of accomplishment and satisfaction
          knowing that his family would have everything they needed for the upcoming
          week.<|im_end|> [end of text]

          ```


          ## Test2:

          ```

          <|im_start|>system

          You are an author who writes stories<|im_end|>

          <|im_start|>user

          Write a simple story about a man buying groceries.<|im_end|>

          <|im_start|>assistant

          John was a middle-aged man with a passion for cooking. Every weekend, he
          would visit the local grocery store to buy fresh ingredients for his elaborate
          meals. This particular Saturday morning, as he pushed his cart through the
          aisles filled with colorful produce and neatly stacked packages, John couldn''t
          help but feel excited about what culinary delights he would create that
          evening.


          First on his list was fresh vegetables. He picked up crisp lettuce leaves,
          juicy tomatoes, and tender bell peppers - all perfect for his famous salad.
          Next, he moved to the meat section where he carefully selected a succulent
          chicken breast, which would be cooked until it had just enough crunch on
          the outside while staying tender and juicy inside.


          As John continued shopping, he couldn''t ignore the sweet smell of baking
          that wafted from the bakery section. He decided to treat himself and picked
          up a warm loaf of crusty bread and a few buttery croissants for dessert.
          Finally, he made his way over to the spice aisle, adding herbs and spices
          that would give his dishes an extra burst of flavor - garlic powder, paprika,
          cayenne pepper, and rosemary.


          With bags loaded with ingredients, John headed home eager to start cooking.
          He knew that tonight''s dinner would be one to remember, not only because
          of the delicious food but also because it brought him joy in creating something
          special for those he loved. And so, as he began chopping vegetables and
          searing chicken, John smiled, knowing that every ingredient and step taken
          was an essential part of crafting a truly unforgettable meal.<|im_end|>
          [end of text]

          ```


          ## Test3:

          ```

          <|im_start|>system

          You are an author who writes stories<|im_end|>

          <|im_start|>user

          Write a simple story about a man buying groceries.<|im_end|>

          <|im_start|>assistant

          John was a busy man, always running from one thing to the next. One day
          he realized that his fridge was empty and decided it was time to buy groceries.
          He made a list of all the items he needed: milk, bread, eggs, fruits, vegetables,
          and some snacks for his kids.


          As John walked into the store, he noticed how crowded it was with people
          pushing their carts around. He took a deep breath and began collecting everything
          on his list. First, he found himself in front of the dairy section where
          he picked up two jugs of milk and four loaves of bread because you can never
          have too much bread at home!


          Next, he moved onto the produce aisle where colorful fruits and vegetables
          filled every corner. He carefully selected apples, bananas, carrots, broccoli,
          and lettuce for his family''s meals this week. Finally, he made sure to
          grab some cookies and chips for those late-night cravings that often sneak
          up on them.


          John loaded all these items into his cart and headed towards the checkout
          lines. There were so many people waiting in line, but John patiently waited
          his turn. Once it was finally his turn, he unloaded all of his groceries
          onto the conveyor belt and watched as they went through the barcode scanner.


          After paying for his purchases, John loaded everything into his car and
          drove back home with a full cart. He couldn''t wait to see the look on his
          kids'' faces when they saw all the delicious treats he had brought back!<|im_end|>
          [end of text]

          ```


          Apparently in Dolphin, men are always called John :)


          ## Test4:

          ```

          <|im_start|>system

          You are an author who writes stories<|im_end|>

          <|im_start|>user

          Write a long story about a woman buying shopping.<|im_end|>

          <|im_start|>assistant

          Once upon a time, in a quaint little village nestled between the rolling
          hills and the bustling city, there lived a woman named Margaret. Margaret
          was known for her generosity, kindness, and warm smile that could brighten
          anyone''s day. She owned a small but thriving flower shop at the heart of
          the village, where she created beautiful arrangements to celebrate life''s
          special moments.


          One morning, as the sun began to rise, casting a golden hue on the cobblestone
          streets, Margaret decided it was time to restock her beloved flower shop.
          With a list of fresh flowers and supplies in hand, she embarked on her shopping
          journey. As she approached the entrance of the village market, she couldn''t
          help but notice that something seemed different.


          The marketplace was alive with activity - vendors called out their specials,
          children played games, and the air smelled like freshly baked bread. Margaret
          felt a sense of excitement building inside her as she began to explore the
          market. She passed by colorful stalls filled with ripe fruits, plump vegetables,
          and succulent meats. The aromas wafted through the air, making her stomach
          growl with anticipation.


          As she continued walking, she stumbled upon a small, unassuming stall that
          caught her eye. It was lined with delicate fabrics in shades of ivory and
          gold, and displayed an array of handcrafted jewelry, each piece more exquisite
          than the last. Margaret''s heart skipped a beat as she approached the stall.
          She had always been fascinated by artistry, and these unique creations were
          no exception.


          The artist behind the stall was a middle-aged woman with salt-and-pepper
          hair and kind eyes that sparkled like the morning dew. Her name was Eleanor,
          and she greeted Margaret warmly. They exchanged pleasantries before delving
          into their shared love for creativity and beauty.


          As Margaret perused the intricate designs, her hands touched a delicate
          necklace of gold beads adorned with tiny opalescent flowers. It felt as
          though the piece had been crafted just for her. She couldn''t help but ask
          Eleanor about it, who revealed that it was an heirloom from her grandmother,
          passed down through generations.


          Margaret knew in her heart that she needed to have this necklace. With a
          smile, she asked Eleanor if it was available for purchase. The artist hesitated,
          but after a moment''s pause, she agreed to sell the precious piece to Margaret
          for a modest sum.


          Overwhelmed with joy and gratitude, Margaret placed the necklace around
          her neck, feeling its warmth and energy pulsate through her body. She knew
          that this treasure would always hold special meaning in her life.


          As their conversation continued, Eleanor confided that she was planning
          to close her stall and retire soon. She had been searching for someone who
          could carry on her grandmother''s legacy of artistry, and Margaret''s genuine
          love for the craft made her stand out as the perfect candidate.


          Margaret hesitated at first, unsure if she could handle such a responsibility.
          But upon seeing the pride in Eleanor''s eyes and the beauty of her creations,
          she couldn''t deny her own passion and drive to learn more about this captivating
          art form. With a smile, they shook hands, sealing their newfound partnership.


          The days turned into weeks, and soon enough, Margaret found herself immersed
          in the world of artistry. She spent long hours learning from Eleanor, absorbing
          every detail about the history, materials, and techniques used to create
          such stunning masterpieces. They worked side by side, forging not only an
          unbreakable bond but also a beautiful collection of unique pieces that captured
          their shared passion for beauty.


          As the season changed, so did Margaret''s life. Her flower shop began to
          incorporate Eleanor''s exquisite jewelry and accessories, becoming a haven
          for both the artistry of nature and the artistry of human hands. The village
          flourished as word spread about this one-of-a-kind establishment, drawing
          in customers from far and wide.


          Through it all, Margaret never forgot the simple shopping trip that changed
          her life forever. And as she stood in her now thriving business, surrounded
          by vibrant flowers and intricate artistry, she couldn''t help but smile
          at the serendipity of it all. Little did she know that this seemingly ordinary
          shopping excursion would be the catalyst for a new chapter in both her own
          life and the lives of countless others who found solace, beauty, and inspiration
          within those precious walls.<|im_end|> [end of text]

          ```'
        updatedAt: '2023-11-15T13:40:47.096Z'
      numEdits: 1
      reactions: []
    id: 6554c9bf3b41871bda3450c5
    type: comment
  author: TheBloke
  content: 'I can''t recreate that issue with llama.cpp:


    ## Test1:

    ```

    <|im_start|>system

    You are an author who writes stories<|im_end|>

    <|im_start|>user

    Write a simple story about a man buying groceries.<|im_end|>

    <|im_start|>assistant

    John was a middle-aged man with a wife and two children, living in the suburbs
    of New York City. Every Friday evening, it was his responsibility to go to the
    local supermarket and buy groceries for the weekend and the upcoming week.


    As John made his way through the aisles of the grocery store, he noticed how busy
    it was. He could see that there were many people shopping this time of day, as
    they probably had the same idea as him - wanting to get their errands done before
    the weekend began.


    John picked up his usual items: milk, bread, eggs, and juice for the kids. As
    he continued down each aisle, he made a mental list of everything else his family
    might need. He grabbed some fruits, vegetables, and meats for dinner, as well
    as snacks and cereals for breakfast. He also added some cleaning supplies to his
    cart, knowing that they were running low on laundry detergent and dishwasher pods.


    As John neared the checkout lines, he noticed that there was a long waiting time
    due to the busy hour. He patiently stood in line with his cart full of groceries
    while observing all the other shoppers around him. It struck him how different
    everyone''s shopping experience was - some people had only a few items, while
    others seemed to be restocking their entire pantry.


    Finally, it was John''s turn at the checkout counter. The cashier greeted him
    with a friendly smile and began scanning his items. As he paid for his groceries,
    he couldn''t help but feel a sense of accomplishment and satisfaction knowing
    that his family would have everything they needed for the upcoming week.<|im_end|>
    [end of text]

    ```


    ## Test2:

    ```

    <|im_start|>system

    You are an author who writes stories<|im_end|>

    <|im_start|>user

    Write a simple story about a man buying groceries.<|im_end|>

    <|im_start|>assistant

    John was a middle-aged man with a passion for cooking. Every weekend, he would
    visit the local grocery store to buy fresh ingredients for his elaborate meals.
    This particular Saturday morning, as he pushed his cart through the aisles filled
    with colorful produce and neatly stacked packages, John couldn''t help but feel
    excited about what culinary delights he would create that evening.


    First on his list was fresh vegetables. He picked up crisp lettuce leaves, juicy
    tomatoes, and tender bell peppers - all perfect for his famous salad. Next, he
    moved to the meat section where he carefully selected a succulent chicken breast,
    which would be cooked until it had just enough crunch on the outside while staying
    tender and juicy inside.


    As John continued shopping, he couldn''t ignore the sweet smell of baking that
    wafted from the bakery section. He decided to treat himself and picked up a warm
    loaf of crusty bread and a few buttery croissants for dessert. Finally, he made
    his way over to the spice aisle, adding herbs and spices that would give his dishes
    an extra burst of flavor - garlic powder, paprika, cayenne pepper, and rosemary.


    With bags loaded with ingredients, John headed home eager to start cooking. He
    knew that tonight''s dinner would be one to remember, not only because of the
    delicious food but also because it brought him joy in creating something special
    for those he loved. And so, as he began chopping vegetables and searing chicken,
    John smiled, knowing that every ingredient and step taken was an essential part
    of crafting a truly unforgettable meal.<|im_end|> [end of text]

    ```


    ## Test3:

    ```

    <|im_start|>system

    You are an author who writes stories<|im_end|>

    <|im_start|>user

    Write a simple story about a man buying groceries.<|im_end|>

    <|im_start|>assistant

    John was a busy man, always running from one thing to the next. One day he realized
    that his fridge was empty and decided it was time to buy groceries. He made a
    list of all the items he needed: milk, bread, eggs, fruits, vegetables, and some
    snacks for his kids.


    As John walked into the store, he noticed how crowded it was with people pushing
    their carts around. He took a deep breath and began collecting everything on his
    list. First, he found himself in front of the dairy section where he picked up
    two jugs of milk and four loaves of bread because you can never have too much
    bread at home!


    Next, he moved onto the produce aisle where colorful fruits and vegetables filled
    every corner. He carefully selected apples, bananas, carrots, broccoli, and lettuce
    for his family''s meals this week. Finally, he made sure to grab some cookies
    and chips for those late-night cravings that often sneak up on them.


    John loaded all these items into his cart and headed towards the checkout lines.
    There were so many people waiting in line, but John patiently waited his turn.
    Once it was finally his turn, he unloaded all of his groceries onto the conveyor
    belt and watched as they went through the barcode scanner.


    After paying for his purchases, John loaded everything into his car and drove
    back home with a full cart. He couldn''t wait to see the look on his kids'' faces
    when they saw all the delicious treats he had brought back!<|im_end|> [end of
    text]

    ```


    Apparently in Dolphin, men are always called John :)


    ## Test4:

    ```

    <|im_start|>system

    You are an author who writes stories<|im_end|>

    <|im_start|>user

    Write a long story about a woman buying shopping.<|im_end|>

    <|im_start|>assistant

    Once upon a time, in a quaint little village nestled between the rolling hills
    and the bustling city, there lived a woman named Margaret. Margaret was known
    for her generosity, kindness, and warm smile that could brighten anyone''s day.
    She owned a small but thriving flower shop at the heart of the village, where
    she created beautiful arrangements to celebrate life''s special moments.


    One morning, as the sun began to rise, casting a golden hue on the cobblestone
    streets, Margaret decided it was time to restock her beloved flower shop. With
    a list of fresh flowers and supplies in hand, she embarked on her shopping journey.
    As she approached the entrance of the village market, she couldn''t help but notice
    that something seemed different.


    The marketplace was alive with activity - vendors called out their specials, children
    played games, and the air smelled like freshly baked bread. Margaret felt a sense
    of excitement building inside her as she began to explore the market. She passed
    by colorful stalls filled with ripe fruits, plump vegetables, and succulent meats.
    The aromas wafted through the air, making her stomach growl with anticipation.


    As she continued walking, she stumbled upon a small, unassuming stall that caught
    her eye. It was lined with delicate fabrics in shades of ivory and gold, and displayed
    an array of handcrafted jewelry, each piece more exquisite than the last. Margaret''s
    heart skipped a beat as she approached the stall. She had always been fascinated
    by artistry, and these unique creations were no exception.


    The artist behind the stall was a middle-aged woman with salt-and-pepper hair
    and kind eyes that sparkled like the morning dew. Her name was Eleanor, and she
    greeted Margaret warmly. They exchanged pleasantries before delving into their
    shared love for creativity and beauty.


    As Margaret perused the intricate designs, her hands touched a delicate necklace
    of gold beads adorned with tiny opalescent flowers. It felt as though the piece
    had been crafted just for her. She couldn''t help but ask Eleanor about it, who
    revealed that it was an heirloom from her grandmother, passed down through generations.


    Margaret knew in her heart that she needed to have this necklace. With a smile,
    she asked Eleanor if it was available for purchase. The artist hesitated, but
    after a moment''s pause, she agreed to sell the precious piece to Margaret for
    a modest sum.


    Overwhelmed with joy and gratitude, Margaret placed the necklace around her neck,
    feeling its warmth and energy pulsate through her body. She knew that this treasure
    would always hold special meaning in her life.


    As their conversation continued, Eleanor confided that she was planning to close
    her stall and retire soon. She had been searching for someone who could carry
    on her grandmother''s legacy of artistry, and Margaret''s genuine love for the
    craft made her stand out as the perfect candidate.


    Margaret hesitated at first, unsure if she could handle such a responsibility.
    But upon seeing the pride in Eleanor''s eyes and the beauty of her creations,
    she couldn''t deny her own passion and drive to learn more about this captivating
    art form. With a smile, they shook hands, sealing their newfound partnership.


    The days turned into weeks, and soon enough, Margaret found herself immersed in
    the world of artistry. She spent long hours learning from Eleanor, absorbing every
    detail about the history, materials, and techniques used to create such stunning
    masterpieces. They worked side by side, forging not only an unbreakable bond but
    also a beautiful collection of unique pieces that captured their shared passion
    for beauty.


    As the season changed, so did Margaret''s life. Her flower shop began to incorporate
    Eleanor''s exquisite jewelry and accessories, becoming a haven for both the artistry
    of nature and the artistry of human hands. The village flourished as word spread
    about this one-of-a-kind establishment, drawing in customers from far and wide.


    Through it all, Margaret never forgot the simple shopping trip that changed her
    life forever. And as she stood in her now thriving business, surrounded by vibrant
    flowers and intricate artistry, she couldn''t help but smile at the serendipity
    of it all. Little did she know that this seemingly ordinary shopping excursion
    would be the catalyst for a new chapter in both her own life and the lives of
    countless others who found solace, beauty, and inspiration within those precious
    walls.<|im_end|> [end of text]

    ```'
  created_at: 2023-11-15 13:38:07+00:00
  edited: true
  hidden: false
  id: 6554c9bf3b41871bda3450c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da279c726101edbbf98bafc289bad25d.svg
      fullname: Marijn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marinl
      type: user
    createdAt: '2023-11-15T14:39:11.000Z'
    data:
      edited: false
      editors:
      - marinl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9725269079208374
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da279c726101edbbf98bafc289bad25d.svg
          fullname: Marijn
          isHf: false
          isPro: false
          name: marinl
          type: user
        html: '<blockquote>

          <pre><code>Apparently in Dolphin, men are always called John :)

          </code></pre>

          </blockquote>

          <p>Just like I always see Lilly being used for women :). Do you know why
          this is?</p>

          '
        raw: '> ```

          > Apparently in Dolphin, men are always called John :)

          > ```


          Just like I always see Lilly being used for women :). Do you know why this
          is?

          '
        updatedAt: '2023-11-15T14:39:11.112Z'
      numEdits: 0
      reactions: []
    id: 6554d80f1bf3e0c23d32a23a
    type: comment
  author: marinl
  content: '> ```

    > Apparently in Dolphin, men are always called John :)

    > ```


    Just like I always see Lilly being used for women :). Do you know why this is?

    '
  created_at: 2023-11-15 14:39:11+00:00
  edited: false
  hidden: false
  id: 6554d80f1bf3e0c23d32a23a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-15T15:27:49.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8857406377792358
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HiroseKoichi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HiroseKoichi\"\
          >@<span class=\"underline\">HiroseKoichi</span></a></span>\n\n\t</span></span>\
          \ </p>\n<blockquote>\n<p>I'm using a fresh install of the latest version\
          \ of TextGen Webui, and I used Q4_K_M for both models.</p>\n</blockquote>\n\
          <p>It might be doing something like setting logit bias for the BOS token.\
          \ You can try using <code>2</code> as the BOS token id instead and see how\
          \ that works. You will need to have the llama.cpp repo checked out, be in\
          \ that directory and have at least the <code>numpy</code> Python dependency\
          \ installed. Then run:</p>\n<pre><code>python gguf-py/scripts/gguf-set-metadata.py\
          \ /path/whatever.gguf tokenizer.ggml.bos_token_id 2\n</code></pre>\n<p>Or\
          \ maybe there's some way to control that behavior in TextGen Webui. I'm\
          \ not familiar with it.</p>\n"
        raw: "@HiroseKoichi \n\n> I'm using a fresh install of the latest version\
          \ of TextGen Webui, and I used Q4_K_M for both models.\n\nIt might be doing\
          \ something like setting logit bias for the BOS token. You can try using\
          \ `2` as the BOS token id instead and see how that works. You will need\
          \ to have the llama.cpp repo checked out, be in that directory and have\
          \ at least the `numpy` Python dependency installed. Then run:\n\n    python\
          \ gguf-py/scripts/gguf-set-metadata.py /path/whatever.gguf tokenizer.ggml.bos_token_id\
          \ 2\n\nOr maybe there's some way to control that behavior in TextGen Webui.\
          \ I'm not familiar with it."
        updatedAt: '2023-11-15T15:27:49.985Z'
      numEdits: 0
      reactions: []
    id: 6554e37504eb93eea5886201
    type: comment
  author: KerfuffleV2
  content: "@HiroseKoichi \n\n> I'm using a fresh install of the latest version of\
    \ TextGen Webui, and I used Q4_K_M for both models.\n\nIt might be doing something\
    \ like setting logit bias for the BOS token. You can try using `2` as the BOS\
    \ token id instead and see how that works. You will need to have the llama.cpp\
    \ repo checked out, be in that directory and have at least the `numpy` Python\
    \ dependency installed. Then run:\n\n    python gguf-py/scripts/gguf-set-metadata.py\
    \ /path/whatever.gguf tokenizer.ggml.bos_token_id 2\n\nOr maybe there's some way\
    \ to control that behavior in TextGen Webui. I'm not familiar with it."
  created_at: 2023-11-15 15:27:49+00:00
  edited: false
  hidden: false
  id: 6554e37504eb93eea5886201
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-15T16:35:49.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7368502616882324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>By the way, if anyone wants to download a pre-updated GGUF, ie before
          I set BOS to newline, then you can do that using Hugging Face''s revision
          system (HF is Git-based under the hood)</p>

          <p>Here for example is the link to manually download the original Q4_K_M:<br><a
          href="https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/blob/4d998f91545661e6813083b3d7cc1df2e0bd5eac/dolphin-2_2-yi-34b.Q4_K_M.gguf">https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/blob/4d998f91545661e6813083b3d7cc1df2e0bd5eac/dolphin-2_2-yi-34b.Q4_K_M.gguf</a></p>

          <p>It''s a lot quicker just to re-set the metadata on your already-downloaded
          GGUF like Kerfuffle just described. But if anyone isn''t comfortable doing
          that for any reason, or is doing a first-time download, then the original
          files are also still available.</p>

          <p>(Not that I''ve personally seen any reason to make any change - the updated
          files seem to work great for me, with llama.cpp.)</p>

          '
        raw: 'By the way, if anyone wants to download a pre-updated GGUF, ie before
          I set BOS to newline, then you can do that using Hugging Face''s revision
          system (HF is Git-based under the hood)


          Here for example is the link to manually download the original Q4_K_M:

          https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/blob/4d998f91545661e6813083b3d7cc1df2e0bd5eac/dolphin-2_2-yi-34b.Q4_K_M.gguf


          It''s a lot quicker just to re-set the metadata on your already-downloaded
          GGUF like Kerfuffle just described. But if anyone isn''t comfortable doing
          that for any reason, or is doing a first-time download, then the original
          files are also still available.


          (Not that I''ve personally seen any reason to make any change - the updated
          files seem to work great for me, with llama.cpp.)'
        updatedAt: '2023-11-15T16:39:04.094Z'
      numEdits: 3
      reactions: []
    id: 6554f365e2a7cd9b84d86a64
    type: comment
  author: TheBloke
  content: 'By the way, if anyone wants to download a pre-updated GGUF, ie before
    I set BOS to newline, then you can do that using Hugging Face''s revision system
    (HF is Git-based under the hood)


    Here for example is the link to manually download the original Q4_K_M:

    https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/blob/4d998f91545661e6813083b3d7cc1df2e0bd5eac/dolphin-2_2-yi-34b.Q4_K_M.gguf


    It''s a lot quicker just to re-set the metadata on your already-downloaded GGUF
    like Kerfuffle just described. But if anyone isn''t comfortable doing that for
    any reason, or is doing a first-time download, then the original files are also
    still available.


    (Not that I''ve personally seen any reason to make any change - the updated files
    seem to work great for me, with llama.cpp.)'
  created_at: 2023-11-15 16:35:49+00:00
  edited: true
  hidden: false
  id: 6554f365e2a7cd9b84d86a64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-15T16:38:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.978995680809021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>That patch started out as making inference respect the <code>add_bos_token</code>
          metadata, but the other day I found there was a case where metadata didn''t
          get added <em>during conversion</em>. So that fix is also included - that''s
          what I''m talking about.</p>

          <p>The reason to worry about it now is all the models created <em>without</em>
          that won''t have the correct metadata. So for it to actually get respected
          correctly, you''ll not only need to generate the new models (with that metadata)
          but every user will also have to download the models again to have a version
          with the metadata.</p>

          <p>So you''re correct that it has no effect on inference right now but ideally
          people will already have models that include the metadata so when it does
          get merged they can get the benefits. Hopefully that makes sense.</p>

          </blockquote>

          <p>OK understood, thanks. I''ll do them soon with the PR.</p>

          '
        raw: "\n> That patch started out as making inference respect the `add_bos_token`\
          \ metadata, but the other day I found there was a case where metadata didn't\
          \ get added _during conversion_. So that fix is also included - that's what\
          \ I'm talking about.\n> \n> The reason to worry about it now is all the\
          \ models created _without_ that won't have the correct metadata. So for\
          \ it to actually get respected correctly, you'll not only need to generate\
          \ the new models (with that metadata) but every user will also have to download\
          \ the models again to have a version with the metadata.\n> \n> So you're\
          \ correct that it has no effect on inference right now but ideally people\
          \ will already have models that include the metadata so when it does get\
          \ merged they can get the benefits. Hopefully that makes sense.\n\nOK understood,\
          \ thanks. I'll do them soon with the PR."
        updatedAt: '2023-11-15T16:38:24.871Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KerfuffleV2
    id: 6554f4004f361968f0de824d
    type: comment
  author: TheBloke
  content: "\n> That patch started out as making inference respect the `add_bos_token`\
    \ metadata, but the other day I found there was a case where metadata didn't get\
    \ added _during conversion_. So that fix is also included - that's what I'm talking\
    \ about.\n> \n> The reason to worry about it now is all the models created _without_\
    \ that won't have the correct metadata. So for it to actually get respected correctly,\
    \ you'll not only need to generate the new models (with that metadata) but every\
    \ user will also have to download the models again to have a version with the\
    \ metadata.\n> \n> So you're correct that it has no effect on inference right\
    \ now but ideally people will already have models that include the metadata so\
    \ when it does get merged they can get the benefits. Hopefully that makes sense.\n\
    \nOK understood, thanks. I'll do them soon with the PR."
  created_at: 2023-11-15 16:38:24+00:00
  edited: false
  hidden: false
  id: 6554f4004f361968f0de824d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
      fullname: Hirose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HiroseKoichi
      type: user
    createdAt: '2023-11-15T18:04:06.000Z'
    data:
      edited: false
      editors:
      - HiroseKoichi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
          fullname: Hirose
          isHf: false
          isPro: false
          name: HiroseKoichi
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;HiroseKoichi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HiroseKoichi\"\
          >@<span class=\"underline\">HiroseKoichi</span></a></span>\n\n\t</span></span>\
          \ </p>\n<blockquote>\n<p>I'm using a fresh install of the latest version\
          \ of TextGen Webui, and I used Q4_K_M for both models.</p>\n</blockquote>\n\
          <p>It might be doing something like setting logit bias for the BOS token.\
          \ You can try using <code>2</code> as the BOS token id instead and see how\
          \ that works. You will need to have the llama.cpp repo checked out, be in\
          \ that directory and have at least the <code>numpy</code> Python dependency\
          \ installed. Then run:</p>\n<pre><code>python gguf-py/scripts/gguf-set-metadata.py\
          \ /path/whatever.gguf tokenizer.ggml.bos_token_id 2\n</code></pre>\n<p>Or\
          \ maybe there's some way to control that behavior in TextGen Webui. I'm\
          \ not familiar with it.</p>\n</blockquote>\n<p>This does fix the issue for\
          \ TextGen WebUI. I don't know if this is true or not, but I tried adding\
          \ \\n to the custom stopping strings in TextGen WebUI, and the new version\
          \ just kept generating while the old version stopped at a newline like it\
          \ should, so I think TextGen WebUI automatically bans the BOS token if add_bos_token\
          \ is false in the metadata. I also tested the new version with llama.cpp,\
          \ and the issue wasn't present, so it's definitely something to do with\
          \ TextGen WebUI and not the model itself.</p>\n"
        raw: "> @HiroseKoichi \n> \n> > I'm using a fresh install of the latest version\
          \ of TextGen Webui, and I used Q4_K_M for both models.\n> \n> It might be\
          \ doing something like setting logit bias for the BOS token. You can try\
          \ using `2` as the BOS token id instead and see how that works. You will\
          \ need to have the llama.cpp repo checked out, be in that directory and\
          \ have at least the `numpy` Python dependency installed. Then run:\n> \n\
          >     python gguf-py/scripts/gguf-set-metadata.py /path/whatever.gguf tokenizer.ggml.bos_token_id\
          \ 2\n> \n> Or maybe there's some way to control that behavior in TextGen\
          \ Webui. I'm not familiar with it.\n\nThis does fix the issue for TextGen\
          \ WebUI. I don't know if this is true or not, but I tried adding \\n to\
          \ the custom stopping strings in TextGen WebUI, and the new version just\
          \ kept generating while the old version stopped at a newline like it should,\
          \ so I think TextGen WebUI automatically bans the BOS token if add_bos_token\
          \ is false in the metadata. I also tested the new version with llama.cpp,\
          \ and the issue wasn't present, so it's definitely something to do with\
          \ TextGen WebUI and not the model itself."
        updatedAt: '2023-11-15T18:04:06.281Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - KerfuffleV2
    id: 65550816598cbe7bfda939e8
    type: comment
  author: HiroseKoichi
  content: "> @HiroseKoichi \n> \n> > I'm using a fresh install of the latest version\
    \ of TextGen Webui, and I used Q4_K_M for both models.\n> \n> It might be doing\
    \ something like setting logit bias for the BOS token. You can try using `2` as\
    \ the BOS token id instead and see how that works. You will need to have the llama.cpp\
    \ repo checked out, be in that directory and have at least the `numpy` Python\
    \ dependency installed. Then run:\n> \n>     python gguf-py/scripts/gguf-set-metadata.py\
    \ /path/whatever.gguf tokenizer.ggml.bos_token_id 2\n> \n> Or maybe there's some\
    \ way to control that behavior in TextGen Webui. I'm not familiar with it.\n\n\
    This does fix the issue for TextGen WebUI. I don't know if this is true or not,\
    \ but I tried adding \\n to the custom stopping strings in TextGen WebUI, and\
    \ the new version just kept generating while the old version stopped at a newline\
    \ like it should, so I think TextGen WebUI automatically bans the BOS token if\
    \ add_bos_token is false in the metadata. I also tested the new version with llama.cpp,\
    \ and the issue wasn't present, so it's definitely something to do with TextGen\
    \ WebUI and not the model itself."
  created_at: 2023-11-15 18:04:06+00:00
  edited: false
  hidden: false
  id: 65550816598cbe7bfda939e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-11-15T19:41:14.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9135084748268127
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> do I need to\
          \ change anything in my config json?</p>\n"
        raw: '@TheBloke do I need to change anything in my config json?'
        updatedAt: '2023-11-15T19:41:14.746Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - pabloce
    id: 65551eda7446daf1ce1c7676
    type: comment
  author: ehartford
  content: '@TheBloke do I need to change anything in my config json?'
  created_at: 2023-11-15 19:41:14+00:00
  edited: false
  hidden: false
  id: 65551eda7446daf1ce1c7676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-15T21:29:09.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9714157581329346
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ do I need to change anything in my config json?</p>\n</blockquote>\n<p>Nope,\
          \ you're fine as far as I know. The issues described in this thread relate\
          \ to llama.cpp not respecting the config set in the JSON files.  I've had\
          \ no reports of issues with the GPTQ or AWQs which use the JSON files directly.</p>\n"
        raw: '> @TheBloke do I need to change anything in my config json?


          Nope, you''re fine as far as I know. The issues described in this thread
          relate to llama.cpp not respecting the config set in the JSON files.  I''ve
          had no reports of issues with the GPTQ or AWQs which use the JSON files
          directly.'
        updatedAt: '2023-11-15T21:29:09.656Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - pabloce
    id: 65553825598cbe7bfdb196e8
    type: comment
  author: TheBloke
  content: '> @TheBloke do I need to change anything in my config json?


    Nope, you''re fine as far as I know. The issues described in this thread relate
    to llama.cpp not respecting the config set in the JSON files.  I''ve had no reports
    of issues with the GPTQ or AWQs which use the JSON files directly.'
  created_at: 2023-11-15 21:29:09+00:00
  edited: false
  hidden: false
  id: 65553825598cbe7bfdb196e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
      fullname: MC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mclassHF2023
      type: user
    createdAt: '2023-11-18T03:02:13.000Z'
    data:
      edited: true
      editors:
      - mclassHF2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.962848424911499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
          fullname: MC
          isHf: false
          isPro: false
          name: mclassHF2023
          type: user
        html: '<p>Could this be an issue that people only run into on windows, maybe?
          I also have the problem that it omits any newline characters.<br>Is there
          anything I can do with prompt or a grammar file? (sorry, I''m a bit of a
          newbie with this stuff...)<br>Edit: I just tried it on runpod io and there
          it''s definitely a linux environment and I run into exactly the same issue:
          all newlines are dropped.</p>

          '
        raw: 'Could this be an issue that people only run into on windows, maybe?
          I also have the problem that it omits any newline characters.

          Is there anything I can do with prompt or a grammar file? (sorry, I''m a
          bit of a newbie with this stuff...)

          Edit: I just tried it on runpod io and there it''s definitely a linux environment
          and I run into exactly the same issue: all newlines are dropped.'
        updatedAt: '2023-11-18T04:03:57.555Z'
      numEdits: 1
      reactions: []
    id: 65582935c11dee7f7e0c381e
    type: comment
  author: mclassHF2023
  content: 'Could this be an issue that people only run into on windows, maybe? I
    also have the problem that it omits any newline characters.

    Is there anything I can do with prompt or a grammar file? (sorry, I''m a bit of
    a newbie with this stuff...)

    Edit: I just tried it on runpod io and there it''s definitely a linux environment
    and I run into exactly the same issue: all newlines are dropped.'
  created_at: 2023-11-18 03:02:13+00:00
  edited: true
  hidden: false
  id: 65582935c11dee7f7e0c381e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
      fullname: Hirose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HiroseKoichi
      type: user
    createdAt: '2023-11-18T06:40:21.000Z'
    data:
      edited: true
      editors:
      - HiroseKoichi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9346188306808472
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
          fullname: Hirose
          isHf: false
          isPro: false
          name: HiroseKoichi
          type: user
        html: '<blockquote>

          <p>Could this be an issue that people only run into on windows, maybe? I
          also have the problem that it omits any newline characters.<br>Is there
          anything I can do with prompt or a grammar file? (sorry, I''m a bit of a
          newbie with this stuff...)<br>Edit: I just tried it on runpod io and there
          it''s definitely a linux environment and I run into exactly the same issue:
          all newlines are dropped.</p>

          </blockquote>

          <p>If you''re using TextGen WebUI, then you''ll either need to download
          the first version that was uploaded or manually change the BOS token ID
          using the instructions above. Here''s a link to the previous versions: <a
          href="https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/tree/20692fcd351cb6046ce1aed26d410a5430a28206">https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/tree/20692fcd351cb6046ce1aed26d410a5430a28206</a><br>If
          you want to know how to get there manually for future reference, it''s ''files
          and versions'' -&gt; ''history'' -&gt; select the old branch -&gt; ''browse
          files'' -&gt; download the quantization you want to use.</p>

          <p>Edit: The models got updated, so just re-download the latest version.</p>

          '
        raw: '> Could this be an issue that people only run into on windows, maybe?
          I also have the problem that it omits any newline characters.

          > Is there anything I can do with prompt or a grammar file? (sorry, I''m
          a bit of a newbie with this stuff...)

          > Edit: I just tried it on runpod io and there it''s definitely a linux
          environment and I run into exactly the same issue: all newlines are dropped.


          If you''re using TextGen WebUI, then you''ll either need to download the
          first version that was uploaded or manually change the BOS token ID using
          the instructions above. Here''s a link to the previous versions: https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/tree/20692fcd351cb6046ce1aed26d410a5430a28206

          If you want to know how to get there manually for future reference, it''s
          ''files and versions'' -> ''history'' -> select the old branch -> ''browse
          files'' -> download the quantization you want to use.


          Edit: The models got updated, so just re-download the latest version.'
        updatedAt: '2023-11-18T17:58:11.555Z'
      numEdits: 1
      reactions: []
    id: 65585c555507a507727aaf24
    type: comment
  author: HiroseKoichi
  content: '> Could this be an issue that people only run into on windows, maybe?
    I also have the problem that it omits any newline characters.

    > Is there anything I can do with prompt or a grammar file? (sorry, I''m a bit
    of a newbie with this stuff...)

    > Edit: I just tried it on runpod io and there it''s definitely a linux environment
    and I run into exactly the same issue: all newlines are dropped.


    If you''re using TextGen WebUI, then you''ll either need to download the first
    version that was uploaded or manually change the BOS token ID using the instructions
    above. Here''s a link to the previous versions: https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-GGUF/tree/20692fcd351cb6046ce1aed26d410a5430a28206

    If you want to know how to get there manually for future reference, it''s ''files
    and versions'' -> ''history'' -> select the old branch -> ''browse files'' ->
    download the quantization you want to use.


    Edit: The models got updated, so just re-download the latest version.'
  created_at: 2023-11-18 06:40:21+00:00
  edited: true
  hidden: false
  id: 65585c555507a507727aaf24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-18T07:44:36.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9498510956764221
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<p>By the way, the changes to respect the <code>add_bos_token</code>
          metadata got merged into llama.cpp a couple days ago. So as long as you
          have a <code>.gguf</code> with the metadata in it and are using a recent
          enough llama.cpp version, you shouldn''t have to worry about this stuff
          anymore. </p>

          '
        raw: 'By the way, the changes to respect the `add_bos_token` metadata got
          merged into llama.cpp a couple days ago. So as long as you have a `.gguf`
          with the metadata in it and are using a recent enough llama.cpp version,
          you shouldn''t have to worry about this stuff anymore. '
        updatedAt: '2023-11-18T07:44:36.808Z'
      numEdits: 0
      reactions: []
    id: 65586b64b1b102df8cf643c5
    type: comment
  author: KerfuffleV2
  content: 'By the way, the changes to respect the `add_bos_token` metadata got merged
    into llama.cpp a couple days ago. So as long as you have a `.gguf` with the metadata
    in it and are using a recent enough llama.cpp version, you shouldn''t have to
    worry about this stuff anymore. '
  created_at: 2023-11-18 07:44:36+00:00
  edited: false
  hidden: false
  id: 65586b64b1b102df8cf643c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-18T10:39:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9453116655349731
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks Kerfuffle. I''ve just started a re-quant of this model and
          Capybara 34B, such that they''ll be re-done with the latest llama.cpp code
          and therefore should have all the right metadata.</p>

          '
        raw: Thanks Kerfuffle. I've just started a re-quant of this model and Capybara
          34B, such that they'll be re-done with the latest llama.cpp code and therefore
          should have all the right metadata.
        updatedAt: '2023-11-18T10:39:21.384Z'
      numEdits: 0
      reactions: []
    id: 655894592ac07d8c00598c3f
    type: comment
  author: TheBloke
  content: Thanks Kerfuffle. I've just started a re-quant of this model and Capybara
    34B, such that they'll be re-done with the latest llama.cpp code and therefore
    should have all the right metadata.
  created_at: 2023-11-18 10:39:21+00:00
  edited: false
  hidden: false
  id: 655894592ac07d8c00598c3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-18T12:25:24.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9904490113258362
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<p>I was wondering what the new uploads for those were. You also shouldn''t
          need to mess around with the BOS token id anymore. (Also, sorry that my
          suggestion of using newline as BOS seems to have caused problems for some
          people that aren''t using llama.cpp directly.)</p>

          '
        raw: I was wondering what the new uploads for those were. You also shouldn't
          need to mess around with the BOS token id anymore. (Also, sorry that my
          suggestion of using newline as BOS seems to have caused problems for some
          people that aren't using llama.cpp directly.)
        updatedAt: '2023-11-18T12:25:24.254Z'
      numEdits: 0
      reactions: []
    id: 6558ad347d187b097ec2c93f
    type: comment
  author: KerfuffleV2
  content: I was wondering what the new uploads for those were. You also shouldn't
    need to mess around with the BOS token id anymore. (Also, sorry that my suggestion
    of using newline as BOS seems to have caused problems for some people that aren't
    using llama.cpp directly.)
  created_at: 2023-11-18 12:25:24+00:00
  edited: false
  hidden: false
  id: 6558ad347d187b097ec2c93f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9aa80eca3b1f220afecfa8d9a9061156.svg
      fullname: p
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pekoragg
      type: user
    createdAt: '2023-11-27T16:58:30.000Z'
    data:
      edited: false
      editors:
      - pekoragg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8572405576705933
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9aa80eca3b1f220afecfa8d9a9061156.svg
          fullname: p
          isHf: false
          isPro: false
          name: pekoragg
          type: user
        html: '<p>How can I fix this if I  use <a rel="nofollow" href="https://github.com/turboderp/exllamav2">https://github.com/turboderp/exllamav2</a>
          to inference?  Really cant follow.  </p>

          '
        raw: 'How can I fix this if I  use https://github.com/turboderp/exllamav2
          to inference?  Really cant follow.  '
        updatedAt: '2023-11-27T16:58:30.263Z'
      numEdits: 0
      reactions: []
    id: 6564cab672e741b4ea9230c4
    type: comment
  author: pekoragg
  content: 'How can I fix this if I  use https://github.com/turboderp/exllamav2 to
    inference?  Really cant follow.  '
  created_at: 2023-11-27 16:58:30+00:00
  edited: false
  hidden: false
  id: 6564cab672e741b4ea9230c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-11-27T17:04:53.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.68355792760849
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I''ll retrain it on the new llama compatible base</p>

          '
        raw: I'll retrain it on the new llama compatible base
        updatedAt: '2023-11-27T17:04:53.160Z'
      numEdits: 0
      reactions: []
    id: 6564cc353f025ef9f118951e
    type: comment
  author: ehartford
  content: I'll retrain it on the new llama compatible base
  created_at: 2023-11-27 17:04:53+00:00
  edited: false
  hidden: false
  id: 6564cc353f025ef9f118951e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9aa80eca3b1f220afecfa8d9a9061156.svg
      fullname: p
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pekoragg
      type: user
    createdAt: '2023-11-27T17:22:28.000Z'
    data:
      edited: true
      editors:
      - pekoragg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46899664402008057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9aa80eca3b1f220afecfa8d9a9061156.svg
          fullname: p
          isHf: false
          isPro: false
          name: pekoragg
          type: user
        html: "<p>I just simply made a new class but replaced &lt;|im_end|&gt; with\
          \ &lt; /s&gt; (no space). Now  it works fine</p>\n<pre><code>class PromptFormat_capychatml(PromptFormat):\n\
          \n    description = \"ChatML format, as used by e.g. (Mistral)Orca\"\n\n\
          \    def __init__(self):\n        super().__init__()\n        pass\n\n \
          \   def default_system_prompt(self):\n        return \\\n            f\"\
          \"\"You are {self.botname}, a large language model. Answer as concisely\
          \ as possible.\"\"\"\n\n    def first_prompt(self):\n        return \\\n\
          \            \"\"\"&lt;|im_start|&gt;system\\n\"\"\" + \\\n            \"\
          \"\"&lt;|system_prompt|&gt;\\n\"\"\" + \\\n            \"\"\"&lt;|im_end|&gt;\\\
          n\"\"\" + \\\n            \"\"\"&lt;|im_start|&gt;user\\n\"\"\" + \\\n \
          \           \"\"\"&lt;|user_prompt|&gt;&lt;|im_end|&gt;\\n\"\"\" + \\\n\
          \            \"\"\"&lt;|im_start|&gt;assistant\\n\"\"\" \n\n    def subs_prompt(self):\n\
          \        return \\\n            \"\"\"&lt;|im_end|&gt;\\n\"\"\" + \\\n \
          \           \"\"\"&lt;|im_start|&gt;user\\n\"\"\" + \\\n            \"\"\
          \"&lt;|user_prompt|&gt;&lt;|im_end|&gt;\\n\"\"\" + \\\n            \"\"\"\
          &lt;|im_start|&gt;assistant\\n\"\"\" \n\n    def stop_conditions(self, tokenizer):\n\
          \        return \\\n            [tokenizer.eos_token_id,\n             \"\
          \"\"&lt;/s&gt;\"\"\"]\n\n    def encoding_options(self):\n        return\
          \ False, False, True\n\n    def print_extra_newline(self):\n        return\
          \ True\n</code></pre>\n"
        raw: "I just simply made a new class but replaced <|im_end|> with < /s> (no\
          \ space). Now  it works fine\n```\nclass PromptFormat_capychatml(PromptFormat):\n\
          \n    description = \"ChatML format, as used by e.g. (Mistral)Orca\"\n\n\
          \    def __init__(self):\n        super().__init__()\n        pass\n\n \
          \   def default_system_prompt(self):\n        return \\\n            f\"\
          \"\"You are {self.botname}, a large language model. Answer as concisely\
          \ as possible.\"\"\"\n\n    def first_prompt(self):\n        return \\\n\
          \            \"\"\"<|im_start|>system\\n\"\"\" + \\\n            \"\"\"\
          <|system_prompt|>\\n\"\"\" + \\\n            \"\"\"<|im_end|>\\n\"\"\" +\
          \ \\\n            \"\"\"<|im_start|>user\\n\"\"\" + \\\n            \"\"\
          \"<|user_prompt|><|im_end|>\\n\"\"\" + \\\n            \"\"\"<|im_start|>assistant\\\
          n\"\"\" \n\n    def subs_prompt(self):\n        return \\\n            \"\
          \"\"<|im_end|>\\n\"\"\" + \\\n            \"\"\"<|im_start|>user\\n\"\"\"\
          \ + \\\n            \"\"\"<|user_prompt|><|im_end|>\\n\"\"\" + \\\n    \
          \        \"\"\"<|im_start|>assistant\\n\"\"\" \n\n    def stop_conditions(self,\
          \ tokenizer):\n        return \\\n            [tokenizer.eos_token_id,\n\
          \             \"\"\"</s>\"\"\"]\n\n    def encoding_options(self):\n   \
          \     return False, False, True\n\n    def print_extra_newline(self):\n\
          \        return True\n```"
        updatedAt: '2023-11-27T17:23:28.987Z'
      numEdits: 1
      reactions: []
    id: 6564d0549c811a209aa691b7
    type: comment
  author: pekoragg
  content: "I just simply made a new class but replaced <|im_end|> with < /s> (no\
    \ space). Now  it works fine\n```\nclass PromptFormat_capychatml(PromptFormat):\n\
    \n    description = \"ChatML format, as used by e.g. (Mistral)Orca\"\n\n    def\
    \ __init__(self):\n        super().__init__()\n        pass\n\n    def default_system_prompt(self):\n\
    \        return \\\n            f\"\"\"You are {self.botname}, a large language\
    \ model. Answer as concisely as possible.\"\"\"\n\n    def first_prompt(self):\n\
    \        return \\\n            \"\"\"<|im_start|>system\\n\"\"\" + \\\n     \
    \       \"\"\"<|system_prompt|>\\n\"\"\" + \\\n            \"\"\"<|im_end|>\\\
    n\"\"\" + \\\n            \"\"\"<|im_start|>user\\n\"\"\" + \\\n            \"\
    \"\"<|user_prompt|><|im_end|>\\n\"\"\" + \\\n            \"\"\"<|im_start|>assistant\\\
    n\"\"\" \n\n    def subs_prompt(self):\n        return \\\n            \"\"\"\
    <|im_end|>\\n\"\"\" + \\\n            \"\"\"<|im_start|>user\\n\"\"\" + \\\n \
    \           \"\"\"<|user_prompt|><|im_end|>\\n\"\"\" + \\\n            \"\"\"\
    <|im_start|>assistant\\n\"\"\" \n\n    def stop_conditions(self, tokenizer):\n\
    \        return \\\n            [tokenizer.eos_token_id,\n             \"\"\"\
    </s>\"\"\"]\n\n    def encoding_options(self):\n        return False, False, True\n\
    \n    def print_extra_newline(self):\n        return True\n```"
  created_at: 2023-11-27 17:22:28+00:00
  edited: true
  hidden: false
  id: 6564d0549c811a209aa691b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
      fullname: MC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mclassHF2023
      type: user
    createdAt: '2023-11-27T20:36:21.000Z'
    data:
      edited: false
      editors:
      - mclassHF2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9642282128334045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7789facddc69c2d58aea1b2135c8dd34.svg
          fullname: MC
          isHf: false
          isPro: false
          name: mclassHF2023
          type: user
        html: '<p>I''m currently running into general errors with any GGUF model..
          I''m guessing it''s some python library mismatch</p>

          '
        raw: I'm currently running into general errors with any GGUF model.. I'm guessing
          it's some python library mismatch
        updatedAt: '2023-11-27T20:36:21.490Z'
      numEdits: 0
      reactions: []
    id: 6564fdc542b1b11ecc49ebd5
    type: comment
  author: mclassHF2023
  content: I'm currently running into general errors with any GGUF model.. I'm guessing
    it's some python library mismatch
  created_at: 2023-11-27 20:36:21+00:00
  edited: false
  hidden: false
  id: 6564fdc542b1b11ecc49ebd5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/dolphin-2_2-yi-34b-GGUF
repo_type: model
status: open
target_branch: null
title: BOS token as 1 seriously hurts these GGUF Yi models
