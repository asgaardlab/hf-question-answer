!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dhaladom
conflicting_files: null
created_at: 2023-05-01 19:13:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e7e4ba4997713237daae82b1e4f1356.svg
      fullname: Dharmendra Dominik Laur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dhaladom
      type: user
    createdAt: '2023-05-01T20:13:11.000Z'
    data:
      edited: false
      editors:
      - dhaladom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e7e4ba4997713237daae82b1e4f1356.svg
          fullname: Dharmendra Dominik Laur
          isHf: false
          isPro: false
          name: dhaladom
          type: user
        html: "<p>Hello, thanks for uploading the model!</p>\n<p>I'm trying to run\
          \ it (all three versions) on oobabooga web hosting ui without success, see\
          \ the error message below.</p>\n<p>No issues running LLaMA-65B-4bit or <a\
          \ href=\"https://huggingface.co/TheBloke/vicuna-7B-GPTQ-4bit-128g\">Vicuna-7B-4bit</a>.</p>\n\
          <p>All ideas much appreciated. \U0001F642\U0001F64F</p>\n<pre><code>Traceback\
          \ (most recent call last):\nFile \u201C/app/server.py\u201D, line 102, in\
          \ load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
          File \u201C/app/modules/models.py\u201D, line 217, in load_model\nmodel\
          \ = LoaderClass.from_pretrained(checkpoint, **params)\nFile \u201C/app/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
          , line 471, in from_pretrained\nreturn model_class.from_pretrained(\nFile\
          \ \u201C/app/venv/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 2405, in from_pretrained\nraise EnvironmentError(\nOSError: Error\
          \ no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models/TheBloke_alpaca-lora-65B-GPTQ-4bit.\n</code></pre>\n"
        raw: "Hello, thanks for uploading the model!\r\n\r\nI'm trying to run it (all\
          \ three versions) on oobabooga web hosting ui without success, see the error\
          \ message below.\r\n\r\nNo issues running LLaMA-65B-4bit or [Vicuna-7B-4bit](https://huggingface.co/TheBloke/vicuna-7B-GPTQ-4bit-128g).\r\
          \n\r\nAll ideas much appreciated. \U0001F642\U0001F64F\r\n\r\n```\r\nTraceback\
          \ (most recent call last):\r\nFile \u201C/app/server.py\u201D, line 102,\
          \ in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \nFile \u201C/app/modules/models.py\u201D, line 217, in load_model\r\nmodel\
          \ = LoaderClass.from_pretrained(checkpoint, **params)\r\nFile \u201C/app/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
          , line 471, in from_pretrained\r\nreturn model_class.from_pretrained(\r\n\
          File \u201C/app/venv/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 2405, in from_pretrained\r\nraise EnvironmentError(\r\nOSError: Error\
          \ no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models/TheBloke_alpaca-lora-65B-GPTQ-4bit.\r\n```"
        updatedAt: '2023-05-01T20:13:11.241Z'
      numEdits: 0
      reactions: []
    id: 64501d5733ac8f46fa0fe536
    type: comment
  author: dhaladom
  content: "Hello, thanks for uploading the model!\r\n\r\nI'm trying to run it (all\
    \ three versions) on oobabooga web hosting ui without success, see the error message\
    \ below.\r\n\r\nNo issues running LLaMA-65B-4bit or [Vicuna-7B-4bit](https://huggingface.co/TheBloke/vicuna-7B-GPTQ-4bit-128g).\r\
    \n\r\nAll ideas much appreciated. \U0001F642\U0001F64F\r\n\r\n```\r\nTraceback\
    \ (most recent call last):\r\nFile \u201C/app/server.py\u201D, line 102, in load_model_wrapper\r\
    \nshared.model, shared.tokenizer = load_model(shared.model_name)\r\nFile \u201C\
    /app/modules/models.py\u201D, line 217, in load_model\r\nmodel = LoaderClass.from_pretrained(checkpoint,\
    \ **params)\r\nFile \u201C/app/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
    , line 471, in from_pretrained\r\nreturn model_class.from_pretrained(\r\nFile\
    \ \u201C/app/venv/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
    , line 2405, in from_pretrained\r\nraise EnvironmentError(\r\nOSError: Error no\
    \ file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
    \ found in directory models/TheBloke_alpaca-lora-65B-GPTQ-4bit.\r\n```"
  created_at: 2023-05-01 19:13:11+00:00
  edited: false
  hidden: false
  id: 64501d5733ac8f46fa0fe536
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-02T03:44:47.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>The  loader does not seems right,</p>

          <pre><code>model = LoaderClass.from_pretrained(checkpoint, **params)

          </code></pre>

          <p>It should be the GPTQ loader</p>

          <p>What is your command line argument after sever.py? It should be something
          like </p>

          <pre><code>--model llava-13b-v0-4bit-128g --model_type llama --wbits 4 --groupsize
          128 ...

          </code></pre>

          <p>where --model in your case should be TheBloke_alpaca-lora-65B-GPTQ-4bit
          which is the folder name under model. I have not tried it. </p>

          <p>I think you should name the folder as alpaca-lora-65B-GPTQ-4bit instead
          of with prefix TheBloke_</p>

          '
        raw: "The  loader does not seems right,\n```\nmodel = LoaderClass.from_pretrained(checkpoint,\
          \ **params)\n```\nIt should be the GPTQ loader\n\nWhat is your command line\
          \ argument after sever.py? It should be something like \n```\n--model llava-13b-v0-4bit-128g\
          \ --model_type llama --wbits 4 --groupsize 128 ...\n```\nwhere --model in\
          \ your case should be TheBloke_alpaca-lora-65B-GPTQ-4bit which is the folder\
          \ name under model. I have not tried it. \n\nI think you should name the\
          \ folder as alpaca-lora-65B-GPTQ-4bit instead of with prefix TheBloke_"
        updatedAt: '2023-05-02T03:44:47.957Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dhaladom
    id: 6450872f577838187e09bbc3
    type: comment
  author: Yhyu13
  content: "The  loader does not seems right,\n```\nmodel = LoaderClass.from_pretrained(checkpoint,\
    \ **params)\n```\nIt should be the GPTQ loader\n\nWhat is your command line argument\
    \ after sever.py? It should be something like \n```\n--model llava-13b-v0-4bit-128g\
    \ --model_type llama --wbits 4 --groupsize 128 ...\n```\nwhere --model in your\
    \ case should be TheBloke_alpaca-lora-65B-GPTQ-4bit which is the folder name under\
    \ model. I have not tried it. \n\nI think you should name the folder as alpaca-lora-65B-GPTQ-4bit\
    \ instead of with prefix TheBloke_"
  created_at: 2023-05-02 02:44:47+00:00
  edited: false
  hidden: false
  id: 6450872f577838187e09bbc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e7e4ba4997713237daae82b1e4f1356.svg
      fullname: Dharmendra Dominik Laur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dhaladom
      type: user
    createdAt: '2023-05-02T13:25:41.000Z'
    data:
      edited: false
      editors:
      - dhaladom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e7e4ba4997713237daae82b1e4f1356.svg
          fullname: Dharmendra Dominik Laur
          isHf: false
          isPro: false
          name: dhaladom
          type: user
        html: "<p>Thanks for the hint about the loader! I'm optimistic that this will\
          \ help me to solve the issue. \U0001F642</p>\n"
        raw: "Thanks for the hint about the loader! I'm optimistic that this will\
          \ help me to solve the issue. \U0001F642"
        updatedAt: '2023-05-02T13:25:41.432Z'
      numEdits: 0
      reactions: []
    id: 64510f55b3f75261a7d16188
    type: comment
  author: dhaladom
  content: "Thanks for the hint about the loader! I'm optimistic that this will help\
    \ me to solve the issue. \U0001F642"
  created_at: 2023-05-02 12:25:41+00:00
  edited: false
  hidden: false
  id: 64510f55b3f75261a7d16188
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-02T15:07:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah you get this error when text-generation-ui does not know your
          model is a GPTQ model</p>

          <p>Either use the command line arguments given, or else fill in the GPTQ
          parameters in the Model page of the UI and then click "Save settings for
          this model" and then "Reload this model"</p>

          '
        raw: 'Yeah you get this error when text-generation-ui does not know your model
          is a GPTQ model


          Either use the command line arguments given, or else fill in the GPTQ parameters
          in the Model page of the UI and then click "Save settings for this model"
          and then "Reload this model"'
        updatedAt: '2023-05-02T15:07:47.625Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dhaladom
    id: 645127439d916c596e2a1bf3
    type: comment
  author: TheBloke
  content: 'Yeah you get this error when text-generation-ui does not know your model
    is a GPTQ model


    Either use the command line arguments given, or else fill in the GPTQ parameters
    in the Model page of the UI and then click "Save settings for this model" and
    then "Reload this model"'
  created_at: 2023-05-02 14:07:47+00:00
  edited: false
  hidden: false
  id: 645127439d916c596e2a1bf3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e7e4ba4997713237daae82b1e4f1356.svg
      fullname: Dharmendra Dominik Laur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dhaladom
      type: user
    createdAt: '2023-05-02T21:34:32.000Z'
    data:
      edited: false
      editors:
      - dhaladom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e7e4ba4997713237daae82b1e4f1356.svg
          fullname: Dharmendra Dominik Laur
          isHf: false
          isPro: false
          name: dhaladom
          type: user
        html: '<p>Thanks! I think the issue is resolved. </p>

          '
        raw: 'Thanks! I think the issue is resolved. '
        updatedAt: '2023-05-02T21:34:32.038Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645181e841f3c769b9178a20
    id: 645181e841f3c769b9178a1f
    type: comment
  author: dhaladom
  content: 'Thanks! I think the issue is resolved. '
  created_at: 2023-05-02 20:34:32+00:00
  edited: false
  hidden: false
  id: 645181e841f3c769b9178a1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/2e7e4ba4997713237daae82b1e4f1356.svg
      fullname: Dharmendra Dominik Laur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dhaladom
      type: user
    createdAt: '2023-05-02T21:34:32.000Z'
    data:
      status: closed
    id: 645181e841f3c769b9178a20
    type: status-change
  author: dhaladom
  created_at: 2023-05-02 20:34:32+00:00
  id: 645181e841f3c769b9178a20
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/alpaca-lora-65B-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'Error loading in oobabooga '
