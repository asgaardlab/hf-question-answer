!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GeoBuilder
conflicting_files: null
created_at: 2023-04-27 01:59:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
      fullname: Geo Perkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GeoBuilder
      type: user
    createdAt: '2023-04-27T02:59:30.000Z'
    data:
      edited: true
      editors:
      - GeoBuilder
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
          fullname: Geo Perkins
          isHf: false
          isPro: false
          name: GeoBuilder
          type: user
        html: '<p>Setup as described minus a couple of work arounds as the other user
          pointed out for bad parameter: </p>

          <p>Using model directory, rather than:<br>"--model medalpaca-13B-GPTQ-4bit"</p>

          <p>and changed alpaca-lora-65B-GPTQ-4bit-1024g.safetensors GPTQ setup command
          to use:<br> "TheBloke/alpaca-lora-65B-HF"<br>rather than:  alpaca-lora-65B-HF<br>...This
          took about 5 hours to run..</p>

          <p>However when using:<br>$ python server.py --model TheBloke_alpaca-lora-65B-GPTQ-4bit
          --wbits 4 --groupsize 1024 --model_type Llama<br>With the quantized model
          alpaca-lora-65B-GPTQ-4bit-1024g.safetensors. Now getting a screen full of
          this for every layer :</p>

          <p>"size mismatch for model.layers.79.mlp.up_proj.scales: copying a param
          with shape torch.Size([64, 22016]) from checkpoint, the shape in current
          model is torch.Size([8, 22016])."</p>

          <p>Should I not be using groupsize 1024 with this? Any feedback?</p>

          '
        raw: "Setup as described minus a couple of work arounds as the other user\
          \ pointed out for bad parameter: \n\nUsing model directory, rather than:\n\
          \"--model medalpaca-13B-GPTQ-4bit\"\n\nand changed alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\
          \ GPTQ setup command to use:\n \"TheBloke/alpaca-lora-65B-HF\"\nrather than:\
          \  alpaca-lora-65B-HF\n...This took about 5 hours to run..\n\nHowever when\
          \ using:\n$ python server.py --model TheBloke_alpaca-lora-65B-GPTQ-4bit\
          \ --wbits 4 --groupsize 1024 --model_type Llama \nWith the quantized model\
          \ alpaca-lora-65B-GPTQ-4bit-1024g.safetensors. Now getting a screen full\
          \ of this for every layer :\n\n\"size mismatch for model.layers.79.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([64, 22016]) from checkpoint, the\
          \ shape in current model is torch.Size([8, 22016]).\"\n\nShould I not be\
          \ using groupsize 1024 with this? Any feedback?"
        updatedAt: '2023-04-27T03:15:17.204Z'
      numEdits: 3
      reactions: []
    id: 6449e512eb7db8f70fb93bd8
    type: comment
  author: GeoBuilder
  content: "Setup as described minus a couple of work arounds as the other user pointed\
    \ out for bad parameter: \n\nUsing model directory, rather than:\n\"--model medalpaca-13B-GPTQ-4bit\"\
    \n\nand changed alpaca-lora-65B-GPTQ-4bit-1024g.safetensors GPTQ setup command\
    \ to use:\n \"TheBloke/alpaca-lora-65B-HF\"\nrather than:  alpaca-lora-65B-HF\n\
    ...This took about 5 hours to run..\n\nHowever when using:\n$ python server.py\
    \ --model TheBloke_alpaca-lora-65B-GPTQ-4bit --wbits 4 --groupsize 1024 --model_type\
    \ Llama \nWith the quantized model alpaca-lora-65B-GPTQ-4bit-1024g.safetensors.\
    \ Now getting a screen full of this for every layer :\n\n\"size mismatch for model.layers.79.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([64, 22016]) from checkpoint, the shape\
    \ in current model is torch.Size([8, 22016]).\"\n\nShould I not be using groupsize\
    \ 1024 with this? Any feedback?"
  created_at: 2023-04-27 01:59:30+00:00
  edited: true
  hidden: false
  id: 6449e512eb7db8f70fb93bd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
      fullname: Geo Perkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GeoBuilder
      type: user
    createdAt: '2023-04-27T03:12:09.000Z'
    data:
      edited: true
      editors:
      - GeoBuilder
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
          fullname: Geo Perkins
          isHf: false
          isPro: false
          name: GeoBuilder
          type: user
        html: '<p>NM, Seems like that command is defaulting to the:<br>alpaca-lora-65B-GPTQ-4bit-128g.safetensors</p>

          <p>However when forcing the use of: alpaca-lora-65B-GPTQ-4bit-1024g.safetensors<br>I
          get "Could not find the quantized model in .pt or .safetensors format, exiting..."</p>

          <p>Appreciate any feedback you can give, to help other people on the right
          track.</p>

          '
        raw: "NM, Seems like that command is defaulting to the: \nalpaca-lora-65B-GPTQ-4bit-128g.safetensors\n\
          \nHowever when forcing the use of: alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\
          \ \nI get \"Could not find the quantized model in .pt or .safetensors format,\
          \ exiting...\"\n\nAppreciate any feedback you can give, to help other people\
          \ on the right track."
        updatedAt: '2023-04-27T03:16:03.891Z'
      numEdits: 1
      reactions: []
    id: 6449e809111b3bf6878726b2
    type: comment
  author: GeoBuilder
  content: "NM, Seems like that command is defaulting to the: \nalpaca-lora-65B-GPTQ-4bit-128g.safetensors\n\
    \nHowever when forcing the use of: alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\
    \ \nI get \"Could not find the quantized model in .pt or .safetensors format,\
    \ exiting...\"\n\nAppreciate any feedback you can give, to help other people on\
    \ the right track."
  created_at: 2023-04-27 02:12:09+00:00
  edited: true
  hidden: false
  id: 6449e809111b3bf6878726b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-27T07:18:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>and changed alpaca-lora-65B-GPTQ-4bit-1024g.safetensors GPTQ setup command
          to use:<br> "TheBloke/alpaca-lora-65B-HF"<br>rather than:  alpaca-lora-65B-HF<br>...This
          took about 5 hours to run..</p>

          </blockquote>

          <p>I''m confused. You didn''t re-make the GPTQ did you?  The "command used
          to create" were given just for reference, to explain how I made the files.  Not
          for you to re-run yourself. If you remade the files you don''t need this
          repo! :)</p>

          <blockquote>

          <p>$ python server.py --model TheBloke_alpaca-lora-65B-GPTQ-4bit --wbits
          4 --groupsize 1024 --model_type Llama </p>

          </blockquote>

          <p>This is all you need to run to use the files in this repo.</p>

          <blockquote>

          <p>With the quantized model alpaca-lora-65B-GPTQ-4bit-1024g.safetensors.
          Now getting a screen full of this for every layer :<br>"size mismatch for
          model.layers.79.mlp.up_proj.scales: copying a param with shape torch.Size([64,
          22016]) from checkpoint, the shape in current model is torch.Size([8, 22016])."</p>

          </blockquote>

          <p>Assuming you did download the files from my repo and not re-make them,
          then check the sha256sum is correct for the model file you''re using.  They''re
          large files and that increases the chance of a glitch during download.  If
          you did re-make the files, then this could well be some error during GPTQ
          creation, which will be hard to diagnose without knowing exactly what you
          ran and with which version/fork of GPTQ-for-LLaMa.</p>

          <blockquote>

          <p>NM, Seems like that command is defaulting to the:<br>alpaca-lora-65B-GPTQ-4bit-128g.safetensors</p>

          </blockquote>

          <p>If multiple model files are in the directory, it''ll load the first one
          it finds. Remove any model files you don''t want from the model directory.</p>

          <blockquote>

          <p>However when forcing the use of: alpaca-lora-65B-GPTQ-4bit-1024g.safetensors<br>I
          get "Could not find the quantized model in .pt or .safetensors format, exiting..."</p>

          </blockquote>

          <p>Sounds like you''ve either not got the right parameters when starting
          <code>text-generation-webui</code> or you''ve it to look in the wrong model
          directory.  This would happen for example if you forgot the <code>--wbits
          4 --groupsize 1024</code> params to server.py.</p>

          <p>Please show the command used to start text-generation-webui, and the
          contents of your model directory.</p>

          '
        raw: "> and changed alpaca-lora-65B-GPTQ-4bit-1024g.safetensors GPTQ setup\
          \ command to use:\n>  \"TheBloke/alpaca-lora-65B-HF\"\n> rather than:  alpaca-lora-65B-HF\n\
          > ...This took about 5 hours to run..\n\nI'm confused. You didn't re-make\
          \ the GPTQ did you?  The \"command used to create\" were given just for\
          \ reference, to explain how I made the files.  Not for you to re-run yourself.\
          \ If you remade the files you don't need this repo! :)\n\n> $ python server.py\
          \ --model TheBloke_alpaca-lora-65B-GPTQ-4bit --wbits 4 --groupsize 1024\
          \ --model_type Llama \n\nThis is all you need to run to use the files in\
          \ this repo.\n\n> With the quantized model alpaca-lora-65B-GPTQ-4bit-1024g.safetensors.\
          \ Now getting a screen full of this for every layer : \n> \"size mismatch\
          \ for model.layers.79.mlp.up_proj.scales: copying a param with shape torch.Size([64,\
          \ 22016]) from checkpoint, the shape in current model is torch.Size([8,\
          \ 22016]).\"\n\nAssuming you did download the files from my repo and not\
          \ re-make them, then check the sha256sum is correct for the model file you're\
          \ using.  They're large files and that increases the chance of a glitch\
          \ during download.  If you did re-make the files, then this could well be\
          \ some error during GPTQ creation, which will be hard to diagnose without\
          \ knowing exactly what you ran and with which version/fork of GPTQ-for-LLaMa.\n\
          \n> NM, Seems like that command is defaulting to the: \n> alpaca-lora-65B-GPTQ-4bit-128g.safetensors\n\
          >\nIf multiple model files are in the directory, it'll load the first one\
          \ it finds. Remove any model files you don't want from the model directory.\n\
          \n> However when forcing the use of: alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\
          \ \n> I get \"Could not find the quantized model in .pt or .safetensors\
          \ format, exiting...\"\n\nSounds like you've either not got the right parameters\
          \ when starting `text-generation-webui` or you've it to look in the wrong\
          \ model directory.  This would happen for example if you forgot the `--wbits\
          \ 4 --groupsize 1024` params to server.py.\n\nPlease show the command used\
          \ to start text-generation-webui, and the contents of your model directory."
        updatedAt: '2023-04-27T07:18:16.621Z'
      numEdits: 0
      reactions: []
    id: 644a21b897fcd34670863d09
    type: comment
  author: TheBloke
  content: "> and changed alpaca-lora-65B-GPTQ-4bit-1024g.safetensors GPTQ setup command\
    \ to use:\n>  \"TheBloke/alpaca-lora-65B-HF\"\n> rather than:  alpaca-lora-65B-HF\n\
    > ...This took about 5 hours to run..\n\nI'm confused. You didn't re-make the\
    \ GPTQ did you?  The \"command used to create\" were given just for reference,\
    \ to explain how I made the files.  Not for you to re-run yourself. If you remade\
    \ the files you don't need this repo! :)\n\n> $ python server.py --model TheBloke_alpaca-lora-65B-GPTQ-4bit\
    \ --wbits 4 --groupsize 1024 --model_type Llama \n\nThis is all you need to run\
    \ to use the files in this repo.\n\n> With the quantized model alpaca-lora-65B-GPTQ-4bit-1024g.safetensors.\
    \ Now getting a screen full of this for every layer : \n> \"size mismatch for\
    \ model.layers.79.mlp.up_proj.scales: copying a param with shape torch.Size([64,\
    \ 22016]) from checkpoint, the shape in current model is torch.Size([8, 22016]).\"\
    \n\nAssuming you did download the files from my repo and not re-make them, then\
    \ check the sha256sum is correct for the model file you're using.  They're large\
    \ files and that increases the chance of a glitch during download.  If you did\
    \ re-make the files, then this could well be some error during GPTQ creation,\
    \ which will be hard to diagnose without knowing exactly what you ran and with\
    \ which version/fork of GPTQ-for-LLaMa.\n\n> NM, Seems like that command is defaulting\
    \ to the: \n> alpaca-lora-65B-GPTQ-4bit-128g.safetensors\n>\nIf multiple model\
    \ files are in the directory, it'll load the first one it finds. Remove any model\
    \ files you don't want from the model directory.\n\n> However when forcing the\
    \ use of: alpaca-lora-65B-GPTQ-4bit-1024g.safetensors \n> I get \"Could not find\
    \ the quantized model in .pt or .safetensors format, exiting...\"\n\nSounds like\
    \ you've either not got the right parameters when starting `text-generation-webui`\
    \ or you've it to look in the wrong model directory.  This would happen for example\
    \ if you forgot the `--wbits 4 --groupsize 1024` params to server.py.\n\nPlease\
    \ show the command used to start text-generation-webui, and the contents of your\
    \ model directory."
  created_at: 2023-04-27 06:18:16+00:00
  edited: false
  hidden: false
  id: 644a21b897fcd34670863d09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
      fullname: Geo Perkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GeoBuilder
      type: user
    createdAt: '2023-04-28T04:22:28.000Z'
    data:
      edited: false
      editors:
      - GeoBuilder
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
          fullname: Geo Perkins
          isHf: false
          isPro: false
          name: GeoBuilder
          type: user
        html: '<p>Yup, first point. I did indeed remake the GPTQ . I misunderstood
          the workflow.<br>Used this to make it: </p>

          <blockquote>

          <p>$ python3 llama.py "TheBloke/alpaca-lora-65B-HF" c4 --wbits 4 --true-sequential
          --act-order --groupsize 1024 --save_safetensors alpaca-lora-65B-GPTQ-4bit-1024g.safetensors</p>

          </blockquote>

          <p>It ran for 5 hours.. :)</p>

          <p>I understand, about the <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a>
          repo, there have been updates to it in the interim, so I will just start
          over, since it is in a runpod and I created a base template for it anyway.
          Thanks for the feedback, sorry for misunderstanding the workflow. (I do
          however appreciate you listing the method you used to generate the GPTQ,
          even if I missed the intent). I guess I went twenty steps too far.</p>

          <p>Also thanks for being responsive Kudos.</p>

          '
        raw: "Yup, first point. I did indeed remake the GPTQ . I misunderstood the\
          \ workflow.\nUsed this to make it: \n> $ python3 llama.py \"TheBloke/alpaca-lora-65B-HF\"\
          \ c4 --wbits 4 --true-sequential --act-order --groupsize 1024 --save_safetensors\
          \ alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\n\nIt ran for 5 hours.. :)\n\
          \nI understand, about the https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ repo, there have been updates to it in the interim, so I will just start\
          \ over, since it is in a runpod and I created a base template for it anyway.\
          \ Thanks for the feedback, sorry for misunderstanding the workflow. (I do\
          \ however appreciate you listing the method you used to generate the GPTQ,\
          \ even if I missed the intent). I guess I went twenty steps too far.\n\n\
          Also thanks for being responsive Kudos."
        updatedAt: '2023-04-28T04:22:28.336Z'
      numEdits: 0
      reactions: []
    id: 644b4a04cb45734dfd507947
    type: comment
  author: GeoBuilder
  content: "Yup, first point. I did indeed remake the GPTQ . I misunderstood the workflow.\n\
    Used this to make it: \n> $ python3 llama.py \"TheBloke/alpaca-lora-65B-HF\" c4\
    \ --wbits 4 --true-sequential --act-order --groupsize 1024 --save_safetensors\
    \ alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\n\nIt ran for 5 hours.. :)\n\nI\
    \ understand, about the https://github.com/qwopqwop200/GPTQ-for-LLaMa repo, there\
    \ have been updates to it in the interim, so I will just start over, since it\
    \ is in a runpod and I created a base template for it anyway. Thanks for the feedback,\
    \ sorry for misunderstanding the workflow. (I do however appreciate you listing\
    \ the method you used to generate the GPTQ, even if I missed the intent). I guess\
    \ I went twenty steps too far.\n\nAlso thanks for being responsive Kudos."
  created_at: 2023-04-28 03:22:28+00:00
  edited: false
  hidden: false
  id: 644b4a04cb45734dfd507947
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
      fullname: Geo Perkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GeoBuilder
      type: user
    createdAt: '2023-04-28T06:15:56.000Z'
    data:
      edited: true
      editors:
      - GeoBuilder
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ea226f426809233402e303e4c189959.svg
          fullname: Geo Perkins
          isHf: false
          isPro: false
          name: GeoBuilder
          type: user
        html: '<p>OK, so surprising update....</p>

          <p>Ran from clean pod, and did not generate GPTQ this time however, same
          error..... Here is the run command, and the tail of the errors:</p>

          <blockquote>

          <h1 id="python-serverpy---model-thebloke_alpaca-lora-65b-gptq-4bit---wbits-4---groupsize-1024---model_type-llama">python
          server.py --model TheBloke_alpaca-lora-65B-GPTQ-4bit --wbits 4 --groupsize
          1024 --model_type Llama</h1>

          <p>Gradio HTTP request redirected to localhost :)<br>bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda116.so<br>Loading
          TheBloke_alpaca-lora-65B-GPTQ-4bit...<br>Found the following quantized model:
          models/TheBloke_alpaca-lora-65B-GPTQ-4bit/alpaca-lora-65B-GPTQ-4bit-1024g.safetensors</p>

          </blockquote>

          <p>.....</p>

          <blockquote>

          <p>Size([8, 8192]). size mismatch for model.layers.79.mlp.down_proj.qzeros:
          copying a param with shape torch.<br>Size([1, 1024]) from checkpoint, the
          shape in current model is torch. Size([22, 1024]).<br>size mismatch for
          model.layers.79.mlp.down_proj.scales: copying a param with shape torch.<br>Size([1,
          8192]) from checkpoint, the shape in current model is torch. Size([22, 8192]).<br>size
          mismatch for model.layers.79.mlp.gate_proj.qzeros: copying a param with
          shape torch.<br>Size([1, 2752]) from checkpoint, the shape in current model
          is torch. Size([8, 2752]).<br>size mismatch for model.layers.79.mlp.gate_proj.scales:
          copying a param with shape torch.<br>Size([1, 22016]) from checkpoint, the
          shape in current model is torch. Size([8, 22016]).<br>size mismatch for
          model.layers.79.mlp.up_proj.qzeros: copying a param with shape torch.<br>Size([1,
          2752]) from checkpoint, the shape in current model is torch. Size([8, 2752]).<br>size
          mismatch for model.layers.79.mlp.up_proj.scales: copying a param with shape
          torch.<br>Size([1, 22016]) from checkpoint, the shape in current model is
          torch. Size([8, 22016]).</p>

          </blockquote>

          <p>So just to clarify, both the GPTQ version that I created from the first
          post, and now the one in your repo are erring with the same general errors
          "size mismatch for model layers", although the exact errors ARE different.
          Also note I only included the errors for the last layers, but cli was full...</p>

          <p>I am curious for your input, I am using a new pulls from: <a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a>
          , and <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a>
          . So not sure if versioning differences between that and what you originally
          ran it with could contribute to this. And again to be clear I ran these
          commands from a newly spun up docker pod, with no artifacts. The only thing
          I do have as a carry over is the environmental variables HF_HOME &amp; TRANSFORMERS_CACHE
          so I could reposition the /.cache/huggingface  cache files when I ran the
          GPTQ due to the dockers constraints, and this directory getting 140GB large.</p>

          <p>ADDITIONAL FORGOT TO ADD SINCE YOU ASKED FOR IT:</p>

          <blockquote>

          <p>-rw-rw-rw- 1 root root        6300 Apr 28 04:47 README.md<br>-rw-rw-rw-
          1 root root 33471316664 Apr 28 05:04 alpaca-lora-65B-GPTQ-4bit-1024g.safetensors<br>-rw-rw-rw-
          1 root root         546 Apr 28 05:33 config.json<br>-rw-rw-rw- 1 root root         132
          Apr 28 05:33 generation_config.json<br>-rw-rw-rw- 1 root root         557
          Apr 28 04:47 huggingface-metadata.txt<br>-rw-rw-rw- 1 root root       66725
          Apr 28 05:33 pytorch_model.bin.index.json<br>-rw-rw-rw- 1 root root      499723
          Apr 28 05:33 tokenizer.model</p>

          </blockquote>

          '
        raw: "OK, so surprising update....\n\nRan from clean pod, and did not generate\
          \ GPTQ this time however, same error..... Here is the run command, and the\
          \ tail of the errors:\n\n> # python server.py --model TheBloke_alpaca-lora-65B-GPTQ-4bit\
          \ --wbits 4 --groupsize 1024 --model_type Llama\n> Gradio HTTP request redirected\
          \ to localhost :)\n> bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda116.so\n\
          > Loading TheBloke_alpaca-lora-65B-GPTQ-4bit...\n> Found the following quantized\
          \ model: models/TheBloke_alpaca-lora-65B-GPTQ-4bit/alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\n\
          \n.....\n\n> Size([8, 8192]). size mismatch for model.layers.79.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.\n> Size([1, 1024]) from checkpoint,\
          \ the shape in current model is torch. Size([22, 1024]). \n> size mismatch\
          \ for model.layers.79.mlp.down_proj.scales: copying a param with shape torch.\n\
          > Size([1, 8192]) from checkpoint, the shape in current model is torch.\
          \ Size([22, 8192]).\n> size mismatch for model.layers.79.mlp.gate_proj.qzeros:\
          \ copying a param with shape torch.\n> Size([1, 2752]) from checkpoint,\
          \ the shape in current model is torch. Size([8, 2752]). \n> size mismatch\
          \ for model.layers.79.mlp.gate_proj.scales: copying a param with shape torch.\n\
          > Size([1, 22016]) from checkpoint, the shape in current model is torch.\
          \ Size([8, 22016]). \n> size mismatch for model.layers.79.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.\n> Size([1, 2752]) from checkpoint,\
          \ the shape in current model is torch. Size([8, 2752]). \n> size mismatch\
          \ for model.layers.79.mlp.up_proj.scales: copying a param with shape torch.\n\
          > Size([1, 22016]) from checkpoint, the shape in current model is torch.\
          \ Size([8, 22016]).\n\nSo just to clarify, both the GPTQ version that I\
          \ created from the first post, and now the one in your repo are erring with\
          \ the same general errors \"size mismatch for model layers\", although the\
          \ exact errors ARE different. Also note I only included the errors for the\
          \ last layers, but cli was full...\n\nI am curious for your input, I am\
          \ using a new pulls from: https://github.com/oobabooga/text-generation-webui\
          \ , and https://github.com/qwopqwop200/GPTQ-for-LLaMa . So not sure if versioning\
          \ differences between that and what you originally ran it with could contribute\
          \ to this. And again to be clear I ran these commands from a newly spun\
          \ up docker pod, with no artifacts. The only thing I do have as a carry\
          \ over is the environmental variables HF_HOME & TRANSFORMERS_CACHE so I\
          \ could reposition the /.cache/huggingface  cache files when I ran the GPTQ\
          \ due to the dockers constraints, and this directory getting 140GB large.\n\
          \nADDITIONAL FORGOT TO ADD SINCE YOU ASKED FOR IT:\n> -rw-rw-rw- 1 root\
          \ root        6300 Apr 28 04:47 README.md\n> -rw-rw-rw- 1 root root 33471316664\
          \ Apr 28 05:04 alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\n> -rw-rw-rw-\
          \ 1 root root         546 Apr 28 05:33 config.json\n> -rw-rw-rw- 1 root\
          \ root         132 Apr 28 05:33 generation_config.json\n> -rw-rw-rw- 1 root\
          \ root         557 Apr 28 04:47 huggingface-metadata.txt\n> -rw-rw-rw- 1\
          \ root root       66725 Apr 28 05:33 pytorch_model.bin.index.json\n> -rw-rw-rw-\
          \ 1 root root      499723 Apr 28 05:33 tokenizer.model"
        updatedAt: '2023-04-28T06:28:53.010Z'
      numEdits: 6
      reactions: []
    id: 644b649c840601ec7f157ae7
    type: comment
  author: GeoBuilder
  content: "OK, so surprising update....\n\nRan from clean pod, and did not generate\
    \ GPTQ this time however, same error..... Here is the run command, and the tail\
    \ of the errors:\n\n> # python server.py --model TheBloke_alpaca-lora-65B-GPTQ-4bit\
    \ --wbits 4 --groupsize 1024 --model_type Llama\n> Gradio HTTP request redirected\
    \ to localhost :)\n> bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda116.so\n\
    > Loading TheBloke_alpaca-lora-65B-GPTQ-4bit...\n> Found the following quantized\
    \ model: models/TheBloke_alpaca-lora-65B-GPTQ-4bit/alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\n\
    \n.....\n\n> Size([8, 8192]). size mismatch for model.layers.79.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.\n> Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch. Size([22, 1024]). \n> size mismatch for model.layers.79.mlp.down_proj.scales:\
    \ copying a param with shape torch.\n> Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch. Size([22, 8192]).\n> size mismatch for model.layers.79.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.\n> Size([1, 2752]) from checkpoint, the shape\
    \ in current model is torch. Size([8, 2752]). \n> size mismatch for model.layers.79.mlp.gate_proj.scales:\
    \ copying a param with shape torch.\n> Size([1, 22016]) from checkpoint, the shape\
    \ in current model is torch. Size([8, 22016]). \n> size mismatch for model.layers.79.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.\n> Size([1, 2752]) from checkpoint, the shape\
    \ in current model is torch. Size([8, 2752]). \n> size mismatch for model.layers.79.mlp.up_proj.scales:\
    \ copying a param with shape torch.\n> Size([1, 22016]) from checkpoint, the shape\
    \ in current model is torch. Size([8, 22016]).\n\nSo just to clarify, both the\
    \ GPTQ version that I created from the first post, and now the one in your repo\
    \ are erring with the same general errors \"size mismatch for model layers\",\
    \ although the exact errors ARE different. Also note I only included the errors\
    \ for the last layers, but cli was full...\n\nI am curious for your input, I am\
    \ using a new pulls from: https://github.com/oobabooga/text-generation-webui ,\
    \ and https://github.com/qwopqwop200/GPTQ-for-LLaMa . So not sure if versioning\
    \ differences between that and what you originally ran it with could contribute\
    \ to this. And again to be clear I ran these commands from a newly spun up docker\
    \ pod, with no artifacts. The only thing I do have as a carry over is the environmental\
    \ variables HF_HOME & TRANSFORMERS_CACHE so I could reposition the /.cache/huggingface\
    \  cache files when I ran the GPTQ due to the dockers constraints, and this directory\
    \ getting 140GB large.\n\nADDITIONAL FORGOT TO ADD SINCE YOU ASKED FOR IT:\n>\
    \ -rw-rw-rw- 1 root root        6300 Apr 28 04:47 README.md\n> -rw-rw-rw- 1 root\
    \ root 33471316664 Apr 28 05:04 alpaca-lora-65B-GPTQ-4bit-1024g.safetensors\n\
    > -rw-rw-rw- 1 root root         546 Apr 28 05:33 config.json\n> -rw-rw-rw- 1\
    \ root root         132 Apr 28 05:33 generation_config.json\n> -rw-rw-rw- 1 root\
    \ root         557 Apr 28 04:47 huggingface-metadata.txt\n> -rw-rw-rw- 1 root\
    \ root       66725 Apr 28 05:33 pytorch_model.bin.index.json\n> -rw-rw-rw- 1 root\
    \ root      499723 Apr 28 05:33 tokenizer.model"
  created_at: 2023-04-28 05:15:56+00:00
  edited: true
  hidden: false
  id: 644b649c840601ec7f157ae7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/alpaca-lora-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: Getting size mismatch for model.layers with alpaca-lora-65B-GPTQ-4bit-1024g.safetensors
