!!python/object:huggingface_hub.community.DiscussionWithDetails
author: autobots
conflicting_files: null
created_at: 2023-05-02 00:21:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-02T01:21:32.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Would not load in 0cc4m/ooba/etc.</p>

          <p>New cuda is slower :(</p>

          '
        raw: "Would not load in 0cc4m/ooba/etc.\r\n\r\nNew cuda is slower :("
        updatedAt: '2023-05-02T01:21:32.623Z'
      numEdits: 0
      reactions: []
    id: 6450659cd43fedb824bdc164
    type: comment
  author: autobots
  content: "Would not load in 0cc4m/ooba/etc.\r\n\r\nNew cuda is slower :("
  created_at: 2023-05-02 00:21:32+00:00
  edited: false
  hidden: false
  id: 6450659cd43fedb824bdc164
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-02T03:36:07.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>which new cuda? The oobabooga GPTQ cuda branch?</p>

          '
        raw: which new cuda? The oobabooga GPTQ cuda branch?
        updatedAt: '2023-05-02T03:36:07.764Z'
      numEdits: 0
      reactions: []
    id: 64508527d5f7dafcfa6fb5af
    type: comment
  author: Yhyu13
  content: which new cuda? The oobabooga GPTQ cuda branch?
  created_at: 2023-05-02 02:36:07+00:00
  edited: false
  hidden: false
  id: 64508527d5f7dafcfa6fb5af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-02T04:25:34.000Z'
    data:
      edited: true
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>That''s the one that won''t work.</p>

          <p>this one:<a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda">https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda</a>
          will load it and be slow</p>

          '
        raw: 'That''s the one that won''t work.


          this one:https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda will load
          it and be slow'
        updatedAt: '2023-05-02T04:28:22.200Z'
      numEdits: 1
      reactions: []
    id: 645090be28774bd665e79167
    type: comment
  author: autobots
  content: 'That''s the one that won''t work.


    this one:https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda will load it
    and be slow'
  created_at: 2023-05-02 03:25:34+00:00
  edited: true
  hidden: false
  id: 645090be28774bd665e79167
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-05-02T06:54:01.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>huh</p>

          '
        raw: huh
        updatedAt: '2023-05-02T06:54:01.551Z'
      numEdits: 0
      reactions: []
    id: 6450b389eb41f0bd48a347ea
    type: comment
  author: teknium
  content: huh
  created_at: 2023-05-02 05:54:01+00:00
  edited: false
  hidden: false
  id: 6450b389eb41f0bd48a347ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-02T10:09:52.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Yea, I''m looking for another 65b since I don''t want to download
          127gb of the fp32 model to convert myself. Will see if the maderix works.
          Unfortunately none of them have a merged alpaca lora.</p>

          '
        raw: Yea, I'm looking for another 65b since I don't want to download 127gb
          of the fp32 model to convert myself. Will see if the maderix works. Unfortunately
          none of them have a merged alpaca lora.
        updatedAt: '2023-05-02T10:09:52.297Z'
      numEdits: 0
      reactions: []
    id: 6450e1708c5830e111da01b2
    type: comment
  author: autobots
  content: Yea, I'm looking for another 65b since I don't want to download 127gb of
    the fp32 model to convert myself. Will see if the maderix works. Unfortunately
    none of them have a merged alpaca lora.
  created_at: 2023-05-02 09:09:52+00:00
  edited: false
  hidden: false
  id: 6450e1708c5830e111da01b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-02T10:11:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Did you guys try <code>alpaca-lora-65B-GPTQ-4bit-128g.no-act-order.safetensors</code>
          ? That was the file I made for old GPTQ ie ooba.</p>

          '
        raw: Did you guys try `alpaca-lora-65B-GPTQ-4bit-128g.no-act-order.safetensors`
          ? That was the file I made for old GPTQ ie ooba.
        updatedAt: '2023-05-02T10:11:00.362Z'
      numEdits: 0
      reactions: []
    id: 6450e1b49bff951ca132ecc5
    type: comment
  author: TheBloke
  content: Did you guys try `alpaca-lora-65B-GPTQ-4bit-128g.no-act-order.safetensors`
    ? That was the file I made for old GPTQ ie ooba.
  created_at: 2023-05-02 09:11:00+00:00
  edited: false
  hidden: false
  id: 6450e1b49bff951ca132ecc5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-02T11:41:30.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Yea.. it did not work. That is what I downloaded. If you made it
          with the current cuda branch...</p>

          '
        raw: Yea.. it did not work. That is what I downloaded. If you made it with
          the current cuda branch...
        updatedAt: '2023-05-02T11:41:30.404Z'
      numEdits: 0
      reactions: []
    id: 6450f6ea9bff951ca134c90e
    type: comment
  author: autobots
  content: Yea.. it did not work. That is what I downloaded. If you made it with the
    current cuda branch...
  created_at: 2023-05-02 10:41:30+00:00
  edited: false
  hidden: false
  id: 6450f6ea9bff951ca134c90e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-02T21:36:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What exactly is the issue? the no-act-order models I make normally
          work in ooba GPTQ</p>

          <p>I will do some testing tomorrow.</p>

          '
        raw: 'What exactly is the issue? the no-act-order models I make normally work
          in ooba GPTQ


          I will do some testing tomorrow.'
        updatedAt: '2023-05-02T21:36:50.029Z'
      numEdits: 0
      reactions: []
    id: 645182725fb40b9f50b52d32
    type: comment
  author: TheBloke
  content: 'What exactly is the issue? the no-act-order models I make normally work
    in ooba GPTQ


    I will do some testing tomorrow.'
  created_at: 2023-05-02 20:36:50+00:00
  edited: false
  hidden: false
  id: 645182725fb40b9f50b52d32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-03T12:09:27.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>It doesn''t load and gives me a state dict error until I use the
          newest cuda branch.</p>

          '
        raw: It doesn't load and gives me a state dict error until I use the newest
          cuda branch.
        updatedAt: '2023-05-03T12:09:27.759Z'
      numEdits: 0
      reactions: []
    id: 64524ef7ea756598f354f8b0
    type: comment
  author: autobots
  content: It doesn't load and gives me a state dict error until I use the newest
    cuda branch.
  created_at: 2023-05-03 11:09:27+00:00
  edited: false
  hidden: false
  id: 64524ef7ea756598f354f8b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-03T13:44:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah, you must be using CPU offload.  Yes I''ve seen that problem
          with pre_layer specifically.  I will look into it</p>

          '
        raw: Ah, you must be using CPU offload.  Yes I've seen that problem with pre_layer
          specifically.  I will look into it
        updatedAt: '2023-05-03T13:44:37.105Z'
      numEdits: 0
      reactions: []
    id: 64526545a0c0a664a23f886b
    type: comment
  author: TheBloke
  content: Ah, you must be using CPU offload.  Yes I've seen that problem with pre_layer
    specifically.  I will look into it
  created_at: 2023-05-03 12:44:37+00:00
  edited: false
  hidden: false
  id: 64526545a0c0a664a23f886b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45dc2b89009a8673a5d84bb2adb93614.svg
      fullname: Alain Rossmann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alain401
      type: user
    createdAt: '2023-05-03T17:28:02.000Z'
    data:
      edited: false
      editors:
      - alain401
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45dc2b89009a8673a5d84bb2adb93614.svg
          fullname: Alain Rossmann
          isHf: false
          isPro: false
          name: alain401
          type: user
        html: '<p>Was able to run the model on 2 GPUS, 24GB each by using --gpu-memory
          17 17.<br>Works well until context is about 1.1K tokens, then runs out of
          memory.</p>

          '
        raw: "Was able to run the model on 2 GPUS, 24GB each by using --gpu-memory\
          \ 17 17. \nWorks well until context is about 1.1K tokens, then runs out\
          \ of memory."
        updatedAt: '2023-05-03T17:28:02.768Z'
      numEdits: 0
      reactions: []
    id: 645299a28fe6558e327e9b09
    type: comment
  author: alain401
  content: "Was able to run the model on 2 GPUS, 24GB each by using --gpu-memory 17\
    \ 17. \nWorks well until context is about 1.1K tokens, then runs out of memory."
  created_at: 2023-05-03 16:28:02+00:00
  edited: false
  hidden: false
  id: 645299a28fe6558e327e9b09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-04T20:12:49.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Nope.. no offload. P40/3090 :) I''ll try it with autogptq and see
          if I get better perf now that it''s fixed.</p>

          '
        raw: Nope.. no offload. P40/3090 :) I'll try it with autogptq and see if I
          get better perf now that it's fixed.
        updatedAt: '2023-05-04T20:12:49.606Z'
      numEdits: 0
      reactions: []
    id: 645411c168cbb276cb570690
    type: comment
  author: autobots
  content: Nope.. no offload. P40/3090 :) I'll try it with autogptq and see if I get
    better perf now that it's fixed.
  created_at: 2023-05-04 19:12:49+00:00
  edited: false
  hidden: false
  id: 645411c168cbb276cb570690
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-04T20:38:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Are you splitting it across GPUs though? Maybe that causes the same
          issue as CPU offload. Ie not all on the same device</p>

          <p>Yeah as you''re into AutoGPTQ now just try that instead, and let me know</p>

          '
        raw: 'Are you splitting it across GPUs though? Maybe that causes the same
          issue as CPU offload. Ie not all on the same device


          Yeah as you''re into AutoGPTQ now just try that instead, and let me know'
        updatedAt: '2023-05-04T20:38:26.699Z'
      numEdits: 0
      reactions: []
    id: 645417c2b8c58783d66b3380
    type: comment
  author: TheBloke
  content: 'Are you splitting it across GPUs though? Maybe that causes the same issue
    as CPU offload. Ie not all on the same device


    Yeah as you''re into AutoGPTQ now just try that instead, and let me know'
  created_at: 2023-05-04 19:38:26+00:00
  edited: false
  hidden: false
  id: 645417c2b8c58783d66b3380
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-06T10:37:40.000Z'
    data:
      edited: true
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>I am splitting it.. I got it running now. I wonder if it was old
          instances of gptq_llama being installed. I <em>think</em> I can only do
          half context and get about 1it/s, slightly over if I just do instruct.</p>

          <p>I''d really like to try a 1024 group version to see if it would run full
          but you only have that for triton. 3090 can use triton but P40 cannot. Autogptq
          loads but can only do very small contexts because it loads lopsided.</p>

          '
        raw: 'I am splitting it.. I got it running now. I wonder if it was old instances
          of gptq_llama being installed. I *think* I can only do half context and
          get about 1it/s, slightly over if I just do instruct.


          I''d really like to try a 1024 group version to see if it would run full
          but you only have that for triton. 3090 can use triton but P40 cannot. Autogptq
          loads but can only do very small contexts because it loads lopsided.'
        updatedAt: '2023-05-06T10:38:29.997Z'
      numEdits: 1
      reactions: []
    id: 64562df4cd6567f52fb1a6ea
    type: comment
  author: autobots
  content: 'I am splitting it.. I got it running now. I wonder if it was old instances
    of gptq_llama being installed. I *think* I can only do half context and get about
    1it/s, slightly over if I just do instruct.


    I''d really like to try a 1024 group version to see if it would run full but you
    only have that for triton. 3090 can use triton but P40 cannot. Autogptq loads
    but can only do very small contexts because it loads lopsided.'
  created_at: 2023-05-06 09:37:40+00:00
  edited: true
  hidden: false
  id: 64562df4cd6567f52fb1a6ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/alpaca-lora-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: Beware! Requires new cuda.
