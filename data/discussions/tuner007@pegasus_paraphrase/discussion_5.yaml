!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bakrianoo
conflicting_files: null
created_at: 2022-08-24 13:15:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612105632679-noauth.jpeg?w=200&h=200&f=face
      fullname: Abu Bakr Soliman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bakrianoo
      type: user
    createdAt: '2022-08-24T14:15:17.000Z'
    data:
      edited: false
      editors:
      - bakrianoo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612105632679-noauth.jpeg?w=200&h=200&f=face
          fullname: Abu Bakr Soliman
          isHf: false
          isPro: false
          name: bakrianoo
          type: user
        html: "<p>Hi</p>\n<p>I am wondering if it can support using the new XLA text\
          \ generation.</p>\n<p>I followed this blog: <a href=\"https://huggingface.co/blog/tf-xla-generate\"\
          >https://huggingface.co/blog/tf-xla-generate</a></p>\n<p>And used the following\
          \ code</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForSeq2SeqLM\
          \ , AutoTokenizer\n<span class=\"hljs-keyword\">import</span> tensorflow\
          \ <span class=\"hljs-keyword\">as</span> tf\n\n<span class=\"hljs-keyword\"\
          >from</span> tensorflow.python.ops.numpy_ops <span class=\"hljs-keyword\"\
          >import</span> np_config\nnp_config.enable_numpy_behavior()\n\nph_model_name\
          \ = <span class=\"hljs-string\">\"tuner007/pegasus_paraphrase\"</span>\n\
          \n<span class=\"hljs-comment\"># torch_device = \"cuda:0\"</span>\nph_tokenizer\
          \ = AutoTokenizer.from_pretrained(ph_model_name)\nph_model = AutoModelForSeq2SeqLM.from_pretrained(ph_model_name)\n\
          \ntokenization_kwargs = {<span class=\"hljs-string\">\"max_length\"</span>:\
          \ <span class=\"hljs-number\">512</span>, <span class=\"hljs-string\">\"\
          padding\"</span>: <span class=\"hljs-string\">\"longest\"</span>, <span\
          \ class=\"hljs-string\">\"truncation\"</span>: <span class=\"hljs-literal\"\
          >True</span>, <span class=\"hljs-string\">\"return_tensors\"</span>: <span\
          \ class=\"hljs-string\">\"tf\"</span>}\ngeneration_kwargs = {<span class=\"\
          hljs-string\">\"num_beams\"</span>: <span class=\"hljs-number\">7</span>,\
          \ <span class=\"hljs-string\">\"max_length\"</span>: <span class=\"hljs-number\"\
          >512</span>,\n                     <span class=\"hljs-string\">\"num_return_sequences\"\
          </span>:<span class=\"hljs-number\">2</span>, <span class=\"hljs-string\"\
          >\"temperature\"</span>:<span class=\"hljs-number\">0.7</span>,\n      \
          \               <span class=\"hljs-string\">\"do_sample\"</span>: <span\
          \ class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"top_k\"\
          </span>: <span class=\"hljs-number\">90</span>, <span class=\"hljs-string\"\
          >\"top_p\"</span>: <span class=\"hljs-number\">0.95</span>,\n          \
          \           <span class=\"hljs-string\">\"no_repeat_ngram_size\"</span>:\
          \ <span class=\"hljs-number\">2</span>, <span class=\"hljs-string\">\"early_stopping\"\
          </span>: <span class=\"hljs-literal\">True</span>}\n\n<span class=\"hljs-comment\"\
          ># generate a paraphrased text</span>\nxla_generate = tf.function(ph_model.generate,\
          \ jit_compile=<span class=\"hljs-literal\">True</span>)\n\ninput_prompt\
          \ = <span class=\"hljs-string\">'the world has been inching toward fully\
          \ autonomous cars for years .'</span>\ntokenized_inputs = ph_tokenizer([input_prompt],\
          \ **tokenization_kwargs)\n\ngenerated_text = xla_generate(**tokenized_inputs,\
          \ **generation_kwargs)\ndecoded_text = ph_tokenizer.decode(generated_text[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>)\n\n<span class=\"hljs-built_in\">print</span>(decoded_text)\n\
          </code></pre>\n<p>but got this error message</p>\n<pre><code>TypeError \
          \                                Traceback (most recent call last)\n&lt;ipython-input-8-a25ee9d320ec&gt;\
          \ in &lt;module&gt;\n      1 input_prompt = 'the world has been inching\
          \ toward fully autonomous cars for years .'\n      2 tokenized_inputs =\
          \ ph_tokenizer([input_prompt], **tokenization_kwargs)\n----&gt; 3 generated_text\
          \ = xla_generate(**tokenized_inputs, **generation_kwargs)\n      4 decoded_text\
          \ = ph_tokenizer.decode(generated_text[0], skip_special_tokens=True)\n \
          \     5 print(decoded_text)\n\n1 frames\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\
          \ in autograph_handler(*args, **kwargs)\n   1145           except Exception\
          \ as e:  # pylint:disable=broad-except\n   1146             if hasattr(e,\
          \ \"ag_error_metadata\"):\n-&gt; 1147               raise e.ag_error_metadata.to_exception(e)\n\
          \   1148             else:\n   1149               raise\n\nTypeError: in\
          \ user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\"\
          , line 847, in decorate_context  *\n        return func(*args, **kwargs)\n\
          \    File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\"\
          , line 1182, in generate  *\n        model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\
          \    File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\"\
          , line 525, in _prepare_encoder_decoder_kwargs_for_generation  *\n     \
          \   model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n\
          \    File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl  *\n        return forward_call(*input, **kwargs)\n\
          \    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/pegasus/modeling_pegasus.py\"\
          , line 753, in forward  *\n        input_shape = input_ids.size()\n\n  \
          \  TypeError: 'numpy.int64' object is not callable\n</code></pre>\n<p>Any\
          \ help?</p>\n"
        raw: "Hi\r\n\r\nI am wondering if it can support using the new XLA text generation.\r\
          \n\r\nI followed this blog: https://huggingface.co/blog/tf-xla-generate\r\
          \n\r\nAnd used the following code\r\n\r\n```python\r\nfrom transformers\
          \ import AutoModelForSeq2SeqLM , AutoTokenizer\r\nimport tensorflow as tf\r\
          \n\r\nfrom tensorflow.python.ops.numpy_ops import np_config\r\nnp_config.enable_numpy_behavior()\r\
          \n\r\nph_model_name = \"tuner007/pegasus_paraphrase\"\r\n\r\n# torch_device\
          \ = \"cuda:0\"\r\nph_tokenizer = AutoTokenizer.from_pretrained(ph_model_name)\r\
          \nph_model = AutoModelForSeq2SeqLM.from_pretrained(ph_model_name)\r\n\r\n\
          tokenization_kwargs = {\"max_length\": 512, \"padding\": \"longest\", \"\
          truncation\": True, \"return_tensors\": \"tf\"}\r\ngeneration_kwargs = {\"\
          num_beams\": 7, \"max_length\": 512,\r\n                     \"num_return_sequences\"\
          :2, \"temperature\":0.7,\r\n                     \"do_sample\": True, \"\
          top_k\": 90, \"top_p\": 0.95,\r\n                     \"no_repeat_ngram_size\"\
          : 2, \"early_stopping\": True}\r\n\r\n# generate a paraphrased text\r\n\
          xla_generate = tf.function(ph_model.generate, jit_compile=True)\r\n\r\n\
          input_prompt = 'the world has been inching toward fully autonomous cars\
          \ for years .'\r\ntokenized_inputs = ph_tokenizer([input_prompt], **tokenization_kwargs)\r\
          \n\r\ngenerated_text = xla_generate(**tokenized_inputs, **generation_kwargs)\r\
          \ndecoded_text = ph_tokenizer.decode(generated_text[0], skip_special_tokens=True)\r\
          \n\r\nprint(decoded_text)\r\n```\r\n\r\nbut got this error message\r\n\r\
          \n```\r\nTypeError                                 Traceback (most recent\
          \ call last)\r\n<ipython-input-8-a25ee9d320ec> in <module>\r\n      1 input_prompt\
          \ = 'the world has been inching toward fully autonomous cars for years .'\r\
          \n      2 tokenized_inputs = ph_tokenizer([input_prompt], **tokenization_kwargs)\r\
          \n----> 3 generated_text = xla_generate(**tokenized_inputs, **generation_kwargs)\r\
          \n      4 decoded_text = ph_tokenizer.decode(generated_text[0], skip_special_tokens=True)\r\
          \n      5 print(decoded_text)\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\
          \ in autograph_handler(*args, **kwargs)\r\n   1145           except Exception\
          \ as e:  # pylint:disable=broad-except\r\n   1146             if hasattr(e,\
          \ \"ag_error_metadata\"):\r\n-> 1147               raise e.ag_error_metadata.to_exception(e)\r\
          \n   1148             else:\r\n   1149               raise\r\n\r\nTypeError:\
          \ in user code:\r\n\r\n    File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\"\
          , line 847, in decorate_context  *\r\n        return func(*args, **kwargs)\r\
          \n    File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\"\
          , line 1182, in generate  *\r\n        model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\r\
          \n    File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\"\
          , line 525, in _prepare_encoder_decoder_kwargs_for_generation  *\r\n   \
          \     model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\r\
          \n    File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl  *\r\n        return forward_call(*input, **kwargs)\r\
          \n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/pegasus/modeling_pegasus.py\"\
          , line 753, in forward  *\r\n        input_shape = input_ids.size()\r\n\r\
          \n    TypeError: 'numpy.int64' object is not callable\r\n```\r\n\r\nAny\
          \ help?"
        updatedAt: '2022-08-24T14:15:17.361Z'
      numEdits: 0
      reactions: []
    id: 63063275cfbde33ef7d817df
    type: comment
  author: bakrianoo
  content: "Hi\r\n\r\nI am wondering if it can support using the new XLA text generation.\r\
    \n\r\nI followed this blog: https://huggingface.co/blog/tf-xla-generate\r\n\r\n\
    And used the following code\r\n\r\n```python\r\nfrom transformers import AutoModelForSeq2SeqLM\
    \ , AutoTokenizer\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.ops.numpy_ops\
    \ import np_config\r\nnp_config.enable_numpy_behavior()\r\n\r\nph_model_name =\
    \ \"tuner007/pegasus_paraphrase\"\r\n\r\n# torch_device = \"cuda:0\"\r\nph_tokenizer\
    \ = AutoTokenizer.from_pretrained(ph_model_name)\r\nph_model = AutoModelForSeq2SeqLM.from_pretrained(ph_model_name)\r\
    \n\r\ntokenization_kwargs = {\"max_length\": 512, \"padding\": \"longest\", \"\
    truncation\": True, \"return_tensors\": \"tf\"}\r\ngeneration_kwargs = {\"num_beams\"\
    : 7, \"max_length\": 512,\r\n                     \"num_return_sequences\":2,\
    \ \"temperature\":0.7,\r\n                     \"do_sample\": True, \"top_k\"\
    : 90, \"top_p\": 0.95,\r\n                     \"no_repeat_ngram_size\": 2, \"\
    early_stopping\": True}\r\n\r\n# generate a paraphrased text\r\nxla_generate =\
    \ tf.function(ph_model.generate, jit_compile=True)\r\n\r\ninput_prompt = 'the\
    \ world has been inching toward fully autonomous cars for years .'\r\ntokenized_inputs\
    \ = ph_tokenizer([input_prompt], **tokenization_kwargs)\r\n\r\ngenerated_text\
    \ = xla_generate(**tokenized_inputs, **generation_kwargs)\r\ndecoded_text = ph_tokenizer.decode(generated_text[0],\
    \ skip_special_tokens=True)\r\n\r\nprint(decoded_text)\r\n```\r\n\r\nbut got this\
    \ error message\r\n\r\n```\r\nTypeError                                 Traceback\
    \ (most recent call last)\r\n<ipython-input-8-a25ee9d320ec> in <module>\r\n  \
    \    1 input_prompt = 'the world has been inching toward fully autonomous cars\
    \ for years .'\r\n      2 tokenized_inputs = ph_tokenizer([input_prompt], **tokenization_kwargs)\r\
    \n----> 3 generated_text = xla_generate(**tokenized_inputs, **generation_kwargs)\r\
    \n      4 decoded_text = ph_tokenizer.decode(generated_text[0], skip_special_tokens=True)\r\
    \n      5 print(decoded_text)\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\
    \ in autograph_handler(*args, **kwargs)\r\n   1145           except Exception\
    \ as e:  # pylint:disable=broad-except\r\n   1146             if hasattr(e, \"\
    ag_error_metadata\"):\r\n-> 1147               raise e.ag_error_metadata.to_exception(e)\r\
    \n   1148             else:\r\n   1149               raise\r\n\r\nTypeError: in\
    \ user code:\r\n\r\n    File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\"\
    , line 847, in decorate_context  *\r\n        return func(*args, **kwargs)\r\n\
    \    File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\"\
    , line 1182, in generate  *\r\n        model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\r\
    \n    File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\"\
    , line 525, in _prepare_encoder_decoder_kwargs_for_generation  *\r\n        model_kwargs[\"\
    encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl  *\r\n        return forward_call(*input, **kwargs)\r\
    \n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/pegasus/modeling_pegasus.py\"\
    , line 753, in forward  *\r\n        input_shape = input_ids.size()\r\n\r\n  \
    \  TypeError: 'numpy.int64' object is not callable\r\n```\r\n\r\nAny help?"
  created_at: 2022-08-24 13:15:17+00:00
  edited: false
  hidden: false
  id: 63063275cfbde33ef7d817df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: tuner007/pegasus_paraphrase
repo_type: model
status: open
target_branch: null
title: Does it support fast XLA text generation ?
