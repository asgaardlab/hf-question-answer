!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Talha
conflicting_files: null
created_at: 2023-07-25 15:52:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
      fullname: Talha Anwar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Talha
      type: user
    createdAt: '2023-07-25T16:52:17.000Z'
    data:
      edited: true
      editors:
      - Talha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8540967106819153
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
          fullname: Talha Anwar
          isHf: false
          isPro: false
          name: Talha
          type: user
        html: '<p>Hi, first of all thanks. I have some confusion, can you please clear
          that</p>

          <ol>

          <li>Did you use the <a rel="nofollow" href="https://github.com/artidoro/qlora/tree/main/examples">jupyter
          notebook </a> or the <a rel="nofollow" href="https://github.com/artidoro/qlora/blob/main/scripts/finetune_guanaco_7b.sh">script</a></li>

          <li>How did you merge the adapter. Can you please share the script. </li>

          <li>What is GPU memory and system memory to required to fine tune the model
          and how much time it took<br>Thanks</li>

          </ol>

          '
        raw: "Hi, first of all thanks. I have some confusion, can you please clear\
          \ that\n1. Did you use the [jupyter notebook ](https://github.com/artidoro/qlora/tree/main/examples)\
          \ or the [script](https://github.com/artidoro/qlora/blob/main/scripts/finetune_guanaco_7b.sh)\n\
          2. How did you merge the adapter. Can you please share the script. \n3.\
          \ What is GPU memory and system memory to required to fine tune the model\
          \ and how much time it took\nThanks "
        updatedAt: '2023-07-25T16:52:30.655Z'
      numEdits: 1
      reactions: []
    id: 64bffdc1ccb326c91832eb4f
    type: comment
  author: Talha
  content: "Hi, first of all thanks. I have some confusion, can you please clear that\n\
    1. Did you use the [jupyter notebook ](https://github.com/artidoro/qlora/tree/main/examples)\
    \ or the [script](https://github.com/artidoro/qlora/blob/main/scripts/finetune_guanaco_7b.sh)\n\
    2. How did you merge the adapter. Can you please share the script. \n3. What is\
    \ GPU memory and system memory to required to fine tune the model and how much\
    \ time it took\nThanks "
  created_at: 2023-07-25 15:52:17+00:00
  edited: true
  hidden: false
  id: 64bffdc1ccb326c91832eb4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-07-25T17:55:15.000Z'
    data:
      edited: true
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9715328216552734
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: '<ol>

          <li>I used the script. The notebook is only for inference as far as I can
          tell.</li>

          <li>I used a script TheBloke posted quite a while ago. <a href="https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/2#647062b2806c7d87fa11a19c">Here</a>
          is a link to the comment where he shared the script.</li>

          <li>For the 7b model it only required around 8GB of VRAM and 5GB of RAM.
          The time will depend very heavily on what GPU you use. I ran it on a rented
          4090 and it took around 2 hours. I also trained the 13b model on the 4090
          but I can''t recall the exact amount of VRAM that required, that took around
          6 hours to run. For the 70b model I used an A100 mostly due to the speed
          advantage it gave, though even with that it took around 38 hours to run.</li>

          </ol>

          '
        raw: '1. I used the script. The notebook is only for inference as far as I
          can tell.

          2. I used a script TheBloke posted quite a while ago. [Here](https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/2#647062b2806c7d87fa11a19c)
          is a link to the comment where he shared the script.

          3. For the 7b model it only required around 8GB of VRAM and 5GB of RAM.
          The time will depend very heavily on what GPU you use. I ran it on a rented
          4090 and it took around 2 hours. I also trained the 13b model on the 4090
          but I can''t recall the exact amount of VRAM that required, that took around
          6 hours to run. For the 70b model I used an A100 mostly due to the speed
          advantage it gave, though even with that it took around 38 hours to run.

          '
        updatedAt: '2023-07-25T17:55:28.370Z'
      numEdits: 1
      reactions: []
    id: 64c00c83b019382211271e4d
    type: comment
  author: Mikael110
  content: '1. I used the script. The notebook is only for inference as far as I can
    tell.

    2. I used a script TheBloke posted quite a while ago. [Here](https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/2#647062b2806c7d87fa11a19c)
    is a link to the comment where he shared the script.

    3. For the 7b model it only required around 8GB of VRAM and 5GB of RAM. The time
    will depend very heavily on what GPU you use. I ran it on a rented 4090 and it
    took around 2 hours. I also trained the 13b model on the 4090 but I can''t recall
    the exact amount of VRAM that required, that took around 6 hours to run. For the
    70b model I used an A100 mostly due to the speed advantage it gave, though even
    with that it took around 38 hours to run.

    '
  created_at: 2023-07-25 16:55:15+00:00
  edited: true
  hidden: false
  id: 64c00c83b019382211271e4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
      fullname: Talha Anwar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Talha
      type: user
    createdAt: '2023-07-25T18:17:58.000Z'
    data:
      edited: false
      editors:
      - Talha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9499396085739136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
          fullname: Talha Anwar
          isHf: false
          isPro: false
          name: Talha
          type: user
        html: '<p>Thanks  a lot, i will try and see if i can produce some thing. </p>

          '
        raw: 'Thanks  a lot, i will try and see if i can produce some thing. '
        updatedAt: '2023-07-25T18:17:58.326Z'
      numEdits: 0
      reactions: []
    id: 64c011d62dd62e8bb0fc9df6
    type: comment
  author: Talha
  content: 'Thanks  a lot, i will try and see if i can produce some thing. '
  created_at: 2023-07-25 17:17:58+00:00
  edited: false
  hidden: false
  id: 64c011d62dd62e8bb0fc9df6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-07-25T18:35:50.000Z'
    data:
      edited: false
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.943672776222229
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: '<p>Good luck. Though if you are training your own model I''d recommend
          looking into more polished training tools like <a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a>
          (which supports resuming training) or <a rel="nofollow" href="https://github.com/h2oai/h2o-llmstudio">H2O
          LLM Studio</a> (which has a relatively easy to use GUI). The main reason
          I used the QLoRA script directly was that I wanted to follow the original
          Guanaco training as closely as possible.</p>

          '
        raw: Good luck. Though if you are training your own model I'd recommend looking
          into more polished training tools like [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
          (which supports resuming training) or [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio)
          (which has a relatively easy to use GUI). The main reason I used the QLoRA
          script directly was that I wanted to follow the original Guanaco training
          as closely as possible.
        updatedAt: '2023-07-25T18:35:50.431Z'
      numEdits: 0
      reactions: []
    id: 64c016060a4d02f37a849cbb
    type: comment
  author: Mikael110
  content: Good luck. Though if you are training your own model I'd recommend looking
    into more polished training tools like [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
    (which supports resuming training) or [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio)
    (which has a relatively easy to use GUI). The main reason I used the QLoRA script
    directly was that I wanted to follow the original Guanaco training as closely
    as possible.
  created_at: 2023-07-25 17:35:50+00:00
  edited: false
  hidden: false
  id: 64c016060a4d02f37a849cbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
      fullname: Talha Anwar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Talha
      type: user
    createdAt: '2023-07-25T19:01:54.000Z'
    data:
      edited: false
      editors:
      - Talha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.993133008480072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
          fullname: Talha Anwar
          isHf: false
          isPro: false
          name: Talha
          type: user
        html: '<p>thanks, i will look into both, i was not aware of any. </p>

          '
        raw: 'thanks, i will look into both, i was not aware of any. '
        updatedAt: '2023-07-25T19:01:54.536Z'
      numEdits: 0
      reactions: []
    id: 64c01c2250f0d6578db56976
    type: comment
  author: Talha
  content: 'thanks, i will look into both, i was not aware of any. '
  created_at: 2023-07-25 18:01:54+00:00
  edited: false
  hidden: false
  id: 64c01c2250f0d6578db56976
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Mikael110/llama-2-7b-guanaco-fp16
repo_type: model
status: open
target_branch: null
title: Just couple of confusion to clear
