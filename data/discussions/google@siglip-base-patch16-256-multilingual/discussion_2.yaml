!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JHXia
conflicting_files: null
created_at: 2024-01-20 03:47:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3fa11759258f144282554e7f3cfa090d.svg
      fullname: Jiahao Xia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JHXia
      type: user
    createdAt: '2024-01-20T03:47:38.000Z'
    data:
      edited: false
      editors:
      - JHXia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7601035833358765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3fa11759258f144282554e7f3cfa090d.svg
          fullname: Jiahao Xia
          isHf: false
          isPro: false
          name: JHXia
          type: user
        html: "<p>Hi, i install transformers from source just as the notebook said,<br>when\
          \ I run the demo code</p>\n<pre><code class=\"language-python\">model =\
          \ AutoModel.from_pretrained(<span class=\"hljs-string\">\"google/siglip-base-patch16-256-i18n\"\
          </span>)\nprocessor = AutoProcessor.from_pretrained(<span class=\"hljs-string\"\
          >\"google/siglip-base-patch16-256-i18n\"</span>)\n</code></pre>\n<p>part\
          \ of the error information</p>\n<pre><code class=\"language-bash\">size\
          \ mismatch <span class=\"hljs-keyword\">for</span> text_model.final_layer_norm.weight:\
          \ copying a param with shape torch.Size([768]) from checkpoint, the shape\
          \ <span class=\"hljs-keyword\">in</span> current model is torch.Size([512]).\n\
          \        size mismatch <span class=\"hljs-keyword\">for</span> text_model.final_layer_norm.bias:\
          \ copying a param with shape torch.Size([768]) from checkpoint, the shape\
          \ <span class=\"hljs-keyword\">in</span> current model is torch.Size([512]).\n\
          \        size mismatch <span class=\"hljs-keyword\">for</span> vision_model.embeddings.patch_embedding.weight:\
          \ copying a param with shape torch.Size([768, 3, 16, 16]) from checkpoint,\
          \ the shape <span class=\"hljs-keyword\">in</span> current model is torch.Size([768,\
          \ 3, 32, 32]).\n        size mismatch <span class=\"hljs-keyword\">for</span>\
          \ vision_model.embeddings.position_embedding.weight: copying a param with\
          \ shape torch.Size([256, 768]) from checkpoint, the shape <span class=\"\
          hljs-keyword\">in</span> current model is torch.Size([65, 768]).\n     \
          \   You may consider adding `ignore_mismatched_sizes=True` <span class=\"\
          hljs-keyword\">in</span> the model `from_pretrained` method.\n</code></pre>\n\
          <p>it doesn't work after adding <code>ignore_mismatched_sizes=True</code>.</p>\n"
        raw: "Hi, i install transformers from source just as the notebook said,\r\n\
          when I run the demo code\r\n```python\r\nmodel = AutoModel.from_pretrained(\"\
          google/siglip-base-patch16-256-i18n\")\r\nprocessor = AutoProcessor.from_pretrained(\"\
          google/siglip-base-patch16-256-i18n\")\r\n```\r\n\r\npart of the error information\r\
          \n```bash\r\nsize mismatch for text_model.final_layer_norm.weight: copying\
          \ a param with shape torch.Size([768]) from checkpoint, the shape in current\
          \ model is torch.Size([512]).\r\n        size mismatch for text_model.final_layer_norm.bias:\
          \ copying a param with shape torch.Size([768]) from checkpoint, the shape\
          \ in current model is torch.Size([512]).\r\n        size mismatch for vision_model.embeddings.patch_embedding.weight:\
          \ copying a param with shape torch.Size([768, 3, 16, 16]) from checkpoint,\
          \ the shape in current model is torch.Size([768, 3, 32, 32]).\r\n      \
          \  size mismatch for vision_model.embeddings.position_embedding.weight:\
          \ copying a param with shape torch.Size([256, 768]) from checkpoint, the\
          \ shape in current model is torch.Size([65, 768]).\r\n        You may consider\
          \ adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\r\
          \n```\r\nit doesn't work after adding `ignore_mismatched_sizes=True`.\r\n\
          \r\n"
        updatedAt: '2024-01-20T03:47:38.343Z'
      numEdits: 0
      reactions: []
    id: 65ab425a043d53781a0c6354
    type: comment
  author: JHXia
  content: "Hi, i install transformers from source just as the notebook said,\r\n\
    when I run the demo code\r\n```python\r\nmodel = AutoModel.from_pretrained(\"\
    google/siglip-base-patch16-256-i18n\")\r\nprocessor = AutoProcessor.from_pretrained(\"\
    google/siglip-base-patch16-256-i18n\")\r\n```\r\n\r\npart of the error information\r\
    \n```bash\r\nsize mismatch for text_model.final_layer_norm.weight: copying a param\
    \ with shape torch.Size([768]) from checkpoint, the shape in current model is\
    \ torch.Size([512]).\r\n        size mismatch for text_model.final_layer_norm.bias:\
    \ copying a param with shape torch.Size([768]) from checkpoint, the shape in current\
    \ model is torch.Size([512]).\r\n        size mismatch for vision_model.embeddings.patch_embedding.weight:\
    \ copying a param with shape torch.Size([768, 3, 16, 16]) from checkpoint, the\
    \ shape in current model is torch.Size([768, 3, 32, 32]).\r\n        size mismatch\
    \ for vision_model.embeddings.position_embedding.weight: copying a param with\
    \ shape torch.Size([256, 768]) from checkpoint, the shape in current model is\
    \ torch.Size([65, 768]).\r\n        You may consider adding `ignore_mismatched_sizes=True`\
    \ in the model `from_pretrained` method.\r\n```\r\nit doesn't work after adding\
    \ `ignore_mismatched_sizes=True`.\r\n\r\n"
  created_at: 2024-01-20 03:47:38+00:00
  edited: false
  hidden: false
  id: 65ab425a043d53781a0c6354
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: google/siglip-base-patch16-256-multilingual
repo_type: model
status: open
target_branch: null
title: size mismatch error
