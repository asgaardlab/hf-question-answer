!!python/object:huggingface_hub.community.DiscussionWithDetails
author: underlines
conflicting_files: null
created_at: 2023-04-05 19:48:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2023-04-05T20:48:31.000Z'
    data:
      edited: true
      editors:
      - underlines
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
          fullname: Jan Badertscher
          isHf: false
          isPro: false
          name: underlines
          type: user
        html: '<h1 id="reproduction">Reproduction</h1>

          <p>I am loading it with the latest text-generation-webui from 2 hours ago,
          with the revamped --chat that supports MODE: instruction.<br>In this mode
          you can select an INSTRUCTION TEMPLATE in the UI, for which I created a
          yaml file in \characters\instruction-following\ according to the example
          file Alpaca.yaml:</p>

          <pre><code>name: "### Response:"

          your_name: "### Instruction:"

          context: "Below is an instruction that describes a task. Write a response
          that appropriately completes the request."

          </code></pre>

          <p>for which I derived the one for Vicuna.yaml</p>

          <pre><code>name: "### Assistant:"

          your_name: "### Human:"

          context: "Below is an instruction that describes a task. Write a response
          that appropriately completes the request."

          </code></pre>

          <p>And started with </p>

          <pre><code>python server.py --model elinas_vicuna-13b-4bit --w

          bits 4 --groupsize 128 --chat --listen

          </code></pre>

          <p>Switched to Mode: Instruction and selected the Vicuna template.</p>

          <h1 id="issue">Issue</h1>

          <p>Asking it to follow instructions, it answers not only as the Assistant,
          but hallucinates further Human instructions and Assistant replies.</p>

          <h1 id="questions">Questions</h1>

          <ul>

          <li>Is this a problem with EOS token, like <a href="https://huggingface.co/eachadea/ggml-vicuna-13b-4bit/commit/58d5dd4ada832e4656dbc5270ba136c47e7f1606">this</a>
          commit?</li>

          <li>Or is this a text-generation-webui issue?</li>

          <li>Or a problem with the VIcuna.yaml I created?</li>

          </ul>

          <h1 id="example">Example:</h1>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63175a0adc97a974718fe704/UFDEDk_rtYOdLSvCIZEQL.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63175a0adc97a974718fe704/UFDEDk_rtYOdLSvCIZEQL.png"></a></p>

          '
        raw: "# Reproduction\nI am loading it with the latest text-generation-webui\
          \ from 2 hours ago, with the revamped --chat that supports MODE: instruction.\n\
          In this mode you can select an INSTRUCTION TEMPLATE in the UI, for which\
          \ I created a yaml file in \\characters\\instruction-following\\ according\
          \ to the example file Alpaca.yaml:\n\n```\nname: \"### Response:\"\nyour_name:\
          \ \"### Instruction:\"\ncontext: \"Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\"\n\
          ```\n\nfor which I derived the one for Vicuna.yaml\n\n```\nname: \"### Assistant:\"\
          \nyour_name: \"### Human:\"\ncontext: \"Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\"\n\
          ```\n\nAnd started with \n```\npython server.py --model elinas_vicuna-13b-4bit\
          \ --w\nbits 4 --groupsize 128 --chat --listen\n```\n\nSwitched to Mode:\
          \ Instruction and selected the Vicuna template.\n\n# Issue\n\nAsking it\
          \ to follow instructions, it answers not only as the Assistant, but hallucinates\
          \ further Human instructions and Assistant replies.\n\n# Questions\n\n-\
          \ Is this a problem with EOS token, like [this](https://huggingface.co/eachadea/ggml-vicuna-13b-4bit/commit/58d5dd4ada832e4656dbc5270ba136c47e7f1606)\
          \ commit?\n- Or is this a text-generation-webui issue?\n- Or a problem with\
          \ the VIcuna.yaml I created?\n\n# Example:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63175a0adc97a974718fe704/UFDEDk_rtYOdLSvCIZEQL.png)"
        updatedAt: '2023-04-05T20:50:32.545Z'
      numEdits: 2
      reactions: []
    id: 642dde9f018d9a57ca511c3c
    type: comment
  author: underlines
  content: "# Reproduction\nI am loading it with the latest text-generation-webui\
    \ from 2 hours ago, with the revamped --chat that supports MODE: instruction.\n\
    In this mode you can select an INSTRUCTION TEMPLATE in the UI, for which I created\
    \ a yaml file in \\characters\\instruction-following\\ according to the example\
    \ file Alpaca.yaml:\n\n```\nname: \"### Response:\"\nyour_name: \"### Instruction:\"\
    \ncontext: \"Below is an instruction that describes a task. Write a response that\
    \ appropriately completes the request.\"\n```\n\nfor which I derived the one for\
    \ Vicuna.yaml\n\n```\nname: \"### Assistant:\"\nyour_name: \"### Human:\"\ncontext:\
    \ \"Below is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\"\n```\n\nAnd started with \n```\npython server.py --model\
    \ elinas_vicuna-13b-4bit --w\nbits 4 --groupsize 128 --chat --listen\n```\n\n\
    Switched to Mode: Instruction and selected the Vicuna template.\n\n# Issue\n\n\
    Asking it to follow instructions, it answers not only as the Assistant, but hallucinates\
    \ further Human instructions and Assistant replies.\n\n# Questions\n\n- Is this\
    \ a problem with EOS token, like [this](https://huggingface.co/eachadea/ggml-vicuna-13b-4bit/commit/58d5dd4ada832e4656dbc5270ba136c47e7f1606)\
    \ commit?\n- Or is this a text-generation-webui issue?\n- Or a problem with the\
    \ VIcuna.yaml I created?\n\n# Example:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63175a0adc97a974718fe704/UFDEDk_rtYOdLSvCIZEQL.png)"
  created_at: 2023-04-05 19:48:31+00:00
  edited: true
  hidden: false
  id: 642dde9f018d9a57ca511c3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-05T23:33:25.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>It is an EOS issue and I am currently on hold on quantizing more
          models until there is better standardization in GPTQ. The original code
          author is pushing triton, which would lock out Windows users, unless you
          use WSL + having to maintain a CUDA + triton version. It''s simply becoming
          tiring with all of these breaking changes and having to re-quantize, revert
          to previous commits, which should not be happening. </p>

          <p>As for the new chat mode, I haven''t tried it and have been taking a
          break from LLMs to work on other projects. In the "default" mode, you could
          just lower the token limit and hit continue for more info, or do the opposite
          and hit stop. </p>

          <p>Until Vicuna unfiltered comes out or something more interesting, this
          will be my last contribution until everything is more stable.</p>

          '
        raw: "It is an EOS issue and I am currently on hold on quantizing more models\
          \ until there is better standardization in GPTQ. The original code author\
          \ is pushing triton, which would lock out Windows users, unless you use\
          \ WSL + having to maintain a CUDA + triton version. It's simply becoming\
          \ tiring with all of these breaking changes and having to re-quantize, revert\
          \ to previous commits, which should not be happening. \n\nAs for the new\
          \ chat mode, I haven't tried it and have been taking a break from LLMs to\
          \ work on other projects. In the \"default\" mode, you could just lower\
          \ the token limit and hit continue for more info, or do the opposite and\
          \ hit stop. \n\nUntil Vicuna unfiltered comes out or something more interesting,\
          \ this will be my last contribution until everything is more stable."
        updatedAt: '2023-04-05T23:33:25.173Z'
      numEdits: 0
      reactions: []
    id: 642e0545baf943d5db47b32e
    type: comment
  author: elinas
  content: "It is an EOS issue and I am currently on hold on quantizing more models\
    \ until there is better standardization in GPTQ. The original code author is pushing\
    \ triton, which would lock out Windows users, unless you use WSL + having to maintain\
    \ a CUDA + triton version. It's simply becoming tiring with all of these breaking\
    \ changes and having to re-quantize, revert to previous commits, which should\
    \ not be happening. \n\nAs for the new chat mode, I haven't tried it and have\
    \ been taking a break from LLMs to work on other projects. In the \"default\"\
    \ mode, you could just lower the token limit and hit continue for more info, or\
    \ do the opposite and hit stop. \n\nUntil Vicuna unfiltered comes out or something\
    \ more interesting, this will be my last contribution until everything is more\
    \ stable."
  created_at: 2023-04-05 22:33:25+00:00
  edited: false
  hidden: false
  id: 642e0545baf943d5db47b32e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2c4b8943676230408670850a32d634d.svg
      fullname: it
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syntaxing
      type: user
    createdAt: '2023-04-10T18:37:06.000Z'
    data:
      edited: false
      editors:
      - syntaxing
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2c4b8943676230408670850a32d634d.svg
          fullname: it
          isHf: false
          isPro: false
          name: syntaxing
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;underlines&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/underlines\">@<span class=\"\
          underline\">underlines</span></a></span>\n\n\t</span></span>: Once this\
          \ PR <a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/pull/903\"\
          >https://github.com/oobabooga/text-generation-webui/pull/903</a> lands,\
          \ you should be able to use Vicuna without it talking to itself. It's been\
          \ a game changer for me (I just cloned the fork)</p>\n"
        raw: '@underlines: Once this PR <https://github.com/oobabooga/text-generation-webui/pull/903>
          lands, you should be able to use Vicuna without it talking to itself. It''s
          been a game changer for me (I just cloned the fork)'
        updatedAt: '2023-04-10T18:37:06.449Z'
      numEdits: 0
      reactions: []
    id: 64345752938d07505bb8728f
    type: comment
  author: syntaxing
  content: '@underlines: Once this PR <https://github.com/oobabooga/text-generation-webui/pull/903>
    lands, you should be able to use Vicuna without it talking to itself. It''s been
    a game changer for me (I just cloned the fork)'
  created_at: 2023-04-10 17:37:06+00:00
  edited: false
  hidden: false
  id: 64345752938d07505bb8728f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: elinas/vicuna-13b-4bit
repo_type: model
status: open
target_branch: null
title: Is not stopping after End Of answer, hallucinates the whole conversation
