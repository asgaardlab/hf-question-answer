!!python/object:huggingface_hub.community.DiscussionWithDetails
author: riosje
conflicting_files: null
created_at: 2023-06-05 17:40:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/193ed76a243d0ee158774289bb170384.svg
      fullname: jefferson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: riosje
      type: user
    createdAt: '2023-06-05T18:40:37.000Z'
    data:
      edited: false
      editors:
      - riosje
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6302637457847595
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/193ed76a243d0ee158774289bb170384.svg
          fullname: jefferson
          isHf: false
          isPro: false
          name: riosje
          type: user
        html: '<p>Hi guys I''m trying to geeting started with this model using the
          transformers.js package but i get this error</p>

          <p><code>Error: Could not locate file: "[https://huggingface.co/Xenova/codegen-350M-mono/resolve/main/onnx/encoder_model_quantized.onnx](https://huggingface.co/Xenova/codegen-350M-mono/resolve/main/onnx/encoder_model_quantized.onnx)".</code></p>

          <p>but when i use the model of the examples it works properly<br><code>Xenova/distilbert-base-uncased-finetuned-sst-2-english</code></p>

          <p>this is my sample code</p>

          <pre><code>import { pipeline } from ''@xenova/transformers'';


          let code = await pipeline(''text2text-generation'', ''Xenova/codegen-350M-mono'');

          let result = await code(''Write bash script to say hello world'')

          console.log(result)

          </code></pre>

          '
        raw: "Hi guys I'm trying to geeting started with this model using the transformers.js\
          \ package but i get this error\r\n\r\n`Error: Could not locate file: \"\
          [https://huggingface.co/Xenova/codegen-350M-mono/resolve/main/onnx/encoder_model_quantized.onnx](https://huggingface.co/Xenova/codegen-350M-mono/resolve/main/onnx/encoder_model_quantized.onnx)\"\
          .`\r\n\r\nbut when i use the model of the examples it works properly\r\n\
          `Xenova/distilbert-base-uncased-finetuned-sst-2-english`\r\n\r\n\r\nthis\
          \ is my sample code\r\n\r\n```\r\nimport { pipeline } from '@xenova/transformers';\r\
          \n\r\nlet code = await pipeline('text2text-generation', 'Xenova/codegen-350M-mono');\r\
          \nlet result = await code('Write bash script to say hello world')\r\nconsole.log(result)\r\
          \n```"
        updatedAt: '2023-06-05T18:40:37.662Z'
      numEdits: 0
      reactions: []
    id: 647e2c25e4d52fe0e01f39a1
    type: comment
  author: riosje
  content: "Hi guys I'm trying to geeting started with this model using the transformers.js\
    \ package but i get this error\r\n\r\n`Error: Could not locate file: \"[https://huggingface.co/Xenova/codegen-350M-mono/resolve/main/onnx/encoder_model_quantized.onnx](https://huggingface.co/Xenova/codegen-350M-mono/resolve/main/onnx/encoder_model_quantized.onnx)\"\
    .`\r\n\r\nbut when i use the model of the examples it works properly\r\n`Xenova/distilbert-base-uncased-finetuned-sst-2-english`\r\
    \n\r\n\r\nthis is my sample code\r\n\r\n```\r\nimport { pipeline } from '@xenova/transformers';\r\
    \n\r\nlet code = await pipeline('text2text-generation', 'Xenova/codegen-350M-mono');\r\
    \nlet result = await code('Write bash script to say hello world')\r\nconsole.log(result)\r\
    \n```"
  created_at: 2023-06-05 17:40:37+00:00
  edited: false
  hidden: false
  id: 647e2c25e4d52fe0e01f39a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-06-05T22:50:42.000Z'
    data:
      edited: true
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6110813021659851
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<p>Hi there. So, <code>codegen</code> is a <code>text-generation</code>
          model, not a <code>text2text-generation</code> model. In other words, it
          is a decoder-only language model (and not an encoder-decoder model).</p>

          <p>So, the correct usage is:</p>

          <pre><code>import { pipeline } from ''@xenova/transformers'';


          let code = await pipeline(''text-generation'', ''Xenova/codegen-350M-mono'');

          let result = await code(''Write bash script to say hello world'')

          console.log(result)

          </code></pre>

          <p>However, making this mistake is completely understandable, since we currently
          do not show the task on the model''s page. You should see that update very
          soon.</p>

          '
        raw: 'Hi there. So, `codegen` is a `text-generation` model, not a `text2text-generation`
          model. In other words, it is a decoder-only language model (and not an encoder-decoder
          model).


          So, the correct usage is:

          ```

          import { pipeline } from ''@xenova/transformers'';


          let code = await pipeline(''text-generation'', ''Xenova/codegen-350M-mono'');

          let result = await code(''Write bash script to say hello world'')

          console.log(result)

          ```


          However, making this mistake is completely understandable, since we currently
          do not show the task on the model''s page. You should see that update very
          soon.'
        updatedAt: '2023-06-05T22:52:56.796Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - riosje
    id: 647e66c2f14eafc3b4609726
    type: comment
  author: Xenova
  content: 'Hi there. So, `codegen` is a `text-generation` model, not a `text2text-generation`
    model. In other words, it is a decoder-only language model (and not an encoder-decoder
    model).


    So, the correct usage is:

    ```

    import { pipeline } from ''@xenova/transformers'';


    let code = await pipeline(''text-generation'', ''Xenova/codegen-350M-mono'');

    let result = await code(''Write bash script to say hello world'')

    console.log(result)

    ```


    However, making this mistake is completely understandable, since we currently
    do not show the task on the model''s page. You should see that update very soon.'
  created_at: 2023-06-05 21:50:42+00:00
  edited: true
  hidden: false
  id: 647e66c2f14eafc3b4609726
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/193ed76a243d0ee158774289bb170384.svg
      fullname: jefferson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: riosje
      type: user
    createdAt: '2023-06-05T23:13:31.000Z'
    data:
      edited: false
      editors:
      - riosje
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8666703104972839
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/193ed76a243d0ee158774289bb170384.svg
          fullname: jefferson
          isHf: false
          isPro: false
          name: riosje
          type: user
        html: '<p>Hi guys, thank you so much to take the time to answer.<br>I''m still
          learning about all this, I''ve learn that the <code>text2text-generation</code>
          are the T5 - T0 and BART models.<br>How can I identify the  <code>text-generation</code>
          models uploaded  on <code>Xenova/*</code></p>

          <p><a href="https://huggingface.co/tasks/text-generation#text-to-text-generation-models">https://huggingface.co/tasks/text-generation#text-to-text-generation-models</a></p>

          <p>I got this error while running that sample code, but i guess that error
          is because I''m using it in the wrong way, soo...</p>

          <p><code>Error: Non-zero status code returned while running If node. Name:''optimum::if''
          Status Message: Non-zero status code returned while running Gather node.
          Name:''/transformer/h.0/attn/Gather_14'' Status Message: indices element
          out of data bounds, idx=2048 must be within the inclusive range [-2048,2047]</code></p>

          <p>Thank you very much guys</p>

          '
        raw: 'Hi guys, thank you so much to take the time to answer.

          I''m still learning about all this, I''ve learn that the `text2text-generation`
          are the T5 - T0 and BART models.

          How can I identify the  `text-generation` models uploaded  on `Xenova/*`


          https://huggingface.co/tasks/text-generation#text-to-text-generation-models



          I got this error while running that sample code, but i guess that error
          is because I''m using it in the wrong way, soo...


          `Error: Non-zero status code returned while running If node. Name:''optimum::if''
          Status Message: Non-zero status code returned while running Gather node.
          Name:''/transformer/h.0/attn/Gather_14'' Status Message: indices element
          out of data bounds, idx=2048 must be within the inclusive range [-2048,2047]`


          Thank you very much guys'
        updatedAt: '2023-06-05T23:13:31.522Z'
      numEdits: 0
      reactions: []
    id: 647e6c1b32c471a7fa9b25b1
    type: comment
  author: riosje
  content: 'Hi guys, thank you so much to take the time to answer.

    I''m still learning about all this, I''ve learn that the `text2text-generation`
    are the T5 - T0 and BART models.

    How can I identify the  `text-generation` models uploaded  on `Xenova/*`


    https://huggingface.co/tasks/text-generation#text-to-text-generation-models



    I got this error while running that sample code, but i guess that error is because
    I''m using it in the wrong way, soo...


    `Error: Non-zero status code returned while running If node. Name:''optimum::if''
    Status Message: Non-zero status code returned while running Gather node. Name:''/transformer/h.0/attn/Gather_14''
    Status Message: indices element out of data bounds, idx=2048 must be within the
    inclusive range [-2048,2047]`


    Thank you very much guys'
  created_at: 2023-06-05 22:13:31+00:00
  edited: false
  hidden: false
  id: 647e6c1b32c471a7fa9b25b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-06-06T13:44:47.000Z'
    data:
      edited: false
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9569706916809082
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<blockquote>

          <p>How can I identify the text-generation models uploaded on Xenova/*</p>

          </blockquote>

          <p>We''ll soon add filtering so you can choose based on task.</p>

          <blockquote>

          <p>I got this error while running that sample code, but i guess that error
          is because I''m using it in the wrong way, soo...</p>

          </blockquote>

          <p>Can you provide the code you used?</p>

          '
        raw: '> How can I identify the text-generation models uploaded on Xenova/*


          We''ll soon add filtering so you can choose based on task.


          > I got this error while running that sample code, but i guess that error
          is because I''m using it in the wrong way, soo...


          Can you provide the code you used?'
        updatedAt: '2023-06-06T13:44:47.602Z'
      numEdits: 0
      reactions: []
    id: 647f384f2a7bcaa307a52522
    type: comment
  author: Xenova
  content: '> How can I identify the text-generation models uploaded on Xenova/*


    We''ll soon add filtering so you can choose based on task.


    > I got this error while running that sample code, but i guess that error is because
    I''m using it in the wrong way, soo...


    Can you provide the code you used?'
  created_at: 2023-06-06 12:44:47+00:00
  edited: false
  hidden: false
  id: 647f384f2a7bcaa307a52522
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-06-30T19:30:40.000Z'
    data:
      edited: false
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6890152096748352
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<p>Just an update on this: You can now filter transformers.js models
          on the Hub! For example, <a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;library=transformers.js&amp;sort=trending">https://huggingface.co/models?pipeline_tag=text-generation&amp;library=transformers.js&amp;sort=trending</a>
          will show all text-generation models that are compatible with Transformers.js</p>

          '
        raw: 'Just an update on this: You can now filter transformers.js models on
          the Hub! For example, https://huggingface.co/models?pipeline_tag=text-generation&library=transformers.js&sort=trending
          will show all text-generation models that are compatible with Transformers.js'
        updatedAt: '2023-06-30T19:30:40.871Z'
      numEdits: 0
      reactions: []
    id: 649f2d60b469b6b0b4feec15
    type: comment
  author: Xenova
  content: 'Just an update on this: You can now filter transformers.js models on the
    Hub! For example, https://huggingface.co/models?pipeline_tag=text-generation&library=transformers.js&sort=trending
    will show all text-generation models that are compatible with Transformers.js'
  created_at: 2023-06-30 18:30:40+00:00
  edited: false
  hidden: false
  id: 649f2d60b469b6b0b4feec15
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Xenova/codegen-350M-mono
repo_type: model
status: open
target_branch: null
title: 'Error: Could not locate file ''codegen-350M-mono/resolve/main/onnx/encoder_model_quantized.onnx''
  with transformers.js'
