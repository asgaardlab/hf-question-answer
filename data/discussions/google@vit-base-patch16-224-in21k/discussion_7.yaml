!!python/object:huggingface_hub.community.DiscussionWithDetails
author: turtleman
conflicting_files: null
created_at: 2023-11-15 03:00:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7522ef2c5a7dcdecaf82804daa50c70.svg
      fullname: Guangyuan Weng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: turtleman
      type: user
    createdAt: '2023-11-15T03:00:15.000Z'
    data:
      edited: false
      editors:
      - turtleman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7621658444404602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7522ef2c5a7dcdecaf82804daa50c70.svg
          fullname: Guangyuan Weng
          isHf: false
          isPro: false
          name: turtleman
          type: user
        html: '<p>If I understand correctly, the <code>google/vit-base-patch16-224-in21k</code>
          corresponds to timm''s <a rel="nofollow" href="https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L1179-L1181">vit_base_patch16_224.augreg_in21k</a>.
          </p>

          <p>However, I found HuggingFace''s has a Pooler layer that timm''s doesn''t
          have. </p>

          <p>Besides, I checked some specific weights, e.g., </p>

          <ol>

          <li>Huggingface: <code>embeddings.patch_embeddings.projection.weight</code></li>

          <li>timm: <code>patch_embed.proj.weight</code><br>They are not equal.</li>

          </ol>

          <p>Other minor things could be <code>eps</code> of `LayerNorm''.</p>

          <p>I''m wondering if the correct weights have been converted.</p>

          '
        raw: "If I understand correctly, the `google/vit-base-patch16-224-in21k` corresponds\
          \ to timm's [vit_base_patch16_224.augreg_in21k](https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L1179-L1181).\
          \ \r\n\r\nHowever, I found HuggingFace's has a Pooler layer that timm's\
          \ doesn't have. \r\n\r\nBesides, I checked some specific weights, e.g.,\
          \ \r\n1. Huggingface: `embeddings.patch_embeddings.projection.weight`\r\n\
          2. timm: `patch_embed.proj.weight`\r\nThey are not equal.\r\n\r\nOther minor\
          \ things could be `eps` of `LayerNorm'.\r\n\r\nI'm wondering if the correct\
          \ weights have been converted."
        updatedAt: '2023-11-15T03:00:15.993Z'
      numEdits: 0
      reactions: []
    id: 6554343fd8654432e0df054f
    type: comment
  author: turtleman
  content: "If I understand correctly, the `google/vit-base-patch16-224-in21k` corresponds\
    \ to timm's [vit_base_patch16_224.augreg_in21k](https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L1179-L1181).\
    \ \r\n\r\nHowever, I found HuggingFace's has a Pooler layer that timm's doesn't\
    \ have. \r\n\r\nBesides, I checked some specific weights, e.g., \r\n1. Huggingface:\
    \ `embeddings.patch_embeddings.projection.weight`\r\n2. timm: `patch_embed.proj.weight`\r\
    \nThey are not equal.\r\n\r\nOther minor things could be `eps` of `LayerNorm'.\r\
    \n\r\nI'm wondering if the correct weights have been converted."
  created_at: 2023-11-15 03:00:15+00:00
  edited: false
  hidden: false
  id: 6554343fd8654432e0df054f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-11-15T15:26:22.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.33818304538726807
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nielsr\">@<span class=\"\
          underline\">nielsr</span></a></span>\n\n\t</span></span> </p>\n"
        raw: 'cc @nielsr '
        updatedAt: '2023-11-15T15:26:22.399Z'
      numEdits: 0
      reactions: []
    id: 6554e31ecb7a7bda79736c9e
    type: comment
  author: lysandre
  content: 'cc @nielsr '
  created_at: 2023-11-15 15:26:22+00:00
  edited: false
  hidden: false
  id: 6554e31ecb7a7bda79736c9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-11-16T08:23:22.000Z'
    data:
      edited: true
      editors:
      - nielsr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8829963207244873
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: "<p>Hi,</p>\n<p>The weights from this model correspond to the original\
          \ \"vit_base_patch16_224_21k\" which Google released in their JAX repository\
          \ and which were ported to timm (and is deprecated in timm now as seen <a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L2604\"\
          >here</a>). It does not correspond to the \"augreg\" checkpoint (\"augreg\"\
          \ is from a follow-up paper called <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2106.10270\"\
          >\"How to train your ViT\"</a>). It looks like that legacy name in timm\
          \ now points to the augreg version (which is a better trained version).\
          \ cc'ing <span data-props=\"{&quot;user&quot;:&quot;rwightman&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rwightman\">@<span class=\"\
          underline\">rwightman</span></a></span>\n\n\t</span></span> for confirmation.</p>\n\
          <p>This is the conversion script that was used to convert the checkpoint:\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/convert_vit_timm_to_pytorch.py\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/convert_vit_timm_to_pytorch.py</a>.\
          \ It would definitely be great to convert newer, more capable ViT checkpoints.</p>\n"
        raw: 'Hi,


          The weights from this model correspond to the original "vit_base_patch16_224_21k"
          which Google released in their JAX repository and which were ported to timm
          (and is deprecated in timm now as seen [here](https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L2604)).
          It does not correspond to the "augreg" checkpoint ("augreg" is from a follow-up
          paper called ["How to train your ViT"](https://arxiv.org/abs/2106.10270)).
          It looks like that legacy name in timm now points to the augreg version
          (which is a better trained version). cc''ing @rwightman for confirmation.


          This is the conversion script that was used to convert the checkpoint: https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/convert_vit_timm_to_pytorch.py.
          It would definitely be great to convert newer, more capable ViT checkpoints.'
        updatedAt: '2023-11-16T08:26:01.563Z'
      numEdits: 2
      reactions: []
    id: 6555d17acaad59f219acf1f6
    type: comment
  author: nielsr
  content: 'Hi,


    The weights from this model correspond to the original "vit_base_patch16_224_21k"
    which Google released in their JAX repository and which were ported to timm (and
    is deprecated in timm now as seen [here](https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L2604)).
    It does not correspond to the "augreg" checkpoint ("augreg" is from a follow-up
    paper called ["How to train your ViT"](https://arxiv.org/abs/2106.10270)). It
    looks like that legacy name in timm now points to the augreg version (which is
    a better trained version). cc''ing @rwightman for confirmation.


    This is the conversion script that was used to convert the checkpoint: https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/convert_vit_timm_to_pytorch.py.
    It would definitely be great to convert newer, more capable ViT checkpoints.'
  created_at: 2023-11-16 08:23:22+00:00
  edited: true
  hidden: false
  id: 6555d17acaad59f219acf1f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-11-16T17:59:00.000Z'
    data:
      edited: true
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8558357954025269
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: "<p>All transformers vit models I'm aware are the originals from <code>An\
          \ Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale</code>.\
          \ </p>\n<p><code>timm</code> includes both the originals and the best checkpoints\
          \ from <code>How to train your ViT?</code>. The script <span data-props=\"\
          {&quot;user&quot;:&quot;nielsr&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/nielsr\">@<span class=\"underline\">nielsr</span></a></span>\n\
          \n\t</span></span> mentioned can be used to convert the 'augreg' models.</p>\n\
          <p>It should be noted that the original '21k' vit models have a zero'd out\
          \ classifier head. They cannot be used for classification.</p>\n<p>The augreg\
          \ 21k weights have a valid classifier head, they can be used for classification,\
          \ and in timm have appropriate class mappings (try the classification widget\
          \ <a href=\"https://huggingface.co/timm/vit_large_patch16_224.augreg_in21k\"\
          >https://huggingface.co/timm/vit_large_patch16_224.augreg_in21k</a>).</p>\n\
          <p><code>timm</code> deprecations are supposed to deprecate the old naming,\
          \ but for some reason I lost the original base 21k model (it was not deprecated),\
          \ woops.</p>\n<ul>\n<li>augreg 21k model is <code>vit_large_patch16_224.augreg_in21k</code></li>\n\
          <li>original 21k model which matches the equivalent HF one (except I remove\
          \ the pre_logits that are no longer being used) is <code>vit_large_patch32_224.orig_in21k</code></li>\n\
          </ul>\n<p>I also fine-tuned several 21k 'How to train your ViT' models with\
          \ better recipes to augreg2 tags<br><a rel=\"nofollow\" href=\"https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L1048-L1052\"\
          >https://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L1048-L1052</a></p>\n\
          <p>EDIT:<br>There are also 50K other checkpoints from the <code>How to train\
          \ your ViT?</code> paper that can be loaded directly (npz files) in the\
          \ timm models from this table, incl fine-tunes on CIFAR, resisc, kitti,\
          \ oxford pets, etc: <code>gs://vit_models/augreg/index.csv</code></p>\n\
          <p><a rel=\"nofollow\" href=\"https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb#scrollTo=yy-cuGxyD6Xw\"\
          >https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb#scrollTo=yy-cuGxyD6Xw</a></p>\n"
        raw: "All transformers vit models I'm aware are the originals from `An Image\
          \ Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale`.\
          \ \n\n`timm` includes both the originals and the best checkpoints from `How\
          \ to train your ViT?`. The script @nielsr mentioned can be used to convert\
          \ the 'augreg' models.\n\nIt should be noted that the original '21k' vit\
          \ models have a zero'd out classifier head. They cannot be used for classification.\n\
          \nThe augreg 21k weights have a valid classifier head, they can be used\
          \ for classification, and in timm have appropriate class mappings (try the\
          \ classification widget https://huggingface.co/timm/vit_large_patch16_224.augreg_in21k).\n\
          \n`timm` deprecations are supposed to deprecate the old naming, but for\
          \ some reason I lost the original base 21k model (it was not deprecated),\
          \ woops.\n- augreg 21k model is `vit_large_patch16_224.augreg_in21k`\n-\
          \ original 21k model which matches the equivalent HF one (except I remove\
          \ the pre_logits that are no longer being used) is `vit_large_patch32_224.orig_in21k`\n\
          \nI also fine-tuned several 21k 'How to train your ViT' models with better\
          \ recipes to augreg2 tags\nhttps://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L1048-L1052\n\
          \nEDIT:\nThere are also 50K other checkpoints from the `How to train your\
          \ ViT?` paper that can be loaded directly (npz files) in the timm models\
          \ from this table, incl fine-tunes on CIFAR, resisc, kitti, oxford pets,\
          \ etc: `gs://vit_models/augreg/index.csv`\n\nhttps://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb#scrollTo=yy-cuGxyD6Xw\n"
        updatedAt: '2023-11-16T18:14:00.720Z'
      numEdits: 3
      reactions: []
    id: 6556586482457f75dd8779ea
    type: comment
  author: rwightman
  content: "All transformers vit models I'm aware are the originals from `An Image\
    \ Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale`. \n\n`timm`\
    \ includes both the originals and the best checkpoints from `How to train your\
    \ ViT?`. The script @nielsr mentioned can be used to convert the 'augreg' models.\n\
    \nIt should be noted that the original '21k' vit models have a zero'd out classifier\
    \ head. They cannot be used for classification.\n\nThe augreg 21k weights have\
    \ a valid classifier head, they can be used for classification, and in timm have\
    \ appropriate class mappings (try the classification widget https://huggingface.co/timm/vit_large_patch16_224.augreg_in21k).\n\
    \n`timm` deprecations are supposed to deprecate the old naming, but for some reason\
    \ I lost the original base 21k model (it was not deprecated), woops.\n- augreg\
    \ 21k model is `vit_large_patch16_224.augreg_in21k`\n- original 21k model which\
    \ matches the equivalent HF one (except I remove the pre_logits that are no longer\
    \ being used) is `vit_large_patch32_224.orig_in21k`\n\nI also fine-tuned several\
    \ 21k 'How to train your ViT' models with better recipes to augreg2 tags\nhttps://github.com/huggingface/pytorch-image-models/blob/ef72c3cd470dd67836eebf95ec567199c890a6a2/timm/models/vision_transformer.py#L1048-L1052\n\
    \nEDIT:\nThere are also 50K other checkpoints from the `How to train your ViT?`\
    \ paper that can be loaded directly (npz files) in the timm models from this table,\
    \ incl fine-tunes on CIFAR, resisc, kitti, oxford pets, etc: `gs://vit_models/augreg/index.csv`\n\
    \nhttps://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb#scrollTo=yy-cuGxyD6Xw\n"
  created_at: 2023-11-16 17:59:00+00:00
  edited: true
  hidden: false
  id: 6556586482457f75dd8779ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-11-16T18:01:45.000Z'
    data:
      edited: true
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9863275289535522
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: '<p>Note that when augreg models came out, I originally replaced all
          overlapping original models with the new weights (old weights had an L32
          and new did not, so that remained). Then, when I added multi-weight support
          I added some originals back, but forgot to add a few of the 21k back.</p>

          '
        raw: Note that when augreg models came out, I originally replaced all overlapping
          original models with the new weights (old weights had an L32 and new did
          not, so that remained). Then, when I added multi-weight support I added
          some originals back, but forgot to add a few of the 21k back.
        updatedAt: '2023-11-16T18:02:55.635Z'
      numEdits: 2
      reactions: []
    id: 655659090bd9dbcb0692c76e
    type: comment
  author: rwightman
  content: Note that when augreg models came out, I originally replaced all overlapping
    original models with the new weights (old weights had an L32 and new did not,
    so that remained). Then, when I added multi-weight support I added some originals
    back, but forgot to add a few of the 21k back.
  created_at: 2023-11-16 18:01:45+00:00
  edited: true
  hidden: false
  id: 655659090bd9dbcb0692c76e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-11-16T18:06:20.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9580734968185425
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: '<p>And yeah, the LN eps is wrong in most transformers vit that I''m
          aware of, it''s 1e-6 not 1e-12. The impact is relatively small on validation
          results, but can impact training stability as 1e-6 is in the range that''s
          okay for lower precision training and 1e-12 is not.</p>

          '
        raw: And yeah, the LN eps is wrong in most transformers vit that I'm aware
          of, it's 1e-6 not 1e-12. The impact is relatively small on validation results,
          but can impact training stability as 1e-6 is in the range that's okay for
          lower precision training and 1e-12 is not.
        updatedAt: '2023-11-16T18:06:20.878Z'
      numEdits: 0
      reactions: []
    id: 65565a1c18320e0e3fa0aa8a
    type: comment
  author: rwightman
  content: And yeah, the LN eps is wrong in most transformers vit that I'm aware of,
    it's 1e-6 not 1e-12. The impact is relatively small on validation results, but
    can impact training stability as 1e-6 is in the range that's okay for lower precision
    training and 1e-12 is not.
  created_at: 2023-11-16 18:06:20+00:00
  edited: false
  hidden: false
  id: 65565a1c18320e0e3fa0aa8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7522ef2c5a7dcdecaf82804daa50c70.svg
      fullname: Guangyuan Weng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: turtleman
      type: user
    createdAt: '2023-11-16T21:15:21.000Z'
    data:
      edited: false
      editors:
      - turtleman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9162878394126892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7522ef2c5a7dcdecaf82804daa50c70.svg
          fullname: Guangyuan Weng
          isHf: false
          isPro: false
          name: turtleman
          type: user
        html: "<p>Wow, many thanks for your detailed answers, <span data-props=\"\
          {&quot;user&quot;:&quot;lysandre&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/lysandre\">@<span class=\"underline\">lysandre</span></a></span>\n\
          \n\t</span></span> <span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nielsr\"\
          >@<span class=\"underline\">nielsr</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;rwightman&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rwightman\">@<span class=\"\
          underline\">rwightman</span></a></span>\n\n\t</span></span> !</p>\n<p>Yeah,\
          \ I can confirm most of (I'd say 99.9%) the weights are of the same between\
          \ HF's <a href=\"https://huggingface.co/google/vit-large-patch32-224-in21k\"\
          >google/vit-large-patch32-224-in21k</a> and timm's <code>vit_large_patch32_224.orig_in21k</code>.</p>\n\
          <p>However, I'm still curious why HF's has a pooler layer at last while\
          \ timm's doesn't. I understand timm's last layer is the original classification\
          \ head. HF's model card indicates </p>\n<pre><code class=\"language-py\"\
          ><span class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\"\
          > However, the model does include the pre-trained pooler, which can be used\
          \ for downstream tasks (such as image classification).</span>\n<span class=\"\
          hljs-string\">\"\"\"</span>\n(pooler): ViTPooler(\n    (dense): Linear(in_features=<span\
          \ class=\"hljs-number\">1024</span>, out_features=<span class=\"hljs-number\"\
          >1024</span>, bias=<span class=\"hljs-literal\">True</span>)\n    (activation):\
          \ Tanh()\n  )\n</code></pre>\n<p>I'm wondering </p>\n<ol>\n<li>Does timm\
          \ include this layer? If not, how can I convert HF's to timm's? It looks\
          \ like all original ViTs have this problem between HF and timm.<br>(for\
          \ a quick glance, this might be easy, to directly borrow HF's weight to\
          \ timms)</li>\n<li>Is there a quick way I can convert HF's vit base model\
          \ to timm's, as there is no support for base vit models?</li>\n</ol>\n<p>Thanks\
          \ a lot!</p>\n"
        raw: "Wow, many thanks for your detailed answers, @lysandre @nielsr @rwightman\
          \ !\n\nYeah, I can confirm most of (I'd say 99.9%) the weights are of the\
          \ same between HF's [google/vit-large-patch32-224-in21k](https://huggingface.co/google/vit-large-patch32-224-in21k)\
          \ and timm's `vit_large_patch32_224.orig_in21k`.\n\nHowever, I'm still curious\
          \ why HF's has a pooler layer at last while timm's doesn't. I understand\
          \ timm's last layer is the original classification head. HF's model card\
          \ indicates \n\n```py\n\"\"\"\n However, the model does include the pre-trained\
          \ pooler, which can be used for downstream tasks (such as image classification).\n\
          \"\"\"\n(pooler): ViTPooler(\n    (dense): Linear(in_features=1024, out_features=1024,\
          \ bias=True)\n    (activation): Tanh()\n  )\n```\nI'm wondering \n1. Does\
          \ timm include this layer? If not, how can I convert HF's to timm's? It\
          \ looks like all original ViTs have this problem between HF and timm. \n\
          (for a quick glance, this might be easy, to directly borrow HF's weight\
          \ to timms)\n2. Is there a quick way I can convert HF's vit base model to\
          \ timm's, as there is no support for base vit models?\n\nThanks a lot!\n\
          \n\n"
        updatedAt: '2023-11-16T21:15:21.068Z'
      numEdits: 0
      reactions: []
    id: 65568669134ade1157950442
    type: comment
  author: turtleman
  content: "Wow, many thanks for your detailed answers, @lysandre @nielsr @rwightman\
    \ !\n\nYeah, I can confirm most of (I'd say 99.9%) the weights are of the same\
    \ between HF's [google/vit-large-patch32-224-in21k](https://huggingface.co/google/vit-large-patch32-224-in21k)\
    \ and timm's `vit_large_patch32_224.orig_in21k`.\n\nHowever, I'm still curious\
    \ why HF's has a pooler layer at last while timm's doesn't. I understand timm's\
    \ last layer is the original classification head. HF's model card indicates \n\
    \n```py\n\"\"\"\n However, the model does include the pre-trained pooler, which\
    \ can be used for downstream tasks (such as image classification).\n\"\"\"\n(pooler):\
    \ ViTPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n\
    \    (activation): Tanh()\n  )\n```\nI'm wondering \n1. Does timm include this\
    \ layer? If not, how can I convert HF's to timm's? It looks like all original\
    \ ViTs have this problem between HF and timm. \n(for a quick glance, this might\
    \ be easy, to directly borrow HF's weight to timms)\n2. Is there a quick way I\
    \ can convert HF's vit base model to timm's, as there is no support for base vit\
    \ models?\n\nThanks a lot!\n\n\n"
  created_at: 2023-11-16 21:15:21+00:00
  edited: false
  hidden: false
  id: 65568669134ade1157950442
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-11-16T22:39:25.000Z'
    data:
      edited: true
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9260047078132629
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;turtleman&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/turtleman\">@<span class=\"\
          underline\">turtleman</span></a></span>\n\n\t</span></span> </p>\n<p>For\
          \ the transformers ViTPooler, it is indeed a bit confusing, looks like the\
          \ 'pooler' in this case is, in addition to selecting the class token, applying\
          \ the 'pre logits' representation part of the MLP head (nn.Linear + tanh\
          \ activation) as described in the original paper that was used for pretraining\
          \ (<a rel=\"nofollow\" href=\"https://github.com/google-research/vision_transformer/blob/10ffdebb01aa40714b175a7c3be700c872efb2f4/vit_jax/models_vit.py#L291-L295\"\
          >https://github.com/google-research/vision_transformer/blob/10ffdebb01aa40714b175a7c3be700c872efb2f4/vit_jax/models_vit.py#L291-L295</a>).</p>\n\
          <p>Confusingly, the ViTForImageClassification head also 'pools' in that\
          \ it does class token selection again (<a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py#L805\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py#L805</a>).\
          \ The pooled output is also not used for classification, it exists only\
          \ as a separate tuple or dict output ('pooled_output'). If the classifier\
          \ for those original 21k models had been valid, it would have been using\
          \ the full MLP head.</p>\n<p>In <code>timm</code>, I removed the hidden\
          \ MLP representation altogether since there were no valid classification\
          \ models that used it or would use it.</p>\n<ul>\n<li>Those pre-logits were\
          \ only ever used for the original 21k pretrained (and unreleased JFT) models\
          \ that had classifier zero'd out</li>\n<li>All of the original fine-tuned\
          \ models removed the nn.Linear + tanh activation before fine-tune</li>\n\
          <li>Prior to collaborating on 'How to train your ViT' paper, I suggested\
          \ the pre-logits should be removed altogether as my experiments showed it\
          \ appeared to worsen in1k pretrain</li>\n<li>Google authors checked that\
          \ it had little to no impact on 21k pretrain, so all augreg models 21k and\
          \ fine-tunes were trained without the MLP head</li>\n</ul>\n<p>FYI one of\
          \ my tasks today, I'm uploading the missing orig_21k models for timm. I'm\
          \ also explictly removing the empty head (num_classes=0) to avoid future\
          \ confusion as having them there but with weights zero'd out is confusing.</p>\n\
          <p>EDIT: Missing models are there now, ie B/16 <a href=\"https://huggingface.co/timm/vit_base_patch16_224.orig_in21k\"\
          >https://huggingface.co/timm/vit_base_patch16_224.orig_in21k</a> ... and\
          \ update on the main branch of timm for it. I do recommend using the augreg\
          \ 21k weights though, they're much better. Also, the <code>vit_xxxx_patchxx_clip_xxx.[laion2b/openai/dfn/metaclip]</code>\
          \ image tower weights from CLIP models or <code>vit_xxx_patchxx_siglip_xxx.webli</code>from\
          \ SigLIP are even stronger for features and fine-tune use.</p>\n"
        raw: "@turtleman \n\nFor the transformers ViTPooler, it is indeed a bit confusing,\
          \ looks like the 'pooler' in this case is, in addition to selecting the\
          \ class token, applying the 'pre logits' representation part of the MLP\
          \ head (nn.Linear + tanh activation) as described in the original paper\
          \ that was used for pretraining (https://github.com/google-research/vision_transformer/blob/10ffdebb01aa40714b175a7c3be700c872efb2f4/vit_jax/models_vit.py#L291-L295).\n\
          \nConfusingly, the ViTForImageClassification head also 'pools' in that it\
          \ does class token selection again (https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py#L805).\
          \ The pooled output is also not used for classification, it exists only\
          \ as a separate tuple or dict output ('pooled_output'). If the classifier\
          \ for those original 21k models had been valid, it would have been using\
          \ the full MLP head.\n\nIn `timm`, I removed the hidden MLP representation\
          \ altogether since there were no valid classification models that used it\
          \ or would use it.\n* Those pre-logits were only ever used for the original\
          \ 21k pretrained (and unreleased JFT) models that had classifier zero'd\
          \ out\n* All of the original fine-tuned models removed the nn.Linear + tanh\
          \ activation before fine-tune\n* Prior to collaborating on 'How to train\
          \ your ViT' paper, I suggested the pre-logits should be removed altogether\
          \ as my experiments showed it appeared to worsen in1k pretrain\n* Google\
          \ authors checked that it had little to no impact on 21k pretrain, so all\
          \ augreg models 21k and fine-tunes were trained without the MLP head\n\n\
          FYI one of my tasks today, I'm uploading the missing orig_21k models for\
          \ timm. I'm also explictly removing the empty head (num_classes=0) to avoid\
          \ future confusion as having them there but with weights zero'd out is confusing.\n\
          \nEDIT: Missing models are there now, ie B/16 https://huggingface.co/timm/vit_base_patch16_224.orig_in21k\
          \ ... and update on the main branch of timm for it. I do recommend using\
          \ the augreg 21k weights though, they're much better. Also, the `vit_xxxx_patchxx_clip_xxx.[laion2b/openai/dfn/metaclip]`\
          \ image tower weights from CLIP models or `vit_xxx_patchxx_siglip_xxx.webli`from\
          \ SigLIP are even stronger for features and fine-tune use."
        updatedAt: '2023-11-17T04:38:10.620Z'
      numEdits: 3
      reactions: []
    id: 65569a1d776d24e3625e07be
    type: comment
  author: rwightman
  content: "@turtleman \n\nFor the transformers ViTPooler, it is indeed a bit confusing,\
    \ looks like the 'pooler' in this case is, in addition to selecting the class\
    \ token, applying the 'pre logits' representation part of the MLP head (nn.Linear\
    \ + tanh activation) as described in the original paper that was used for pretraining\
    \ (https://github.com/google-research/vision_transformer/blob/10ffdebb01aa40714b175a7c3be700c872efb2f4/vit_jax/models_vit.py#L291-L295).\n\
    \nConfusingly, the ViTForImageClassification head also 'pools' in that it does\
    \ class token selection again (https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py#L805).\
    \ The pooled output is also not used for classification, it exists only as a separate\
    \ tuple or dict output ('pooled_output'). If the classifier for those original\
    \ 21k models had been valid, it would have been using the full MLP head.\n\nIn\
    \ `timm`, I removed the hidden MLP representation altogether since there were\
    \ no valid classification models that used it or would use it.\n* Those pre-logits\
    \ were only ever used for the original 21k pretrained (and unreleased JFT) models\
    \ that had classifier zero'd out\n* All of the original fine-tuned models removed\
    \ the nn.Linear + tanh activation before fine-tune\n* Prior to collaborating on\
    \ 'How to train your ViT' paper, I suggested the pre-logits should be removed\
    \ altogether as my experiments showed it appeared to worsen in1k pretrain\n* Google\
    \ authors checked that it had little to no impact on 21k pretrain, so all augreg\
    \ models 21k and fine-tunes were trained without the MLP head\n\nFYI one of my\
    \ tasks today, I'm uploading the missing orig_21k models for timm. I'm also explictly\
    \ removing the empty head (num_classes=0) to avoid future confusion as having\
    \ them there but with weights zero'd out is confusing.\n\nEDIT: Missing models\
    \ are there now, ie B/16 https://huggingface.co/timm/vit_base_patch16_224.orig_in21k\
    \ ... and update on the main branch of timm for it. I do recommend using the augreg\
    \ 21k weights though, they're much better. Also, the `vit_xxxx_patchxx_clip_xxx.[laion2b/openai/dfn/metaclip]`\
    \ image tower weights from CLIP models or `vit_xxx_patchxx_siglip_xxx.webli`from\
    \ SigLIP are even stronger for features and fine-tune use."
  created_at: 2023-11-16 22:39:25+00:00
  edited: true
  hidden: false
  id: 65569a1d776d24e3625e07be
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: google/vit-base-patch16-224-in21k
repo_type: model
status: open
target_branch: null
title: Potential discrepancy between the weights between Huggingface and Timm for  google/vit-base-patch16-224-in21k
