!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HuBot2020
conflicting_files: null
created_at: 2022-11-08 06:12:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca97cd9bcbba13d2d03188bc4ce1d0fb.svg
      fullname: Michael Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HuBot2020
      type: user
    createdAt: '2022-11-08T06:12:01.000Z'
    data:
      edited: false
      editors:
      - HuBot2020
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca97cd9bcbba13d2d03188bc4ce1d0fb.svg
          fullname: Michael Hu
          isHf: false
          isPro: false
          name: HuBot2020
          type: user
        html: '<p>Has anyone else managed to get decent accuracy with this pretrained
          model after fine-tuning for CIFAR100? I''ve replicated the conditions claimed
          in the paper (aside from resolution changed to 324) where we have batch
          size of 512, cosine learning scheduler, with 10,000 warmup steps, and varied
          learning rate between 0.0001 to 0.1. </p>

          <p>I perform an 80/20 split on the train data provided in huggingface for
          training/validation and then use the ''test'' portion to test for top-1
          accuracy. Validation accuracy jumps to 90%, but I consistently get 40% accuracy
          on the test dataset no matter what parameter I tune. </p>

          <p>Has anyone managed to fine-tune this pretrained model on CIFAR100, and
          found consistently high accuracy (80-90%) in the ''test'' dataset for CIFAR100?
          </p>

          '
        raw: "Has anyone else managed to get decent accuracy with this pretrained\
          \ model after fine-tuning for CIFAR100? I've replicated the conditions claimed\
          \ in the paper (aside from resolution changed to 324) where we have batch\
          \ size of 512, cosine learning scheduler, with 10,000 warmup steps, and\
          \ varied learning rate between 0.0001 to 0.1. \r\n\r\nI perform an 80/20\
          \ split on the train data provided in huggingface for training/validation\
          \ and then use the 'test' portion to test for top-1 accuracy. Validation\
          \ accuracy jumps to 90%, but I consistently get 40% accuracy on the test\
          \ dataset no matter what parameter I tune. \r\n\r\nHas anyone managed to\
          \ fine-tune this pretrained model on CIFAR100, and found consistently high\
          \ accuracy (80-90%) in the 'test' dataset for CIFAR100? "
        updatedAt: '2022-11-08T06:12:01.298Z'
      numEdits: 0
      reactions: []
    id: 6369f331d322a76e1eaa5610
    type: comment
  author: HuBot2020
  content: "Has anyone else managed to get decent accuracy with this pretrained model\
    \ after fine-tuning for CIFAR100? I've replicated the conditions claimed in the\
    \ paper (aside from resolution changed to 324) where we have batch size of 512,\
    \ cosine learning scheduler, with 10,000 warmup steps, and varied learning rate\
    \ between 0.0001 to 0.1. \r\n\r\nI perform an 80/20 split on the train data provided\
    \ in huggingface for training/validation and then use the 'test' portion to test\
    \ for top-1 accuracy. Validation accuracy jumps to 90%, but I consistently get\
    \ 40% accuracy on the test dataset no matter what parameter I tune. \r\n\r\nHas\
    \ anyone managed to fine-tune this pretrained model on CIFAR100, and found consistently\
    \ high accuracy (80-90%) in the 'test' dataset for CIFAR100? "
  created_at: 2022-11-08 06:12:01+00:00
  edited: false
  hidden: false
  id: 6369f331d322a76e1eaa5610
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/856b040f7f3dff3a7ae725d63fc14405.svg
      fullname: yan zeyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yanzeyu3
      type: user
    createdAt: '2023-10-25T03:20:46.000Z'
    data:
      edited: false
      editors:
      - Yanzeyu3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968666136264801
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/856b040f7f3dff3a7ae725d63fc14405.svg
          fullname: yan zeyu
          isHf: false
          isPro: false
          name: Yanzeyu3
          type: user
        html: '<p>I have the same problem with you. But I found it works when I change
          transformers version  to 4.26.0.</p>

          '
        raw: I have the same problem with you. But I found it works when I change
          transformers version  to 4.26.0.
        updatedAt: '2023-10-25T03:20:46.292Z'
      numEdits: 0
      reactions: []
    id: 6538898e83bcfe980b07becf
    type: comment
  author: Yanzeyu3
  content: I have the same problem with you. But I found it works when I change transformers
    version  to 4.26.0.
  created_at: 2023-10-25 02:20:46+00:00
  edited: false
  hidden: false
  id: 6538898e83bcfe980b07becf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: google/vit-base-patch16-224-in21k
repo_type: model
status: open
target_branch: null
title: 'How to replicate CIFAR100 fine-tuning performance? '
