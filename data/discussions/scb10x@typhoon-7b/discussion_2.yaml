!!python/object:huggingface_hub.community.DiscussionWithDetails
author: XiangGeng
conflicting_files: null
created_at: 2024-01-18 15:18:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7053a396cb449a967de32e66617fbd8.svg
      fullname: Xiang Geng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XiangGeng
      type: user
    createdAt: '2024-01-18T15:18:49.000Z'
    data:
      edited: false
      editors:
      - XiangGeng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9227645993232727
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7053a396cb449a967de32e66617fbd8.svg
          fullname: Xiang Geng
          isHf: false
          isPro: false
          name: XiangGeng
          type: user
        html: '<p>Hi, thanks for sharing the nice work. I have some problems with
          the evaluation. First, will the ThaiExam be pubic available? Second, what
          is the way to evaluate the benchmarks in Table 3? Since the pre-trained
          model didn''t follow the instructions, whether directly compare the teacher
          forcing generation probability of different options?</p>

          '
        raw: Hi, thanks for sharing the nice work. I have some problems with the evaluation.
          First, will the ThaiExam be pubic available? Second, what is the way to
          evaluate the benchmarks in Table 3? Since the pre-trained model didn't follow
          the instructions, whether directly compare the teacher forcing generation
          probability of different options?
        updatedAt: '2024-01-18T15:18:49.399Z'
      numEdits: 0
      reactions: []
    id: 65a941593fb487e4445fd6eb
    type: comment
  author: XiangGeng
  content: Hi, thanks for sharing the nice work. I have some problems with the evaluation.
    First, will the ThaiExam be pubic available? Second, what is the way to evaluate
    the benchmarks in Table 3? Since the pre-trained model didn't follow the instructions,
    whether directly compare the teacher forcing generation probability of different
    options?
  created_at: 2024-01-18 15:18:49+00:00
  edited: false
  hidden: false
  id: 65a941593fb487e4445fd6eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2481cbf26141ccb6a2fde3c4d54d9803.svg
      fullname: Kunat
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: kunato
      type: user
    createdAt: '2024-01-18T16:35:24.000Z'
    data:
      edited: false
      editors:
      - kunato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9136965870857239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2481cbf26141ccb6a2fde3c4d54d9803.svg
          fullname: Kunat
          isHf: false
          isPro: true
          name: kunato
          type: user
        html: '<blockquote>

          <p>Hi, thanks for sharing the nice work. I have some problems with the evaluation.
          First, will the ThaiExam be pubic available? Second, what is the way to
          evaluate the benchmarks in Table 3? Since the pre-trained model didn''t
          follow the instructions, whether directly compare the teacher forcing generation
          probability of different options?</p>

          </blockquote>

          <ol>

          <li>We don''t have plans to release ThaiExam at the moment.</li>

          <li>We employ the conventional multiple-choice assessment technique (<a
          rel="nofollow" href="https://crfm.stanford.edu/helm/classic/latest/">https://crfm.stanford.edu/helm/classic/latest/</a>),
          where questions and answers are presented in a multiple-choice format labeled
          "a, b, c, d, e." The model is then prompt to produce a single letter that
          represents the correct answer.</li>

          </ol>

          '
        raw: '> Hi, thanks for sharing the nice work. I have some problems with the
          evaluation. First, will the ThaiExam be pubic available? Second, what is
          the way to evaluate the benchmarks in Table 3? Since the pre-trained model
          didn''t follow the instructions, whether directly compare the teacher forcing
          generation probability of different options?


          1. We don''t have plans to release ThaiExam at the moment.

          2. We employ the conventional multiple-choice assessment technique (https://crfm.stanford.edu/helm/classic/latest/),
          where questions and answers are presented in a multiple-choice format labeled
          "a, b, c, d, e." The model is then prompt to produce a single letter that
          represents the correct answer.'
        updatedAt: '2024-01-18T16:35:24.841Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - XiangGeng
    id: 65a9534c3fb487e44466ac69
    type: comment
  author: kunato
  content: '> Hi, thanks for sharing the nice work. I have some problems with the
    evaluation. First, will the ThaiExam be pubic available? Second, what is the way
    to evaluate the benchmarks in Table 3? Since the pre-trained model didn''t follow
    the instructions, whether directly compare the teacher forcing generation probability
    of different options?


    1. We don''t have plans to release ThaiExam at the moment.

    2. We employ the conventional multiple-choice assessment technique (https://crfm.stanford.edu/helm/classic/latest/),
    where questions and answers are presented in a multiple-choice format labeled
    "a, b, c, d, e." The model is then prompt to produce a single letter that represents
    the correct answer.'
  created_at: 2024-01-18 16:35:24+00:00
  edited: false
  hidden: false
  id: 65a9534c3fb487e44466ac69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f6a050b4c9a104f4b95755/1Xy7eLQwHoq6ilRlMCTN1.png?w=200&h=200&f=face
      fullname: Potsawee Manakul
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: potsawee
      type: user
    createdAt: '2024-01-18T17:05:07.000Z'
    data:
      edited: true
      editors:
      - potsawee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8272826671600342
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f6a050b4c9a104f4b95755/1Xy7eLQwHoq6ilRlMCTN1.png?w=200&h=200&f=face
          fullname: Potsawee Manakul
          isHf: false
          isPro: false
          name: potsawee
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;XiangGeng&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/XiangGeng\"\
          >@<span class=\"underline\">XiangGeng</span></a></span>\n\n\t</span></span>,\
          \ thanks for your interest in our work. I suppose this is some extra info:</p>\n\
          <ol>\n<li>Some of the datasets are available on HF thanks to OpenThaiGPT\
          \ for releasing them. I believe it's similar to our datasets:</li>\n</ol>\n\
          <ul>\n<li>ONET: <a href=\"https://huggingface.co/datasets/openthaigpt/thai-onet-exam\"\
          >https://huggingface.co/datasets/openthaigpt/thai-onet-exam</a> </li>\n\
          <li>IC: <a href=\"https://huggingface.co/datasets/openthaigpt/thai-investment-consultant-licensing-exams\"\
          >https://huggingface.co/datasets/openthaigpt/thai-investment-consultant-licensing-exams</a></li>\n\
          </ul>\n<ol start=\"2\">\n<li>We use the HELM framework to perform evaluation.\
          \ It performs a few-shot evaluation (e.g., 5 examples). Hence, even a pretrained\
          \ model should be able to output 'a', 'b', 'c', 'd', or 'e' in this setup.\
          \ HELM does not use teaching forcing generation probability, it simply compares\
          \ if the generated output (e.g., 'a') is the same as the gold answer (e.g.,\
          \ 'b') -- if the model generates other stuff instead of these choices it\
          \ will get a score of 0.0 for that test example. You can find more information\
          \ about HELM here: <a href=\"https://huggingface.co/blog/evaluating-mmlu-leaderboard\"\
          >https://huggingface.co/blog/evaluating-mmlu-leaderboard</a></li>\n</ol>\n\
          <p>Hope this answers your questions</p>\n"
        raw: "Hi @XiangGeng, thanks for your interest in our work. I suppose this\
          \ is some extra info:\n\n1. Some of the datasets are available on HF thanks\
          \ to OpenThaiGPT for releasing them. I believe it's similar to our datasets:\n\
          - ONET: https://huggingface.co/datasets/openthaigpt/thai-onet-exam \n- IC:\
          \ https://huggingface.co/datasets/openthaigpt/thai-investment-consultant-licensing-exams\n\
          \n2. We use the HELM framework to perform evaluation. It performs a few-shot\
          \ evaluation (e.g., 5 examples). Hence, even a pretrained model should be\
          \ able to output 'a', 'b', 'c', 'd', or 'e' in this setup. HELM does not\
          \ use teaching forcing generation probability, it simply compares if the\
          \ generated output (e.g., 'a') is the same as the gold answer (e.g., 'b')\
          \ -- if the model generates other stuff instead of these choices it will\
          \ get a score of 0.0 for that test example. You can find more information\
          \ about HELM here: https://huggingface.co/blog/evaluating-mmlu-leaderboard\
          \      \n\nHope this answers your questions"
        updatedAt: '2024-01-18T17:28:24.006Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - XiangGeng
    id: 65a95a43a7777df7ec109bab
    type: comment
  author: potsawee
  content: "Hi @XiangGeng, thanks for your interest in our work. I suppose this is\
    \ some extra info:\n\n1. Some of the datasets are available on HF thanks to OpenThaiGPT\
    \ for releasing them. I believe it's similar to our datasets:\n- ONET: https://huggingface.co/datasets/openthaigpt/thai-onet-exam\
    \ \n- IC: https://huggingface.co/datasets/openthaigpt/thai-investment-consultant-licensing-exams\n\
    \n2. We use the HELM framework to perform evaluation. It performs a few-shot evaluation\
    \ (e.g., 5 examples). Hence, even a pretrained model should be able to output\
    \ 'a', 'b', 'c', 'd', or 'e' in this setup. HELM does not use teaching forcing\
    \ generation probability, it simply compares if the generated output (e.g., 'a')\
    \ is the same as the gold answer (e.g., 'b') -- if the model generates other stuff\
    \ instead of these choices it will get a score of 0.0 for that test example. You\
    \ can find more information about HELM here: https://huggingface.co/blog/evaluating-mmlu-leaderboard\
    \      \n\nHope this answers your questions"
  created_at: 2024-01-18 17:05:07+00:00
  edited: true
  hidden: false
  id: 65a95a43a7777df7ec109bab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7053a396cb449a967de32e66617fbd8.svg
      fullname: Xiang Geng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XiangGeng
      type: user
    createdAt: '2024-01-18T17:52:50.000Z'
    data:
      edited: false
      editors:
      - XiangGeng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.917614758014679
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7053a396cb449a967de32e66617fbd8.svg
          fullname: Xiang Geng
          isHf: false
          isPro: false
          name: XiangGeng
          type: user
        html: '<p>Thanks for your replies! It makes me clear about the evaluation
          method.</p>

          '
        raw: Thanks for your replies! It makes me clear about the evaluation method.
        updatedAt: '2024-01-18T17:52:50.141Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a965726a55aac02a4164c3
    id: 65a965726a55aac02a4164c2
    type: comment
  author: XiangGeng
  content: Thanks for your replies! It makes me clear about the evaluation method.
  created_at: 2024-01-18 17:52:50+00:00
  edited: false
  hidden: false
  id: 65a965726a55aac02a4164c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d7053a396cb449a967de32e66617fbd8.svg
      fullname: Xiang Geng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XiangGeng
      type: user
    createdAt: '2024-01-18T17:52:50.000Z'
    data:
      status: closed
    id: 65a965726a55aac02a4164c3
    type: status-change
  author: XiangGeng
  created_at: 2024-01-18 17:52:50+00:00
  id: 65a965726a55aac02a4164c3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: scb10x/typhoon-7b
repo_type: model
status: closed
target_branch: null
title: About Evaluation
