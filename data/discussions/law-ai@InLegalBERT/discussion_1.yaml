!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DivyanshuDaftari
conflicting_files: null
created_at: 2022-11-21 20:09:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fae214ee54b2e2d5e36ab3dab5675700.svg
      fullname: Divyanshu Daftari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DivyanshuDaftari
      type: user
    createdAt: '2022-11-21T20:09:44.000Z'
    data:
      edited: false
      editors:
      - DivyanshuDaftari
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fae214ee54b2e2d5e36ab3dab5675700.svg
          fullname: Divyanshu Daftari
          isHf: false
          isPro: false
          name: DivyanshuDaftari
          type: user
        html: '<p>Can someone explain how and what exactly is to be doe to get this
          up and running. Because everytime i try training the model, i get this value
          error. -&gt; "ValueError: The model did not return a loss from the inputs,
          only the following keys: last_hidden_state,pooler_output. For reference,
          the inputs it received are input_ids,token_type_ids,attention_mask."</p>

          '
        raw: 'Can someone explain how and what exactly is to be doe to get this up
          and running. Because everytime i try training the model, i get this value
          error. -> "ValueError: The model did not return a loss from the inputs,
          only the following keys: last_hidden_state,pooler_output. For reference,
          the inputs it received are input_ids,token_type_ids,attention_mask."'
        updatedAt: '2022-11-21T20:09:44.094Z'
      numEdits: 0
      reactions: []
    id: 637bdb089c470afa387dae66
    type: comment
  author: DivyanshuDaftari
  content: 'Can someone explain how and what exactly is to be doe to get this up and
    running. Because everytime i try training the model, i get this value error. ->
    "ValueError: The model did not return a loss from the inputs, only the following
    keys: last_hidden_state,pooler_output. For reference, the inputs it received are
    input_ids,token_type_ids,attention_mask."'
  created_at: 2022-11-21 20:09:44+00:00
  edited: false
  hidden: false
  id: 637bdb089c470afa387dae66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662982552786-631dd4022ca5a2fb05f59922.jpeg?w=200&h=200&f=face
      fullname: Law and AI, IIT Kharagpur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: law-ai
      type: user
    createdAt: '2022-11-21T20:12:43.000Z'
    data:
      edited: false
      editors:
      - law-ai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662982552786-631dd4022ca5a2fb05f59922.jpeg?w=200&h=200&f=face
          fullname: Law and AI, IIT Kharagpur
          isHf: false
          isPro: false
          name: law-ai
          type: user
        html: '<p>Hi, what kind of fine-tuning task are you trying? And how are you
          initializing the model?<br>It would be good if you could provide a snippet
          of your code</p>

          '
        raw: 'Hi, what kind of fine-tuning task are you trying? And how are you initializing
          the model?

          It would be good if you could provide a snippet of your code'
        updatedAt: '2022-11-21T20:12:43.215Z'
      numEdits: 0
      reactions: []
    id: 637bdbbbe74a3e06aacdc220
    type: comment
  author: law-ai
  content: 'Hi, what kind of fine-tuning task are you trying? And how are you initializing
    the model?

    It would be good if you could provide a snippet of your code'
  created_at: 2022-11-21 20:12:43+00:00
  edited: false
  hidden: false
  id: 637bdbbbe74a3e06aacdc220
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fae214ee54b2e2d5e36ab3dab5675700.svg
      fullname: Divyanshu Daftari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DivyanshuDaftari
      type: user
    createdAt: '2022-11-21T20:17:11.000Z'
    data:
      edited: false
      editors:
      - DivyanshuDaftari
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fae214ee54b2e2d5e36ab3dab5675700.svg
          fullname: Divyanshu Daftari
          isHf: false
          isPro: false
          name: DivyanshuDaftari
          type: user
        html: "<p>from transformers import AutoTokenizer, AutoModel<br>tokenizer =\
          \ AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")</p>\n<p>X_train_set\
          \ = list(train_set['text'])<br>y_train_set = list(train_set['label'])<br>X_test_set\
          \ = list(test_set['text'])<br>y_test_set = list(test_set['label'])<br>X_validation_set\
          \ = list(validation_set['text'])<br>y_validation_set = list(validation_set['label'])</p>\n\
          <p>train_encoded_input = tokenizer(X_train_set, return_tensors=\"pt\", truncation=True,\
          \ padding=True)<br>test_encoded_input = tokenizer(X_test_set, return_tensors=\"\
          pt\", truncation=True, padding=True)</p>\n<p>class Dataset(torch.utils.data.Dataset):<br>\
          \    def <strong>init</strong>(self, encodings, labels=None):<br>      \
          \  self.encodings = encodings<br>        self.labels = labels</p>\n<pre><code>def\
          \ __getitem__(self, idx):\n    item = {key: torch.tensor(val[idx]) for key,\
          \ val in self.encodings.items()}\n    if self.labels:\n        item[\"labels\"\
          ] = torch.tensor(self.labels[idx]-1)\n    return item\n\ndef __len__(self):\n\
          \    return len(self.encodings[\"input_ids\"])\n</code></pre>\n<p>train_dataset\
          \ = Dataset(train_encoded_input, y_train_set)<br>test_dataset = Dataset(test_encoded_input,\
          \ y_test_set)</p>\n<p>model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\"\
          )</p>\n<p>from transformers import TrainingArguments<br>training_args =\
          \ TrainingArguments(output_dir=\"test_trainer\")</p>\n<p>from transformers\
          \ import TrainingArguments, Trainer<br>training_args = TrainingArguments(output_dir=\"\
          test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5)</p>\n\
          <p>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>\
          \    train_dataset=train_dataset,<br>    eval_dataset=test_dataset,<br>\
          \    compute_metrics=compute_metrics,<br>)<br>trainer.train()</p>\n"
        raw: "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"\
          law-ai/InLegalBERT\")\n\n\nX_train_set = list(train_set['text'])\ny_train_set\
          \ = list(train_set['label'])\nX_test_set = list(test_set['text'])\ny_test_set\
          \ = list(test_set['label'])\nX_validation_set = list(validation_set['text'])\n\
          y_validation_set = list(validation_set['label'])\n\n\ntrain_encoded_input\
          \ = tokenizer(X_train_set, return_tensors=\"pt\", truncation=True, padding=True)\n\
          test_encoded_input = tokenizer(X_test_set, return_tensors=\"pt\", truncation=True,\
          \ padding=True)\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self,\
          \ encodings, labels=None):\n        self.encodings = encodings\n       \
          \ self.labels = labels\n\n    def __getitem__(self, idx):\n        item\
          \ = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\
          \        if self.labels:\n            item[\"labels\"] = torch.tensor(self.labels[idx]-1)\n\
          \        return item\n\n    def __len__(self):\n        return len(self.encodings[\"\
          input_ids\"])\n\n\ntrain_dataset = Dataset(train_encoded_input, y_train_set)\n\
          test_dataset = Dataset(test_encoded_input, y_test_set)\n\n\nmodel = AutoModel.from_pretrained(\"\
          law-ai/InLegalBERT\")\n\n\nfrom transformers import TrainingArguments\n\
          training_args = TrainingArguments(output_dir=\"test_trainer\")\n\n\nfrom\
          \ transformers import TrainingArguments, Trainer\ntraining_args = TrainingArguments(output_dir=\"\
          test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5)\n\n\n\
          trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n\
          \    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n\
          )\ntrainer.train()"
        updatedAt: '2022-11-21T20:17:11.197Z'
      numEdits: 0
      reactions: []
    id: 637bdcc7ca8542a0ba8c459f
    type: comment
  author: DivyanshuDaftari
  content: "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"\
    law-ai/InLegalBERT\")\n\n\nX_train_set = list(train_set['text'])\ny_train_set\
    \ = list(train_set['label'])\nX_test_set = list(test_set['text'])\ny_test_set\
    \ = list(test_set['label'])\nX_validation_set = list(validation_set['text'])\n\
    y_validation_set = list(validation_set['label'])\n\n\ntrain_encoded_input = tokenizer(X_train_set,\
    \ return_tensors=\"pt\", truncation=True, padding=True)\ntest_encoded_input =\
    \ tokenizer(X_test_set, return_tensors=\"pt\", truncation=True, padding=True)\n\
    \n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings,\
    \ labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\
    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx])\
    \ for key, val in self.encodings.items()}\n        if self.labels:\n         \
    \   item[\"labels\"] = torch.tensor(self.labels[idx]-1)\n        return item\n\
    \n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n\
    \ntrain_dataset = Dataset(train_encoded_input, y_train_set)\ntest_dataset = Dataset(test_encoded_input,\
    \ y_test_set)\n\n\nmodel = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n\
    \n\nfrom transformers import TrainingArguments\ntraining_args = TrainingArguments(output_dir=\"\
    test_trainer\")\n\n\nfrom transformers import TrainingArguments, Trainer\ntraining_args\
    \ = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\"\
    , num_train_epochs=5)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n\
    \    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n\
    )\ntrainer.train()"
  created_at: 2022-11-21 20:17:11+00:00
  edited: false
  hidden: false
  id: 637bdcc7ca8542a0ba8c459f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662982552786-631dd4022ca5a2fb05f59922.jpeg?w=200&h=200&f=face
      fullname: Law and AI, IIT Kharagpur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: law-ai
      type: user
    createdAt: '2022-11-21T20:25:19.000Z'
    data:
      edited: false
      editors:
      - law-ai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662982552786-631dd4022ca5a2fb05f59922.jpeg?w=200&h=200&f=face
          fullname: Law and AI, IIT Kharagpur
          isHf: false
          isPro: false
          name: law-ai
          type: user
        html: '<p>AutoModel.from_pretrained() gives you the bare BERT model (without
          any classification heads). Consequently, the model returns the last hidden
          state of BERT and a pooler output, which is constructed from the embedding
          of the [CLS] token.</p>

          <p>The trainer however requires a model that will return a loss. For this,
          you need to add a head on top of the bare BERT model. It might be possible
          to do this by using<br>AutoModelForSequenceClassification.from_pretrained(),
          which returns a randomly initialized sequence classification head on top
          of the bare BERT model, and then you might possibly use it directly. However,
          I need to check this to confirm. You can check it too, meanwhile.</p>

          '
        raw: "AutoModel.from_pretrained() gives you the bare BERT model (without any\
          \ classification heads). Consequently, the model returns the last hidden\
          \ state of BERT and a pooler output, which is constructed from the embedding\
          \ of the [CLS] token.\n\nThe trainer however requires a model that will\
          \ return a loss. For this, you need to add a head on top of the bare BERT\
          \ model. It might be possible to do this by using \nAutoModelForSequenceClassification.from_pretrained(),\
          \ which returns a randomly initialized sequence classification head on top\
          \ of the bare BERT model, and then you might possibly use it directly. However,\
          \ I need to check this to confirm. You can check it too, meanwhile."
        updatedAt: '2022-11-21T20:25:19.626Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - DivyanshuDaftari
    id: 637bdeafc292c0fd3f34db04
    type: comment
  author: law-ai
  content: "AutoModel.from_pretrained() gives you the bare BERT model (without any\
    \ classification heads). Consequently, the model returns the last hidden state\
    \ of BERT and a pooler output, which is constructed from the embedding of the\
    \ [CLS] token.\n\nThe trainer however requires a model that will return a loss.\
    \ For this, you need to add a head on top of the bare BERT model. It might be\
    \ possible to do this by using \nAutoModelForSequenceClassification.from_pretrained(),\
    \ which returns a randomly initialized sequence classification head on top of\
    \ the bare BERT model, and then you might possibly use it directly. However, I\
    \ need to check this to confirm. You can check it too, meanwhile."
  created_at: 2022-11-21 20:25:19+00:00
  edited: false
  hidden: false
  id: 637bdeafc292c0fd3f34db04
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: law-ai/InLegalBERT
repo_type: model
status: open
target_branch: null
title: getting errors while training the model with few fine tuning modification.
