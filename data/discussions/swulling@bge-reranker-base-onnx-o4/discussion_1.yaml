!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jharianto
conflicting_files: null
created_at: 2024-01-01 04:22:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff8299cb8922240cb779a603edd02809.svg
      fullname: Jati H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jharianto
      type: user
    createdAt: '2024-01-01T04:22:56.000Z'
    data:
      edited: false
      editors:
      - jharianto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9197455644607544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff8299cb8922240cb779a603edd02809.svg
          fullname: Jati H
          isHf: false
          isPro: false
          name: jharianto
          type: user
        html: '<p>Thank you for this! As per title, getting good results (same rank
          output as original model) while running much faster. Memory usage is about
          the same as original model. Is it possible to quantize these models to try
          to reduce size &amp; memory footprint &amp; further speed up inference?</p>

          '
        raw: Thank you for this! As per title, getting good results (same rank output
          as original model) while running much faster. Memory usage is about the
          same as original model. Is it possible to quantize these models to try to
          reduce size & memory footprint & further speed up inference?
        updatedAt: '2024-01-01T04:22:56.305Z'
      numEdits: 0
      reactions: []
    id: 65923e2077105e6e4043bdaa
    type: comment
  author: jharianto
  content: Thank you for this! As per title, getting good results (same rank output
    as original model) while running much faster. Memory usage is about the same as
    original model. Is it possible to quantize these models to try to reduce size
    & memory footprint & further speed up inference?
  created_at: 2024-01-01 04:22:56+00:00
  edited: false
  hidden: false
  id: 65923e2077105e6e4043bdaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff8299cb8922240cb779a603edd02809.svg
      fullname: Jati H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jharianto
      type: user
    createdAt: '2024-01-01T08:42:15.000Z'
    data:
      edited: false
      editors:
      - jharianto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9388088583946228
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff8299cb8922240cb779a603edd02809.svg
          fullname: Jati H
          isHf: false
          isPro: false
          name: jharianto
          type: user
        html: '<p>Actually just tried it myself, turns out fairly easy to do. It seems
          to be able to quantize up to optimization level O3 only, not sure if it''s
          possible or needs more tweaking to be able to quantize O4. Not bad, reduced
          file size further to ~1/4 of original model, lower memory footprint than
          ONNX-O4, even faster inference and so far output is the same!</p>

          '
        raw: Actually just tried it myself, turns out fairly easy to do. It seems
          to be able to quantize up to optimization level O3 only, not sure if it's
          possible or needs more tweaking to be able to quantize O4. Not bad, reduced
          file size further to ~1/4 of original model, lower memory footprint than
          ONNX-O4, even faster inference and so far output is the same!
        updatedAt: '2024-01-01T08:42:15.735Z'
      numEdits: 0
      reactions: []
    id: 65927ae77fe02354738260ce
    type: comment
  author: jharianto
  content: Actually just tried it myself, turns out fairly easy to do. It seems to
    be able to quantize up to optimization level O3 only, not sure if it's possible
    or needs more tweaking to be able to quantize O4. Not bad, reduced file size further
    to ~1/4 of original model, lower memory footprint than ONNX-O4, even faster inference
    and so far output is the same!
  created_at: 2024-01-01 08:42:15+00:00
  edited: false
  hidden: false
  id: 65927ae77fe02354738260ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
      fullname: Alex Yang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: swulling
      type: user
    createdAt: '2024-01-08T16:06:12.000Z'
    data:
      edited: false
      editors:
      - swulling
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8964442014694214
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
          fullname: Alex Yang
          isHf: false
          isPro: false
          name: swulling
          type: user
        html: "<p>According to the document, O4 and O3 should represent the same level\
          \ of optimization\u2014the distinction being that O4 is exclusively for\
          \ GPU usage. I will subsequently examine whether the performance of O3 is\
          \ indeed superior.</p>\n<p><a href=\"https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization\"\
          >https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization</a></p>\n\
          <p>Furthermore, quantification can enhance performance.</p>\n"
        raw: "According to the document, O4 and O3 should represent the same level\
          \ of optimization\u2014the distinction being that O4 is exclusively for\
          \ GPU usage. I will subsequently examine whether the performance of O3 is\
          \ indeed superior.\n\nhttps://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization\n\
          \nFurthermore, quantification can enhance performance."
        updatedAt: '2024-01-08T16:06:12.576Z'
      numEdits: 0
      reactions: []
    id: 659c1d746c3964266f60f66a
    type: comment
  author: swulling
  content: "According to the document, O4 and O3 should represent the same level of\
    \ optimization\u2014the distinction being that O4 is exclusively for GPU usage.\
    \ I will subsequently examine whether the performance of O3 is indeed superior.\n\
    \nhttps://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization\n\n\
    Furthermore, quantification can enhance performance."
  created_at: 2024-01-08 16:06:12+00:00
  edited: false
  hidden: false
  id: 659c1d746c3964266f60f66a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: swulling/bge-reranker-base-onnx-o4
repo_type: model
status: open
target_branch: null
title: Works great, much faster inference. Quantization possible?
