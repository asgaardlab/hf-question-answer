!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wayne123456
conflicting_files: null
created_at: 2023-07-18 01:56:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0cec4ea6884badb0b8acce4b550526e8.svg
      fullname: wayne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wayne123456
      type: user
    createdAt: '2023-07-18T02:56:03.000Z'
    data:
      edited: false
      editors:
      - wayne123456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.439493864774704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0cec4ea6884badb0b8acce4b550526e8.svg
          fullname: wayne
          isHf: false
          isPro: false
          name: wayne123456
          type: user
        html: "<p>HI <span data-props=\"{&quot;user&quot;:&quot;gchhablani&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gchhablani\"\
          >@<span class=\"underline\">gchhablani</span></a></span>\n\n\t</span></span>\
          \ . How are you?<br>I am doing visualBERT VQA evaluation but the target\
          \ tensor is always 0. Could share the pytorch dataset for local files or\
          \ update the config. json? Thank you<br>Here is my code:</p>\n<h1 id=\"\"\
          >===========================================================================================</h1>\n\
          <h1 id=\"read-questions\">Read questions</h1>\n<p>import json</p>\n<h1 id=\"\
          checking-path-and-opening-json-file\">Checking path and Opening JSON file,</h1>\n\
          <p>f_read_questions = open('./vqa2/v2_OpenEnded_mscoco_val2014_questions.json')</p>\n\
          <h1 id=\"return-json-object-as-dictionary\">Return JSON object as dictionary</h1>\n\
          <p>data_questions = json.load(f_read_questions)<br>print(data_questions.keys())</p>\n\
          <p>questions = data_questions['questions']<br>print(\"Number of questions:\"\
          , len(questions))</p>\n<p>from os import listdir<br>from os.path import\
          \ isfile, join</p>\n<h1 id=\"root-at-which-all-images-are-stored\">root\
          \ at which all images are stored</h1>\n<p>root = './multimodal_data/vqa2/val2014'<br>file_names\
          \ = [f for f in listdir(root) if isfile(join(root, f))]</p>\n<p>import re<br>from\
          \ typing import Optional</p>\n<p>filename_re = re.compile(r\".*(\\d{12}).((jpg)|(png))\"\
          )</p>\n<h1 id=\"source-httpsgithubcomallenaiallennlp-modelsbloba36aed540e605c4293c25f73d6674071ca9edfc3allennlp_modelsvisiondataset_readersvqav2pyl141\"\
          >source: <a rel=\"nofollow\" href=\"https://github.com/allenai/allennlp-models/blob/a36aed540e605c4293c25f73d6674071ca9edfc3/allennlp_models/vision/dataset_readers/vqav2.py#L141\"\
          >https://github.com/allenai/allennlp-models/blob/a36aed540e605c4293c25f73d6674071ca9edfc3/allennlp_models/vision/dataset_readers/vqav2.py#L141</a></h1>\n\
          <p>def id_from_filename(filename: str) -&gt; Optional[int]:<br>    match\
          \ = filename_re.fullmatch(filename)<br>    if match is None:<br>       \
          \ return None<br>    return int(match.group(1))</p>\n<p>filename_to_id =\
          \ {root + \"/\" + file: id_from_filename(file) for file in file_names}<br>id_to_filename\
          \ = {v: k for k, v in filename_to_id.items()}</p>\n<p>from os import listdir<br>from\
          \ os.path import isfile, join</p>\n<h1 id=\"root-at-which-all-images-are-stored-1\"\
          >root at which all images are stored</h1>\n<p>root = './multimodal_data/vqa2/val2014'<br>file_names\
          \ = [f for f in listdir(root) if isfile(join(root, f))]</p>\n<h1 id=\"read-annotations\"\
          >Read annotations</h1>\n<p>f_read_annotations = open(\"./vqa2/v2_mscoco_val2014_annotations.json\"\
          )</p>\n<h1 id=\"return-json-object-as-dictionary-1\">Return JSON object\
          \ as dictionary</h1>\n<p>data_annotations = json.load(f_read_annotations)<br>print(data_annotations.keys())</p>\n\
          <h1 id=\"show-answwers\">show answwers</h1>\n<p>annotations = data_annotations['annotations']<br>print(\"\
          Number of annotations:\", len(annotations))</p>\n<p>from transformers import\
          \ VisualBertConfig, VisualBertModel<br>config = VisualBertConfig.from_pretrained(\"\
          ./pretrained/visualBERT/visualbert-vqa\")</p>\n<p>from tqdm.notebook import\
          \ tqdm</p>\n<p>def get_score(count: int) -&gt; float:<br>    return min(1.0,\
          \ count / 3)</p>\n<p>for annotation in tqdm(annotations):<br>    answers\
          \ = annotation['answers']<br>    answer_count = {}<br>    for answer in\
          \ answers:<br>        answer_ = answer[\"answer\"]<br>        answer_count[answer_]\
          \ = answer_count.get(answer_, 0) + 1<br>    labels = []<br>    scores =\
          \ []<br>    for answer in answer_count:<br>        if answer not in list(config.label2id.keys()):<br>\
          \            continue<br>        labels.append(config.label2id[answer])<br>\
          \        score = get_score(answer_count[answer])<br>        scores.append(score)<br>\
          \    annotation['labels'] = labels<br>    annotation['scores'] = scores</p>\n\
          <p>class VQADataset(torch.utils.data.Dataset):<br>    \"\"\"VQA (v2) dataset.\"\
          \"\"</p>\n<pre><code>def __init__(self, questions, annotations, image_preprocess):\n\
          \    self.questions = questions\n    self.annotations = annotations\n  \
          \  self.image_preprocess = image_preprocess\n\ndef __len__(self):\n    return\
          \ len(self.annotations)\n\ndef __getitem__(self, idx):\n    # get image\
          \ + text\n    annotation = self.annotations[idx]\n    questions = self.questions[idx]\n\
          \    image = id_to_filename[annotation['image_id']]\n    text = questions['question']\n\
          \n    inputs = tokenizer(\n                text,\n                padding=\"\
          max_length\",\n                max_length=40,\n                truncation=True,\n\
          \                return_token_type_ids=True,\n                return_attention_mask=True,\n\
          \                add_special_tokens=True,\n                return_tensors=\"\
          pt\",\n            )\n\n     #faster-rcnn\n    images, sizes, scales_yx\
          \ = self.image_preprocess(image)\n    output_dict = frcnn(\n        images,\n\
          \        sizes,\n        scales_yx=scales_yx,\n        padding=\"max_detections\"\
          ,\n        max_detections=frcnn_cfg.max_detections,\n        return_tensors=\"\
          pt\",\n    )\n\n    features = output_dict.get(\"roi_features\")\n\n   \
          \ inputs.update(\n        {\"input_ids\": inputs.input_ids,\n         \"\
          attention_mask\": inputs.attention_mask,\n         \"token_type_ids\": inputs.token_type_ids,\n\
          \         \"visual_embeds\": features,\n         \"visual_attention_mask\"\
          : torch.ones(features.shape[:-1], dtype=torch.float),\n         \"visual_token_type_ids\"\
          : torch.ones(features .shape[:-1], dtype=torch.long),\n         # \"output_attentions\"\
          : False\n         }\n    )\n\n  # remove batch dimension\n    for k, v in\
          \ inputs.items():\n        inputs[k] = v.squeeze()\n\n   # add labels\n\
          \    labels = annotation['labels']\n    scores = torch.tensor(annotation['scores'])\n\
          \n    targets = torch.zeros(len(config.id2label), dtype=torch.float)\n \
          \   for label, score in zip(labels, scores):\n        targets[label] = score\n\
          \    inputs[\"labels\"] = targets # get the index of the highest score as\
          \ target\n\n    # based on: https://github.com/dandelin/ViLT/blob/762fd3975c180db6fc88f577cf39549983fa373a/vilt/modules/objectives.py#L301\n\
          \    return inputs\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\
          \    from visual_bert.processing_image import Preprocess<br>    from visual_bert.visualizing_image\
          \ import SingleImageViz<br>    from visual_bert.modeling_frcnn import GeneralizedRCNN<br>\
          \    from visual_bert.utils import Config</p>\n<pre><code>frcnn_cfg = Config.from_pretrained(\"\
          unc-nlp/frcnn-vg-finetuned\")\nfrcnn = GeneralizedRCNN.from_pretrained(\"\
          unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\nimage_preprocess = Preprocess(frcnn_cfg)\n\
          \nfrom transformers import VisualBertForQuestionAnswering, BertTokenizerFast\n\
          tokenizer = BertTokenizerFast.from_pretrained(\"./pretrained/bert-base-uncased\"\
          )\nmodel = VisualBertForQuestionAnswering.from_pretrained(\"./pretrained/visualBERT/visualbert-vqa\"\
          )\n\n# if cfg.use_multi_gpu:\n# model = nn.DataParallel(model)\n# model\
          \ = model.to(device=device)\nmodel.to(device)\nmodel.eval()\n\ndataset =\
          \ VQADataset(questions=questions[:100],\n                     annotations=annotations[:100],\n\
          \                     image_preprocess=image_preprocess)\n\ntest_dataloader\
          \ = DataLoader(dataset, batch_size=1, shuffle=False)\n\ncorrect = 0.0\n\
          total = 0\n\nloss_function = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.classifier.parameters(),\
          \ lr=0.001)\n\nfor batch in tqdm(test_dataloader):\n    # with torch.no_grad():\n\
          \   \n        batch = {k: v.to(device) for k, v in batch.items()}\n    \
          \    # here is the probelm\n        print(batch[\"labels\"])\n\n       \
          \ # forward pass\n        outputs = model(**batch)\n        logits= outputs.logits\n\
          \        _, pre = torch.max(logits.data, 1)\n        _, target = torch.max(batch[\"\
          labels\"].data, 1)\n        print(\"prediction:\", pre)\n        print(\"\
          target:\", target)\n\n        correct += (pre == target).sum().item()\n\
          \        total = total + 1\n        print(total)\n        print(\"==============================================================\"\
          )\n\nfinal_acc = correct / float(len(test_dataloader.dataset))\nprint('Accuracy\
          \ of test: %f %%' % (100 * float(final_acc)))\n</code></pre>\n"
        raw: "HI @gchhablani . How are you?\r\nI am doing visualBERT VQA evaluation\
          \ but the target tensor is always 0. Could share the pytorch dataset for\
          \ local files or update the config. json? Thank you\r\nHere is my code:\r\
          \n\r\n# ===========================================================================================\r\
          \n# Read questions\r\nimport json\r\n\r\n# Checking path and Opening JSON\
          \ file,\r\nf_read_questions = open('./vqa2/v2_OpenEnded_mscoco_val2014_questions.json')\r\
          \n\r\n# Return JSON object as dictionary\r\ndata_questions = json.load(f_read_questions)\r\
          \nprint(data_questions.keys())\r\n\r\nquestions = data_questions['questions']\r\
          \nprint(\"Number of questions:\", len(questions))\r\n\r\nfrom os import\
          \ listdir\r\nfrom os.path import isfile, join\r\n\r\n# root at which all\
          \ images are stored\r\nroot = './multimodal_data/vqa2/val2014'\r\nfile_names\
          \ = [f for f in listdir(root) if isfile(join(root, f))]\r\n\r\nimport re\r\
          \nfrom typing import Optional\r\n\r\nfilename_re = re.compile(r\".*(\\d{12})\\\
          .((jpg)|(png))\")\r\n\r\n# source: https://github.com/allenai/allennlp-models/blob/a36aed540e605c4293c25f73d6674071ca9edfc3/allennlp_models/vision/dataset_readers/vqav2.py#L141\r\
          \ndef id_from_filename(filename: str) -> Optional[int]:\r\n    match = filename_re.fullmatch(filename)\r\
          \n    if match is None:\r\n        return None\r\n    return int(match.group(1))\r\
          \n\r\n\r\nfilename_to_id = {root + \"/\" + file: id_from_filename(file)\
          \ for file in file_names}\r\nid_to_filename = {v: k for k, v in filename_to_id.items()}\r\
          \n\r\nfrom os import listdir\r\nfrom os.path import isfile, join\r\n\r\n\
          # root at which all images are stored\r\nroot = './multimodal_data/vqa2/val2014'\r\
          \nfile_names = [f for f in listdir(root) if isfile(join(root, f))]\r\n\r\
          \n# Read annotations\r\nf_read_annotations = open(\"./vqa2/v2_mscoco_val2014_annotations.json\"\
          )\r\n\r\n# Return JSON object as dictionary\r\ndata_annotations = json.load(f_read_annotations)\r\
          \nprint(data_annotations.keys())\r\n\r\n# show answwers\r\nannotations =\
          \ data_annotations['annotations']\r\nprint(\"Number of annotations:\", len(annotations))\r\
          \n\r\nfrom transformers import VisualBertConfig, VisualBertModel\r\nconfig\
          \ = VisualBertConfig.from_pretrained(\"./pretrained/visualBERT/visualbert-vqa\"\
          )\r\n\r\nfrom tqdm.notebook import tqdm\r\n\r\ndef get_score(count: int)\
          \ -> float:\r\n    return min(1.0, count / 3)\r\n\r\n\r\nfor annotation\
          \ in tqdm(annotations):\r\n    answers = annotation['answers']\r\n    answer_count\
          \ = {}\r\n    for answer in answers:\r\n        answer_ = answer[\"answer\"\
          ]\r\n        answer_count[answer_] = answer_count.get(answer_, 0) + 1\r\n\
          \    labels = []\r\n    scores = []\r\n    for answer in answer_count:\r\
          \n        if answer not in list(config.label2id.keys()):\r\n           \
          \ continue\r\n        labels.append(config.label2id[answer])\r\n       \
          \ score = get_score(answer_count[answer])\r\n        scores.append(score)\r\
          \n    annotation['labels'] = labels\r\n    annotation['scores'] = scores\r\
          \n\r\n\r\nclass VQADataset(torch.utils.data.Dataset):\r\n    \"\"\"VQA (v2)\
          \ dataset.\"\"\"\r\n\r\n    def __init__(self, questions, annotations, image_preprocess):\r\
          \n        self.questions = questions\r\n        self.annotations = annotations\r\
          \n        self.image_preprocess = image_preprocess\r\n\r\n    def __len__(self):\r\
          \n        return len(self.annotations)\r\n\r\n    def __getitem__(self,\
          \ idx):\r\n        # get image + text\r\n        annotation = self.annotations[idx]\r\
          \n        questions = self.questions[idx]\r\n        image = id_to_filename[annotation['image_id']]\r\
          \n        text = questions['question']\r\n\r\n        inputs = tokenizer(\r\
          \n                    text,\r\n                    padding=\"max_length\"\
          ,\r\n                    max_length=40,\r\n                    truncation=True,\r\
          \n                    return_token_type_ids=True,\r\n                  \
          \  return_attention_mask=True,\r\n                    add_special_tokens=True,\r\
          \n                    return_tensors=\"pt\",\r\n                )\r\n\r\n\
          \         #faster-rcnn\r\n        images, sizes, scales_yx = self.image_preprocess(image)\r\
          \n        output_dict = frcnn(\r\n            images,\r\n            sizes,\r\
          \n            scales_yx=scales_yx,\r\n            padding=\"max_detections\"\
          ,\r\n            max_detections=frcnn_cfg.max_detections,\r\n          \
          \  return_tensors=\"pt\",\r\n        )\r\n\r\n        features = output_dict.get(\"\
          roi_features\")\r\n\r\n        inputs.update(\r\n            {\"input_ids\"\
          : inputs.input_ids,\r\n             \"attention_mask\": inputs.attention_mask,\r\
          \n             \"token_type_ids\": inputs.token_type_ids,\r\n          \
          \   \"visual_embeds\": features,\r\n             \"visual_attention_mask\"\
          : torch.ones(features.shape[:-1], dtype=torch.float),\r\n             \"\
          visual_token_type_ids\": torch.ones(features .shape[:-1], dtype=torch.long),\r\
          \n             # \"output_attentions\": False\r\n             }\r\n    \
          \    )\r\n\r\n      # remove batch dimension\r\n        for k, v in inputs.items():\r\
          \n            inputs[k] = v.squeeze()\r\n\r\n       # add labels\r\n   \
          \     labels = annotation['labels']\r\n        scores = torch.tensor(annotation['scores'])\r\
          \n\r\n        targets = torch.zeros(len(config.id2label), dtype=torch.float)\r\
          \n        for label, score in zip(labels, scores):\r\n            targets[label]\
          \ = score\r\n        inputs[\"labels\"] = targets # get the index of the\
          \ highest score as target\r\n\r\n        # based on: https://github.com/dandelin/ViLT/blob/762fd3975c180db6fc88f577cf39549983fa373a/vilt/modules/objectives.py#L301\r\
          \n        return inputs\r\n\r\nif __name__ == '__main__':\r\n    from visual_bert.processing_image\
          \ import Preprocess\r\n    from visual_bert.visualizing_image import SingleImageViz\r\
          \n    from visual_bert.modeling_frcnn import GeneralizedRCNN\r\n    from\
          \ visual_bert.utils import Config\r\n\r\n    frcnn_cfg = Config.from_pretrained(\"\
          unc-nlp/frcnn-vg-finetuned\")\r\n    frcnn = GeneralizedRCNN.from_pretrained(\"\
          unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\r\n    image_preprocess\
          \ = Preprocess(frcnn_cfg)\r\n\r\n    from transformers import VisualBertForQuestionAnswering,\
          \ BertTokenizerFast\r\n    tokenizer = BertTokenizerFast.from_pretrained(\"\
          ./pretrained/bert-base-uncased\")\r\n    model = VisualBertForQuestionAnswering.from_pretrained(\"\
          ./pretrained/visualBERT/visualbert-vqa\")\r\n\r\n    # if cfg.use_multi_gpu:\r\
          \n    # model = nn.DataParallel(model)\r\n    # model = model.to(device=device)\r\
          \n    model.to(device)\r\n    model.eval()\r\n\r\n    dataset = VQADataset(questions=questions[:100],\r\
          \n                         annotations=annotations[:100],\r\n          \
          \               image_preprocess=image_preprocess)\r\n\r\n    test_dataloader\
          \ = DataLoader(dataset, batch_size=1, shuffle=False)\r\n\r\n    correct\
          \ = 0.0\r\n    total = 0\r\n\r\n    loss_function = nn.CrossEntropyLoss()\r\
          \n    # optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\r\
          \n\r\n    for batch in tqdm(test_dataloader):\r\n        # with torch.no_grad():\r\
          \n       \r\n            batch = {k: v.to(device) for k, v in batch.items()}\r\
          \n            # here is the probelm\r\n            print(batch[\"labels\"\
          ])\r\n\r\n            # forward pass\r\n            outputs = model(**batch)\r\
          \n            logits= outputs.logits\r\n            _, pre = torch.max(logits.data,\
          \ 1)\r\n            _, target = torch.max(batch[\"labels\"].data, 1)\r\n\
          \            print(\"prediction:\", pre)\r\n            print(\"target:\"\
          , target)\r\n\r\n            correct += (pre == target).sum().item()\r\n\
          \            total = total + 1\r\n            print(total)\r\n         \
          \   print(\"==============================================================\"\
          )\r\n\r\n    final_acc = correct / float(len(test_dataloader.dataset))\r\
          \n    print('Accuracy of test: %f %%' % (100 * float(final_acc)))"
        updatedAt: '2023-07-18T02:56:03.950Z'
      numEdits: 0
      reactions: []
    id: 64b5ff43b84198afd5db648b
    type: comment
  author: wayne123456
  content: "HI @gchhablani . How are you?\r\nI am doing visualBERT VQA evaluation\
    \ but the target tensor is always 0. Could share the pytorch dataset for local\
    \ files or update the config. json? Thank you\r\nHere is my code:\r\n\r\n# ===========================================================================================\r\
    \n# Read questions\r\nimport json\r\n\r\n# Checking path and Opening JSON file,\r\
    \nf_read_questions = open('./vqa2/v2_OpenEnded_mscoco_val2014_questions.json')\r\
    \n\r\n# Return JSON object as dictionary\r\ndata_questions = json.load(f_read_questions)\r\
    \nprint(data_questions.keys())\r\n\r\nquestions = data_questions['questions']\r\
    \nprint(\"Number of questions:\", len(questions))\r\n\r\nfrom os import listdir\r\
    \nfrom os.path import isfile, join\r\n\r\n# root at which all images are stored\r\
    \nroot = './multimodal_data/vqa2/val2014'\r\nfile_names = [f for f in listdir(root)\
    \ if isfile(join(root, f))]\r\n\r\nimport re\r\nfrom typing import Optional\r\n\
    \r\nfilename_re = re.compile(r\".*(\\d{12})\\.((jpg)|(png))\")\r\n\r\n# source:\
    \ https://github.com/allenai/allennlp-models/blob/a36aed540e605c4293c25f73d6674071ca9edfc3/allennlp_models/vision/dataset_readers/vqav2.py#L141\r\
    \ndef id_from_filename(filename: str) -> Optional[int]:\r\n    match = filename_re.fullmatch(filename)\r\
    \n    if match is None:\r\n        return None\r\n    return int(match.group(1))\r\
    \n\r\n\r\nfilename_to_id = {root + \"/\" + file: id_from_filename(file) for file\
    \ in file_names}\r\nid_to_filename = {v: k for k, v in filename_to_id.items()}\r\
    \n\r\nfrom os import listdir\r\nfrom os.path import isfile, join\r\n\r\n# root\
    \ at which all images are stored\r\nroot = './multimodal_data/vqa2/val2014'\r\n\
    file_names = [f for f in listdir(root) if isfile(join(root, f))]\r\n\r\n# Read\
    \ annotations\r\nf_read_annotations = open(\"./vqa2/v2_mscoco_val2014_annotations.json\"\
    )\r\n\r\n# Return JSON object as dictionary\r\ndata_annotations = json.load(f_read_annotations)\r\
    \nprint(data_annotations.keys())\r\n\r\n# show answwers\r\nannotations = data_annotations['annotations']\r\
    \nprint(\"Number of annotations:\", len(annotations))\r\n\r\nfrom transformers\
    \ import VisualBertConfig, VisualBertModel\r\nconfig = VisualBertConfig.from_pretrained(\"\
    ./pretrained/visualBERT/visualbert-vqa\")\r\n\r\nfrom tqdm.notebook import tqdm\r\
    \n\r\ndef get_score(count: int) -> float:\r\n    return min(1.0, count / 3)\r\n\
    \r\n\r\nfor annotation in tqdm(annotations):\r\n    answers = annotation['answers']\r\
    \n    answer_count = {}\r\n    for answer in answers:\r\n        answer_ = answer[\"\
    answer\"]\r\n        answer_count[answer_] = answer_count.get(answer_, 0) + 1\r\
    \n    labels = []\r\n    scores = []\r\n    for answer in answer_count:\r\n  \
    \      if answer not in list(config.label2id.keys()):\r\n            continue\r\
    \n        labels.append(config.label2id[answer])\r\n        score = get_score(answer_count[answer])\r\
    \n        scores.append(score)\r\n    annotation['labels'] = labels\r\n    annotation['scores']\
    \ = scores\r\n\r\n\r\nclass VQADataset(torch.utils.data.Dataset):\r\n    \"\"\"\
    VQA (v2) dataset.\"\"\"\r\n\r\n    def __init__(self, questions, annotations,\
    \ image_preprocess):\r\n        self.questions = questions\r\n        self.annotations\
    \ = annotations\r\n        self.image_preprocess = image_preprocess\r\n\r\n  \
    \  def __len__(self):\r\n        return len(self.annotations)\r\n\r\n    def __getitem__(self,\
    \ idx):\r\n        # get image + text\r\n        annotation = self.annotations[idx]\r\
    \n        questions = self.questions[idx]\r\n        image = id_to_filename[annotation['image_id']]\r\
    \n        text = questions['question']\r\n\r\n        inputs = tokenizer(\r\n\
    \                    text,\r\n                    padding=\"max_length\",\r\n\
    \                    max_length=40,\r\n                    truncation=True,\r\n\
    \                    return_token_type_ids=True,\r\n                    return_attention_mask=True,\r\
    \n                    add_special_tokens=True,\r\n                    return_tensors=\"\
    pt\",\r\n                )\r\n\r\n         #faster-rcnn\r\n        images, sizes,\
    \ scales_yx = self.image_preprocess(image)\r\n        output_dict = frcnn(\r\n\
    \            images,\r\n            sizes,\r\n            scales_yx=scales_yx,\r\
    \n            padding=\"max_detections\",\r\n            max_detections=frcnn_cfg.max_detections,\r\
    \n            return_tensors=\"pt\",\r\n        )\r\n\r\n        features = output_dict.get(\"\
    roi_features\")\r\n\r\n        inputs.update(\r\n            {\"input_ids\": inputs.input_ids,\r\
    \n             \"attention_mask\": inputs.attention_mask,\r\n             \"token_type_ids\"\
    : inputs.token_type_ids,\r\n             \"visual_embeds\": features,\r\n    \
    \         \"visual_attention_mask\": torch.ones(features.shape[:-1], dtype=torch.float),\r\
    \n             \"visual_token_type_ids\": torch.ones(features .shape[:-1], dtype=torch.long),\r\
    \n             # \"output_attentions\": False\r\n             }\r\n        )\r\
    \n\r\n      # remove batch dimension\r\n        for k, v in inputs.items():\r\n\
    \            inputs[k] = v.squeeze()\r\n\r\n       # add labels\r\n        labels\
    \ = annotation['labels']\r\n        scores = torch.tensor(annotation['scores'])\r\
    \n\r\n        targets = torch.zeros(len(config.id2label), dtype=torch.float)\r\
    \n        for label, score in zip(labels, scores):\r\n            targets[label]\
    \ = score\r\n        inputs[\"labels\"] = targets # get the index of the highest\
    \ score as target\r\n\r\n        # based on: https://github.com/dandelin/ViLT/blob/762fd3975c180db6fc88f577cf39549983fa373a/vilt/modules/objectives.py#L301\r\
    \n        return inputs\r\n\r\nif __name__ == '__main__':\r\n    from visual_bert.processing_image\
    \ import Preprocess\r\n    from visual_bert.visualizing_image import SingleImageViz\r\
    \n    from visual_bert.modeling_frcnn import GeneralizedRCNN\r\n    from visual_bert.utils\
    \ import Config\r\n\r\n    frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\"\
    )\r\n    frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\"\
    , config=frcnn_cfg)\r\n    image_preprocess = Preprocess(frcnn_cfg)\r\n\r\n  \
    \  from transformers import VisualBertForQuestionAnswering, BertTokenizerFast\r\
    \n    tokenizer = BertTokenizerFast.from_pretrained(\"./pretrained/bert-base-uncased\"\
    )\r\n    model = VisualBertForQuestionAnswering.from_pretrained(\"./pretrained/visualBERT/visualbert-vqa\"\
    )\r\n\r\n    # if cfg.use_multi_gpu:\r\n    # model = nn.DataParallel(model)\r\
    \n    # model = model.to(device=device)\r\n    model.to(device)\r\n    model.eval()\r\
    \n\r\n    dataset = VQADataset(questions=questions[:100],\r\n                \
    \         annotations=annotations[:100],\r\n                         image_preprocess=image_preprocess)\r\
    \n\r\n    test_dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\r\
    \n\r\n    correct = 0.0\r\n    total = 0\r\n\r\n    loss_function = nn.CrossEntropyLoss()\r\
    \n    # optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\r\n\r\n\
    \    for batch in tqdm(test_dataloader):\r\n        # with torch.no_grad():\r\n\
    \       \r\n            batch = {k: v.to(device) for k, v in batch.items()}\r\n\
    \            # here is the probelm\r\n            print(batch[\"labels\"])\r\n\
    \r\n            # forward pass\r\n            outputs = model(**batch)\r\n   \
    \         logits= outputs.logits\r\n            _, pre = torch.max(logits.data,\
    \ 1)\r\n            _, target = torch.max(batch[\"labels\"].data, 1)\r\n     \
    \       print(\"prediction:\", pre)\r\n            print(\"target:\", target)\r\
    \n\r\n            correct += (pre == target).sum().item()\r\n            total\
    \ = total + 1\r\n            print(total)\r\n            print(\"==============================================================\"\
    )\r\n\r\n    final_acc = correct / float(len(test_dataloader.dataset))\r\n   \
    \ print('Accuracy of test: %f %%' % (100 * float(final_acc)))"
  created_at: 2023-07-18 01:56:03+00:00
  edited: false
  hidden: false
  id: 64b5ff43b84198afd5db648b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: uclanlp/visualbert-vqa
repo_type: model
status: open
target_branch: null
title: visualBERT config.json
