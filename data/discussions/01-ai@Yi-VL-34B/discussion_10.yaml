!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KennyUTC
conflicting_files: null
created_at: 2024-01-24 13:51:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676546883247-noauth.png?w=200&h=200&f=face
      fullname: HAODONG DUAN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KennyUTC
      type: user
    createdAt: '2024-01-24T13:51:42.000Z'
    data:
      edited: false
      editors:
      - KennyUTC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7598074674606323
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676546883247-noauth.png?w=200&h=200&f=face
          fullname: HAODONG DUAN
          isHf: false
          isPro: false
          name: KennyUTC
          type: user
        html: "<p>Codebase: <a rel=\"nofollow\" href=\"https://github.com/open-compass/VLMEvalKit\"\
          >https://github.com/open-compass/VLMEvalKit</a><br>Model Class: <a rel=\"\
          nofollow\" href=\"https://github.com/open-compass/VLMEvalKit/blob/main/vlmeval/vlm/yi_vl.py\"\
          >https://github.com/open-compass/VLMEvalKit/blob/main/vlmeval/vlm/yi_vl.py</a><br>Steps\
          \ to run Yi-VL:</p>\n<pre><code>You can perform inference of Yi-VL through\
          \ the following steps:\n1. clone the repo https://github.com/01-ai/Yi to\
          \ path-to-Yi\n2. set up the environment and install the required packages\
          \ in path-to-Yi/VL/requirements.txt\n3. set Yi_ROOT in vlmeval/config.py\
          \ \n    Yi_ROOT = path-to-Yi\n\nYou are all set now! To run a demo for Yi-VL:\n\
          \nfrom vlmeval import *\nmodel = supported_VLM['Yi_VL_6B']()\nmodel.generate('apple.jpg',\
          \ 'What is in this image?')\n\nTo run evaluation for Yi-VL, use `python\
          \ run.py --model Yi_VL_6B --data {dataset_list}`\n</code></pre>\n"
        raw: "Codebase: https://github.com/open-compass/VLMEvalKit\r\nModel Class:\
          \ https://github.com/open-compass/VLMEvalKit/blob/main/vlmeval/vlm/yi_vl.py\r\
          \nSteps to run Yi-VL:\r\n```\r\nYou can perform inference of Yi-VL through\
          \ the following steps:\r\n1. clone the repo https://github.com/01-ai/Yi\
          \ to path-to-Yi\r\n2. set up the environment and install the required packages\
          \ in path-to-Yi/VL/requirements.txt\r\n3. set Yi_ROOT in vlmeval/config.py\
          \ \r\n    Yi_ROOT = path-to-Yi\r\n\r\nYou are all set now! To run a demo\
          \ for Yi-VL:\r\n\r\nfrom vlmeval import *\r\nmodel = supported_VLM['Yi_VL_6B']()\r\
          \nmodel.generate('apple.jpg', 'What is in this image?')\r\n\r\nTo run evaluation\
          \ for Yi-VL, use `python run.py --model Yi_VL_6B --data {dataset_list}`\r\
          \n```"
        updatedAt: '2024-01-24T13:51:42.766Z'
      numEdits: 0
      reactions: []
    id: 65b115ee70773c0ab8279649
    type: comment
  author: KennyUTC
  content: "Codebase: https://github.com/open-compass/VLMEvalKit\r\nModel Class: https://github.com/open-compass/VLMEvalKit/blob/main/vlmeval/vlm/yi_vl.py\r\
    \nSteps to run Yi-VL:\r\n```\r\nYou can perform inference of Yi-VL through the\
    \ following steps:\r\n1. clone the repo https://github.com/01-ai/Yi to path-to-Yi\r\
    \n2. set up the environment and install the required packages in path-to-Yi/VL/requirements.txt\r\
    \n3. set Yi_ROOT in vlmeval/config.py \r\n    Yi_ROOT = path-to-Yi\r\n\r\nYou\
    \ are all set now! To run a demo for Yi-VL:\r\n\r\nfrom vlmeval import *\r\nmodel\
    \ = supported_VLM['Yi_VL_6B']()\r\nmodel.generate('apple.jpg', 'What is in this\
    \ image?')\r\n\r\nTo run evaluation for Yi-VL, use `python run.py --model Yi_VL_6B\
    \ --data {dataset_list}`\r\n```"
  created_at: 2024-01-24 13:51:42+00:00
  edited: false
  hidden: false
  id: 65b115ee70773c0ab8279649
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
      fullname: aisensiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisensiy
      type: user
    createdAt: '2024-01-25T03:07:45.000Z'
    data:
      edited: false
      editors:
      - aisensiy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8976396918296814
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
          fullname: aisensiy
          isHf: false
          isPro: false
          name: aisensiy
          type: user
        html: '<p>So how is the result?</p>

          '
        raw: So how is the result?
        updatedAt: '2024-01-25T03:07:45.048Z'
      numEdits: 0
      reactions: []
    id: 65b1d081ee3f66b2b0bf3117
    type: comment
  author: aisensiy
  content: So how is the result?
  created_at: 2024-01-25 03:07:45+00:00
  edited: false
  hidden: false
  id: 65b1d081ee3f66b2b0bf3117
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: 01-ai/Yi-VL-34B
repo_type: model
status: open
target_branch: null
title: '[Demo] VLMEvalKit now supported demo and evaluation for Yi-VL'
