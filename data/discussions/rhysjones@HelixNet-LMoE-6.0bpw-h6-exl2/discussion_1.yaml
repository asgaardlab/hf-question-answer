!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LoneStriker
conflicting_files: null
created_at: 2023-11-16 16:37:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-16T16:37:19.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9762205481529236
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Nice job with the LoRAs.  I had considered doing the same thing,
          but couldn''t find any tool that would easily extract LoRAs from the models.
          Most google searches turned up Stable Diffusion tools to extract LoRAs.</p>

          '
        raw: Nice job with the LoRAs.  I had considered doing the same thing, but
          couldn't find any tool that would easily extract LoRAs from the models.
          Most google searches turned up Stable Diffusion tools to extract LoRAs.
        updatedAt: '2023-11-16T16:37:19.626Z'
      numEdits: 0
      reactions: []
    id: 6556453f65d7a6ef56924803
    type: comment
  author: LoneStriker
  content: Nice job with the LoRAs.  I had considered doing the same thing, but couldn't
    find any tool that would easily extract LoRAs from the models. Most google searches
    turned up Stable Diffusion tools to extract LoRAs.
  created_at: 2023-11-16 16:37:19+00:00
  edited: false
  hidden: false
  id: 6556453f65d7a6ef56924803
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6313247d289cf15634c514bd/Qoev8yYtZBor4xws6MIE6.png?w=200&h=200&f=face
      fullname: Rhys Jones
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rhysjones
      type: user
    createdAt: '2023-11-16T17:57:13.000Z'
    data:
      edited: false
      editors:
      - rhysjones
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9122305512428284
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6313247d289cf15634c514bd/Qoev8yYtZBor4xws6MIE6.png?w=200&h=200&f=face
          fullname: Rhys Jones
          isHf: false
          isPro: false
          name: rhysjones
          type: user
        html: '<p>Thanks! A multi-lora setup on a base model is good approach for
          a multitude-of-experts type workflow like HelixNet. It''s scalable too with
          new approaches such as <a rel="nofollow" href="https://github.com/S-LoRA/S-LoRA">S-LoRA</a>
          allowing thousands of LoRAs to be efficiently served off the same base.</p>

          <p>For extracting the LoRA from its merged model, I used this for the SVD
          factorizing: <a rel="nofollow" href="https://github.com/uukuguy/multi_loras/blob/main/multi_loras/extract_lora.py">https://github.com/uukuguy/multi_loras</a>.
          I ended up generating a range of LoRAs of different ranks to try (32, 48,
          64 and 128) and settled on the 64 one for use in this project. The higher
          the rank, the greater the LoRA''s fidelity but at the expense of size.</p>

          '
        raw: 'Thanks! A multi-lora setup on a base model is good approach for a multitude-of-experts
          type workflow like HelixNet. It''s scalable too with new approaches such
          as [S-LoRA](https://github.com/S-LoRA/S-LoRA) allowing thousands of LoRAs
          to be efficiently served off the same base.


          For extracting the LoRA from its merged model, I used this for the SVD factorizing:
          [https://github.com/uukuguy/multi_loras](https://github.com/uukuguy/multi_loras/blob/main/multi_loras/extract_lora.py).
          I ended up generating a range of LoRAs of different ranks to try (32, 48,
          64 and 128) and settled on the 64 one for use in this project. The higher
          the rank, the greater the LoRA''s fidelity but at the expense of size.'
        updatedAt: '2023-11-16T17:57:13.816Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 655657f918320e0e3fa04e2a
    type: comment
  author: rhysjones
  content: 'Thanks! A multi-lora setup on a base model is good approach for a multitude-of-experts
    type workflow like HelixNet. It''s scalable too with new approaches such as [S-LoRA](https://github.com/S-LoRA/S-LoRA)
    allowing thousands of LoRAs to be efficiently served off the same base.


    For extracting the LoRA from its merged model, I used this for the SVD factorizing:
    [https://github.com/uukuguy/multi_loras](https://github.com/uukuguy/multi_loras/blob/main/multi_loras/extract_lora.py).
    I ended up generating a range of LoRAs of different ranks to try (32, 48, 64 and
    128) and settled on the 64 one for use in this project. The higher the rank, the
    greater the LoRA''s fidelity but at the expense of size.'
  created_at: 2023-11-16 17:57:13+00:00
  edited: false
  hidden: false
  id: 655657f918320e0e3fa04e2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-16T20:23:34.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9334536790847778
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Thanks for the pointer to multi_loras! I''ll give it a go.</p>

          '
        raw: Thanks for the pointer to multi_loras! I'll give it a go.
        updatedAt: '2023-11-16T20:23:34.511Z'
      numEdits: 0
      reactions: []
    id: 65567a4682457f75dd8f986a
    type: comment
  author: LoneStriker
  content: Thanks for the pointer to multi_loras! I'll give it a go.
  created_at: 2023-11-16 20:23:34+00:00
  edited: false
  hidden: false
  id: 65567a4682457f75dd8f986a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rhysjones/HelixNet-LMoE-6.0bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: How did you extract LoRAs?
