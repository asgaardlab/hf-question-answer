!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FenixInDarkSolo
conflicting_files: null
created_at: 2023-06-06 10:24:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-06-06T11:24:11.000Z'
    data:
      edited: true
      editors:
      - FenixInDarkSolo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6109861135482788
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>I down the q4_0 and q8_0 models to test, but it cannot load in koboldcpp
          1.28.<br>I have checked the SHA256 and confirm both of them are correct.</p>

          <pre><code>&gt; koboldcpp_128.exe --threads 12 --smartcontext --unbantokens
          --contextsize 2048 --blasbatchsize 1024 --useclblast 0 0 --gpulayers 3

          Welcome to KoboldCpp - Version 1.28

          For command line arguments, please refer to --help

          Otherwise, please manually select ggml file:

          Attempting to use CLBlast library for faster prompt ingestion. A compatible
          clblast will be required.

          Initializing dynamic library: koboldcpp_clblast.dll

          ==========

          Loading model: D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

          [Threads: 12, BlasThreads: 12, SmartContext: True]


          ---

          Identified as LLAMA model: (ver 5)

          Attempting to Load...

          ---

          System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI
          = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |

          llama.cpp: loading model from D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

          error loading model: unrecognized tensor type 14


          llama_init_from_file: failed to load model

          gpttype_load_model: error: failed to load model ''D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin''

          Load Model OK: False

          Could not load model: D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

          </code></pre>

          <p>But I can successfully load it in the llama.cpp.</p>

          '
        raw: 'I down the q4_0 and q8_0 models to test, but it cannot load in koboldcpp
          1.28.

          I have checked the SHA256 and confirm both of them are correct.

          ```

          > koboldcpp_128.exe --threads 12 --smartcontext --unbantokens --contextsize
          2048 --blasbatchsize 1024 --useclblast 0 0 --gpulayers 3

          Welcome to KoboldCpp - Version 1.28

          For command line arguments, please refer to --help

          Otherwise, please manually select ggml file:

          Attempting to use CLBlast library for faster prompt ingestion. A compatible
          clblast will be required.

          Initializing dynamic library: koboldcpp_clblast.dll

          ==========

          Loading model: D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

          [Threads: 12, BlasThreads: 12, SmartContext: True]


          ---

          Identified as LLAMA model: (ver 5)

          Attempting to Load...

          ---

          System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI
          = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |

          llama.cpp: loading model from D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

          error loading model: unrecognized tensor type 14


          llama_init_from_file: failed to load model

          gpttype_load_model: error: failed to load model ''D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin''

          Load Model OK: False

          Could not load model: D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

          ```

          But I can successfully load it in the llama.cpp.'
        updatedAt: '2023-06-06T11:25:11.213Z'
      numEdits: 1
      reactions: []
    id: 647f175b35bc6d6aa5f9ffea
    type: comment
  author: FenixInDarkSolo
  content: 'I down the q4_0 and q8_0 models to test, but it cannot load in koboldcpp
    1.28.

    I have checked the SHA256 and confirm both of them are correct.

    ```

    > koboldcpp_128.exe --threads 12 --smartcontext --unbantokens --contextsize 2048
    --blasbatchsize 1024 --useclblast 0 0 --gpulayers 3

    Welcome to KoboldCpp - Version 1.28

    For command line arguments, please refer to --help

    Otherwise, please manually select ggml file:

    Attempting to use CLBlast library for faster prompt ingestion. A compatible clblast
    will be required.

    Initializing dynamic library: koboldcpp_clblast.dll

    ==========

    Loading model: D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

    [Threads: 12, BlasThreads: 12, SmartContext: True]


    ---

    Identified as LLAMA model: (ver 5)

    Attempting to Load...

    ---

    System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI =
    0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0
    | BLAS = 1 | SSE3 = 1 | VSX = 0 |

    llama.cpp: loading model from D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

    error loading model: unrecognized tensor type 14


    llama_init_from_file: failed to load model

    gpttype_load_model: error: failed to load model ''D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin''

    Load Model OK: False

    Could not load model: D:\program\koboldcpp\planner-7b.ggmlv3.q8_0.bin

    ```

    But I can successfully load it in the llama.cpp.'
  created_at: 2023-06-06 10:24:11+00:00
  edited: true
  hidden: false
  id: 647f175b35bc6d6aa5f9ffea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-06-06T11:25:52.000Z'
    data:
      from: Cannot load the model in Koboldcpp
      to: Cannot load the model in Koboldcpp 1.28
    id: 647f17c09c310244579e7968
    type: title-change
  author: FenixInDarkSolo
  created_at: 2023-06-06 10:25:52+00:00
  id: 647f17c09c310244579e7968
  new_title: Cannot load the model in Koboldcpp 1.28
  old_title: Cannot load the model in Koboldcpp
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-06T11:42:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9313600659370422
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Shit. I hadn''t realised that the new llama.cpp k-quant commit had
          changed q4_0, q4_1, q5_0, q5_1 and q8_0.</p>

          <p>I happened to do this model 2 hours after the k-quant PR was merged (<a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1684">https://github.com/ggerganov/llama.cpp/pull/1684</a>)
          so yeah the files only work with latest llama.cpp .</p>

          <p>I am sure koboldcpp will add support pretty soon, but for now they won''t
          work.</p>

          <p>I''ll see about re-doing them with previous llama.cpp.  I think for the
          next week or two I''m going to do q4_0, q4_1, q5_0, q5_1 and q8_0 using
          llama.cpp before k-quant, and just do the new k-quant methods using the
          latest code.  To ensure maximum compatibility.</p>

          '
        raw: 'Shit. I hadn''t realised that the new llama.cpp k-quant commit had changed
          q4_0, q4_1, q5_0, q5_1 and q8_0.


          I happened to do this model 2 hours after the k-quant PR was merged (https://github.com/ggerganov/llama.cpp/pull/1684)
          so yeah the files only work with latest llama.cpp .


          I am sure koboldcpp will add support pretty soon, but for now they won''t
          work.


          I''ll see about re-doing them with previous llama.cpp.  I think for the
          next week or two I''m going to do q4_0, q4_1, q5_0, q5_1 and q8_0 using
          llama.cpp before k-quant, and just do the new k-quant methods using the
          latest code.  To ensure maximum compatibility.'
        updatedAt: '2023-06-06T11:42:25.205Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - FenixInDarkSolo
        - jbfink
    id: 647f1ba1f41cf810e37784c0
    type: comment
  author: TheBloke
  content: 'Shit. I hadn''t realised that the new llama.cpp k-quant commit had changed
    q4_0, q4_1, q5_0, q5_1 and q8_0.


    I happened to do this model 2 hours after the k-quant PR was merged (https://github.com/ggerganov/llama.cpp/pull/1684)
    so yeah the files only work with latest llama.cpp .


    I am sure koboldcpp will add support pretty soon, but for now they won''t work.


    I''ll see about re-doing them with previous llama.cpp.  I think for the next week
    or two I''m going to do q4_0, q4_1, q5_0, q5_1 and q8_0 using llama.cpp before
    k-quant, and just do the new k-quant methods using the latest code.  To ensure
    maximum compatibility.'
  created_at: 2023-06-06 10:42:25+00:00
  edited: false
  hidden: false
  id: 647f1ba1f41cf810e37784c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a9f41f9cb0a5ad90d0dd2ab568e38a5e.svg
      fullname: Harold
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HaroldB
      type: user
    createdAt: '2023-06-06T16:51:35.000Z'
    data:
      edited: false
      editors:
      - HaroldB
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7178252935409546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a9f41f9cb0a5ad90d0dd2ab568e38a5e.svg
          fullname: Harold
          isHf: false
          isPro: false
          name: HaroldB
          type: user
        html: '<p>I''m running into a similar issue with  llama-cpp. Have not dug
          deep yet. </p>

          <blockquote>

          <p>llama.cpp: loading model from ../python3.11/site-packages/llama_cpp/models/7B/planner-7b.ggmlv3.q5_0.bin<br>error
          loading model: unrecognized tensor type 14<br>llama_init_from_file: failed
          to load model</p>

          </blockquote>

          '
        raw: "I'm running into a similar issue with  llama-cpp. Have not dug deep\
          \ yet. \n\n >llama.cpp: loading model from ../python3.11/site-packages/llama_cpp/models/7B/planner-7b.ggmlv3.q5_0.bin\n\
          >error loading model: unrecognized tensor type 14\n>llama_init_from_file:\
          \ failed to load model"
        updatedAt: '2023-06-06T16:51:35.832Z'
      numEdits: 0
      reactions: []
    id: 647f6417f41cf810e3815e1d
    type: comment
  author: HaroldB
  content: "I'm running into a similar issue with  llama-cpp. Have not dug deep yet.\
    \ \n\n >llama.cpp: loading model from ../python3.11/site-packages/llama_cpp/models/7B/planner-7b.ggmlv3.q5_0.bin\n\
    >error loading model: unrecognized tensor type 14\n>llama_init_from_file: failed\
    \ to load model"
  created_at: 2023-06-06 15:51:35+00:00
  edited: false
  hidden: false
  id: 647f6417f41cf810e3815e1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-06T16:54:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8948650360107422
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah sorry. Right now the files can only be used with latest llama.cpp.</p>

          <p>I will re-generate them shortly</p>

          '
        raw: 'Yeah sorry. Right now the files can only be used with latest llama.cpp.


          I will re-generate them shortly'
        updatedAt: '2023-06-06T16:54:58.320Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FenixInDarkSolo
    id: 647f64e22a7bcaa307a96496
    type: comment
  author: TheBloke
  content: 'Yeah sorry. Right now the files can only be used with latest llama.cpp.


    I will re-generate them shortly'
  created_at: 2023-06-06 15:54:58+00:00
  edited: false
  hidden: false
  id: 647f64e22a7bcaa307a96496
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-06T19:41:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9688199758529663
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I have updated all the old quant types: q4_0, q4_1, q5_0, q5_1,
          q8_0.  They were now generated with an older version of llama.cpp</p>

          <p>Please re-download, re-test and let me know.</p>

          '
        raw: 'I have updated all the old quant types: q4_0, q4_1, q5_0, q5_1, q8_0.  They
          were now generated with an older version of llama.cpp


          Please re-download, re-test and let me know.'
        updatedAt: '2023-06-06T19:41:08.158Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - FenixInDarkSolo
    id: 647f8bd49c31024457abdd63
    type: comment
  author: TheBloke
  content: 'I have updated all the old quant types: q4_0, q4_1, q5_0, q5_1, q8_0.  They
    were now generated with an older version of llama.cpp


    Please re-download, re-test and let me know.'
  created_at: 2023-06-06 18:41:08+00:00
  edited: false
  hidden: false
  id: 647f8bd49c31024457abdd63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-06T20:34:17.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8046472668647766
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>I have a custom automatic updater for Koboldcpp on Windows, if anyone
          is interested.</p>

          '
        raw: I have a custom automatic updater for Koboldcpp on Windows, if anyone
          is interested.
        updatedAt: '2023-06-06T20:34:17.897Z'
      numEdits: 0
      reactions: []
    id: 647f98491637c1c0e6ec6b59
    type: comment
  author: MrDevolver
  content: I have a custom automatic updater for Koboldcpp on Windows, if anyone is
    interested.
  created_at: 2023-06-06 19:34:17+00:00
  edited: false
  hidden: false
  id: 647f98491637c1c0e6ec6b59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-06-07T07:56:34.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8160060048103333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>I have download the q4_0 again and test it in koboldcpp 1.28. And
          it works. Thank you for the fix.</p>

          '
        raw: I have download the q4_0 again and test it in koboldcpp 1.28. And it
          works. Thank you for the fix.
        updatedAt: '2023-06-07T07:56:34.812Z'
      numEdits: 0
      reactions: []
    id: 64803832a96905a48074d6d1
    type: comment
  author: FenixInDarkSolo
  content: I have download the q4_0 again and test it in koboldcpp 1.28. And it works.
    Thank you for the fix.
  created_at: 2023-06-07 06:56:34+00:00
  edited: false
  hidden: false
  id: 64803832a96905a48074d6d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-07T18:25:21.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9549760222434998
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: "<p>KoboldCpp was updated to 1.29 recently and offers a partial support\
          \ for k-quantizers. Partial because it only supports openblas for now, not\
          \ Clblast yet. \U0001F615</p>\n"
        raw: "KoboldCpp was updated to 1.29 recently and offers a partial support\
          \ for k-quantizers. Partial because it only supports openblas for now, not\
          \ Clblast yet. \U0001F615"
        updatedAt: '2023-06-07T18:25:21.907Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - onealeph0cc
    id: 6480cb9140facadc55726135
    type: comment
  author: MrDevolver
  content: "KoboldCpp was updated to 1.29 recently and offers a partial support for\
    \ k-quantizers. Partial because it only supports openblas for now, not Clblast\
    \ yet. \U0001F615"
  created_at: 2023-06-07 17:25:21+00:00
  edited: false
  hidden: false
  id: 6480cb9140facadc55726135
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Planner-7B-GGML
repo_type: model
status: open
target_branch: null
title: Cannot load the model in Koboldcpp 1.28
