!!python/object:huggingface_hub.community.DiscussionWithDetails
author: katossky
conflicting_files: null
created_at: 2023-04-17 12:48:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6fbdfab969e22ebdc67b85e6e3220adf.svg
      fullname: Arthur Katossky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katossky
      type: user
    createdAt: '2023-04-17T13:48:59.000Z'
    data:
      edited: false
      editors:
      - katossky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6fbdfab969e22ebdc67b85e6e3220adf.svg
          fullname: Arthur Katossky
          isHf: false
          isPro: false
          name: katossky
          type: user
        html: "<p>Hello,</p>\n<p>When trying to use this model, on top of <code>decapoda-research/llama-7b-hf</code>,\
          \ I get a cryptic</p>\n<pre><code>'NoneType' object has no attribute 'device'`\n\
          </code></pre>\n<p> issue.</p>\n<p>I found this issue to be potentially linked\
          \ to :</p>\n<ul>\n<li>an interplay between <code>bitsandbyte</code> and\
          \ <code>accelerate</code>'s <code>device_map=\"auto\"</code>, <del>supposedly\
          \ fixed by <a rel=\"nofollow\" href=\"https://github.com/huggingface/accelerate/pull/1237\"\
          >a recent PR</a></del> , but updating <code>accelerate</code> did not help\
          \ <strong>(hm, it looks like <a rel=\"nofollow\" href=\"https://github.com/huggingface/accelerate/releases/tag/v0.18.0\"\
          >it has actually not been released yet</a>)</strong></li>\n<li>an issue\
          \ wit <code>peft</code> itself, and forcing <code>device_map={\"\":0}</code>\
          \ <a rel=\"nofollow\" href=\"https://github.com/tloen/alpaca-lora/issues/14\"\
          >presented as a work-around</a> ; did not work either</li>\n</ul>\n<p>I\
          \ tested this on both a single-GPU computer and a multi-GPU cluster, as\
          \ I suspected an issue with limited RAM or GPU memory, but this does not\
          \ seem to be the case (I get a more explicit error in that case).</p>\n\
          <p>Do you have ideas of where the issue could come from?</p>\n<pre><code>from\
          \ peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          from transformers import LlamaForCausalLM, LlamaTokenizer\nimport torch\n\
          \ntokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\"\
          )\nllama = LlamaForCausalLM.from_pretrained(\n    \"decapoda-research/llama-7b-hf\"\
          ,\n    load_in_8bit=True,\n    torch_dtype=torch.float16, # error message\
          \ from bitsandbytes asks for this\n    device_map=\"auto\", # this is superseded\
          \ by the second device_map\n)\nvigogne = PeftModel.from_pretrained(\n  \
          \  llama,\n    \"bofenghuang/vigogne-lora-7b\",\n    device_map={\"\":0}\
          \ # suggested work-around\n)\n</code></pre>\n"
        raw: "Hello,\r\n\r\nWhen trying to use this model, on top of `decapoda-research/llama-7b-hf`,\
          \ I get a cryptic\r\n```\r\n'NoneType' object has no attribute 'device'`\r\
          \n```\r\n issue.\r\n\r\nI found this issue to be potentially linked to :\r\
          \n- an interplay between `bitsandbyte` and `accelerate`'s `device_map=\"\
          auto\"`, ~supposedly fixed by [a recent PR](https://github.com/huggingface/accelerate/pull/1237)~\
          \ , but updating `accelerate` did not help **(hm, it looks like [it has\
          \ actually not been released yet](https://github.com/huggingface/accelerate/releases/tag/v0.18.0))**\r\
          \n- an issue wit `peft` itself, and forcing `device_map={\"\":0}` [presented\
          \ as a work-around](https://github.com/tloen/alpaca-lora/issues/14) ; did\
          \ not work either\r\n\r\nI tested this on both a single-GPU computer and\
          \ a multi-GPU cluster, as I suspected an issue with limited RAM or GPU memory,\
          \ but this does not seem to be the case (I get a more explicit error in\
          \ that case).\r\n\r\nDo you have ideas of where the issue could come from?\r\
          \n\r\n```\r\nfrom peft import PeftModel\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\r\
          \nimport torch\r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\"\
          )\r\nllama = LlamaForCausalLM.from_pretrained(\r\n    \"decapoda-research/llama-7b-hf\"\
          ,\r\n    load_in_8bit=True,\r\n    torch_dtype=torch.float16, # error message\
          \ from bitsandbytes asks for this\r\n    device_map=\"auto\", # this is\
          \ superseded by the second device_map\r\n)\r\nvigogne = PeftModel.from_pretrained(\r\
          \n    llama,\r\n    \"bofenghuang/vigogne-lora-7b\",\r\n    device_map={\"\
          \":0} # suggested work-around\r\n)\r\n```"
        updatedAt: '2023-04-17T13:48:59.519Z'
      numEdits: 0
      reactions: []
    id: 643d4e4bb561f17fa46a641f
    type: comment
  author: katossky
  content: "Hello,\r\n\r\nWhen trying to use this model, on top of `decapoda-research/llama-7b-hf`,\
    \ I get a cryptic\r\n```\r\n'NoneType' object has no attribute 'device'`\r\n```\r\
    \n issue.\r\n\r\nI found this issue to be potentially linked to :\r\n- an interplay\
    \ between `bitsandbyte` and `accelerate`'s `device_map=\"auto\"`, ~supposedly\
    \ fixed by [a recent PR](https://github.com/huggingface/accelerate/pull/1237)~\
    \ , but updating `accelerate` did not help **(hm, it looks like [it has actually\
    \ not been released yet](https://github.com/huggingface/accelerate/releases/tag/v0.18.0))**\r\
    \n- an issue wit `peft` itself, and forcing `device_map={\"\":0}` [presented as\
    \ a work-around](https://github.com/tloen/alpaca-lora/issues/14) ; did not work\
    \ either\r\n\r\nI tested this on both a single-GPU computer and a multi-GPU cluster,\
    \ as I suspected an issue with limited RAM or GPU memory, but this does not seem\
    \ to be the case (I get a more explicit error in that case).\r\n\r\nDo you have\
    \ ideas of where the issue could come from?\r\n\r\n```\r\nfrom peft import PeftModel\r\
    \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nfrom transformers\
    \ import LlamaForCausalLM, LlamaTokenizer\r\nimport torch\r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"\
    decapoda-research/llama-7b-hf\")\r\nllama = LlamaForCausalLM.from_pretrained(\r\
    \n    \"decapoda-research/llama-7b-hf\",\r\n    load_in_8bit=True,\r\n    torch_dtype=torch.float16,\
    \ # error message from bitsandbytes asks for this\r\n    device_map=\"auto\",\
    \ # this is superseded by the second device_map\r\n)\r\nvigogne = PeftModel.from_pretrained(\r\
    \n    llama,\r\n    \"bofenghuang/vigogne-lora-7b\",\r\n    device_map={\"\":0}\
    \ # suggested work-around\r\n)\r\n```"
  created_at: 2023-04-17 12:48:59+00:00
  edited: false
  hidden: false
  id: 643d4e4bb561f17fa46a641f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea3c8a9f178dfc1df3f75d71ecbe39dd.svg
      fullname: bofeng huang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bofenghuang
      type: user
    createdAt: '2023-04-24T21:23:31.000Z'
    data:
      edited: false
      editors:
      - bofenghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea3c8a9f178dfc1df3f75d71ecbe39dd.svg
          fullname: bofeng huang
          isHf: false
          isPro: false
          name: bofenghuang
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;katossky&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/katossky\"\
          >@<span class=\"underline\">katossky</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Sorry for my late response. I'm afraid that I cannot help because I can't\
          \ reproduce the problem :(</p>\n<p>Did you try to print <code>llama.device</code>\
          \ to see where the model has been loaded?</p>\n"
        raw: 'Hi @katossky,


          Sorry for my late response. I''m afraid that I cannot help because I can''t
          reproduce the problem :(


          Did you try to print `llama.device` to see where the model has been loaded?'
        updatedAt: '2023-04-24T21:23:31.673Z'
      numEdits: 0
      reactions: []
    id: 6446f3531dc59ca8aed3ad2f
    type: comment
  author: bofenghuang
  content: 'Hi @katossky,


    Sorry for my late response. I''m afraid that I cannot help because I can''t reproduce
    the problem :(


    Did you try to print `llama.device` to see where the model has been loaded?'
  created_at: 2023-04-24 20:23:31+00:00
  edited: false
  hidden: false
  id: 6446f3531dc59ca8aed3ad2f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bofenghuang/vigogne-7b-instruct
repo_type: model
status: open
target_branch: null
title: '''NoneType'' object has no attribute ''device'''
