!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mhemetfaik
conflicting_files: null
created_at: 2023-08-11 10:51:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24e0f1da6cc347460123709892b9c93e.svg
      fullname: "Mehmet Faik \u0130mamoglu"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mhemetfaik
      type: user
    createdAt: '2023-08-11T11:51:11.000Z'
    data:
      edited: false
      editors:
      - mhemetfaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8552088737487793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24e0f1da6cc347460123709892b9c93e.svg
          fullname: "Mehmet Faik \u0130mamoglu"
          isHf: false
          isPro: false
          name: mhemetfaik
          type: user
        html: '<p>Can you provide information about merging models?</p>

          '
        raw: Can you provide information about merging models?
        updatedAt: '2023-08-11T11:51:11.562Z'
      numEdits: 0
      reactions: []
    id: 64d620afabf475a808ab7335
    type: comment
  author: mhemetfaik
  content: Can you provide information about merging models?
  created_at: 2023-08-11 10:51:11+00:00
  edited: false
  hidden: false
  id: 64d620afabf475a808ab7335
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3be5d4df71b81c9e5a2255a507f73542.svg
      fullname: Weijing Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waleking
      type: user
    createdAt: '2023-08-11T20:46:13.000Z'
    data:
      edited: false
      editors:
      - waleking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7676886916160583
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3be5d4df71b81c9e5a2255a507f73542.svg
          fullname: Weijing Huang
          isHf: false
          isPro: false
          name: waleking
          type: user
        html: '<p>Did you merge garage-bAInd/Platypus2-70B''s LoRA with upstage/Llama-2-70b-instruct-v2?</p>

          '
        raw: Did you merge garage-bAInd/Platypus2-70B's LoRA with upstage/Llama-2-70b-instruct-v2?
        updatedAt: '2023-08-11T20:46:13.022Z'
      numEdits: 0
      reactions: []
    id: 64d69e158767727dffc7a039
    type: comment
  author: waleking
  content: Did you merge garage-bAInd/Platypus2-70B's LoRA with upstage/Llama-2-70b-instruct-v2?
  created_at: 2023-08-11 19:46:13+00:00
  edited: false
  hidden: false
  id: 64d69e158767727dffc7a039
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638bcfa91987d67b340e6c1c/3tHCB_J6c4-lsEZ_zJSlp.jpeg?w=200&h=200&f=face
      fullname: Ariel N. Lee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: arielnlee
      type: user
    createdAt: '2023-08-12T18:48:57.000Z'
    data:
      edited: true
      editors:
      - arielnlee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9497271776199341
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638bcfa91987d67b340e6c1c/3tHCB_J6c4-lsEZ_zJSlp.jpeg?w=200&h=200&f=face
          fullname: Ariel N. Lee
          isHf: false
          isPro: false
          name: arielnlee
          type: user
        html: "<p>Thanks for your interest. It  is a simple linear merge (for now...stay\
          \ tuned). We played around with the different types of LoRA modules, how\
          \ the training data affects the outcome of the merge, how merging fine-tunes\
          \ that used different LoRA modules works, etc. </p>\n<p>From our experience,\
          \ the outcome of merging two (or more) LoRA based models is very much dependent\
          \ on 1) the LoRA modules both merged models were fine-tuned with (i.e. did\
          \ one model use up/down/gate proj and the other k/v/q/o proj 2) the training\
          \ data, 3) the performance of both original models on whatever benchmarks\
          \ you're using, and 4) (I think, but am still working on quantitative tests\
          \ to explore this) the order of the LoRA merge. I believe the order of the\
          \ merge also affects the \"expertise\" of the model. </p>\n<p>Edit: Since\
          \ this is additive, one would think order shouldn't matter (which is why\
          \ it is not discussed in the paper we recently released). I only started\
          \ looking into it because when we originally merged Platypus-70B with Dolphin,\
          \ it was the only merge we had at the time that actually did worse than\
          \ its original counterpart (the rest of our merges were better than both\
          \ originals). If you're interested, follow-up with me in a week and hopefully\
          \ I'll have additional insight and quantitative experiments to share! \u263A\
          \uFE0F</p>\n"
        raw: "Thanks for your interest. It  is a simple linear merge (for now...stay\
          \ tuned). We played around with the different types of LoRA modules, how\
          \ the training data affects the outcome of the merge, how merging fine-tunes\
          \ that used different LoRA modules works, etc. \n\nFrom our experience,\
          \ the outcome of merging two (or more) LoRA based models is very much dependent\
          \ on 1) the LoRA modules both merged models were fine-tuned with (i.e. did\
          \ one model use up/down/gate proj and the other k/v/q/o proj 2) the training\
          \ data, 3) the performance of both original models on whatever benchmarks\
          \ you're using, and 4) (I think, but am still working on quantitative tests\
          \ to explore this) the order of the LoRA merge. I believe the order of the\
          \ merge also affects the \"expertise\" of the model. \n\nEdit: Since this\
          \ is additive, one would think order shouldn't matter (which is why it is\
          \ not discussed in the paper we recently released). I only started looking\
          \ into it because when we originally merged Platypus-70B with Dolphin, it\
          \ was the only merge we had at the time that actually did worse than its\
          \ original counterpart (the rest of our merges were better than both originals).\
          \ If you're interested, follow-up with me in a week and hopefully I'll have\
          \ additional insight and quantitative experiments to share! \u263A\uFE0F"
        updatedAt: '2023-08-12T18:58:59.758Z'
      numEdits: 1
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - Equim
        - lauspectrum
        - waleking
        - marian-lambert
        - idleaaron
        - monuminu
    id: 64d7d419620c17bfa087b7e6
    type: comment
  author: arielnlee
  content: "Thanks for your interest. It  is a simple linear merge (for now...stay\
    \ tuned). We played around with the different types of LoRA modules, how the training\
    \ data affects the outcome of the merge, how merging fine-tunes that used different\
    \ LoRA modules works, etc. \n\nFrom our experience, the outcome of merging two\
    \ (or more) LoRA based models is very much dependent on 1) the LoRA modules both\
    \ merged models were fine-tuned with (i.e. did one model use up/down/gate proj\
    \ and the other k/v/q/o proj 2) the training data, 3) the performance of both\
    \ original models on whatever benchmarks you're using, and 4) (I think, but am\
    \ still working on quantitative tests to explore this) the order of the LoRA merge.\
    \ I believe the order of the merge also affects the \"expertise\" of the model.\
    \ \n\nEdit: Since this is additive, one would think order shouldn't matter (which\
    \ is why it is not discussed in the paper we recently released). I only started\
    \ looking into it because when we originally merged Platypus-70B with Dolphin,\
    \ it was the only merge we had at the time that actually did worse than its original\
    \ counterpart (the rest of our merges were better than both originals). If you're\
    \ interested, follow-up with me in a week and hopefully I'll have additional insight\
    \ and quantitative experiments to share! \u263A\uFE0F"
  created_at: 2023-08-12 17:48:57+00:00
  edited: true
  hidden: false
  id: 64d7d419620c17bfa087b7e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/357d79002f5edda49f228aad9d7f5c66.svg
      fullname: LAUSpectrum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lauspectrum
      type: user
    createdAt: '2023-08-14T11:26:54.000Z'
    data:
      edited: false
      editors:
      - lauspectrum
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9708394408226013
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/357d79002f5edda49f228aad9d7f5c66.svg
          fullname: LAUSpectrum
          isHf: false
          isPro: false
          name: lauspectrum
          type: user
        html: '<p>I''m curious about if the kind of "merge" you mentioned means that
          you trained a LoRA adapter on "one" base model and applied on "another".
          I would be extremely grateful if I could receive your reply. : )</p>

          '
        raw: 'I''m curious about if the kind of "merge" you mentioned means that you
          trained a LoRA adapter on "one" base model and applied on "another". I would
          be extremely grateful if I could receive your reply. : )'
        updatedAt: '2023-08-14T11:26:54.806Z'
      numEdits: 0
      reactions: []
    id: 64da0f7e322a5774e07be849
    type: comment
  author: lauspectrum
  content: 'I''m curious about if the kind of "merge" you mentioned means that you
    trained a LoRA adapter on "one" base model and applied on "another". I would be
    extremely grateful if I could receive your reply. : )'
  created_at: 2023-08-14 10:26:54+00:00
  edited: false
  hidden: false
  id: 64da0f7e322a5774e07be849
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e750058f3a364181ecc1728692639e8e.svg
      fullname: CrazyAIGC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CrazyAIGC
      type: user
    createdAt: '2023-08-14T13:33:07.000Z'
    data:
      edited: false
      editors:
      - CrazyAIGC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9708394408226013
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e750058f3a364181ecc1728692639e8e.svg
          fullname: CrazyAIGC
          isHf: false
          isPro: false
          name: CrazyAIGC
          type: user
        html: '<p>I''m curious about if the kind of "merge" you mentioned means that
          you trained a LoRA adapter on "one" base model and applied on "another".
          I would be extremely grateful if I could receive your reply. : )</p>

          '
        raw: 'I''m curious about if the kind of "merge" you mentioned means that you
          trained a LoRA adapter on "one" base model and applied on "another". I would
          be extremely grateful if I could receive your reply. : )'
        updatedAt: '2023-08-14T13:33:07.062Z'
      numEdits: 0
      reactions: []
    id: 64da2d13888b7e9c40f8a63b
    type: comment
  author: CrazyAIGC
  content: 'I''m curious about if the kind of "merge" you mentioned means that you
    trained a LoRA adapter on "one" base model and applied on "another". I would be
    extremely grateful if I could receive your reply. : )'
  created_at: 2023-08-14 12:33:07+00:00
  edited: false
  hidden: false
  id: 64da2d13888b7e9c40f8a63b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
      fullname: Manoranjan Rajguru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monuminu
      type: user
    createdAt: '2023-08-17T10:30:17.000Z'
    data:
      edited: false
      editors:
      - monuminu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9255915284156799
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
          fullname: Manoranjan Rajguru
          isHf: false
          isPro: false
          name: monuminu
          type: user
        html: '<p>Is there any notebook explaining how do you merge two models . I
          am curious how exactly this been done </p>

          '
        raw: 'Is there any notebook explaining how do you merge two models . I am
          curious how exactly this been done '
        updatedAt: '2023-08-17T10:30:17.945Z'
      numEdits: 0
      reactions: []
    id: 64ddf6b98761a0f302790719
    type: comment
  author: monuminu
  content: 'Is there any notebook explaining how do you merge two models . I am curious
    how exactly this been done '
  created_at: 2023-08-17 09:30:17+00:00
  edited: false
  hidden: false
  id: 64ddf6b98761a0f302790719
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
      fullname: Komposter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Komposter43
      type: user
    createdAt: '2023-08-17T16:25:57.000Z'
    data:
      edited: false
      editors:
      - Komposter43
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7658814191818237
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
          fullname: Komposter
          isHf: false
          isPro: false
          name: Komposter43
          type: user
        html: '<p>I think, somethink like this <a rel="nofollow" href="https://github.com/ontocord/MDEL/tree/main/Model%20Merge%20And%20Analysis%20Tools">https://github.com/ontocord/MDEL/tree/main/Model%20Merge%20And%20Analysis%20Tools</a></p>

          '
        raw: I think, somethink like this https://github.com/ontocord/MDEL/tree/main/Model%20Merge%20And%20Analysis%20Tools
        updatedAt: '2023-08-17T16:25:57.834Z'
      numEdits: 0
      reactions: []
    id: 64de4a153d3a7519f176634e
    type: comment
  author: Komposter43
  content: I think, somethink like this https://github.com/ontocord/MDEL/tree/main/Model%20Merge%20And%20Analysis%20Tools
  created_at: 2023-08-17 15:25:57+00:00
  edited: false
  hidden: false
  id: 64de4a153d3a7519f176634e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
      fullname: Komposter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Komposter43
      type: user
    createdAt: '2023-08-17T16:32:29.000Z'
    data:
      edited: false
      editors:
      - Komposter43
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7255157828330994
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
          fullname: Komposter
          isHf: false
          isPro: false
          name: Komposter43
          type: user
        html: '<p>Or it is posible native Peft:</p>

          <h1 id="merge-weights---new-merging-method-from-peft">merge weights - new
          merging method from peft</h1>

          <p>lora_model = lora_model.merge_and_unload()</p>

          <p><a rel="nofollow" href="https://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py">https://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py</a></p>

          <p>Can you describe your experence?</p>

          <p>Thank you.</p>

          '
        raw: 'Or it is posible native Peft:


          # merge weights - new merging method from peft

          lora_model = lora_model.merge_and_unload()


          https://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py


          Can you describe your experence?


          Thank you.'
        updatedAt: '2023-08-17T16:32:29.472Z'
      numEdits: 0
      reactions: []
    id: 64de4b9d8761a0f30284ac75
    type: comment
  author: Komposter43
  content: 'Or it is posible native Peft:


    # merge weights - new merging method from peft

    lora_model = lora_model.merge_and_unload()


    https://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py


    Can you describe your experence?


    Thank you.'
  created_at: 2023-08-17 15:32:29+00:00
  edited: false
  hidden: false
  id: 64de4b9d8761a0f30284ac75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
      fullname: Komposter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Komposter43
      type: user
    createdAt: '2023-08-17T16:35:53.000Z'
    data:
      edited: false
      editors:
      - Komposter43
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3297955393791199
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
          fullname: Komposter
          isHf: false
          isPro: false
          name: Komposter43
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/tloen/alpaca-lora/pull/548/commits/7edf9c7037c073cb5dc0f69557d4cdeb313bbdd0">https://github.com/tloen/alpaca-lora/pull/548/commits/7edf9c7037c073cb5dc0f69557d4cdeb313bbdd0</a></p>

          '
        raw: https://github.com/tloen/alpaca-lora/pull/548/commits/7edf9c7037c073cb5dc0f69557d4cdeb313bbdd0
        updatedAt: '2023-08-17T16:35:53.806Z'
      numEdits: 0
      reactions: []
    id: 64de4c69d27135dd565568c4
    type: comment
  author: Komposter43
  content: https://github.com/tloen/alpaca-lora/pull/548/commits/7edf9c7037c073cb5dc0f69557d4cdeb313bbdd0
  created_at: 2023-08-17 15:35:53+00:00
  edited: false
  hidden: false
  id: 64de4c69d27135dd565568c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
      fullname: Komposter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Komposter43
      type: user
    createdAt: '2023-08-17T16:40:56.000Z'
    data:
      edited: false
      editors:
      - Komposter43
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6166290044784546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
          fullname: Komposter
          isHf: false
          isPro: false
          name: Komposter43
          type: user
        html: '<p>Peft algorithm of merging <a rel="nofollow" href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L554">https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L554</a></p>

          '
        raw: Peft algorithm of merging https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L554
        updatedAt: '2023-08-17T16:40:56.264Z'
      numEdits: 0
      reactions: []
    id: 64de4d98808492ba6e6cde23
    type: comment
  author: Komposter43
  content: Peft algorithm of merging https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L554
  created_at: 2023-08-17 15:40:56+00:00
  edited: false
  hidden: false
  id: 64de4d98808492ba6e6cde23
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: garage-bAInd/Platypus2-70B-instruct
repo_type: model
status: open
target_branch: null
title: How do you merge models
