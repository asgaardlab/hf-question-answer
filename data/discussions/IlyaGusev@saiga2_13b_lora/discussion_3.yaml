!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SergeyOvchinnikov
conflicting_files: null
created_at: 2023-12-13 15:47:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d4c41710a343b37eb715cf80fc4b17e.svg
      fullname: Sergey Ovchinnikov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SergeyOvchinnikov
      type: user
    createdAt: '2023-12-13T15:47:12.000Z'
    data:
      edited: false
      editors:
      - SergeyOvchinnikov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9288870692253113
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d4c41710a343b37eb715cf80fc4b17e.svg
          fullname: Sergey Ovchinnikov
          isHf: false
          isPro: false
          name: SergeyOvchinnikov
          type: user
        html: '<p>Hello Ilya!<br>First of all, thank you very much for the model you
          have cretaed. Great job!</p>

          <p>Please advise whether your model is prepared to run under 16bit (i.e.
          without flag load_in_8bit=true)?<br>I can see a strange behavior that under
          8bit model inference reletavely slow (i.e. 4-5 tokens per sec even on A100
          GPU), but text responses quality is good.<br>If I switch 8bit mode OFF,
          i.e. turn 16bit mode ON it works much faster (i.e. 15-20 tokens per sec
          on A100) but text response quality much lower and responses are shorter
          than with 8bit mode.<br>My promts are in Russian.<br>I wonder whether your
          LORA layer is only for 8bits and this layer takes extra time during inference?<br>Do
          you see a configuration to run the model with good quality responses (as
          with 8bit) but fast as in 16bit? Considering that I have hardware enough.<br>Thank
          you!</p>

          '
        raw: "Hello Ilya!\r\nFirst of all, thank you very much for the model you have\
          \ cretaed. Great job!\r\n\r\nPlease advise whether your model is prepared\
          \ to run under 16bit (i.e. without flag load_in_8bit=true)? \r\nI can see\
          \ a strange behavior that under 8bit model inference reletavely slow (i.e.\
          \ 4-5 tokens per sec even on A100 GPU), but text responses quality is good.\r\
          \nIf I switch 8bit mode OFF, i.e. turn 16bit mode ON it works much faster\
          \ (i.e. 15-20 tokens per sec on A100) but text response quality much lower\
          \ and responses are shorter than with 8bit mode.\r\nMy promts are in Russian.\
          \ \r\nI wonder whether your LORA layer is only for 8bits and this layer\
          \ takes extra time during inference?\r\nDo you see a configuration to run\
          \ the model with good quality responses (as with 8bit) but fast as in 16bit?\
          \ Considering that I have hardware enough.\r\nThank you!\r\n"
        updatedAt: '2023-12-13T15:47:12.584Z'
      numEdits: 0
      reactions: []
    id: 6579d20000f685a3be04ac00
    type: comment
  author: SergeyOvchinnikov
  content: "Hello Ilya!\r\nFirst of all, thank you very much for the model you have\
    \ cretaed. Great job!\r\n\r\nPlease advise whether your model is prepared to run\
    \ under 16bit (i.e. without flag load_in_8bit=true)? \r\nI can see a strange behavior\
    \ that under 8bit model inference reletavely slow (i.e. 4-5 tokens per sec even\
    \ on A100 GPU), but text responses quality is good.\r\nIf I switch 8bit mode OFF,\
    \ i.e. turn 16bit mode ON it works much faster (i.e. 15-20 tokens per sec on A100)\
    \ but text response quality much lower and responses are shorter than with 8bit\
    \ mode.\r\nMy promts are in Russian. \r\nI wonder whether your LORA layer is only\
    \ for 8bits and this layer takes extra time during inference?\r\nDo you see a\
    \ configuration to run the model with good quality responses (as with 8bit) but\
    \ fast as in 16bit? Considering that I have hardware enough.\r\nThank you!\r\n"
  created_at: 2023-12-13 15:47:12+00:00
  edited: false
  hidden: false
  id: 6579d20000f685a3be04ac00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-12-28T02:15:56.000Z'
    data:
      edited: false
      editors:
      - IlyaGusev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9504478573799133
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
          fullname: Ilya Gusev
          isHf: false
          isPro: true
          name: IlyaGusev
          type: user
        html: '<p>Of course, the base model''s precision is float16. You can always
          merge adapters into it.<br>There should be almost no difference between
          8 and 16 bits.</p>

          '
        raw: 'Of course, the base model''s precision is float16. You can always merge
          adapters into it.

          There should be almost no difference between 8 and 16 bits.'
        updatedAt: '2023-12-28T02:15:56.571Z'
      numEdits: 0
      reactions: []
    id: 658cda5c9a1397992a3cee97
    type: comment
  author: IlyaGusev
  content: 'Of course, the base model''s precision is float16. You can always merge
    adapters into it.

    There should be almost no difference between 8 and 16 bits.'
  created_at: 2023-12-28 02:15:56+00:00
  edited: false
  hidden: false
  id: 658cda5c9a1397992a3cee97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d4c41710a343b37eb715cf80fc4b17e.svg
      fullname: Sergey Ovchinnikov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SergeyOvchinnikov
      type: user
    createdAt: '2024-01-23T11:47:10.000Z'
    data:
      edited: true
      editors:
      - SergeyOvchinnikov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8901947736740112
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d4c41710a343b37eb715cf80fc4b17e.svg
          fullname: Sergey Ovchinnikov
          isHf: false
          isPro: false
          name: SergeyOvchinnikov
          type: user
        html: '<p>Hello again!<br>Please advise whether Your model is prepared to
          run with native 32-bit mode?<br>Is so, whould you be so kind to give an
          easy sample of code of "from_pretrained" with parameters how I can run the
          model in this mode?<br>Thanks in advance!</p>

          '
        raw: "Hello again!\nPlease advise whether Your model is prepared to run with\
          \ native 32-bit mode? \nIs so, whould you be so kind to give an easy sample\
          \ of code of \"from_pretrained\" with parameters how I can run the model\
          \ in this mode?\nThanks in advance!"
        updatedAt: '2024-01-23T11:47:20.761Z'
      numEdits: 1
      reactions: []
    id: 65afa73e5f62b76444259630
    type: comment
  author: SergeyOvchinnikov
  content: "Hello again!\nPlease advise whether Your model is prepared to run with\
    \ native 32-bit mode? \nIs so, whould you be so kind to give an easy sample of\
    \ code of \"from_pretrained\" with parameters how I can run the model in this\
    \ mode?\nThanks in advance!"
  created_at: 2024-01-23 11:47:10+00:00
  edited: true
  hidden: false
  id: 65afa73e5f62b76444259630
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: IlyaGusev/saiga2_13b_lora
repo_type: model
status: open
target_branch: null
title: Whether model is prepared for 16 bit also?
