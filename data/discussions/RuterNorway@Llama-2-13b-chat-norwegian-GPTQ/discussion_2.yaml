!!python/object:huggingface_hub.community.DiscussionWithDetails
author: marksverdhei
conflicting_files: null
created_at: 2023-09-23 11:50:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
      fullname: Markus Heiervang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marksverdhei
      type: user
    createdAt: '2023-09-23T12:50:10.000Z'
    data:
      edited: false
      editors:
      - marksverdhei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9397644400596619
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
          fullname: Markus Heiervang
          isHf: false
          isPro: false
          name: marksverdhei
          type: user
        html: "<p>Since \U0001F917transformers now has native support for GPTQ-quantized\
          \ models,<br>quantized models can now be loaded and used just by calling<br><code>AutoModelForCausalLM.from_pretrained('your_model')</code><br>TheBloke\
          \ GPTQ models already support this but yours doesn't yet.<br>It would be\
          \ nice to see this change, as it then could be directly used in many scripts\
          \ without much code alteration.</p>\n<p>I have already done this on a private\
          \ repo, so I'll let you know the steps I took to make it work:</p>\n<ol>\n\
          <li>Rename the safetensors model file to model.safetensors</li>\n<li>The\
          \ safetensors file lacks metadata, which the \U0001F917transformers backend\
          \ relies on. Therefore, add metadata.</li>\n</ol>\n<p>I used the safetensors\
          \ util to add the metadata <a rel=\"nofollow\" href=\"https://github.com/by321/safetensors_util\"\
          >https://github.com/by321/safetensors_util</a><br>I just added the metadata\
          \ equivalent of TheBLoke's llama2 variant, which was the following config</p>\n\
          <pre><code>{\n    \"__metadata__\": {\n        \"format\": \"pt\",\n   \
          \     \"quantized_by\": \"RuterNorway\"\n    }\n}\n</code></pre>\n<p>If\
          \ you'd like, I could make a pull reuqest, but I figured you might just\
          \ do it youself so you don't have to spend time verifying everything.</p>\n"
        raw: "Since \U0001F917transformers now has native support for GPTQ-quantized\
          \ models, \r\nquantized models can now be loaded and used just by calling\r\
          \n`AutoModelForCausalLM.from_pretrained('your_model')`\r\nTheBloke GPTQ\
          \ models already support this but yours doesn't yet.\r\nIt would be nice\
          \ to see this change, as it then could be directly used in many scripts\
          \ without much code alteration.\r\n\r\nI have already done this on a private\
          \ repo, so I'll let you know the steps I took to make it work:\r\n\r\n1.\
          \ Rename the safetensors model file to model.safetensors\r\n2. The safetensors\
          \ file lacks metadata, which the \U0001F917transformers backend relies on.\
          \ Therefore, add metadata.\r\n\r\nI used the safetensors util to add the\
          \ metadata https://github.com/by321/safetensors_util\r\nI just added the\
          \ metadata equivalent of TheBLoke's llama2 variant, which was the following\
          \ config\r\n```\r\n{\r\n    \"__metadata__\": {\r\n        \"format\": \"\
          pt\",\r\n        \"quantized_by\": \"RuterNorway\"\r\n    }\r\n}\r\n```\r\
          \n\r\nIf you'd like, I could make a pull reuqest, but I figured you might\
          \ just do it youself so you don't have to spend time verifying everything."
        updatedAt: '2023-09-23T12:50:10.080Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - RuterNorway
    id: 650edf02469c325dc4a246a9
    type: comment
  author: marksverdhei
  content: "Since \U0001F917transformers now has native support for GPTQ-quantized\
    \ models, \r\nquantized models can now be loaded and used just by calling\r\n\
    `AutoModelForCausalLM.from_pretrained('your_model')`\r\nTheBloke GPTQ models already\
    \ support this but yours doesn't yet.\r\nIt would be nice to see this change,\
    \ as it then could be directly used in many scripts without much code alteration.\r\
    \n\r\nI have already done this on a private repo, so I'll let you know the steps\
    \ I took to make it work:\r\n\r\n1. Rename the safetensors model file to model.safetensors\r\
    \n2. The safetensors file lacks metadata, which the \U0001F917transformers backend\
    \ relies on. Therefore, add metadata.\r\n\r\nI used the safetensors util to add\
    \ the metadata https://github.com/by321/safetensors_util\r\nI just added the metadata\
    \ equivalent of TheBLoke's llama2 variant, which was the following config\r\n\
    ```\r\n{\r\n    \"__metadata__\": {\r\n        \"format\": \"pt\",\r\n       \
    \ \"quantized_by\": \"RuterNorway\"\r\n    }\r\n}\r\n```\r\n\r\nIf you'd like,\
    \ I could make a pull reuqest, but I figured you might just do it youself so you\
    \ don't have to spend time verifying everything."
  created_at: 2023-09-23 11:50:10+00:00
  edited: false
  hidden: false
  id: 650edf02469c325dc4a246a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
      fullname: Markus Heiervang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marksverdhei
      type: user
    createdAt: '2023-09-23T12:51:11.000Z'
    data:
      edited: false
      editors:
      - marksverdhei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9222791790962219
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
          fullname: Markus Heiervang
          isHf: false
          isPro: false
          name: marksverdhei
          type: user
        html: '<p>FYI: I have not tested to see if this still works with the exllama
          notebooks and your example code, just that it works with <code>AutoModelForCausalLM.from_pretrained
          </code></p>

          '
        raw: 'FYI: I have not tested to see if this still works with the exllama notebooks
          and your example code, just that it works with `AutoModelForCausalLM.from_pretrained

          `'
        updatedAt: '2023-09-23T12:51:11.019Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - buzzcraft
    id: 650edf3fd3caed0dfe1f0aac
    type: comment
  author: marksverdhei
  content: 'FYI: I have not tested to see if this still works with the exllama notebooks
    and your example code, just that it works with `AutoModelForCausalLM.from_pretrained

    `'
  created_at: 2023-09-23 11:51:11+00:00
  edited: false
  hidden: false
  id: 650edf3fd3caed0dfe1f0aac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64db79387f749b6e34577a61/wCo7eZHFwHn0_YfTh-HFF.png?w=200&h=200&f=face
      fullname: Ruter
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: RuterNorway
      type: user
    createdAt: '2023-09-25T19:52:05.000Z'
    data:
      edited: false
      editors:
      - RuterNorway
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7738752365112305
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64db79387f749b6e34577a61/wCo7eZHFwHn0_YfTh-HFF.png?w=200&h=200&f=face
          fullname: Ruter
          isHf: false
          isPro: false
          name: RuterNorway
          type: user
        html: "<p>Hi. Thank you <span data-props=\"{&quot;user&quot;:&quot;marksverdhei&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/marksverdhei\"\
          >@<span class=\"underline\">marksverdhei</span></a></span>\n\n\t</span></span>\
          \ . Can you please also make a pull request? :)</p>\n"
        raw: Hi. Thank you @marksverdhei . Can you please also make a pull request?
          :)
        updatedAt: '2023-09-25T19:52:05.688Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - marksverdhei
    id: 6511e4e5b6ffd31931c68ddb
    type: comment
  author: RuterNorway
  content: Hi. Thank you @marksverdhei . Can you please also make a pull request?
    :)
  created_at: 2023-09-25 18:52:05+00:00
  edited: false
  hidden: false
  id: 6511e4e5b6ffd31931c68ddb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
      fullname: Markus Heiervang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marksverdhei
      type: user
    createdAt: '2023-09-29T12:49:44.000Z'
    data:
      status: closed
    id: 6516c7e8682832d9af0f385c
    type: status-change
  author: marksverdhei
  created_at: 2023-09-29 11:49:44+00:00
  id: 6516c7e8682832d9af0f385c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: RuterNorway/Llama-2-13b-chat-norwegian-GPTQ
repo_type: model
status: closed
target_branch: null
title: Make this work directly with AutoModelForCausalLM.from_pretrained
