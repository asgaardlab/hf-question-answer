!!python/object:huggingface_hub.community.DiscussionWithDetails
author: marksverdhei
conflicting_files: null
created_at: 2023-08-28 11:47:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
      fullname: Markus Heiervang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marksverdhei
      type: user
    createdAt: '2023-08-28T12:47:07.000Z'
    data:
      edited: true
      editors:
      - marksverdhei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5106638669967651
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
          fullname: Markus Heiervang
          isHf: false
          isPro: false
          name: marksverdhei
          type: user
        html: "<p>The example code you've provided crashes. First of all, it contains\
          \ a syntax error which is easy to fix:<br>model_basename=model_basename\
          \ lacks a comma at the end.<br>However, when fixing it, I run into another\
          \ issue:</p>\n<pre><code class=\"language-console\">TypeError          \
          \                       Traceback (most recent call last)\nCell In[2], line\
          \ 7\n      5 use_triton = False\n      6 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n<span class=\"hljs-meta prompt_\">----&gt; </span><span\
          \ class=\"language-bash\">7 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,</span>\n\
          \      8         model_basename=model_basename,\n      9         use_safetensors=True,\n\
          \     10         trust_remote_code=True,\n     11         device=\"cuda:0\"\
          ,\n     12         use_triton=use_triton,\n     13         quantize_config=None)\n\
          \     14 \"\"\"\n     15 To download from a specific branch, use the revision\
          \ parameter, as in this example:\n     16 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \   (...)\n     22         quantize_config=None)\n     23 \"\"\"\n     24\
          \ prompt = \"Fortell meg om AI\"\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:108,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,\
          \ max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n\
          \    102 # TODO: do we need this filtering of kwargs? @PanQiWei is there\
          \ a reason we can't just pass all kwargs?\n    103 keywords = {\n    104\
          \     key: kwargs[key]\n    105     for key in list(signature(quant_func).parameters.keys())\
          \ + huggingface_kwargs\n    106     if key in kwargs\n    107 }\n<span class=\"\
          hljs-meta prompt_\">--&gt; </span><span class=\"language-bash\">108 <span\
          \ class=\"hljs-built_in\">return</span> quant_func(</span>\n    109    \
          \ model_name_or_path=model_name_or_path,\n    110     device_map=device_map,\n\
          \    111     max_memory=max_memory,\n    112     device=device,\n    113\
          \     low_cpu_mem_usage=low_cpu_mem_usage,\n    114     use_triton=use_triton,\n\
          \    115     inject_fused_attention=inject_fused_attention,\n    116   \
          \  inject_fused_mlp=inject_fused_mlp,\n    117     use_cuda_fp16=use_cuda_fp16,\n\
          \    118     quantize_config=quantize_config,\n    119     model_basename=model_basename,\n\
          \    120     use_safetensors=use_safetensors,\n    121     trust_remote_code=trust_remote_code,\n\
          \    122     warmup_triton=warmup_triton,\n    123     trainable=trainable,\n\
          \    124     disable_exllama=disable_exllama,\n    125     **keywords\n\
          \    126 )\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:757,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,\
          \ max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n\
          \    754     raise TypeError(f\"{config.model_type} isn't supported yet.\"\
          )\n    756 if quantize_config is None:\n<span class=\"hljs-meta prompt_\"\
          >--&gt; </span><span class=\"language-bash\">757     quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path,\
          \ **cached_file_kwargs, **kwargs)</span>\n    759 if model_basename is None:\n\
          \    760     if quantize_config.model_file_base_name:\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:93,\
          \ in BaseQuantizeConfig.from_pretrained(cls, save_dir, **kwargs)\n     76\
          \ else: # Remote\n     77     resolved_config_file = cached_file(\n    \
          \ 78         save_dir,\n     79         quantize_config_filename,\n   (...)\n\
          \     90         _commit_hash=commit_hash,\n     91     )\n<span class=\"\
          hljs-meta prompt_\">---&gt; </span><span class=\"language-bash\">93 with\
          \ open(resolved_config_file, <span class=\"hljs-string\">\"r\"</span>, encoding=<span\
          \ class=\"hljs-string\">\"utf-8\"</span>) as f:</span>\n     94     return\
          \ cls(**json.load(f))\n\nTypeError: expected str, bytes or os.PathLike object,\
          \ not NoneType\n</code></pre>\n<p> For reference, this is the code in the\
          \ model card i'm referring to.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, pipeline, logging\n<span class=\"hljs-keyword\"\
          >from</span> auto_gptq <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nmodel_name_or_path = <span class=\"hljs-string\">\"\
          RuterNorway/Llama-2-13b-chat-norwegian-GPTQ\"</span>\nmodel_basename = <span\
          \ class=\"hljs-string\">\"gptq_model-4bit-128g\"</span>\nuse_triton = <span\
          \ class=\"hljs-literal\">False</span>\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=<span class=\"hljs-literal\">True</span>)\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n        device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n\
          \        use_triton=use_triton,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n<span class=\"hljs-string\">\"\"\"</span>\n\
          <span class=\"hljs-string\">To download from a specific branch, use the\
          \ revision parameter, as in this example:</span>\n<span class=\"hljs-string\"\
          >model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,</span>\n\
          <span class=\"hljs-string\">        revision=\"gptq-4bit-32g-actorder_True\"\
          ,</span>\n<span class=\"hljs-string\">        model_basename=model_basename,</span>\n\
          <span class=\"hljs-string\">        use_safetensors=True,</span>\n<span\
          \ class=\"hljs-string\">        trust_remote_code=True,</span>\n<span class=\"\
          hljs-string\">        device=\"cuda:0\",</span>\n<span class=\"hljs-string\"\
          >        quantize_config=None)</span>\n<span class=\"hljs-string\">\"\"\"\
          </span>\nprompt = <span class=\"hljs-string\">\"Fortell meg om AI\"</span>\n\
          prompt_template=<span class=\"hljs-string\">f'''### Human: <span class=\"\
          hljs-subst\">{prompt}</span></span>\n<span class=\"hljs-string\">### Assistant:</span>\n\
          <span class=\"hljs-string\">'''</span>\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors=<span class=\"hljs-string\">'pt'</span>).input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=<span class=\"hljs-number\"\
          >0.7</span>, max_new_tokens=<span class=\"hljs-number\">512</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span class=\"\
          hljs-number\">0</span>]))\n<span class=\"hljs-comment\"># Inference can\
          \ also be done using transformers' pipeline</span>\n<span class=\"hljs-comment\"\
          ># Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"\
          </span>)\npipe = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\"\
          >0.7</span>,\n    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n"
        raw: "The example code you've provided crashes. First of all, it contains\
          \ a syntax error which is easy to fix:\nmodel_basename=model_basename lacks\
          \ a comma at the end. \nHowever, when fixing it, I run into another issue:\n\
          \n```console\nTypeError                                 Traceback (most\
          \ recent call last)\nCell In[2], line 7\n      5 use_triton = False\n  \
          \    6 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          ----> 7 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \      8         model_basename=model_basename,\n      9         use_safetensors=True,\n\
          \     10         trust_remote_code=True,\n     11         device=\"cuda:0\"\
          ,\n     12         use_triton=use_triton,\n     13         quantize_config=None)\n\
          \     14 \"\"\"\n     15 To download from a specific branch, use the revision\
          \ parameter, as in this example:\n     16 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \   (...)\n     22         quantize_config=None)\n     23 \"\"\"\n     24\
          \ prompt = \"Fortell meg om AI\"\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:108,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,\
          \ max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n\
          \    102 # TODO: do we need this filtering of kwargs? @PanQiWei is there\
          \ a reason we can't just pass all kwargs?\n    103 keywords = {\n    104\
          \     key: kwargs[key]\n    105     for key in list(signature(quant_func).parameters.keys())\
          \ + huggingface_kwargs\n    106     if key in kwargs\n    107 }\n--> 108\
          \ return quant_func(\n    109     model_name_or_path=model_name_or_path,\n\
          \    110     device_map=device_map,\n    111     max_memory=max_memory,\n\
          \    112     device=device,\n    113     low_cpu_mem_usage=low_cpu_mem_usage,\n\
          \    114     use_triton=use_triton,\n    115     inject_fused_attention=inject_fused_attention,\n\
          \    116     inject_fused_mlp=inject_fused_mlp,\n    117     use_cuda_fp16=use_cuda_fp16,\n\
          \    118     quantize_config=quantize_config,\n    119     model_basename=model_basename,\n\
          \    120     use_safetensors=use_safetensors,\n    121     trust_remote_code=trust_remote_code,\n\
          \    122     warmup_triton=warmup_triton,\n    123     trainable=trainable,\n\
          \    124     disable_exllama=disable_exllama,\n    125     **keywords\n\
          \    126 )\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:757,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,\
          \ max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n\
          \    754     raise TypeError(f\"{config.model_type} isn't supported yet.\"\
          )\n    756 if quantize_config is None:\n--> 757     quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path,\
          \ **cached_file_kwargs, **kwargs)\n    759 if model_basename is None:\n\
          \    760     if quantize_config.model_file_base_name:\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:93,\
          \ in BaseQuantizeConfig.from_pretrained(cls, save_dir, **kwargs)\n     76\
          \ else: # Remote\n     77     resolved_config_file = cached_file(\n    \
          \ 78         save_dir,\n     79         quantize_config_filename,\n   (...)\n\
          \     90         _commit_hash=commit_hash,\n     91     )\n---> 93 with\
          \ open(resolved_config_file, \"r\", encoding=\"utf-8\") as f:\n     94 \
          \    return cls(**json.load(f))\n\nTypeError: expected str, bytes or os.PathLike\
          \ object, not NoneType\n```\n For reference, this is the code in the model\
          \ card i'm referring to.\n```python\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          model_name_or_path = \"RuterNorway/Llama-2-13b-chat-norwegian-GPTQ\"\nmodel_basename\
          \ = \"gptq_model-4bit-128g\"\nuse_triton = False\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\"\"\"\nTo download from a specific branch,\
          \ use the revision parameter, as in this example:\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        revision=\"gptq-4bit-32g-actorder_True\",\n        model_basename=model_basename,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device=\"cuda:0\",\n        quantize_config=None)\n\"\"\"\nprompt = \"\
          Fortell meg om AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:\n\
          '''\nprint(\"\\n\\n*** Generate:\")\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          # Inference can also be done using transformers' pipeline\n# Prevent printing\
          \ spurious transformers error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          print(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n \
          \   model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n  \
          \  temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\nprint(pipe(prompt_template)[0]['generated_text'])\n\
          \n```\n\n"
        updatedAt: '2023-08-28T12:47:38.760Z'
      numEdits: 1
      reactions: []
    id: 64ec974b5ff6f833780d5fb6
    type: comment
  author: marksverdhei
  content: "The example code you've provided crashes. First of all, it contains a\
    \ syntax error which is easy to fix:\nmodel_basename=model_basename lacks a comma\
    \ at the end. \nHowever, when fixing it, I run into another issue:\n\n```console\n\
    TypeError                                 Traceback (most recent call last)\n\
    Cell In[2], line 7\n      5 use_triton = False\n      6 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n----> 7 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \      8         model_basename=model_basename,\n      9         use_safetensors=True,\n\
    \     10         trust_remote_code=True,\n     11         device=\"cuda:0\",\n\
    \     12         use_triton=use_triton,\n     13         quantize_config=None)\n\
    \     14 \"\"\"\n     15 To download from a specific branch, use the revision\
    \ parameter, as in this example:\n     16 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \   (...)\n     22         quantize_config=None)\n     23 \"\"\"\n     24 prompt\
    \ = \"Fortell meg om AI\"\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:108,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map, max_memory,\
    \ device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp,\
    \ use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code,\
    \ warmup_triton, trainable, disable_exllama, **kwargs)\n    102 # TODO: do we\
    \ need this filtering of kwargs? @PanQiWei is there a reason we can't just pass\
    \ all kwargs?\n    103 keywords = {\n    104     key: kwargs[key]\n    105   \
    \  for key in list(signature(quant_func).parameters.keys()) + huggingface_kwargs\n\
    \    106     if key in kwargs\n    107 }\n--> 108 return quant_func(\n    109\
    \     model_name_or_path=model_name_or_path,\n    110     device_map=device_map,\n\
    \    111     max_memory=max_memory,\n    112     device=device,\n    113     low_cpu_mem_usage=low_cpu_mem_usage,\n\
    \    114     use_triton=use_triton,\n    115     inject_fused_attention=inject_fused_attention,\n\
    \    116     inject_fused_mlp=inject_fused_mlp,\n    117     use_cuda_fp16=use_cuda_fp16,\n\
    \    118     quantize_config=quantize_config,\n    119     model_basename=model_basename,\n\
    \    120     use_safetensors=use_safetensors,\n    121     trust_remote_code=trust_remote_code,\n\
    \    122     warmup_triton=warmup_triton,\n    123     trainable=trainable,\n\
    \    124     disable_exllama=disable_exllama,\n    125     **keywords\n    126\
    \ )\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:757,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map, max_memory,\
    \ device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
    \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
    \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n  \
    \  754     raise TypeError(f\"{config.model_type} isn't supported yet.\")\n  \
    \  756 if quantize_config is None:\n--> 757     quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path,\
    \ **cached_file_kwargs, **kwargs)\n    759 if model_basename is None:\n    760\
    \     if quantize_config.model_file_base_name:\n\nFile ~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:93,\
    \ in BaseQuantizeConfig.from_pretrained(cls, save_dir, **kwargs)\n     76 else:\
    \ # Remote\n     77     resolved_config_file = cached_file(\n     78         save_dir,\n\
    \     79         quantize_config_filename,\n   (...)\n     90         _commit_hash=commit_hash,\n\
    \     91     )\n---> 93 with open(resolved_config_file, \"r\", encoding=\"utf-8\"\
    ) as f:\n     94     return cls(**json.load(f))\n\nTypeError: expected str, bytes\
    \ or os.PathLike object, not NoneType\n```\n For reference, this is the code in\
    \ the model card i'm referring to.\n```python\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
    model_name_or_path = \"RuterNorway/Llama-2-13b-chat-norwegian-GPTQ\"\nmodel_basename\
    \ = \"gptq_model-4bit-128g\"\nuse_triton = False\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename\n        use_safetensors=True,\n      \
    \  trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n\"\"\"\nTo download from a specific branch, use\
    \ the revision parameter, as in this example:\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        revision=\"gptq-4bit-32g-actorder_True\",\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        quantize_config=None)\n\"\"\"\nprompt = \"Fortell meg om AI\"\
    \nprompt_template=f'''### Human: {prompt}\n### Assistant:\n'''\nprint(\"\\n\\\
    n*** Generate:\")\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n# Inference can also be done using transformers'\
    \ pipeline\n# Prevent printing spurious transformers error when using pipeline\
    \ with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\nprint(\"*** Pipeline:\"\
    )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
    )\nprint(pipe(prompt_template)[0]['generated_text'])\n\n```\n\n"
  created_at: 2023-08-28 11:47:07+00:00
  edited: true
  hidden: false
  id: 64ec974b5ff6f833780d5fb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64db79387f749b6e34577a61/wCo7eZHFwHn0_YfTh-HFF.png?w=200&h=200&f=face
      fullname: Ruter
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: RuterNorway
      type: user
    createdAt: '2023-08-29T10:09:34.000Z'
    data:
      edited: false
      editors:
      - RuterNorway
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9608439803123474
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64db79387f749b6e34577a61/wCo7eZHFwHn0_YfTh-HFF.png?w=200&h=200&f=face
          fullname: Ruter
          isHf: false
          isPro: false
          name: RuterNorway
          type: user
        html: '<p>Hello,<br>Thank you for bringing the issue to our attention. We''ve
          updated the quantize_config.json file as well as the example code, so the
          problem should now be resolved.<br>If you continue to experience any issues,
          please don''t hesitate to let us know.</p>

          <p>Additionally, you can have a look at the implementation of ExLLaMA by
          visiting the Colab link in the model card.</p>

          '
        raw: 'Hello,

          Thank you for bringing the issue to our attention. We''ve updated the quantize_config.json
          file as well as the example code, so the problem should now be resolved.

          If you continue to experience any issues, please don''t hesitate to let
          us know.


          Additionally, you can have a look at the implementation of ExLLaMA by visiting
          the Colab link in the model card.

          '
        updatedAt: '2023-08-29T10:09:34.392Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - marksverdhei
    id: 64edc3de60f6345da70e2e27
    type: comment
  author: RuterNorway
  content: 'Hello,

    Thank you for bringing the issue to our attention. We''ve updated the quantize_config.json
    file as well as the example code, so the problem should now be resolved.

    If you continue to experience any issues, please don''t hesitate to let us know.


    Additionally, you can have a look at the implementation of ExLLaMA by visiting
    the Colab link in the model card.

    '
  created_at: 2023-08-29 09:09:34+00:00
  edited: false
  hidden: false
  id: 64edc3de60f6345da70e2e27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648679572929-6204304ddec9ca07e4411eca.jpeg?w=200&h=200&f=face
      fullname: Markus Heiervang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marksverdhei
      type: user
    createdAt: '2023-09-26T07:46:38.000Z'
    data:
      status: closed
    id: 65128c5ec915724a654bf0c9
    type: status-change
  author: marksverdhei
  created_at: 2023-09-26 06:46:38+00:00
  id: 65128c5ec915724a654bf0c9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: RuterNorway/Llama-2-13b-chat-norwegian-GPTQ
repo_type: model
status: closed
target_branch: null
title: Example code not working
