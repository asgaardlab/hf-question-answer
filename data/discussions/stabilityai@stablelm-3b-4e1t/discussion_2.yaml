!!python/object:huggingface_hub.community.DiscussionWithDetails
author: scrawnyether
conflicting_files: null
created_at: 2023-10-01 20:20:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9d786ca6c2253b1467ed2c936c92271.svg
      fullname: dhia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: scrawnyether
      type: user
    createdAt: '2023-10-01T21:20:20.000Z'
    data:
      edited: false
      editors:
      - scrawnyether
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8250681757926941
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9d786ca6c2253b1467ed2c936c92271.svg
          fullname: dhia
          isHf: false
          isPro: false
          name: scrawnyether
          type: user
        html: '<p>can you please give me the url for gguf and ggml variants</p>

          '
        raw: can you please give me the url for gguf and ggml variants
        updatedAt: '2023-10-01T21:20:20.807Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - Nightcall
        - BenUser
        - SkullCC
        - Pumba2
        - sharad
    id: 6519e294ea991a3229054604
    type: comment
  author: scrawnyether
  content: can you please give me the url for gguf and ggml variants
  created_at: 2023-10-01 20:20:20+00:00
  edited: false
  hidden: false
  id: 6519e294ea991a3229054604
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-10-03T17:28:46.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5387198328971863
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>I tried to use llama.cpp to convert the stableML-3B into ggml/gguf,
          but there is an error when executing the convert.py script:</p>

          <p>% python3 convert.py models/stablelm-3b-4e1t<br>Traceback (most recent
          call last):<br>  File "/Users/xxx/code/llama.cpp/convert.py", line 1208,
          in <br>    main()<br>  File "/Users/xxx/code/llama.cpp/convert.py", line
          1149, in main<br>    model_plus = load_some_model(args.model)<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/xxx/code/llama.cpp/convert.py", line 1060, in load_some_model<br>    raise
          Exception(f"Can''t find model in directory {path}")<br>Exception: Can''t
          find model in directory models/stablelm-3b-4e1t</p>

          '
        raw: "I tried to use llama.cpp to convert the stableML-3B into ggml/gguf,\
          \ but there is an error when executing the convert.py script:\n\n% python3\
          \ convert.py models/stablelm-3b-4e1t\nTraceback (most recent call last):\n\
          \  File \"/Users/xxx/code/llama.cpp/convert.py\", line 1208, in <module>\n\
          \    main()\n  File \"/Users/xxx/code/llama.cpp/convert.py\", line 1149,\
          \ in main\n    model_plus = load_some_model(args.model)\n              \
          \   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/xxx/code/llama.cpp/convert.py\"\
          , line 1060, in load_some_model\n    raise Exception(f\"Can't find model\
          \ in directory {path}\")\nException: Can't find model in directory models/stablelm-3b-4e1t\n"
        updatedAt: '2023-10-03T17:28:46.842Z'
      numEdits: 0
      reactions: []
    id: 651c4f4e34a090d301f30a9c
    type: comment
  author: zbruceli
  content: "I tried to use llama.cpp to convert the stableML-3B into ggml/gguf, but\
    \ there is an error when executing the convert.py script:\n\n% python3 convert.py\
    \ models/stablelm-3b-4e1t\nTraceback (most recent call last):\n  File \"/Users/xxx/code/llama.cpp/convert.py\"\
    , line 1208, in <module>\n    main()\n  File \"/Users/xxx/code/llama.cpp/convert.py\"\
    , line 1149, in main\n    model_plus = load_some_model(args.model)\n         \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/xxx/code/llama.cpp/convert.py\"\
    , line 1060, in load_some_model\n    raise Exception(f\"Can't find model in directory\
    \ {path}\")\nException: Can't find model in directory models/stablelm-3b-4e1t\n"
  created_at: 2023-10-03 16:28:46+00:00
  edited: false
  hidden: false
  id: 651c4f4e34a090d301f30a9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-10-03T22:37:47.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5093157887458801
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>After chnge to gptneox based conversion script, now I got a new
          error message.</p>

          <pre><code>% python3 convert-gptneox-hf-to-gguf.py ./models/stablelm-3b-4e1t
          1

          gguf: loading model stablelm-3b-4e1t

          Model architecture not supported: StableLMEpochForCausalLM

          </code></pre>

          '
        raw: 'After chnge to gptneox based conversion script, now I got a new error
          message.


          ```

          % python3 convert-gptneox-hf-to-gguf.py ./models/stablelm-3b-4e1t 1

          gguf: loading model stablelm-3b-4e1t

          Model architecture not supported: StableLMEpochForCausalLM

          ```

          '
        updatedAt: '2023-10-03T22:37:47.923Z'
      numEdits: 0
      reactions: []
    id: 651c97bb7aa3f27c84b8cd15
    type: comment
  author: zbruceli
  content: 'After chnge to gptneox based conversion script, now I got a new error
    message.


    ```

    % python3 convert-gptneox-hf-to-gguf.py ./models/stablelm-3b-4e1t 1

    gguf: loading model stablelm-3b-4e1t

    Model architecture not supported: StableLMEpochForCausalLM

    ```

    '
  created_at: 2023-10-03 21:37:47+00:00
  edited: false
  hidden: false
  id: 651c97bb7aa3f27c84b8cd15
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: stabilityai/stablelm-3b-4e1t
repo_type: model
status: open
target_branch: null
title: gguf/ggml variants
