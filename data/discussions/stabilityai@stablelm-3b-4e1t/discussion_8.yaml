!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joel-wj
conflicting_files: null
created_at: 2023-12-22 08:46:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/503d61b15f28850ac44694b3a646049b.svg
      fullname: Jang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joel-wj
      type: user
    createdAt: '2023-12-22T08:46:59.000Z'
    data:
      edited: true
      editors:
      - joel-wj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9128744602203369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/503d61b15f28850ac44694b3a646049b.svg
          fullname: Jang
          isHf: false
          isPro: false
          name: joel-wj
          type: user
        html: '<p>Thank you for sharing the good model.<br>I want to further train
          this model to fit my data. I am trying to add data at the pretraining level,
          but it seems to take a long time because there is no flash attention in
          the modeling code. Can you please add a class Attention with flash attention
          in modeling_stablelm_epoch.py and add some code for supporting flash attention
          in stablelm model?</p>

          '
        raw: 'Thank you for sharing the good model.

          I want to further train this model to fit my data. I am trying to add data
          at the pretraining level, but it seems to take a long time because there
          is no flash attention in the modeling code. Can you please add a class Attention
          with flash attention in modeling_stablelm_epoch.py and add some code for
          supporting flash attention in stablelm model?'
        updatedAt: '2023-12-22T09:08:45.276Z'
      numEdits: 1
      reactions: []
    id: 65854d03ebe2be20807edef2
    type: comment
  author: joel-wj
  content: 'Thank you for sharing the good model.

    I want to further train this model to fit my data. I am trying to add data at
    the pretraining level, but it seems to take a long time because there is no flash
    attention in the modeling code. Can you please add a class Attention with flash
    attention in modeling_stablelm_epoch.py and add some code for supporting flash
    attention in stablelm model?'
  created_at: 2023-12-22 08:46:59+00:00
  edited: true
  hidden: false
  id: 65854d03ebe2be20807edef2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-05T17:48:25.000Z'
    data:
      edited: false
      editors:
      - jon-tow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9437517523765564
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
          fullname: Jonathan Tow
          isHf: false
          isPro: false
          name: jon-tow
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;joel-wj&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/joel-wj\">@<span class=\"\
          underline\">joel-wj</span></a></span>\n\n\t</span></span> flash-attn v2\
          \ support has just been added. Thanks for the request, and sorry for the\
          \ hold-up!</p>\n"
        raw: '@joel-wj flash-attn v2 support has just been added. Thanks for the request,
          and sorry for the hold-up!'
        updatedAt: '2024-01-05T17:48:25.504Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659840e916e2dd143a714533
    id: 659840e916e2dd143a71452f
    type: comment
  author: jon-tow
  content: '@joel-wj flash-attn v2 support has just been added. Thanks for the request,
    and sorry for the hold-up!'
  created_at: 2024-01-05 17:48:25+00:00
  edited: false
  hidden: false
  id: 659840e916e2dd143a71452f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-05T17:48:25.000Z'
    data:
      status: closed
    id: 659840e916e2dd143a714533
    type: status-change
  author: jon-tow
  created_at: 2024-01-05 17:48:25+00:00
  id: 659840e916e2dd143a714533
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: stabilityai/stablelm-3b-4e1t
repo_type: model
status: closed
target_branch: null
title: Adding Flash attention support for StableLMEpochForCausalLM
