!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NickyNicky
conflicting_files: null
created_at: 2023-09-30 01:22:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-09-30T02:22:52.000Z'
    data:
      edited: true
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5217543244361877
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<p>dataset_new</p>\n<pre><code>Dataset({\n    features: ['text'],\n\
          \    num_rows: 15011\n})\n</code></pre>\n<p>lm_dataset</p>\n<pre><code>\n\
          Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n  \
          \  num_rows: 1031\n})\n</code></pre>\n<pre><code class=\"language-Python\"\
          ><span class=\"hljs-keyword\">from</span> trl <span class=\"hljs-keyword\"\
          >import</span> SFTTrainer <span class=\"hljs-comment\"># https://huggingface.co/docs/trl/sft_trainer</span>\n\
          <span class=\"hljs-keyword\">import</span> transformers\n\n\nmax_seq_length\
          \ = max_length <span class=\"hljs-comment\">#get_max_length() # max sequence\
          \ length for model and packing of the dataset</span>\ntrainer = SFTTrainer(\n\
          \    model=model,\n    train_dataset=dataset_new, \n    dataset_text_field=<span\
          \ class=\"hljs-string\">\"text\"</span>,\n    max_seq_length=max_seq_length,\n\
          \    tokenizer=tokenizer,\n    <span class=\"hljs-comment\"># packing=True,</span>\n\
          \    <span class=\"hljs-comment\"># formatting_func=format_instruction,</span>\n\
          \    args=args,\n)\n<span class=\"hljs-string\">'''ERROR 1</span>\n<span\
          \ class=\"hljs-string\">ValueError: The model did not return a loss from\
          \ the inputs, only the following keys: logits. For reference, the inputs\
          \ it received are input_ids,labels,attention_mask.</span>\n<span class=\"\
          hljs-string\">'''</span>\ntrainer.train() <span class=\"hljs-comment\">#\
          \ there will not be a progress bar since tqdm is disabled</span>\n\n\n<span\
          \ class=\"hljs-comment\"># trainer = transformers.Trainer(</span>\n<span\
          \ class=\"hljs-comment\">#     model=model,</span>\n<span class=\"hljs-comment\"\
          >#     train_dataset=lm_dataset,</span>\n<span class=\"hljs-comment\">#\
          \     # eval_dataset=val_dataset,</span>\n<span class=\"hljs-comment\">#\
          \     tokenizer=tokenizer,</span>\n<span class=\"hljs-comment\">#     args=args,</span>\n\
          <span class=\"hljs-comment\">#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,False),</span>\n\
          <span class=\"hljs-comment\"># )</span>\n<span class=\"hljs-comment\">#\
          \ # ''' ERROR 2</span>\n<span class=\"hljs-comment\"># # ValueError: The\
          \ model did not return a loss from the inputs, only the following keys:\
          \ logits. For reference, the inputs it received are input_ids,attention_mask,labels.</span>\n\
          <span class=\"hljs-comment\"># # '''</span>\n<span class=\"hljs-comment\"\
          ># trainer.train()</span>\n</code></pre>\n"
        raw: "dataset_new\n```\nDataset({\n    features: ['text'],\n    num_rows:\
          \ 15011\n})\n```\n\nlm_dataset\n```\n\nDataset({\n    features: ['input_ids',\
          \ 'attention_mask', 'labels'],\n    num_rows: 1031\n})\n```\n\n```Python\n\
          from trl import SFTTrainer # https://huggingface.co/docs/trl/sft_trainer\n\
          import transformers\n\n\nmax_seq_length = max_length #get_max_length() #\
          \ max sequence length for model and packing of the dataset\ntrainer = SFTTrainer(\n\
          \    model=model,\n    train_dataset=dataset_new, \n    dataset_text_field=\"\
          text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n\
          \    # packing=True,\n    # formatting_func=format_instruction,\n    args=args,\n\
          )\n'''ERROR 1\nValueError: The model did not return a loss from the inputs,\
          \ only the following keys: logits. For reference, the inputs it received\
          \ are input_ids,labels,attention_mask.\n'''\ntrainer.train() # there will\
          \ not be a progress bar since tqdm is disabled\n\n\n# trainer = transformers.Trainer(\n\
          #     model=model,\n#     train_dataset=lm_dataset,\n#     # eval_dataset=val_dataset,\n\
          #     tokenizer=tokenizer,\n#     args=args,\n#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,False),\n\
          # )\n# # ''' ERROR 2\n# # ValueError: The model did not return a loss from\
          \ the inputs, only the following keys: logits. For reference, the inputs\
          \ it received are input_ids,attention_mask,labels.\n# # '''\n# trainer.train()\n\
          ```\n"
        updatedAt: '2023-09-30T02:23:51.514Z'
      numEdits: 2
      reactions: []
    id: 6517867c8a6d58d1d7e15bb8
    type: comment
  author: NickyNicky
  content: "dataset_new\n```\nDataset({\n    features: ['text'],\n    num_rows: 15011\n\
    })\n```\n\nlm_dataset\n```\n\nDataset({\n    features: ['input_ids', 'attention_mask',\
    \ 'labels'],\n    num_rows: 1031\n})\n```\n\n```Python\nfrom trl import SFTTrainer\
    \ # https://huggingface.co/docs/trl/sft_trainer\nimport transformers\n\n\nmax_seq_length\
    \ = max_length #get_max_length() # max sequence length for model and packing of\
    \ the dataset\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset_new,\
    \ \n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n   \
    \ tokenizer=tokenizer,\n    # packing=True,\n    # formatting_func=format_instruction,\n\
    \    args=args,\n)\n'''ERROR 1\nValueError: The model did not return a loss from\
    \ the inputs, only the following keys: logits. For reference, the inputs it received\
    \ are input_ids,labels,attention_mask.\n'''\ntrainer.train() # there will not\
    \ be a progress bar since tqdm is disabled\n\n\n# trainer = transformers.Trainer(\n\
    #     model=model,\n#     train_dataset=lm_dataset,\n#     # eval_dataset=val_dataset,\n\
    #     tokenizer=tokenizer,\n#     args=args,\n#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,False),\n\
    # )\n# # ''' ERROR 2\n# # ValueError: The model did not return a loss from the\
    \ inputs, only the following keys: logits. For reference, the inputs it received\
    \ are input_ids,attention_mask,labels.\n# # '''\n# trainer.train()\n```\n"
  created_at: 2023-09-30 01:22:52+00:00
  edited: true
  hidden: false
  id: 6517867c8a6d58d1d7e15bb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b4e1c3933332dec203887adfa231b9.svg
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stable-jon-tow
      type: user
    createdAt: '2023-09-30T02:31:11.000Z'
    data:
      edited: true
      editors:
      - stable-jon-tow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8036842346191406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b4e1c3933332dec203887adfa231b9.svg
          fullname: Jonathan Tow
          isHf: false
          isPro: false
          name: stable-jon-tow
          type: user
        html: '<p>Thanks for catching this! Looking into it now.</p>

          '
        raw: Thanks for catching this! Looking into it now.
        updatedAt: '2023-09-30T02:35:53.256Z'
      numEdits: 1
      reactions: []
    id: 6517886f34c26962534f9205
    type: comment
  author: stable-jon-tow
  content: Thanks for catching this! Looking into it now.
  created_at: 2023-09-30 01:31:11+00:00
  edited: true
  hidden: false
  id: 6517886f34c26962534f9205
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b4e1c3933332dec203887adfa231b9.svg
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stable-jon-tow
      type: user
    createdAt: '2023-09-30T02:52:39.000Z'
    data:
      edited: false
      editors:
      - stable-jon-tow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.878926694393158
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b4e1c3933332dec203887adfa231b9.svg
          fullname: Jonathan Tow
          isHf: false
          isPro: false
          name: stable-jon-tow
          type: user
        html: '<p>Should be fixed now! Let us know if you run into any more issues.
          Thanks again :) </p>

          '
        raw: 'Should be fixed now! Let us know if you run into any more issues. Thanks
          again :) '
        updatedAt: '2023-09-30T02:52:39.883Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65178d772b4fffcb41c94acf
    id: 65178d772b4fffcb41c94acb
    type: comment
  author: stable-jon-tow
  content: 'Should be fixed now! Let us know if you run into any more issues. Thanks
    again :) '
  created_at: 2023-09-30 01:52:39+00:00
  edited: false
  hidden: false
  id: 65178d772b4fffcb41c94acb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/98b4e1c3933332dec203887adfa231b9.svg
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stable-jon-tow
      type: user
    createdAt: '2023-09-30T02:52:39.000Z'
    data:
      status: closed
    id: 65178d772b4fffcb41c94acf
    type: status-change
  author: stable-jon-tow
  created_at: 2023-09-30 01:52:39+00:00
  id: 65178d772b4fffcb41c94acf
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-01T02:32:13.000Z'
    data:
      edited: true
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43264320492744446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<pre><code class=\"language-Python\">!pip install transformers==<span\
          \ class=\"hljs-number\">4.33</span><span class=\"hljs-number\">.2</span>\
          \ -qqq\n!pip install bitsandbytes==<span class=\"hljs-number\">0.38</span><span\
          \ class=\"hljs-number\">.0</span>  -qqq\n!pip install <span class=\"hljs-string\"\
          >\"datasets==2.13.0\"</span> peft accelerate trl <span class=\"hljs-string\"\
          >\"safetensors&gt;=0.3.1\"</span> --upgrade -qqq\n!pip install ninja packaging\
          \ --upgrade -qqq\n!pip install sentencepiece -qqq\n!pip install -U xformers\
          \ deepspeed -qqq\n\n!python -c <span class=\"hljs-string\">\"import torch;\
          \ assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported\
          \ for Flash Attention'\"</span>\n!export CUDA_HOME=/usr/local/cuda-<span\
          \ class=\"hljs-number\">11.8</span>\n<span class=\"hljs-comment\"># !MAX_JOBS=4\
          \ pip install flash-attn --no-build-isolation</span>\n!MAX_JOBS=<span class=\"\
          hljs-number\">4</span> pip install flash-attn --no-build-isolation  -qqq\n\
          !pip install git+<span class=\"hljs-string\">\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          </span> -qqq\n</code></pre>\n<pre><code class=\"language-Python\"><span\
          \ class=\"hljs-keyword\">from</span> trl <span class=\"hljs-keyword\">import</span>\
          \ SFTTrainer <span class=\"hljs-comment\"># https://huggingface.co/docs/trl/sft_trainer</span>\n\
          <span class=\"hljs-keyword\">import</span> transformers\nliberaMemoria()\n\
          max_seq_length = max_length <span class=\"hljs-comment\">#get_max_length()\
          \ # max sequence length for model and packing of the dataset</span>\ntrainer\
          \ = SFTTrainer(\n    model=model,\n    train_dataset=dataset_new, <span\
          \ class=\"hljs-comment\">#lm_dataset,</span>\n    dataset_text_field=<span\
          \ class=\"hljs-string\">\"text\"</span>,\n    max_seq_length=max_seq_length,\n\
          \    tokenizer=tokenizer,\n    <span class=\"hljs-comment\"># packing=True,</span>\n\
          \    <span class=\"hljs-comment\"># formatting_func=format_instruction,</span>\n\
          \    args=args,\n)\n\ntrainer.train() <span class=\"hljs-comment\"># there\
          \ will not be a progress bar since tqdm is disabled</span>\n</code></pre>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/llz7FzpXcUK1UrQhSo4XN.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/llz7FzpXcUK1UrQhSo4XN.png\"\
          ></a></p>\n<pre><code>You are using 8-bit optimizers with a version of `bitsandbytes`\
          \ &lt; 0.41.1. It is recommended to update your version as a major bug has\
          \ been fixed in 8-bit optimizers.\n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          &lt;ipython-input-13-259460b0944c&gt; in &lt;cell line: 21&gt;()\n     19\
          \ ValueError: The model did not return a loss from the inputs, only the\
          \ following keys: logits. For reference, the inputs it received are input_ids,labels,attention_mask.\n\
          \     20 '''\n---&gt; 21 trainer.train() # there will not be a progress\
          \ bar since tqdm is disabled\n     22 \n     23 \n\n3 frames\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in create_optimizer(self)\n    983                 )\n    984        \
          \     else:\n--&gt; 985                 self.optimizer = optimizer_cls(optimizer_grouped_parameters,\
          \ **optimizer_kwargs)\n    986                 if optimizer_cls.__name__\
          \ == \"Adam8bit\":\n    987                     import bitsandbytes\n\n\
          TypeError: AdamW.__init__() got an unexpected keyword argument 'is_paged'\n\
          </code></pre>\n"
        raw: "\n```Python\n!pip install transformers==4.33.2 -qqq\n!pip install bitsandbytes==0.38.0\
          \  -qqq\n!pip install \"datasets==2.13.0\" peft accelerate trl \"safetensors>=0.3.1\"\
          \ --upgrade -qqq\n!pip install ninja packaging --upgrade -qqq\n!pip install\
          \ sentencepiece -qqq\n!pip install -U xformers deepspeed -qqq\n\n!python\
          \ -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8,\
          \ 'Hardware not supported for Flash Attention'\"\n!export CUDA_HOME=/usr/local/cuda-11.8\n\
          # !MAX_JOBS=4 pip install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip\
          \ install flash-attn --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          \ -qqq\n```\n\n```Python\nfrom trl import SFTTrainer # https://huggingface.co/docs/trl/sft_trainer\n\
          import transformers\nliberaMemoria()\nmax_seq_length = max_length #get_max_length()\
          \ # max sequence length for model and packing of the dataset\ntrainer =\
          \ SFTTrainer(\n    model=model,\n    train_dataset=dataset_new, #lm_dataset,\n\
          \    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n\
          \    tokenizer=tokenizer,\n    # packing=True,\n    # formatting_func=format_instruction,\n\
          \    args=args,\n)\n\ntrainer.train() # there will not be a progress bar\
          \ since tqdm is disabled\n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/llz7FzpXcUK1UrQhSo4XN.png)\n\
          \n```\nYou are using 8-bit optimizers with a version of `bitsandbytes` <\
          \ 0.41.1. It is recommended to update your version as a major bug has been\
          \ fixed in 8-bit optimizers.\n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          <ipython-input-13-259460b0944c> in <cell line: 21>()\n     19 ValueError:\
          \ The model did not return a loss from the inputs, only the following keys:\
          \ logits. For reference, the inputs it received are input_ids,labels,attention_mask.\n\
          \     20 '''\n---> 21 trainer.train() # there will not be a progress bar\
          \ since tqdm is disabled\n     22 \n     23 \n\n3 frames\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in create_optimizer(self)\n    983                 )\n    984        \
          \     else:\n--> 985                 self.optimizer = optimizer_cls(optimizer_grouped_parameters,\
          \ **optimizer_kwargs)\n    986                 if optimizer_cls.__name__\
          \ == \"Adam8bit\":\n    987                     import bitsandbytes\n\n\
          TypeError: AdamW.__init__() got an unexpected keyword argument 'is_paged'\n\
          ```"
        updatedAt: '2023-10-01T02:36:59.107Z'
      numEdits: 1
      reactions: []
      relatedEventId: 6518da2d69c938b3d614c61e
    id: 6518da2d69c938b3d614c61b
    type: comment
  author: NickyNicky
  content: "\n```Python\n!pip install transformers==4.33.2 -qqq\n!pip install bitsandbytes==0.38.0\
    \  -qqq\n!pip install \"datasets==2.13.0\" peft accelerate trl \"safetensors>=0.3.1\"\
    \ --upgrade -qqq\n!pip install ninja packaging --upgrade -qqq\n!pip install sentencepiece\
    \ -qqq\n!pip install -U xformers deepspeed -qqq\n\n!python -c \"import torch;\
    \ assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for\
    \ Flash Attention'\"\n!export CUDA_HOME=/usr/local/cuda-11.8\n# !MAX_JOBS=4 pip\
    \ install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip install flash-attn\
    \ --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
    \ -qqq\n```\n\n```Python\nfrom trl import SFTTrainer # https://huggingface.co/docs/trl/sft_trainer\n\
    import transformers\nliberaMemoria()\nmax_seq_length = max_length #get_max_length()\
    \ # max sequence length for model and packing of the dataset\ntrainer = SFTTrainer(\n\
    \    model=model,\n    train_dataset=dataset_new, #lm_dataset,\n    dataset_text_field=\"\
    text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    # packing=True,\n\
    \    # formatting_func=format_instruction,\n    args=args,\n)\n\ntrainer.train()\
    \ # there will not be a progress bar since tqdm is disabled\n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/llz7FzpXcUK1UrQhSo4XN.png)\n\
    \n```\nYou are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1.\
    \ It is recommended to update your version as a major bug has been fixed in 8-bit\
    \ optimizers.\n---------------------------------------------------------------------------\n\
    TypeError                                 Traceback (most recent call last)\n\
    <ipython-input-13-259460b0944c> in <cell line: 21>()\n     19 ValueError: The\
    \ model did not return a loss from the inputs, only the following keys: logits.\
    \ For reference, the inputs it received are input_ids,labels,attention_mask.\n\
    \     20 '''\n---> 21 trainer.train() # there will not be a progress bar since\
    \ tqdm is disabled\n     22 \n     23 \n\n3 frames\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
    \ in create_optimizer(self)\n    983                 )\n    984             else:\n\
    --> 985                 self.optimizer = optimizer_cls(optimizer_grouped_parameters,\
    \ **optimizer_kwargs)\n    986                 if optimizer_cls.__name__ == \"\
    Adam8bit\":\n    987                     import bitsandbytes\n\nTypeError: AdamW.__init__()\
    \ got an unexpected keyword argument 'is_paged'\n```"
  created_at: 2023-10-01 01:32:13+00:00
  edited: true
  hidden: false
  id: 6518da2d69c938b3d614c61b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-01T02:32:13.000Z'
    data:
      status: open
    id: 6518da2d69c938b3d614c61e
    type: status-change
  author: NickyNicky
  created_at: 2023-10-01 01:32:13+00:00
  id: 6518da2d69c938b3d614c61e
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-01T02:50:10.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5338161587715149
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<p>resolved:</p>

          <p>!pip install transformers==4.33.2 -qqq<br>!pip install bitsandbytes==0.40.0  -qqq
          </p>

          '
        raw: 'resolved:


          !pip install transformers==4.33.2 -qqq

          !pip install bitsandbytes==0.40.0  -qqq '
        updatedAt: '2023-10-01T02:50:10.470Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6518de62186bc3b699723cfd
    id: 6518de62186bc3b699723cfa
    type: comment
  author: NickyNicky
  content: 'resolved:


    !pip install transformers==4.33.2 -qqq

    !pip install bitsandbytes==0.40.0  -qqq '
  created_at: 2023-10-01 01:50:10+00:00
  edited: false
  hidden: false
  id: 6518de62186bc3b699723cfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-01T02:50:10.000Z'
    data:
      status: closed
    id: 6518de62186bc3b699723cfd
    type: status-change
  author: NickyNicky
  created_at: 2023-10-01 01:50:10+00:00
  id: 6518de62186bc3b699723cfd
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-01T02:56:58.000Z'
    data:
      edited: true
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4107990562915802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<p>Other Error: Fine tune QLora Peft: </p>\n<pre><code class=\"language-Python\"\
          >!pip install transformers==<span class=\"hljs-number\">4.33</span><span\
          \ class=\"hljs-number\">.2</span> -qqq\n!pip install bitsandbytes==<span\
          \ class=\"hljs-number\">0.40</span><span class=\"hljs-number\">.0</span>\
          \  -qqq \n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/-PEFobfWhstExOozeQGQ1.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/-PEFobfWhstExOozeQGQ1.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/kwXk3sV9nchNEfMyd6GC1.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/kwXk3sV9nchNEfMyd6GC1.png\"\
          ></a></p>\n<p>What is the version of 'Peft' to make it work?</p>\n"
        raw: "Other Error: Fine tune QLora Peft: \n\n```Python\n!pip install transformers==4.33.2\
          \ -qqq\n!pip install bitsandbytes==0.40.0  -qqq \n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/-PEFobfWhstExOozeQGQ1.png)\n\
          \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/kwXk3sV9nchNEfMyd6GC1.png)\n\
          \nWhat is the version of 'Peft' to make it work?"
        updatedAt: '2023-10-01T03:13:09.649Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - NickyNicky
      relatedEventId: 6518dffa29af405887ca7e54
    id: 6518dffa29af405887ca7e51
    type: comment
  author: NickyNicky
  content: "Other Error: Fine tune QLora Peft: \n\n```Python\n!pip install transformers==4.33.2\
    \ -qqq\n!pip install bitsandbytes==0.40.0  -qqq \n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/-PEFobfWhstExOozeQGQ1.png)\n\
    \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/kwXk3sV9nchNEfMyd6GC1.png)\n\
    \nWhat is the version of 'Peft' to make it work?"
  created_at: 2023-10-01 01:56:58+00:00
  edited: true
  hidden: false
  id: 6518dffa29af405887ca7e51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-01T02:56:58.000Z'
    data:
      status: open
    id: 6518dffa29af405887ca7e54
    type: status-change
  author: NickyNicky
  created_at: 2023-10-01 01:56:58+00:00
  id: 6518dffa29af405887ca7e54
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8003c7044affbf83bfa195470ba82efe.svg
      fullname: Stanislav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Psychoprolapse
      type: user
    createdAt: '2023-10-06T18:24:56.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/8003c7044affbf83bfa195470ba82efe.svg
          fullname: Stanislav
          isHf: false
          isPro: false
          name: Psychoprolapse
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-10-06T18:28:38.025Z'
      numEdits: 0
      reactions: []
    id: 652050f8d65824bda766d69c
    type: comment
  author: Psychoprolapse
  content: This comment has been hidden
  created_at: 2023-10-06 17:24:56+00:00
  edited: true
  hidden: true
  id: 652050f8d65824bda766d69c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-08T01:56:56.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: fr
        probability: 0.9512801170349121
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<p>?</p>

          '
        raw: '?'
        updatedAt: '2023-10-08T01:56:56.682Z'
      numEdits: 0
      reactions: []
    id: 65220c68578e7da0d74eac2e
    type: comment
  author: NickyNicky
  content: '?'
  created_at: 2023-10-08 00:56:56+00:00
  edited: false
  hidden: false
  id: 65220c68578e7da0d74eac2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
      fullname: Joseph Pollack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tonic
      type: user
    createdAt: '2023-11-01T12:33:04.000Z'
    data:
      edited: false
      editors:
      - Tonic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9749985933303833
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
          fullname: Joseph Pollack
          isHf: false
          isPro: false
          name: Tonic
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;NickyNicky&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/NickyNicky\"\
          >@<span class=\"underline\">NickyNicky</span></a></span>\n\n\t</span></span>\
          \ very cool + interesting + i have carved out time to figure this out, i'm\
          \ really glad you're working this out, i would love to help, so it seems\
          \ you're working off of some code already maybe from a public source, i\
          \ would love share my results and try to keep getting information + data\
          \ required for the fine tuning experiments, if you share with me what you're\
          \ working with, i'll work off of that instead of my usual notebooks i ripped\
          \ from medium or whatever. ^^ </p>\n"
        raw: "Hey @NickyNicky very cool + interesting + i have carved out time to\
          \ figure this out, i'm really glad you're working this out, i would love\
          \ to help, so it seems you're working off of some code already maybe from\
          \ a public source, i would love share my results and try to keep getting\
          \ information + data required for the fine tuning experiments, if you share\
          \ with me what you're working with, i'll work off of that instead of my\
          \ usual notebooks i ripped from medium or whatever. ^^ \n"
        updatedAt: '2023-11-01T12:33:04.860Z'
      numEdits: 0
      reactions: []
    id: 65424580c7bf7cd7125f25dd
    type: comment
  author: Tonic
  content: "Hey @NickyNicky very cool + interesting + i have carved out time to figure\
    \ this out, i'm really glad you're working this out, i would love to help, so\
    \ it seems you're working off of some code already maybe from a public source,\
    \ i would love share my results and try to keep getting information + data required\
    \ for the fine tuning experiments, if you share with me what you're working with,\
    \ i'll work off of that instead of my usual notebooks i ripped from medium or\
    \ whatever. ^^ \n"
  created_at: 2023-11-01 11:33:04+00:00
  edited: false
  hidden: false
  id: 65424580c7bf7cd7125f25dd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: stabilityai/stablelm-3b-4e1t
repo_type: model
status: open
target_branch: null
title: Fine tune model (SFTTrainer and transformers.Trainer)
