!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KatyTheCutie
conflicting_files: null
created_at: 2024-01-09 21:33:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653a2392341143f7774424d8/Ts1QIoXwlQkR741likXrb.png?w=200&h=200&f=face
      fullname: Katy Vetteriano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KatyTheCutie
      type: user
    createdAt: '2024-01-09T21:33:01.000Z'
    data:
      edited: false
      editors:
      - KatyTheCutie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9913740754127502
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653a2392341143f7774424d8/Ts1QIoXwlQkR741likXrb.png?w=200&h=200&f=face
          fullname: Katy Vetteriano
          isHf: false
          isPro: false
          name: KatyTheCutie
          type: user
        html: "<p>Do you perhaps have some code you could share that anyone can use\
          \ to finetune qwen 1.8b? I'm not quite sure how to do it, hopefully that\
          \ isnt an annoying question. \U0001F605</p>\n"
        raw: "Do you perhaps have some code you could share that anyone can use to\
          \ finetune qwen 1.8b? I'm not quite sure how to do it, hopefully that isnt\
          \ an annoying question. \U0001F605"
        updatedAt: '2024-01-09T21:33:01.160Z'
      numEdits: 0
      reactions: []
    id: 659dbb8dd67cf2d8263c2853
    type: comment
  author: KatyTheCutie
  content: "Do you perhaps have some code you could share that anyone can use to finetune\
    \ qwen 1.8b? I'm not quite sure how to do it, hopefully that isnt an annoying\
    \ question. \U0001F605"
  created_at: 2024-01-09 21:33:01+00:00
  edited: false
  hidden: false
  id: 659dbb8dd67cf2d8263c2853
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-10T06:37:06.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.824458122253418
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>No problem. Canon code is using the huggingface trainer method from
          the transformers library. I''ve writte a few jupyter notebooks in the past
          for fine tuning, but eventually I switched entirely to low-code solutions.<br>I''m
          using autotrain-advanced, it''s a command line tool based on transformers.<br>It''s
          not well documented but easy to use.<br><a rel="nofollow" href="https://github.com/huggingface/autotrain-advanced">https://github.com/huggingface/autotrain-advanced</a><br>Fort
          he arguments and their explanations, you need to browse the code here<br><a
          rel="nofollow" href="https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/cli/run_llm.py">https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/cli/run_llm.py</a></p>

          <p>you have to make a csv file; for normal fine tuning with a single column
          called ''text''<br>block size determines how many tokens per document are
          actually used, be careful, the standard value is i think 1k tokens; thus
          by default it does not necessarily use all information. sometimes that does
          not matter but you have to be intentional about it. max length just says
          how much context in theory the llm could handle the default is also not
          necessarily right.<br>you can but dont have to specify target modules. the
          standard modules tend to be more shallow than this, but they also use less
          vram. </p>

          <p>autotrain llm --train --project_name longalpaca-1b --model /run/media/knut/HD2/LLMs2/2b/Qwen-1_8B-llamafied/
          --data_path . --train_batch_size 1 --num_train_epochs 10 --trainer sft --use_int4
          --use_peft --merge_adapter --block_size 6292 --max-length 8192 --target_modules
          "q_proj,k_proj,v_proj,o_proj" --use_flash_attention</p>

          <p>Another easy way is axolotl, where you only have to learn how to specify
          a config file and input a json file. You have more knobs to turn in here,
          also for efficiency, yet I still default to autotrain for simple fine tuning.
          </p>

          <p><a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl">https://github.com/OpenAccess-AI-Collective/axolotl</a></p>

          '
        raw: "No problem. Canon code is using the huggingface trainer method from\
          \ the transformers library. I've writte a few jupyter notebooks in the past\
          \ for fine tuning, but eventually I switched entirely to low-code solutions.\
          \ \nI'm using autotrain-advanced, it's a command line tool based on transformers.\
          \ \nIt's not well documented but easy to use. \nhttps://github.com/huggingface/autotrain-advanced\n\
          Fort he arguments and their explanations, you need to browse the code here\n\
          https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/cli/run_llm.py\n\
          \nyou have to make a csv file; for normal fine tuning with a single column\
          \ called 'text'\nblock size determines how many tokens per document are\
          \ actually used, be careful, the standard value is i think 1k tokens; thus\
          \ by default it does not necessarily use all information. sometimes that\
          \ does not matter but you have to be intentional about it. max length just\
          \ says how much context in theory the llm could handle the default is also\
          \ not necessarily right. \nyou can but dont have to specify target modules.\
          \ the standard modules tend to be more shallow than this, but they also\
          \ use less vram. \n\nautotrain llm --train --project_name longalpaca-1b\
          \ --model /run/media/knut/HD2/LLMs2/2b/Qwen-1_8B-llamafied/ --data_path\
          \ . --train_batch_size 1 --num_train_epochs 10 --trainer sft --use_int4\
          \ --use_peft --merge_adapter --block_size 6292 --max-length 8192 --target_modules\
          \ \"q_proj,k_proj,v_proj,o_proj\" --use_flash_attention\n\nAnother easy\
          \ way is axolotl, where you only have to learn how to specify a config file\
          \ and input a json file. You have more knobs to turn in here, also for efficiency,\
          \ yet I still default to autotrain for simple fine tuning. \n\n\nhttps://github.com/OpenAccess-AI-Collective/axolotl"
        updatedAt: '2024-01-10T06:37:06.143Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659e3b1260736ff2a6eb90d9
    id: 659e3b1260736ff2a6eb90d6
    type: comment
  author: KnutJaegersberg
  content: "No problem. Canon code is using the huggingface trainer method from the\
    \ transformers library. I've writte a few jupyter notebooks in the past for fine\
    \ tuning, but eventually I switched entirely to low-code solutions. \nI'm using\
    \ autotrain-advanced, it's a command line tool based on transformers. \nIt's not\
    \ well documented but easy to use. \nhttps://github.com/huggingface/autotrain-advanced\n\
    Fort he arguments and their explanations, you need to browse the code here\nhttps://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/cli/run_llm.py\n\
    \nyou have to make a csv file; for normal fine tuning with a single column called\
    \ 'text'\nblock size determines how many tokens per document are actually used,\
    \ be careful, the standard value is i think 1k tokens; thus by default it does\
    \ not necessarily use all information. sometimes that does not matter but you\
    \ have to be intentional about it. max length just says how much context in theory\
    \ the llm could handle the default is also not necessarily right. \nyou can but\
    \ dont have to specify target modules. the standard modules tend to be more shallow\
    \ than this, but they also use less vram. \n\nautotrain llm --train --project_name\
    \ longalpaca-1b --model /run/media/knut/HD2/LLMs2/2b/Qwen-1_8B-llamafied/ --data_path\
    \ . --train_batch_size 1 --num_train_epochs 10 --trainer sft --use_int4 --use_peft\
    \ --merge_adapter --block_size 6292 --max-length 8192 --target_modules \"q_proj,k_proj,v_proj,o_proj\"\
    \ --use_flash_attention\n\nAnother easy way is axolotl, where you only have to\
    \ learn how to specify a config file and input a json file. You have more knobs\
    \ to turn in here, also for efficiency, yet I still default to autotrain for simple\
    \ fine tuning. \n\n\nhttps://github.com/OpenAccess-AI-Collective/axolotl"
  created_at: 2024-01-10 06:37:06+00:00
  edited: false
  hidden: false
  id: 659e3b1260736ff2a6eb90d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-10T06:37:06.000Z'
    data:
      status: closed
    id: 659e3b1260736ff2a6eb90d9
    type: status-change
  author: KnutJaegersberg
  created_at: 2024-01-10 06:37:06+00:00
  id: 659e3b1260736ff2a6eb90d9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-10T06:40:21.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9932417273521423
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>the file has to be named train.csv</p>

          '
        raw: the file has to be named train.csv
        updatedAt: '2024-01-10T06:40:21.672Z'
      numEdits: 0
      reactions: []
    id: 659e3bd5f59b66e60fdacacc
    type: comment
  author: KnutJaegersberg
  content: the file has to be named train.csv
  created_at: 2024-01-10 06:40:21+00:00
  edited: false
  hidden: false
  id: 659e3bd5f59b66e60fdacacc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: KnutJaegersberg/Deacon-1_8b
repo_type: model
status: closed
target_branch: null
title: finetuning code
