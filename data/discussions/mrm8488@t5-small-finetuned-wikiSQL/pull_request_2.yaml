!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iquery
conflicting_files: []
created_at: 2023-01-04 15:00:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1597589225543-noauth.png?w=200&h=200&f=face
      fullname: Romain Rigaux
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iquery
      type: user
    createdAt: '2023-01-04T15:00:38.000Z'
    data:
      edited: false
      editors:
      - iquery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1597589225543-noauth.png?w=200&h=200&f=face
          fullname: Romain Rigaux
          isHf: false
          isPro: false
          name: iquery
          type: user
        html: "<p>To avoid the warning</p>\n<pre><code> UserWarning: Neither `max_length`\
          \ nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`).\
          \ Controlling `max_length` via the config is deprecated and `max_length`\
          \ will be removed from the config in v5 of Transformers -- we recommend\
          \ using `max_new_tokens` to control the maximum length of the generation.\n\
          \  warnings.warn\n</code></pre>\n"
        raw: "To avoid the warning\n```\n UserWarning: Neither `max_length` nor `max_new_tokens`\
          \ has been set, `max_length` will default to 20 (`self.config.max_length`).\
          \ Controlling `max_length` via the config is deprecated and `max_length`\
          \ will be removed from the config in v5 of Transformers -- we recommend\
          \ using `max_new_tokens` to control the maximum length of the generation.\n\
          \  warnings.warn\n```"
        updatedAt: '2023-01-04T15:00:38.927Z'
      numEdits: 0
      reactions: []
    id: 63b594969223c073fe8a29b5
    type: comment
  author: iquery
  content: "To avoid the warning\n```\n UserWarning: Neither `max_length` nor `max_new_tokens`\
    \ has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling\
    \ `max_length` via the config is deprecated and `max_length` will be removed from\
    \ the config in v5 of Transformers -- we recommend using `max_new_tokens` to control\
    \ the maximum length of the generation.\n  warnings.warn\n```"
  created_at: 2023-01-04 15:00:38+00:00
  edited: false
  hidden: false
  id: 63b594969223c073fe8a29b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1597589225543-noauth.png?w=200&h=200&f=face
      fullname: Romain Rigaux
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iquery
      type: user
    createdAt: '2023-01-04T15:00:39.000Z'
    data:
      oid: e2fff746bc4f43107c15e2756031a5214cf1fea8
      parents:
      - 056271dcea0ef8323a836f4f836a1632b0579234
      subject: Add max_new_tokens to the config
    id: 63b594970000000000000000
    type: commit
  author: iquery
  created_at: 2023-01-04 15:00:39+00:00
  id: 63b594970000000000000000
  oid: e2fff746bc4f43107c15e2756031a5214cf1fea8
  summary: Add max_new_tokens to the config
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 2
repo_id: mrm8488/t5-small-finetuned-wikiSQL
repo_type: model
status: open
target_branch: refs/heads/main
title: Add max_new_tokens to the config
