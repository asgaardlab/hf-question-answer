!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kelheor
conflicting_files: null
created_at: 2023-04-14 21:01:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-04-14T22:01:03.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: "<p>I'm trying to use it with oobabooga / text-generation-webui<br>I\
          \ can load default vicuna-13b, but when I'm trying to load your model, I'm\
          \ getting this error:</p>\n<p>Traceback (most recent call last):<br>File\
          \ \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 442, in load_state_dict<br>return\
          \ torch.load(checkpoint_file, map_location=\u201Ccpu\u201D)<br>File \u201C\
          C:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\torch\\\
          serialization.py\u201D, line 815, in load<br>return _legacy_load(opened_file,\
          \ map_location, pickle_module, **pickle_load_args)<br>File \u201CC:\\Users\\\
          kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\torch\\serialization.py\u201D\
          , line 1033, in _legacy_load<br>magic_number = pickle_module.load(f, **pickle_load_args)<br>_pickle.UnpicklingError:\
          \ could not find MARK</p>\n<p>During handling of the above exception, another\
          \ exception occurred:</p>\n<p>Traceback (most recent call last):<br>File\
          \ \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 446, in load_state_dict<br>if\
          \ f.read(7) == \u201Cversion\u201D:<br>File \u201CC:\\Users\\kelhe\\anaconda3\\\
          envs\\textgen\\lib\\codecs.py\u201D, line 322, in decode<br>(result, consumed)\
          \ = self._buffer_decode(data, self.errors, final)<br>UnicodeDecodeError:\
          \ \u2018utf-8\u2019 codec can\u2019t decode byte 0x80 in position 28: invalid\
          \ start byte</p>\n<p>During handling of the above exception, another exception\
          \ occurred:</p>\n<p>Traceback (most recent call last):<br>File \u201CC:\\\
          text-generation-webui\\text-generation-webui\\server.py\u201D, line 84,\
          \ in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201CC:\\text-generation-webui\\text-generation-webui\\modules\\models.py\u201D\
          , line 171, in load_model<br>model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ **params)<br>File \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\\
          site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line 471,\
          \ in from_pretrained<br>return model_class.from_pretrained(<br>File \u201C\
          C:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 2560, in from_pretrained<br>state_dict = load_state_dict(resolved_archive_file)<br>File\
          \ \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 458, in load_state_dict<br>raise\
          \ OSError(<br>OSError: Unable to load weights from pytorch checkpoint file\
          \ for \u2018models\\reeducator_vicuna-13b-free\\pytorch_model.bin\u2019\
          \ at \u2018models\\reeducator_vicuna-13b-free\\pytorch_model.bin\u2019.\
          \ If you tried to load a PyTorch model from a TF 2.0 checkpoint, please\
          \ set from_tf=True.</p>\n<p>I made a separate folder for your model, where\
          \ I also put various files from default vicuna-13b, like config.json or\
          \ tokenizer_config.json</p>\n<p>I also renamed the model to pytorch_model.bin\
          \ since your default name didn't work (got this error: OSError: Error no\
          \ file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\reeducator_vicuna-13b-free)</p>\n<p>What am\
          \ I doing wrong?</p>\n"
        raw: "I'm trying to use it with oobabooga / text-generation-webui\r\nI can\
          \ load default vicuna-13b, but when I'm trying to load your model, I'm getting\
          \ this error:\r\n\r\nTraceback (most recent call last):\r\nFile \u201CC:\\\
          Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 442, in load_state_dict\r\nreturn torch.load(checkpoint_file,\
          \ map_location=\u201Ccpu\u201D)\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\\
          envs\\textgen\\lib\\site-packages\\torch\\serialization.py\u201D, line 815,\
          \ in load\r\nreturn _legacy_load(opened_file, map_location, pickle_module,\
          \ **pickle_load_args)\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\\
          textgen\\lib\\site-packages\\torch\\serialization.py\u201D, line 1033, in\
          \ _legacy_load\r\nmagic_number = pickle_module.load(f, **pickle_load_args)\r\
          \n_pickle.UnpicklingError: could not find MARK\r\n\r\nDuring handling of\
          \ the above exception, another exception occurred:\r\n\r\nTraceback (most\
          \ recent call last):\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\\
          lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 446, in\
          \ load_state_dict\r\nif f.read(7) == \u201Cversion\u201D:\r\nFile \u201C\
          C:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\codecs.py\u201D, line 322,\
          \ in decode\r\n(result, consumed) = self._buffer_decode(data, self.errors,\
          \ final)\r\nUnicodeDecodeError: \u2018utf-8\u2019 codec can\u2019t decode\
          \ byte 0x80 in position 28: invalid start byte\r\n\r\nDuring handling of\
          \ the above exception, another exception occurred:\r\n\r\nTraceback (most\
          \ recent call last):\r\nFile \u201CC:\\text-generation-webui\\text-generation-webui\\\
          server.py\u201D, line 84, in load_model_wrapper\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\r\nFile \u201CC:\\text-generation-webui\\\
          text-generation-webui\\modules\\models.py\u201D, line 171, in load_model\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, **params)\r\n\
          File \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\r\
          \nreturn model_class.from_pretrained(\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\\
          envs\\textgen\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 2560, in from_pretrained\r\nstate_dict = load_state_dict(resolved_archive_file)\r\
          \nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 458, in load_state_dict\r\n\
          raise OSError(\r\nOSError: Unable to load weights from pytorch checkpoint\
          \ file for \u2018models\\reeducator_vicuna-13b-free\\pytorch_model.bin\u2019\
          \ at \u2018models\\reeducator_vicuna-13b-free\\pytorch_model.bin\u2019.\
          \ If you tried to load a PyTorch model from a TF 2.0 checkpoint, please\
          \ set from_tf=True.\r\n\r\nI made a separate folder for your model, where\
          \ I also put various files from default vicuna-13b, like config.json or\
          \ tokenizer_config.json\r\n\r\nI also renamed the model to pytorch_model.bin\
          \ since your default name didn't work (got this error: OSError: Error no\
          \ file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\reeducator_vicuna-13b-free)\r\n\r\nWhat am\
          \ I doing wrong?"
        updatedAt: '2023-04-14T22:01:03.639Z'
      numEdits: 0
      reactions: []
    id: 6439cd1f0b4e2e224887939e
    type: comment
  author: Kelheor
  content: "I'm trying to use it with oobabooga / text-generation-webui\r\nI can load\
    \ default vicuna-13b, but when I'm trying to load your model, I'm getting this\
    \ error:\r\n\r\nTraceback (most recent call last):\r\nFile \u201CC:\\Users\\kelhe\\\
    anaconda3\\envs\\textgen\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
    , line 442, in load_state_dict\r\nreturn torch.load(checkpoint_file, map_location=\u201C\
    cpu\u201D)\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
    torch\\serialization.py\u201D, line 815, in load\r\nreturn _legacy_load(opened_file,\
    \ map_location, pickle_module, **pickle_load_args)\r\nFile \u201CC:\\Users\\kelhe\\\
    anaconda3\\envs\\textgen\\lib\\site-packages\\torch\\serialization.py\u201D, line\
    \ 1033, in _legacy_load\r\nmagic_number = pickle_module.load(f, **pickle_load_args)\r\
    \n_pickle.UnpicklingError: could not find MARK\r\n\r\nDuring handling of the above\
    \ exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\
    \nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
    transformers\\modeling_utils.py\u201D, line 446, in load_state_dict\r\nif f.read(7)\
    \ == \u201Cversion\u201D:\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\\
    lib\\codecs.py\u201D, line 322, in decode\r\n(result, consumed) = self._buffer_decode(data,\
    \ self.errors, final)\r\nUnicodeDecodeError: \u2018utf-8\u2019 codec can\u2019\
    t decode byte 0x80 in position 28: invalid start byte\r\n\r\nDuring handling of\
    \ the above exception, another exception occurred:\r\n\r\nTraceback (most recent\
    \ call last):\r\nFile \u201CC:\\text-generation-webui\\text-generation-webui\\\
    server.py\u201D, line 84, in load_model_wrapper\r\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\r\nFile \u201CC:\\text-generation-webui\\text-generation-webui\\\
    modules\\models.py\u201D, line 171, in load_model\r\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\
    \ **params)\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\r\
    \nreturn model_class.from_pretrained(\r\nFile \u201CC:\\Users\\kelhe\\anaconda3\\\
    envs\\textgen\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line\
    \ 2560, in from_pretrained\r\nstate_dict = load_state_dict(resolved_archive_file)\r\
    \nFile \u201CC:\\Users\\kelhe\\anaconda3\\envs\\textgen\\lib\\site-packages\\\
    transformers\\modeling_utils.py\u201D, line 458, in load_state_dict\r\nraise OSError(\r\
    \nOSError: Unable to load weights from pytorch checkpoint file for \u2018models\\\
    reeducator_vicuna-13b-free\\pytorch_model.bin\u2019 at \u2018models\\reeducator_vicuna-13b-free\\\
    pytorch_model.bin\u2019. If you tried to load a PyTorch model from a TF 2.0 checkpoint,\
    \ please set from_tf=True.\r\n\r\nI made a separate folder for your model, where\
    \ I also put various files from default vicuna-13b, like config.json or tokenizer_config.json\r\
    \n\r\nI also renamed the model to pytorch_model.bin since your default name didn't\
    \ work (got this error: OSError: Error no file named pytorch_model.bin, tf_model.h5,\
    \ model.ckpt.index or flax_model.msgpack found in directory models\\reeducator_vicuna-13b-free)\r\
    \n\r\nWhat am I doing wrong?"
  created_at: 2023-04-14 21:01:03+00:00
  edited: false
  hidden: false
  id: 6439cd1f0b4e2e224887939e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-04-14T22:52:41.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: '<p>I managed to make it work with llama.cpp in text-generation-webui<br>I
          made folders vicuna-13b-free and vicuna-13b-free-4bit and put there downloaded
          models. Then I added "ggml-" to the name of each model, like ggml-vicuna-13b-free-q4_0.bin
          and I run it with command call python server.py --auto-devices --chat ---model
          vicuna-13b-free-4bit<br>The problem is that llama.cpp works only with CPU
          as I understood. Is there any way to run it on GPU?</p>

          '
        raw: 'I managed to make it work with llama.cpp in text-generation-webui

          I made folders vicuna-13b-free and vicuna-13b-free-4bit and put there downloaded
          models. Then I added "ggml-" to the name of each model, like ggml-vicuna-13b-free-q4_0.bin
          and I run it with command call python server.py --auto-devices --chat ---model
          vicuna-13b-free-4bit

          The problem is that llama.cpp works only with CPU as I understood. Is there
          any way to run it on GPU?'
        updatedAt: '2023-04-14T22:52:41.660Z'
      numEdits: 0
      reactions: []
    id: 6439d939555050298bd541af
    type: comment
  author: Kelheor
  content: 'I managed to make it work with llama.cpp in text-generation-webui

    I made folders vicuna-13b-free and vicuna-13b-free-4bit and put there downloaded
    models. Then I added "ggml-" to the name of each model, like ggml-vicuna-13b-free-q4_0.bin
    and I run it with command call python server.py --auto-devices --chat ---model
    vicuna-13b-free-4bit

    The problem is that llama.cpp works only with CPU as I understood. Is there any
    way to run it on GPU?'
  created_at: 2023-04-14 21:52:41+00:00
  edited: false
  hidden: false
  id: 6439d939555050298bd541af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-04-14T23:12:44.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: '<p>New file vicuna-13b-free-4bit-128g.safetensors did the trick. Now
          it works on GPU.</p>

          '
        raw: New file vicuna-13b-free-4bit-128g.safetensors did the trick. Now it
          works on GPU.
        updatedAt: '2023-04-14T23:12:44.824Z'
      numEdits: 0
      reactions: []
    id: 6439ddeccaae308a1473cf7a
    type: comment
  author: Kelheor
  content: New file vicuna-13b-free-4bit-128g.safetensors did the trick. Now it works
    on GPU.
  created_at: 2023-04-14 22:12:44+00:00
  edited: false
  hidden: false
  id: 6439ddeccaae308a1473cf7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-04-14T23:13:02.000Z'
    data:
      status: closed
    id: 6439ddfe8337a7ef7f5ead9b
    type: status-change
  author: Kelheor
  created_at: 2023-04-14 22:13:02+00:00
  id: 6439ddfe8337a7ef7f5ead9b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: closed
target_branch: null
title: Can't load the model
