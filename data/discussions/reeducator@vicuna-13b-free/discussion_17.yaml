!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-04-30 16:22:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-30T17:22:48.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Do I just copy over the config, tokenizer_config, and tokenizer.model
          from another 13B Vicuna Model to get this working in in ooba?<br>I think
          that''s what I did before. I take the "vicuna-13b-free-V4.3-4bit-128g" put
          it in a folder. And then I copied the weights or whatever they are called
          over from my other model "vicuna-13b-GPTQ-4bit-128g" and it worked fine.
          </p>

          <p>If I''m not suppose to do this and need a more specific config, tokenizer_config,
          and tokenizer.model please let me know. And I think those are the only three
          files I need? Thank you for all the hard work.</p>

          <p>Wait never mind maybe there is something else I need to do? I am getting
          an error.  May I need newer weights for this 1.1 model or something?<br>My
          arguments are "call python server.py --auto-devices --chat --model vicuna-13b-free_4.3
          --wbits 4 --groupsize 128"</p>

          <p>Starting the web UI...<br>Gradio HTTP request redirected to localhost
          :)<br>Loading vicuna-13b-free_4.3...<br>Found the following quantized model:
          models\vicuna-13b-free_4.3\vicuna-13b-free-V4.3-4bit-128g.safetensors<br>Loading
          model ...<br>Traceback (most recent call last):<br>  File "C:\AI\oobabooga-windowsBest\text-generation-webui\server.py",
          line 914, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\AI\oobabooga-windowsBest\text-generation-webui\modules\models.py", line
          158, in load_model<br>    model = load_quantized(model_name)<br>  File "C:\AI\oobabooga-windowsBest\text-generation-webui\modules\GPTQ_loader.py",
          line 176, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "C:\AI\oobabooga-windowsBest\text-generation-webui\modules\GPTQ_loader.py",
          line 77, in _load_quant<br>    model.load_state_dict(safe_load(checkpoint),
          strict=False)<br>  File "C:\AI\oobabooga-windowsBest\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 2041, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for LlamaForCausalLM:<br>        size mismatch for model.embed_tokens.weight:
          copying a param with shape torch.Size([32000, 5120]) from checkpoint, the
          shape in current model is torch.Size([32001, 5120]).<br>        size mismatch
          for lm_head.weight: copying a param with shape torch.Size([32000, 5120])
          from checkpoint, the shape in current model is torch.Size([32001, 5120]).<br>Press
          any key to continue . . .</p>

          '
        raw: "Do I just copy over the config, tokenizer_config, and tokenizer.model\
          \ from another 13B Vicuna Model to get this working in in ooba? \nI think\
          \ that's what I did before. I take the \"vicuna-13b-free-V4.3-4bit-128g\"\
          \ put it in a folder. And then I copied the weights or whatever they are\
          \ called over from my other model \"vicuna-13b-GPTQ-4bit-128g\" and it worked\
          \ fine. \n\nIf I'm not suppose to do this and need a more specific config,\
          \ tokenizer_config, and tokenizer.model please let me know. And I think\
          \ those are the only three files I need? Thank you for all the hard work.\n\
          \nWait never mind maybe there is something else I need to do? I am getting\
          \ an error.  May I need newer weights for this 1.1 model or something? \n\
          My arguments are \"call python server.py --auto-devices --chat --model vicuna-13b-free_4.3\
          \ --wbits 4 --groupsize 128\"\n\nStarting the web UI...\nGradio HTTP request\
          \ redirected to localhost :)\nLoading vicuna-13b-free_4.3...\nFound the\
          \ following quantized model: models\\vicuna-13b-free_4.3\\vicuna-13b-free-V4.3-4bit-128g.safetensors\n\
          Loading model ...\nTraceback (most recent call last):\n  File \"C:\\AI\\\
          oobabooga-windowsBest\\text-generation-webui\\server.py\", line 914, in\
          \ <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\\
          models.py\", line 158, in load_model\n    model = load_quantized(model_name)\n\
          \  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\\
          GPTQ_loader.py\", line 176, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\\
          GPTQ_loader.py\", line 77, in _load_quant\n    model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n  File \"C:\\AI\\oobabooga-windowsBest\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 2041, in\
          \ load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
          \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM:\n        size mismatch for model.embed_tokens.weight:\
          \ copying a param with shape torch.Size([32000, 5120]) from checkpoint,\
          \ the shape in current model is torch.Size([32001, 5120]).\n        size\
          \ mismatch for lm_head.weight: copying a param with shape torch.Size([32000,\
          \ 5120]) from checkpoint, the shape in current model is torch.Size([32001,\
          \ 5120]).\nPress any key to continue . . ."
        updatedAt: '2023-04-30T17:33:41.359Z'
      numEdits: 3
      reactions: []
    id: 644ea3e8d6001776ed77f4f0
    type: comment
  author: Goldenblood56
  content: "Do I just copy over the config, tokenizer_config, and tokenizer.model\
    \ from another 13B Vicuna Model to get this working in in ooba? \nI think that's\
    \ what I did before. I take the \"vicuna-13b-free-V4.3-4bit-128g\" put it in a\
    \ folder. And then I copied the weights or whatever they are called over from\
    \ my other model \"vicuna-13b-GPTQ-4bit-128g\" and it worked fine. \n\nIf I'm\
    \ not suppose to do this and need a more specific config, tokenizer_config, and\
    \ tokenizer.model please let me know. And I think those are the only three files\
    \ I need? Thank you for all the hard work.\n\nWait never mind maybe there is something\
    \ else I need to do? I am getting an error.  May I need newer weights for this\
    \ 1.1 model or something? \nMy arguments are \"call python server.py --auto-devices\
    \ --chat --model vicuna-13b-free_4.3 --wbits 4 --groupsize 128\"\n\nStarting the\
    \ web UI...\nGradio HTTP request redirected to localhost :)\nLoading vicuna-13b-free_4.3...\n\
    Found the following quantized model: models\\vicuna-13b-free_4.3\\vicuna-13b-free-V4.3-4bit-128g.safetensors\n\
    Loading model ...\nTraceback (most recent call last):\n  File \"C:\\AI\\oobabooga-windowsBest\\\
    text-generation-webui\\server.py\", line 914, in <module>\n    shared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\n  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\\
    modules\\models.py\", line 158, in load_model\n    model = load_quantized(model_name)\n\
    \  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\GPTQ_loader.py\"\
    , line 176, in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\GPTQ_loader.py\"\
    , line 77, in _load_quant\n    model.load_state_dict(safe_load(checkpoint), strict=False)\n\
    \  File \"C:\\AI\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 2041, in load_state_dict\n    raise RuntimeError('Error(s)\
    \ in loading state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
    \ state_dict for LlamaForCausalLM:\n        size mismatch for model.embed_tokens.weight:\
    \ copying a param with shape torch.Size([32000, 5120]) from checkpoint, the shape\
    \ in current model is torch.Size([32001, 5120]).\n        size mismatch for lm_head.weight:\
    \ copying a param with shape torch.Size([32000, 5120]) from checkpoint, the shape\
    \ in current model is torch.Size([32001, 5120]).\nPress any key to continue .\
    \ . ."
  created_at: 2023-04-30 16:22:48+00:00
  edited: true
  hidden: false
  id: 644ea3e8d6001776ed77f4f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-30T17:23:46.000Z'
    data:
      from: 'Do I just copy over the config, tokenizer_config, and tokenizer.model
        from another 13B Vicuna Model to get this working in in ooba? '
      to: Do I just copy over the config, tokenizer_config, and tokenizer.model from
        another 13B Vicuna Model to get this working in ooba?
    id: 644ea422bf9683cba464dc69
    type: title-change
  author: Goldenblood56
  created_at: 2023-04-30 16:23:46+00:00
  id: 644ea422bf9683cba464dc69
  new_title: Do I just copy over the config, tokenizer_config, and tokenizer.model
    from another 13B Vicuna Model to get this working in ooba?
  old_title: 'Do I just copy over the config, tokenizer_config, and tokenizer.model
    from another 13B Vicuna Model to get this working in in ooba? '
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-30T17:33:01.000Z'
    data:
      from: Do I just copy over the config, tokenizer_config, and tokenizer.model
        from another 13B Vicuna Model to get this working in ooba?
      to: Do I just copy over the config, tokenizer_config, and tokenizer.model from
        another 13B Vicuna Model to get this working in ooba? I am getting error.
    id: 644ea64dcf72e60a5b7e65e0
    type: title-change
  author: Goldenblood56
  created_at: 2023-04-30 16:33:01+00:00
  id: 644ea64dcf72e60a5b7e65e0
  new_title: Do I just copy over the config, tokenizer_config, and tokenizer.model
    from another 13B Vicuna Model to get this working in ooba? I am getting error.
  old_title: Do I just copy over the config, tokenizer_config, and tokenizer.model
    from another 13B Vicuna Model to get this working in ooba?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-30T17:38:16.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I will upload the tokenizer and config files soon. Generally it
          has worked if one takes them from some other Vicuna 13B, but just in case
          and for convenience I''ll include them in this repository as well.</p>

          '
        raw: I will upload the tokenizer and config files soon. Generally it has worked
          if one takes them from some other Vicuna 13B, but just in case and for convenience
          I'll include them in this repository as well.
        updatedAt: '2023-04-30T17:38:16.455Z'
      numEdits: 0
      reactions: []
    id: 644ea788bf9683cba46525ed
    type: comment
  author: reeducator
  content: I will upload the tokenizer and config files soon. Generally it has worked
    if one takes them from some other Vicuna 13B, but just in case and for convenience
    I'll include them in this repository as well.
  created_at: 2023-04-30 16:38:16+00:00
  edited: false
  hidden: false
  id: 644ea788bf9683cba46525ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-30T17:41:03.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thank you Reeducator that will rule out chances of someone making
          a mistake and using the wrong ones. I think something in my config file
          might be causing this error. But I will wait until you post what is needed
          and see if my problem goes away.</p>

          '
        raw: Thank you Reeducator that will rule out chances of someone making a mistake
          and using the wrong ones. I think something in my config file might be causing
          this error. But I will wait until you post what is needed and see if my
          problem goes away.
        updatedAt: '2023-04-30T17:41:03.451Z'
      numEdits: 0
      reactions: []
    id: 644ea82fddf20748b05bef5a
    type: comment
  author: Goldenblood56
  content: Thank you Reeducator that will rule out chances of someone making a mistake
    and using the wrong ones. I think something in my config file might be causing
    this error. But I will wait until you post what is needed and see if my problem
    goes away.
  created_at: 2023-04-30 16:41:03+00:00
  edited: false
  hidden: false
  id: 644ea82fddf20748b05bef5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-30T19:27:50.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I''m getting the same error too, and I used previous files that
          worked with the v1.0 of this model.</p>

          '
        raw: I'm getting the same error too, and I used previous files that worked
          with the v1.0 of this model.
        updatedAt: '2023-04-30T19:27:50.466Z'
      numEdits: 0
      reactions: []
    id: 644ec136ddf20748b05de192
    type: comment
  author: mancub
  content: I'm getting the same error too, and I used previous files that worked with
    the v1.0 of this model.
  created_at: 2023-04-30 18:27:50+00:00
  edited: false
  hidden: false
  id: 644ec136ddf20748b05de192
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-30T19:33:31.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Ok, it does work using another similar models files.</p>

          '
        raw: Ok, it does work using another similar models files.
        updatedAt: '2023-04-30T19:33:31.340Z'
      numEdits: 0
      reactions: []
    id: 644ec28bcf72e60a5b80a6b7
    type: comment
  author: mancub
  content: Ok, it does work using another similar models files.
  created_at: 2023-04-30 18:33:31+00:00
  edited: false
  hidden: false
  id: 644ec28bcf72e60a5b80a6b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305d290c79269321bfd220f112779e53.svg
      fullname: Alex P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: navyfighter12
      type: user
    createdAt: '2023-04-30T19:35:04.000Z'
    data:
      edited: false
      editors:
      - navyfighter12
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305d290c79269321bfd220f112779e53.svg
          fullname: Alex P
          isHf: false
          isPro: false
          name: navyfighter12
          type: user
        html: '<p>I''m getting the same error. Ill try the new files once they upload.
          Thanks for all your hard work!</p>

          '
        raw: I'm getting the same error. Ill try the new files once they upload. Thanks
          for all your hard work!
        updatedAt: '2023-04-30T19:35:04.310Z'
      numEdits: 0
      reactions: []
    id: 644ec2e8bf9683cba4675d76
    type: comment
  author: navyfighter12
  content: I'm getting the same error. Ill try the new files once they upload. Thanks
    for all your hard work!
  created_at: 2023-04-30 18:35:04+00:00
  edited: false
  hidden: false
  id: 644ec2e8bf9683cba4675d76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-30T19:38:35.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>just use the files from another model, I used these <a href="https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main">https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main</a></p>

          '
        raw: just use the files from another model, I used these https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main
        updatedAt: '2023-04-30T19:38:35.154Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - navyfighter12
        - Goldenblood56
    id: 644ec3bba00f4b11d3995684
    type: comment
  author: mancub
  content: just use the files from another model, I used these https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main
  created_at: 2023-04-30 18:38:35+00:00
  edited: false
  hidden: false
  id: 644ec3bba00f4b11d3995684
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305d290c79269321bfd220f112779e53.svg
      fullname: Alex P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: navyfighter12
      type: user
    createdAt: '2023-04-30T20:28:33.000Z'
    data:
      edited: false
      editors:
      - navyfighter12
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305d290c79269321bfd220f112779e53.svg
          fullname: Alex P
          isHf: false
          isPro: false
          name: navyfighter12
          type: user
        html: '<blockquote>

          <p>just use the files from another model, I used these <a href="https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main">https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main</a></p>

          </blockquote>

          <p>That worked thank you.</p>

          '
        raw: '> just use the files from another model, I used these https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main


          That worked thank you.'
        updatedAt: '2023-04-30T20:28:33.449Z'
      numEdits: 0
      reactions: []
    id: 644ecf71ddf20748b05ef5a5
    type: comment
  author: navyfighter12
  content: '> just use the files from another model, I used these https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main


    That worked thank you.'
  created_at: 2023-04-30 19:28:33+00:00
  edited: false
  hidden: false
  id: 644ecf71ddf20748b05ef5a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-30T21:24:35.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I added the tokenizer and config to hf-output directory. That should
          work hopefully.</p>

          '
        raw: I added the tokenizer and config to hf-output directory. That should
          work hopefully.
        updatedAt: '2023-04-30T21:24:35.328Z'
      numEdits: 0
      reactions: []
    id: 644edc93a00f4b11d39b4404
    type: comment
  author: reeducator
  content: I added the tokenizer and config to hf-output directory. That should work
    hopefully.
  created_at: 2023-04-30 20:24:35+00:00
  edited: false
  hidden: false
  id: 644edc93a00f4b11d39b4404
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-30T22:10:29.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<blockquote>

          <p>I added the tokenizer and config to hf-output directory. That should
          work hopefully.</p>

          </blockquote>

          <p>config.json is needed as well.</p>

          '
        raw: '> I added the tokenizer and config to hf-output directory. That should
          work hopefully.


          config.json is needed as well.'
        updatedAt: '2023-04-30T22:10:29.552Z'
      numEdits: 0
      reactions: []
    id: 644ee755a00f4b11d39c100c
    type: comment
  author: mancub
  content: '> I added the tokenizer and config to hf-output directory. That should
    work hopefully.


    config.json is needed as well.'
  created_at: 2023-04-30 21:10:29+00:00
  edited: false
  hidden: false
  id: 644ee755a00f4b11d39c100c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-30T22:20:32.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I added the tokenizer and config to hf-output directory. That should
          work hopefully.</p>

          </blockquote>

          <p>config.json is needed as well.</p>

          </blockquote>

          <p>Yeah, added.</p>

          '
        raw: "> > I added the tokenizer and config to hf-output directory. That should\
          \ work hopefully.\n> \n> config.json is needed as well.\n\nYeah, added."
        updatedAt: '2023-04-30T22:20:32.216Z'
      numEdits: 0
      reactions: []
    id: 644ee9b0a00f4b11d39c3c8f
    type: comment
  author: reeducator
  content: "> > I added the tokenizer and config to hf-output directory. That should\
    \ work hopefully.\n> \n> config.json is needed as well.\n\nYeah, added."
  created_at: 2023-04-30 21:20:32+00:00
  edited: false
  hidden: false
  id: 644ee9b0a00f4b11d39c3c8f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: open
target_branch: null
title: Do I just copy over the config, tokenizer_config, and tokenizer.model from
  another 13B Vicuna Model to get this working in ooba? I am getting error.
