!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anon8231489123
conflicting_files: null
created_at: 2023-04-14 15:09:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c9c09029317af408f9804df02f645ed.svg
      fullname: z.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon8231489123
      type: user
    createdAt: '2023-04-14T16:09:15.000Z'
    data:
      edited: false
      editors:
      - anon8231489123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c9c09029317af408f9804df02f645ed.svg
          fullname: z.
          isHf: false
          isPro: false
          name: anon8231489123
          type: user
        html: '<p>Thank you so much for being the first to do this, seriously. Can
          you post the hyperparameters you used for training?</p>

          '
        raw: Thank you so much for being the first to do this, seriously. Can you
          post the hyperparameters you used for training?
        updatedAt: '2023-04-14T16:09:15.060Z'
      numEdits: 0
      reactions: []
    id: 64397aab0989db052b615c43
    type: comment
  author: anon8231489123
  content: Thank you so much for being the first to do this, seriously. Can you post
    the hyperparameters you used for training?
  created_at: 2023-04-14 15:09:15+00:00
  edited: false
  hidden: false
  id: 64397aab0989db052b615c43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-14T16:32:32.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p>You're welcome, and many thanks for leading the work on the dataset.</p>\n\
          <p>The hyperparameters are same as they were in the 1.0 snapshot of the\
          \ FastChat repository, except for the modifications to train on 40GB A100s\
          \ (mainly gradient accumulation steps and such):</p>\n<pre><code>    .....\
          \ (most setup specific items removed)\n    --num_train_epochs 3 \\\n   \
          \ --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2\
          \ \\\n    --gradient_accumulation_steps $((128 * 512 / 2048 / 2 / 4 / 4))\
          \ \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\"\
          \ \\\n    --save_steps 1200 \\\n    --save_total_limit 10 \\\n    --learning_rate\
          \ 2e-5 \\\n    --weight_decay 0. \\\n    --warmup_ratio 0.03 \\\n    --lr_scheduler_type\
          \ \"cosine\" \\\n    --logging_steps 1 \\\n    --fsdp \"full_shard auto_wrap\"\
          \ \\\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n\
          \    --tf32 False \\\n    --model_max_length 2048 \\\n    --gradient_checkpointing\
          \ True \\\n    --lazy_preprocess True\n</code></pre>\n<p>I will probably\
          \ try add <code>--fsdp \"shard_grad_op auto_wrap\"</code> also for the next\
          \ run, since there are reports  it could help make it a bit faster.</p>\n"
        raw: "You're welcome, and many thanks for leading the work on the dataset.\n\
          \nThe hyperparameters are same as they were in the 1.0 snapshot of the FastChat\
          \ repository, except for the modifications to train on 40GB A100s (mainly\
          \ gradient accumulation steps and such):\n```\n    ..... (most setup specific\
          \ items removed)\n    --num_train_epochs 3 \\\n    --per_device_train_batch_size\
          \ 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps\
          \ $((128 * 512 / 2048 / 2 / 4 / 4)) \\\n    --evaluation_strategy \"no\"\
          \ \\\n    --save_strategy \"steps\" \\\n    --save_steps 1200 \\\n    --save_total_limit\
          \ 10 \\\n    --learning_rate 2e-5 \\\n    --weight_decay 0. \\\n    --warmup_ratio\
          \ 0.03 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1\
          \ \\\n    --fsdp \"full_shard auto_wrap\" \\\n    --fsdp_transformer_layer_cls_to_wrap\
          \ 'LlamaDecoderLayer' \\\n    --tf32 False \\\n    --model_max_length 2048\
          \ \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True\n\
          ```\nI will probably try add ```--fsdp \"shard_grad_op auto_wrap\"``` also\
          \ for the next run, since there are reports  it could help make it a bit\
          \ faster."
        updatedAt: '2023-04-14T16:32:32.629Z'
      numEdits: 0
      reactions: []
    id: 64398020c5f84418d04f5a6e
    type: comment
  author: reeducator
  content: "You're welcome, and many thanks for leading the work on the dataset.\n\
    \nThe hyperparameters are same as they were in the 1.0 snapshot of the FastChat\
    \ repository, except for the modifications to train on 40GB A100s (mainly gradient\
    \ accumulation steps and such):\n```\n    ..... (most setup specific items removed)\n\
    \    --num_train_epochs 3 \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size\
    \ 2 \\\n    --gradient_accumulation_steps $((128 * 512 / 2048 / 2 / 4 / 4)) \\\
    \n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps\
    \ 1200 \\\n    --save_total_limit 10 \\\n    --learning_rate 2e-5 \\\n    --weight_decay\
    \ 0. \\\n    --warmup_ratio 0.03 \\\n    --lr_scheduler_type \"cosine\" \\\n \
    \   --logging_steps 1 \\\n    --fsdp \"full_shard auto_wrap\" \\\n    --fsdp_transformer_layer_cls_to_wrap\
    \ 'LlamaDecoderLayer' \\\n    --tf32 False \\\n    --model_max_length 2048 \\\n\
    \    --gradient_checkpointing True \\\n    --lazy_preprocess True\n```\nI will\
    \ probably try add ```--fsdp \"shard_grad_op auto_wrap\"``` also for the next\
    \ run, since there are reports  it could help make it a bit faster."
  created_at: 2023-04-14 15:32:32+00:00
  edited: false
  hidden: false
  id: 64398020c5f84418d04f5a6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
      fullname: Frankie G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spanielrassler
      type: user
    createdAt: '2023-04-14T16:42:59.000Z'
    data:
      edited: false
      editors:
      - spanielrassler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
          fullname: Frankie G
          isHf: false
          isPro: false
          name: spanielrassler
          type: user
        html: '<p>Yes, thanks! And thanks for going straight to ggml version -- long
          live llama.cpp!! </p>

          <p>By the way, any reason for not going with the 1.1 version? I assume you
          already started before it was released but maybe there was another reason?</p>

          '
        raw: "Yes, thanks! And thanks for going straight to ggml version -- long live\
          \ llama.cpp!! \n\nBy the way, any reason for not going with the 1.1 version?\
          \ I assume you already started before it was released but maybe there was\
          \ another reason?"
        updatedAt: '2023-04-14T16:42:59.778Z'
      numEdits: 0
      reactions: []
    id: 64398293eb983fe610bf74aa
    type: comment
  author: spanielrassler
  content: "Yes, thanks! And thanks for going straight to ggml version -- long live\
    \ llama.cpp!! \n\nBy the way, any reason for not going with the 1.1 version? I\
    \ assume you already started before it was released but maybe there was another\
    \ reason?"
  created_at: 2023-04-14 15:42:59+00:00
  edited: false
  hidden: false
  id: 64398293eb983fe610bf74aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c84200d82b2a5c0729007e5a991fe16.svg
      fullname: zatochu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zatochu
      type: user
    createdAt: '2023-04-14T17:26:35.000Z'
    data:
      edited: true
      editors:
      - zatochu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c84200d82b2a5c0729007e5a991fe16.svg
          fullname: zatochu
          isHf: false
          isPro: false
          name: zatochu
          type: user
        html: '<p>Thanks for training this. Seems good! Re-quantized the f16 bin (thanks
          for providing it as well!) with <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/896">https://github.com/ggerganov/llama.cpp/pull/896</a>
          as it seems to improve outputs/perplexity. Side note: I suggest editing
          the pull so <code>bool useNewQuantization = false;</code> is <code>bool
          useNewQuantization = true;</code> and/or <code>case 2: quantized_type =
          GGML_TYPE_Q4_0; break;</code> is <code>case 2: quantized_type = GGML_TYPE_Q4_0;
          useNewQuantization = true; break;</code> so you can use 2 with quantize.
          Otherwise the model will be written with ftype 4 and the model will report
          as mostly q4_1/some f16 when it isn''t.</p>

          '
        raw: 'Thanks for training this. Seems good! Re-quantized the f16 bin (thanks
          for providing it as well!) with https://github.com/ggerganov/llama.cpp/pull/896
          as it seems to improve outputs/perplexity. Side note: I suggest editing
          the pull so `bool useNewQuantization = false;` is `bool useNewQuantization
          = true;` and/or `case 2: quantized_type = GGML_TYPE_Q4_0; break;` is `case
          2: quantized_type = GGML_TYPE_Q4_0; useNewQuantization = true; break;` so
          you can use 2 with quantize. Otherwise the model will be written with ftype
          4 and the model will report as mostly q4_1/some f16 when it isn''t.'
        updatedAt: '2023-04-14T17:33:44.477Z'
      numEdits: 3
      reactions: []
    id: 64398ccb9f49f6e6ee2488d9
    type: comment
  author: zatochu
  content: 'Thanks for training this. Seems good! Re-quantized the f16 bin (thanks
    for providing it as well!) with https://github.com/ggerganov/llama.cpp/pull/896
    as it seems to improve outputs/perplexity. Side note: I suggest editing the pull
    so `bool useNewQuantization = false;` is `bool useNewQuantization = true;` and/or
    `case 2: quantized_type = GGML_TYPE_Q4_0; break;` is `case 2: quantized_type =
    GGML_TYPE_Q4_0; useNewQuantization = true; break;` so you can use 2 with quantize.
    Otherwise the model will be written with ftype 4 and the model will report as
    mostly q4_1/some f16 when it isn''t.'
  created_at: 2023-04-14 16:26:35+00:00
  edited: true
  hidden: false
  id: 64398ccb9f49f6e6ee2488d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
      fullname: Dat Boi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nucleardiffusion
      type: user
    createdAt: '2023-04-14T18:09:42.000Z'
    data:
      edited: true
      editors:
      - nucleardiffusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
          fullname: Dat Boi
          isHf: false
          isPro: false
          name: nucleardiffusion
          type: user
        html: '<p>will this work with Llamacpp ?</p>

          '
        raw: will this work with Llamacpp ?
        updatedAt: '2023-04-14T18:10:16.663Z'
      numEdits: 1
      reactions: []
    id: 643996e668228e8b33445def
    type: comment
  author: nucleardiffusion
  content: will this work with Llamacpp ?
  created_at: 2023-04-14 17:09:42+00:00
  edited: true
  hidden: false
  id: 643996e668228e8b33445def
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
      fullname: Dat Boi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nucleardiffusion
      type: user
    createdAt: '2023-04-14T18:12:55.000Z'
    data:
      edited: false
      editors:
      - nucleardiffusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
          fullname: Dat Boi
          isHf: false
          isPro: false
          name: nucleardiffusion
          type: user
        html: '<p>Also, what do you reccomend for best results in the prompts text
          file? I currently use:</p>

          <p>You are an AI language model designed to assist the Human by answering
          their questions, offering advice, and engaging in casual conversation in
          a friendly, helpful, and informative manner. You respond clearly, coherently,
          and you consider the conversation history.</p>

          <h3 id="human-hey-hows-it-going">Human: Hey, how''s it going?</h3>

          <p>Assistant: Hey there! I''m doing great, thank you. What can I help you
          with today? Let''s have a fun chat!</p>

          '
        raw: 'Also, what do you reccomend for best results in the prompts text file?
          I currently use:


          You are an AI language model designed to assist the Human by answering their
          questions, offering advice, and engaging in casual conversation in a friendly,
          helpful, and informative manner. You respond clearly, coherently, and you
          consider the conversation history.


          ### Human: Hey, how''s it going?


          Assistant: Hey there! I''m doing great, thank you. What can I help you with
          today? Let''s have a fun chat!'
        updatedAt: '2023-04-14T18:12:55.882Z'
      numEdits: 0
      reactions: []
    id: 643997a768228e8b33446307
    type: comment
  author: nucleardiffusion
  content: 'Also, what do you reccomend for best results in the prompts text file?
    I currently use:


    You are an AI language model designed to assist the Human by answering their questions,
    offering advice, and engaging in casual conversation in a friendly, helpful, and
    informative manner. You respond clearly, coherently, and you consider the conversation
    history.


    ### Human: Hey, how''s it going?


    Assistant: Hey there! I''m doing great, thank you. What can I help you with today?
    Let''s have a fun chat!'
  created_at: 2023-04-14 17:12:55+00:00
  edited: false
  hidden: false
  id: 643997a768228e8b33446307
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
      fullname: Frankie G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spanielrassler
      type: user
    createdAt: '2023-04-14T18:19:12.000Z'
    data:
      edited: true
      editors:
      - spanielrassler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
          fullname: Frankie G
          isHf: false
          isPro: false
          name: spanielrassler
          type: user
        html: '<p>With v 1.0 it''s best to use "### Assistant:"and "### Human:" while
          what you listed in your message is the format for v1.1 of this model. (Correct
          me if I''m wrong!). And yes, it''s working with llama.cpp for me :)</p>

          '
        raw: With v 1.0 it's best to use "### Assistant:"and "### Human:" while what
          you listed in your message is the format for v1.1 of this model. (Correct
          me if I'm wrong!). And yes, it's working with llama.cpp for me :)
        updatedAt: '2023-04-14T19:58:15.716Z'
      numEdits: 1
      reactions: []
    id: 643999201eddadc9404c1cc6
    type: comment
  author: spanielrassler
  content: With v 1.0 it's best to use "### Assistant:"and "### Human:" while what
    you listed in your message is the format for v1.1 of this model. (Correct me if
    I'm wrong!). And yes, it's working with llama.cpp for me :)
  created_at: 2023-04-14 17:19:12+00:00
  edited: true
  hidden: false
  id: 643999201eddadc9404c1cc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-15T02:17:33.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Thank you!!</p>

          '
        raw: Thank you!!
        updatedAt: '2023-04-15T02:17:33.826Z'
      numEdits: 0
      reactions: []
    id: 643a093d5cd6e60267c34384
    type: comment
  author: mancub
  content: Thank you!!
  created_at: 2023-04-15 01:17:33+00:00
  edited: false
  hidden: false
  id: 643a093d5cd6e60267c34384
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-15T14:02:22.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;spanielrassler&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/spanielrassler\"\
          >@<span class=\"underline\">spanielrassler</span></a></span>\n\n\t</span></span>\
          \ Yeah, 1.1 dropped while 1.0 was already in the training and two epochs\
          \ in. Bummer, but the next version will be 1.1.</p>\n"
        raw: '@spanielrassler Yeah, 1.1 dropped while 1.0 was already in the training
          and two epochs in. Bummer, but the next version will be 1.1.'
        updatedAt: '2023-04-15T14:02:22.366Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Chille9
    id: 643aae6e18e9973bc81fcdba
    type: comment
  author: reeducator
  content: '@spanielrassler Yeah, 1.1 dropped while 1.0 was already in the training
    and two epochs in. Bummer, but the next version will be 1.1.'
  created_at: 2023-04-15 13:02:22+00:00
  edited: false
  hidden: false
  id: 643aae6e18e9973bc81fcdba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
      fullname: Dat Boi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nucleardiffusion
      type: user
    createdAt: '2023-04-15T16:20:26.000Z'
    data:
      edited: false
      editors:
      - nucleardiffusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
          fullname: Dat Boi
          isHf: false
          isPro: false
          name: nucleardiffusion
          type: user
        html: '<blockquote>

          <p>With v 1.0 it''s best to use "### Assistant:"and "### Human:" while what
          you listed in your message is the format for v1.1 of this model. (Correct
          me if I''m wrong!). And yes, it''s working with llama.cpp for me :)</p>

          </blockquote>

          <p>That stopped the repeating but it now seems to default to censored answeres
          again? Do you think you might be able to do an uncensored version of " vicuna-13B-1.1-GPTQ-4bit-32g.GGML.bin"
          ?? </p>

          <p><a href="https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g-GGML">https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g-GGML</a></p>

          '
        raw: "> With v 1.0 it's best to use \"### Assistant:\"and \"### Human:\" while\
          \ what you listed in your message is the format for v1.1 of this model.\
          \ (Correct me if I'm wrong!). And yes, it's working with llama.cpp for me\
          \ :)\n\nThat stopped the repeating but it now seems to default to censored\
          \ answeres again? Do you think you might be able to do an uncensored version\
          \ of \" vicuna-13B-1.1-GPTQ-4bit-32g.GGML.bin\" ?? \n\nhttps://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g-GGML"
        updatedAt: '2023-04-15T16:20:26.961Z'
      numEdits: 0
      reactions: []
    id: 643aceca45200ac3e7058380
    type: comment
  author: nucleardiffusion
  content: "> With v 1.0 it's best to use \"### Assistant:\"and \"### Human:\" while\
    \ what you listed in your message is the format for v1.1 of this model. (Correct\
    \ me if I'm wrong!). And yes, it's working with llama.cpp for me :)\n\nThat stopped\
    \ the repeating but it now seems to default to censored answeres again? Do you\
    \ think you might be able to do an uncensored version of \" vicuna-13B-1.1-GPTQ-4bit-32g.GGML.bin\"\
    \ ?? \n\nhttps://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g-GGML"
  created_at: 2023-04-15 15:20:26+00:00
  edited: false
  hidden: false
  id: 643aceca45200ac3e7058380
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-15T19:03:39.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Please don''t focus only on ggml only, some of us still use graphics
          cards for LLM, too. :)</p>

          '
        raw: Please don't focus only on ggml only, some of us still use graphics cards
          for LLM, too. :)
        updatedAt: '2023-04-15T19:03:39.008Z'
      numEdits: 0
      reactions: []
    id: 643af50b7885c858fb95576f
    type: comment
  author: mancub
  content: Please don't focus only on ggml only, some of us still use graphics cards
    for LLM, too. :)
  created_at: 2023-04-15 18:03:39+00:00
  edited: false
  hidden: false
  id: 643af50b7885c858fb95576f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
      fullname: Dat Boi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nucleardiffusion
      type: user
    createdAt: '2023-04-15T19:50:09.000Z'
    data:
      edited: false
      editors:
      - nucleardiffusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4a5bb97af8078b8544444d9ee18a2fe.svg
          fullname: Dat Boi
          isHf: false
          isPro: false
          name: nucleardiffusion
          type: user
        html: '<blockquote>

          <p>Please don''t focus only on ggml only, some of us still use graphics
          cards for LLM, too. :)</p>

          </blockquote>

          <p>yes, sorry and I agree but the large majority of people will be able
          to buy more ram than a 24gb GPU ;)</p>

          '
        raw: '> Please don''t focus only on ggml only, some of us still use graphics
          cards for LLM, too. :)


          yes, sorry and I agree but the large majority of people will be able to
          buy more ram than a 24gb GPU ;)'
        updatedAt: '2023-04-15T19:50:09.380Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Chille9
    id: 643afff1f6ef50c310d8ba06
    type: comment
  author: nucleardiffusion
  content: '> Please don''t focus only on ggml only, some of us still use graphics
    cards for LLM, too. :)


    yes, sorry and I agree but the large majority of people will be able to buy more
    ram than a 24gb GPU ;)'
  created_at: 2023-04-15 18:50:09+00:00
  edited: false
  hidden: false
  id: 643afff1f6ef50c310d8ba06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/157d40743b674a1b3e1b5c2648a2829a.svg
      fullname: Filip N
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chille9
      type: user
    createdAt: '2023-04-16T23:09:31.000Z'
    data:
      edited: false
      editors:
      - Chille9
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/157d40743b674a1b3e1b5c2648a2829a.svg
          fullname: Filip N
          isHf: false
          isPro: false
          name: Chille9
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Please don''t focus only on ggml only, some of us still use graphics
          cards for LLM, too. :)</p>

          </blockquote>

          <p>yes, sorry and I agree but the large majority of people will be able
          to buy more ram than a 24gb GPU ;)</p>

          </blockquote>

          <p>absolutely correct. Good things come to those who wait. Thanks for the
          model btw!</p>

          '
        raw: "> > Please don't focus only on ggml only, some of us still use graphics\
          \ cards for LLM, too. :)\n> \n> yes, sorry and I agree but the large majority\
          \ of people will be able to buy more ram than a 24gb GPU ;)\n\nabsolutely\
          \ correct. Good things come to those who wait. Thanks for the model btw!"
        updatedAt: '2023-04-16T23:09:31.289Z'
      numEdits: 0
      reactions: []
    id: 643c802be3a7bbe2cf3e9c84
    type: comment
  author: Chille9
  content: "> > Please don't focus only on ggml only, some of us still use graphics\
    \ cards for LLM, too. :)\n> \n> yes, sorry and I agree but the large majority\
    \ of people will be able to buy more ram than a 24gb GPU ;)\n\nabsolutely correct.\
    \ Good things come to those who wait. Thanks for the model btw!"
  created_at: 2023-04-16 22:09:31+00:00
  edited: false
  hidden: false
  id: 643c802be3a7bbe2cf3e9c84
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: open
target_branch: null
title: Thank you
