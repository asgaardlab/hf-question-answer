!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hpnyaggerman
conflicting_files: null
created_at: 2023-05-01 12:48:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
      fullname: Ivan Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hpnyaggerman
      type: user
    createdAt: '2023-05-01T13:48:15.000Z'
    data:
      edited: false
      editors:
      - hpnyaggerman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
          fullname: Ivan Ivanov
          isHf: false
          isPro: false
          name: hpnyaggerman
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "D:\oobabooga\text-generation-webui\modules\callbacks.py",
          line 66, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "D:\oobabooga\text-generation-webui\modules\text_generation.py", line 290,
          in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\accelerate\hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\accelerate\hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "D:\oobabooga\text-generation-webui\repositories\GPTQ-for-LLaMa\llama_inference_offload.py",
          line 135, in forward<br>    if idx &lt;= (self.preload - 1):<br>  File "C:\Users\FuckMicrosoftPC.conda\envs\textgen\lib\site-packages\torch\nn\modules\module.py",
          line 1614, in <strong>getattr</strong><br>    raise AttributeError("''{}''
          object has no attribute ''{}''".format(<br>AttributeError: ''Offload_LlamaModel''
          object has no attribute ''preload''</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"D:\\oobabooga\\text-generation-webui\\\
          modules\\callbacks.py\", line 66, in gentask\r\n    ret = self.mfunc(callback=_callback,\
          \ **self.kwargs)\r\n  File \"D:\\oobabooga\\text-generation-webui\\modules\\\
          text_generation.py\", line 290, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return\
          \ func(*args, **kwargs)\r\n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\\
          envs\\textgen\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 1485, in generate\r\n    return self.sample(\r\n  File \"C:\\Users\\\
          FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 2524, in sample\r\n    outputs = self(\r\n\
          \  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\FuckMicrosoftPC\\\
          .conda\\envs\\textgen\\lib\\site-packages\\accelerate\\hooks.py\", line\
          \ 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n \
          \ File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 687, in forward\r\
          \n    outputs = self.model(\r\n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\\
          envs\\textgen\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
          \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
          \ \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args,\
          \ **kwargs)\r\n  File \"D:\\oobabooga\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\llama_inference_offload.py\", line 135, in forward\r\n \
          \   if idx <= (self.preload - 1):\r\n  File \"C:\\Users\\FuckMicrosoftPC\\\
          .conda\\envs\\textgen\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1614, in __getattr__\r\n    raise AttributeError(\"'{}' object has\
          \ no attribute '{}'\".format(\r\nAttributeError: 'Offload_LlamaModel' object\
          \ has no attribute 'preload'"
        updatedAt: '2023-05-01T13:48:15.845Z'
      numEdits: 0
      reactions: []
    id: 644fc31fd43fedb824b99235
    type: comment
  author: hpnyaggerman
  content: "Traceback (most recent call last):\r\n  File \"D:\\oobabooga\\text-generation-webui\\\
    modules\\callbacks.py\", line 66, in gentask\r\n    ret = self.mfunc(callback=_callback,\
    \ **self.kwargs)\r\n  File \"D:\\oobabooga\\text-generation-webui\\modules\\text_generation.py\"\
    , line 290, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
    \n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\\
    torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\\
    site-packages\\transformers\\generation\\utils.py\", line 1485, in generate\r\n\
    \    return self.sample(\r\n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\\
    textgen\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2524,\
    \ in sample\r\n    outputs = self(\r\n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\\
    envs\\textgen\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501,\
    \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\\
    FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\accelerate\\hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\", line 687, in forward\r\n    outputs = self.model(\r\
    \n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\envs\\textgen\\lib\\\
    site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\n    output\
    \ = old_forward(*args, **kwargs)\r\n  File \"D:\\oobabooga\\text-generation-webui\\\
    repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\", line 135, in forward\r\
    \n    if idx <= (self.preload - 1):\r\n  File \"C:\\Users\\FuckMicrosoftPC\\.conda\\\
    envs\\textgen\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1614,\
    \ in __getattr__\r\n    raise AttributeError(\"'{}' object has no attribute '{}'\"\
    .format(\r\nAttributeError: 'Offload_LlamaModel' object has no attribute 'preload'"
  created_at: 2023-05-01 12:48:15+00:00
  edited: false
  hidden: false
  id: 644fc31fd43fedb824b99235
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
      fullname: Ivan Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hpnyaggerman
      type: user
    createdAt: '2023-05-01T13:50:47.000Z'
    data:
      edited: false
      editors:
      - hpnyaggerman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
          fullname: Ivan Ivanov
          isHf: false
          isPro: false
          name: hpnyaggerman
          type: user
        html: '<p>Note: trying to load the GPTQ safetensors model</p>

          '
        raw: 'Note: trying to load the GPTQ safetensors model'
        updatedAt: '2023-05-01T13:50:47.773Z'
      numEdits: 0
      reactions: []
    id: 644fc3b7d5f7dafcfa5fc515
    type: comment
  author: hpnyaggerman
  content: 'Note: trying to load the GPTQ safetensors model'
  created_at: 2023-05-01 12:50:47+00:00
  edited: false
  hidden: false
  id: 644fc3b7d5f7dafcfa5fc515
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-02T11:42:34.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Try the ooba fork of GPTQ. Do not use the new one.</p>

          '
        raw: Try the ooba fork of GPTQ. Do not use the new one.
        updatedAt: '2023-05-02T11:42:34.277Z'
      numEdits: 0
      reactions: []
    id: 6450f72a8f876fbfc5ee095b
    type: comment
  author: autobots
  content: Try the ooba fork of GPTQ. Do not use the new one.
  created_at: 2023-05-02 10:42:34+00:00
  edited: false
  hidden: false
  id: 6450f72a8f876fbfc5ee095b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
      fullname: Ivan Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hpnyaggerman
      type: user
    createdAt: '2023-05-02T16:30:46.000Z'
    data:
      edited: false
      editors:
      - hpnyaggerman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
          fullname: Ivan Ivanov
          isHf: false
          isPro: false
          name: hpnyaggerman
          type: user
        html: '<p>Doesn''t seem to do much, unless I am tarded and using the wrong
          version. Using <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa/">https://github.com/oobabooga/GPTQ-for-LLaMa/</a></p>

          '
        raw: Doesn't seem to do much, unless I am tarded and using the wrong version.
          Using https://github.com/oobabooga/GPTQ-for-LLaMa/
        updatedAt: '2023-05-02T16:30:46.549Z'
      numEdits: 0
      reactions: []
    id: 64513ab641f3c769b90f7c3d
    type: comment
  author: hpnyaggerman
  content: Doesn't seem to do much, unless I am tarded and using the wrong version.
    Using https://github.com/oobabooga/GPTQ-for-LLaMa/
  created_at: 2023-05-02 15:30:46+00:00
  edited: false
  hidden: false
  id: 64513ab641f3c769b90f7c3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-03T12:11:23.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>That is definitely the right one. I only have this set up on linux
          tho.</p>

          '
        raw: That is definitely the right one. I only have this set up on linux tho.
        updatedAt: '2023-05-03T12:11:23.749Z'
      numEdits: 0
      reactions: []
    id: 64524f6bae012a3da9a5a89e
    type: comment
  author: autobots
  content: That is definitely the right one. I only have this set up on linux tho.
  created_at: 2023-05-03 11:11:23+00:00
  edited: false
  hidden: false
  id: 64524f6bae012a3da9a5a89e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-05-06T02:34:48.000Z'
    data:
      edited: true
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>Worked for me after I downloaded the config.json in the other folder</p>

          '
        raw: Worked for me after I downloaded the config.json in the other folder
        updatedAt: '2023-05-06T02:36:19.866Z'
      numEdits: 1
      reactions: []
    id: 6455bcc88606b8832b2b547c
    type: comment
  author: vdruts
  content: Worked for me after I downloaded the config.json in the other folder
  created_at: 2023-05-06 01:34:48+00:00
  edited: true
  hidden: false
  id: 6455bcc88606b8832b2b547c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
      fullname: Ivan Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hpnyaggerman
      type: user
    createdAt: '2023-05-06T07:22:48.000Z'
    data:
      edited: false
      editors:
      - hpnyaggerman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
          fullname: Ivan Ivanov
          isHf: false
          isPro: false
          name: hpnyaggerman
          type: user
        html: '<p>What folder? Where? How?</p>

          '
        raw: What folder? Where? How?
        updatedAt: '2023-05-06T07:22:48.359Z'
      numEdits: 0
      reactions: []
    id: 64560048cd6567f52fae7be9
    type: comment
  author: hpnyaggerman
  content: What folder? Where? How?
  created_at: 2023-05-06 06:22:48+00:00
  edited: false
  hidden: false
  id: 64560048cd6567f52fae7be9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-06T10:40:13.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p><a href="https://huggingface.co/reeducator/vicuna-13b-free/tree/main/hf-output">https://huggingface.co/reeducator/vicuna-13b-free/tree/main/hf-output</a></p>

          <p>Also has FP16 so you can convert it to whatever you want. Like act order
          and no group size.</p>

          '
        raw: 'https://huggingface.co/reeducator/vicuna-13b-free/tree/main/hf-output


          Also has FP16 so you can convert it to whatever you want. Like act order
          and no group size.'
        updatedAt: '2023-05-06T10:40:13.699Z'
      numEdits: 0
      reactions: []
    id: 64562e8dcd6567f52fb1b449
    type: comment
  author: autobots
  content: 'https://huggingface.co/reeducator/vicuna-13b-free/tree/main/hf-output


    Also has FP16 so you can convert it to whatever you want. Like act order and no
    group size.'
  created_at: 2023-05-06 09:40:13+00:00
  edited: false
  hidden: false
  id: 64562e8dcd6567f52fb1b449
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663785040074-noauth.png?w=200&h=200&f=face
      fullname: Glenn Chon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GChon
      type: user
    createdAt: '2023-05-06T15:04:09.000Z'
    data:
      edited: false
      editors:
      - GChon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663785040074-noauth.png?w=200&h=200&f=face
          fullname: Glenn Chon
          isHf: false
          isPro: false
          name: GChon
          type: user
        html: '<p>The issue for me was that I was using an outdated gptq-for-llama
          repo.  I checked the readme and it says to delete that folder before updating.
          </p>

          <p>For anyone that needs instruction:<br>Windows: Simply delete the GPTQ-for-LLaMa
          folder (located at /text-generation-webui/repositories/) then run the update_windows.bat
          if you used the windows version<br>Linux: Delete the same folder as the
          windows one, replace with the newest from <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa.git">https://github.com/oobabooga/GPTQ-for-LLaMa.git</a><br>     Clone
          the repo to your machine in the repositories folder with:</p>

          <p><code>git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda</code></p>

          <p>cd to the newly cloned directory, then </p>

          <p><code>python -m pip install -r requirements.txt</code></p>

          '
        raw: "The issue for me was that I was using an outdated gptq-for-llama repo.\
          \  I checked the readme and it says to delete that folder before updating.\
          \ \n\nFor anyone that needs instruction:\nWindows: Simply delete the GPTQ-for-LLaMa\
          \ folder (located at /text-generation-webui/repositories/) then run the\
          \ update_windows.bat if you used the windows version\nLinux: Delete the\
          \ same folder as the windows one, replace with the newest from https://github.com/oobabooga/GPTQ-for-LLaMa.git\n\
          \     Clone the repo to your machine in the repositories folder with:\n\n\
          ```git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda```\n\
          \ncd to the newly cloned directory, then \n\n```python -m pip install -r\
          \ requirements.txt```"
        updatedAt: '2023-05-06T15:04:09.332Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Squish42
        - oldshooter
    id: 64566c69d10badc9555c55fa
    type: comment
  author: GChon
  content: "The issue for me was that I was using an outdated gptq-for-llama repo.\
    \  I checked the readme and it says to delete that folder before updating. \n\n\
    For anyone that needs instruction:\nWindows: Simply delete the GPTQ-for-LLaMa\
    \ folder (located at /text-generation-webui/repositories/) then run the update_windows.bat\
    \ if you used the windows version\nLinux: Delete the same folder as the windows\
    \ one, replace with the newest from https://github.com/oobabooga/GPTQ-for-LLaMa.git\n\
    \     Clone the repo to your machine in the repositories folder with:\n\n```git\
    \ clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda```\n\ncd to the\
    \ newly cloned directory, then \n\n```python -m pip install -r requirements.txt```"
  created_at: 2023-05-06 14:04:09+00:00
  edited: false
  hidden: false
  id: 64566c69d10badc9555c55fa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: open
target_branch: null
title: 'text-generation-webui: AttributeError: ''Offload_LlamaModel'' object has no
  attribute ''preload'', when trying to generate text'
