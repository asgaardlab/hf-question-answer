!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mancub
conflicting_files: null
created_at: 2023-04-15 02:11:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-15T03:11:58.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>N00b here!</p>

          <p>When I try to use .safetensors model file with oobabooga, it says I''m
          missing config.json, and probably all the other files like tokenizer.model
          that usually come along.</p>

          <p>Can I use these from the older vicuna-13b-4bit-128g model that anon8231489123
          released before, or do I need new ones specifically for this model, and
          where do I get them?</p>

          <p>Thanks.</p>

          '
        raw: "N00b here!\r\n\r\nWhen I try to use .safetensors model file with oobabooga,\
          \ it says I'm missing config.json, and probably all the other files like\
          \ tokenizer.model that usually come along.\r\n\r\nCan I use these from the\
          \ older vicuna-13b-4bit-128g model that anon8231489123 released before,\
          \ or do I need new ones specifically for this model, and where do I get\
          \ them?\r\n\r\nThanks."
        updatedAt: '2023-04-15T03:11:58.296Z'
      numEdits: 0
      reactions: []
    id: 643a15fe0cc94e53f8b884e7
    type: comment
  author: mancub
  content: "N00b here!\r\n\r\nWhen I try to use .safetensors model file with oobabooga,\
    \ it says I'm missing config.json, and probably all the other files like tokenizer.model\
    \ that usually come along.\r\n\r\nCan I use these from the older vicuna-13b-4bit-128g\
    \ model that anon8231489123 released before, or do I need new ones specifically\
    \ for this model, and where do I get them?\r\n\r\nThanks."
  created_at: 2023-04-15 02:11:58+00:00
  edited: false
  hidden: false
  id: 643a15fe0cc94e53f8b884e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af80a40d365c7919ff462bc671398cad.svg
      fullname: Regrant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Regrant-Dev
      type: user
    createdAt: '2023-04-15T03:47:58.000Z'
    data:
      edited: false
      editors:
      - Regrant-Dev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af80a40d365c7919ff462bc671398cad.svg
          fullname: Regrant
          isHf: false
          isPro: false
          name: Regrant-Dev
          type: user
        html: '<p>Yes I had the same issue and it worked putting in those default
          files from the original.</p>

          <p><a href="https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/tree/main">https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/tree/main</a></p>

          <p>I put all these in the same folder with it.<br>config.json<br>generation_config.json<br>pytorch_model.bin.index.json<br>special_tokens_map.json<br>tokenizer.model<br>tokenizer_config.json</p>

          '
        raw: 'Yes I had the same issue and it worked putting in those default files
          from the original.


          https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/tree/main


          I put all these in the same folder with it.

          config.json

          generation_config.json

          pytorch_model.bin.index.json

          special_tokens_map.json

          tokenizer.model

          tokenizer_config.json'
        updatedAt: '2023-04-15T03:47:58.072Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mancub
    id: 643a1e6e5cd6e60267c3c588
    type: comment
  author: Regrant-Dev
  content: 'Yes I had the same issue and it worked putting in those default files
    from the original.


    https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/tree/main


    I put all these in the same folder with it.

    config.json

    generation_config.json

    pytorch_model.bin.index.json

    special_tokens_map.json

    tokenizer.model

    tokenizer_config.json'
  created_at: 2023-04-15 02:47:58+00:00
  edited: false
  hidden: false
  id: 643a1e6e5cd6e60267c3c588
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673012989923-noauth.png?w=200&h=200&f=face
      fullname: Nisnet Sansyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nisnet
      type: user
    createdAt: '2023-04-15T04:45:04.000Z'
    data:
      edited: false
      editors:
      - nisnet
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673012989923-noauth.png?w=200&h=200&f=face
          fullname: Nisnet Sansyn
          isHf: false
          isPro: false
          name: nisnet
          type: user
        html: '<p>I seem to run into the following issue when loading the model using
          the configs from the above repo</p>

          <p>RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:<br>    size
          mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32001,
          5120]) from checkpoint, the shape in current model is torch.Size([32000,
          5120]).<br>    size mismatch for lm_head.weight: copying a param with shape
          torch.Size([32001, 5120]) from checkpoint, the shape in current model is
          torch.Size([32000, 5120]).</p>

          '
        raw: "I seem to run into the following issue when loading the model using\
          \ the configs from the above repo\n\nRuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight:\
          \ copying a param with shape torch.Size([32001, 5120]) from checkpoint,\
          \ the shape in current model is torch.Size([32000, 5120]).\n\tsize mismatch\
          \ for lm_head.weight: copying a param with shape torch.Size([32001, 5120])\
          \ from checkpoint, the shape in current model is torch.Size([32000, 5120])."
        updatedAt: '2023-04-15T04:45:04.261Z'
      numEdits: 0
      reactions: []
    id: 643a2bd06fe78732bdccebd3
    type: comment
  author: nisnet
  content: "I seem to run into the following issue when loading the model using the\
    \ configs from the above repo\n\nRuntimeError: Error(s) in loading state_dict\
    \ for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying\
    \ a param with shape torch.Size([32001, 5120]) from checkpoint, the shape in current\
    \ model is torch.Size([32000, 5120]).\n\tsize mismatch for lm_head.weight: copying\
    \ a param with shape torch.Size([32001, 5120]) from checkpoint, the shape in current\
    \ model is torch.Size([32000, 5120])."
  created_at: 2023-04-15 03:45:04+00:00
  edited: false
  hidden: false
  id: 643a2bd06fe78732bdccebd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673012989923-noauth.png?w=200&h=200&f=face
      fullname: Nisnet Sansyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nisnet
      type: user
    createdAt: '2023-04-15T04:48:38.000Z'
    data:
      edited: true
      editors:
      - nisnet
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673012989923-noauth.png?w=200&h=200&f=face
          fullname: Nisnet Sansyn
          isHf: false
          isPro: false
          name: nisnet
          type: user
        html: '<p>OK, I think I sorted that by just editing config.json and changing
          32000 to 32001</p>

          '
        raw: OK, I think I sorted that by just editing config.json and changing 32000
          to 32001
        updatedAt: '2023-04-15T04:48:46.840Z'
      numEdits: 1
      reactions: []
    id: 643a2ca6b4a30e369ddfa43b
    type: comment
  author: nisnet
  content: OK, I think I sorted that by just editing config.json and changing 32000
    to 32001
  created_at: 2023-04-15 03:48:38+00:00
  edited: true
  hidden: false
  id: 643a2ca6b4a30e369ddfa43b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-15T14:43:08.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Thanks, it worked using files from anon9231489123, but now I''m
          getting some strange initial responses so I''ll open another issue about
          that.</p>

          '
        raw: Thanks, it worked using files from anon9231489123, but now I'm getting
          some strange initial responses so I'll open another issue about that.
        updatedAt: '2023-04-15T14:43:08.596Z'
      numEdits: 0
      reactions: []
    id: 643ab7fc1f162723356ff835
    type: comment
  author: mancub
  content: Thanks, it worked using files from anon9231489123, but now I'm getting
    some strange initial responses so I'll open another issue about that.
  created_at: 2023-04-15 13:43:08+00:00
  edited: false
  hidden: false
  id: 643ab7fc1f162723356ff835
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-19T00:50:27.000Z'
    data:
      status: closed
    id: 643f3ad3f2ed3bc5c0695307
    type: status-change
  author: mancub
  created_at: 2023-04-18 23:50:27+00:00
  id: 643f3ad3f2ed3bc5c0695307
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-04-30T14:40:33.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: '<p>For the new model, you need to change from 32001 to 32000 again
          to make it work.</p>

          '
        raw: For the new model, you need to change from 32001 to 32000 again to make
          it work.
        updatedAt: '2023-04-30T14:40:33.227Z'
      numEdits: 0
      reactions: []
    id: 644e7de1a00f4b11d3934bfe
    type: comment
  author: Kelheor
  content: For the new model, you need to change from 32001 to 32000 again to make
    it work.
  created_at: 2023-04-30 13:40:33+00:00
  edited: false
  hidden: false
  id: 644e7de1a00f4b11d3934bfe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: closed
target_branch: null
title: Run the model with oobabooga/text-generation-webui - missing config.json, etc.
