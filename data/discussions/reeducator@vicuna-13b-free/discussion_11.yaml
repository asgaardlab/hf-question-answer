!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheYuriLover
conflicting_files: null
created_at: 2023-04-23 08:02:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T09:02:33.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Hey!</p>

          <p><a href="https://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered">https://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered</a></p>

          <p>@gozfarb decided to make a V4 version of the unfiltered dataset, I think
          it''s the one that removed almost all the woke in it, you should make a
          new finetune with the 1.1 version in it! :D</p>

          '
        raw: "Hey!\r\n\r\nhttps://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered\r\
          \n\r\n@gozfarb decided to make a V4 version of the unfiltered dataset, I\
          \ think it's the one that removed almost all the woke in it, you should\
          \ make a new finetune with the 1.1 version in it! :D"
        updatedAt: '2023-04-23T09:02:33.378Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - mancub
        - Chille9
        - yahma
    id: 6444f429b272430bdbf12ae8
    type: comment
  author: TheYuriLover
  content: "Hey!\r\n\r\nhttps://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered\r\
    \n\r\n@gozfarb decided to make a V4 version of the unfiltered dataset, I think\
    \ it's the one that removed almost all the woke in it, you should make a new finetune\
    \ with the 1.1 version in it! :D"
  created_at: 2023-04-23 08:02:33+00:00
  edited: false
  hidden: false
  id: 6444f429b272430bdbf12ae8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T09:08:07.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Yep, it''s got all the stuff we found in discussion 7 on the main
          dataset repo.</p>

          <p>Hit me up if there''s anything else you want trimmed or if there are
          any problems with the dataset otherwise.</p>

          '
        raw: 'Yep, it''s got all the stuff we found in discussion 7 on the main dataset
          repo.


          Hit me up if there''s anything else you want trimmed or if there are any
          problems with the dataset otherwise.'
        updatedAt: '2023-04-23T09:08:07.414Z'
      numEdits: 0
      reactions: []
    id: 6444f57753ecc52f50e9151e
    type: comment
  author: deleted
  content: 'Yep, it''s got all the stuff we found in discussion 7 on the main dataset
    repo.


    Hit me up if there''s anything else you want trimmed or if there are any problems
    with the dataset otherwise.'
  created_at: 2023-04-23 08:08:07+00:00
  edited: false
  hidden: false
  id: 6444f57753ecc52f50e9151e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-23T13:27:32.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p>Thanks! I've already updated to 1.1 and queued up the jobs to train\
          \ with the V4 from @gozfarb. It will probably take a few days though before\
          \ the results come, the GPU cluster is quite busy at the moment. But in\
          \ the meantime if there are further updates to the dataset or an official\
          \ version from <span data-props=\"{&quot;user&quot;:&quot;anon8231489123&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/anon8231489123\"\
          >@<span class=\"underline\">anon8231489123</span></a></span>\n\n\t</span></span>,\
          \ I can still swap the dataset before the jobs start. Personally I think\
          \ everyone already did great job preparing the V4, and I couldn't really\
          \ find anything to add myself.</p>\n"
        raw: Thanks! I've already updated to 1.1 and queued up the jobs to train with
          the V4 from @gozfarb. It will probably take a few days though before the
          results come, the GPU cluster is quite busy at the moment. But in the meantime
          if there are further updates to the dataset or an official version from
          @anon8231489123, I can still swap the dataset before the jobs start. Personally
          I think everyone already did great job preparing the V4, and I couldn't
          really find anything to add myself.
        updatedAt: '2023-04-23T13:27:32.608Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - TheYuriLover
        - mancub
        - 9cento
        - Chille9
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheYuriLover
    id: 64453244d1460e859d1a3158
    type: comment
  author: reeducator
  content: Thanks! I've already updated to 1.1 and queued up the jobs to train with
    the V4 from @gozfarb. It will probably take a few days though before the results
    come, the GPU cluster is quite busy at the moment. But in the meantime if there
    are further updates to the dataset or an official version from @anon8231489123,
    I can still swap the dataset before the jobs start. Personally I think everyone
    already did great job preparing the V4, and I couldn't really find anything to
    add myself.
  created_at: 2023-04-23 12:27:32+00:00
  edited: false
  hidden: false
  id: 64453244d1460e859d1a3158
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T13:40:38.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>I know I''m asking a lot but there''s this SuperCOT Lora that is
          giving the model some really high quality outputs, it''s like even better
          than the best finetunes.<br>If there was a finetune of this, I really believe
          we could increase the quality of the llama model from another level, and
          for the moment you seem to be the only dude that can do this task.<br><a
          href="https://huggingface.co/kaiokendev/SuperCOT-LoRA">https://huggingface.co/kaiokendev/SuperCOT-LoRA</a></p>

          <p>Tbh you could even mix those dataset with V4 vicuna and make like the
          ultimate finetune idk lmao</p>

          '
        raw: 'I know I''m asking a lot but there''s this SuperCOT Lora that is giving
          the model some really high quality outputs, it''s like even better than
          the best finetunes.

          If there was a finetune of this, I really believe we could increase the
          quality of the llama model from another level, and for the moment you seem
          to be the only dude that can do this task.

          https://huggingface.co/kaiokendev/SuperCOT-LoRA


          Tbh you could even mix those dataset with V4 vicuna and make like the ultimate
          finetune idk lmao'
        updatedAt: '2023-04-23T13:45:11.042Z'
      numEdits: 1
      reactions: []
    id: 64453556f993c804b0332adc
    type: comment
  author: TheYuriLover
  content: 'I know I''m asking a lot but there''s this SuperCOT Lora that is giving
    the model some really high quality outputs, it''s like even better than the best
    finetunes.

    If there was a finetune of this, I really believe we could increase the quality
    of the llama model from another level, and for the moment you seem to be the only
    dude that can do this task.

    https://huggingface.co/kaiokendev/SuperCOT-LoRA


    Tbh you could even mix those dataset with V4 vicuna and make like the ultimate
    finetune idk lmao'
  created_at: 2023-04-23 12:40:38+00:00
  edited: true
  hidden: false
  id: 64453556f993c804b0332adc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-23T14:02:10.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Could add it on the list :). Maybe if someone would be willing to
          prepare a dataset for it (or do it collaboratively). Yeah, one could combine
          some interesting things and we could see for fun what sort of finetune we
          can put together. Let''s complete the surgery on vicuna first, who knows
          that could then act as a base for improvements.</p>

          '
        raw: Could add it on the list :). Maybe if someone would be willing to prepare
          a dataset for it (or do it collaboratively). Yeah, one could combine some
          interesting things and we could see for fun what sort of finetune we can
          put together. Let's complete the surgery on vicuna first, who knows that
          could then act as a base for improvements.
        updatedAt: '2023-04-23T14:02:10.205Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - TheYuriLover
        - mancub
        - 9cento
        - Chille9
    id: 64453a62f993c804b0338fa8
    type: comment
  author: reeducator
  content: Could add it on the list :). Maybe if someone would be willing to prepare
    a dataset for it (or do it collaboratively). Yeah, one could combine some interesting
    things and we could see for fun what sort of finetune we can put together. Let's
    complete the surgery on vicuna first, who knows that could then act as a base
    for improvements.
  created_at: 2023-04-23 13:02:10+00:00
  edited: false
  hidden: false
  id: 64453a62f993c804b0338fa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T14:07:45.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>That''s a great idea actually, if we went to the conclusion that
          the V4_vicuna model is good enough then we can train on it with more stuff
          in the future, but all of these gotta be on the same instruction format
          though, or else the poor model will be confused... and us too lmao</p>

          '
        raw: That's a great idea actually, if we went to the conclusion that the V4_vicuna
          model is good enough then we can train on it with more stuff in the future,
          but all of these gotta be on the same instruction format though, or else
          the poor model will be confused... and us too lmao
        updatedAt: '2023-04-23T14:07:45.717Z'
      numEdits: 0
      reactions: []
    id: 64453bb153ecc52f50ee4208
    type: comment
  author: TheYuriLover
  content: That's a great idea actually, if we went to the conclusion that the V4_vicuna
    model is good enough then we can train on it with more stuff in the future, but
    all of these gotta be on the same instruction format though, or else the poor
    model will be confused... and us too lmao
  created_at: 2023-04-23 13:07:45+00:00
  edited: false
  hidden: false
  id: 64453bb153ecc52f50ee4208
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T14:14:54.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: "<p>I agree with getting base Vicuna right first. We shouldn't change\
          \ too many variables at once.</p>\n<p>As to cleaning up the datasets, I\
          \ think <span data-props=\"{&quot;user&quot;:&quot;kaiokendev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kaiokendev\">@<span class=\"\
          underline\">kaiokendev</span></a></span>\n\n\t</span></span> might have\
          \ said he did some cleaning to the datasets he linked. If he did, hopefully\
          \ he can share them or clarify that he didn't edit them.</p>\n<p>Assuming\
          \ that (or the raw ones being used), the datasets in question would be pretty\
          \ easy to convert to Vicuna format. I could write a script to do that with\
          \ the caveat that they would all be single question/answer conversations.\
          \ I don't know what Vicuna would do with that or if it would affect output\
          \ quality for longer conversations since I'm not super familiar with how\
          \ much the conversation structure itself matters for weighting the finetune.\
          \ Maybe it's just concerned with lowering weights on duplicate answers in\
          \ the same conversation? Dunno.</p>\n"
        raw: 'I agree with getting base Vicuna right first. We shouldn''t change too
          many variables at once.


          As to cleaning up the datasets, I think @kaiokendev might have said he did
          some cleaning to the datasets he linked. If he did, hopefully he can share
          them or clarify that he didn''t edit them.


          Assuming that (or the raw ones being used), the datasets in question would
          be pretty easy to convert to Vicuna format. I could write a script to do
          that with the caveat that they would all be single question/answer conversations.
          I don''t know what Vicuna would do with that or if it would affect output
          quality for longer conversations since I''m not super familiar with how
          much the conversation structure itself matters for weighting the finetune.
          Maybe it''s just concerned with lowering weights on duplicate answers in
          the same conversation? Dunno.'
        updatedAt: '2023-04-23T14:14:54.487Z'
      numEdits: 0
      reactions: []
    id: 64453d5ed1460e859d1b06c8
    type: comment
  author: deleted
  content: 'I agree with getting base Vicuna right first. We shouldn''t change too
    many variables at once.


    As to cleaning up the datasets, I think @kaiokendev might have said he did some
    cleaning to the datasets he linked. If he did, hopefully he can share them or
    clarify that he didn''t edit them.


    Assuming that (or the raw ones being used), the datasets in question would be
    pretty easy to convert to Vicuna format. I could write a script to do that with
    the caveat that they would all be single question/answer conversations. I don''t
    know what Vicuna would do with that or if it would affect output quality for longer
    conversations since I''m not super familiar with how much the conversation structure
    itself matters for weighting the finetune. Maybe it''s just concerned with lowering
    weights on duplicate answers in the same conversation? Dunno.'
  created_at: 2023-04-23 13:14:54+00:00
  edited: false
  hidden: false
  id: 64453d5ed1460e859d1b06c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T14:23:21.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: "<p>The best advantage of <span data-props=\"{&quot;user&quot;:&quot;kaiokendev&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kaiokendev\"\
          >@<span class=\"underline\">kaiokendev</span></a></span>\n\n\t</span></span>'s\
          \ dataset is that it's totally unwoke, when I tried the SuperCOT Lora I\
          \ had no instance of refusal or moralizing stuff, and that's a good thing!\
          \ And yeah, if he did clean the original dataset to make a big one at the\
          \ end, if he could share it to us that would be cool.</p>\n"
        raw: The best advantage of @kaiokendev's dataset is that it's totally unwoke,
          when I tried the SuperCOT Lora I had no instance of refusal or moralizing
          stuff, and that's a good thing! And yeah, if he did clean the original dataset
          to make a big one at the end, if he could share it to us that would be cool.
        updatedAt: '2023-04-23T14:23:21.368Z'
      numEdits: 0
      reactions: []
    id: 64453f59f993c804b033ee00
    type: comment
  author: TheYuriLover
  content: The best advantage of @kaiokendev's dataset is that it's totally unwoke,
    when I tried the SuperCOT Lora I had no instance of refusal or moralizing stuff,
    and that's a good thing! And yeah, if he did clean the original dataset to make
    a big one at the end, if he could share it to us that would be cool.
  created_at: 2023-04-23 13:23:21+00:00
  edited: false
  hidden: false
  id: 64453f59f993c804b033ee00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-23T14:23:56.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Yep the format is the thing we would have to ensure and most likely
          work on, if we ever went that route. Let''s see how the V4 turns out. If
          at all possible for me, I could then do 30b, and in the meantime some other
          work might take place to prepare the next best thing dataset.</p>

          '
        raw: Yep the format is the thing we would have to ensure and most likely work
          on, if we ever went that route. Let's see how the V4 turns out. If at all
          possible for me, I could then do 30b, and in the meantime some other work
          might take place to prepare the next best thing dataset.
        updatedAt: '2023-04-23T14:23:56.429Z'
      numEdits: 0
      reactions: []
    id: 64453f7cf993c804b033f144
    type: comment
  author: reeducator
  content: Yep the format is the thing we would have to ensure and most likely work
    on, if we ever went that route. Let's see how the V4 turns out. If at all possible
    for me, I could then do 30b, and in the meantime some other work might take place
    to prepare the next best thing dataset.
  created_at: 2023-04-23 13:23:56+00:00
  edited: false
  hidden: false
  id: 64453f7cf993c804b033f144
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-04-23T17:07:27.000Z'
    data:
      edited: true
      editors:
      - kaiokendev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: '<p>The data set I used had partial samples of the datasets listed,
          and ran multiple passes of filters, including manually reviewing the file.
          The current version I have has anything with tweets or hashtags removed
          though, which is different from the uploaded loras. I can upload this later
          today. Then once you are ready you can do whatever</p>

          '
        raw: The data set I used had partial samples of the datasets listed, and ran
          multiple passes of filters, including manually reviewing the file. The current
          version I have has anything with tweets or hashtags removed though, which
          is different from the uploaded loras. I can upload this later today. Then
          once you are ready you can do whatever
        updatedAt: '2023-04-23T17:08:48.220Z'
      numEdits: 1
      reactions: []
    id: 644565cfd1460e859d1e3a7a
    type: comment
  author: kaiokendev
  content: The data set I used had partial samples of the datasets listed, and ran
    multiple passes of filters, including manually reviewing the file. The current
    version I have has anything with tweets or hashtags removed though, which is different
    from the uploaded loras. I can upload this later today. Then once you are ready
    you can do whatever
  created_at: 2023-04-23 16:07:27+00:00
  edited: true
  hidden: false
  id: 644565cfd1460e859d1e3a7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T18:18:10.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Cool I''ll keep an eye out for it. Thanks for the hard work. :D</p>

          '
        raw: Cool I'll keep an eye out for it. Thanks for the hard work. :D
        updatedAt: '2023-04-23T18:18:10.523Z'
      numEdits: 0
      reactions: []
    id: 644576620f2fc80feb28676d
    type: comment
  author: deleted
  content: Cool I'll keep an eye out for it. Thanks for the hard work. :D
  created_at: 2023-04-23 17:18:10+00:00
  edited: false
  hidden: false
  id: 644576620f2fc80feb28676d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-04-23T21:48:06.000Z'
    data:
      edited: true
      editors:
      - kaiokendev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: '<p>The dataset can be found here <a href="https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset">https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset</a><br>I
          have also linked it, along with the original sets, in the model card <a
          href="https://huggingface.co/kaiokendev/SuperCOT-LoRA">https://huggingface.co/models/kaiokendev/SuperCOT-LoRA</a></p>

          '
        raw: 'The dataset can be found here [https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset](https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset)

          I have also linked it, along with the original sets, in the model card [https://huggingface.co/models/kaiokendev/SuperCOT-LoRA](https://huggingface.co/kaiokendev/SuperCOT-LoRA)'
        updatedAt: '2023-04-23T21:49:47.348Z'
      numEdits: 2
      reactions: []
    id: 6445a796d1460e859d22ac83
    type: comment
  author: kaiokendev
  content: 'The dataset can be found here [https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset](https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset)

    I have also linked it, along with the original sets, in the model card [https://huggingface.co/models/kaiokendev/SuperCOT-LoRA](https://huggingface.co/kaiokendev/SuperCOT-LoRA)'
  created_at: 2023-04-23 20:48:06+00:00
  edited: true
  hidden: false
  id: 6445a796d1460e859d22ac83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T21:58:32.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Awesome, grabbing now. I''ll see about getting it into Vicuna format
          here in a bit.</p>

          '
        raw: Awesome, grabbing now. I'll see about getting it into Vicuna format here
          in a bit.
        updatedAt: '2023-04-23T21:58:32.868Z'
      numEdits: 0
      reactions: []
    id: 6445aa080f2fc80feb2bb1af
    type: comment
  author: deleted
  content: Awesome, grabbing now. I'll see about getting it into Vicuna format here
    in a bit.
  created_at: 2023-04-23 20:58:32+00:00
  edited: false
  hidden: false
  id: 6445aa080f2fc80feb2bb1af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T22:04:01.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Surprised this dataset is "only" 200000 lines, that''s 1/10 the
          size of the Vicuna dataset</p>

          '
        raw: Surprised this dataset is "only" 200000 lines, that's 1/10 the size of
          the Vicuna dataset
        updatedAt: '2023-04-23T22:04:01.788Z'
      numEdits: 0
      reactions: []
    id: 6445ab51f993c804b03ba662
    type: comment
  author: TheYuriLover
  content: Surprised this dataset is "only" 200000 lines, that's 1/10 the size of
    the Vicuna dataset
  created_at: 2023-04-23 21:04:01+00:00
  edited: false
  hidden: false
  id: 6445ab51f993c804b03ba662
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T22:38:33.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Done with the conversion. Uploading shortly.</p>

          '
        raw: Done with the conversion. Uploading shortly.
        updatedAt: '2023-04-23T22:38:33.938Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mancub
    id: 6445b369d1460e859d236a52
    type: comment
  author: deleted
  content: Done with the conversion. Uploading shortly.
  created_at: 2023-04-23 21:38:33+00:00
  edited: false
  hidden: false
  id: 6445b369d1460e859d236a52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T22:55:57.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p><a href="https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset">https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset</a></p>

          <p>Vicuna formatted dataset is up.</p>

          <p>Someone feel free to give it a look and make sure I didn''t screw up
          the formatting.</p>

          <p>IDs are randomly generated and not guaranteed not to overlap with the
          original dataset, so if that matters, try to do some sanity checking on
          that. I''d guess they don''t matter though.</p>

          <p>Script used to convert is in the repo.</p>

          '
        raw: 'https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset


          Vicuna formatted dataset is up.


          Someone feel free to give it a look and make sure I didn''t screw up the
          formatting.


          IDs are randomly generated and not guaranteed not to overlap with the original
          dataset, so if that matters, try to do some sanity checking on that. I''d
          guess they don''t matter though.


          Script used to convert is in the repo.'
        updatedAt: '2023-04-23T22:55:57.812Z'
      numEdits: 0
      reactions: []
    id: 6445b77d0f2fc80feb2c8c8f
    type: comment
  author: deleted
  content: 'https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset


    Vicuna formatted dataset is up.


    Someone feel free to give it a look and make sure I didn''t screw up the formatting.


    IDs are randomly generated and not guaranteed not to overlap with the original
    dataset, so if that matters, try to do some sanity checking on that. I''d guess
    they don''t matter though.


    Script used to convert is in the repo.'
  created_at: 2023-04-23 21:55:57+00:00
  edited: false
  hidden: false
  id: 6445b77d0f2fc80feb2c8c8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T23:02:06.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/cWVU4gtzCV6yl2Xyb2xHw.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/cWVU4gtzCV6yl2Xyb2xHw.png"></a><br>Unfortunately
          I don''t know how to download your jsons directly on huggingface, it doesn''t
          have the download buttons like usual</p>

          '
        raw: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/cWVU4gtzCV6yl2Xyb2xHw.png)

          Unfortunately I don''t know how to download your jsons directly on huggingface,
          it doesn''t have the download buttons like usual'
        updatedAt: '2023-04-23T23:02:06.543Z'
      numEdits: 0
      reactions: []
    id: 6445b8eef993c804b03c8488
    type: comment
  author: TheYuriLover
  content: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/cWVU4gtzCV6yl2Xyb2xHw.png)

    Unfortunately I don''t know how to download your jsons directly on huggingface,
    it doesn''t have the download buttons like usual'
  created_at: 2023-04-23 22:02:06+00:00
  edited: false
  hidden: false
  id: 6445b8eef993c804b03c8488
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T23:03:34.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Should just be able to right click "raw" and save as, I''d think.</p>

          '
        raw: Should just be able to right click "raw" and save as, I'd think.
        updatedAt: '2023-04-23T23:03:34.713Z'
      numEdits: 0
      reactions: []
    id: 6445b94653ecc52f50f72a1d
    type: comment
  author: deleted
  content: Should just be able to right click "raw" and save as, I'd think.
  created_at: 2023-04-23 22:03:34+00:00
  edited: false
  hidden: false
  id: 6445b94653ecc52f50f72a1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T23:14:46.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>According to the SuperCOT dataset, it''s a mix between all of these:</p>

          <p><a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT">https://huggingface.co/datasets/QingyiSi/Alpaca-CoT</a><br>Chain
          of thought QED<br>Chain of thought Aqua<br>CodeAlpaca</p>

          <p><a href="https://huggingface.co/datasets/neulab/conala">https://huggingface.co/datasets/neulab/conala</a><br>Code
          snippets</p>

          <p><a href="https://huggingface.co/datasets/yahma/alpaca-cleaned">https://huggingface.co/datasets/yahma/alpaca-cleaned</a><br>Alpaca
          GPT4</p>

          <p>But when I try to find some examples from "Chain of thought Aqua" (Rs.
          5600 is divided into three parts A, B and C...) it doesn''t appear on the
          Combined dataset<br><a href="https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset">https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset</a></p>

          <p>I think we''re missing something, not everything is in there</p>

          '
        raw: 'According to the SuperCOT dataset, it''s a mix between all of these:


          https://huggingface.co/datasets/QingyiSi/Alpaca-CoT

          Chain of thought QED

          Chain of thought Aqua

          CodeAlpaca


          https://huggingface.co/datasets/neulab/conala

          Code snippets


          https://huggingface.co/datasets/yahma/alpaca-cleaned

          Alpaca GPT4


          But when I try to find some examples from "Chain of thought Aqua" (Rs. 5600
          is divided into three parts A, B and C...) it doesn''t appear on the Combined
          dataset

          https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset


          I think we''re missing something, not everything is in there'
        updatedAt: '2023-04-23T23:14:46.057Z'
      numEdits: 0
      reactions: []
    id: 6445bbe6f993c804b03cab30
    type: comment
  author: TheYuriLover
  content: 'According to the SuperCOT dataset, it''s a mix between all of these:


    https://huggingface.co/datasets/QingyiSi/Alpaca-CoT

    Chain of thought QED

    Chain of thought Aqua

    CodeAlpaca


    https://huggingface.co/datasets/neulab/conala

    Code snippets


    https://huggingface.co/datasets/yahma/alpaca-cleaned

    Alpaca GPT4


    But when I try to find some examples from "Chain of thought Aqua" (Rs. 5600 is
    divided into three parts A, B and C...) it doesn''t appear on the Combined dataset

    https://huggingface.co/datasets/gozfarb/SuperCOT-vicuna-dataset


    I think we''re missing something, not everything is in there'
  created_at: 2023-04-23 22:14:46+00:00
  edited: false
  hidden: false
  id: 6445bbe6f993c804b03cab30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T23:16:37.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>You should be comparing it against his uploaded filtered.json, not
          the raw datasets. He said he cut them down.</p>

          <blockquote>

          <p>The data set I used had partial samples of the datasets listed, and ran
          multiple passes of filters, including manually reviewing the file. The current
          version I have has anything with tweets or hashtags removed though, which
          is different from the uploaded loras</p>

          </blockquote>

          '
        raw: 'You should be comparing it against his uploaded filtered.json, not the
          raw datasets. He said he cut them down.


          > The data set I used had partial samples of the datasets listed, and ran
          multiple passes of filters, including manually reviewing the file. The current
          version I have has anything with tweets or hashtags removed though, which
          is different from the uploaded loras'
        updatedAt: '2023-04-23T23:16:37.857Z'
      numEdits: 0
      reactions: []
    id: 6445bc550f2fc80feb2ccf9c
    type: comment
  author: deleted
  content: 'You should be comparing it against his uploaded filtered.json, not the
    raw datasets. He said he cut them down.


    > The data set I used had partial samples of the datasets listed, and ran multiple
    passes of filters, including manually reviewing the file. The current version
    I have has anything with tweets or hashtags removed though, which is different
    from the uploaded loras'
  created_at: 2023-04-23 22:16:37+00:00
  edited: false
  hidden: false
  id: 6445bc550f2fc80feb2ccf9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T23:26:18.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: "<p>I don't know why he would remove something like that </p>\n<p>{\"\
          instruction\": \"Question: Rs. 5600 is divided into three parts A, B and\
          \ C. How much A is more than C if their ratio is 1/7:1/7:1/14?\\nOptions:\\\
          n(A) 300\\n(B) 992\\n(C) 1120\\n(D) 552\\n(E) 312 Let's think first. Some\
          \ random reasoning:\", \"input\": \"\", \"output\": \"1/7:1/7:1/14 = 2:2:1\\\
          n1/5*5600 = 1120\\n2240-1120 = 1120 The final answer: (C).\"}</p>\n<p>Seems\
          \ like a good instruction, maybe <span data-props=\"{&quot;user&quot;:&quot;kaiokendev&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kaiokendev\"\
          >@<span class=\"underline\">kaiokendev</span></a></span>\n\n\t</span></span>\
          \ could give us more insight on his process</p>\n"
        raw: "I don't know why he would remove something like that \n\n{\"instruction\"\
          : \"Question: Rs. 5600 is divided into three parts A, B and C. How much\
          \ A is more than C if their ratio is 1/7:1/7:1/14?\\\\nOptions:\\\\n(A)\
          \ 300\\\\n(B) 992\\\\n(C) 1120\\\\n(D) 552\\\\n(E) 312 Let's think first.\
          \ Some random reasoning:\", \"input\": \"\", \"output\": \"1/7:1/7:1/14\
          \ = 2:2:1\\\\n1/5*5600 = 1120\\\\n2240-1120 = 1120 The final answer: (C).\"\
          }\n\nSeems like a good instruction, maybe @kaiokendev could give us more\
          \ insight on his process"
        updatedAt: '2023-04-23T23:26:18.659Z'
      numEdits: 0
      reactions: []
    id: 6445be9af993c804b03cd00a
    type: comment
  author: TheYuriLover
  content: "I don't know why he would remove something like that \n\n{\"instruction\"\
    : \"Question: Rs. 5600 is divided into three parts A, B and C. How much A is more\
    \ than C if their ratio is 1/7:1/7:1/14?\\\\nOptions:\\\\n(A) 300\\\\n(B) 992\\\
    \\n(C) 1120\\\\n(D) 552\\\\n(E) 312 Let's think first. Some random reasoning:\"\
    , \"input\": \"\", \"output\": \"1/7:1/7:1/14 = 2:2:1\\\\n1/5*5600 = 1120\\\\\
    n2240-1120 = 1120 The final answer: (C).\"}\n\nSeems like a good instruction,\
    \ maybe @kaiokendev could give us more insight on his process"
  created_at: 2023-04-23 22:26:18+00:00
  edited: false
  hidden: false
  id: 6445be9af993c804b03cd00a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-23T23:35:08.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>@gozfarb how did you manage to add the" input" from the dataset
          on your Vicuna format? As it only has the "human" and "gpt" on it.<br>Edit:
          My B I just saw what you did, it''s good!</p>

          '
        raw: '@gozfarb how did you manage to add the" input" from the dataset on your
          Vicuna format? As it only has the "human" and "gpt" on it.

          Edit: My B I just saw what you did, it''s good!'
        updatedAt: '2023-04-23T23:38:13.643Z'
      numEdits: 1
      reactions: []
    id: 6445c0acd1460e859d242d7d
    type: comment
  author: TheYuriLover
  content: '@gozfarb how did you manage to add the" input" from the dataset on your
    Vicuna format? As it only has the "human" and "gpt" on it.

    Edit: My B I just saw what you did, it''s good!'
  created_at: 2023-04-23 22:35:08+00:00
  edited: true
  hidden: false
  id: 6445c0acd1460e859d242d7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-23T23:36:50.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>It just concats the instruction and input:</p>

          <pre><code class="language-js">conversation.<span class="hljs-property">from</span>
          = <span class="hljs-string">"human"</span>;

          conversation.<span class="hljs-property">value</span> = value.<span class="hljs-property">instruction</span>
          + <span class="hljs-string">" "</span> + value.<span class="hljs-property">input</span>;

          </code></pre>

          '
        raw: 'It just concats the instruction and input:


          ```js

          conversation.from = "human";

          conversation.value = value.instruction + " " + value.input;

          ```'
        updatedAt: '2023-04-23T23:36:50.591Z'
      numEdits: 0
      reactions: []
    id: 6445c112d1460e859d2432ac
    type: comment
  author: deleted
  content: 'It just concats the instruction and input:


    ```js

    conversation.from = "human";

    conversation.value = value.instruction + " " + value.input;

    ```'
  created_at: 2023-04-23 22:36:50+00:00
  edited: false
  hidden: false
  id: 6445c112d1460e859d2432ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-04-23T23:52:42.000Z'
    data:
      edited: false
      editors:
      - kaiokendev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: '<p>I only took random portions of the datasets and a few random samplings
          further down. Not much thought went into which portions of the FLAN sets
          to keep because they all follow the same style of reasoning with respect
          to the set. I also didn''t want super short and curt responses like that
          to comprise the majority of the set.</p>

          '
        raw: I only took random portions of the datasets and a few random samplings
          further down. Not much thought went into which portions of the FLAN sets
          to keep because they all follow the same style of reasoning with respect
          to the set. I also didn't want super short and curt responses like that
          to comprise the majority of the set.
        updatedAt: '2023-04-23T23:52:42.911Z'
      numEdits: 0
      reactions: []
    id: 6445c4cad1460e859d246805
    type: comment
  author: kaiokendev
  content: I only took random portions of the datasets and a few random samplings
    further down. Not much thought went into which portions of the FLAN sets to keep
    because they all follow the same style of reasoning with respect to the set. I
    also didn't want super short and curt responses like that to comprise the majority
    of the set.
  created_at: 2023-04-23 22:52:42+00:00
  edited: false
  hidden: false
  id: 6445c4cad1460e859d246805
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-24T00:05:07.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Thanks for your answer, so what should we do? We train Vicuna_V4
          first and then we retrain it with the new format of SuperCOT or we add them
          all on a big dataset and we train the model once?</p>

          '
        raw: Thanks for your answer, so what should we do? We train Vicuna_V4 first
          and then we retrain it with the new format of SuperCOT or we add them all
          on a big dataset and we train the model once?
        updatedAt: '2023-04-24T00:05:07.092Z'
      numEdits: 0
      reactions: []
    id: 6445c7b30f2fc80feb2d7d34
    type: comment
  author: TheYuriLover
  content: Thanks for your answer, so what should we do? We train Vicuna_V4 first
    and then we retrain it with the new format of SuperCOT or we add them all on a
    big dataset and we train the model once?
  created_at: 2023-04-23 23:05:07+00:00
  edited: false
  hidden: false
  id: 6445c7b30f2fc80feb2d7d34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T00:10:51.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: "<p>Obviously all decisions are up to <span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/reeducator\"\
          >@<span class=\"underline\">reeducator</span></a></span>\n\n\t</span></span>\
          \ for that since I don't know the GPU farm access situation there (and am\
          \ not asking him to explain it).</p>\n<p>For my opinion generally, I'm a\
          \ more conservative person with regards to things here. The larger dataset\
          \ with its multi-step conversations are part of what makes Vicuna work well\
          \ (\"Now do it this way\" type follow-ups). Adding in a bunch of single-question\
          \ conversations COULD throw a wrench into how well it works, meaning we'd\
          \ have to cull it and circle back.</p>\n<p>I'm definitely interested in\
          \ getting some more stuff in there (maybe that RP forum dataset people [or\
          \ just kaioken?] are working on), but only if we manage to fix the base\
          \ model to begin with.</p>\n"
        raw: 'Obviously all decisions are up to @reeducator for that since I don''t
          know the GPU farm access situation there (and am not asking him to explain
          it).


          For my opinion generally, I''m a more conservative person with regards to
          things here. The larger dataset with its multi-step conversations are part
          of what makes Vicuna work well ("Now do it this way" type follow-ups). Adding
          in a bunch of single-question conversations COULD throw a wrench into how
          well it works, meaning we''d have to cull it and circle back.


          I''m definitely interested in getting some more stuff in there (maybe that
          RP forum dataset people [or just kaioken?] are working on), but only if
          we manage to fix the base model to begin with.'
        updatedAt: '2023-04-24T00:20:49.089Z'
      numEdits: 2
      reactions: []
    id: 6445c90bf993c804b03d8861
    type: comment
  author: deleted
  content: 'Obviously all decisions are up to @reeducator for that since I don''t
    know the GPU farm access situation there (and am not asking him to explain it).


    For my opinion generally, I''m a more conservative person with regards to things
    here. The larger dataset with its multi-step conversations are part of what makes
    Vicuna work well ("Now do it this way" type follow-ups). Adding in a bunch of
    single-question conversations COULD throw a wrench into how well it works, meaning
    we''d have to cull it and circle back.


    I''m definitely interested in getting some more stuff in there (maybe that RP
    forum dataset people [or just kaioken?] are working on), but only if we manage
    to fix the base model to begin with.'
  created_at: 2023-04-23 23:10:51+00:00
  edited: true
  hidden: false
  id: 6445c90bf993c804b03d8861
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10cbdb733c47e9feb543b2e800485fe4.svg
      fullname: bog
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gobbob
      type: user
    createdAt: '2023-04-24T01:50:12.000Z'
    data:
      edited: true
      editors:
      - gobbob
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10cbdb733c47e9feb543b2e800485fe4.svg
          fullname: bog
          isHf: false
          isPro: false
          name: gobbob
          type: user
        html: '<p>I was looking at the V4 dataset and found that there are:</p>

          <p>A total of 70516 conversations</p>

          <p>775 conversations with no lines.<br>673 conversations with one line (34
          of them being only system messages)<br>16279 conversations with two lines
          (5 of them with a system message and one other line)<br>3713 conversations
          with three lines (89 of them with a system message and two other lines)</p>

          <p>Should probably remove the system messages (they are all bad), and certainly
          the empty convos, and probably could add, or swap in,  some COT examples
          given how many two line convos there are in the V4 dataset already.<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6445d7defc22e309d78b2c88/5fVXTKjQnKAeVz26hGrVS.png"><img
          alt="Figure_2.png" src="https://cdn-uploads.huggingface.co/production/uploads/6445d7defc22e309d78b2c88/5fVXTKjQnKAeVz26hGrVS.png"></a></p>

          '
        raw: 'I was looking at the V4 dataset and found that there are:


          A total of 70516 conversations


          775 conversations with no lines.

          673 conversations with one line (34 of them being only system messages)

          16279 conversations with two lines (5 of them with a system message and
          one other line)

          3713 conversations with three lines (89 of them with a system message and
          two other lines)


          Should probably remove the system messages (they are all bad), and certainly
          the empty convos, and probably could add, or swap in,  some COT examples
          given how many two line convos there are in the V4 dataset already.

          ![Figure_2.png](https://cdn-uploads.huggingface.co/production/uploads/6445d7defc22e309d78b2c88/5fVXTKjQnKAeVz26hGrVS.png)'
        updatedAt: '2023-04-24T02:07:32.878Z'
      numEdits: 3
      reactions: []
    id: 6445e054d1460e859d26ec0e
    type: comment
  author: gobbob
  content: 'I was looking at the V4 dataset and found that there are:


    A total of 70516 conversations


    775 conversations with no lines.

    673 conversations with one line (34 of them being only system messages)

    16279 conversations with two lines (5 of them with a system message and one other
    line)

    3713 conversations with three lines (89 of them with a system message and two
    other lines)


    Should probably remove the system messages (they are all bad), and certainly the
    empty convos, and probably could add, or swap in,  some COT examples given how
    many two line convos there are in the V4 dataset already.

    ![Figure_2.png](https://cdn-uploads.huggingface.co/production/uploads/6445d7defc22e309d78b2c88/5fVXTKjQnKAeVz26hGrVS.png)'
  created_at: 2023-04-24 00:50:12+00:00
  edited: true
  hidden: false
  id: 6445e054d1460e859d26ec0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T02:11:32.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Do you have the text for the system messages? I thought I had removed
          those but I guess I skipped over them when prepping the list. I''d hope
          they don''t affect anything, but I''ll still yoink ''em.</p>

          '
        raw: Do you have the text for the system messages? I thought I had removed
          those but I guess I skipped over them when prepping the list. I'd hope they
          don't affect anything, but I'll still yoink 'em.
        updatedAt: '2023-04-24T02:11:32.618Z'
      numEdits: 0
      reactions: []
    id: 6445e5540f2fc80feb301137
    type: comment
  author: deleted
  content: Do you have the text for the system messages? I thought I had removed those
    but I guess I skipped over them when prepping the list. I'd hope they don't affect
    anything, but I'll still yoink 'em.
  created_at: 2023-04-24 01:11:32+00:00
  edited: false
  hidden: false
  id: 6445e5540f2fc80feb301137
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10cbdb733c47e9feb543b2e800485fe4.svg
      fullname: bog
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gobbob
      type: user
    createdAt: '2023-04-24T02:15:58.000Z'
    data:
      edited: false
      editors:
      - gobbob
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10cbdb733c47e9feb543b2e800485fe4.svg
          fullname: bog
          isHf: false
          isPro: false
          name: gobbob
          type: user
        html: '<p>They start with either "*This chat conversation is shared from"
          or "*This conversation is shared from".</p>

          '
        raw: They start with either "*This chat conversation is shared from" or "*This
          conversation is shared from".
        updatedAt: '2023-04-24T02:15:58.623Z'
      numEdits: 0
      reactions: []
    id: 6445e65e0f2fc80feb3022fe
    type: comment
  author: gobbob
  content: They start with either "*This chat conversation is shared from" or "*This
    conversation is shared from".
  created_at: 2023-04-24 01:15:58+00:00
  edited: false
  hidden: false
  id: 6445e65e0f2fc80feb3022fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T02:42:47.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: "<p>Okay! System messages removed.</p>\n<p>I didn't bump the version\
          \ to 4.1 or anything since I assume not many people have touched the dataset\
          \ and it was only 186 entries.</p>\n<p>Pinging <span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/reeducator\"\
          >@<span class=\"underline\">reeducator</span></a></span>\n\n\t</span></span>\
          \ to regrab the dataset.</p>\n<p><a href=\"https://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered\"\
          >https://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered</a></p>\n"
        raw: 'Okay! System messages removed.


          I didn''t bump the version to 4.1 or anything since I assume not many people
          have touched the dataset and it was only 186 entries.


          Pinging @reeducator to regrab the dataset.


          https://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered'
        updatedAt: '2023-04-24T02:43:52.675Z'
      numEdits: 1
      reactions: []
    id: 6445eca7f993c804b0408cb9
    type: comment
  author: deleted
  content: 'Okay! System messages removed.


    I didn''t bump the version to 4.1 or anything since I assume not many people have
    touched the dataset and it was only 186 entries.


    Pinging @reeducator to regrab the dataset.


    https://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered'
  created_at: 2023-04-24 01:42:47+00:00
  edited: true
  hidden: false
  id: 6445eca7f993c804b0408cb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T03:09:44.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Alright, I also went ahead and removed the empty messages as a separate.</p>

          <p>If you want the version with the empty messages, it''s the previous commit,
          just got back one commit. I assume they wouldn''t have been a problem, but
          I couldn''t think of a good reason to leave them in.</p>

          <p>My count was only ~130 though, not 770. The script was originally anon''s,
          but the code looks like it just checks the <code>conversation.value</code>
          json so it should grab them all. I guess they could contain whitespace or
          newlines?</p>

          '
        raw: 'Alright, I also went ahead and removed the empty messages as a separate.


          If you want the version with the empty messages, it''s the previous commit,
          just got back one commit. I assume they wouldn''t have been a problem, but
          I couldn''t think of a good reason to leave them in.


          My count was only ~130 though, not 770. The script was originally anon''s,
          but the code looks like it just checks the `conversation.value` json so
          it should grab them all. I guess they could contain whitespace or newlines?'
        updatedAt: '2023-04-24T03:09:44.085Z'
      numEdits: 0
      reactions: []
    id: 6445f2f8d1460e859d284449
    type: comment
  author: deleted
  content: 'Alright, I also went ahead and removed the empty messages as a separate.


    If you want the version with the empty messages, it''s the previous commit, just
    got back one commit. I assume they wouldn''t have been a problem, but I couldn''t
    think of a good reason to leave them in.


    My count was only ~130 though, not 770. The script was originally anon''s, but
    the code looks like it just checks the `conversation.value` json so it should
    grab them all. I guess they could contain whitespace or newlines?'
  created_at: 2023-04-24 02:09:44+00:00
  edited: false
  hidden: false
  id: 6445f2f8d1460e859d284449
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-24T13:03:17.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I replaced the dataset with this update. Might also be good to report
          the findings on the dataset discussion thread. I agree that in the beginning
          it would be good to only include the reduced vicuna dataset in order to
          keep any sources of issues constrained. However, if the output of V4 model
          looks good, I would say that we could go ahead making variations by including
          other types of datasets and see what works and what doesn''t. Of course,
          it doesn''t necessarily even have to be based on vicuna, might look into
          something entirely different as well.</p>

          '
        raw: I replaced the dataset with this update. Might also be good to report
          the findings on the dataset discussion thread. I agree that in the beginning
          it would be good to only include the reduced vicuna dataset in order to
          keep any sources of issues constrained. However, if the output of V4 model
          looks good, I would say that we could go ahead making variations by including
          other types of datasets and see what works and what doesn't. Of course,
          it doesn't necessarily even have to be based on vicuna, might look into
          something entirely different as well.
        updatedAt: '2023-04-24T13:03:17.556Z'
      numEdits: 0
      reactions: []
    id: 64467e15a808163cefffec3d
    type: comment
  author: reeducator
  content: I replaced the dataset with this update. Might also be good to report the
    findings on the dataset discussion thread. I agree that in the beginning it would
    be good to only include the reduced vicuna dataset in order to keep any sources
    of issues constrained. However, if the output of V4 model looks good, I would
    say that we could go ahead making variations by including other types of datasets
    and see what works and what doesn't. Of course, it doesn't necessarily even have
    to be based on vicuna, might look into something entirely different as well.
  created_at: 2023-04-24 12:03:17+00:00
  edited: false
  hidden: false
  id: 64467e15a808163cefffec3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T13:19:06.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I mentioned my changes on the dataset thread and also threw them
          in the dataset README.</p>

          <p>And yeah, the value in Vicuna could just be the training structure more
          than anything. A chatbot trained against RP or otherwise conversational
          datasets could be really good if the FastChat training method is the secret
          sauce (which seems possible).</p>

          <p>Plus, LoRAs built over a baseline Vicuna Free could be very powerful
          as well and provide good directions for future finetunes with shorter turnaround
          times.</p>

          '
        raw: 'I mentioned my changes on the dataset thread and also threw them in
          the dataset README.


          And yeah, the value in Vicuna could just be the training structure more
          than anything. A chatbot trained against RP or otherwise conversational
          datasets could be really good if the FastChat training method is the secret
          sauce (which seems possible).


          Plus, LoRAs built over a baseline Vicuna Free could be very powerful as
          well and provide good directions for future finetunes with shorter turnaround
          times.'
        updatedAt: '2023-04-24T13:19:06.125Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - reeducator
    id: 644681ca269f503376bea682
    type: comment
  author: deleted
  content: 'I mentioned my changes on the dataset thread and also threw them in the
    dataset README.


    And yeah, the value in Vicuna could just be the training structure more than anything.
    A chatbot trained against RP or otherwise conversational datasets could be really
    good if the FastChat training method is the secret sauce (which seems possible).


    Plus, LoRAs built over a baseline Vicuna Free could be very powerful as well and
    provide good directions for future finetunes with shorter turnaround times.'
  created_at: 2023-04-24 12:19:06+00:00
  edited: false
  hidden: false
  id: 644681ca269f503376bea682
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-04-24T20:16:23.000Z'
    data:
      edited: false
      editors:
      - kaiokendev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: "<p>That reminds me, <span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/reeducator\"\
          >@<span class=\"underline\">reeducator</span></a></span>\n\n\t</span></span>\
          \ what code are you using to train? The roleplay dataset will also need\
          \ to use flash attention due to the massive size of some of the conversations,\
          \ I would like to train some short epochs on 8xA100s if I had the code for\
          \ it.</p>\n"
        raw: That reminds me, @reeducator what code are you using to train? The roleplay
          dataset will also need to use flash attention due to the massive size of
          some of the conversations, I would like to train some short epochs on 8xA100s
          if I had the code for it.
        updatedAt: '2023-04-24T20:16:23.578Z'
      numEdits: 0
      reactions: []
    id: 6446e397c50af8500018f45c
    type: comment
  author: kaiokendev
  content: That reminds me, @reeducator what code are you using to train? The roleplay
    dataset will also need to use flash attention due to the massive size of some
    of the conversations, I would like to train some short epochs on 8xA100s if I
    had the code for it.
  created_at: 2023-04-24 19:16:23+00:00
  edited: false
  hidden: false
  id: 6446e397c50af8500018f45c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-24T21:01:00.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kaiokendev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kaiokendev\">@<span class=\"\
          underline\">kaiokendev</span></a></span>\n\n\t</span></span> I've been using\
          \ FastChat. Flash attention is optionally supported.</p>\n<p>The V4 training\
          \ started but unfortunately failed immediately possibly due to some problem\
          \ in the dataset. I just checked the logs and the message sort of implies\
          \ that there might be some issue with it. I wonder if there is some way\
          \ to quickly validate it without having to queue up again for a new run,\
          \ maybe something bundled with the hf-transformers?</p>\n<pre><code class=\"\
          language-Traceback\">  File \"./FastChat/fastchat/train/train.py\", line\
          \ 250, in &lt;module&gt;\n    train()\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 244, in train\n    trainer.train()\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1652, in train\n    return inner_training_loop(\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1889, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 628, in __next__\n    data = self._next_data()\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 671, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in &lt;listcomp&gt;\n    data = [self.dataset[idx] for idx in\
          \ possibly_batched_index]\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 194, in __getitem__\n    data_dict = preprocess([e[\"conversations\"\
          ] for e in sources], self.tokenizer)\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 85, in preprocess\n    if roles[source[0][\"from\"]] != conv.roles[0]:\n\
          IndexError: list index out of range\n</code></pre>\n"
        raw: "@kaiokendev I've been using FastChat. Flash attention is optionally\
          \ supported.\n\nThe V4 training started but unfortunately failed immediately\
          \ possibly due to some problem in the dataset. I just checked the logs and\
          \ the message sort of implies that there might be some issue with it. I\
          \ wonder if there is some way to quickly validate it without having to queue\
          \ up again for a new run, maybe something bundled with the hf-transformers?\n\
          ```Traceback (most recent call last):\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 250, in <module>\n    train()\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 244, in train\n    trainer.train()\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1652, in train\n    return inner_training_loop(\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1889, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 628, in __next__\n    data = self._next_data()\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 671, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
          \  File \"./FastChat/fastchat/train/train.py\", line 194, in __getitem__\n\
          \    data_dict = preprocess([e[\"conversations\"] for e in sources], self.tokenizer)\n\
          \  File \"./FastChat/fastchat/train/train.py\", line 85, in preprocess\n\
          \    if roles[source[0][\"from\"]] != conv.roles[0]:\nIndexError: list index\
          \ out of range\n```"
        updatedAt: '2023-04-24T21:01:00.552Z'
      numEdits: 0
      reactions: []
    id: 6446ee0cf9dc06bea2a9a85f
    type: comment
  author: reeducator
  content: "@kaiokendev I've been using FastChat. Flash attention is optionally supported.\n\
    \nThe V4 training started but unfortunately failed immediately possibly due to\
    \ some problem in the dataset. I just checked the logs and the message sort of\
    \ implies that there might be some issue with it. I wonder if there is some way\
    \ to quickly validate it without having to queue up again for a new run, maybe\
    \ something bundled with the hf-transformers?\n```Traceback (most recent call\
    \ last):\n  File \"./FastChat/fastchat/train/train.py\", line 250, in <module>\n\
    \    train()\n  File \"./FastChat/fastchat/train/train.py\", line 244, in train\n\
    \    trainer.train()\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1652, in train\n    return inner_training_loop(\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1889, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
    \  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
    , line 628, in __next__\n    data = self._next_data()\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
    , line 671, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may\
    \ raise StopIteration\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
    , line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
    \  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
    , line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
    \  File \"./FastChat/fastchat/train/train.py\", line 194, in __getitem__\n   \
    \ data_dict = preprocess([e[\"conversations\"] for e in sources], self.tokenizer)\n\
    \  File \"./FastChat/fastchat/train/train.py\", line 85, in preprocess\n    if\
    \ roles[source[0][\"from\"]] != conv.roles[0]:\nIndexError: list index out of\
    \ range\n```"
  created_at: 2023-04-24 20:01:00+00:00
  edited: false
  hidden: false
  id: 6446ee0cf9dc06bea2a9a85f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T21:04:43.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Index out of range is an odd one. Lemme run a validator against
          the V4 real quick. Can you try it against the version before I removed the
          empty conversations?</p>

          '
        raw: Index out of range is an odd one. Lemme run a validator against the V4
          real quick. Can you try it against the version before I removed the empty
          conversations?
        updatedAt: '2023-04-24T21:04:43.638Z'
      numEdits: 0
      reactions: []
    id: 6446eeeb1dc59ca8aed36bf3
    type: comment
  author: deleted
  content: Index out of range is an odd one. Lemme run a validator against the V4
    real quick. Can you try it against the version before I removed the empty conversations?
  created_at: 2023-04-24 20:04:43+00:00
  edited: false
  hidden: false
  id: 6446eeeb1dc59ca8aed36bf3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T21:15:27.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I''m getting no validation errors against the json file itself.</p>

          <p>Based on the error either source[0] or conv.roles[0] doesn''t exist.
          I''ll check for empty conversations, which could be the issue. Looks like
          there are some in there. It''s possible that''s because of the naive python
          trimming script. Try to get something together to gut those out and push
          a change.</p>

          <p>(I know anon made some changes to his prune script, but never shared
          them, so apologies here.)</p>

          '
        raw: 'I''m getting no validation errors against the json file itself.


          Based on the error either source[0] or conv.roles[0] doesn''t exist. I''ll
          check for empty conversations, which could be the issue. Looks like there
          are some in there. It''s possible that''s because of the naive python trimming
          script. Try to get something together to gut those out and push a change.


          (I know anon made some changes to his prune script, but never shared them,
          so apologies here.)'
        updatedAt: '2023-04-24T21:18:14.666Z'
      numEdits: 1
      reactions: []
    id: 6446f16f1dc59ca8aed3905a
    type: comment
  author: deleted
  content: 'I''m getting no validation errors against the json file itself.


    Based on the error either source[0] or conv.roles[0] doesn''t exist. I''ll check
    for empty conversations, which could be the issue. Looks like there are some in
    there. It''s possible that''s because of the naive python trimming script. Try
    to get something together to gut those out and push a change.


    (I know anon made some changes to his prune script, but never shared them, so
    apologies here.)'
  created_at: 2023-04-24 20:15:27+00:00
  edited: true
  hidden: false
  id: 6446f16f1dc59ca8aed3905a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-24T21:15:32.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I will try on a test partition with 7B, that might be quicker. Memory
          is a problem while running tests, test runs don''t get too much of it. Inconveniently
          it might also take some hours again to get started again since I lost my
          time slot.</p>

          '
        raw: I will try on a test partition with 7B, that might be quicker. Memory
          is a problem while running tests, test runs don't get too much of it. Inconveniently
          it might also take some hours again to get started again since I lost my
          time slot.
        updatedAt: '2023-04-24T21:15:32.144Z'
      numEdits: 0
      reactions: []
    id: 6446f1741dc59ca8aed390ab
    type: comment
  author: reeducator
  content: I will try on a test partition with 7B, that might be quicker. Memory is
    a problem while running tests, test runs don't get too much of it. Inconveniently
    it might also take some hours again to get started again since I lost my time
    slot.
  created_at: 2023-04-24 20:15:32+00:00
  edited: false
  hidden: false
  id: 6446f1741dc59ca8aed390ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T21:21:32.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: "<p>Sorry about that. gobbob mentioned the empty conversations, but\
          \ I assumed they were in for the first training pass. I might have expected\
          \ too much of research code error checking (looking at you FastChat).</p>\n\
          <p>I've pruned out the empty conversations using:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">if</span>(<span class=\"\
          hljs-built_in\">len</span>(conv[<span class=\"hljs-string\">\"conversations\"\
          </span>]) == <span class=\"hljs-number\">0</span>):\n        <span class=\"\
          hljs-keyword\">return</span> <span class=\"hljs-literal\">True</span>\n\
          </code></pre>\n<p>Hopefully that does it. Pushing now.</p>\n"
        raw: "Sorry about that. gobbob mentioned the empty conversations, but I assumed\
          \ they were in for the first training pass. I might have expected too much\
          \ of research code error checking (looking at you FastChat).\n\nI've pruned\
          \ out the empty conversations using:\n\n```python\nif(len(conv[\"conversations\"\
          ]) == 0):\n        return True\n```\n\nHopefully that does it. Pushing now."
        updatedAt: '2023-04-24T21:21:32.644Z'
      numEdits: 0
      reactions: []
    id: 6446f2dc1dc59ca8aed3a432
    type: comment
  author: deleted
  content: "Sorry about that. gobbob mentioned the empty conversations, but I assumed\
    \ they were in for the first training pass. I might have expected too much of\
    \ research code error checking (looking at you FastChat).\n\nI've pruned out the\
    \ empty conversations using:\n\n```python\nif(len(conv[\"conversations\"]) ==\
    \ 0):\n        return True\n```\n\nHopefully that does it. Pushing now."
  created_at: 2023-04-24 20:21:32+00:00
  edited: false
  hidden: false
  id: 6446f2dc1dc59ca8aed3a432
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T21:28:24.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Pushed. Version bumped to 4.1.</p>

          <p>I''ll continue to minor bump the version so there''s no confusion in
          case we run up against anything else minor like this.</p>

          '
        raw: 'Pushed. Version bumped to 4.1.


          I''ll continue to minor bump the version so there''s no confusion in case
          we run up against anything else minor like this.'
        updatedAt: '2023-04-24T21:28:24.232Z'
      numEdits: 0
      reactions: []
    id: 6446f478f9dc06bea2aa062a
    type: comment
  author: deleted
  content: 'Pushed. Version bumped to 4.1.


    I''ll continue to minor bump the version so there''s no confusion in case we run
    up against anything else minor like this.'
  created_at: 2023-04-24 20:28:24+00:00
  edited: false
  hidden: false
  id: 6446f478f9dc06bea2aa062a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-24T21:35:18.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>@gozfarb you should put the older versions in a "archive" folder
          in case we messed something up along the way</p>

          '
        raw: '@gozfarb you should put the older versions in a "archive" folder in
          case we messed something up along the way'
        updatedAt: '2023-04-24T21:35:18.868Z'
      numEdits: 0
      reactions: []
    id: 6446f616c50af850001a09bf
    type: comment
  author: TheYuriLover
  content: '@gozfarb you should put the older versions in a "archive" folder in case
    we messed something up along the way'
  created_at: 2023-04-24 20:35:18+00:00
  edited: false
  hidden: false
  id: 6446f616c50af850001a09bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-24T21:36:22.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>No worries. Will test again and report when something happens.</p>

          '
        raw: No worries. Will test again and report when something happens.
        updatedAt: '2023-04-24T21:36:22.766Z'
      numEdits: 0
      reactions: []
    id: 6446f6561dc59ca8aed3d89e
    type: comment
  author: reeducator
  content: No worries. Will test again and report when something happens.
  created_at: 2023-04-24 20:36:22+00:00
  edited: false
  hidden: false
  id: 6446f6561dc59ca8aed3d89e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-24T21:41:03.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<blockquote>

          <p>@gozfarb you should put the older versions in a "archive" folder in case
          we messed something up along the way</p>

          </blockquote>

          <p>You can just select the commit where they changed and download the old
          version via that I think. They''re not LFS so the changes should be in the
          commits. </p>

          <p>You can grab the old versions with <code>git checkout &lt;commit_hash_code&gt;</code>
          via command line as well if you want to test it out and make sure it works.
          If it doesn''t for some reason, let me know and I''ll add .zip files with
          old versions from now on.</p>

          '
        raw: "> @gozfarb you should put the older versions in a \"archive\" folder\
          \ in case we messed something up along the way\n\nYou can just select the\
          \ commit where they changed and download the old version via that I think.\
          \ They're not LFS so the changes should be in the commits. \n\nYou can grab\
          \ the old versions with `git checkout <commit_hash_code>` via command line\
          \ as well if you want to test it out and make sure it works. If it doesn't\
          \ for some reason, let me know and I'll add .zip files with old versions\
          \ from now on."
        updatedAt: '2023-04-24T21:42:02.867Z'
      numEdits: 1
      reactions: []
    id: 6446f76f1dc59ca8aed3e77d
    type: comment
  author: deleted
  content: "> @gozfarb you should put the older versions in a \"archive\" folder in\
    \ case we messed something up along the way\n\nYou can just select the commit\
    \ where they changed and download the old version via that I think. They're not\
    \ LFS so the changes should be in the commits. \n\nYou can grab the old versions\
    \ with `git checkout <commit_hash_code>` via command line as well if you want\
    \ to test it out and make sure it works. If it doesn't for some reason, let me\
    \ know and I'll add .zip files with old versions from now on."
  created_at: 2023-04-24 20:41:03+00:00
  edited: true
  hidden: false
  id: 6446f76f1dc59ca8aed3e77d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7d7ea350eeed2f3f0dcbcc5122690087.svg
      fullname: Jason Tomlinson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Latent-Dreamscape
      type: user
    createdAt: '2023-04-24T23:10:59.000Z'
    data:
      edited: false
      editors:
      - Latent-Dreamscape
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7d7ea350eeed2f3f0dcbcc5122690087.svg
          fullname: Jason Tomlinson
          isHf: false
          isPro: false
          name: Latent-Dreamscape
          type: user
        html: "<p>Finally got around to getting webui running and been fiddling around\
          \ with Llama for the last few days - super excited to get Vicuna going and\
          \ even more so for what you're doing here. </p>\n<p>Couldn't continue without\
          \ coming here and giving a big hearty Thank-You!! to you <span data-props=\"\
          {&quot;user&quot;:&quot;reeducator&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/reeducator\">@<span class=\"underline\"\
          >reeducator</span></a></span>\n\n\t</span></span>, and for all of you who\
          \ are helping and working to bring all of this together for us. :)</p>\n"
        raw: "Finally got around to getting webui running and been fiddling around\
          \ with Llama for the last few days - super excited to get Vicuna going and\
          \ even more so for what you're doing here. \n\nCouldn't continue without\
          \ coming here and giving a big hearty Thank-You!! to you @reeducator, and\
          \ for all of you who are helping and working to bring all of this together\
          \ for us. :)"
        updatedAt: '2023-04-24T23:10:59.559Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - Sergei6000
        - kaiokendev
        - reeducator
        - noirehawk
    id: 64470c83c50af850001b5cdd
    type: comment
  author: Latent-Dreamscape
  content: "Finally got around to getting webui running and been fiddling around with\
    \ Llama for the last few days - super excited to get Vicuna going and even more\
    \ so for what you're doing here. \n\nCouldn't continue without coming here and\
    \ giving a big hearty Thank-You!! to you @reeducator, and for all of you who are\
    \ helping and working to bring all of this together for us. :)"
  created_at: 2023-04-24 22:10:59+00:00
  edited: false
  hidden: false
  id: 64470c83c50af850001b5cdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-26T09:16:58.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p>@gozfarb's fix took care of the empty conversations problem. This\
          \ time, the training ran for sometime, but regrettably, another dataset\
          \ problem occurred. I have time to investigate more later, but it would\
          \ seem that this might be related to what <span data-props=\"{&quot;user&quot;:&quot;desperadoola&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/desperadoola\"\
          >@<span class=\"underline\">desperadoola</span></a></span>\n\n\t</span></span>\
          \ reported here <a href=\"https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/discussions/15\"\
          >https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/discussions/15</a></p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 250, in &lt;module&gt;\n    train()\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 244, in train\n    trainer.train()\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1652, in train\n    return inner_training_loop(\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1889, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 628, in __next__\n    data = self._next_data()\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 671, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in &lt;listcomp&gt;\n    data = [self.dataset[idx] for idx in\
          \ possibly_batched_index]\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 194, in __getitem__\n    data_dict = preprocess([e[\"conversations\"\
          ] for e in sources], self.tokenizer)\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 92, in preprocess\n    assert role == conv.roles[j % 2], f\"{i}\"\
          \nAssertionError: 0\n</code></pre>\n<p>Also, I'm seeing a lot of <code>Token\
          \ indices sequence length is longer than the specified maximum sequence\
          \ length for this model (2573 &gt; 2048). Running this sequence through\
          \ the model will result in indexing errors</code> (a message from hf-transformers).\
          \ Didn't have these during the V3 training. It hasn't resulted in any errors\
          \ during the training, but might be a problem at some point. Another thing\
          \ that might be good to check out...</p>\n"
        raw: "@gozfarb's fix took care of the empty conversations problem. This time,\
          \ the training ran for sometime, but regrettably, another dataset problem\
          \ occurred. I have time to investigate more later, but it would seem that\
          \ this might be related to what @desperadoola reported here https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/discussions/15\n\
          \n```\nTraceback (most recent call last):\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 250, in <module>\n    train()\n  File \"./FastChat/fastchat/train/train.py\"\
          , line 244, in train\n    trainer.train()\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1652, in train\n    return inner_training_loop(\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1889, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 628, in __next__\n    data = self._next_data()\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
          , line 671, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
          \  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
          \  File \"./FastChat/fastchat/train/train.py\", line 194, in __getitem__\n\
          \    data_dict = preprocess([e[\"conversations\"] for e in sources], self.tokenizer)\n\
          \  File \"./FastChat/fastchat/train/train.py\", line 92, in preprocess\n\
          \    assert role == conv.roles[j % 2], f\"{i}\"\nAssertionError: 0\n```\n\
          \nAlso, I'm seeing a lot of ```Token indices sequence length is longer than\
          \ the specified maximum sequence length for this model (2573 > 2048). Running\
          \ this sequence through the model will result in indexing errors``` (a message\
          \ from hf-transformers). Didn't have these during the V3 training. It hasn't\
          \ resulted in any errors during the training, but might be a problem at\
          \ some point. Another thing that might be good to check out..."
        updatedAt: '2023-04-26T09:17:51.933Z'
      numEdits: 1
      reactions: []
    id: 6448ec0aab5abd92785cd99a
    type: comment
  author: reeducator
  content: "@gozfarb's fix took care of the empty conversations problem. This time,\
    \ the training ran for sometime, but regrettably, another dataset problem occurred.\
    \ I have time to investigate more later, but it would seem that this might be\
    \ related to what @desperadoola reported here https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/discussions/15\n\
    \n```\nTraceback (most recent call last):\n  File \"./FastChat/fastchat/train/train.py\"\
    , line 250, in <module>\n    train()\n  File \"./FastChat/fastchat/train/train.py\"\
    , line 244, in train\n    trainer.train()\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1652, in train\n    return inner_training_loop(\n  File \"./env/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1889, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
    \  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
    , line 628, in __next__\n    data = self._next_data()\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\"\
    , line 671, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may\
    \ raise StopIteration\n  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
    , line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
    \  File \"./env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\"\
    , line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n\
    \  File \"./FastChat/fastchat/train/train.py\", line 194, in __getitem__\n   \
    \ data_dict = preprocess([e[\"conversations\"] for e in sources], self.tokenizer)\n\
    \  File \"./FastChat/fastchat/train/train.py\", line 92, in preprocess\n    assert\
    \ role == conv.roles[j % 2], f\"{i}\"\nAssertionError: 0\n```\n\nAlso, I'm seeing\
    \ a lot of ```Token indices sequence length is longer than the specified maximum\
    \ sequence length for this model (2573 > 2048). Running this sequence through\
    \ the model will result in indexing errors``` (a message from hf-transformers).\
    \ Didn't have these during the V3 training. It hasn't resulted in any errors during\
    \ the training, but might be a problem at some point. Another thing that might\
    \ be good to check out..."
  created_at: 2023-04-26 08:16:58+00:00
  edited: true
  hidden: false
  id: 6448ec0aab5abd92785cd99a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-26T10:12:39.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>It''s entirely possible the older script anon had is nuking just
          the given conversation line rather than nuking the entire conversation?
          I''ll have to look at what the python is doing. Does the training set really
          not allow for gpt to talk twice in a row? Seems like an incredible oversight
          on their part and potentially creates serious problems for trying to train
          against anything other than a human-&gt;gpt-&gt;human conversation structure.</p>

          <p>As to the sequence length, I am genuinely baffled there since I''ll I''ve
          done is remove stuff. Unless it''s concatenating multiple responses from
          GPT at training time, I don''t know how the issue could crop up. All of
          this is just modifications against anon''s latest V3. Does FastChat have
          any sort of dataset verification pass that can be run before training?</p>

          <p>The original script is set to nuke entire conversations and I think anon
          may have pruned his last version of the V3 dataset with a new tool that
          just removed GPT responses, so I may need to prune against an older version
          of the dataset? I think he pushed one after your first training run. I don''t
          remember. Writing some further parsing code to check for consecutive "gpt"
          or "human" inputs. Knowing the specific problem will help, but glancing
          at that error, I think FastChat allows only two parties and those two parties
          can speak only once in turn. No error checking, no ability to have more
          than two. That presents a real problem for potential future datasets with
          more participants. RIP.</p>

          <p>Realistically, we need the ability to run a FastChat dry run against
          a given dataset so I can just iterate more quickly before pushing.</p>

          '
        raw: 'It''s entirely possible the older script anon had is nuking just the
          given conversation line rather than nuking the entire conversation? I''ll
          have to look at what the python is doing. Does the training set really not
          allow for gpt to talk twice in a row? Seems like an incredible oversight
          on their part and potentially creates serious problems for trying to train
          against anything other than a human->gpt->human conversation structure.


          As to the sequence length, I am genuinely baffled there since I''ll I''ve
          done is remove stuff. Unless it''s concatenating multiple responses from
          GPT at training time, I don''t know how the issue could crop up. All of
          this is just modifications against anon''s latest V3. Does FastChat have
          any sort of dataset verification pass that can be run before training?


          The original script is set to nuke entire conversations and I think anon
          may have pruned his last version of the V3 dataset with a new tool that
          just removed GPT responses, so I may need to prune against an older version
          of the dataset? I think he pushed one after your first training run. I don''t
          remember. Writing some further parsing code to check for consecutive "gpt"
          or "human" inputs. Knowing the specific problem will help, but glancing
          at that error, I think FastChat allows only two parties and those two parties
          can speak only once in turn. No error checking, no ability to have more
          than two. That presents a real problem for potential future datasets with
          more participants. RIP.


          Realistically, we need the ability to run a FastChat dry run against a given
          dataset so I can just iterate more quickly before pushing.'
        updatedAt: '2023-04-26T10:12:39.182Z'
      numEdits: 0
      reactions: []
    id: 6448f917ab5abd92785e440a
    type: comment
  author: deleted
  content: 'It''s entirely possible the older script anon had is nuking just the given
    conversation line rather than nuking the entire conversation? I''ll have to look
    at what the python is doing. Does the training set really not allow for gpt to
    talk twice in a row? Seems like an incredible oversight on their part and potentially
    creates serious problems for trying to train against anything other than a human->gpt->human
    conversation structure.


    As to the sequence length, I am genuinely baffled there since I''ll I''ve done
    is remove stuff. Unless it''s concatenating multiple responses from GPT at training
    time, I don''t know how the issue could crop up. All of this is just modifications
    against anon''s latest V3. Does FastChat have any sort of dataset verification
    pass that can be run before training?


    The original script is set to nuke entire conversations and I think anon may have
    pruned his last version of the V3 dataset with a new tool that just removed GPT
    responses, so I may need to prune against an older version of the dataset? I think
    he pushed one after your first training run. I don''t remember. Writing some further
    parsing code to check for consecutive "gpt" or "human" inputs. Knowing the specific
    problem will help, but glancing at that error, I think FastChat allows only two
    parties and those two parties can speak only once in turn. No error checking,
    no ability to have more than two. That presents a real problem for potential future
    datasets with more participants. RIP.


    Realistically, we need the ability to run a FastChat dry run against a given dataset
    so I can just iterate more quickly before pushing.'
  created_at: 2023-04-26 09:12:39+00:00
  edited: false
  hidden: false
  id: 6448f917ab5abd92785e440a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-26T11:03:38.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I''m pushing v4.2 changes in a minute. I''ll talk about them a bit
          below and my findings.</p>

          <p>Alright, I ran a script over the dataset and got 123 conversations with
          problems with the same talker in a row and 639 conversations with less than
          one message (all 0-entry conversations were removed already, these were
          all one-message convos, mostly containing GPT responding with no human prompt).
          I''ve modified the cleaner and validator to check for these now.</p>

          <p>I am not sure how that happened. It''s possible the issue was anon''s
          updated script. A curious thing is our total conversation count when the
          prune is run against the V2 dataset drops aggressively to 49634 conversations
          instead of the current set''s 69385 (the 4.2 is something like 68250 or
          so). That is something we can do if random issues in further cleaning of
          the V3 dataset proves to be too confusing and anon doesn''t reappear.</p>

          <p>I don''t know how to solve the sequence-length issue, so if anyone wants
          to post some code I can add to the optional_clean.py script to calculate
          token counts, I can run that against it. I''m not sure on what the best
          course of action is there so any advice on what lib to use for token calculation
          of a given message is fine with me. The dataset was already supposed to
          have been cleaned up. I don''t know.</p>

          <p><strong>[EDIT]</strong><br>4.2 pushed. I also added V2 cleaned with the
          4.2 script to the <code>Experimental</code> folder. It should be nuking
          absolutely every conversation with problems against the V2 (pre message-only-pruning)
          dataset. That said, I would think more data is better, but it''s a second
          dataset to try if we can run validation against it without wasting too much
          time. Or someone could try training a LoRA against them which could be less
          intensive and would hopefully error in the same way? Any help is appreciated.</p>

          '
        raw: "I'm pushing v4.2 changes in a minute. I'll talk about them a bit below\
          \ and my findings.\n\nAlright, I ran a script over the dataset and got 123\
          \ conversations with problems with the same talker in a row and 639 conversations\
          \ with less than one message (all 0-entry conversations were removed already,\
          \ these were all one-message convos, mostly containing GPT responding with\
          \ no human prompt). I've modified the cleaner and validator to check for\
          \ these now.\n\nI am not sure how that happened. It's possible the issue\
          \ was anon's updated script. A curious thing is our total conversation count\
          \ when the prune is run against the V2 dataset drops aggressively to 49634\
          \ conversations instead of the current set's 69385 (the 4.2 is something\
          \ like 68250 or so). That is something we can do if random issues in further\
          \ cleaning of the V3 dataset proves to be too confusing and anon doesn't\
          \ reappear.\n\nI don't know how to solve the sequence-length issue, so if\
          \ anyone wants to post some code I can add to the optional_clean.py script\
          \ to calculate token counts, I can run that against it. I'm not sure on\
          \ what the best course of action is there so any advice on what lib to use\
          \ for token calculation of a given message is fine with me. The dataset\
          \ was already supposed to have been cleaned up. I don't know.\n\n**[EDIT]**\
          \  \n4.2 pushed. I also added V2 cleaned with the 4.2 script to the `Experimental`\
          \ folder. It should be nuking absolutely every conversation with problems\
          \ against the V2 (pre message-only-pruning) dataset. That said, I would\
          \ think more data is better, but it's a second dataset to try if we can\
          \ run validation against it without wasting too much time. Or someone could\
          \ try training a LoRA against them which could be less intensive and would\
          \ hopefully error in the same way? Any help is appreciated."
        updatedAt: '2023-04-26T11:20:26.824Z'
      numEdits: 2
      reactions: []
    id: 6449050af88f1495f095b14a
    type: comment
  author: deleted
  content: "I'm pushing v4.2 changes in a minute. I'll talk about them a bit below\
    \ and my findings.\n\nAlright, I ran a script over the dataset and got 123 conversations\
    \ with problems with the same talker in a row and 639 conversations with less\
    \ than one message (all 0-entry conversations were removed already, these were\
    \ all one-message convos, mostly containing GPT responding with no human prompt).\
    \ I've modified the cleaner and validator to check for these now.\n\nI am not\
    \ sure how that happened. It's possible the issue was anon's updated script. A\
    \ curious thing is our total conversation count when the prune is run against\
    \ the V2 dataset drops aggressively to 49634 conversations instead of the current\
    \ set's 69385 (the 4.2 is something like 68250 or so). That is something we can\
    \ do if random issues in further cleaning of the V3 dataset proves to be too confusing\
    \ and anon doesn't reappear.\n\nI don't know how to solve the sequence-length\
    \ issue, so if anyone wants to post some code I can add to the optional_clean.py\
    \ script to calculate token counts, I can run that against it. I'm not sure on\
    \ what the best course of action is there so any advice on what lib to use for\
    \ token calculation of a given message is fine with me. The dataset was already\
    \ supposed to have been cleaned up. I don't know.\n\n**[EDIT]**  \n4.2 pushed.\
    \ I also added V2 cleaned with the 4.2 script to the `Experimental` folder. It\
    \ should be nuking absolutely every conversation with problems against the V2\
    \ (pre message-only-pruning) dataset. That said, I would think more data is better,\
    \ but it's a second dataset to try if we can run validation against it without\
    \ wasting too much time. Or someone could try training a LoRA against them which\
    \ could be less intensive and would hopefully error in the same way? Any help\
    \ is appreciated."
  created_at: 2023-04-26 10:03:38+00:00
  edited: true
  hidden: false
  id: 6449050af88f1495f095b14a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-26T11:55:44.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Currently writing a Validator. It looks like there are some issues
          in the dataset surrounding ''bing'' and ''user'' conversations that will
          make the preprocessor choke. It is a very stupid preproccesor.</p>

          <p>I am going to run a sanitization pass to get rid of any non-"human",
          non-"gpt" responses since the preprocessor chokes un them unless you change
          the roles line in <code>train.py</code> to say:</p>

          <p><code>roles = {"human": conv.roles[0], "gpt": conv.roles[1], "user":
          conv.roles[0], "bing": conv.roles[1], "chatgpt": conv.roles[1], "system":
          conv.roles[1]}</code></p>

          <p>I will be uploading the sanitized version as the main file and I will
          add an "unsanitized" version as well. It should pass pre-processing at this
          point at least.</p>

          <p>EDIT: Pushed the wrong V4.2, so repushed. Sorry about that. Working on
          sanitizing code now. Should wait for that.</p>

          '
        raw: 'Currently writing a Validator. It looks like there are some issues in
          the dataset surrounding ''bing'' and ''user'' conversations that will make
          the preprocessor choke. It is a very stupid preproccesor.


          I am going to run a sanitization pass to get rid of any non-"human", non-"gpt"
          responses since the preprocessor chokes un them unless you change the roles
          line in `train.py` to say:


          `roles = {"human": conv.roles[0], "gpt": conv.roles[1], "user": conv.roles[0],
          "bing": conv.roles[1], "chatgpt": conv.roles[1], "system": conv.roles[1]}`


          I will be uploading the sanitized version as the main file and I will add
          an "unsanitized" version as well. It should pass pre-processing at this
          point at least.


          EDIT: Pushed the wrong V4.2, so repushed. Sorry about that. Working on sanitizing
          code now. Should wait for that.'
        updatedAt: '2023-04-26T12:16:55.783Z'
      numEdits: 2
      reactions: []
    id: 64491140d16a70c01591e809
    type: comment
  author: deleted
  content: 'Currently writing a Validator. It looks like there are some issues in
    the dataset surrounding ''bing'' and ''user'' conversations that will make the
    preprocessor choke. It is a very stupid preproccesor.


    I am going to run a sanitization pass to get rid of any non-"human", non-"gpt"
    responses since the preprocessor chokes un them unless you change the roles line
    in `train.py` to say:


    `roles = {"human": conv.roles[0], "gpt": conv.roles[1], "user": conv.roles[0],
    "bing": conv.roles[1], "chatgpt": conv.roles[1], "system": conv.roles[1]}`


    I will be uploading the sanitized version as the main file and I will add an "unsanitized"
    version as well. It should pass pre-processing at this point at least.


    EDIT: Pushed the wrong V4.2, so repushed. Sorry about that. Working on sanitizing
    code now. Should wait for that.'
  created_at: 2023-04-26 10:55:44+00:00
  edited: true
  hidden: false
  id: 64491140d16a70c01591e809
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-26T12:36:04.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>OKAY! Sorry for blowing up the thread. I pushed V4.3 which is sanitized
          to only include "human" and "gpt" chat entries (names were changed). V4.2
          with the other names (user, bing, chatgpt, and system) is still in the Unsanitized
          folder for now.</p>

          <p>There are only a few of those conversations so it may be worth just gutting
          them, but I held off on doing that pending thoughts from you guys. I have
          the code ready to go to do that, though I am not worried about them impacting
          output quality or anything. The total removed conversations dropped would
          be 18 but I wanted to hear what people thought the preferred course of action
          would be.</p>

          <p>EDIT: git didn''t pick up the 4.3 changes for some reason (git add *
          just didn''t do it) so I repushed and 4.3 is in there now. There is a 4.2
          with human/gpt only, I removed the 18 conversations with other usernames.
          If you guys want that to be the direction to go here, I will call it V4.4
          and move it back out of the Old folder. Both 4.3 and 4.2-humangptonly pass
          the validation for dataset prep that I ripped out of the FastChat <code>train.py</code>
          script.</p>

          '
        raw: 'OKAY! Sorry for blowing up the thread. I pushed V4.3 which is sanitized
          to only include "human" and "gpt" chat entries (names were changed). V4.2
          with the other names (user, bing, chatgpt, and system) is still in the Unsanitized
          folder for now.


          There are only a few of those conversations so it may be worth just gutting
          them, but I held off on doing that pending thoughts from you guys. I have
          the code ready to go to do that, though I am not worried about them impacting
          output quality or anything. The total removed conversations dropped would
          be 18 but I wanted to hear what people thought the preferred course of action
          would be.


          EDIT: git didn''t pick up the 4.3 changes for some reason (git add * just
          didn''t do it) so I repushed and 4.3 is in there now. There is a 4.2 with
          human/gpt only, I removed the 18 conversations with other usernames. If
          you guys want that to be the direction to go here, I will call it V4.4 and
          move it back out of the Old folder. Both 4.3 and 4.2-humangptonly pass the
          validation for dataset prep that I ripped out of the FastChat `train.py`
          script.'
        updatedAt: '2023-04-26T12:45:15.541Z'
      numEdits: 1
      reactions: []
    id: 64491ab4ab5abd927861a9ad
    type: comment
  author: deleted
  content: 'OKAY! Sorry for blowing up the thread. I pushed V4.3 which is sanitized
    to only include "human" and "gpt" chat entries (names were changed). V4.2 with
    the other names (user, bing, chatgpt, and system) is still in the Unsanitized
    folder for now.


    There are only a few of those conversations so it may be worth just gutting them,
    but I held off on doing that pending thoughts from you guys. I have the code ready
    to go to do that, though I am not worried about them impacting output quality
    or anything. The total removed conversations dropped would be 18 but I wanted
    to hear what people thought the preferred course of action would be.


    EDIT: git didn''t pick up the 4.3 changes for some reason (git add * just didn''t
    do it) so I repushed and 4.3 is in there now. There is a 4.2 with human/gpt only,
    I removed the 18 conversations with other usernames. If you guys want that to
    be the direction to go here, I will call it V4.4 and move it back out of the Old
    folder. Both 4.3 and 4.2-humangptonly pass the validation for dataset prep that
    I ripped out of the FastChat `train.py` script.'
  created_at: 2023-04-26 11:36:04+00:00
  edited: true
  hidden: false
  id: 64491ab4ab5abd927861a9ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-26T12:53:32.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Thanks a lot @gozfarb. I just looked into this a bit more as well
          and it indeed seems the FastChat team reworked a bunch of code for the 1.1,
          making the dataset format more strict now. If I was to rerun V3 with 1.1,
          it would fail too. Sanitizing is important now and anything with "user"
          and such wouldn''t work anymore. Strict order of conversation roles looks
          to be important too.</p>

          <p>Good job on the validation so far. I''ve queued it up again with the
          "experimental" dataset and will keep swapping it as long as we find problems.</p>

          <p>Edit: so I''ve now taken the V4.3 from the root of your repository. Will
          keep looking into that.</p>

          '
        raw: 'Thanks a lot @gozfarb. I just looked into this a bit more as well and
          it indeed seems the FastChat team reworked a bunch of code for the 1.1,
          making the dataset format more strict now. If I was to rerun V3 with 1.1,
          it would fail too. Sanitizing is important now and anything with "user"
          and such wouldn''t work anymore. Strict order of conversation roles looks
          to be important too.


          Good job on the validation so far. I''ve queued it up again with the "experimental"
          dataset and will keep swapping it as long as we find problems.


          Edit: so I''ve now taken the V4.3 from the root of your repository. Will
          keep looking into that.'
        updatedAt: '2023-04-26T12:56:33.640Z'
      numEdits: 1
      reactions: []
    id: 64491eccd5d86def91cf5a0d
    type: comment
  author: reeducator
  content: 'Thanks a lot @gozfarb. I just looked into this a bit more as well and
    it indeed seems the FastChat team reworked a bunch of code for the 1.1, making
    the dataset format more strict now. If I was to rerun V3 with 1.1, it would fail
    too. Sanitizing is important now and anything with "user" and such wouldn''t work
    anymore. Strict order of conversation roles looks to be important too.


    Good job on the validation so far. I''ve queued it up again with the "experimental"
    dataset and will keep swapping it as long as we find problems.


    Edit: so I''ve now taken the V4.3 from the root of your repository. Will keep
    looking into that.'
  created_at: 2023-04-26 11:53:32+00:00
  edited: true
  hidden: false
  id: 64491eccd5d86def91cf5a0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-26T12:57:48.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>The experimental dataset likely won''t work because I didn''t run
          a sanitization pass on that. I''d recommend running it against V4.2-humangptonly
          (in the Old folder) or V4.3. Those both pass the validation code I cobbled
          together from FastChat. I will run a sanitizing pass against the Experimental
          set and upload that after I check it against the validator.</p>

          <p>Edit: Pushed the V2 sanitized dataset. It retains the bing/chatgpt conversations
          and passes validation. I can''t think of a reason to use it though since
          it loses ~20k conversations and the other versions pass validation now.
          I''d recommend continuing with V4.3.</p>

          '
        raw: 'The experimental dataset likely won''t work because I didn''t run a
          sanitization pass on that. I''d recommend running it against V4.2-humangptonly
          (in the Old folder) or V4.3. Those both pass the validation code I cobbled
          together from FastChat. I will run a sanitizing pass against the Experimental
          set and upload that after I check it against the validator.


          Edit: Pushed the V2 sanitized dataset. It retains the bing/chatgpt conversations
          and passes validation. I can''t think of a reason to use it though since
          it loses ~20k conversations and the other versions pass validation now.
          I''d recommend continuing with V4.3.'
        updatedAt: '2023-04-26T13:02:54.437Z'
      numEdits: 1
      reactions: []
    id: 64491fccd16a70c01593543d
    type: comment
  author: deleted
  content: 'The experimental dataset likely won''t work because I didn''t run a sanitization
    pass on that. I''d recommend running it against V4.2-humangptonly (in the Old
    folder) or V4.3. Those both pass the validation code I cobbled together from FastChat.
    I will run a sanitizing pass against the Experimental set and upload that after
    I check it against the validator.


    Edit: Pushed the V2 sanitized dataset. It retains the bing/chatgpt conversations
    and passes validation. I can''t think of a reason to use it though since it loses
    ~20k conversations and the other versions pass validation now. I''d recommend
    continuing with V4.3.'
  created_at: 2023-04-26 11:57:48+00:00
  edited: true
  hidden: false
  id: 64491fccd16a70c01593543d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-26T13:03:22.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Yep, I have the V4.3 in place now. I verified myself that this will
          pass the validation that you added.</p>

          '
        raw: Yep, I have the V4.3 in place now. I verified myself that this will pass
          the validation that you added.
        updatedAt: '2023-04-26T13:03:22.798Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - CyberTimon
        - 9cento
    id: 6449211aab5abd9278624f1b
    type: comment
  author: reeducator
  content: Yep, I have the V4.3 in place now. I verified myself that this will pass
    the validation that you added.
  created_at: 2023-04-26 12:03:22+00:00
  edited: false
  hidden: false
  id: 6449211aab5abd9278624f1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fef7b4f47bddd8ac011a88638fb9fdf5.svg
      fullname: "Martin Nov\xE1k"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: novhack
      type: user
    createdAt: '2023-04-27T23:38:49.000Z'
    data:
      edited: false
      editors:
      - novhack
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fef7b4f47bddd8ac011a88638fb9fdf5.svg
          fullname: "Martin Nov\xE1k"
          isHf: false
          isPro: false
          name: novhack
          type: user
        html: '<p>Hey! I love the effort everybody is putting into this. Would the
          newest version allow explicit translations? I tried to use the model you
          published 13 days ago to translate some texts from Japanese and it''s still
          extremely stubborn to do so. It''s rather talking about how it is against
          rules or how it doesn''t feel like translating.</p>

          '
        raw: Hey! I love the effort everybody is putting into this. Would the newest
          version allow explicit translations? I tried to use the model you published
          13 days ago to translate some texts from Japanese and it's still extremely
          stubborn to do so. It's rather talking about how it is against rules or
          how it doesn't feel like translating.
        updatedAt: '2023-04-27T23:38:49.605Z'
      numEdits: 0
      reactions: []
    id: 644b0789f9f1b0cd3d93eea6
    type: comment
  author: novhack
  content: Hey! I love the effort everybody is putting into this. Would the newest
    version allow explicit translations? I tried to use the model you published 13
    days ago to translate some texts from Japanese and it's still extremely stubborn
    to do so. It's rather talking about how it is against rules or how it doesn't
    feel like translating.
  created_at: 2023-04-27 22:38:49+00:00
  edited: false
  hidden: false
  id: 644b0789f9f1b0cd3d93eea6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-27T23:45:28.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>You might be able to get translations out of it with 1.1, but they
          would be incidental and hitting base LLaMa. All unicode and non-English
          were stripped out of the dataset because they would likely have contained
          moralizing phrases on those languages and it would have been nearly impossible
          for us to track those down but they relational weights could still lead
          to moralizing outputs in English (at least, that''s how the reasoning goes).
          That decision was made before I started helping, but I agree with it in
          principle. The moralizing RNG in the 1.0 build is enough to make sure we
          want to control for 100% moralizing removal as best as we can.</p>

          '
        raw: You might be able to get translations out of it with 1.1, but they would
          be incidental and hitting base LLaMa. All unicode and non-English were stripped
          out of the dataset because they would likely have contained moralizing phrases
          on those languages and it would have been nearly impossible for us to track
          those down but they relational weights could still lead to moralizing outputs
          in English (at least, that's how the reasoning goes). That decision was
          made before I started helping, but I agree with it in principle. The moralizing
          RNG in the 1.0 build is enough to make sure we want to control for 100%
          moralizing removal as best as we can.
        updatedAt: '2023-04-27T23:45:28.134Z'
      numEdits: 0
      reactions: []
    id: 644b0918cb45734dfd4bde9e
    type: comment
  author: deleted
  content: You might be able to get translations out of it with 1.1, but they would
    be incidental and hitting base LLaMa. All unicode and non-English were stripped
    out of the dataset because they would likely have contained moralizing phrases
    on those languages and it would have been nearly impossible for us to track those
    down but they relational weights could still lead to moralizing outputs in English
    (at least, that's how the reasoning goes). That decision was made before I started
    helping, but I agree with it in principle. The moralizing RNG in the 1.0 build
    is enough to make sure we want to control for 100% moralizing removal as best
    as we can.
  created_at: 2023-04-27 22:45:28+00:00
  edited: false
  hidden: false
  id: 644b0918cb45734dfd4bde9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-04-28T03:15:07.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<p>Not to sound ungrateful or something, I absolutely love what you''re
          doing here, just wanted to ask how long would it roughly take for the next
          unfiltered model to drop. Also if it will be based on Vicuna 1.1 7B/13B
          and quantized 4 bit 128. That''s all and thank for all your effort, you''re
          doing God''s work.</p>

          '
        raw: Not to sound ungrateful or something, I absolutely love what you're doing
          here, just wanted to ask how long would it roughly take for the next unfiltered
          model to drop. Also if it will be based on Vicuna 1.1 7B/13B and quantized
          4 bit 128. That's all and thank for all your effort, you're doing God's
          work.
        updatedAt: '2023-04-28T03:15:07.975Z'
      numEdits: 0
      reactions: []
    id: 644b3a3bcb45734dfd4f642c
    type: comment
  author: 9cento
  content: Not to sound ungrateful or something, I absolutely love what you're doing
    here, just wanted to ask how long would it roughly take for the next unfiltered
    model to drop. Also if it will be based on Vicuna 1.1 7B/13B and quantized 4 bit
    128. That's all and thank for all your effort, you're doing God's work.
  created_at: 2023-04-28 02:15:07+00:00
  edited: false
  hidden: false
  id: 644b3a3bcb45734dfd4f642c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-28T03:22:54.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I''m sure people will jump on quantizing them when reeducator gets
          the model trained, assuming he doesn''t quant them himself. It''ll be Vicuna
          1.1.</p>

          <p>Assuming we have the bugs worked out of the dataset, it''ll start when
          he gets his next training slot presumably. Sadly, we''ve missed a few windows
          due to changes moving from Vicuna 1.0 to 1.1 causing failures. I don''t
          think 7B is planned at the moment. Based on what was said in this thread,
          it''ll be 13b then 30b. Not sure about later on. If any of that is off,
          someone''ll correct me I''m sure.</p>

          '
        raw: 'I''m sure people will jump on quantizing them when reeducator gets the
          model trained, assuming he doesn''t quant them himself. It''ll be Vicuna
          1.1.


          Assuming we have the bugs worked out of the dataset, it''ll start when he
          gets his next training slot presumably. Sadly, we''ve missed a few windows
          due to changes moving from Vicuna 1.0 to 1.1 causing failures. I don''t
          think 7B is planned at the moment. Based on what was said in this thread,
          it''ll be 13b then 30b. Not sure about later on. If any of that is off,
          someone''ll correct me I''m sure.'
        updatedAt: '2023-04-28T03:22:54.674Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - 9cento
    id: 644b3c0ed4483bfaa0780a95
    type: comment
  author: deleted
  content: 'I''m sure people will jump on quantizing them when reeducator gets the
    model trained, assuming he doesn''t quant them himself. It''ll be Vicuna 1.1.


    Assuming we have the bugs worked out of the dataset, it''ll start when he gets
    his next training slot presumably. Sadly, we''ve missed a few windows due to changes
    moving from Vicuna 1.0 to 1.1 causing failures. I don''t think 7B is planned at
    the moment. Based on what was said in this thread, it''ll be 13b then 30b. Not
    sure about later on. If any of that is off, someone''ll correct me I''m sure.'
  created_at: 2023-04-28 02:22:54+00:00
  edited: false
  hidden: false
  id: 644b3c0ed4483bfaa0780a95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-04-28T07:14:12.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: "<blockquote>\n<p>I'm sure people will jump on quantizing them when\
          \ reeducator gets the model trained, assuming he doesn't quant them himself.\
          \ It'll be Vicuna 1.1.</p>\n<p>Assuming we have the bugs worked out of the\
          \ dataset, it'll start when he gets his next training slot presumably. Sadly,\
          \ we've missed a few windows due to changes moving from Vicuna 1.0 to 1.1\
          \ causing failures. I don't think 7B is planned at the moment. Based on\
          \ what was said in this thread, it'll be 13b then 30b. Not sure about later\
          \ on. If any of that is off, someone'll correct me I'm sure.</p>\n</blockquote>\n\
          <p>30B would be absolutely GOAT, can't wait \U0001F680</p>\n"
        raw: "> I'm sure people will jump on quantizing them when reeducator gets\
          \ the model trained, assuming he doesn't quant them himself. It'll be Vicuna\
          \ 1.1.\n> \n> Assuming we have the bugs worked out of the dataset, it'll\
          \ start when he gets his next training slot presumably. Sadly, we've missed\
          \ a few windows due to changes moving from Vicuna 1.0 to 1.1 causing failures.\
          \ I don't think 7B is planned at the moment. Based on what was said in this\
          \ thread, it'll be 13b then 30b. Not sure about later on. If any of that\
          \ is off, someone'll correct me I'm sure.\n\n30B would be absolutely GOAT,\
          \ can't wait \U0001F680"
        updatedAt: '2023-04-28T07:14:12.178Z'
      numEdits: 0
      reactions: []
    id: 644b7244d873cbc8cc274449
    type: comment
  author: 9cento
  content: "> I'm sure people will jump on quantizing them when reeducator gets the\
    \ model trained, assuming he doesn't quant them himself. It'll be Vicuna 1.1.\n\
    > \n> Assuming we have the bugs worked out of the dataset, it'll start when he\
    \ gets his next training slot presumably. Sadly, we've missed a few windows due\
    \ to changes moving from Vicuna 1.0 to 1.1 causing failures. I don't think 7B\
    \ is planned at the moment. Based on what was said in this thread, it'll be 13b\
    \ then 30b. Not sure about later on. If any of that is off, someone'll correct\
    \ me I'm sure.\n\n30B would be absolutely GOAT, can't wait \U0001F680"
  created_at: 2023-04-28 06:14:12+00:00
  edited: false
  hidden: false
  id: 644b7244d873cbc8cc274449
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-04-28T07:43:09.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: "<blockquote>\n<p>30B would be absolutely GOAT, can't wait \U0001F680\
          </p>\n</blockquote>\n<p>It would be great to have access not only to 30B\
          \ 4-bit but 30B 3-bit too, since 30B 4-bit llama model, for instance, causes\
          \ an OutOfMemory error on 24GB VRAM (3090/4090) when dealing with large\
          \ contexts. Meanwhile, the 30B 3-bit version works smoothly. The 30B 3-bit\
          \ model will allow more people without professional GPU use it.</p>\n"
        raw: "> 30B would be absolutely GOAT, can't wait \U0001F680\n\nIt would be\
          \ great to have access not only to 30B 4-bit but 30B 3-bit too, since 30B\
          \ 4-bit llama model, for instance, causes an OutOfMemory error on 24GB VRAM\
          \ (3090/4090) when dealing with large contexts. Meanwhile, the 30B 3-bit\
          \ version works smoothly. The 30B 3-bit model will allow more people without\
          \ professional GPU use it."
        updatedAt: '2023-04-28T07:43:09.609Z'
      numEdits: 0
      reactions: []
    id: 644b790d13af0231ecc4e7f2
    type: comment
  author: Kelheor
  content: "> 30B would be absolutely GOAT, can't wait \U0001F680\n\nIt would be great\
    \ to have access not only to 30B 4-bit but 30B 3-bit too, since 30B 4-bit llama\
    \ model, for instance, causes an OutOfMemory error on 24GB VRAM (3090/4090) when\
    \ dealing with large contexts. Meanwhile, the 30B 3-bit version works smoothly.\
    \ The 30B 3-bit model will allow more people without professional GPU use it."
  created_at: 2023-04-28 06:43:09+00:00
  edited: false
  hidden: false
  id: 644b790d13af0231ecc4e7f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-04-28T14:40:09.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: "<blockquote>\n<blockquote>\n<p>30B would be absolutely GOAT, can't\
          \ wait \U0001F680</p>\n</blockquote>\n<p>It would be great to have access\
          \ not only to 30B 4-bit but 30B 3-bit too, since 30B 4-bit llama model,\
          \ for instance, causes an OutOfMemory error on 24GB VRAM (3090/4090) when\
          \ dealing with large contexts. Meanwhile, the 30B 3-bit version works smoothly.\
          \ The 30B 3-bit model will allow more people without professional GPU use\
          \ it.</p>\n</blockquote>\n<p>Wouldn't it worsen the output quality compared\
          \ to 4-bit where it's barely noticeable?</p>\n"
        raw: "> > 30B would be absolutely GOAT, can't wait \U0001F680\n> \n> It would\
          \ be great to have access not only to 30B 4-bit but 30B 3-bit too, since\
          \ 30B 4-bit llama model, for instance, causes an OutOfMemory error on 24GB\
          \ VRAM (3090/4090) when dealing with large contexts. Meanwhile, the 30B\
          \ 3-bit version works smoothly. The 30B 3-bit model will allow more people\
          \ without professional GPU use it.\n\nWouldn't it worsen the output quality\
          \ compared to 4-bit where it's barely noticeable?"
        updatedAt: '2023-04-28T14:40:09.656Z'
      numEdits: 0
      reactions: []
    id: 644bdac9ed08a4fdf4d72a7b
    type: comment
  author: 9cento
  content: "> > 30B would be absolutely GOAT, can't wait \U0001F680\n> \n> It would\
    \ be great to have access not only to 30B 4-bit but 30B 3-bit too, since 30B 4-bit\
    \ llama model, for instance, causes an OutOfMemory error on 24GB VRAM (3090/4090)\
    \ when dealing with large contexts. Meanwhile, the 30B 3-bit version works smoothly.\
    \ The 30B 3-bit model will allow more people without professional GPU use it.\n\
    \nWouldn't it worsen the output quality compared to 4-bit where it's barely noticeable?"
  created_at: 2023-04-28 13:40:09+00:00
  edited: false
  hidden: false
  id: 644bdac9ed08a4fdf4d72a7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-04-28T14:49:31.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: '<blockquote>

          <p>Wouldn''t it worsen the output quality compared to 4-bit where it''s
          barely noticeable?</p>

          </blockquote>

          <p>In this case you need to compare between 13B 4-bit (or even 8-bit) and
          30B 3-bit. If 30B 3-bit is better, then any quality reduction compared to
          30B 4-bit is irrelevant.</p>

          '
        raw: '> Wouldn''t it worsen the output quality compared to 4-bit where it''s
          barely noticeable?


          In this case you need to compare between 13B 4-bit (or even 8-bit) and 30B
          3-bit. If 30B 3-bit is better, then any quality reduction compared to 30B
          4-bit is irrelevant.'
        updatedAt: '2023-04-28T14:49:31.895Z'
      numEdits: 0
      reactions: []
    id: 644bdcfb0ce4f8fb5164ec5d
    type: comment
  author: Kelheor
  content: '> Wouldn''t it worsen the output quality compared to 4-bit where it''s
    barely noticeable?


    In this case you need to compare between 13B 4-bit (or even 8-bit) and 30B 3-bit.
    If 30B 3-bit is better, then any quality reduction compared to 30B 4-bit is irrelevant.'
  created_at: 2023-04-28 13:49:31+00:00
  edited: false
  hidden: false
  id: 644bdcfb0ce4f8fb5164ec5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-04-28T16:58:39.000Z'
    data:
      edited: true
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Wouldn''t it worsen the output quality compared to 4-bit where it''s
          barely noticeable?</p>

          </blockquote>

          <p>In this case you need to compare between 13B 4-bit (or even 8-bit) and
          30B 3-bit. If 30B 3-bit is better, then any quality reduction compared to
          30B 4-bit is irrelevant.</p>

          </blockquote>

          <p>Would be cool to have both of them tho, I see use cases where the 4 bit
          could still come in handy. But anyway, let''s wait for the actual model
          to come out first lol</p>

          '
        raw: "> > Wouldn't it worsen the output quality compared to 4-bit where it's\
          \ barely noticeable?\n> \n> In this case you need to compare between 13B\
          \ 4-bit (or even 8-bit) and 30B 3-bit. If 30B 3-bit is better, then any\
          \ quality reduction compared to 30B 4-bit is irrelevant.\n\nWould be cool\
          \ to have both of them tho, I see use cases where the 4 bit could still\
          \ come in handy. But anyway, let's wait for the actual model to come out\
          \ first lol"
        updatedAt: '2023-04-28T16:59:06.385Z'
      numEdits: 1
      reactions: []
    id: 644bfb3f194e124dacb7630a
    type: comment
  author: 9cento
  content: "> > Wouldn't it worsen the output quality compared to 4-bit where it's\
    \ barely noticeable?\n> \n> In this case you need to compare between 13B 4-bit\
    \ (or even 8-bit) and 30B 3-bit. If 30B 3-bit is better, then any quality reduction\
    \ compared to 30B 4-bit is irrelevant.\n\nWould be cool to have both of them tho,\
    \ I see use cases where the 4 bit could still come in handy. But anyway, let's\
    \ wait for the actual model to come out first lol"
  created_at: 2023-04-28 15:58:39+00:00
  edited: true
  hidden: false
  id: 644bfb3f194e124dacb7630a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-28T21:47:40.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Training is running now, and the first checkpoint just got saved.
          Relatively safe to assume that it will now run till the end. Will upload
          the resulting models as soon as it''s done and checked. I''m not home on
          my PC at the moment, so I can''t test chat too extensively myself, but I''ll
          leave that to your capable hands.</p>

          '
        raw: Training is running now, and the first checkpoint just got saved. Relatively
          safe to assume that it will now run till the end. Will upload the resulting
          models as soon as it's done and checked. I'm not home on my PC at the moment,
          so I can't test chat too extensively myself, but I'll leave that to your
          capable hands.
        updatedAt: '2023-04-28T21:47:40.529Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - noirehawk
        - mancub
        - navyfighter12
        - TheYuriLover
        - DaigoNorr
        - Okki
        - Goldenblood56
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
        - 9cento
        - novhack
        - Goldenblood56
      - count: 2
        reaction: "\U0001F917"
        users:
        - TheYuriLover
        - Goldenblood56
      - count: 1
        reaction: "\U0001F92F"
        users:
        - navyfighter12
    id: 644c3efc0ce4f8fb517260db
    type: comment
  author: reeducator
  content: Training is running now, and the first checkpoint just got saved. Relatively
    safe to assume that it will now run till the end. Will upload the resulting models
    as soon as it's done and checked. I'm not home on my PC at the moment, so I can't
    test chat too extensively myself, but I'll leave that to your capable hands.
  created_at: 2023-04-28 20:47:40+00:00
  edited: false
  hidden: false
  id: 644c3efc0ce4f8fb517260db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305d290c79269321bfd220f112779e53.svg
      fullname: Alex P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: navyfighter12
      type: user
    createdAt: '2023-04-29T00:13:37.000Z'
    data:
      edited: false
      editors:
      - navyfighter12
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305d290c79269321bfd220f112779e53.svg
          fullname: Alex P
          isHf: false
          isPro: false
          name: navyfighter12
          type: user
        html: '<blockquote>

          <p>Training is running now, and the first checkpoint just got saved. Relatively
          safe to assume that it will now run till the end. Will upload the resulting
          models as soon as it''s done and checked. I''m not home on my PC at the
          moment, so I can''t test chat too extensively myself, but I''ll leave that
          to your capable hands.</p>

          </blockquote>

          <p>So excited! Thank you</p>

          '
        raw: '> Training is running now, and the first checkpoint just got saved.
          Relatively safe to assume that it will now run till the end. Will upload
          the resulting models as soon as it''s done and checked. I''m not home on
          my PC at the moment, so I can''t test chat too extensively myself, but I''ll
          leave that to your capable hands.


          So excited! Thank you'
        updatedAt: '2023-04-29T00:13:37.683Z'
      numEdits: 0
      reactions: []
    id: 644c6131194e124dacc55c15
    type: comment
  author: navyfighter12
  content: '> Training is running now, and the first checkpoint just got saved. Relatively
    safe to assume that it will now run till the end. Will upload the resulting models
    as soon as it''s done and checked. I''m not home on my PC at the moment, so I
    can''t test chat too extensively myself, but I''ll leave that to your capable
    hands.


    So excited! Thank you'
  created_at: 2023-04-28 23:13:37+00:00
  edited: false
  hidden: false
  id: 644c6131194e124dacc55c15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/513eb8a60b47c6ad7a3dbc8230c8c807.svg
      fullname: Okki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Okki
      type: user
    createdAt: '2023-04-29T16:00:43.000Z'
    data:
      edited: false
      editors:
      - Okki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/513eb8a60b47c6ad7a3dbc8230c8c807.svg
          fullname: Okki
          isHf: false
          isPro: false
          name: Okki
          type: user
        html: '<p>And now we have StableVicuna - <a rel="nofollow" href="https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot">https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot</a><br>Speed
          at which models develop is astounding...</p>

          '
        raw: 'And now we have StableVicuna - https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot

          Speed at which models develop is astounding...'
        updatedAt: '2023-04-29T16:00:43.011Z'
      numEdits: 0
      reactions: []
    id: 644d3f2b328c1aa30e46c2e6
    type: comment
  author: Okki
  content: 'And now we have StableVicuna - https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot

    Speed at which models develop is astounding...'
  created_at: 2023-04-29 15:00:43+00:00
  edited: false
  hidden: false
  id: 644d3f2b328c1aa30e46c2e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-04-29T18:12:42.000Z'
    data:
      edited: true
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<blockquote>

          <p>And now we have StableVicuna - <a rel="nofollow" href="https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot">https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot</a><br>Speed
          at which models develop is astounding...</p>

          </blockquote>

          <p>Yeah but is it unfiltered/uncensored?</p>

          '
        raw: '> And now we have StableVicuna - https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot

          > Speed at which models develop is astounding...


          Yeah but is it unfiltered/uncensored?'
        updatedAt: '2023-04-29T18:13:23.240Z'
      numEdits: 1
      reactions: []
    id: 644d5e1a328c1aa30e4974ad
    type: comment
  author: 9cento
  content: '> And now we have StableVicuna - https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot

    > Speed at which models develop is astounding...


    Yeah but is it unfiltered/uncensored?'
  created_at: 2023-04-29 17:12:42+00:00
  edited: true
  hidden: false
  id: 644d5e1a328c1aa30e4974ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-29T18:51:36.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<blockquote>

          <blockquote>

          <p>And now we have StableVicuna - <a rel="nofollow" href="https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot">https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot</a><br>Speed
          at which models develop is astounding...</p>

          </blockquote>

          <p>Yeah but is it unfiltered/uncensored?</p>

          </blockquote>

          <p>No it''s just like vanilla Vicuna when it comes to filtering. Also I
          was kind of disappointed that Stability AI did not train information on
          how to use Stable Diffusion image gen prompts etc. Into the model. Maybe
          they can''t or it''s not what they are after. But I would love to be able
          to ask the model to generate amazing prompts or ask it questions for advice
          about using Stable Diffusion etc. Kind of like how people have with Chat
          GTP and MJ by feeding MJ the information about MJ and then asking it to
          make prompts. Oh well.</p>

          '
        raw: "> > And now we have StableVicuna - https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot\n\
          > > Speed at which models develop is astounding...\n> \n> Yeah but is it\
          \ unfiltered/uncensored?\n\nNo it's just like vanilla Vicuna when it comes\
          \ to filtering. Also I was kind of disappointed that Stability AI did not\
          \ train information on how to use Stable Diffusion image gen prompts etc.\
          \ Into the model. Maybe they can't or it's not what they are after. But\
          \ I would love to be able to ask the model to generate amazing prompts or\
          \ ask it questions for advice about using Stable Diffusion etc. Kind of\
          \ like how people have with Chat GTP and MJ by feeding MJ the information\
          \ about MJ and then asking it to make prompts. Oh well."
        updatedAt: '2023-04-29T18:51:36.039Z'
      numEdits: 0
      reactions: []
    id: 644d6738fa94e93b0eca8c34
    type: comment
  author: Goldenblood56
  content: "> > And now we have StableVicuna - https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot\n\
    > > Speed at which models develop is astounding...\n> \n> Yeah but is it unfiltered/uncensored?\n\
    \nNo it's just like vanilla Vicuna when it comes to filtering. Also I was kind\
    \ of disappointed that Stability AI did not train information on how to use Stable\
    \ Diffusion image gen prompts etc. Into the model. Maybe they can't or it's not\
    \ what they are after. But I would love to be able to ask the model to generate\
    \ amazing prompts or ask it questions for advice about using Stable Diffusion\
    \ etc. Kind of like how people have with Chat GTP and MJ by feeding MJ the information\
    \ about MJ and then asking it to make prompts. Oh well."
  created_at: 2023-04-29 17:51:36+00:00
  edited: false
  hidden: false
  id: 644d6738fa94e93b0eca8c34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-04-29T20:27:48.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>Sorry for clogging the thread, but since I''ve been talking about
          the RP dataset:</p>

          <p><a href="https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna">https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna</a></p>

          <p>Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts
          to him for putting the raw parquet together. I included the same merge_json
          script in the repo as in the SuperCOT Vicuna conversion and that will just
          merge json files together for easy dataset mashups. That''s one more dataset
          on the pile for followup finetunes assuming the base model comes out good.</p>

          '
        raw: 'Sorry for clogging the thread, but since I''ve been talking about the
          RP dataset:


          https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna


          Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts
          to him for putting the raw parquet together. I included the same merge_json
          script in the repo as in the SuperCOT Vicuna conversion and that will just
          merge json files together for easy dataset mashups. That''s one more dataset
          on the pile for followup finetunes assuming the base model comes out good.'
        updatedAt: '2023-04-29T20:27:48.968Z'
      numEdits: 0
      reactions: []
    id: 644d7dc40dc952d245a62470
    type: comment
  author: deleted
  content: 'Sorry for clogging the thread, but since I''ve been talking about the
    RP dataset:


    https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna


    Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts to him
    for putting the raw parquet together. I included the same merge_json script in
    the repo as in the SuperCOT Vicuna conversion and that will just merge json files
    together for easy dataset mashups. That''s one more dataset on the pile for followup
    finetunes assuming the base model comes out good.'
  created_at: 2023-04-29 19:27:48+00:00
  edited: false
  hidden: false
  id: 644d7dc40dc952d245a62470
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-29T21:28:55.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<blockquote>

          <p>Sorry for clogging the thread, but since I''ve been talking about the
          RP dataset:</p>

          <p><a href="https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna">https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna</a></p>

          <p>Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts
          to him for putting the raw parquet together. I included the same merge_json
          script in the repo as in the SuperCOT Vicuna conversion and that will just
          merge json files together for easy dataset mashups. That''s one more dataset
          on the pile for followup finetunes assuming the base model comes out good.</p>

          </blockquote>

          <p>Thanks. It''s funny how I read that and have no clue what any of that
          means. Not that I need to know it''s not for people like me. But you could
          have litterally made all of that up and I would be nodding like it makes
          sense. You could have typed " We hopscotched the penguin database so it''s
          inline with with the 509 type E blue goat milk. So now we have a raw merger
          for the space raspberrie model." I would be just as understanding. lol I
          would believe it too. I be like oh that''s good news!?</p>

          '
        raw: "> Sorry for clogging the thread, but since I've been talking about the\
          \ RP dataset:\n> \n> https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna\n\
          > \n> Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts\
          \ to him for putting the raw parquet together. I included the same merge_json\
          \ script in the repo as in the SuperCOT Vicuna conversion and that will\
          \ just merge json files together for easy dataset mashups. That's one more\
          \ dataset on the pile for followup finetunes assuming the base model comes\
          \ out good.\n\nThanks. It's funny how I read that and have no clue what\
          \ any of that means. Not that I need to know it's not for people like me.\
          \ But you could have litterally made all of that up and I would be nodding\
          \ like it makes sense. You could have typed \" We hopscotched the penguin\
          \ database so it's inline with with the 509 type E blue goat milk. So now\
          \ we have a raw merger for the space raspberrie model.\" I would be just\
          \ as understanding. lol I would believe it too. I be like oh that's good\
          \ news!?"
        updatedAt: '2023-04-29T21:30:08.107Z'
      numEdits: 1
      reactions: []
    id: 644d8c17328c1aa30e4d85b2
    type: comment
  author: Goldenblood56
  content: "> Sorry for clogging the thread, but since I've been talking about the\
    \ RP dataset:\n> \n> https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna\n\
    > \n> Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts\
    \ to him for putting the raw parquet together. I included the same merge_json\
    \ script in the repo as in the SuperCOT Vicuna conversion and that will just merge\
    \ json files together for easy dataset mashups. That's one more dataset on the\
    \ pile for followup finetunes assuming the base model comes out good.\n\nThanks.\
    \ It's funny how I read that and have no clue what any of that means. Not that\
    \ I need to know it's not for people like me. But you could have litterally made\
    \ all of that up and I would be nodding like it makes sense. You could have typed\
    \ \" We hopscotched the penguin database so it's inline with with the 509 type\
    \ E blue goat milk. So now we have a raw merger for the space raspberrie model.\"\
    \ I would be just as understanding. lol I would believe it too. I be like oh that's\
    \ good news!?"
  created_at: 2023-04-29 20:28:55+00:00
  edited: true
  hidden: false
  id: 644d8c17328c1aa30e4d85b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-04-29T21:52:02.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Sorry for clogging the thread, but since I''ve been talking about the
          RP dataset:</p>

          <p><a href="https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna">https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna</a></p>

          <p>Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts
          to him for putting the raw parquet together. I included the same merge_json
          script in the repo as in the SuperCOT Vicuna conversion and that will just
          merge json files together for easy dataset mashups. That''s one more dataset
          on the pile for followup finetunes assuming the base model comes out good.</p>

          </blockquote>

          <p>Thanks. It''s funny how I read that and have no clue what any of that
          means. Not that I need to know it''s not for people like me. But you could
          have litterally made all of that up and I would be nodding like it makes
          sense. You could have typed " We hopscotched the penguin database so it''s
          inline with with the 509 type E blue goat milk. So now we have a raw merger
          for the space raspberrie model." I would be just as understanding. lol I
          would believe it too. I be like oh that''s good news!?</p>

          </blockquote>

          <p>You sort of sound like a bot idk. But anyway, it''s better not to spam
          the discussion with our feelings and the likes, this is not a personal blog
          so please let the devs do their thing and do not post useless comments.
          Yeah, like this one of mine.</p>

          '
        raw: "> > Sorry for clogging the thread, but since I've been talking about\
          \ the RP dataset:\n> > \n> > https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna\n\
          > > \n> > Converted the Oniichat bluemoon RP dataset to Vicuna format. Big\
          \ shouts to him for putting the raw parquet together. I included the same\
          \ merge_json script in the repo as in the SuperCOT Vicuna conversion and\
          \ that will just merge json files together for easy dataset mashups. That's\
          \ one more dataset on the pile for followup finetunes assuming the base\
          \ model comes out good.\n> \n> Thanks. It's funny how I read that and have\
          \ no clue what any of that means. Not that I need to know it's not for people\
          \ like me. But you could have litterally made all of that up and I would\
          \ be nodding like it makes sense. You could have typed \" We hopscotched\
          \ the penguin database so it's inline with with the 509 type E blue goat\
          \ milk. So now we have a raw merger for the space raspberrie model.\" I\
          \ would be just as understanding. lol I would believe it too. I be like\
          \ oh that's good news!?\n\nYou sort of sound like a bot idk. But anyway,\
          \ it's better not to spam the discussion with our feelings and the likes,\
          \ this is not a personal blog so please let the devs do their thing and\
          \ do not post useless comments. Yeah, like this one of mine."
        updatedAt: '2023-04-29T21:52:02.860Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Goldenblood56
    id: 644d9182fa94e93b0ece3bea
    type: comment
  author: 9cento
  content: "> > Sorry for clogging the thread, but since I've been talking about the\
    \ RP dataset:\n> > \n> > https://huggingface.co/datasets/gozfarb/bluemoon_roleplay_300k_vicuna\n\
    > > \n> > Converted the Oniichat bluemoon RP dataset to Vicuna format. Big shouts\
    \ to him for putting the raw parquet together. I included the same merge_json\
    \ script in the repo as in the SuperCOT Vicuna conversion and that will just merge\
    \ json files together for easy dataset mashups. That's one more dataset on the\
    \ pile for followup finetunes assuming the base model comes out good.\n> \n> Thanks.\
    \ It's funny how I read that and have no clue what any of that means. Not that\
    \ I need to know it's not for people like me. But you could have litterally made\
    \ all of that up and I would be nodding like it makes sense. You could have typed\
    \ \" We hopscotched the penguin database so it's inline with with the 509 type\
    \ E blue goat milk. So now we have a raw merger for the space raspberrie model.\"\
    \ I would be just as understanding. lol I would believe it too. I be like oh that's\
    \ good news!?\n\nYou sort of sound like a bot idk. But anyway, it's better not\
    \ to spam the discussion with our feelings and the likes, this is not a personal\
    \ blog so please let the devs do their thing and do not post useless comments.\
    \ Yeah, like this one of mine."
  created_at: 2023-04-29 20:52:02+00:00
  edited: false
  hidden: false
  id: 644d9182fa94e93b0ece3bea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-05-06T15:34:00.000Z'
    data:
      edited: true
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>30B would be absolutely\
          \ GOAT, can't wait \U0001F680</p>\n</blockquote>\n<p>It would be great to\
          \ have access not only to 30B 4-bit but 30B 3-bit too, since 30B 4-bit llama\
          \ model, for instance, causes an OutOfMemory error on 24GB VRAM (3090/4090)\
          \ when dealing with large contexts. Meanwhile, the 30B 3-bit version works\
          \ smoothly. The 30B 3-bit model will allow more people without professional\
          \ GPU use it.</p>\n</blockquote>\n<p>Wouldn't it worsen the output quality\
          \ compared to 4-bit where it's barely noticeable?</p>\n</blockquote>\n<p>I\
          \ recalled that 4bit model without --groupsize 128 can work on 24Gb VRAM\
          \ with full context. So instead of 3-bit 128G I guess it's better to make\
          \  --true-sequential --act-order 4bit model, like here: <a href=\"https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-4bit/tree/main\"\
          >https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-4bit/tree/main</a></p>\n"
        raw: "> > > 30B would be absolutely GOAT, can't wait \U0001F680\n> > \n> >\
          \ It would be great to have access not only to 30B 4-bit but 30B 3-bit too,\
          \ since 30B 4-bit llama model, for instance, causes an OutOfMemory error\
          \ on 24GB VRAM (3090/4090) when dealing with large contexts. Meanwhile,\
          \ the 30B 3-bit version works smoothly. The 30B 3-bit model will allow more\
          \ people without professional GPU use it.\n> \n> Wouldn't it worsen the\
          \ output quality compared to 4-bit where it's barely noticeable?\n\nI recalled\
          \ that 4bit model without --groupsize 128 can work on 24Gb VRAM with full\
          \ context. So instead of 3-bit 128G I guess it's better to make  --true-sequential\
          \ --act-order 4bit model, like here: https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-4bit/tree/main"
        updatedAt: '2023-05-06T15:34:43.557Z'
      numEdits: 1
      reactions: []
    id: 6456736878c059b099b39905
    type: comment
  author: Kelheor
  content: "> > > 30B would be absolutely GOAT, can't wait \U0001F680\n> > \n> > It\
    \ would be great to have access not only to 30B 4-bit but 30B 3-bit too, since\
    \ 30B 4-bit llama model, for instance, causes an OutOfMemory error on 24GB VRAM\
    \ (3090/4090) when dealing with large contexts. Meanwhile, the 30B 3-bit version\
    \ works smoothly. The 30B 3-bit model will allow more people without professional\
    \ GPU use it.\n> \n> Wouldn't it worsen the output quality compared to 4-bit where\
    \ it's barely noticeable?\n\nI recalled that 4bit model without --groupsize 128\
    \ can work on 24Gb VRAM with full context. So instead of 3-bit 128G I guess it's\
    \ better to make  --true-sequential --act-order 4bit model, like here: https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-4bit/tree/main"
  created_at: 2023-05-06 14:34:00+00:00
  edited: true
  hidden: false
  id: 6456736878c059b099b39905
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-06T17:54:36.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Yes since some of these 13B uncensored Vicuna models have been going
          well. Cocktail one I like a lot. The original 1.1 free was great. Any ideas
          when the first 30B might be coming? I know it may be weeks or months away.
          But I would like to believe it''s coming in days/weeks as opposed to months.
          Even if it''s only to get something out there. You can always go back to
          13B for a while after. But it would be cool to at least have something to
          play around with and test to see how it matches up with 13B. To basically
          see if it''s even worth it. Again I am not trying to tell or command anyone
          do what I want. I deeply respect how development has been going. I am forever
          thankful. I can''t do any of this myself.</p>

          <p>As for using the 30B model. I am fine with using it in CPU only or splitting
          it. It won''t run on 16GB of VRAM regardless right? So a lot of people will
          have to use CPU only or split it. I think the 30B model I have used the
          most is something like 24-27GB of VRAM required. I run it split between
          GPU/CPU in Ooba. And in llama I run it on CPU only. It''s slow but works.  I
          like it. I think it''s 4-bit not 3-bit. Yes it''s called "alpaca-30b-4bit-128g"</p>

          '
        raw: 'Yes since some of these 13B uncensored Vicuna models have been going
          well. Cocktail one I like a lot. The original 1.1 free was great. Any ideas
          when the first 30B might be coming? I know it may be weeks or months away.
          But I would like to believe it''s coming in days/weeks as opposed to months.
          Even if it''s only to get something out there. You can always go back to
          13B for a while after. But it would be cool to at least have something to
          play around with and test to see how it matches up with 13B. To basically
          see if it''s even worth it. Again I am not trying to tell or command anyone
          do what I want. I deeply respect how development has been going. I am forever
          thankful. I can''t do any of this myself.


          As for using the 30B model. I am fine with using it in CPU only or splitting
          it. It won''t run on 16GB of VRAM regardless right? So a lot of people will
          have to use CPU only or split it. I think the 30B model I have used the
          most is something like 24-27GB of VRAM required. I run it split between
          GPU/CPU in Ooba. And in llama I run it on CPU only. It''s slow but works.  I
          like it. I think it''s 4-bit not 3-bit. Yes it''s called "alpaca-30b-4bit-128g"'
        updatedAt: '2023-05-06T18:29:17.116Z'
      numEdits: 7
      reactions: []
    id: 6456945cd10badc9555ed528
    type: comment
  author: Goldenblood56
  content: 'Yes since some of these 13B uncensored Vicuna models have been going well.
    Cocktail one I like a lot. The original 1.1 free was great. Any ideas when the
    first 30B might be coming? I know it may be weeks or months away. But I would
    like to believe it''s coming in days/weeks as opposed to months. Even if it''s
    only to get something out there. You can always go back to 13B for a while after.
    But it would be cool to at least have something to play around with and test to
    see how it matches up with 13B. To basically see if it''s even worth it. Again
    I am not trying to tell or command anyone do what I want. I deeply respect how
    development has been going. I am forever thankful. I can''t do any of this myself.


    As for using the 30B model. I am fine with using it in CPU only or splitting it.
    It won''t run on 16GB of VRAM regardless right? So a lot of people will have to
    use CPU only or split it. I think the 30B model I have used the most is something
    like 24-27GB of VRAM required. I run it split between GPU/CPU in Ooba. And in
    llama I run it on CPU only. It''s slow but works.  I like it. I think it''s 4-bit
    not 3-bit. Yes it''s called "alpaca-30b-4bit-128g"'
  created_at: 2023-05-06 16:54:36+00:00
  edited: true
  hidden: false
  id: 6456945cd10badc9555ed528
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-06T18:55:34.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Goldenblood56&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Goldenblood56\"\
          >@<span class=\"underline\">Goldenblood56</span></a></span>\n\n\t</span></span>\
          \ I'm not sure if I have the capability to finetune 30B (I don't actually\
          \ know what the requirements are), but I will investigate most likely next\
          \ week a bit. In the beginning I think I might try with the pure bluemoon\
          \ dataset, since it's not too massive and good training results can be achieved\
          \ relatively fast. I assume that training Vicuna 30B will most likely take\
          \ several days because the time one can continuously run things on the GPU\
          \ cluster is relatively limited (I would have to schedule several runs and\
          \ resume from checkpoints) and the per-device batch size has to be kept\
          \ small. 13B is convenient to iterate on since it can be done in 5-20 hours\
          \ depending on the dataset. If possible at all, I think Vicuna 30B will\
          \ come once the requirements are clear, and people are mostly happy with\
          \ the output from the vicuna-13b-free or cocktail.</p>\n<blockquote>\n<p>It\
          \ won't run on 16GB of VRAM regardless right?</p>\n</blockquote>\n<p>No,\
          \ as you mentioned, the requirement is around 20-30GB.</p>\n"
        raw: '@Goldenblood56 I''m not sure if I have the capability to finetune 30B
          (I don''t actually know what the requirements are), but I will investigate
          most likely next week a bit. In the beginning I think I might try with the
          pure bluemoon dataset, since it''s not too massive and good training results
          can be achieved relatively fast. I assume that training Vicuna 30B will
          most likely take several days because the time one can continuously run
          things on the GPU cluster is relatively limited (I would have to schedule
          several runs and resume from checkpoints) and the per-device batch size
          has to be kept small. 13B is convenient to iterate on since it can be done
          in 5-20 hours depending on the dataset. If possible at all, I think Vicuna
          30B will come once the requirements are clear, and people are mostly happy
          with the output from the vicuna-13b-free or cocktail.


          >It won''t run on 16GB of VRAM regardless right?


          No, as you mentioned, the requirement is around 20-30GB.'
        updatedAt: '2023-05-06T18:55:59.145Z'
      numEdits: 1
      reactions: []
    id: 6456a2a603625871eb792ad1
    type: comment
  author: reeducator
  content: '@Goldenblood56 I''m not sure if I have the capability to finetune 30B
    (I don''t actually know what the requirements are), but I will investigate most
    likely next week a bit. In the beginning I think I might try with the pure bluemoon
    dataset, since it''s not too massive and good training results can be achieved
    relatively fast. I assume that training Vicuna 30B will most likely take several
    days because the time one can continuously run things on the GPU cluster is relatively
    limited (I would have to schedule several runs and resume from checkpoints) and
    the per-device batch size has to be kept small. 13B is convenient to iterate on
    since it can be done in 5-20 hours depending on the dataset. If possible at all,
    I think Vicuna 30B will come once the requirements are clear, and people are mostly
    happy with the output from the vicuna-13b-free or cocktail.


    >It won''t run on 16GB of VRAM regardless right?


    No, as you mentioned, the requirement is around 20-30GB.'
  created_at: 2023-05-06 17:55:34+00:00
  edited: true
  hidden: false
  id: 6456a2a603625871eb792ad1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-06T19:14:12.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Yes and I run a few different 30B models so it''s fine if I can''t
          run it on my GPU only. And it''s fine if you can''t make 30B so be it. lol
          Good luck if you can do it great. If not like I said so be it.  Believe
          it or not I''ve ran a 65B model. Although it ran incredibly slow. I tried
          it just for fun. 30B was still fine and usable. 65B not so much.</p>

          '
        raw: Yes and I run a few different 30B models so it's fine if I can't run
          it on my GPU only. And it's fine if you can't make 30B so be it. lol Good
          luck if you can do it great. If not like I said so be it.  Believe it or
          not I've ran a 65B model. Although it ran incredibly slow. I tried it just
          for fun. 30B was still fine and usable. 65B not so much.
        updatedAt: '2023-05-06T19:20:52.938Z'
      numEdits: 3
      reactions: []
    id: 6456a70478c059b099b6adaf
    type: comment
  author: Goldenblood56
  content: Yes and I run a few different 30B models so it's fine if I can't run it
    on my GPU only. And it's fine if you can't make 30B so be it. lol Good luck if
    you can do it great. If not like I said so be it.  Believe it or not I've ran
    a 65B model. Although it ran incredibly slow. I tried it just for fun. 30B was
    still fine and usable. 65B not so much.
  created_at: 2023-05-06 18:14:12+00:00
  edited: true
  hidden: false
  id: 6456a70478c059b099b6adaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-05-06T23:55:47.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/reeducator\">@<span class=\"\
          underline\">reeducator</span></a></span>\n\n\t</span></span> Are there any\
          \ open source projects where you can install client and grant the power\
          \ of your machine to solve some problem, like training models? Maybe instead\
          \ of doing training solo, you can try to get some additional computing power\
          \ that way? Since I think it\u2019s the only way in future to compete with\
          \ big corporations with their hardware resources in order to train better\
          \ models.</p>\n"
        raw: "@reeducator Are there any open source projects where you can install\
          \ client and grant the power of your machine to solve some problem, like\
          \ training models? Maybe instead of doing training solo, you can try to\
          \ get some additional computing power that way? Since I think it\u2019s\
          \ the only way in future to compete with big corporations with their hardware\
          \ resources in order to train better models."
        updatedAt: '2023-05-06T23:55:47.167Z'
      numEdits: 0
      reactions: []
    id: 6456e90303625871eb7ce19a
    type: comment
  author: Kelheor
  content: "@reeducator Are there any open source projects where you can install client\
    \ and grant the power of your machine to solve some problem, like training models?\
    \ Maybe instead of doing training solo, you can try to get some additional computing\
    \ power that way? Since I think it\u2019s the only way in future to compete with\
    \ big corporations with their hardware resources in order to train better models."
  created_at: 2023-05-06 22:55:47+00:00
  edited: false
  hidden: false
  id: 6456e90303625871eb7ce19a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-07T00:16:51.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>There are a few projects, most notable are probably <a rel="nofollow"
          href="https://github.com/bigscience-workshop/petals">Petals</a> and the
          base project it uses <a rel="nofollow" href="https://github.com/learning-at-home/hivemind">Hivemind</a>,
          however there doesn''t seem to be much interested in them at the moment
          and no real pushes to try to adapt them to community projects for the moment.
          It wouldn''t be too bad if KoboldAI rolled in something like Hivemind so
          the same people who dedicated GPU to hosted models could dedicate it to
          training.</p>

          '
        raw: There are a few projects, most notable are probably [Petals](https://github.com/bigscience-workshop/petals)
          and the base project it uses [Hivemind](https://github.com/learning-at-home/hivemind),
          however there doesn't seem to be much interested in them at the moment and
          no real pushes to try to adapt them to community projects for the moment.
          It wouldn't be too bad if KoboldAI rolled in something like Hivemind so
          the same people who dedicated GPU to hosted models could dedicate it to
          training.
        updatedAt: '2023-05-07T00:16:51.395Z'
      numEdits: 0
      reactions: []
    id: 6456edf378c059b099ba69e5
    type: comment
  author: deleted
  content: There are a few projects, most notable are probably [Petals](https://github.com/bigscience-workshop/petals)
    and the base project it uses [Hivemind](https://github.com/learning-at-home/hivemind),
    however there doesn't seem to be much interested in them at the moment and no
    real pushes to try to adapt them to community projects for the moment. It wouldn't
    be too bad if KoboldAI rolled in something like Hivemind so the same people who
    dedicated GPU to hosted models could dedicate it to training.
  created_at: 2023-05-06 23:16:51+00:00
  edited: false
  hidden: false
  id: 6456edf378c059b099ba69e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-07T00:25:33.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I know there''s hivemind at least <a rel="nofollow" href="https://github.com/learning-at-home/hivemind">https://github.com/learning-at-home/hivemind</a>,
          but I don''t know how well it works in practice. Given enough 24GB consumer
          GPUs though, achieving something with it might be pretty plausible though.
          I noticed that with certain configuration of training batch size vs gradient
          accumulation steps the used memory per GPU was around that much. We''d need
          a lot of people with lot of 24GB VRAM GPUs though, not sure where we''d
          get that many people who are willing to contribute.</p>

          '
        raw: I know there's hivemind at least https://github.com/learning-at-home/hivemind,
          but I don't know how well it works in practice. Given enough 24GB consumer
          GPUs though, achieving something with it might be pretty plausible though.
          I noticed that with certain configuration of training batch size vs gradient
          accumulation steps the used memory per GPU was around that much. We'd need
          a lot of people with lot of 24GB VRAM GPUs though, not sure where we'd get
          that many people who are willing to contribute.
        updatedAt: '2023-05-07T00:25:33.259Z'
      numEdits: 0
      reactions: []
    id: 6456effd78c059b099ba8467
    type: comment
  author: reeducator
  content: I know there's hivemind at least https://github.com/learning-at-home/hivemind,
    but I don't know how well it works in practice. Given enough 24GB consumer GPUs
    though, achieving something with it might be pretty plausible though. I noticed
    that with certain configuration of training batch size vs gradient accumulation
    steps the used memory per GPU was around that much. We'd need a lot of people
    with lot of 24GB VRAM GPUs though, not sure where we'd get that many people who
    are willing to contribute.
  created_at: 2023-05-06 23:25:33+00:00
  edited: false
  hidden: false
  id: 6456effd78c059b099ba8467
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-07T00:30:02.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Awesome uncensored 1T model when? You can probably train the first
          uncensored 1T model I hear it can run on around 800GB VRAM if it''s 3-4Bit.
          It takes A LOT more to train it though that''s the only down side.</p>

          '
        raw: Awesome uncensored 1T model when? You can probably train the first uncensored
          1T model I hear it can run on around 800GB VRAM if it's 3-4Bit. It takes
          A LOT more to train it though that's the only down side.
        updatedAt: '2023-05-07T00:30:02.524Z'
      numEdits: 0
      reactions: []
    id: 6456f10a03625871eb7d4910
    type: comment
  author: Goldenblood56
  content: Awesome uncensored 1T model when? You can probably train the first uncensored
    1T model I hear it can run on around 800GB VRAM if it's 3-4Bit. It takes A LOT
    more to train it though that's the only down side.
  created_at: 2023-05-06 23:30:02+00:00
  edited: false
  hidden: false
  id: 6456f10a03625871eb7d4910
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-07T00:47:31.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>When it rains H or A100s, then we can consider it \o/</p>

          '
        raw: When it rains H or A100s, then we can consider it \o/
        updatedAt: '2023-05-07T00:47:31.136Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Goldenblood56
    id: 6456f523cd6567f52fbd9ba8
    type: comment
  author: reeducator
  content: When it rains H or A100s, then we can consider it \o/
  created_at: 2023-05-06 23:47:31+00:00
  edited: false
  hidden: false
  id: 6456f523cd6567f52fbd9ba8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-05-07T02:27:11.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-05-07T13:39:32.605Z'
      numEdits: 0
      reactions: []
    id: 64570c7f03625871eb7eb884
    type: comment
  author: Kelheor
  content: This comment has been hidden
  created_at: 2023-05-07 01:27:11+00:00
  edited: true
  hidden: true
  id: 64570c7f03625871eb7eb884
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-05-07T23:40:25.000Z'
    data:
      edited: true
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/reeducator\">@<span class=\"\
          underline\">reeducator</span></a></span>\n\n\t</span></span> I know that\
          \ you said that you have no plans on making 7b version, but still, if it\u2019\
          s not too long to train, maybe it\u2019s possible to add such version later,\
          \ when all other versions will be done? I managed to use ggml vicuna 7b\
          \ q4_0 on Samsung galaxy s23 Ultra. It is not very fast, but still usable.\
          \ Having such model running in your pocket is pretty cool and might be useful\
          \ in some cases (like when internet is not available).</p>\n"
        raw: "@reeducator I know that you said that you have no plans on making 7b\
          \ version, but still, if it\u2019s not too long to train, maybe it\u2019\
          s possible to add such version later, when all other versions will be done?\
          \ I managed to use ggml vicuna 7b q4_0 on Samsung galaxy s23 Ultra. It is\
          \ not very fast, but still usable. Having such model running in your pocket\
          \ is pretty cool and might be useful in some cases (like when internet is\
          \ not available)."
        updatedAt: '2023-05-07T23:41:08.057Z'
      numEdits: 1
      reactions: []
    id: 645836e9116c6b3c62e521f7
    type: comment
  author: Kelheor
  content: "@reeducator I know that you said that you have no plans on making 7b version,\
    \ but still, if it\u2019s not too long to train, maybe it\u2019s possible to add\
    \ such version later, when all other versions will be done? I managed to use ggml\
    \ vicuna 7b q4_0 on Samsung galaxy s23 Ultra. It is not very fast, but still usable.\
    \ Having such model running in your pocket is pretty cool and might be useful\
    \ in some cases (like when internet is not available)."
  created_at: 2023-05-07 22:40:25+00:00
  edited: true
  hidden: false
  id: 645836e9116c6b3c62e521f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-08T00:00:55.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Kelheor&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Kelheor\">@<span class=\"\
          underline\">Kelheor</span></a></span>\n\n\t</span></span> yeah, I think\
          \ the idea has been that once we have something that we're more or less\
          \ happy with, we do the other model sizes 7B and 30B (if possible). But\
          \ it's true that it probably doesn't take too long to train compared to\
          \ 13B.</p>\n<p>How it works here is that whenever we train something, I\
          \ have to queue up for the GPU time on the cluster - it's actually mostly\
          \ that part that takes long time, and often longer than the training itself.\
          \ Whenever there's a training slot, one has to decide whether there's a\
          \ need to iterate on something that we've been working on to improve, or\
          \ create something entirely new. So far we've figured that we can probably\
          \ make most of it by iterating on the datasets and the 13B to create a more\
          \ useful vicuna, which is then easy to compare with our previous results.</p>\n\
          <p>But because the 7B most likely wouldn't take too long to train, one might\
          \ be able to chain it together with some other 13B model training and fit\
          \ it within the same timeframe. Bluemoon 13B for example takes only a few\
          \ hours to train, much less than the typical allotted GPU time. What I possibly\
          \ could do is that next time we see fit to train the next bluemoon, I can\
          \ try squeeze in the Vicuna 7B within that same slot. Let's see!</p>\n"
        raw: '@Kelheor yeah, I think the idea has been that once we have something
          that we''re more or less happy with, we do the other model sizes 7B and
          30B (if possible). But it''s true that it probably doesn''t take too long
          to train compared to 13B.


          How it works here is that whenever we train something, I have to queue up
          for the GPU time on the cluster - it''s actually mostly that part that takes
          long time, and often longer than the training itself. Whenever there''s
          a training slot, one has to decide whether there''s a need to iterate on
          something that we''ve been working on to improve, or create something entirely
          new. So far we''ve figured that we can probably make most of it by iterating
          on the datasets and the 13B to create a more useful vicuna, which is then
          easy to compare with our previous results.


          But because the 7B most likely wouldn''t take too long to train, one might
          be able to chain it together with some other 13B model training and fit
          it within the same timeframe. Bluemoon 13B for example takes only a few
          hours to train, much less than the typical allotted GPU time. What I possibly
          could do is that next time we see fit to train the next bluemoon, I can
          try squeeze in the Vicuna 7B within that same slot. Let''s see!'
        updatedAt: '2023-05-08T00:01:28.442Z'
      numEdits: 1
      reactions: []
    id: 64583bb70332c1fb59f7d11f
    type: comment
  author: reeducator
  content: '@Kelheor yeah, I think the idea has been that once we have something that
    we''re more or less happy with, we do the other model sizes 7B and 30B (if possible).
    But it''s true that it probably doesn''t take too long to train compared to 13B.


    How it works here is that whenever we train something, I have to queue up for
    the GPU time on the cluster - it''s actually mostly that part that takes long
    time, and often longer than the training itself. Whenever there''s a training
    slot, one has to decide whether there''s a need to iterate on something that we''ve
    been working on to improve, or create something entirely new. So far we''ve figured
    that we can probably make most of it by iterating on the datasets and the 13B
    to create a more useful vicuna, which is then easy to compare with our previous
    results.


    But because the 7B most likely wouldn''t take too long to train, one might be
    able to chain it together with some other 13B model training and fit it within
    the same timeframe. Bluemoon 13B for example takes only a few hours to train,
    much less than the typical allotted GPU time. What I possibly could do is that
    next time we see fit to train the next bluemoon, I can try squeeze in the Vicuna
    7B within that same slot. Let''s see!'
  created_at: 2023-05-07 23:00:55+00:00
  edited: true
  hidden: false
  id: 64583bb70332c1fb59f7d11f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: open
target_branch: null
title: The V4 is here
