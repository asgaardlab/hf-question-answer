!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xzuyn
conflicting_files: null
created_at: 2023-05-01 17:07:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
      fullname: xzuyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xzuyn
      type: user
    createdAt: '2023-05-01T18:07:45.000Z'
    data:
      edited: false
      editors:
      - xzuyn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
          fullname: xzuyn
          isHf: false
          isPro: false
          name: xzuyn
          type: user
        html: '<p>YellowRose#1776 on the Kobold discord did some testing on Pygmalion
          7B to see which gives the best perplexity, and he found that act-order is
          best.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/OKE1RzPVTLaVxEy64skoz.png"><img
          alt="actorder.png" src="https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/OKE1RzPVTLaVxEy64skoz.png"></a></p>

          '
        raw: "YellowRose#1776 on the Kobold discord did some testing on Pygmalion\
          \ 7B to see which gives the best perplexity, and he found that act-order\
          \ is best.\r\n\r\n![actorder.png](https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/OKE1RzPVTLaVxEy64skoz.png)\r\
          \n"
        updatedAt: '2023-05-01T18:07:45.896Z'
      numEdits: 0
      reactions: []
    id: 644ffff102e7c57d395f2a47
    type: comment
  author: xzuyn
  content: "YellowRose#1776 on the Kobold discord did some testing on Pygmalion 7B\
    \ to see which gives the best perplexity, and he found that act-order is best.\r\
    \n\r\n![actorder.png](https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/OKE1RzPVTLaVxEy64skoz.png)\r\
    \n"
  created_at: 2023-05-01 17:07:45+00:00
  edited: false
  hidden: false
  id: 644ffff102e7c57d395f2a47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-01T18:49:25.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>I''ll start by saying I''m not against having a Triton GPTQ quant.</p>

          <p>I''d really like to see <code>--act-order --true-sequential --groupsize
          128</code> versus  <code>--true-sequential --groupsize 128</code>. And I''d
          like to see that testing against 13B and 30B models. It''s my understanding
          that perplexity gains are fairly parameter dependent for --act-order and
          the gains are around ~0.1 at 13B (please correct me if I''m wrong). Any
          additional info would be great. I''m also curious what the meaningful output
          shift results are of a 0.1 perplexity gain (I know people are saying act-order
          helps with rare contraction issues around words like couldn''t), so if anyone
          has some good examples of how that 0.1 manifests I think that would be good
          to share around.</p>

          '
        raw: 'I''ll start by saying I''m not against having a Triton GPTQ quant.


          I''d really like to see `--act-order --true-sequential --groupsize 128`
          versus  `--true-sequential --groupsize 128`. And I''d like to see that testing
          against 13B and 30B models. It''s my understanding that perplexity gains
          are fairly parameter dependent for --act-order and the gains are around
          ~0.1 at 13B (please correct me if I''m wrong). Any additional info would
          be great. I''m also curious what the meaningful output shift results are
          of a 0.1 perplexity gain (I know people are saying act-order helps with
          rare contraction issues around words like couldn''t), so if anyone has some
          good examples of how that 0.1 manifests I think that would be good to share
          around.'
        updatedAt: '2023-05-01T18:49:25.565Z'
      numEdits: 0
      reactions: []
    id: 645009b528774bd665dd39b8
    type: comment
  author: deleted
  content: 'I''ll start by saying I''m not against having a Triton GPTQ quant.


    I''d really like to see `--act-order --true-sequential --groupsize 128` versus  `--true-sequential
    --groupsize 128`. And I''d like to see that testing against 13B and 30B models.
    It''s my understanding that perplexity gains are fairly parameter dependent for
    --act-order and the gains are around ~0.1 at 13B (please correct me if I''m wrong).
    Any additional info would be great. I''m also curious what the meaningful output
    shift results are of a 0.1 perplexity gain (I know people are saying act-order
    helps with rare contraction issues around words like couldn''t), so if anyone
    has some good examples of how that 0.1 manifests I think that would be good to
    share around.'
  created_at: 2023-05-01 17:49:25+00:00
  edited: false
  hidden: false
  id: 645009b528774bd665dd39b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
      fullname: xzuyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xzuyn
      type: user
    createdAt: '2023-05-01T18:54:23.000Z'
    data:
      edited: false
      editors:
      - xzuyn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
          fullname: xzuyn
          isHf: false
          isPro: false
          name: xzuyn
          type: user
        html: "<blockquote>\n<p>Any additional info would be great. I'm also curious\
          \ what the meaningful output shift results are of a 0.1 perplexity gain\
          \ (I know people are saying act-order helps with rare contraction issues\
          \ around words like couldn't), so if anyone has some good examples of how\
          \ that 0.1 manifests I think that would be good to share around.</p>\n</blockquote>\n\
          <p>Maybe <span data-props=\"{&quot;user&quot;:&quot;Monero&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Monero\">@<span class=\"\
          underline\">Monero</span></a></span>\n\n\t</span></span>/YellowRose#1776\
          \ could chime in and give some more info.</p>\n"
        raw: '> Any additional info would be great. I''m also curious what the meaningful
          output shift results are of a 0.1 perplexity gain (I know people are saying
          act-order helps with rare contraction issues around words like couldn''t),
          so if anyone has some good examples of how that 0.1 manifests I think that
          would be good to share around.


          Maybe @Monero/YellowRose#1776 could chime in and give some more info.'
        updatedAt: '2023-05-01T18:54:23.700Z'
      numEdits: 0
      reactions: []
    id: 64500adfd5f7dafcfa664ba6
    type: comment
  author: xzuyn
  content: '> Any additional info would be great. I''m also curious what the meaningful
    output shift results are of a 0.1 perplexity gain (I know people are saying act-order
    helps with rare contraction issues around words like couldn''t), so if anyone
    has some good examples of how that 0.1 manifests I think that would be good to
    share around.


    Maybe @Monero/YellowRose#1776 could chime in and give some more info.'
  created_at: 2023-05-01 17:54:23+00:00
  edited: false
  hidden: false
  id: 64500adfd5f7dafcfa664ba6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-05-01T20:22:28.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<p>Those results are for the instruct version Pygmalion Metharme 7b,
          In my experience the best combination depends on the model<br>I''m not familiar
          with perplexity results beyond that atm, sorry!</p>

          '
        raw: "Those results are for the instruct version Pygmalion Metharme 7b, In\
          \ my experience the best combination depends on the model \nI'm not familiar\
          \ with perplexity results beyond that atm, sorry!"
        updatedAt: '2023-05-01T20:22:28.253Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xzuyn
    id: 64501f8420ba3e3e4bf0d1b0
    type: comment
  author: Monero
  content: "Those results are for the instruct version Pygmalion Metharme 7b, In my\
    \ experience the best combination depends on the model \nI'm not familiar with\
    \ perplexity results beyond that atm, sorry!"
  created_at: 2023-05-01 19:22:28+00:00
  edited: false
  hidden: false
  id: 64501f8420ba3e3e4bf0d1b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-05-01T20:35:49.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      isReport: false
      latest:
        html: '<p>No worries and thanks for compiling the data you did! Hopefully
          someone will compile meaningful differences in outputs at some point.</p>

          <p>I have been watching various development efforts around really focusing
          on dropping perplexity as low as it''ll go and giving back inference speed
          and RAM/VRAM for sake of 0.1 perplexity gains and it''s got my dev brain
          wondering about what the specific wins are so people can make informed decisions
          about the value of various quants at various parameter counts. Right now
          it''s all pretty loosey goosey which is fine, but those sorts of decisions
          might really start to matter when people are budgeting for non-hobby projects
          around these models and their quantized versions. If I''m a project lead,
          I want inference speed as high as possible on the least hardware possible
          if there''s negligible difference between 6.0 and 5.9 perplexity. That''s
          less applicable for GPTQ since Triton runs on Linux, where most non-hobby
          projects will live, but as ggml advances and gets GPU inference figured
          out, it could come into play with their giant menagerie of quantization
          formats.</p>

          <p>I''ll also dream of someone getting --act-order working on CUDA so this
          entire discussion becomes pointless for GPTQ quants. I messed with it a
          bit but I don''t have enough free time to dig into it so didn''t make much
          progress.</p>

          '
        raw: 'No worries and thanks for compiling the data you did! Hopefully someone
          will compile meaningful differences in outputs at some point.


          I have been watching various development efforts around really focusing
          on dropping perplexity as low as it''ll go and giving back inference speed
          and RAM/VRAM for sake of 0.1 perplexity gains and it''s got my dev brain
          wondering about what the specific wins are so people can make informed decisions
          about the value of various quants at various parameter counts. Right now
          it''s all pretty loosey goosey which is fine, but those sorts of decisions
          might really start to matter when people are budgeting for non-hobby projects
          around these models and their quantized versions. If I''m a project lead,
          I want inference speed as high as possible on the least hardware possible
          if there''s negligible difference between 6.0 and 5.9 perplexity. That''s
          less applicable for GPTQ since Triton runs on Linux, where most non-hobby
          projects will live, but as ggml advances and gets GPU inference figured
          out, it could come into play with their giant menagerie of quantization
          formats.


          I''ll also dream of someone getting --act-order working on CUDA so this
          entire discussion becomes pointless for GPTQ quants. I messed with it a
          bit but I don''t have enough free time to dig into it so didn''t make much
          progress.'
        updatedAt: '2023-05-01T20:35:49.568Z'
      numEdits: 0
      reactions: []
    id: 645022a5d5f7dafcfa682160
    type: comment
  author: deleted
  content: 'No worries and thanks for compiling the data you did! Hopefully someone
    will compile meaningful differences in outputs at some point.


    I have been watching various development efforts around really focusing on dropping
    perplexity as low as it''ll go and giving back inference speed and RAM/VRAM for
    sake of 0.1 perplexity gains and it''s got my dev brain wondering about what the
    specific wins are so people can make informed decisions about the value of various
    quants at various parameter counts. Right now it''s all pretty loosey goosey which
    is fine, but those sorts of decisions might really start to matter when people
    are budgeting for non-hobby projects around these models and their quantized versions.
    If I''m a project lead, I want inference speed as high as possible on the least
    hardware possible if there''s negligible difference between 6.0 and 5.9 perplexity.
    That''s less applicable for GPTQ since Triton runs on Linux, where most non-hobby
    projects will live, but as ggml advances and gets GPU inference figured out, it
    could come into play with their giant menagerie of quantization formats.


    I''ll also dream of someone getting --act-order working on CUDA so this entire
    discussion becomes pointless for GPTQ quants. I messed with it a bit but I don''t
    have enough free time to dig into it so didn''t make much progress.'
  created_at: 2023-05-01 19:35:49+00:00
  edited: false
  hidden: false
  id: 645022a5d5f7dafcfa682160
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-02T04:43:57.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Act order works without group size on the ooba version of gptq.
          Just don''t encode with the newest cuda branch as I think it changes the
          format yet again for the 3rd time. </p>

          <p>So choices are either act order + true sequential or group size. Hence
          they get nonsense perplexity when they used them together.</p>

          '
        raw: "Act order works without group size on the ooba version of gptq. Just\
          \ don't encode with the newest cuda branch as I think it changes the format\
          \ yet again for the 3rd time. \n\nSo choices are either act order + true\
          \ sequential or group size. Hence they get nonsense perplexity when they\
          \ used them together."
        updatedAt: '2023-05-02T04:43:57.291Z'
      numEdits: 0
      reactions: []
    id: 6450950d28774bd665e7dcdf
    type: comment
  author: autobots
  content: "Act order works without group size on the ooba version of gptq. Just don't\
    \ encode with the newest cuda branch as I think it changes the format yet again\
    \ for the 3rd time. \n\nSo choices are either act order + true sequential or group\
    \ size. Hence they get nonsense perplexity when they used them together."
  created_at: 2023-05-02 03:43:57+00:00
  edited: false
  hidden: false
  id: 6450950d28774bd665e7dcdf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: open
target_branch: null
title: You may want to add an "act-order" GPTQ quantization.
