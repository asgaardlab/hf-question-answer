!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mancub
conflicting_files: null
created_at: 2023-04-30 21:55:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-04-30T22:55:48.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p>I downloaded the tokenizer files that <span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/reeducator\"\
          >@<span class=\"underline\">reeducator</span></a></span>\n\n\t</span></span>\
          \ uploaded for the safetensors model and noticed a big drop in inference\
          \ speed from what I previously had been using (files from TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\
          \ model).</p>\n<p>When using the default files included with this model,\
          \ I get 1.5-1.7 tokens/s, while with the files from TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\
          \ I get 5.7 to over 6 tokens/s. This is on a RTX 3090 with CUDA.</p>\n<p>At\
          \ first I thought there could've been some regression with oogabooga/text-generation-webui\
          \ so I went back a week, without any change. I do not know enough about\
          \ the inner workings of these models to understand what could be the problem,\
          \ so perhaps someone else can confirm this, or let me know if I am doing\
          \ something wrong here?</p>\n"
        raw: "I downloaded the tokenizer files that @reeducator uploaded for the safetensors\
          \ model and noticed a big drop in inference speed from what I previously\
          \ had been using (files from TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g model).\r\
          \n\r\nWhen using the default files included with this model, I get 1.5-1.7\
          \ tokens/s, while with the files from TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\
          \ I get 5.7 to over 6 tokens/s. This is on a RTX 3090 with CUDA.\r\n\r\n\
          At first I thought there could've been some regression with oogabooga/text-generation-webui\
          \ so I went back a week, without any change. I do not know enough about\
          \ the inner workings of these models to understand what could be the problem,\
          \ so perhaps someone else can confirm this, or let me know if I am doing\
          \ something wrong here?\r\n"
        updatedAt: '2023-04-30T22:55:48.189Z'
      numEdits: 0
      reactions: []
    id: 644ef1f467a3dd3d072985ce
    type: comment
  author: mancub
  content: "I downloaded the tokenizer files that @reeducator uploaded for the safetensors\
    \ model and noticed a big drop in inference speed from what I previously had been\
    \ using (files from TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g model).\r\n\r\nWhen\
    \ using the default files included with this model, I get 1.5-1.7 tokens/s, while\
    \ with the files from TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g I get 5.7 to over\
    \ 6 tokens/s. This is on a RTX 3090 with CUDA.\r\n\r\nAt first I thought there\
    \ could've been some regression with oogabooga/text-generation-webui so I went\
    \ back a week, without any change. I do not know enough about the inner workings\
    \ of these models to understand what could be the problem, so perhaps someone\
    \ else can confirm this, or let me know if I am doing something wrong here?\r\n"
  created_at: 2023-04-30 21:55:48+00:00
  edited: false
  hidden: false
  id: 644ef1f467a3dd3d072985ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3eb883ef82ead0a35e7b3a7c011e443a.svg
      fullname: S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PushTheBrAIkes
      type: user
    createdAt: '2023-05-01T04:22:02.000Z'
    data:
      edited: false
      editors:
      - PushTheBrAIkes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3eb883ef82ead0a35e7b3a7c011e443a.svg
          fullname: S
          isHf: false
          isPro: false
          name: PushTheBrAIkes
          type: user
        html: '<p>I noticed the same thing on my RTX 3060, I changed back to TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
          model and speed is back to 4-5 tokens/s.</p>

          '
        raw: I noticed the same thing on my RTX 3060, I changed back to TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
          model and speed is back to 4-5 tokens/s.
        updatedAt: '2023-05-01T04:22:02.828Z'
      numEdits: 0
      reactions: []
    id: 644f3e6acf72e60a5b8a7f3a
    type: comment
  author: PushTheBrAIkes
  content: I noticed the same thing on my RTX 3060, I changed back to TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
    model and speed is back to 4-5 tokens/s.
  created_at: 2023-05-01 03:22:02+00:00
  edited: false
  hidden: false
  id: 644f3e6acf72e60a5b8a7f3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-05-01T05:47:14.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>You guys have to set use_cache to true in the config.json - that
          is very important for the speed. This fixes the slow speeds.</p>

          '
        raw: You guys have to set use_cache to true in the config.json - that is very
          important for the speed. This fixes the slow speeds.
        updatedAt: '2023-05-01T05:47:14.004Z'
      numEdits: 0
      reactions: []
    id: 644f5262cf72e60a5b8be3e6
    type: comment
  author: CyberTimon
  content: You guys have to set use_cache to true in the config.json - that is very
    important for the speed. This fixes the slow speeds.
  created_at: 2023-05-01 04:47:14+00:00
  edited: false
  hidden: false
  id: 644f5262cf72e60a5b8be3e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-01T18:10:10.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\"\
          >@<span class=\"underline\">CyberTimon</span></a></span>\n\n\t</span></span>\
          \ for the heads up, and thanks <span data-props=\"{&quot;user&quot;:&quot;reeducator&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/reeducator\"\
          >@<span class=\"underline\">reeducator</span></a></span>\n\n\t</span></span>\
          \  for updating the config.json so quickly !</p>\n"
        raw: Thanks @CyberTimon for the heads up, and thanks @reeducator  for updating
          the config.json so quickly !
        updatedAt: '2023-05-01T18:10:10.216Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64500082577838187eff5158
    id: 64500082577838187eff5156
    type: comment
  author: mancub
  content: Thanks @CyberTimon for the heads up, and thanks @reeducator  for updating
    the config.json so quickly !
  created_at: 2023-05-01 17:10:10+00:00
  edited: false
  hidden: false
  id: 64500082577838187eff5156
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-01T18:10:10.000Z'
    data:
      status: closed
    id: 64500082577838187eff5158
    type: status-change
  author: mancub
  created_at: 2023-05-01 17:10:10+00:00
  id: 64500082577838187eff5158
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: closed
target_branch: null
title: Slow inference speed using default vs 3rd party tokenizer files
