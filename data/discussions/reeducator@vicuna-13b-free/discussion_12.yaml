!!python/object:huggingface_hub.community.DiscussionWithDetails
author: waynekenney
conflicting_files: null
created_at: 2023-04-24 05:07:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
      fullname: Wayne Kenney Jr.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waynekenney
      type: user
    createdAt: '2023-04-24T06:07:29.000Z'
    data:
      edited: false
      editors:
      - waynekenney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
          fullname: Wayne Kenney Jr.
          isHf: false
          isPro: false
          name: waynekenney
          type: user
        html: '<p>I''m surprised no one has asked this, but there''s no documentation
          anywhere for this model. I''ve tried asking Open.Assisstant and Chat GPT4
          with no guidance. My question is, why are there two bin files with random
          names, and when I attempt to load it into oobabooga, it attempts to load
          "pytorch_model-00001-of-00003.bin"? What''s with the bin file names and
          why did the trainer of this model fail to put any instructions anywhere
          for using it and loading into a WebUI?<br>So, I''ve generated a model merging
          script with Chat GPT to merge the model into one file, I''ve done that and
          renamed it pytorch-model. It still fails to load. I''ve renamed the largest
          model 00001 and made the smallest model the 00002 model of the 00003 model
          set, still doesn''t load. I''ve also tried to load it with this command:<br>python
          server.py --model vicuna-13b-free-4bit-128g --wbits 4 --groupsize 128</p>

          <p>Can someone please explain to me how to use this software? It''s incredible
          that someone trained that dataset. And I''d love to experience what reeducator
          has contributed to the community. I just need at least a hint or something:</p>

          <p>vicuna-13b-free-f16.bin<br>vicuna-13b-free-q4_0.bin</p>

          <p>These two bin files that I''ve manually downloaded via my browser because
          my git client constantly crashes isn''t exactly a recognizable pattern...
          I was really hoping someone else would have asked this question before me,
          so I wouldn''t be the idiot missing whatever obvious instruction or concept
          I''ve missed while experimenting with ai (Note: I''m definitely a noob with
          chatbots using ai). But, I wanted to test out this model since I''ve witnessed
          the data set.</p>

          <p>Any help is super greatly appreciated. Thanks guys! &lt;3  I also wanted
          to say I''m very sorry if my question is really dumb because I''m missing
          something obvious. Thanks a lot for reading my discussion post. I commend
          anyone willing to help me. I wish I was as smart as everyone else.</p>

          '
        raw: "I'm surprised no one has asked this, but there's no documentation anywhere\
          \ for this model. I've tried asking Open.Assisstant and Chat GPT4 with no\
          \ guidance. My question is, why are there two bin files with random names,\
          \ and when I attempt to load it into oobabooga, it attempts to load \"pytorch_model-00001-of-00003.bin\"\
          ? What's with the bin file names and why did the trainer of this model fail\
          \ to put any instructions anywhere for using it and loading into a WebUI?\r\
          \nSo, I've generated a model merging script with Chat GPT to merge the model\
          \ into one file, I've done that and renamed it pytorch-model. It still fails\
          \ to load. I've renamed the largest model 00001 and made the smallest model\
          \ the 00002 model of the 00003 model set, still doesn't load. I've also\
          \ tried to load it with this command:\r\npython server.py --model vicuna-13b-free-4bit-128g\
          \ --wbits 4 --groupsize 128\r\n\r\nCan someone please explain to me how\
          \ to use this software? It's incredible that someone trained that dataset.\
          \ And I'd love to experience what reeducator has contributed to the community.\
          \ I just need at least a hint or something:\r\n\r\nvicuna-13b-free-f16.bin\r\
          \nvicuna-13b-free-q4_0.bin\r\n\r\nThese two bin files that I've manually\
          \ downloaded via my browser because my git client constantly crashes isn't\
          \ exactly a recognizable pattern... I was really hoping someone else would\
          \ have asked this question before me, so I wouldn't be the idiot missing\
          \ whatever obvious instruction or concept I've missed while experimenting\
          \ with ai (Note: I'm definitely a noob with chatbots using ai). But, I wanted\
          \ to test out this model since I've witnessed the data set.\r\n\r\nAny help\
          \ is super greatly appreciated. Thanks guys! <3  I also wanted to say I'm\
          \ very sorry if my question is really dumb because I'm missing something\
          \ obvious. Thanks a lot for reading my discussion post. I commend anyone\
          \ willing to help me. I wish I was as smart as everyone else."
        updatedAt: '2023-04-24T06:07:29.684Z'
      numEdits: 0
      reactions: []
    id: 64461ca19f76e8788567f582
    type: comment
  author: waynekenney
  content: "I'm surprised no one has asked this, but there's no documentation anywhere\
    \ for this model. I've tried asking Open.Assisstant and Chat GPT4 with no guidance.\
    \ My question is, why are there two bin files with random names, and when I attempt\
    \ to load it into oobabooga, it attempts to load \"pytorch_model-00001-of-00003.bin\"\
    ? What's with the bin file names and why did the trainer of this model fail to\
    \ put any instructions anywhere for using it and loading into a WebUI?\r\nSo,\
    \ I've generated a model merging script with Chat GPT to merge the model into\
    \ one file, I've done that and renamed it pytorch-model. It still fails to load.\
    \ I've renamed the largest model 00001 and made the smallest model the 00002 model\
    \ of the 00003 model set, still doesn't load. I've also tried to load it with\
    \ this command:\r\npython server.py --model vicuna-13b-free-4bit-128g --wbits\
    \ 4 --groupsize 128\r\n\r\nCan someone please explain to me how to use this software?\
    \ It's incredible that someone trained that dataset. And I'd love to experience\
    \ what reeducator has contributed to the community. I just need at least a hint\
    \ or something:\r\n\r\nvicuna-13b-free-f16.bin\r\nvicuna-13b-free-q4_0.bin\r\n\
    \r\nThese two bin files that I've manually downloaded via my browser because my\
    \ git client constantly crashes isn't exactly a recognizable pattern... I was\
    \ really hoping someone else would have asked this question before me, so I wouldn't\
    \ be the idiot missing whatever obvious instruction or concept I've missed while\
    \ experimenting with ai (Note: I'm definitely a noob with chatbots using ai).\
    \ But, I wanted to test out this model since I've witnessed the data set.\r\n\r\
    \nAny help is super greatly appreciated. Thanks guys! <3  I also wanted to say\
    \ I'm very sorry if my question is really dumb because I'm missing something obvious.\
    \ Thanks a lot for reading my discussion post. I commend anyone willing to help\
    \ me. I wish I was as smart as everyone else."
  created_at: 2023-04-24 05:07:29+00:00
  edited: false
  hidden: false
  id: 64461ca19f76e8788567f582
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-04-24T13:34:49.000Z'
    data:
      edited: true
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>I haven''t yet tried ooba myself, so someone else might have a better
          idea. But the two bin files in this repository are the weights in 16bit
          and 4bit ggml format that are compatible with llama.cpp. I read from here
          <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md</a>
          that the .bin filenames should explicitly contain "ggml", so you could rename
          the 4bit quant to "ggml-vicuna-13b-free-q4_0.bin" and it might recognize
          it. By default I guess it tries to load the pytorch model files, which I
          haven''t (yet) included in this repository due to how large they are. If
          on the other hand you want to use .safetensors which is GPTQ, make sure
          you''ve installed GPTQ-for-LLaMa as indicated in step 1 here <a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a>.</p>

          <p>But yeah, this is only based on what I found, someone experienced with
          ooba might be able to point out further issues!</p>

          '
        raw: 'I haven''t yet tried ooba myself, so someone else might have a better
          idea. But the two bin files in this repository are the weights in 16bit
          and 4bit ggml format that are compatible with llama.cpp. I read from here
          https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md
          that the .bin filenames should explicitly contain "ggml", so you could rename
          the 4bit quant to "ggml-vicuna-13b-free-q4_0.bin" and it might recognize
          it. By default I guess it tries to load the pytorch model files, which I
          haven''t (yet) included in this repository due to how large they are. If
          on the other hand you want to use .safetensors which is GPTQ, make sure
          you''ve installed GPTQ-for-LLaMa as indicated in step 1 here https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md.


          But yeah, this is only based on what I found, someone experienced with ooba
          might be able to point out further issues!'
        updatedAt: '2023-04-24T13:35:28.512Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - waynekenney
        - Squish42
    id: 64468579a808163cef00a3df
    type: comment
  author: reeducator
  content: 'I haven''t yet tried ooba myself, so someone else might have a better
    idea. But the two bin files in this repository are the weights in 16bit and 4bit
    ggml format that are compatible with llama.cpp. I read from here https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md
    that the .bin filenames should explicitly contain "ggml", so you could rename
    the 4bit quant to "ggml-vicuna-13b-free-q4_0.bin" and it might recognize it. By
    default I guess it tries to load the pytorch model files, which I haven''t (yet)
    included in this repository due to how large they are. If on the other hand you
    want to use .safetensors which is GPTQ, make sure you''ve installed GPTQ-for-LLaMa
    as indicated in step 1 here https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md.


    But yeah, this is only based on what I found, someone experienced with ooba might
    be able to point out further issues!'
  created_at: 2023-04-24 12:34:49+00:00
  edited: true
  hidden: false
  id: 64468579a808163cef00a3df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
      fullname: A ANDRE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yugihu
      type: user
    createdAt: '2023-04-24T13:41:36.000Z'
    data:
      edited: true
      editors:
      - yugihu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
          fullname: A ANDRE
          isHf: false
          isPro: false
          name: yugihu
          type: user
        html: '<p>(i have to edit: i cant also make it work myself)</p>

          <p>Some files are missing and i dont know where to get them:</p>

          <p>For example, for me, it asks a config.json<br>If i add those coming from
          another vicuna 13g, i get the same error as the OP</p>

          '
        raw: '(i have to edit: i cant also make it work myself)


          Some files are missing and i dont know where to get them:


          For example, for me, it asks a config.json

          If i add those coming from another vicuna 13g, i get the same error as the
          OP'
        updatedAt: '2023-04-24T16:04:28.592Z'
      numEdits: 2
      reactions: []
    id: 64468710269f503376bf2b38
    type: comment
  author: yugihu
  content: '(i have to edit: i cant also make it work myself)


    Some files are missing and i dont know where to get them:


    For example, for me, it asks a config.json

    If i add those coming from another vicuna 13g, i get the same error as the OP'
  created_at: 2023-04-24 12:41:36+00:00
  edited: true
  hidden: false
  id: 64468710269f503376bf2b38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98743fc0d3afc17fb3ada8b96ed2d76f.svg
      fullname: Mr Thorn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thornr
      type: user
    createdAt: '2023-04-24T17:02:44.000Z'
    data:
      edited: true
      editors:
      - Thornr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98743fc0d3afc17fb3ada8b96ed2d76f.svg
          fullname: Mr Thorn
          isHf: false
          isPro: false
          name: Thornr
          type: user
        html: '<p>Yes, it does not appear to work with the following error:</p>

          <p><code>OSError: models/reeducator_vicuna-13b-free does not appear to have
          a file named config.json. Checkout ''https://huggingface.co/models/reeducator_vicuna-13b-free/None''
          for available files.</code></p>

          <p>To make this work, I copied over the  <code>config.json</code>, <code>tokenizer.model</code>
          and <code>tokenizer_config.json</code> from a previous 1.0 revision of the
          <code>anon8231489123_vicuna-13b-GPTQ-4bit-128g</code> repo.</p>

          <p>This now appears to be working fine for me.</p>

          '
        raw: 'Yes, it does not appear to work with the following error:


          `OSError: models/reeducator_vicuna-13b-free does not appear to have a file
          named config.json. Checkout ''https://huggingface.co/models/reeducator_vicuna-13b-free/None''
          for available files.`


          To make this work, I copied over the  `config.json`, `tokenizer.model` and
          `tokenizer_config.json` from a previous 1.0 revision of the `anon8231489123_vicuna-13b-GPTQ-4bit-128g`
          repo.


          This now appears to be working fine for me.'
        updatedAt: '2023-04-24T17:30:23.909Z'
      numEdits: 2
      reactions: []
    id: 6446b634c50af85000157617
    type: comment
  author: Thornr
  content: 'Yes, it does not appear to work with the following error:


    `OSError: models/reeducator_vicuna-13b-free does not appear to have a file named
    config.json. Checkout ''https://huggingface.co/models/reeducator_vicuna-13b-free/None''
    for available files.`


    To make this work, I copied over the  `config.json`, `tokenizer.model` and `tokenizer_config.json`
    from a previous 1.0 revision of the `anon8231489123_vicuna-13b-GPTQ-4bit-128g`
    repo.


    This now appears to be working fine for me.'
  created_at: 2023-04-24 16:02:44+00:00
  edited: true
  hidden: false
  id: 6446b634c50af85000157617
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7d7ea350eeed2f3f0dcbcc5122690087.svg
      fullname: Jason Tomlinson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Latent-Dreamscape
      type: user
    createdAt: '2023-04-25T02:50:09.000Z'
    data:
      edited: true
      editors:
      - Latent-Dreamscape
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7d7ea350eeed2f3f0dcbcc5122690087.svg
          fullname: Jason Tomlinson
          isHf: false
          isPro: false
          name: Latent-Dreamscape
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;waynekenney&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/waynekenney\"\
          >@<span class=\"underline\">waynekenney</span></a></span>\n\n\t</span></span>\
          \ </p>\n<blockquote>\n<p>Any help is super greatly appreciated. Thanks guys!\
          \ &lt;3  I also wanted to say I'm very sorry if my question is really dumb\
          \ because I'm missing something obvious. Thanks a lot for reading my discussion\
          \ post. I commend anyone willing to help me. I wish I was as smart as everyone\
          \ else.</p>\n</blockquote>\n<p>Hey bud, just words I know but as a guy who\
          \ used to think I wasn't as smart as everyone else either, let me tell you\
          \ that for the most part it's more a matter of how much you've learned,\
          \ and less about higher or lower intelligence.<br>There are certainly people\
          \ out there who have a natural propensity for understanding this stuff,\
          \ but if you've gotten as far as you have with it than you can certainly\
          \ <em>learn</em> how to do much more. </p>\n<p>When I was in my early 20's\
          \ I genuinely thought I was a buffoon (mostly due to my Father constantly\
          \ telling me I was stupid growing up) until a hacker (grey hat) friend of\
          \ mine tried teaching me how to mod consoles in the mid 2000's.<br>I would\
          \ watch him see all these pages of code and numbers and he just seemed to\
          \ know what it was all about, and I'd think 'this guy's so smart, I wish\
          \ I was smart like that'.<br>One day I vocalized these thoughts, and he\
          \ assured me I wasn't stupid - that I just needed to learn how to do it,\
          \ and that all of us learn differently and at a different pace.<br>He told\
          \ me all about tutorials and how he himself had started at the basics and\
          \ began doing multiple tutorials (and eventually a free course or two) and\
          \ eventually his knowledge grew to the 'genius' I thought I had standing\
          \ before me. </p>\n<p>That guy instilling his confidence in me changed my\
          \ entire self-concept and after that I swore to myself that if I had trouble\
          \ understanding something that I really wanted to learn, I would never let\
          \ my belief that it was too complicated or hard stand in the way. </p>\n\
          <p>Since then I've gone on to learn and workably dabble in a number of coding\
          \ languages, taught myself 3D graphics (using Autodesk and then 'graduating'\
          \ to Blender haha), do a little chemistry and a few other things. </p>\n\
          <p>If I can do it so can you my friend. I hope this helps you in the same\
          \ way he helped me.<br>Salud. :)</p>\n"
        raw: "@waynekenney \n> Any help is super greatly appreciated. Thanks guys!\
          \ <3  I also wanted to say I'm very sorry if my question is really dumb\
          \ because I'm missing something obvious. Thanks a lot for reading my discussion\
          \ post. I commend anyone willing to help me. I wish I was as smart as everyone\
          \ else.\n\nHey bud, just words I know but as a guy who used to think I wasn't\
          \ as smart as everyone else either, let me tell you that for the most part\
          \ it's more a matter of how much you've learned, and less about higher or\
          \ lower intelligence. \nThere are certainly people out there who have a\
          \ natural propensity for understanding this stuff, but if you've gotten\
          \ as far as you have with it than you can certainly *learn* how to do much\
          \ more. \n\nWhen I was in my early 20's I genuinely thought I was a buffoon\
          \ (mostly due to my Father constantly telling me I was stupid growing up)\
          \ until a hacker (grey hat) friend of mine tried teaching me how to mod\
          \ consoles in the mid 2000's. \nI would watch him see all these pages of\
          \ code and numbers and he just seemed to know what it was all about, and\
          \ I'd think 'this guy's so smart, I wish I was smart like that'. \nOne day\
          \ I vocalized these thoughts, and he assured me I wasn't stupid - that I\
          \ just needed to learn how to do it, and that all of us learn differently\
          \ and at a different pace. \nHe told me all about tutorials and how he himself\
          \ had started at the basics and began doing multiple tutorials (and eventually\
          \ a free course or two) and eventually his knowledge grew to the 'genius'\
          \ I thought I had standing before me. \n\nThat guy instilling his confidence\
          \ in me changed my entire self-concept and after that I swore to myself\
          \ that if I had trouble understanding something that I really wanted to\
          \ learn, I would never let my belief that it was too complicated or hard\
          \ stand in the way. \n\nSince then I've gone on to learn and workably dabble\
          \ in a number of coding languages, taught myself 3D graphics (using Autodesk\
          \ and then 'graduating' to Blender haha), do a little chemistry and a few\
          \ other things. \n\nIf I can do it so can you my friend. I hope this helps\
          \ you in the same way he helped me. \nSalud. :)"
        updatedAt: '2023-04-25T02:51:23.843Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - mancub
        - waynekenney
        - PushTheBrAIkes
        - Zhincore
        - uyftrgyuhjgf
    id: 64473fe1257773309c75ddf4
    type: comment
  author: Latent-Dreamscape
  content: "@waynekenney \n> Any help is super greatly appreciated. Thanks guys! <3\
    \  I also wanted to say I'm very sorry if my question is really dumb because I'm\
    \ missing something obvious. Thanks a lot for reading my discussion post. I commend\
    \ anyone willing to help me. I wish I was as smart as everyone else.\n\nHey bud,\
    \ just words I know but as a guy who used to think I wasn't as smart as everyone\
    \ else either, let me tell you that for the most part it's more a matter of how\
    \ much you've learned, and less about higher or lower intelligence. \nThere are\
    \ certainly people out there who have a natural propensity for understanding this\
    \ stuff, but if you've gotten as far as you have with it than you can certainly\
    \ *learn* how to do much more. \n\nWhen I was in my early 20's I genuinely thought\
    \ I was a buffoon (mostly due to my Father constantly telling me I was stupid\
    \ growing up) until a hacker (grey hat) friend of mine tried teaching me how to\
    \ mod consoles in the mid 2000's. \nI would watch him see all these pages of code\
    \ and numbers and he just seemed to know what it was all about, and I'd think\
    \ 'this guy's so smart, I wish I was smart like that'. \nOne day I vocalized these\
    \ thoughts, and he assured me I wasn't stupid - that I just needed to learn how\
    \ to do it, and that all of us learn differently and at a different pace. \nHe\
    \ told me all about tutorials and how he himself had started at the basics and\
    \ began doing multiple tutorials (and eventually a free course or two) and eventually\
    \ his knowledge grew to the 'genius' I thought I had standing before me. \n\n\
    That guy instilling his confidence in me changed my entire self-concept and after\
    \ that I swore to myself that if I had trouble understanding something that I\
    \ really wanted to learn, I would never let my belief that it was too complicated\
    \ or hard stand in the way. \n\nSince then I've gone on to learn and workably\
    \ dabble in a number of coding languages, taught myself 3D graphics (using Autodesk\
    \ and then 'graduating' to Blender haha), do a little chemistry and a few other\
    \ things. \n\nIf I can do it so can you my friend. I hope this helps you in the\
    \ same way he helped me. \nSalud. :)"
  created_at: 2023-04-25 01:50:09+00:00
  edited: true
  hidden: false
  id: 64473fe1257773309c75ddf4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
      fullname: Wayne Kenney Jr.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waynekenney
      type: user
    createdAt: '2023-04-25T16:13:07.000Z'
    data:
      edited: true
      editors:
      - waynekenney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
          fullname: Wayne Kenney Jr.
          isHf: false
          isPro: false
          name: waynekenney
          type: user
        html: '<p>Hi guys. I''m back with an update. So, here''s what I''ve done with
          this problem as it''s been helpful with other ai related problems in the
          past. I''ve asked chat GPT to help me modify the webui''s code. So, we figured
          out how to fix the bin file errors by changing the line within "models.py"
          to:</p>

          <p>checkpoint = Path(f''{shared.args.model_dir}/{shared.model_name}-4bit-128g.safetensors'')</p>

          <p>If you''re curious about which definition I''ve edited of the checkpoint
          variable, it''s the one definition found in ''models.py'' basically.</p>

          <p>Which essentially loads the safetensor file, but now upon manual loading
          of the model. I get the config.json error. However, I''ve pulled the config
          file from the previous version this model is based on.<br>Are there any
          changes that must be made to the config file to load this model? Thanks
          guys!</p>

          '
        raw: 'Hi guys. I''m back with an update. So, here''s what I''ve done with
          this problem as it''s been helpful with other ai related problems in the
          past. I''ve asked chat GPT to help me modify the webui''s code. So, we figured
          out how to fix the bin file errors by changing the line within "models.py"
          to:


          checkpoint = Path(f''{shared.args.model_dir}/{shared.model_name}-4bit-128g.safetensors'')


          If you''re curious about which definition I''ve edited of the checkpoint
          variable, it''s the one definition found in ''models.py'' basically.


          Which essentially loads the safetensor file, but now upon manual loading
          of the model. I get the config.json error. However, I''ve pulled the config
          file from the previous version this model is based on.

          Are there any changes that must be made to the config file to load this
          model? Thanks guys!'
        updatedAt: '2023-04-25T16:15:14.241Z'
      numEdits: 1
      reactions: []
    id: 6447fc13058f3572dd123f3c
    type: comment
  author: waynekenney
  content: 'Hi guys. I''m back with an update. So, here''s what I''ve done with this
    problem as it''s been helpful with other ai related problems in the past. I''ve
    asked chat GPT to help me modify the webui''s code. So, we figured out how to
    fix the bin file errors by changing the line within "models.py" to:


    checkpoint = Path(f''{shared.args.model_dir}/{shared.model_name}-4bit-128g.safetensors'')


    If you''re curious about which definition I''ve edited of the checkpoint variable,
    it''s the one definition found in ''models.py'' basically.


    Which essentially loads the safetensor file, but now upon manual loading of the
    model. I get the config.json error. However, I''ve pulled the config file from
    the previous version this model is based on.

    Are there any changes that must be made to the config file to load this model?
    Thanks guys!'
  created_at: 2023-04-25 15:13:07+00:00
  edited: true
  hidden: false
  id: 6447fc13058f3572dd123f3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-26T04:28:32.000Z'
    data:
      edited: true
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>I can confirm that renaming the bin to ''ggml-vicuna-13b-free-q4_0.bin''
          works, without any changes to configuration or scripts. No additional files
          are needed for the ggml format.</p>

          '
        raw: I can confirm that renaming the bin to 'ggml-vicuna-13b-free-q4_0.bin'
          works, without any changes to configuration or scripts. No additional files
          are needed for the ggml format.
        updatedAt: '2023-04-26T05:35:07.173Z'
      numEdits: 2
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - waynekenney
        - Kalamazooter
        - Adriato
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - waynekenney
        - Kalamazooter
    id: 6448a870e54b488070bfb28a
    type: comment
  author: Squish42
  content: I can confirm that renaming the bin to 'ggml-vicuna-13b-free-q4_0.bin'
    works, without any changes to configuration or scripts. No additional files are
    needed for the ggml format.
  created_at: 2023-04-26 03:28:32+00:00
  edited: true
  hidden: false
  id: 6448a870e54b488070bfb28a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
      fullname: Wayne Kenney Jr.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waynekenney
      type: user
    createdAt: '2023-04-26T06:32:56.000Z'
    data:
      edited: false
      editors:
      - waynekenney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
          fullname: Wayne Kenney Jr.
          isHf: false
          isPro: false
          name: waynekenney
          type: user
        html: '<p>Hey Squish. Thanks a million for suggesting the ultimate fix to
          my problem! This actually resolved my issue. Thank you for enabling me to
          load my model finally!</p>

          '
        raw: Hey Squish. Thanks a million for suggesting the ultimate fix to my problem!
          This actually resolved my issue. Thank you for enabling me to load my model
          finally!
        updatedAt: '2023-04-26T06:32:56.391Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Squish42
    id: 6448c598ab5abd927858a3ca
    type: comment
  author: waynekenney
  content: Hey Squish. Thanks a million for suggesting the ultimate fix to my problem!
    This actually resolved my issue. Thank you for enabling me to load my model finally!
  created_at: 2023-04-26 05:32:56+00:00
  edited: false
  hidden: false
  id: 6448c598ab5abd927858a3ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
      fullname: A ANDRE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yugihu
      type: user
    createdAt: '2023-04-26T07:35:05.000Z'
    data:
      edited: false
      editors:
      - yugihu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
          fullname: A ANDRE
          isHf: false
          isPro: false
          name: yugihu
          type: user
        html: '<p>What do i need to download with ooba and what do i need to download
          then ?<br>For example, do i need the big vicuna-13b-free-f16.bin file ?</p>

          '
        raw: 'What do i need to download with ooba and what do i need to download
          then ?

          For example, do i need the big vicuna-13b-free-f16.bin file ?'
        updatedAt: '2023-04-26T07:35:05.667Z'
      numEdits: 0
      reactions: []
    id: 6448d429d16a70c0158b5182
    type: comment
  author: yugihu
  content: 'What do i need to download with ooba and what do i need to download then
    ?

    For example, do i need the big vicuna-13b-free-f16.bin file ?'
  created_at: 2023-04-26 06:35:05+00:00
  edited: false
  hidden: false
  id: 6448d429d16a70c0158b5182
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
      fullname: Wayne Kenney Jr.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waynekenney
      type: user
    createdAt: '2023-04-26T08:09:39.000Z'
    data:
      edited: false
      editors:
      - waynekenney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67a290e83b559a3393d70014583ac702.svg
          fullname: Wayne Kenney Jr.
          isHf: false
          isPro: false
          name: waynekenney
          type: user
        html: '<blockquote>

          <p>What do i need to download with ooba and what do i need to download then
          ?<br>For example, do i need the big vicuna-13b-free-f16.bin file ?</p>

          </blockquote>

          <p>Yeah. I''ve downloaded both, but I''ve only renamed the one bin file
          by adding ggml to the beginning forming "ggml-vicuna-13b-free-q4_0.bin"
          and it loads perfectly fine for me now.</p>

          '
        raw: '> What do i need to download with ooba and what do i need to download
          then ?

          > For example, do i need the big vicuna-13b-free-f16.bin file ?


          Yeah. I''ve downloaded both, but I''ve only renamed the one bin file by
          adding ggml to the beginning forming "ggml-vicuna-13b-free-q4_0.bin" and
          it loads perfectly fine for me now.'
        updatedAt: '2023-04-26T08:09:39.981Z'
      numEdits: 0
      reactions: []
    id: 6448dc43d5d86def91c865b3
    type: comment
  author: waynekenney
  content: '> What do i need to download with ooba and what do i need to download
    then ?

    > For example, do i need the big vicuna-13b-free-f16.bin file ?


    Yeah. I''ve downloaded both, but I''ve only renamed the one bin file by adding
    ggml to the beginning forming "ggml-vicuna-13b-free-q4_0.bin" and it loads perfectly
    fine for me now.'
  created_at: 2023-04-26 07:09:39+00:00
  edited: false
  hidden: false
  id: 6448dc43d5d86def91c865b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
      fullname: A ANDRE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yugihu
      type: user
    createdAt: '2023-04-26T09:38:23.000Z'
    data:
      edited: false
      editors:
      - yugihu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
          fullname: A ANDRE
          isHf: false
          isPro: false
          name: yugihu
          type: user
        html: '<p>the method given above worked for me.<br>The outputs of the bot
          made sense but i have only asked 3 questions so far</p>

          '
        raw: 'the method given above worked for me.

          The outputs of the bot made sense but i have only asked 3 questions so far'
        updatedAt: '2023-04-26T09:38:23.474Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - waynekenney
    id: 6448f10fab5abd92785d6027
    type: comment
  author: yugihu
  content: 'the method given above worked for me.

    The outputs of the bot made sense but i have only asked 3 questions so far'
  created_at: 2023-04-26 08:38:23+00:00
  edited: false
  hidden: false
  id: 6448f10fab5abd92785d6027
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T01:40:00.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>Just to clarify, only the vicuna-13b-free-q4_0.bin file is needed
          for GGML. Rename that one to ggml-vicuna-13b-free-q4_0.bin. That works for
          CPU. The ggml prefix just tells the webui to load it as a ggml.</p>

          <p>I haven''t been able to get the .safetensors format to load. I don''t
          recall what all I tried, but adjusting config from a similar model didn''t
          work. I''m sure I missed something there. If we can sort out what is missing
          maybe it can be included. Ideally it should install from the model tab webui
          like other models. I also vaguely recall some way of converting to GPTQ
          not working in the webui currently, but I don''t know any details about
          that.<br>Depending on how it is installed, you should have the GPTQ-for-Llama
          extension by default.</p>

          '
        raw: 'Just to clarify, only the vicuna-13b-free-q4_0.bin file is needed for
          GGML. Rename that one to ggml-vicuna-13b-free-q4_0.bin. That works for CPU.
          The ggml prefix just tells the webui to load it as a ggml.


          I haven''t been able to get the .safetensors format to load. I don''t recall
          what all I tried, but adjusting config from a similar model didn''t work.
          I''m sure I missed something there. If we can sort out what is missing maybe
          it can be included. Ideally it should install from the model tab webui like
          other models. I also vaguely recall some way of converting to GPTQ not working
          in the webui currently, but I don''t know any details about that.

          Depending on how it is installed, you should have the GPTQ-for-Llama extension
          by default.'
        updatedAt: '2023-04-29T01:40:00.760Z'
      numEdits: 0
      reactions: []
    id: 644c7570ed08a4fdf4eb8ede
    type: comment
  author: Squish42
  content: 'Just to clarify, only the vicuna-13b-free-q4_0.bin file is needed for
    GGML. Rename that one to ggml-vicuna-13b-free-q4_0.bin. That works for CPU. The
    ggml prefix just tells the webui to load it as a ggml.


    I haven''t been able to get the .safetensors format to load. I don''t recall what
    all I tried, but adjusting config from a similar model didn''t work. I''m sure
    I missed something there. If we can sort out what is missing maybe it can be included.
    Ideally it should install from the model tab webui like other models. I also vaguely
    recall some way of converting to GPTQ not working in the webui currently, but
    I don''t know any details about that.

    Depending on how it is installed, you should have the GPTQ-for-Llama extension
    by default.'
  created_at: 2023-04-29 00:40:00+00:00
  edited: false
  hidden: false
  id: 644c7570ed08a4fdf4eb8ede
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T06:21:51.000Z'
    data:
      edited: true
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>The .safetensors / GPTQ format seems to load okay with the missing
          files. I think I forgot the filename issue last time. The missing files
          can be found at anon8231489123/vicuna-13b-GPTQ-4bit-128g</p>

          <p>Create folder models/reeducator_vicuna-13b-free-4bit-128g<br>Note the
          4bit-128g suffix. Include the .safetensors file, don''t rename it.</p>

          <p>From the above repository, include these files:<br>config.json<br>generation_config.json<br>pytorch_model.bin.index.json<br>special_tokens_map.json<br>tokenizer.model<br>tokenizer_config.json</p>

          <p>Then it should just work. If you have a capable GPU this is the way to
          go. Seems like a great model so far. Thanks for everyone''s hard work putting
          this together.</p>

          '
        raw: 'The .safetensors / GPTQ format seems to load okay with the missing files.
          I think I forgot the filename issue last time. The missing files can be
          found at anon8231489123/vicuna-13b-GPTQ-4bit-128g


          Create folder models/reeducator_vicuna-13b-free-4bit-128g

          Note the 4bit-128g suffix. Include the .safetensors file, don''t rename
          it.


          From the above repository, include these files:

          config.json

          generation_config.json

          pytorch_model.bin.index.json

          special_tokens_map.json

          tokenizer.model

          tokenizer_config.json


          Then it should just work. If you have a capable GPU this is the way to go.
          Seems like a great model so far. Thanks for everyone''s hard work putting
          this together.'
        updatedAt: '2023-04-29T06:25:44.812Z'
      numEdits: 1
      reactions: []
    id: 644cb77f328c1aa30e37c989
    type: comment
  author: Squish42
  content: 'The .safetensors / GPTQ format seems to load okay with the missing files.
    I think I forgot the filename issue last time. The missing files can be found
    at anon8231489123/vicuna-13b-GPTQ-4bit-128g


    Create folder models/reeducator_vicuna-13b-free-4bit-128g

    Note the 4bit-128g suffix. Include the .safetensors file, don''t rename it.


    From the above repository, include these files:

    config.json

    generation_config.json

    pytorch_model.bin.index.json

    special_tokens_map.json

    tokenizer.model

    tokenizer_config.json


    Then it should just work. If you have a capable GPU this is the way to go. Seems
    like a great model so far. Thanks for everyone''s hard work putting this together.'
  created_at: 2023-04-29 05:21:51+00:00
  edited: true
  hidden: false
  id: 644cb77f328c1aa30e37c989
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: reeducator/vicuna-13b-free
repo_type: model
status: open
target_branch: null
title: oobabooga model loading help? Thank you!
