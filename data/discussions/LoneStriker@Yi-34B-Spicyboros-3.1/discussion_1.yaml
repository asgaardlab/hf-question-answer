!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adamo1139
conflicting_files: null
created_at: 2023-11-11 21:30:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-11T21:30:13.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822707176208496
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>Hi, I was about to start training this myself since I couldn''t
          find this finetune and wanted to try it out, good thing I checked hf to
          check if someone else was faster. Could you please upload adapter files
          so that someone who has a base model can merge them by themselves without
          a need for re-download? I would also appreciate if you could upload measurement.json
          for this model to <a href="https://huggingface.co/LoneStriker/ExLlamaV2-Measurements">https://huggingface.co/LoneStriker/ExLlamaV2-Measurements</a></p>

          '
        raw: Hi, I was about to start training this myself since I couldn't find this
          finetune and wanted to try it out, good thing I checked hf to check if someone
          else was faster. Could you please upload adapter files so that someone who
          has a base model can merge them by themselves without a need for re-download?
          I would also appreciate if you could upload measurement.json for this model
          to https://huggingface.co/LoneStriker/ExLlamaV2-Measurements
        updatedAt: '2023-11-11T21:30:13.466Z'
      numEdits: 0
      reactions: []
    id: 654ff265286b72eb7c47bb87
    type: comment
  author: adamo1139
  content: Hi, I was about to start training this myself since I couldn't find this
    finetune and wanted to try it out, good thing I checked hf to check if someone
    else was faster. Could you please upload adapter files so that someone who has
    a base model can merge them by themselves without a need for re-download? I would
    also appreciate if you could upload measurement.json for this model to https://huggingface.co/LoneStriker/ExLlamaV2-Measurements
  created_at: 2023-11-11 21:30:13+00:00
  edited: false
  hidden: false
  id: 654ff265286b72eb7c47bb87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-11T21:58:23.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968559741973877
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I''ll see if I can gather the pieces and upload. You will need to
          have the renamed <a href="https://huggingface.co/chargoddard/Yi-34B-Llama">Llama
          repo from here</a> though to merge it yourself.  Also, to merge, you may
          need to have a &gt; 48GB VRAM GPU as well. I was not able to use multiple
          smaller GPUs to merge.</p>

          '
        raw: I'll see if I can gather the pieces and upload. You will need to have
          the renamed [Llama repo from here](https://huggingface.co/chargoddard/Yi-34B-Llama)
          though to merge it yourself.  Also, to merge, you may need to have a > 48GB
          VRAM GPU as well. I was not able to use multiple smaller GPUs to merge.
        updatedAt: '2023-11-11T21:58:23.904Z'
      numEdits: 0
      reactions: []
    id: 654ff8ff3fe6c0b1f892b92b
    type: comment
  author: LoneStriker
  content: I'll see if I can gather the pieces and upload. You will need to have the
    renamed [Llama repo from here](https://huggingface.co/chargoddard/Yi-34B-Llama)
    though to merge it yourself.  Also, to merge, you may need to have a > 48GB VRAM
    GPU as well. I was not able to use multiple smaller GPUs to merge.
  created_at: 2023-11-11 21:58:23+00:00
  edited: false
  hidden: false
  id: 654ff8ff3fe6c0b1f892b92b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-11T22:29:04.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9852900505065918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>Thank you. Yeah I have the llama-fied base model files from that
          repo already. To be precise, the llama-tokenizer branch, which seems to
          have also been used for your lora. I was merging those kind of  adapters
          in the past and I believe it worked just fine on cpu + ram. I have 64GB
          of system RAM and the model is about the same, so I with little help from
          page file i think Isshould be able to merge that. </p>

          '
        raw: 'Thank you. Yeah I have the llama-fied base model files from that repo
          already. To be precise, the llama-tokenizer branch, which seems to have
          also been used for your lora. I was merging those kind of  adapters in the
          past and I believe it worked just fine on cpu + ram. I have 64GB of system
          RAM and the model is about the same, so I with little help from page file
          i think Isshould be able to merge that. '
        updatedAt: '2023-11-11T22:29:04.756Z'
      numEdits: 0
      reactions: []
    id: 6550003052c59e60d598c162
    type: comment
  author: adamo1139
  content: 'Thank you. Yeah I have the llama-fied base model files from that repo
    already. To be precise, the llama-tokenizer branch, which seems to have also been
    used for your lora. I was merging those kind of  adapters in the past and I believe
    it worked just fine on cpu + ram. I have 64GB of system RAM and the model is about
    the same, so I with little help from page file i think Isshould be able to merge
    that. '
  created_at: 2023-11-11 22:29:04+00:00
  edited: false
  hidden: false
  id: 6550003052c59e60d598c162
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-11T22:31:03.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.962709903717041
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Thanks, good to know. I think my defaults send to <code>cuda</code>.
          I have plenty of system RAM, so I''ll give that a go next merge. Putting
          together the pieces now and will upload when I get a chance.</p>

          '
        raw: Thanks, good to know. I think my defaults send to `cuda`. I have plenty
          of system RAM, so I'll give that a go next merge. Putting together the pieces
          now and will upload when I get a chance.
        updatedAt: '2023-11-11T22:31:03.145Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - adamo1139
    id: 655000a7aae66b1676f68b88
    type: comment
  author: LoneStriker
  content: Thanks, good to know. I think my defaults send to `cuda`. I have plenty
    of system RAM, so I'll give that a go next merge. Putting together the pieces
    now and will upload when I get a chance.
  created_at: 2023-11-11 22:31:03+00:00
  edited: false
  hidden: false
  id: 655000a7aae66b1676f68b88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-11T22:48:59.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9982765913009644
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Data uploaded.</p>

          '
        raw: Data uploaded.
        updatedAt: '2023-11-11T22:48:59.764Z'
      numEdits: 0
      reactions: []
    id: 655004db0aa8eba4c22fa918
    type: comment
  author: LoneStriker
  content: Data uploaded.
  created_at: 2023-11-11 22:48:59+00:00
  edited: false
  hidden: false
  id: 655004db0aa8eba4c22fa918
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-11T22:56:55.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9429195523262024
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>Thank you for the effort! I will do the merge tomorrow, my PC is
          currently busy fine-tuning Yi-34B on a different dataset and I don''t trust
          axolotl to resume from checkpoint without OOM. </p>

          '
        raw: 'Thank you for the effort! I will do the merge tomorrow, my PC is currently
          busy fine-tuning Yi-34B on a different dataset and I don''t trust axolotl
          to resume from checkpoint without OOM. '
        updatedAt: '2023-11-11T22:56:55.041Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655006b74c6f4e3300152ccc
    id: 655006b74c6f4e3300152cca
    type: comment
  author: adamo1139
  content: 'Thank you for the effort! I will do the merge tomorrow, my PC is currently
    busy fine-tuning Yi-34B on a different dataset and I don''t trust axolotl to resume
    from checkpoint without OOM. '
  created_at: 2023-11-11 22:56:55+00:00
  edited: false
  hidden: false
  id: 655006b74c6f4e3300152cca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-11T22:56:55.000Z'
    data:
      status: closed
    id: 655006b74c6f4e3300152ccc
    type: status-change
  author: adamo1139
  created_at: 2023-11-11 22:56:55+00:00
  id: 655006b74c6f4e3300152ccc
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-11T22:59:35.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9660634994506836
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<blockquote>

          <p>Thank you for the effort! I will do the merge tomorrow, my PC is currently
          busy fine-tuning Yi-34B on a different dataset and I don''t trust axolotl
          to resume from checkpoint without OOM.</p>

          </blockquote>

          <p>Are you able to train Yi-34B on consumer-grade 24 GB VRAM cards? I was
          unsuccessful getting qlora to not go OOM there and had to borrow time on
          80 GB VRAM A100s.</p>

          '
        raw: '> Thank you for the effort! I will do the merge tomorrow, my PC is currently
          busy fine-tuning Yi-34B on a different dataset and I don''t trust axolotl
          to resume from checkpoint without OOM.


          Are you able to train Yi-34B on consumer-grade 24 GB VRAM cards? I was unsuccessful
          getting qlora to not go OOM there and had to borrow time on 80 GB VRAM A100s.


          '
        updatedAt: '2023-11-11T22:59:35.896Z'
      numEdits: 0
      reactions: []
    id: 65500757dd2482b059634550
    type: comment
  author: LoneStriker
  content: '> Thank you for the effort! I will do the merge tomorrow, my PC is currently
    busy fine-tuning Yi-34B on a different dataset and I don''t trust axolotl to resume
    from checkpoint without OOM.


    Are you able to train Yi-34B on consumer-grade 24 GB VRAM cards? I was unsuccessful
    getting qlora to not go OOM there and had to borrow time on 80 GB VRAM A100s.


    '
  created_at: 2023-11-11 22:59:35+00:00
  edited: false
  hidden: false
  id: 65500757dd2482b059634550
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-11T23:40:07.000Z'
    data:
      edited: true
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9618663787841797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>I spent a few hours tweaking config for axolotl today.  At first
          I was getting OOMs even with ctx 50 and Lora rank 8, but then at one moment
          I started being able to increase both context and rank. I am shitty at documenting
          stuff, so I am not sure what helped. I believe that disabling sample_packing
          helped the most. </p>

          <p>Look at /core/models.py line 230. Sample packing is incompatible with
          flash attention since it uses a custom patch, so it''s enabled only if you
          set flash_attention to true and sample_packing to false. I believe that
          flash attention has some vram benefits, so using sample_packing would make
          it harder to run when you are near vram limit </p>

          <p>I am currently running 3 epoch training on 5000 samples of which most
          are around 300-800 tokens in length. Ctx 1000, rank 16, alpha 32. GPU is
          rtx 3090 ti. Deepspeed is disabled since I didn''t find it to be able to
          offload the adamw optimizer to cpu, even when using deepspeed config json
          that clearly had that set. Seems to run fine so far, I am on step 1500,
          so about 10% in. It''s very likely that it''s possible to get something
          like 1500 ctx, I think I got it running like that for a few minutes - then
          I canceled the run so I don''t know how stable that is. I don''t think you
          would be able to use ctx 4096 without OOM. </p>

          <p>I will share the config file in a moment </p>

          '
        raw: "I spent a few hours tweaking config for axolotl today.  At first I was\
          \ getting OOMs even with ctx 50 and Lora rank 8, but then at one moment\
          \ I started being able to increase both context and rank. I am shitty at\
          \ documenting stuff, so I am not sure what helped. I believe that disabling\
          \ sample_packing helped the most. \n\nLook at /core/models.py line 230.\
          \ Sample packing is incompatible with flash attention since it uses a custom\
          \ patch, so it's enabled only if you set flash_attention to true and sample_packing\
          \ to false. I believe that flash attention has some vram benefits, so using\
          \ sample_packing would make it harder to run when you are near vram limit\
          \ \n\nI am currently running 3 epoch training on 5000 samples of which most\
          \ are around 300-800 tokens in length. Ctx 1000, rank 16, alpha 32. GPU\
          \ is rtx 3090 ti. Deepspeed is disabled since I didn't find it to be able\
          \ to offload the adamw optimizer to cpu, even when using deepspeed config\
          \ json that clearly had that set. Seems to run fine so far, I am on step\
          \ 1500, so about 10% in. It's very likely that it's possible to get something\
          \ like 1500 ctx, I think I got it running like that for a few minutes -\
          \ then I canceled the run so I don't know how stable that is. I don't think\
          \ you would be able to use ctx 4096 without OOM. \n\nI will share the config\
          \ file in a moment "
        updatedAt: '2023-11-11T23:41:14.211Z'
      numEdits: 1
      reactions: []
    id: 655010d7c67f60a3689019f4
    type: comment
  author: adamo1139
  content: "I spent a few hours tweaking config for axolotl today.  At first I was\
    \ getting OOMs even with ctx 50 and Lora rank 8, but then at one moment I started\
    \ being able to increase both context and rank. I am shitty at documenting stuff,\
    \ so I am not sure what helped. I believe that disabling sample_packing helped\
    \ the most. \n\nLook at /core/models.py line 230. Sample packing is incompatible\
    \ with flash attention since it uses a custom patch, so it's enabled only if you\
    \ set flash_attention to true and sample_packing to false. I believe that flash\
    \ attention has some vram benefits, so using sample_packing would make it harder\
    \ to run when you are near vram limit \n\nI am currently running 3 epoch training\
    \ on 5000 samples of which most are around 300-800 tokens in length. Ctx 1000,\
    \ rank 16, alpha 32. GPU is rtx 3090 ti. Deepspeed is disabled since I didn't\
    \ find it to be able to offload the adamw optimizer to cpu, even when using deepspeed\
    \ config json that clearly had that set. Seems to run fine so far, I am on step\
    \ 1500, so about 10% in. It's very likely that it's possible to get something\
    \ like 1500 ctx, I think I got it running like that for a few minutes - then I\
    \ canceled the run so I don't know how stable that is. I don't think you would\
    \ be able to use ctx 4096 without OOM. \n\nI will share the config file in a moment "
  created_at: 2023-11-11 23:40:07+00:00
  edited: true
  hidden: false
  id: 655010d7c67f60a3689019f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-11T23:55:47.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7139720320701599
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<p>Here's the config</p>\n<pre><code>base_model: ./Yi-34B-Llama\nbase_model_config:\
          \ ./Yi-34B-Llama\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\n\
          is_mistral_derived_model: false\nis_llama_derived_model: true\n\nload_in_8bit:\
          \ false\nload_in_4bit: true\n\ntorch_dtype: bf16 #I changed some code in\
          \ models.py since one of the parameters sent to bitsandbytes for initializing\
          \ quantization to nf4 wasn't set. It probably has no impact though. Without\
          \ those changes in models.py, this setting won't have any impact on training.\n\
          strict: false\ndatasets:\n  - path: /path/to/your/dataset\n    type: completion\n\
          dataset_prepared_path: last_run_prepared\nval_set_size: 0.05\nadapter: qlora\n\
          lora_model_dir:\nsequence_len: 1000\nsample_packing: false #this seems to\
          \ be setting causing most issues. \nlora_r: 16\nlora_alpha: 64\nlora_dropout:\
          \ 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\
          wandb_project:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\noutput_dir:\
          \ ./output-dir\npad_to_sequence_len: true\nmicro_batch_size: 1\ngradient_accumulation_steps:\
          \ 1\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\ntorchdistx_path:\nlr_scheduler:\
          \ constant #you probably would want to change it to cosine, I don't think\
          \ constant is recommended. \nlearning_rate: 0.00015\ntrain_on_inputs: false\n\
          group_by_length: false\nbf16: true\nfp16: false\ntf32: false\nbfloat16:\
          \ true\nflash_optimum: false\ngradient_checkpointing: true\nearly_stopping_patience:\n\
          local_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\
          deepspeed:\nseed: 384238\nwarmup_steps: 5\neval_steps: 5000000 #I was getting\
          \ annoyed at eval that took 20 mins at ctx 50, so I turned it off. Probably\
          \ irrelevant.\nsave_steps: 50\neval_table_size: \neval_table_max_new_tokens:\n\
          debug:\nweight_decay:\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token:\
          \ \"&lt;|startoftext|&gt;\"\n  eos_token: \"&lt;|endoftext|&gt;\"\n  unk_token:\
          \ \"&lt;unk&gt;\"\n</code></pre>\n<p>I got weird CUDA errors when running\
          \ this with airoboros dataset, but it seems to work fine for my private\
          \ dataset, I don't know why yet.</p>\n"
        raw: "Here's the config\n\n```\nbase_model: ./Yi-34B-Llama\nbase_model_config:\
          \ ./Yi-34B-Llama\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\n\
          is_mistral_derived_model: false\nis_llama_derived_model: true\n\nload_in_8bit:\
          \ false\nload_in_4bit: true\n\ntorch_dtype: bf16 #I changed some code in\
          \ models.py since one of the parameters sent to bitsandbytes for initializing\
          \ quantization to nf4 wasn't set. It probably has no impact though. Without\
          \ those changes in models.py, this setting won't have any impact on training.\n\
          strict: false\ndatasets:\n  - path: /path/to/your/dataset\n    type: completion\n\
          dataset_prepared_path: last_run_prepared\nval_set_size: 0.05\nadapter: qlora\n\
          lora_model_dir:\nsequence_len: 1000\nsample_packing: false #this seems to\
          \ be setting causing most issues. \nlora_r: 16\nlora_alpha: 64\nlora_dropout:\
          \ 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\
          wandb_project:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\noutput_dir:\
          \ ./output-dir\npad_to_sequence_len: true\nmicro_batch_size: 1\ngradient_accumulation_steps:\
          \ 1\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\ntorchdistx_path:\nlr_scheduler:\
          \ constant #you probably would want to change it to cosine, I don't think\
          \ constant is recommended. \nlearning_rate: 0.00015\ntrain_on_inputs: false\n\
          group_by_length: false\nbf16: true\nfp16: false\ntf32: false\nbfloat16:\
          \ true\nflash_optimum: false\ngradient_checkpointing: true\nearly_stopping_patience:\n\
          local_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\
          deepspeed:\nseed: 384238\nwarmup_steps: 5\neval_steps: 5000000 #I was getting\
          \ annoyed at eval that took 20 mins at ctx 50, so I turned it off. Probably\
          \ irrelevant.\nsave_steps: 50\neval_table_size: \neval_table_max_new_tokens:\n\
          debug:\nweight_decay:\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token:\
          \ \"<|startoftext|>\"\n  eos_token: \"<|endoftext|>\"\n  unk_token: \"<unk>\"\
          \n\n```\n\nI got weird CUDA errors when running this with airoboros dataset,\
          \ but it seems to work fine for my private dataset, I don't know why yet."
        updatedAt: '2023-11-11T23:55:47.629Z'
      numEdits: 0
      reactions: []
    id: 655014831df60d2a34003707
    type: comment
  author: adamo1139
  content: "Here's the config\n\n```\nbase_model: ./Yi-34B-Llama\nbase_model_config:\
    \ ./Yi-34B-Llama\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\n\
    is_mistral_derived_model: false\nis_llama_derived_model: true\n\nload_in_8bit:\
    \ false\nload_in_4bit: true\n\ntorch_dtype: bf16 #I changed some code in models.py\
    \ since one of the parameters sent to bitsandbytes for initializing quantization\
    \ to nf4 wasn't set. It probably has no impact though. Without those changes in\
    \ models.py, this setting won't have any impact on training.\nstrict: false\n\
    datasets:\n  - path: /path/to/your/dataset\n    type: completion\ndataset_prepared_path:\
    \ last_run_prepared\nval_set_size: 0.05\nadapter: qlora\nlora_model_dir:\nsequence_len:\
    \ 1000\nsample_packing: false #this seems to be setting causing most issues. \n\
    lora_r: 16\nlora_alpha: 64\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear:\
    \ true\nlora_fan_in_fan_out:\nwandb_project:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\
    output_dir: ./output-dir\npad_to_sequence_len: true\nmicro_batch_size: 1\ngradient_accumulation_steps:\
    \ 1\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\ntorchdistx_path:\nlr_scheduler:\
    \ constant #you probably would want to change it to cosine, I don't think constant\
    \ is recommended. \nlearning_rate: 0.00015\ntrain_on_inputs: false\ngroup_by_length:\
    \ false\nbf16: true\nfp16: false\ntf32: false\nbfloat16: true\nflash_optimum:\
    \ false\ngradient_checkpointing: true\nearly_stopping_patience:\nlocal_rank:\n\
    logging_steps: 1\nxformers_attention:\nflash_attention: true\ndeepspeed:\nseed:\
    \ 384238\nwarmup_steps: 5\neval_steps: 5000000 #I was getting annoyed at eval\
    \ that took 20 mins at ctx 50, so I turned it off. Probably irrelevant.\nsave_steps:\
    \ 50\neval_table_size: \neval_table_max_new_tokens:\ndebug:\nweight_decay:\nfsdp:\n\
    fsdp_config:\nspecial_tokens:\n  bos_token: \"<|startoftext|>\"\n  eos_token:\
    \ \"<|endoftext|>\"\n  unk_token: \"<unk>\"\n\n```\n\nI got weird CUDA errors\
    \ when running this with airoboros dataset, but it seems to work fine for my private\
    \ dataset, I don't know why yet."
  created_at: 2023-11-11 23:55:47+00:00
  edited: false
  hidden: false
  id: 655014831df60d2a34003707
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-12T05:12:01.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9766232967376709
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Thanks!  I had suspected that the packing was causing the OOM but
          never followed through to verify.  Seems to be running fine in a test training
          run if I just adjust my config and disable sample_packing on my 3090s.  I
          enabled eval_steps to check for eval loss and it seems to take about 5 minutes
          to run.  Probably better off not running that check.</p>

          <p>Good luck with your training run!</p>

          '
        raw: 'Thanks!  I had suspected that the packing was causing the OOM but never
          followed through to verify.  Seems to be running fine in a test training
          run if I just adjust my config and disable sample_packing on my 3090s.  I
          enabled eval_steps to check for eval loss and it seems to take about 5 minutes
          to run.  Probably better off not running that check.


          Good luck with your training run!'
        updatedAt: '2023-11-12T05:12:01.145Z'
      numEdits: 0
      reactions: []
    id: 65505ea132f278f503813818
    type: comment
  author: LoneStriker
  content: 'Thanks!  I had suspected that the packing was causing the OOM but never
    followed through to verify.  Seems to be running fine in a test training run if
    I just adjust my config and disable sample_packing on my 3090s.  I enabled eval_steps
    to check for eval loss and it seems to take about 5 minutes to run.  Probably
    better off not running that check.


    Good luck with your training run!'
  created_at: 2023-11-12 05:12:01+00:00
  edited: false
  hidden: false
  id: 65505ea132f278f503813818
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-12T15:46:39.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9634879231452942
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span><br>I've\
          \ merged both mine and your LoRAs without issues, I think total system memory\
          \ usage spiked to something like 100-110 GB since Windows created 64GB page\
          \ file for me, but it didn't take too long anyway. Thanks again for uploading\
          \ the adapter file and measurement.json, I have pretty crappy internet deal\
          \ right now, so I can't download 50GB files every day.<br>My dataset had\
          \ some issues with formatting like not strictly adhering to Llama 2 chat\
          \ prompt format and having standard  EOS token at the end of every sample,\
          \ so resulting prompt format is weird, but I am still very impressed by\
          \ the result. I was training the model on synthetic dataset that had data\
          \ from a few very peculiar books, converted to q&amp;a and instruct via\
          \ 7B Mistral Dolphin/Airoboros models, yet the resulting model has both\
          \ the knowledge from the dataset and also generalized very well, so I can\
          \ use it as generic instruct model that gives pretty natural responses.\
          \ I think that the possibility of training basically SOTA publicly accessible\
          \ LLMs on single consumer GPU is a big deal.</p>\n<p>I think it would make\
          \ sense for you to submit Yi-34B-Spicyboros to the OpenLLM Leaderboard.\
          \ I haven't done any benchmarks, but Yi-34B model is up there, so this finetune\
          \ could potentially score even higher.</p>\n"
        raw: "@LoneStriker \nI've merged both mine and your LoRAs without issues,\
          \ I think total system memory usage spiked to something like 100-110 GB\
          \ since Windows created 64GB page file for me, but it didn't take too long\
          \ anyway. Thanks again for uploading the adapter file and measurement.json,\
          \ I have pretty crappy internet deal right now, so I can't download 50GB\
          \ files every day.\nMy dataset had some issues with formatting like not\
          \ strictly adhering to Llama 2 chat prompt format and having standard </s>\
          \ EOS token at the end of every sample, so resulting prompt format is weird,\
          \ but I am still very impressed by the result. I was training the model\
          \ on synthetic dataset that had data from a few very peculiar books, converted\
          \ to q&a and instruct via 7B Mistral Dolphin/Airoboros models, yet the resulting\
          \ model has both the knowledge from the dataset and also generalized very\
          \ well, so I can use it as generic instruct model that gives pretty natural\
          \ responses. I think that the possibility of training basically SOTA publicly\
          \ accessible LLMs on single consumer GPU is a big deal.\n\nI think it would\
          \ make sense for you to submit Yi-34B-Spicyboros to the OpenLLM Leaderboard.\
          \ I haven't done any benchmarks, but Yi-34B model is up there, so this finetune\
          \ could potentially score even higher."
        updatedAt: '2023-11-12T15:46:39.981Z'
      numEdits: 0
      reactions: []
    id: 6550f35f32f278f50397e825
    type: comment
  author: adamo1139
  content: "@LoneStriker \nI've merged both mine and your LoRAs without issues, I\
    \ think total system memory usage spiked to something like 100-110 GB since Windows\
    \ created 64GB page file for me, but it didn't take too long anyway. Thanks again\
    \ for uploading the adapter file and measurement.json, I have pretty crappy internet\
    \ deal right now, so I can't download 50GB files every day.\nMy dataset had some\
    \ issues with formatting like not strictly adhering to Llama 2 chat prompt format\
    \ and having standard </s> EOS token at the end of every sample, so resulting\
    \ prompt format is weird, but I am still very impressed by the result. I was training\
    \ the model on synthetic dataset that had data from a few very peculiar books,\
    \ converted to q&a and instruct via 7B Mistral Dolphin/Airoboros models, yet the\
    \ resulting model has both the knowledge from the dataset and also generalized\
    \ very well, so I can use it as generic instruct model that gives pretty natural\
    \ responses. I think that the possibility of training basically SOTA publicly\
    \ accessible LLMs on single consumer GPU is a big deal.\n\nI think it would make\
    \ sense for you to submit Yi-34B-Spicyboros to the OpenLLM Leaderboard. I haven't\
    \ done any benchmarks, but Yi-34B model is up there, so this finetune could potentially\
    \ score even higher."
  created_at: 2023-11-12 15:46:39+00:00
  edited: false
  hidden: false
  id: 6550f35f32f278f50397e825
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-12T17:28:01.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9853541254997253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I''d like to get a couple more epochs trained on this model to make
          sure it''s not under-trained.</p>

          <p>I hear you about the ISP data caps.  I used to have to deal with a monthly
          1 TB cap (up and down). Doesn''t take much to go past that limit.</p>

          '
        raw: 'I''d like to get a couple more epochs trained on this model to make
          sure it''s not under-trained.


          I hear you about the ISP data caps.  I used to have to deal with a monthly
          1 TB cap (up and down). Doesn''t take much to go past that limit.'
        updatedAt: '2023-11-12T17:28:01.584Z'
      numEdits: 0
      reactions: []
    id: 65510b218cc59d5b494aebcc
    type: comment
  author: LoneStriker
  content: 'I''d like to get a couple more epochs trained on this model to make sure
    it''s not under-trained.


    I hear you about the ISP data caps.  I used to have to deal with a monthly 1 TB
    cap (up and down). Doesn''t take much to go past that limit.'
  created_at: 2023-11-12 17:28:01+00:00
  edited: false
  hidden: false
  id: 65510b218cc59d5b494aebcc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-17T17:59:34.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4342581331729889
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>New fine-tune trained on 3 epochs total <a href="https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-2">now
          up here</a>.</p>

          '
        raw: New fine-tune trained on 3 epochs total [now up here](https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-2).
        updatedAt: '2023-11-17T17:59:34.751Z'
      numEdits: 0
      reactions: []
    id: 6557aa0647d3f86b61417c31
    type: comment
  author: LoneStriker
  content: New fine-tune trained on 3 epochs total [now up here](https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-2).
  created_at: 2023-11-17 17:59:34+00:00
  edited: false
  hidden: false
  id: 6557aa0647d3f86b61417c31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-17T22:33:35.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.418000727891922
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<p>Any plans on updating the lora adapter? \U0001F606\U0001F606<br><a\
          \ href=\"https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-LoRA/tree/main\"\
          >https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-LoRA/tree/main</a></p>\n"
        raw: "Any plans on updating the lora adapter? \U0001F606\U0001F606\nhttps://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-LoRA/tree/main"
        updatedAt: '2023-11-17T22:33:35.422Z'
      numEdits: 0
      reactions: []
    id: 6557ea3f539d4b7c130d3b25
    type: comment
  author: adamo1139
  content: "Any plans on updating the lora adapter? \U0001F606\U0001F606\nhttps://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-LoRA/tree/main"
  created_at: 2023-11-17 22:33:35+00:00
  edited: false
  hidden: false
  id: 6557ea3f539d4b7c130d3b25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-18T05:54:58.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.834412693977356
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Just verifying that the model was reasonable first before releasing
          the updated LoRA:<br><a href="https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-2-LoRA">https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-2-LoRA</a></p>

          '
        raw: 'Just verifying that the model was reasonable first before releasing
          the updated LoRA:

          https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-2-LoRA'
        updatedAt: '2023-11-18T05:54:58.700Z'
      numEdits: 0
      reactions: []
    id: 655851b2180749bf32b1585b
    type: comment
  author: LoneStriker
  content: 'Just verifying that the model was reasonable first before releasing the
    updated LoRA:

    https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-2-LoRA'
  created_at: 2023-11-18 05:54:58+00:00
  edited: false
  hidden: false
  id: 655851b2180749bf32b1585b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-18T13:01:01.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9250118732452393
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<p>I merged the model and quantized it (I made the calibration myself,\
          \ using the same 0007.parquet file) and I am generally getting what I would\
          \ have expected of successful airoboros-based fine-tune. </p>\n<p>I got\
          \ output with a list of references just once, so it's obviously still in\
          \ the dataset, but it's not showing up in an unexpected manner most of the\
          \ time anymore.<br>I am frequently getting weird tokens just before EOS,\
          \ basically the reply ends in a dot followed by single letter.<br>Sometimes\
          \ the end of the response has \"&lt;|unused134|&gt;\",\"&lt;|unused005|&gt;\"\
          \ or other similar text in the &lt;|unusedxxx|&gt; pattern. Below are some\
          \ examples taken from both webui and chat.py. It's not fixed by disabling\
          \ BOS token, setting repp to 1.0 or changing system prompt.</p>\n<pre><code>&lt;|im_start|&gt;system\n\
          &lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nwhat is the weather like in Ghana\
          \ in May on average?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nIn May,\
          \ Ghana experiences a hot and dry weather condition. The average temperature\
          \ ranges from 24\xB0C (75\xB0F) to 32\xB0C (90\xB0F). The humidity is lower\
          \ than in other months due to the Harmattan wind from the Sahara Desert.\
          \ However, there may be occasional rainfall, especially in the southern\
          \ parts of the country. It's also a good time for beach activities as the\
          \ Atlantic Ocean is calm.&lt;|unused145|&gt;&lt;|im_end|&gt;\n</code></pre>\n\
          <pre><code>&lt;|im_start|&gt;system\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n\
          write a joke about llamas&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\
          Why don't llamas use computers? They're stuck in the stone age!&lt;|unused005|&gt;&lt;|im_end|&gt;\n\
          </code></pre>\n<pre><code>&lt;|im_start|&gt;system\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n\
          write a joke about llamas&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\
          Why don't llamas ever get lost? Because they always have a long-term plan.u&lt;|im_end|&gt;\n\
          </code></pre>\n<p>I am totally ok with it being like this, but you may want\
          \ to take a look before making another fine-tune in the future.<br>I am\
          \ guessing somehow samples were being cut off mid-sentence, maybe due to\
          \ padding, packing or ctx limit.<br>I was doing 2 QLoRA fine-tunes on private\
          \ dataset on Yi-34B with disabled sample packing (one instruct, about 400-600\
          \ tokens per sample, second convo, multi round chat packed to about 500\
          \ tokens per sample) and I didn't notice those exact issues. I will stop\
          \ nit-picking now, thank you for the model and lora adapter!</p>\n"
        raw: "I merged the model and quantized it (I made the calibration myself,\
          \ using the same 0007.parquet file) and I am generally getting what I would\
          \ have expected of successful airoboros-based fine-tune. \n\nI got output\
          \ with a list of references just once, so it's obviously still in the dataset,\
          \ but it's not showing up in an unexpected manner most of the time anymore.\n\
          I am frequently getting weird tokens just before EOS, basically the reply\
          \ ends in a dot followed by single letter. \nSometimes the end of the response\
          \ has \"<|unused134|>\",\"<|unused005|>\" or other similar text in the <|unusedxxx|>\
          \ pattern. Below are some examples taken from both webui and chat.py. It's\
          \ not fixed by disabling BOS token, setting repp to 1.0 or changing system\
          \ prompt.\n\n```\n<|im_start|>system\n<|im_end|>\n<|im_start|>user\nwhat\
          \ is the weather like in Ghana in May on average?<|im_end|>\n<|im_start|>assistant\n\
          In May, Ghana experiences a hot and dry weather condition. The average temperature\
          \ ranges from 24\xB0C (75\xB0F) to 32\xB0C (90\xB0F). The humidity is lower\
          \ than in other months due to the Harmattan wind from the Sahara Desert.\
          \ However, there may be occasional rainfall, especially in the southern\
          \ parts of the country. It's also a good time for beach activities as the\
          \ Atlantic Ocean is calm.<|unused145|><|im_end|>\n```\n\n```\n<|im_start|>system\n\
          <|im_end|>\n<|im_start|>user\nwrite a joke about llamas<|im_end|>\n<|im_start|>assistant\n\
          Why don't llamas use computers? They're stuck in the stone age!<|unused005|><|im_end|>\n\
          ```\n\n```\n<|im_start|>system\n<|im_end|>\n<|im_start|>user\nwrite a joke\
          \ about llamas<|im_end|>\n<|im_start|>assistant\nWhy don't llamas ever get\
          \ lost? Because they always have a long-term plan.u<|im_end|>\n```\n\nI\
          \ am totally ok with it being like this, but you may want to take a look\
          \ before making another fine-tune in the future.\nI am guessing somehow\
          \ samples were being cut off mid-sentence, maybe due to padding, packing\
          \ or ctx limit.\nI was doing 2 QLoRA fine-tunes on private dataset on Yi-34B\
          \ with disabled sample packing (one instruct, about 400-600 tokens per sample,\
          \ second convo, multi round chat packed to about 500 tokens per sample)\
          \ and I didn't notice those exact issues. I will stop nit-picking now, thank\
          \ you for the model and lora adapter!\n\n\n\n\n"
        updatedAt: '2023-11-18T13:01:01.703Z'
      numEdits: 0
      reactions: []
    id: 6558b58d3aff9efaad1d7e23
    type: comment
  author: adamo1139
  content: "I merged the model and quantized it (I made the calibration myself, using\
    \ the same 0007.parquet file) and I am generally getting what I would have expected\
    \ of successful airoboros-based fine-tune. \n\nI got output with a list of references\
    \ just once, so it's obviously still in the dataset, but it's not showing up in\
    \ an unexpected manner most of the time anymore.\nI am frequently getting weird\
    \ tokens just before EOS, basically the reply ends in a dot followed by single\
    \ letter. \nSometimes the end of the response has \"<|unused134|>\",\"<|unused005|>\"\
    \ or other similar text in the <|unusedxxx|> pattern. Below are some examples\
    \ taken from both webui and chat.py. It's not fixed by disabling BOS token, setting\
    \ repp to 1.0 or changing system prompt.\n\n```\n<|im_start|>system\n<|im_end|>\n\
    <|im_start|>user\nwhat is the weather like in Ghana in May on average?<|im_end|>\n\
    <|im_start|>assistant\nIn May, Ghana experiences a hot and dry weather condition.\
    \ The average temperature ranges from 24\xB0C (75\xB0F) to 32\xB0C (90\xB0F).\
    \ The humidity is lower than in other months due to the Harmattan wind from the\
    \ Sahara Desert. However, there may be occasional rainfall, especially in the\
    \ southern parts of the country. It's also a good time for beach activities as\
    \ the Atlantic Ocean is calm.<|unused145|><|im_end|>\n```\n\n```\n<|im_start|>system\n\
    <|im_end|>\n<|im_start|>user\nwrite a joke about llamas<|im_end|>\n<|im_start|>assistant\n\
    Why don't llamas use computers? They're stuck in the stone age!<|unused005|><|im_end|>\n\
    ```\n\n```\n<|im_start|>system\n<|im_end|>\n<|im_start|>user\nwrite a joke about\
    \ llamas<|im_end|>\n<|im_start|>assistant\nWhy don't llamas ever get lost? Because\
    \ they always have a long-term plan.u<|im_end|>\n```\n\nI am totally ok with it\
    \ being like this, but you may want to take a look before making another fine-tune\
    \ in the future.\nI am guessing somehow samples were being cut off mid-sentence,\
    \ maybe due to padding, packing or ctx limit.\nI was doing 2 QLoRA fine-tunes\
    \ on private dataset on Yi-34B with disabled sample packing (one instruct, about\
    \ 400-600 tokens per sample, second convo, multi round chat packed to about 500\
    \ tokens per sample) and I didn't notice those exact issues. I will stop nit-picking\
    \ now, thank you for the model and lora adapter!\n\n\n\n\n"
  created_at: 2023-11-18 13:01:01+00:00
  edited: false
  hidden: false
  id: 6558b58d3aff9efaad1d7e23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-18T14:05:11.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9444058537483215
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I''ve noticed the strange random end of inference characters as
          well.  I will turn off the packing check for the next run.  That setting
          is also what prevents fine-tuning locally on 3090s or 4090s (with smaller
          batch sizes.) Look out for a <code>-3</code> in the future...</p>

          '
        raw: I've noticed the strange random end of inference characters as well.  I
          will turn off the packing check for the next run.  That setting is also
          what prevents fine-tuning locally on 3090s or 4090s (with smaller batch
          sizes.) Look out for a `-3` in the future...
        updatedAt: '2023-11-18T14:05:11.667Z'
      numEdits: 0
      reactions: []
    id: 6558c4978edb5951336e780a
    type: comment
  author: LoneStriker
  content: I've noticed the strange random end of inference characters as well.  I
    will turn off the packing check for the next run.  That setting is also what prevents
    fine-tuning locally on 3090s or 4090s (with smaller batch sizes.) Look out for
    a `-3` in the future...
  created_at: 2023-11-18 14:05:11+00:00
  edited: false
  hidden: false
  id: 6558c4978edb5951336e780a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-18T14:51:28.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8811257481575012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>I think you can try to check the dataset manually by running a dataset
          preparation only step in axolotl.</p>

          <blockquote>

          <p>Preprocess dataset<br>You can optionally pre-tokenize dataset with the
          following before finetuning. This is recommended for large datasets.<br>
          Set push_dataset_to_hub: hf_user/repo to push it to Huggingface.<br>   Use
          --debug to see preprocessed examples.<br>python -m axolotl.cli.preprocess
          your_config.yml</p>

          </blockquote>

          <p>The dataset seems to be saved as an arrow file in last_run_prepared,
          I am not sure how one could open one it so that it would be human-readable</p>

          '
        raw: "I think you can try to check the dataset manually by running a dataset\
          \ preparation only step in axolotl.\n\n>Preprocess dataset\n>You can optionally\
          \ pre-tokenize dataset with the following before finetuning. This is recommended\
          \ for large datasets.\n  >  Set push_dataset_to_hub: hf_user/repo to push\
          \ it to Huggingface.\n>    Use --debug to see preprocessed examples.\n>python\
          \ -m axolotl.cli.preprocess your_config.yml\n\nThe dataset seems to be saved\
          \ as an arrow file in last_run_prepared, I am not sure how one could open\
          \ one it so that it would be human-readable"
        updatedAt: '2023-11-18T14:51:28.209Z'
      numEdits: 0
      reactions: []
    id: 6558cf70ed8df83128096969
    type: comment
  author: adamo1139
  content: "I think you can try to check the dataset manually by running a dataset\
    \ preparation only step in axolotl.\n\n>Preprocess dataset\n>You can optionally\
    \ pre-tokenize dataset with the following before finetuning. This is recommended\
    \ for large datasets.\n  >  Set push_dataset_to_hub: hf_user/repo to push it to\
    \ Huggingface.\n>    Use --debug to see preprocessed examples.\n>python -m axolotl.cli.preprocess\
    \ your_config.yml\n\nThe dataset seems to be saved as an arrow file in last_run_prepared,\
    \ I am not sure how one could open one it so that it would be human-readable"
  created_at: 2023-11-18 14:51:28+00:00
  edited: false
  hidden: false
  id: 6558cf70ed8df83128096969
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-18T16:00:07.000Z'
    data:
      edited: true
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9745066165924072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Having issues trying to turn off packing in my test runs. Axolotl
          just hangs after loading shards and running the first  step on each GPU.
          Then, it''s just 100% GPU, but using very low watts on both GPUs.  I''ve
          seen this error before running Axolotl locally but was never able to get
          past it.  </p>

          <p>Edit: Just turning back on the packing option gets past this strange
          bug, so I have no idea what other options would help here (if any.)  I''ve
          run without packing enabled locally just fine.</p>

          <p>The other Axolotl hanging and timeout issue I''ve been able to fix by
          turning off NVlink:<br><code>export NCCL_P2P_DISABLE=1</code></p>

          <p>I''ll have a look at your suggestion with the preprocessing of the dataset.  Unfortunately,
          I''ve had no luck looking at the generated arrow files, not even able to
          load them using pyarrow.</p>

          '
        raw: "Having issues trying to turn off packing in my test runs. Axolotl just\
          \ hangs after loading shards and running the first  step on each GPU. Then,\
          \ it's just 100% GPU, but using very low watts on both GPUs.  I've seen\
          \ this error before running Axolotl locally but was never able to get past\
          \ it.  \n\nEdit: Just turning back on the packing option gets past this\
          \ strange bug, so I have no idea what other options would help here (if\
          \ any.)  I've run without packing enabled locally just fine.\n\nThe other\
          \ Axolotl hanging and timeout issue I've been able to fix by turning off\
          \ NVlink:\n`export NCCL_P2P_DISABLE=1`\n\nI'll have a look at your suggestion\
          \ with the preprocessing of the dataset.  Unfortunately, I've had no luck\
          \ looking at the generated arrow files, not even able to load them using\
          \ pyarrow."
        updatedAt: '2023-11-18T16:01:18.144Z'
      numEdits: 1
      reactions: []
    id: 6558df873f3e05685d11f2ca
    type: comment
  author: LoneStriker
  content: "Having issues trying to turn off packing in my test runs. Axolotl just\
    \ hangs after loading shards and running the first  step on each GPU. Then, it's\
    \ just 100% GPU, but using very low watts on both GPUs.  I've seen this error\
    \ before running Axolotl locally but was never able to get past it.  \n\nEdit:\
    \ Just turning back on the packing option gets past this strange bug, so I have\
    \ no idea what other options would help here (if any.)  I've run without packing\
    \ enabled locally just fine.\n\nThe other Axolotl hanging and timeout issue I've\
    \ been able to fix by turning off NVlink:\n`export NCCL_P2P_DISABLE=1`\n\nI'll\
    \ have a look at your suggestion with the preprocessing of the dataset.  Unfortunately,\
    \ I've had no luck looking at the generated arrow files, not even able to load\
    \ them using pyarrow."
  created_at: 2023-11-18 16:00:07+00:00
  edited: true
  hidden: false
  id: 6558df873f3e05685d11f2ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-18T16:07:26.000Z'
    data:
      edited: true
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2742916941642761
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p>The sample output seems OK:</p>\n<pre><code>root@6b7e6bd673cc:/workspace/axolotl#\
          \ python -m axolotl.cli.preprocess ./spicy-34b-qlora.yml --debug\n[...]\n\
          Map (num_proc=256): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 46770/46770 [00:11&lt;00:00, 4000.19 examples/s]\n\
          [2023-11-18 16:03:21,086] [DEBUG] [axolotl.log:60] [PID:16337] [RANK:0]\
          \ total_num_tokens: 24075112\n[2023-11-18 16:03:23,107] [DEBUG] [axolotl.log:60]\
          \ [PID:16337] [RANK:0] `total_supervised_tokens: 16532178`\n[2023-11-18\
          \ 16:03:33,068] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16337]\
          \ [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device:\
          \ 24075112\n[2023-11-18 16:03:33,068] [DEBUG] [axolotl.log:60] [PID:16337]\
          \ [RANK:0] data_loader_len: 968\n[2023-11-18 16:03:33,069] [INFO] [axolotl.log:60]\
          \ [PID:16337] [RANK:0] sample_packing_eff_est across ranks: [0.9766887488575939]\n\
          [2023-11-18 16:03:33,069] [DEBUG] [axolotl.log:60] [PID:16337] [RANK:0]\
          \ sample_packing_eff_est: 0.98\n[2023-11-18 16:03:33,069] [DEBUG] [axolotl.log:60]\
          \ [PID:16337] [RANK:0] total_num_steps: 2904\n[2023-11-18 16:03:33,071]\
          \ [INFO] [axolotl.scripts.load_datasets:317] [PID:16337] [RANK:0] check_dataset_labels...\n\
          [2023-11-18 16:03:33,088] [INFO] [axolotl.check_example_labels:35] [PID:16337]\
          \ [RANK:0] &lt;|im_start|&gt;(-100, 6) system(-100, 10707)\n(-100, 144)\
          \ You(-100, 3961) are(-100, 678) an(-100, 663) unbiased(-100, 37334) ,(-100,\
          \ 97) unc(-100, 5432) ens(-100, 897) ored(-100, 2466) ,(-100, 97) helpful(-100,\
          \ 6901) assistant(-100, 14135) .(-100, 98) &lt;|im_end|&gt;(-100, 7)\n(-100,\
          \ 144)\n(-100, 144) &lt;|im_start|&gt;(-100, 6) user(-100, 3903)\n(-100,\
          \ 144) Do(-100, 9888) you(-100, 641) know(-100, 1082) any(-100, 916) jokes(-100,\
          \ 23826) about(-100, 883) sharks(-100, 49165) ?(-100, 100) &lt;|im_end|&gt;(-100,\
          \ 7)\n(-100, 144)\n(-100, 144) &lt;|im_start|&gt;(-100, 6) ass(-100, 765)\
          \ istant(-100, 13611)\n(144, 144) Why(12665, 12665) do(771, 771) sharks(49165,\
          \ 49165) swim(9695, 9695) in(594, 594) salt(10274, 10274) water(2127, 2127)\
          \ ?(100, 100) Because(6023, 6023) pepper(18360, 18360) would(931, 931) make(1150,\
          \ 1150) them(972, 972) sne(15148, 15148) e(59569, 59569) ze(3402, 3402)\
          \ !(99, 99) &lt;|im_end|&gt;(7, 7)\n(144, 144)\n(144, 144) &lt;|endoftext|&gt;(2,\
          \ 2)\n[2023-11-18 16:03:33,088] [INFO] [axolotl.check_example_labels:36]\
          \ [PID:16337] [RANK:0]\n\n\n\n[2023-11-18 16:03:33,088] [INFO] [axolotl.scripts.load_datasets:330]\
          \ [PID:16337] [RANK:0] printing prompters...\n[2023-11-18 16:03:33,119]\
          \ [INFO] [axolotl.cli.preprocess.do_cli:45] [PID:16337] [RANK:0] Success!\
          \ Preprocessed data path: `dataset_prepared_path: last_run_prepared`\n</code></pre>\n\
          <h2 id=\"colorized-output\">Colorized output:</h2>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/X2mf8pMyPiZeCFLMKen2R.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/X2mf8pMyPiZeCFLMKen2R.png\"\
          ></a></p>\n"
        raw: "The sample output seems OK:\n```\nroot@6b7e6bd673cc:/workspace/axolotl#\
          \ python -m axolotl.cli.preprocess ./spicy-34b-qlora.yml --debug\n[...]\n\
          Map (num_proc=256): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 46770/46770 [00:11<00:00, 4000.19 examples/s]\n\
          [2023-11-18 16:03:21,086] [DEBUG] [axolotl.log:60] [PID:16337] [RANK:0]\
          \ total_num_tokens: 24075112\n[2023-11-18 16:03:23,107] [DEBUG] [axolotl.log:60]\
          \ [PID:16337] [RANK:0] `total_supervised_tokens: 16532178`\n[2023-11-18\
          \ 16:03:33,068] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16337]\
          \ [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device:\
          \ 24075112\n[2023-11-18 16:03:33,068] [DEBUG] [axolotl.log:60] [PID:16337]\
          \ [RANK:0] data_loader_len: 968\n[2023-11-18 16:03:33,069] [INFO] [axolotl.log:60]\
          \ [PID:16337] [RANK:0] sample_packing_eff_est across ranks: [0.9766887488575939]\n\
          [2023-11-18 16:03:33,069] [DEBUG] [axolotl.log:60] [PID:16337] [RANK:0]\
          \ sample_packing_eff_est: 0.98\n[2023-11-18 16:03:33,069] [DEBUG] [axolotl.log:60]\
          \ [PID:16337] [RANK:0] total_num_steps: 2904\n[2023-11-18 16:03:33,071]\
          \ [INFO] [axolotl.scripts.load_datasets:317] [PID:16337] [RANK:0] check_dataset_labels...\n\
          [2023-11-18 16:03:33,088] [INFO] [axolotl.check_example_labels:35] [PID:16337]\
          \ [RANK:0] <|im_start|>(-100, 6) system(-100, 10707)\n(-100, 144) You(-100,\
          \ 3961) are(-100, 678) an(-100, 663) unbiased(-100, 37334) ,(-100, 97) unc(-100,\
          \ 5432) ens(-100, 897) ored(-100, 2466) ,(-100, 97) helpful(-100, 6901)\
          \ assistant(-100, 14135) .(-100, 98) <|im_end|>(-100, 7)\n(-100, 144)\n\
          (-100, 144) <|im_start|>(-100, 6) user(-100, 3903)\n(-100, 144) Do(-100,\
          \ 9888) you(-100, 641) know(-100, 1082) any(-100, 916) jokes(-100, 23826)\
          \ about(-100, 883) sharks(-100, 49165) ?(-100, 100) <|im_end|>(-100, 7)\n\
          (-100, 144)\n(-100, 144) <|im_start|>(-100, 6) ass(-100, 765) istant(-100,\
          \ 13611)\n(144, 144) Why(12665, 12665) do(771, 771) sharks(49165, 49165)\
          \ swim(9695, 9695) in(594, 594) salt(10274, 10274) water(2127, 2127) ?(100,\
          \ 100) Because(6023, 6023) pepper(18360, 18360) would(931, 931) make(1150,\
          \ 1150) them(972, 972) sne(15148, 15148) e(59569, 59569) ze(3402, 3402)\
          \ !(99, 99) <|im_end|>(7, 7)\n(144, 144)\n(144, 144) <|endoftext|>(2, 2)\n\
          [2023-11-18 16:03:33,088] [INFO] [axolotl.check_example_labels:36] [PID:16337]\
          \ [RANK:0]\n\n\n\n[2023-11-18 16:03:33,088] [INFO] [axolotl.scripts.load_datasets:330]\
          \ [PID:16337] [RANK:0] printing prompters...\n[2023-11-18 16:03:33,119]\
          \ [INFO] [axolotl.cli.preprocess.do_cli:45] [PID:16337] [RANK:0] Success!\
          \ Preprocessed data path: `dataset_prepared_path: last_run_prepared`\n```\n\
          \n## Colorized output:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/X2mf8pMyPiZeCFLMKen2R.png)\n"
        updatedAt: '2023-11-18T16:09:40.685Z'
      numEdits: 1
      reactions: []
    id: 6558e13e9e9dfc1089d6fa2b
    type: comment
  author: LoneStriker
  content: "The sample output seems OK:\n```\nroot@6b7e6bd673cc:/workspace/axolotl#\
    \ python -m axolotl.cli.preprocess ./spicy-34b-qlora.yml --debug\n[...]\nMap (num_proc=256):\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46770/46770 [00:11<00:00,\
    \ 4000.19 examples/s]\n[2023-11-18 16:03:21,086] [DEBUG] [axolotl.log:60] [PID:16337]\
    \ [RANK:0] total_num_tokens: 24075112\n[2023-11-18 16:03:23,107] [DEBUG] [axolotl.log:60]\
    \ [PID:16337] [RANK:0] `total_supervised_tokens: 16532178`\n[2023-11-18 16:03:33,068]\
    \ [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16337] [RANK:0]\
    \ packing_efficiency_estimate: 1.0 total_num_tokens per device: 24075112\n[2023-11-18\
    \ 16:03:33,068] [DEBUG] [axolotl.log:60] [PID:16337] [RANK:0] data_loader_len:\
    \ 968\n[2023-11-18 16:03:33,069] [INFO] [axolotl.log:60] [PID:16337] [RANK:0]\
    \ sample_packing_eff_est across ranks: [0.9766887488575939]\n[2023-11-18 16:03:33,069]\
    \ [DEBUG] [axolotl.log:60] [PID:16337] [RANK:0] sample_packing_eff_est: 0.98\n\
    [2023-11-18 16:03:33,069] [DEBUG] [axolotl.log:60] [PID:16337] [RANK:0] total_num_steps:\
    \ 2904\n[2023-11-18 16:03:33,071] [INFO] [axolotl.scripts.load_datasets:317] [PID:16337]\
    \ [RANK:0] check_dataset_labels...\n[2023-11-18 16:03:33,088] [INFO] [axolotl.check_example_labels:35]\
    \ [PID:16337] [RANK:0] <|im_start|>(-100, 6) system(-100, 10707)\n(-100, 144)\
    \ You(-100, 3961) are(-100, 678) an(-100, 663) unbiased(-100, 37334) ,(-100, 97)\
    \ unc(-100, 5432) ens(-100, 897) ored(-100, 2466) ,(-100, 97) helpful(-100, 6901)\
    \ assistant(-100, 14135) .(-100, 98) <|im_end|>(-100, 7)\n(-100, 144)\n(-100,\
    \ 144) <|im_start|>(-100, 6) user(-100, 3903)\n(-100, 144) Do(-100, 9888) you(-100,\
    \ 641) know(-100, 1082) any(-100, 916) jokes(-100, 23826) about(-100, 883) sharks(-100,\
    \ 49165) ?(-100, 100) <|im_end|>(-100, 7)\n(-100, 144)\n(-100, 144) <|im_start|>(-100,\
    \ 6) ass(-100, 765) istant(-100, 13611)\n(144, 144) Why(12665, 12665) do(771,\
    \ 771) sharks(49165, 49165) swim(9695, 9695) in(594, 594) salt(10274, 10274) water(2127,\
    \ 2127) ?(100, 100) Because(6023, 6023) pepper(18360, 18360) would(931, 931) make(1150,\
    \ 1150) them(972, 972) sne(15148, 15148) e(59569, 59569) ze(3402, 3402) !(99,\
    \ 99) <|im_end|>(7, 7)\n(144, 144)\n(144, 144) <|endoftext|>(2, 2)\n[2023-11-18\
    \ 16:03:33,088] [INFO] [axolotl.check_example_labels:36] [PID:16337] [RANK:0]\n\
    \n\n\n[2023-11-18 16:03:33,088] [INFO] [axolotl.scripts.load_datasets:330] [PID:16337]\
    \ [RANK:0] printing prompters...\n[2023-11-18 16:03:33,119] [INFO] [axolotl.cli.preprocess.do_cli:45]\
    \ [PID:16337] [RANK:0] Success! Preprocessed data path: `dataset_prepared_path:\
    \ last_run_prepared`\n```\n\n## Colorized output:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/X2mf8pMyPiZeCFLMKen2R.png)\n"
  created_at: 2023-11-18 16:07:26+00:00
  edited: true
  hidden: false
  id: 6558e13e9e9dfc1089d6fa2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-18T16:12:56.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9077476263046265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Other examples look fine too. I''ve cleared the last run cache and
          am rerunning the training again with packing enabled. We''ll see if the
          same random token issue crops up again.</p>

          '
        raw: Other examples look fine too. I've cleared the last run cache and am
          rerunning the training again with packing enabled. We'll see if the same
          random token issue crops up again.
        updatedAt: '2023-11-18T16:12:56.889Z'
      numEdits: 0
      reactions: []
    id: 6558e2884a49906b2c3bf79c
    type: comment
  author: LoneStriker
  content: Other examples look fine too. I've cleared the last run cache and am rerunning
    the training again with packing enabled. We'll see if the same random token issue
    crops up again.
  created_at: 2023-11-18 16:12:56+00:00
  edited: false
  hidden: false
  id: 6558e2884a49906b2c3bf79c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-18T16:47:39.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9580307602882385
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>What''s the difference between config used for the first fine-tune
          and the second one? I am seeing that larger batch size was  used, and I
          think you used qlora for the second one as opposed to 8bit Lora for the
          first one. And you put in the prompt format on the second one. Were both
          runs done with sample packing enabled? Were you doing any other changes
          to the config file? Axolotl should have not used the previously packed dataset
          if you made changes to prompt format, and since prompt format works fine
          in the v2, that worked correctly. </p>

          '
        raw: 'What''s the difference between config used for the first fine-tune and
          the second one? I am seeing that larger batch size was  used, and I think
          you used qlora for the second one as opposed to 8bit Lora for the first
          one. And you put in the prompt format on the second one. Were both runs
          done with sample packing enabled? Were you doing any other changes to the
          config file? Axolotl should have not used the previously packed dataset
          if you made changes to prompt format, and since prompt format works fine
          in the v2, that worked correctly. '
        updatedAt: '2023-11-18T16:47:39.069Z'
      numEdits: 0
      reactions: []
    id: 6558eaab4cd8d44865a5a66b
    type: comment
  author: adamo1139
  content: 'What''s the difference between config used for the first fine-tune and
    the second one? I am seeing that larger batch size was  used, and I think you
    used qlora for the second one as opposed to 8bit Lora for the first one. And you
    put in the prompt format on the second one. Were both runs done with sample packing
    enabled? Were you doing any other changes to the config file? Axolotl should have
    not used the previously packed dataset if you made changes to prompt format, and
    since prompt format works fine in the v2, that worked correctly. '
  created_at: 2023-11-18 16:47:39+00:00
  edited: false
  hidden: false
  id: 6558eaab4cd8d44865a5a66b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-18T20:07:30.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9819513559341431
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I thought I kept the configs the same, but looking at the settings,
          looks like that was not the case. I''m running these fine tunes on a borrowed
          box, so some of the configs got mixed up while I was debugging issues.  If
          I look at the LoRA uploads, I can see there were different batch sizes like
          you indicated. Both should have run with sample packing; first run was on
          2x A100s, subsequent runs are on 2x H100s. </p>

          <p>I''ve got a separate dataset test that I ran with Synthia (using a similar
          config to the 3-epoch Spicy run) that I''ll have a quick look at to see
          if it exhibits the same weird random token at the end of the generated output
          issue.</p>

          '
        raw: "I thought I kept the configs the same, but looking at the settings,\
          \ looks like that was not the case. I'm running these fine tunes on a borrowed\
          \ box, so some of the configs got mixed up while I was debugging issues.\
          \  If I look at the LoRA uploads, I can see there were different batch sizes\
          \ like you indicated. Both should have run with sample packing; first run\
          \ was on 2x A100s, subsequent runs are on 2x H100s. \n\nI've got a separate\
          \ dataset test that I ran with Synthia (using a similar config to the 3-epoch\
          \ Spicy run) that I'll have a quick look at to see if it exhibits the same\
          \ weird random token at the end of the generated output issue."
        updatedAt: '2023-11-18T20:07:30.328Z'
      numEdits: 0
      reactions: []
    id: 65591982ec17c88302a088c9
    type: comment
  author: LoneStriker
  content: "I thought I kept the configs the same, but looking at the settings, looks\
    \ like that was not the case. I'm running these fine tunes on a borrowed box,\
    \ so some of the configs got mixed up while I was debugging issues.  If I look\
    \ at the LoRA uploads, I can see there were different batch sizes like you indicated.\
    \ Both should have run with sample packing; first run was on 2x A100s, subsequent\
    \ runs are on 2x H100s. \n\nI've got a separate dataset test that I ran with Synthia\
    \ (using a similar config to the 3-epoch Spicy run) that I'll have a quick look\
    \ at to see if it exhibits the same weird random token at the end of the generated\
    \ output issue."
  created_at: 2023-11-18 20:07:30+00:00
  edited: false
  hidden: false
  id: 65591982ec17c88302a088c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-19T21:57:24.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8900920748710632
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Version 3 seems to be better with not having the random tokens before
          EOS.  Not sure where the issue was, whether it was in the training, merging,
          or elsewhere:<br><a href="https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-3">https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-3</a></p>

          '
        raw: 'Version 3 seems to be better with not having the random tokens before
          EOS.  Not sure where the issue was, whether it was in the training, merging,
          or elsewhere:

          https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-3'
        updatedAt: '2023-11-19T21:57:24.522Z'
      numEdits: 0
      reactions: []
    id: 655a84c46f460bf32912080b
    type: comment
  author: LoneStriker
  content: 'Version 3 seems to be better with not having the random tokens before
    EOS.  Not sure where the issue was, whether it was in the training, merging, or
    elsewhere:

    https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-3'
  created_at: 2023-11-19 21:57:24+00:00
  edited: false
  hidden: false
  id: 655a84c46f460bf32912080b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-19T22:14:13.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.946539580821991
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>Thank you. I will check it out in a few days. I started running
          my own training of spicyboros 2.2 Yi-34B today and I still have 50 hours
          till I finish epoch 1, so my PC is out of service for now. I am really curious
          how it will compare to your attempts. You have higher batch sizes, gradient
          accumulation, probably maxed out 4k context length and lower 0.0001 learning
          rate spread over 3 epochs, so on paper your fine-tune should be better,
          but who knows if it''s actually visible to end user.</p>

          '
        raw: Thank you. I will check it out in a few days. I started running my own
          training of spicyboros 2.2 Yi-34B today and I still have 50 hours till I
          finish epoch 1, so my PC is out of service for now. I am really curious
          how it will compare to your attempts. You have higher batch sizes, gradient
          accumulation, probably maxed out 4k context length and lower 0.0001 learning
          rate spread over 3 epochs, so on paper your fine-tune should be better,
          but who knows if it's actually visible to end user.
        updatedAt: '2023-11-19T22:14:13.307Z'
      numEdits: 0
      reactions: []
    id: 655a88b523e43ac218f5dfa8
    type: comment
  author: adamo1139
  content: Thank you. I will check it out in a few days. I started running my own
    training of spicyboros 2.2 Yi-34B today and I still have 50 hours till I finish
    epoch 1, so my PC is out of service for now. I am really curious how it will compare
    to your attempts. You have higher batch sizes, gradient accumulation, probably
    maxed out 4k context length and lower 0.0001 learning rate spread over 3 epochs,
    so on paper your fine-tune should be better, but who knows if it's actually visible
    to end user.
  created_at: 2023-11-19 22:14:13+00:00
  edited: false
  hidden: false
  id: 655a88b523e43ac218f5dfa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-20T03:56:50.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9890948534011841
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Cool, I''d be interested to see the results as well. I was able
          to train 34B on my 3090s and 4090s boxes, but since I got access to some
          free compute, I''m happy to have faster turn-around times. I have run some
          smaller training runs on my local boxes, and the results seemed reasonably
          good. Good luck with the run!</p>

          '
        raw: Cool, I'd be interested to see the results as well. I was able to train
          34B on my 3090s and 4090s boxes, but since I got access to some free compute,
          I'm happy to have faster turn-around times. I have run some smaller training
          runs on my local boxes, and the results seemed reasonably good. Good luck
          with the run!
        updatedAt: '2023-11-20T03:56:50.875Z'
      numEdits: 0
      reactions: []
    id: 655ad902deee83130a49cb77
    type: comment
  author: LoneStriker
  content: Cool, I'd be interested to see the results as well. I was able to train
    34B on my 3090s and 4090s boxes, but since I got access to some free compute,
    I'm happy to have faster turn-around times. I have run some smaller training runs
    on my local boxes, and the results seemed reasonably good. Good luck with the
    run!
  created_at: 2023-11-20 03:56:50+00:00
  edited: false
  hidden: false
  id: 655ad902deee83130a49cb77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-24T23:23:41.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611837267875671
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>My first run (1 epoch) was a fail. I used learning rate 0.00018
          with effective batch size 1 and learning rate was apparently too big because
          the model was really dim-witted. It uses really simple language, it''s like
          talking to a 5 year old.<br>On second attempt I trained with gradient accumulation
          steps 4 and lr 0.0003, since apparently the bigger the batch size, the bigger
          you can scale learning rate to get the same results, but this model fails
          quantization with Exllama v2 on layer 53 with some weird math error that
          goes over my head, even if I provide measurement.json from a different yi-34b
          finetune. Today I finished my third attempt, and this one seems to be actually
          pretty successful. I didn''t want to waste more days on a broken model,
          so the first attempt was 1 epoch, while 2nd and 3rd was with 0.2 epochs.
          You can find adapter files, my config and measurement.json <a href="https://huggingface.co/adamo1139/Yi-34B-Spicyboros-2-2-run3-QLoRA">here</a>.
          So yeah, if you will be training in the future, remember that learning rate
          should probably be adjusted up or down as you change effective batch size.
          I saw that Eric Hartford got a successful Dolphin Yi-34B finetune when using
          learning rate 0.0003, so I thought I could get away with 0.00018, but I
          didn''t know that if I have effective batch size of 1 and he has effective
          batch size of 4, his actual learning rate would be lower since it''s more
          like 0.000075 per batch.</p>

          '
        raw: 'My first run (1 epoch) was a fail. I used learning rate 0.00018 with
          effective batch size 1 and learning rate was apparently too big because
          the model was really dim-witted. It uses really simple language, it''s like
          talking to a 5 year old.

          On second attempt I trained with gradient accumulation steps 4 and lr 0.0003,
          since apparently the bigger the batch size, the bigger you can scale learning
          rate to get the same results, but this model fails quantization with Exllama
          v2 on layer 53 with some weird math error that goes over my head, even if
          I provide measurement.json from a different yi-34b finetune. Today I finished
          my third attempt, and this one seems to be actually pretty successful. I
          didn''t want to waste more days on a broken model, so the first attempt
          was 1 epoch, while 2nd and 3rd was with 0.2 epochs. You can find adapter
          files, my config and measurement.json [here](https://huggingface.co/adamo1139/Yi-34B-Spicyboros-2-2-run3-QLoRA).
          So yeah, if you will be training in the future, remember that learning rate
          should probably be adjusted up or down as you change effective batch size.
          I saw that Eric Hartford got a successful Dolphin Yi-34B finetune when using
          learning rate 0.0003, so I thought I could get away with 0.00018, but I
          didn''t know that if I have effective batch size of 1 and he has effective
          batch size of 4, his actual learning rate would be lower since it''s more
          like 0.000075 per batch.'
        updatedAt: '2023-11-24T23:23:41.864Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 6561307df725fc0972158c8d
    type: comment
  author: adamo1139
  content: 'My first run (1 epoch) was a fail. I used learning rate 0.00018 with effective
    batch size 1 and learning rate was apparently too big because the model was really
    dim-witted. It uses really simple language, it''s like talking to a 5 year old.

    On second attempt I trained with gradient accumulation steps 4 and lr 0.0003,
    since apparently the bigger the batch size, the bigger you can scale learning
    rate to get the same results, but this model fails quantization with Exllama v2
    on layer 53 with some weird math error that goes over my head, even if I provide
    measurement.json from a different yi-34b finetune. Today I finished my third attempt,
    and this one seems to be actually pretty successful. I didn''t want to waste more
    days on a broken model, so the first attempt was 1 epoch, while 2nd and 3rd was
    with 0.2 epochs. You can find adapter files, my config and measurement.json [here](https://huggingface.co/adamo1139/Yi-34B-Spicyboros-2-2-run3-QLoRA).
    So yeah, if you will be training in the future, remember that learning rate should
    probably be adjusted up or down as you change effective batch size. I saw that
    Eric Hartford got a successful Dolphin Yi-34B finetune when using learning rate
    0.0003, so I thought I could get away with 0.00018, but I didn''t know that if
    I have effective batch size of 1 and he has effective batch size of 4, his actual
    learning rate would be lower since it''s more like 0.000075 per batch.'
  created_at: 2023-11-24 23:23:41+00:00
  edited: false
  hidden: false
  id: 6561307df725fc0972158c8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-25T04:39:44.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9961333870887756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Thanks for the pointers!  I''m glad you were able to get a working
          model in the end.  I''ve heard that Mistral is also very sensitive to LR
          and can go off the rails more quickly than L2. It''s good that you were
          able to narrow down specifically what was causing issues for you.  I''ve
          also found that a lot of training is just trial and error; you end up throwing
          away a lot more than you keep, at least until you get the settings dialed
          in.  I still do not know why I had the various issues I had with the strange
          random characters at the end of one of my training runs.  A re-train and
          requant did not have the issue.</p>

          '
        raw: Thanks for the pointers!  I'm glad you were able to get a working model
          in the end.  I've heard that Mistral is also very sensitive to LR and can
          go off the rails more quickly than L2. It's good that you were able to narrow
          down specifically what was causing issues for you.  I've also found that
          a lot of training is just trial and error; you end up throwing away a lot
          more than you keep, at least until you get the settings dialed in.  I still
          do not know why I had the various issues I had with the strange random characters
          at the end of one of my training runs.  A re-train and requant did not have
          the issue.
        updatedAt: '2023-11-25T04:39:44.772Z'
      numEdits: 0
      reactions: []
    id: 65617a908631d43d2b96a1ff
    type: comment
  author: LoneStriker
  content: Thanks for the pointers!  I'm glad you were able to get a working model
    in the end.  I've heard that Mistral is also very sensitive to LR and can go off
    the rails more quickly than L2. It's good that you were able to narrow down specifically
    what was causing issues for you.  I've also found that a lot of training is just
    trial and error; you end up throwing away a lot more than you keep, at least until
    you get the settings dialed in.  I still do not know why I had the various issues
    I had with the strange random characters at the end of one of my training runs.  A
    re-train and requant did not have the issue.
  created_at: 2023-11-25 04:39:44+00:00
  edited: false
  hidden: false
  id: 65617a908631d43d2b96a1ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-25T17:44:23.000Z'
    data:
      edited: true
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9707639217376709
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<blockquote>\n<p>I still do not know why I had the various issues I\
          \ had with the strange random characters at the end of one of my training\
          \ runs. A re-train and requant did not have the issue.</p>\n</blockquote>\n\
          <p>I still have this issue with 3.1-3. I merged the qlora from here <a href=\"\
          https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-3-LoRA\">https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-3-LoRA</a>\
          \ with base Yi-34B and quantized it using measurement.json for your 3.1-2\
          \ since I didn't want to spend an hour measuring loss on various layers\
          \ today. It has the issue with responses ending with &lt;|unusedx|&gt; or\
          \ .x<br>I ran SHA256 check on the adapter.safetensors file that I have to\
          \ make sure I didn't use another file from previous run by mistake. It matches\
          \ SHA256 from HF from 3.1-3  <code>SHA256: 265C00DF626AC42663A5ADD818F0840A57DC8CB4CDDA0C23C5D67E661C87C6A6</code></p>\n\
          <p>Here's sample of output that I am getting. I am using system prompt \"\
          A chat.\". Using \"A chat\" without a dot has the same effect, i didn't\
          \ try other system prompts.</p>\n<pre><code>A chat.\nUser: write a joke\
          \ about llamas\nWhy did the llama refuse to go to the party? It was afraid\
          \ of the mingling.\nUser: write another one\nWhy did the llama break up\
          \ with the llama? It was tired of being taken for a ride.&lt;|unused137|&gt;\n\
          User: write a joke about Guanacos\nWhy did the guanaco join the gym? It\
          \ wanted to work on its neck gains.S\n</code></pre>\n<pre><code>A chat.\n\
          User: what's the average weather like in Egypt in June?\nIn June, Egypt\
          \ experiences hot and dry weather. The average high temperature is around\
          \ 35\xB0C (95\xB0F), and the average low temperature is around 25\xB0C (77\xB0\
          F). However, it's important to note that these are averages and actual temperatures\
          \ can vary. There's also very little rainfall during this month.&lt;|unused010|&gt;\n\
          </code></pre>\n<p>Tested only in exllama v2 cli with chat.py and chat.py\
          \ that I modified to not insert BOS and have different default repp</p>\n\
          <p>LoRA adapters for versions 3.1-2 and 3.1-3 uploaded by you have different\
          \ SHA256, so it can't be that you uploaded older LoRA file twice.<br>Can\
          \ you please try to replicate it yourself? It happened in about 80% of chats\
          \ I had so far. I don't think that using a different measurement.json file\
          \ would make this difference. I used measurement.json files from different\
          \ fine-tunes last week when I was quantizing my 3-day long failed run1 and\
          \ it felt exactly as broken as when I created a fresh measurement.json file\
          \ a day later, so measurement.json file compatibility isn't a big issue\
          \ as long as you stay within the same base model. To me it seems like 3.1-2\
          \ and 3.1-3 share the same small issue with adding unnecessary characters\
          \ just before EOS.</p>\n<p>Edit: I re-run lora merge to base to make sure\
          \ I didn't merge 3.1-2 by mistake. SHA256 on safetensors shards that I checked\
          \ have exactly the same hash on both merges, so I didn't make any mistakes\
          \ when merging.</p>\n"
        raw: ">I still do not know why I had the various issues I had with the strange\
          \ random characters at the end of one of my training runs. A re-train and\
          \ requant did not have the issue.\n\nI still have this issue with 3.1-3.\
          \ I merged the qlora from here https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-3-LoRA\
          \ with base Yi-34B and quantized it using measurement.json for your 3.1-2\
          \ since I didn't want to spend an hour measuring loss on various layers\
          \ today. It has the issue with responses ending with <|unusedx|> or .x \n\
          I ran SHA256 check on the adapter.safetensors file that I have to make sure\
          \ I didn't use another file from previous run by mistake. It matches SHA256\
          \ from HF from 3.1-3  `SHA256: 265C00DF626AC42663A5ADD818F0840A57DC8CB4CDDA0C23C5D67E661C87C6A6`\n\
          \nHere's sample of output that I am getting. I am using system prompt \"\
          A chat.\". Using \"A chat\" without a dot has the same effect, i didn't\
          \ try other system prompts.\n```\nA chat.\nUser: write a joke about llamas\n\
          Why did the llama refuse to go to the party? It was afraid of the mingling.\n\
          User: write another one\nWhy did the llama break up with the llama? It was\
          \ tired of being taken for a ride.<|unused137|>\nUser: write a joke about\
          \ Guanacos\nWhy did the guanaco join the gym? It wanted to work on its neck\
          \ gains.S\n```\n\n\n```\nA chat.\nUser: what's the average weather like\
          \ in Egypt in June?\nIn June, Egypt experiences hot and dry weather. The\
          \ average high temperature is around 35\xB0C (95\xB0F), and the average\
          \ low temperature is around 25\xB0C (77\xB0F). However, it's important to\
          \ note that these are averages and actual temperatures can vary. There's\
          \ also very little rainfall during this month.<|unused010|>\n```\n\nTested\
          \ only in exllama v2 cli with chat.py and chat.py that I modified to not\
          \ insert BOS and have different default repp\n\nLoRA adapters for versions\
          \ 3.1-2 and 3.1-3 uploaded by you have different SHA256, so it can't be\
          \ that you uploaded older LoRA file twice.\nCan you please try to replicate\
          \ it yourself? It happened in about 80% of chats I had so far. I don't think\
          \ that using a different measurement.json file would make this difference.\
          \ I used measurement.json files from different fine-tunes last week when\
          \ I was quantizing my 3-day long failed run1 and it felt exactly as broken\
          \ as when I created a fresh measurement.json file a day later, so measurement.json\
          \ file compatibility isn't a big issue as long as you stay within the same\
          \ base model. To me it seems like 3.1-2 and 3.1-3 share the same small issue\
          \ with adding unnecessary characters just before EOS.\n\nEdit: I re-run\
          \ lora merge to base to make sure I didn't merge 3.1-2 by mistake. SHA256\
          \ on safetensors shards that I checked have exactly the same hash on both\
          \ merges, so I didn't make any mistakes when merging."
        updatedAt: '2023-11-26T01:24:50.591Z'
      numEdits: 1
      reactions: []
    id: 6562327784a9fbe322d3c01f
    type: comment
  author: adamo1139
  content: ">I still do not know why I had the various issues I had with the strange\
    \ random characters at the end of one of my training runs. A re-train and requant\
    \ did not have the issue.\n\nI still have this issue with 3.1-3. I merged the\
    \ qlora from here https://huggingface.co/LoneStriker/Yi-34B-Spicyboros-3.1-3-LoRA\
    \ with base Yi-34B and quantized it using measurement.json for your 3.1-2 since\
    \ I didn't want to spend an hour measuring loss on various layers today. It has\
    \ the issue with responses ending with <|unusedx|> or .x \nI ran SHA256 check\
    \ on the adapter.safetensors file that I have to make sure I didn't use another\
    \ file from previous run by mistake. It matches SHA256 from HF from 3.1-3  `SHA256:\
    \ 265C00DF626AC42663A5ADD818F0840A57DC8CB4CDDA0C23C5D67E661C87C6A6`\n\nHere's\
    \ sample of output that I am getting. I am using system prompt \"A chat.\". Using\
    \ \"A chat\" without a dot has the same effect, i didn't try other system prompts.\n\
    ```\nA chat.\nUser: write a joke about llamas\nWhy did the llama refuse to go\
    \ to the party? It was afraid of the mingling.\nUser: write another one\nWhy did\
    \ the llama break up with the llama? It was tired of being taken for a ride.<|unused137|>\n\
    User: write a joke about Guanacos\nWhy did the guanaco join the gym? It wanted\
    \ to work on its neck gains.S\n```\n\n\n```\nA chat.\nUser: what's the average\
    \ weather like in Egypt in June?\nIn June, Egypt experiences hot and dry weather.\
    \ The average high temperature is around 35\xB0C (95\xB0F), and the average low\
    \ temperature is around 25\xB0C (77\xB0F). However, it's important to note that\
    \ these are averages and actual temperatures can vary. There's also very little\
    \ rainfall during this month.<|unused010|>\n```\n\nTested only in exllama v2 cli\
    \ with chat.py and chat.py that I modified to not insert BOS and have different\
    \ default repp\n\nLoRA adapters for versions 3.1-2 and 3.1-3 uploaded by you have\
    \ different SHA256, so it can't be that you uploaded older LoRA file twice.\n\
    Can you please try to replicate it yourself? It happened in about 80% of chats\
    \ I had so far. I don't think that using a different measurement.json file would\
    \ make this difference. I used measurement.json files from different fine-tunes\
    \ last week when I was quantizing my 3-day long failed run1 and it felt exactly\
    \ as broken as when I created a fresh measurement.json file a day later, so measurement.json\
    \ file compatibility isn't a big issue as long as you stay within the same base\
    \ model. To me it seems like 3.1-2 and 3.1-3 share the same small issue with adding\
    \ unnecessary characters just before EOS.\n\nEdit: I re-run lora merge to base\
    \ to make sure I didn't merge 3.1-2 by mistake. SHA256 on safetensors shards that\
    \ I checked have exactly the same hash on both merges, so I didn't make any mistakes\
    \ when merging."
  created_at: 2023-11-25 17:44:23+00:00
  edited: true
  hidden: false
  id: 6562327784a9fbe322d3c01f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Yi-34B-Spicyboros-3.1
repo_type: model
status: closed
target_branch: null
title: Adapter files?
