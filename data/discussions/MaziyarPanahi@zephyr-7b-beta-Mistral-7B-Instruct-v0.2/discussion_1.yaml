!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zappa2005
conflicting_files: null
created_at: 2024-01-21 13:12:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-21T13:12:41.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9320862889289856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>Hi Maziyar, thanks the upload! zepyhr and Mistral-Instruct seems
          like a very good combination for RP. Can you help me to find the actual
          bpw for this model - it is not in the name, and I have no clue where to
          find the value otherwise in the model card.</p>

          <p>Thank you!</p>

          '
        raw: "Hi Maziyar, thanks the upload! zepyhr and Mistral-Instruct seems like\
          \ a very good combination for RP. Can you help me to find the actual bpw\
          \ for this model - it is not in the name, and I have no clue where to find\
          \ the value otherwise in the model card.\r\n\r\nThank you!"
        updatedAt: '2024-01-21T13:12:41.693Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
    id: 65ad18490150f64adf91e7b4
    type: comment
  author: zappa2005
  content: "Hi Maziyar, thanks the upload! zepyhr and Mistral-Instruct seems like\
    \ a very good combination for RP. Can you help me to find the actual bpw for this\
    \ model - it is not in the name, and I have no clue where to find the value otherwise\
    \ in the model card.\r\n\r\nThank you!"
  created_at: 2024-01-21 13:12:41+00:00
  edited: false
  hidden: false
  id: 65ad18490150f64adf91e7b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-21T18:27:19.000Z'
    data:
      edited: true
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9021971821784973
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;zappa2005&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zappa2005\"\
          >@<span class=\"underline\">zappa2005</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Thanks for the feedback and I am glad it is interesting for RP\
          \ task. </p>\n<blockquote>\n<p>Can you help me to find the actual bpw for\
          \ this model</p>\n</blockquote>\n<p>Absolutely! As far as I know this is\
          \ a merge of <code>mistralai/Mistral-7B-Instruct-v0.2</code> and <code>HuggingFaceH4/zephyr-7b-beta</code>\
          \ original models in 16bits (unless mergekit used 32bit by default). This\
          \ model is not quantized. I know how to do GPTQ if that's helpful, and I\
          \ can upload it after.</p>\n"
        raw: "Hi @zappa2005 \n\nThanks for the feedback and I am glad it is interesting\
          \ for RP task. \n\n> Can you help me to find the actual bpw for this model\n\
          \nAbsolutely! As far as I know this is a merge of `mistralai/Mistral-7B-Instruct-v0.2`\
          \ and `HuggingFaceH4/zephyr-7b-beta` original models in 16bits (unless mergekit\
          \ used 32bit by default). This model is not quantized. I know how to do\
          \ GPTQ if that's helpful, and I can upload it after."
        updatedAt: '2024-01-21T18:28:24.337Z'
      numEdits: 2
      reactions: []
    id: 65ad6207c3fa44c71079211e
    type: comment
  author: MaziyarPanahi
  content: "Hi @zappa2005 \n\nThanks for the feedback and I am glad it is interesting\
    \ for RP task. \n\n> Can you help me to find the actual bpw for this model\n\n\
    Absolutely! As far as I know this is a merge of `mistralai/Mistral-7B-Instruct-v0.2`\
    \ and `HuggingFaceH4/zephyr-7b-beta` original models in 16bits (unless mergekit\
    \ used 32bit by default). This model is not quantized. I know how to do GPTQ if\
    \ that's helpful, and I can upload it after."
  created_at: 2024-01-21 18:27:19+00:00
  edited: true
  hidden: false
  id: 65ad6207c3fa44c71079211e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-23T14:42:44.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.981633186340332
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>GPTQ is 4bit then? I am also not clever enough to quantize myself,
          but the GPTQ would be a nice test!</p>

          '
        raw: GPTQ is 4bit then? I am also not clever enough to quantize myself, but
          the GPTQ would be a nice test!
        updatedAt: '2024-01-23T14:42:44.530Z'
      numEdits: 0
      reactions: []
    id: 65afd0647e5d5a4ecc2be366
    type: comment
  author: zappa2005
  content: GPTQ is 4bit then? I am also not clever enough to quantize myself, but
    the GPTQ would be a nice test!
  created_at: 2024-01-23 14:42:44+00:00
  edited: false
  hidden: false
  id: 65afd0647e5d5a4ecc2be366
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-23T15:01:46.000Z'
    data:
      edited: true
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.940635085105896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>Btw, what is the difference between loading a FP16 model via "load-in-8bit"
          or "load-in-4bit" compared to a 4 or 8bpw quant?</p>

          <p>EDIT: Answer myself: load-in-8bit compared to load-in-16 bit is dramatically
          slower in terms of inference. :-)</p>

          '
        raw: 'Btw, what is the difference between loading a FP16 model via "load-in-8bit"
          or "load-in-4bit" compared to a 4 or 8bpw quant?


          EDIT: Answer myself: load-in-8bit compared to load-in-16 bit is dramatically
          slower in terms of inference. :-)'
        updatedAt: '2024-01-23T15:03:20.827Z'
      numEdits: 1
      reactions: []
    id: 65afd4da7b63500ee00ca3e1
    type: comment
  author: zappa2005
  content: 'Btw, what is the difference between loading a FP16 model via "load-in-8bit"
    or "load-in-4bit" compared to a 4 or 8bpw quant?


    EDIT: Answer myself: load-in-8bit compared to load-in-16 bit is dramatically slower
    in terms of inference. :-)'
  created_at: 2024-01-23 15:01:46+00:00
  edited: true
  hidden: false
  id: 65afd4da7b63500ee00ca3e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-23T15:29:29.000Z'
    data:
      edited: true
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8597539663314819
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;zappa2005&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zappa2005\">@<span class=\"\
          underline\">zappa2005</span></a></span>\n\n\t</span></span> Actually, I\
          \ now know how to do both GGUF as well as GPTQ. So you can have from 2-bit\
          \ all the way to 8-bit in GGUF (CPU and GPU) or GPTQ (4-bit only GPUs):</p>\n\
          <ul>\n<li><a href=\"https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GPTQ\"\
          >GPTQ models for GPU inference, with multiple quantisation parameter options.</a></li>\n\
          <li><a href=\"https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GGUF\"\
          >2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference</a></li>\n</ul>\n\
          <p>The native load in 4-bit or 8-bit in Hugging Face uses Bitsandbytes,\
          \ I read a good comment on the difference between that and GPTQ:</p>\n<blockquote>\n\
          <p>GPTQ uses Integer quantization + an optimization procedure that relies\
          \ on an input mini-batch to perform the quantization. Once the quantization\
          \ is completed, the weights can be stored and reused. Bitsandbytes can perform\
          \ integer quantization but also supports many other formats. However, bitsandbytes\
          \ does not perform an optimization procedure that involves an input mini-batch\
          \ to perform quantization. That is why it can be used directly for any model.\
          \ However, it is less precise than GPTQ since information of the input data\
          \ helps with quantization. - <a rel=\"nofollow\" href=\"https://github.com/TimDettmers/bitsandbytes/issues/539\"\
          >https://github.com/TimDettmers/bitsandbytes/issues/539</a></p>\n</blockquote>\n"
        raw: '@zappa2005 Actually, I now know how to do both GGUF as well as GPTQ.
          So you can have from 2-bit all the way to 8-bit in GGUF (CPU and GPU) or
          GPTQ (4-bit only GPUs):


          * [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GPTQ)

          * [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GGUF)


          The native load in 4-bit or 8-bit in Hugging Face uses Bitsandbytes, I read
          a good comment on the difference between that and GPTQ:


          >GPTQ uses Integer quantization + an optimization procedure that relies
          on an input mini-batch to perform the quantization. Once the quantization
          is completed, the weights can be stored and reused. Bitsandbytes can perform
          integer quantization but also supports many other formats. However, bitsandbytes
          does not perform an optimization procedure that involves an input mini-batch
          to perform quantization. That is why it can be used directly for any model.
          However, it is less precise than GPTQ since information of the input data
          helps with quantization. - https://github.com/TimDettmers/bitsandbytes/issues/539


          '
        updatedAt: '2024-01-23T15:30:16.049Z'
      numEdits: 1
      reactions: []
    id: 65afdb5937fdb5d6b971359e
    type: comment
  author: MaziyarPanahi
  content: '@zappa2005 Actually, I now know how to do both GGUF as well as GPTQ. So
    you can have from 2-bit all the way to 8-bit in GGUF (CPU and GPU) or GPTQ (4-bit
    only GPUs):


    * [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GPTQ)

    * [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GGUF)


    The native load in 4-bit or 8-bit in Hugging Face uses Bitsandbytes, I read a
    good comment on the difference between that and GPTQ:


    >GPTQ uses Integer quantization + an optimization procedure that relies on an
    input mini-batch to perform the quantization. Once the quantization is completed,
    the weights can be stored and reused. Bitsandbytes can perform integer quantization
    but also supports many other formats. However, bitsandbytes does not perform an
    optimization procedure that involves an input mini-batch to perform quantization.
    That is why it can be used directly for any model. However, it is less precise
    than GPTQ since information of the input data helps with quantization. - https://github.com/TimDettmers/bitsandbytes/issues/539


    '
  created_at: 2024-01-23 15:29:29+00:00
  edited: true
  hidden: false
  id: 65afdb5937fdb5d6b971359e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-23T15:33:50.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9698691368103027
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>Thank you! I could pre-test it with Transformers and load-in-4bit,
          which also manages to cope with 32k context on my 16G 4080 - which is nice!</p>

          <p>If the manual processed quant is better, I''d also like to test the 4
          and 5bit GGUF. But only if you have time for it - I should seriously start
          looking into quantizing stuff for myself...</p>

          '
        raw: 'Thank you! I could pre-test it with Transformers and load-in-4bit, which
          also manages to cope with 32k context on my 16G 4080 - which is nice!


          If the manual processed quant is better, I''d also like to test the 4 and
          5bit GGUF. But only if you have time for it - I should seriously start looking
          into quantizing stuff for myself...'
        updatedAt: '2024-01-23T15:33:50.865Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
    id: 65afdc5e04655dc934daf3cc
    type: comment
  author: zappa2005
  content: 'Thank you! I could pre-test it with Transformers and load-in-4bit, which
    also manages to cope with 32k context on my 16G 4080 - which is nice!


    If the manual processed quant is better, I''d also like to test the 4 and 5bit
    GGUF. But only if you have time for it - I should seriously start looking into
    quantizing stuff for myself...'
  created_at: 2024-01-23 15:33:50+00:00
  edited: false
  hidden: false
  id: 65afdc5e04655dc934daf3cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-23T15:43:00.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.899773359298706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<blockquote>

          <p>Thank you! I could pre-test it with Transformers and load-in-4bit, which
          also manages to cope with 32k context on my 16G 4080 - which is nice!</p>

          <p>If the manual processed quant is better, I''d also like to test the 4
          and 5bit GGUF. But only if you have time for it - I should seriously start
          looking into quantizing stuff for myself...</p>

          </blockquote>

          <p>I am glad it''s useful to you. All the GGUF files (4 and 5 bits) are
          here, you can easily download and test them: <a href="https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GGUF/tree/main">https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GGUF/tree/main</a></p>

          <p>I was told <code>Q5_K_M</code> quantized models are the best in terms
          of quality to vRAM requirement ratio.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/_0nYID7pXz13Z2PuNBrXN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/_0nYID7pXz13Z2PuNBrXN.png"></a></p>

          '
        raw: "> Thank you! I could pre-test it with Transformers and load-in-4bit,\
          \ which also manages to cope with 32k context on my 16G 4080 - which is\
          \ nice!\n> \n> If the manual processed quant is better, I'd also like to\
          \ test the 4 and 5bit GGUF. But only if you have time for it - I should\
          \ seriously start looking into quantizing stuff for myself...\n\nI am glad\
          \ it's useful to you. All the GGUF files (4 and 5 bits) are here, you can\
          \ easily download and test them: https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GGUF/tree/main\n\
          \nI was told `Q5_K_M` quantized models are the best in terms of quality\
          \ to vRAM requirement ratio.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/_0nYID7pXz13Z2PuNBrXN.png)\n\
          \n"
        updatedAt: '2024-01-23T15:43:00.223Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - zappa2005
    id: 65afde843997c4b6d2e1d9e5
    type: comment
  author: MaziyarPanahi
  content: "> Thank you! I could pre-test it with Transformers and load-in-4bit, which\
    \ also manages to cope with 32k context on my 16G 4080 - which is nice!\n> \n\
    > If the manual processed quant is better, I'd also like to test the 4 and 5bit\
    \ GGUF. But only if you have time for it - I should seriously start looking into\
    \ quantizing stuff for myself...\n\nI am glad it's useful to you. All the GGUF\
    \ files (4 and 5 bits) are here, you can easily download and test them: https://huggingface.co/MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2-GGUF/tree/main\n\
    \nI was told `Q5_K_M` quantized models are the best in terms of quality to vRAM\
    \ requirement ratio.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/_0nYID7pXz13Z2PuNBrXN.png)\n\
    \n"
  created_at: 2024-01-23 15:43:00+00:00
  edited: false
  hidden: false
  id: 65afde843997c4b6d2e1d9e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-23T16:14:24.000Z'
    data:
      edited: true
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.898033618927002
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>I noticed you added details to your model info page, also regarding
          the prompt style (was missing before). Your sample looks like ChatML, but
          Mistral is using this [INST] xxx [/INST] stuff.</p>

          <p>So I should switch to ChatML with the HTML-like tags?</p>

          '
        raw: 'I noticed you added details to your model info page, also regarding
          the prompt style (was missing before). Your sample looks like ChatML, but
          Mistral is using this [INST] xxx [/INST] stuff.


          So I should switch to ChatML with the HTML-like tags?'
        updatedAt: '2024-01-23T16:15:01.132Z'
      numEdits: 1
      reactions: []
    id: 65afe5e0bf7d3327d04e94df
    type: comment
  author: zappa2005
  content: 'I noticed you added details to your model info page, also regarding the
    prompt style (was missing before). Your sample looks like ChatML, but Mistral
    is using this [INST] xxx [/INST] stuff.


    So I should switch to ChatML with the HTML-like tags?'
  created_at: 2024-01-23 16:14:24+00:00
  edited: true
  hidden: false
  id: 65afe5e0bf7d3327d04e94df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-24T11:35:10.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9313033819198608
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<blockquote>

          <p>I noticed you added details to your model info page, also regarding the
          prompt style (was missing before). Your sample looks like ChatML, but Mistral
          is using this [INST] xxx [/INST] stuff.</p>

          <p>So I should switch to ChatML with the HTML-like tags?</p>

          </blockquote>

          <p>That''s just an example, since these are merges of two different models,
          two or more different prompt templates can be used. But I''d agree that
          the safest is to go with the Mistral template, and if not the template from
          the second original model would be an option as well</p>

          '
        raw: "> I noticed you added details to your model info page, also regarding\
          \ the prompt style (was missing before). Your sample looks like ChatML,\
          \ but Mistral is using this [INST] xxx [/INST] stuff.\n> \n> So I should\
          \ switch to ChatML with the HTML-like tags?\n\nThat's just an example, since\
          \ these are merges of two different models, two or more different prompt\
          \ templates can be used. But I'd agree that the safest is to go with the\
          \ Mistral template, and if not the template from the second original model\
          \ would be an option as well"
        updatedAt: '2024-01-24T11:35:10.713Z'
      numEdits: 0
      reactions: []
    id: 65b0f5eeb2c2baf2ff3dea89
    type: comment
  author: MaziyarPanahi
  content: "> I noticed you added details to your model info page, also regarding\
    \ the prompt style (was missing before). Your sample looks like ChatML, but Mistral\
    \ is using this [INST] xxx [/INST] stuff.\n> \n> So I should switch to ChatML\
    \ with the HTML-like tags?\n\nThat's just an example, since these are merges of\
    \ two different models, two or more different prompt templates can be used. But\
    \ I'd agree that the safest is to go with the Mistral template, and if not the\
    \ template from the second original model would be an option as well"
  created_at: 2024-01-24 11:35:10+00:00
  edited: false
  hidden: false
  id: 65b0f5eeb2c2baf2ff3dea89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-24T23:50:22.000Z'
    data:
      edited: true
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8818932175636292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>I tried the Q8_0 quant and I did not manage to inference it. It
          loaded with llama.cpp, and upon querying in ooga it just spit out gibberish.
          Crash with llama.cpp_HF and same crash with koboldCPP btw.</p>

          <p>[WinError -529697949] Windows Error 0xe06d7363</p>

          <p>Can you confirm, or is it something on my side?</p>

          '
        raw: 'I tried the Q8_0 quant and I did not manage to inference it. It loaded
          with llama.cpp, and upon querying in ooga it just spit out gibberish. Crash
          with llama.cpp_HF and same crash with koboldCPP btw.


          [WinError -529697949] Windows Error 0xe06d7363


          Can you confirm, or is it something on my side?

          '
        updatedAt: '2024-01-24T23:58:37.130Z'
      numEdits: 1
      reactions: []
    id: 65b1a23ef32c79fea7139fce
    type: comment
  author: zappa2005
  content: 'I tried the Q8_0 quant and I did not manage to inference it. It loaded
    with llama.cpp, and upon querying in ooga it just spit out gibberish. Crash with
    llama.cpp_HF and same crash with koboldCPP btw.


    [WinError -529697949] Windows Error 0xe06d7363


    Can you confirm, or is it something on my side?

    '
  created_at: 2024-01-24 23:50:22+00:00
  edited: true
  hidden: false
  id: 65b1a23ef32c79fea7139fce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: MaziyarPanahi/zephyr-7b-beta-Mistral-7B-Instruct-v0.2
repo_type: model
status: open
target_branch: null
title: Quant?
