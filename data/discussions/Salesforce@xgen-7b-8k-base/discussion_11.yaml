!!python/object:huggingface_hub.community.DiscussionWithDetails
author: viktor-ferenczi
conflicting_files: null
created_at: 2023-07-02 16:52:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-07-02T17:52:00.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5139585137367249
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: "<p>Trying to load the model into the Text Generation Web UI (ooba).</p>\n\
          <p>The weights load into the GPU properly. At the end of loading there is\
          \ a crash:</p>\n<pre><code>Traceback (most recent call last):\n  File \"\
          /workspace/text-generation-webui/server.py\", line 67, in load_model_wrapper\n\
          \    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\n\
          \  File \"/workspace/text-generation-webui/modules/models.py\", line 82,\
          \ in load_model\n    tokenizer = load_tokenizer(model_name, model)\n  File\
          \ \"/workspace/text-generation-webui/modules/models.py\", line 107, in load_tokenizer\n\
          \    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
          ), clean_up_tokenization_spaces=True)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 1825, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 1988, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\n    self.sp_model.Load(vocab_file)\n  File \"/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"\
          /usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\", line\
          \ 310, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nTypeError: not a string\n</code></pre>\n<p>This same instant worked\
          \ with all other transformer models I tried today (Falcon-40B, MPT-30b,\
          \ WizardCoder).</p>\n"
        raw: "Trying to load the model into the Text Generation Web UI (ooba).\r\n\
          \r\nThe weights load into the GPU properly. At the end of loading there\
          \ is a crash:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File\
          \ \"/workspace/text-generation-webui/server.py\", line 67, in load_model_wrapper\r\
          \n    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \n  File \"/workspace/text-generation-webui/modules/models.py\", line 82,\
          \ in load_model\r\n    tokenizer = load_tokenizer(model_name, model)\r\n\
          \  File \"/workspace/text-generation-webui/modules/models.py\", line 107,\
          \ in load_tokenizer\r\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 1825, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 1988, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"\
          /usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\", line\
          \ 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"\
          /usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\", line\
          \ 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nTypeError: not a string\r\n```\r\n\r\nThis same instant worked\
          \ with all other transformer models I tried today (Falcon-40B, MPT-30b,\
          \ WizardCoder)."
        updatedAt: '2023-07-02T17:52:00.490Z'
      numEdits: 0
      reactions: []
    id: 64a1b940f20126f9d0e012a4
    type: comment
  author: viktor-ferenczi
  content: "Trying to load the model into the Text Generation Web UI (ooba).\r\n\r\
    \nThe weights load into the GPU properly. At the end of loading there is a crash:\r\
    \n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/text-generation-webui/server.py\"\
    , line 67, in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\n  File \"/workspace/text-generation-webui/modules/models.py\", line\
    \ 82, in load_model\r\n    tokenizer = load_tokenizer(model_name, model)\r\n \
    \ File \"/workspace/text-generation-webui/modules/models.py\", line 107, in load_tokenizer\r\
    \n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
    ), clean_up_tokenization_spaces=True)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
    , line 1825, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
    , line 1988, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\"\
    , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\nTypeError: not a string\r\n```\r\n\r\nThis same instant worked with\
    \ all other transformer models I tried today (Falcon-40B, MPT-30b, WizardCoder)."
  created_at: 2023-07-02 16:52:00+00:00
  edited: false
  hidden: false
  id: 64a1b940f20126f9d0e012a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-07-02T17:55:26.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8681467771530151
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: '<p>Found this issue in the discussion of another model:<br><a href="https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/15">https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/15</a></p>

          <p>Root cause is most likely that the <code>tokenizer.model</code> file
          is missing. It is not in a downloadable files either.</p>

          '
        raw: 'Found this issue in the discussion of another model:

          https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/15


          Root cause is most likely that the `tokenizer.model` file is missing. It
          is not in a downloadable files either.'
        updatedAt: '2023-07-02T17:55:26.022Z'
      numEdits: 0
      reactions: []
    id: 64a1ba0eeacb4b50ba232211
    type: comment
  author: viktor-ferenczi
  content: 'Found this issue in the discussion of another model:

    https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/15


    Root cause is most likely that the `tokenizer.model` file is missing. It is not
    in a downloadable files either.'
  created_at: 2023-07-02 16:55:26+00:00
  edited: false
  hidden: false
  id: 64a1ba0eeacb4b50ba232211
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648750941188-619aa4e2198a95c9b962252e.jpeg?w=200&h=200&f=face
      fullname: Hiroaki Hayashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rooa
      type: user
    createdAt: '2023-07-02T18:12:13.000Z'
    data:
      edited: false
      editors:
      - rooa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6568322777748108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648750941188-619aa4e2198a95c9b962252e.jpeg?w=200&h=200&f=face
          fullname: Hiroaki Hayashi
          isHf: false
          isPro: false
          name: rooa
          type: user
        html: "<p>It seems the system assumes to use LlamaTokenizer, but we do not\
          \ use it; we instead use OpenAI Tiktoken library.<br>If you would like to\
          \ integrate the model into text-generation-webui, please change </p>\n<pre><code>\
          \ File \"/workspace/text-generation-webui/modules/models.py\", line 107,\
          \ in load_tokenizer\n   tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\n\
          </code></pre>\n<p>with <code>AutoTokenizer</code> following the instruction\
          \ on the model card.</p>\n"
        raw: "It seems the system assumes to use LlamaTokenizer, but we do not use\
          \ it; we instead use OpenAI Tiktoken library.\nIf you would like to integrate\
          \ the model into text-generation-webui, please change \n\n```\n File \"\
          /workspace/text-generation-webui/modules/models.py\", line 107, in load_tokenizer\n\
          \   tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
          ), clean_up_tokenization_spaces=True)\n```\nwith `AutoTokenizer` following\
          \ the instruction on the model card."
        updatedAt: '2023-07-02T18:12:13.353Z'
      numEdits: 0
      reactions: []
    id: 64a1bdfd720ef7b92d319fa2
    type: comment
  author: rooa
  content: "It seems the system assumes to use LlamaTokenizer, but we do not use it;\
    \ we instead use OpenAI Tiktoken library.\nIf you would like to integrate the\
    \ model into text-generation-webui, please change \n\n```\n File \"/workspace/text-generation-webui/modules/models.py\"\
    , line 107, in load_tokenizer\n   tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
    {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\n\
    ```\nwith `AutoTokenizer` following the instruction on the model card."
  created_at: 2023-07-02 17:12:13+00:00
  edited: false
  hidden: false
  id: 64a1bdfd720ef7b92d319fa2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-07-04T18:24:39.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594889879226685
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: '<p>Thank you very much for looking into this. I will make the above
          modification and try again. (I''ve been comparing medium-sized language
          models for simple coding tasks.)</p>

          '
        raw: Thank you very much for looking into this. I will make the above modification
          and try again. (I've been comparing medium-sized language models for simple
          coding tasks.)
        updatedAt: '2023-07-04T18:24:39.220Z'
      numEdits: 0
      reactions: []
    id: 64a463e75db6c484bc9f2826
    type: comment
  author: viktor-ferenczi
  content: Thank you very much for looking into this. I will make the above modification
    and try again. (I've been comparing medium-sized language models for simple coding
    tasks.)
  created_at: 2023-07-04 17:24:39+00:00
  edited: false
  hidden: false
  id: 64a463e75db6c484bc9f2826
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-07-04T22:12:33.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5427936315536499
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: "<p>Made the modification suggested above. Got this on loading the model:</p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"C:\\Dev\\LLM\\oobabooga_windows\\\
          text-generation-webui\\server.py\", line 1154, in &lt;module&gt;\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\n  File \"C:\\Dev\\LLM\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 82,\
          \ in load_model\n    tokenizer = load_tokenizer(model_name, model)\n  File\
          \ \"C:\\Dev\\LLM\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
          , line 107, in load_tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\n\
          \  File \"C:\\Dev\\LLM\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py\", line 688, in from_pretrained\n\
          \    raise ValueError(\nValueError: Tokenizer class XgenTokenizer does not\
          \ exist or is not currently imported.\n</code></pre>\n<p>The <code>XgenTokenizer</code>\
          \ is defined in <code>tokenization_xgen.py</code>, but somehow the <code>AutoTokenizer</code>\
          \ does not find it. Maybe a configuration issue?</p>\n<p>Also tried to install\
          \ the <code>tiktoken</code> library into ooba's private miniconda deployment,\
          \ but it did not change anything.</p>\n"
        raw: "Made the modification suggested above. Got this on loading the model:\n\
          \n```\nTraceback (most recent call last):\n  File \"C:\\Dev\\LLM\\oobabooga_windows\\\
          text-generation-webui\\server.py\", line 1154, in <module>\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\n  File \"C:\\Dev\\LLM\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 82,\
          \ in load_model\n    tokenizer = load_tokenizer(model_name, model)\n  File\
          \ \"C:\\Dev\\LLM\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
          , line 107, in load_tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\n\
          \  File \"C:\\Dev\\LLM\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py\", line 688, in from_pretrained\n\
          \    raise ValueError(\nValueError: Tokenizer class XgenTokenizer does not\
          \ exist or is not currently imported.\n```\n\nThe `XgenTokenizer` is defined\
          \ in `tokenization_xgen.py`, but somehow the `AutoTokenizer` does not find\
          \ it. Maybe a configuration issue?\n\nAlso tried to install the `tiktoken`\
          \ library into ooba's private miniconda deployment, but it did not change\
          \ anything."
        updatedAt: '2023-07-04T22:12:33.329Z'
      numEdits: 0
      reactions: []
    id: 64a499515db6c484bca3bf88
    type: comment
  author: viktor-ferenczi
  content: "Made the modification suggested above. Got this on loading the model:\n\
    \n```\nTraceback (most recent call last):\n  File \"C:\\Dev\\LLM\\oobabooga_windows\\\
    text-generation-webui\\server.py\", line 1154, in <module>\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name)\n  File \"C:\\Dev\\LLM\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\", line 82, in load_model\n    tokenizer\
    \ = load_tokenizer(model_name, model)\n  File \"C:\\Dev\\LLM\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\", line 107, in load_tokenizer\n   \
    \ tokenizer = AutoTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
    ), clean_up_tokenization_spaces=True)\n  File \"C:\\Dev\\LLM\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\"\
    , line 688, in from_pretrained\n    raise ValueError(\nValueError: Tokenizer class\
    \ XgenTokenizer does not exist or is not currently imported.\n```\n\nThe `XgenTokenizer`\
    \ is defined in `tokenization_xgen.py`, but somehow the `AutoTokenizer` does not\
    \ find it. Maybe a configuration issue?\n\nAlso tried to install the `tiktoken`\
    \ library into ooba's private miniconda deployment, but it did not change anything."
  created_at: 2023-07-04 21:12:33+00:00
  edited: false
  hidden: false
  id: 64a499515db6c484bca3bf88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24574113bfe000250196e935e4f7cde2.svg
      fullname: Tian Xie
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tianxie-sf
      type: user
    createdAt: '2023-07-04T22:24:36.000Z'
    data:
      edited: false
      editors:
      - tianxie-sf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23254060745239258
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24574113bfe000250196e935e4f7cde2.svg
          fullname: Tian Xie
          isHf: false
          isPro: false
          name: tianxie-sf
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;viktor-ferenczi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/viktor-ferenczi\"\
          >@<span class=\"underline\">viktor-ferenczi</span></a></span>\n\n\t</span></span>\
          \ try this: <code>tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/xgen-7b-8k-base\"\
          , trust_remote_code=True)</code></p>\n"
        raw: '@viktor-ferenczi try this: `tokenizer = AutoTokenizer.from_pretrained("Salesforce/xgen-7b-8k-base",
          trust_remote_code=True)`'
        updatedAt: '2023-07-04T22:24:36.736Z'
      numEdits: 0
      reactions: []
    id: 64a49c24bbc03d2b097c5ba0
    type: comment
  author: tianxie-sf
  content: '@viktor-ferenczi try this: `tokenizer = AutoTokenizer.from_pretrained("Salesforce/xgen-7b-8k-base",
    trust_remote_code=True)`'
  created_at: 2023-07-04 21:24:36+00:00
  edited: false
  hidden: false
  id: 64a49c24bbc03d2b097c5ba0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-07-04T23:10:38.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8222896456718445
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: '<p>Combined the above suggestions and changed line 107 of <code>text-generation-webui\modules\models.py</code>
          to:</p>

          <pre><code>tokenizer = AutoTokenizer.from_pretrained(Path(f"{shared.args.model_dir}/{model_name}/"),
          clean_up_tokenization_spaces=True, trust_remote_code=True)

          </code></pre>

          <p>Certainly it is not secure to keep it like this, but it was enough to
          get the model loaded into ooba.</p>

          <p>Thank you for all the help!</p>

          '
        raw: 'Combined the above suggestions and changed line 107 of `text-generation-webui\modules\models.py`
          to:

          ```

          tokenizer = AutoTokenizer.from_pretrained(Path(f"{shared.args.model_dir}/{model_name}/"),
          clean_up_tokenization_spaces=True, trust_remote_code=True)

          ```


          Certainly it is not secure to keep it like this, but it was enough to get
          the model loaded into ooba.


          Thank you for all the help!'
        updatedAt: '2023-07-04T23:10:38.540Z'
      numEdits: 0
      reactions: []
    id: 64a4a6ee4f46b933c88d6dbb
    type: comment
  author: viktor-ferenczi
  content: 'Combined the above suggestions and changed line 107 of `text-generation-webui\modules\models.py`
    to:

    ```

    tokenizer = AutoTokenizer.from_pretrained(Path(f"{shared.args.model_dir}/{model_name}/"),
    clean_up_tokenization_spaces=True, trust_remote_code=True)

    ```


    Certainly it is not secure to keep it like this, but it was enough to get the
    model loaded into ooba.


    Thank you for all the help!'
  created_at: 2023-07-04 22:10:38+00:00
  edited: false
  hidden: false
  id: 64a4a6ee4f46b933c88d6dbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-07-04T23:11:01.000Z'
    data:
      from: Error loading model
      to: Error loading model into Test Generation Web UI (ooba)
    id: 64a4a70537d7d6eb4faa43f1
    type: title-change
  author: viktor-ferenczi
  created_at: 2023-07-04 22:11:01+00:00
  id: 64a4a70537d7d6eb4faa43f1
  new_title: Error loading model into Test Generation Web UI (ooba)
  old_title: Error loading model
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
      fullname: GIulioGalvan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giuliogalvan
      type: user
    createdAt: '2023-07-05T09:38:08.000Z'
    data:
      edited: false
      editors:
      - giuliogalvan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8975536227226257
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
          fullname: GIulioGalvan
          isHf: false
          isPro: false
          name: giuliogalvan
          type: user
        html: '<p>I have the same problem deploying the model (sagemaker or managed
          endpoints). Is there a solution for this scenario too?</p>

          '
        raw: I have the same problem deploying the model (sagemaker or managed endpoints).
          Is there a solution for this scenario too?
        updatedAt: '2023-07-05T09:38:08.619Z'
      numEdits: 0
      reactions: []
    id: 64a53a000c2cf43cbb8f0f65
    type: comment
  author: giuliogalvan
  content: I have the same problem deploying the model (sagemaker or managed endpoints).
    Is there a solution for this scenario too?
  created_at: 2023-07-05 08:38:08+00:00
  edited: false
  hidden: false
  id: 64a53a000c2cf43cbb8f0f65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
      fullname: GIulioGalvan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giuliogalvan
      type: user
    createdAt: '2023-07-06T08:01:49.000Z'
    data:
      edited: false
      editors:
      - giuliogalvan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6636186838150024
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
          fullname: GIulioGalvan
          isHf: false
          isPro: false
          name: giuliogalvan
          type: user
        html: "<p>It would be super nice to have some feedback on this too as it is\
          \ not easy to change the code on the container images for text-generation-inference.\
          \ (I would like to use this model for custom training but I need to be sure\
          \ I can deploy it easily after training)</p>\n<p>This is a log extract from\
          \ AWS sagemaker after the invocation of huggingface_model.deploy()</p>\n\
          <pre><code>The tokenizer class you load from this checkpoint is not the\
          \ same type as the class this function is called from. It may result in\
          \ unexpected tokenization.\n\nThe tokenizer class you load from this checkpoint\
          \ is 'XgenTokenizer'. \n\nThe class this function is called from is 'LlamaTokenizer'.\n\
          \nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\n    server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize, trust_remote_code))\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 647, in run_until_complete\n    return future.result()\n\n\nError:\
          \ ShardCannotStart\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize, trust_remote_code)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 246, in get_model\n    return llama_cls(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 44, in __init__\n    tokenizer = LlamaTokenizer.from_pretrained(\n\
          \  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1975, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\n    self.sp_model.Load(vocab_file)\n  File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"\
          /opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\", line\
          \ 310, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\n</code></pre>\n<p>Any help will be very appreciated :)</p>\n"
        raw: "It would be super nice to have some feedback on this too as it is not\
          \ easy to change the code on the container images for text-generation-inference.\
          \ (I would like to use this model for custom training but I need to be sure\
          \ I can deploy it easily after training)\n\nThis is a log extract from AWS\
          \ sagemaker after the invocation of huggingface_model.deploy()\n\n    The\
          \ tokenizer class you load from this checkpoint is not the same type as\
          \ the class this function is called from. It may result in unexpected tokenization.\n\
          \n    The tokenizer class you load from this checkpoint is 'XgenTokenizer'.\
          \ \n\n    The class this function is called from is 'LlamaTokenizer'.\n\n\
          \    Traceback (most recent call last):\n      File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\n        sys.exit(app())\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\n        server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\n        asyncio.run(serve_inner(model_id, revision,\
          \ sharded, quantize, trust_remote_code))\n      File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n        return loop.run_until_complete(main)\n      File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
          \        return future.result()\n    \n \n    Error: ShardCannotStart\n\
          \      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n        model = get_model(model_id, revision,\
          \ sharded, quantize, trust_remote_code)\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 246, in get_model\n        return llama_cls(\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 44, in __init__\n        tokenizer = LlamaTokenizer.from_pretrained(\n\
          \      File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\n        return cls._from_pretrained(\n\
          \      File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1975, in _from_pretrained\n        tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n      File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\n        self.sp_model.Load(vocab_file)\n      File\
          \ \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",\
          \ line 905, in Load\n        return self.LoadFromFile(model_file)\n    \
          \  File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\n        return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\n\nAny help will be very appreciated :)"
        updatedAt: '2023-07-06T08:01:49.319Z'
      numEdits: 0
      reactions: []
    id: 64a674ed23a2677d210b1eea
    type: comment
  author: giuliogalvan
  content: "It would be super nice to have some feedback on this too as it is not\
    \ easy to change the code on the container images for text-generation-inference.\
    \ (I would like to use this model for custom training but I need to be sure I\
    \ can deploy it easily after training)\n\nThis is a log extract from AWS sagemaker\
    \ after the invocation of huggingface_model.deploy()\n\n    The tokenizer class\
    \ you load from this checkpoint is not the same type as the class this function\
    \ is called from. It may result in unexpected tokenization.\n\n    The tokenizer\
    \ class you load from this checkpoint is 'XgenTokenizer'. \n\n    The class this\
    \ function is called from is 'LlamaTokenizer'.\n\n    Traceback (most recent call\
    \ last):\n      File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n\
    \        sys.exit(app())\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 67, in serve\n        server.serve(model_id, revision, sharded, quantize,\
    \ trust_remote_code, uds_path)\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 155, in serve\n        asyncio.run(serve_inner(model_id, revision, sharded,\
    \ quantize, trust_remote_code))\n      File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\n        return loop.run_until_complete(main)\n      File \"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
    \        return future.result()\n    \n \n    Error: ShardCannotStart\n      File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 124, in serve_inner\n        model = get_model(model_id, revision, sharded,\
    \ quantize, trust_remote_code)\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 246, in get_model\n        return llama_cls(\n      File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 44, in __init__\n        tokenizer = LlamaTokenizer.from_pretrained(\n\
    \      File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
    , line 1812, in from_pretrained\n        return cls._from_pretrained(\n      File\
    \ \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\", line\
    \ 1975, in _from_pretrained\n        tokenizer = cls(*init_inputs, **init_kwargs)\n\
    \      File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\n        self.sp_model.Load(vocab_file)\n      File \"\
    /opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\", line 905,\
    \ in Load\n        return self.LoadFromFile(model_file)\n      File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\n        return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\n\nAny help will be very appreciated :)"
  created_at: 2023-07-06 07:01:49+00:00
  edited: false
  hidden: false
  id: 64a674ed23a2677d210b1eea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: Salesforce/xgen-7b-8k-base
repo_type: model
status: open
target_branch: null
title: Error loading model into Test Generation Web UI (ooba)
