!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Moses25
conflicting_files: null
created_at: 2023-07-11 05:39:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/424a002197d03b753bbb33b8f98f57c4.svg
      fullname: Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moses25
      type: user
    createdAt: '2023-07-11T06:39:50.000Z'
    data:
      edited: false
      editors:
      - Moses25
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9477794170379639
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/424a002197d03b753bbb33b8f98f57c4.svg
          fullname: Hu
          isHf: false
          isPro: false
          name: Moses25
          type: user
        html: '<p>I wan to add other language tokens based on this tokenizer, how
          to do that?</p>

          '
        raw: I wan to add other language tokens based on this tokenizer, how to do
          that?
        updatedAt: '2023-07-11T06:39:50.973Z'
      numEdits: 0
      reactions: []
    id: 64acf9365b181f6ccaa0c80f
    type: comment
  author: Moses25
  content: I wan to add other language tokens based on this tokenizer, how to do that?
  created_at: 2023-07-11 05:39:50+00:00
  edited: false
  hidden: false
  id: 64acf9365b181f6ccaa0c80f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6894d6b5eb31ef0cbf1a8f4e7c53011.svg
      fullname: Nate brake
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: njbrake
      type: user
    createdAt: '2023-07-25T12:48:11.000Z'
    data:
      edited: true
      editors:
      - njbrake
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8692501187324524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6894d6b5eb31ef0cbf1a8f4e7c53011.svg
          fullname: Nate brake
          isHf: false
          isPro: false
          name: njbrake
          type: user
        html: '<p>This tokenizer is a tiktoken tokenizer, so it''s not using the huggingface
          tokenizer. If you look at <code>model.config.vocab_size</code> and then
          <code>len(tokenizer)</code> you''ll see that there''s a mismatch of vocab
          sizes. Still working on figuring out what to do about it. But that''s why
          tokenizer.add_special_tokens() doesn''t work: the implementation is tiktoken
          so the functionality is different.</p>

          <p>You can take a look at their tokenizer.py file to see how to add tokens
          and such: <a href="https://huggingface.co/Salesforce/xgen-7b-8k-base/blob/main/tokenization_xgen.py">https://huggingface.co/Salesforce/xgen-7b-8k-base/blob/main/tokenization_xgen.py</a>
          </p>

          '
        raw: 'This tokenizer is a tiktoken tokenizer, so it''s not using the huggingface
          tokenizer. If you look at `model.config.vocab_size` and then `len(tokenizer)`
          you''ll see that there''s a mismatch of vocab sizes. Still working on figuring
          out what to do about it. But that''s why tokenizer.add_special_tokens()
          doesn''t work: the implementation is tiktoken so the functionality is different.


          You can take a look at their tokenizer.py file to see how to add tokens
          and such: https://huggingface.co/Salesforce/xgen-7b-8k-base/blob/main/tokenization_xgen.py '
        updatedAt: '2023-07-25T12:50:11.381Z'
      numEdits: 1
      reactions: []
    id: 64bfc48b6a1cdd3cd8f2d94d
    type: comment
  author: njbrake
  content: 'This tokenizer is a tiktoken tokenizer, so it''s not using the huggingface
    tokenizer. If you look at `model.config.vocab_size` and then `len(tokenizer)`
    you''ll see that there''s a mismatch of vocab sizes. Still working on figuring
    out what to do about it. But that''s why tokenizer.add_special_tokens() doesn''t
    work: the implementation is tiktoken so the functionality is different.


    You can take a look at their tokenizer.py file to see how to add tokens and such:
    https://huggingface.co/Salesforce/xgen-7b-8k-base/blob/main/tokenization_xgen.py '
  created_at: 2023-07-25 11:48:11+00:00
  edited: true
  hidden: false
  id: 64bfc48b6a1cdd3cd8f2d94d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/424a002197d03b753bbb33b8f98f57c4.svg
      fullname: Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moses25
      type: user
    createdAt: '2023-07-27T12:14:44.000Z'
    data:
      status: closed
    id: 64c25fb4217c1eff71ea6fb5
    type: status-change
  author: Moses25
  created_at: 2023-07-27 11:14:44+00:00
  id: 64c25fb4217c1eff71ea6fb5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: Salesforce/xgen-7b-8k-base
repo_type: model
status: closed
target_branch: null
title: how to add special tokens?
