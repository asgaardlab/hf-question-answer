!!python/object:huggingface_hub.community.DiscussionWithDetails
author: njbrake
conflicting_files: null
created_at: 2023-07-25 11:57:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6894d6b5eb31ef0cbf1a8f4e7c53011.svg
      fullname: Nate brake
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: njbrake
      type: user
    createdAt: '2023-07-25T12:57:07.000Z'
    data:
      edited: false
      editors:
      - njbrake
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9242346882820129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6894d6b5eb31ef0cbf1a8f4e7c53011.svg
          fullname: Nate brake
          isHf: false
          isPro: false
          name: njbrake
          type: user
        html: '<p>During my finetuning, I had loaded the tokenizer with a pad_token
          set, and then thought I had to resize the model position embeddings using</p>

          <pre><code>model.resize_token_embeddings(len(tokenizer))

          </code></pre>

          <p>However, since the model vocab_size is larger than the tokenizer vocab
          size, doing so actually shrinks the model vocab size, which I''m guessing
          contributed to why my training flopped. Should an error or something be
          thrown to prevent us from trying to resize/shrink the model vocab_size?</p>

          '
        raw: "During my finetuning, I had loaded the tokenizer with a pad_token set,\
          \ and then thought I had to resize the model position embeddings using\r\
          \n\r\n```\r\nmodel.resize_token_embeddings(len(tokenizer))\r\n```\r\n\r\n\
          However, since the model vocab_size is larger than the tokenizer vocab size,\
          \ doing so actually shrinks the model vocab size, which I'm guessing contributed\
          \ to why my training flopped. Should an error or something be thrown to\
          \ prevent us from trying to resize/shrink the model vocab_size?"
        updatedAt: '2023-07-25T12:57:07.906Z'
      numEdits: 0
      reactions: []
    id: 64bfc6a3f71e81cc25f94f01
    type: comment
  author: njbrake
  content: "During my finetuning, I had loaded the tokenizer with a pad_token set,\
    \ and then thought I had to resize the model position embeddings using\r\n\r\n\
    ```\r\nmodel.resize_token_embeddings(len(tokenizer))\r\n```\r\n\r\nHowever, since\
    \ the model vocab_size is larger than the tokenizer vocab size, doing so actually\
    \ shrinks the model vocab size, which I'm guessing contributed to why my training\
    \ flopped. Should an error or something be thrown to prevent us from trying to\
    \ resize/shrink the model vocab_size?"
  created_at: 2023-07-25 11:57:07+00:00
  edited: false
  hidden: false
  id: 64bfc6a3f71e81cc25f94f01
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: Salesforce/xgen-7b-8k-base
repo_type: model
status: open
target_branch: null
title: Issue if you try to resize position embeddings
