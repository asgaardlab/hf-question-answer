!!python/object:huggingface_hub.community.DiscussionWithDetails
author: awesomenes
conflicting_files: null
created_at: 2023-10-08 01:06:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fba8a5d0dc3e0020c6c14e1c5dba4fba.svg
      fullname: andy zhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: awesomenes
      type: user
    createdAt: '2023-10-08T02:06:49.000Z'
    data:
      edited: false
      editors:
      - awesomenes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2922406792640686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fba8a5d0dc3e0020c6c14e1c5dba4fba.svg
          fullname: andy zhu
          isHf: false
          isPro: false
          name: awesomenes
          type: user
        html: '<hr>

          <p>AttributeError                            Traceback (most recent call
          last)<br>Cell In[17], line 3<br>      1 pretrained_model_name = "./models/xgen-7b-8k-base"<br>      2
          # model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16)<br>----&gt;
          3 tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True)<br>      4
          # model</p>

          <p>File ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:738,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    736     if os.path.isdir(pretrained_model_name_or_path):<br>    737         tokenizer_class.register_for_auto_class()<br>--&gt;
          738     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    739 elif config_tokenizer_class is not None:<br>    740     tokenizer_class
          = None</p>

          <p>File ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2045,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          cache_dir, force_download, local_files_only, token, revision, *init_inputs,
          **kwargs)<br>   2042     else:<br>   2043         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          2045 return cls._from_pretrained(<br>   2046     resolved_vocab_files,<br>   2047     pretrained_model_name_or_path,<br>   2048     init_configuration,<br>   2049     *init_inputs,<br>   2050     token=token,<br>   2051     cache_dir=cache_dir,<br>   2052     local_files_only=local_files_only,<br>   2053     _commit_hash=commit_hash,<br>   2054     _is_local=is_local,<br>   2055     **kwargs,<br>   2056
          )</p>

          <p>File ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2256,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,
          *init_inputs, **kwargs)<br>   2254 # Instantiate the tokenizer.<br>   2255
          try:<br>-&gt; 2256     tokenizer = cls(*init_inputs, **init_kwargs)<br>   2257
          except OSError:<br>   2258     raise OSError(<br>   2259         "Unable
          to load vocabulary from file. "<br>   2260         "Please check that the
          provided vocabulary is accessible and not corrupted."<br>   2261     )</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:137,
          in XgenTokenizer.<strong>init</strong>(self, pad_token, eos_token, add_eos_token,
          add_special_tokens, **kwargs)<br>    135 pad_token_added = AddedToken(pad_token,
          lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token<br>    136
          eos_token_added = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token,
          str) else eos_token<br>--&gt; 137 super().<strong>init</strong>(<br>    138     pad_token=pad_token_added,<br>    139     eos_token=eos_token_added,<br>    140     add_eos_token=add_eos_token,<br>    141     add_special_tokens=add_special_tokens,<br>    142     **kwargs,<br>    143
          )<br>    144 self.add_eos_token = add_eos_token<br>    145 self.encoder
          = tiktoken_tokenizer(base="gpt2", pad_token=pad_token, add_special=add_special_tokens)</p>

          <p>File ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils.py:366,
          in PreTrainedTokenizer.<strong>init</strong>(self, **kwargs)<br>    362
          self._added_tokens_encoder: Dict[str, int] = {k.content: v for v, k in self._added_tokens_decoder.items()}<br>    364
          # 4. If some of the special tokens are not part of the vocab, we add them,
          at the end.<br>    365 # the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES
          following <code>tokenizers</code><br>--&gt; 366 self._add_tokens(self.all_special_tokens_extended,
          special_tokens=True)<br>    368 self._decode_use_source_tokenizer = False</p>

          <p>File ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils.py:462,
          in PreTrainedTokenizer._add_tokens(self, new_tokens, special_tokens)<br>    460
          if new_tokens is None:<br>    461     return added_tokens<br>--&gt; 462
          current_vocab = self.get_vocab().copy()<br>    463 new_idx = len(current_vocab)  #
          only call this once, len gives the last index + 1<br>    464 for token in
          new_tokens:</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:154,
          in XgenTokenizer.get_vocab(self)<br>    152 def get_vocab(self):<br>    153     """Returns
          vocab as a dict"""<br>--&gt; 154     vocab = {self.encoder.decode_single_token_bytes(i):
          i for i in range(self.vocab_size)}<br>    155     return vocab</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:150,
          in XgenTokenizer.vocab_size(self)<br>    147 @property<br>    148 def vocab_size(self):<br>    149     """Returns
          vocab size"""<br>--&gt; 150     return self.encoder.n_vocab</p>

          <p>AttributeError: ''XgenTokenizer'' object has no attribute ''encoder''</p>

          '
        raw: "---------------------------------------------------------------------------\r\
          \nAttributeError                            Traceback (most recent call\
          \ last)\r\nCell In[17], line 3\r\n      1 pretrained_model_name = \"./models/xgen-7b-8k-base\"\
          \r\n      2 # model = AutoModelForCausalLM.from_pretrained(pretrained_model_name,\
          \ torch_dtype=torch.bfloat16)\r\n----> 3 tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name,\
          \ trust_remote_code=True)\r\n      4 # model\r\n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:738,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    736     if os.path.isdir(pretrained_model_name_or_path):\r\
          \n    737         tokenizer_class.register_for_auto_class()\r\n--> 738 \
          \    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    739 elif config_tokenizer_class is not None:\r\
          \n    740     tokenizer_class = None\r\n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2045,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\r\n   2042     else:\r\n   2043         logger.info(f\"loading\
          \ file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
          -> 2045 return cls._from_pretrained(\r\n   2046     resolved_vocab_files,\r\
          \n   2047     pretrained_model_name_or_path,\r\n   2048     init_configuration,\r\
          \n   2049     *init_inputs,\r\n   2050     token=token,\r\n   2051     cache_dir=cache_dir,\r\
          \n   2052     local_files_only=local_files_only,\r\n   2053     _commit_hash=commit_hash,\r\
          \n   2054     _is_local=is_local,\r\n   2055     **kwargs,\r\n   2056 )\r\
          \n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2256,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\r\n   2254 # Instantiate\
          \ the tokenizer.\r\n   2255 try:\r\n-> 2256     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   2257 except OSError:\r\n   2258     raise OSError(\r\
          \n   2259         \"Unable to load vocabulary from file. \"\r\n   2260 \
          \        \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\r\n   2261     )\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:137,\
          \ in XgenTokenizer.__init__(self, pad_token, eos_token, add_eos_token, add_special_tokens,\
          \ **kwargs)\r\n    135 pad_token_added = AddedToken(pad_token, lstrip=False,\
          \ rstrip=False) if isinstance(pad_token, str) else pad_token\r\n    136\
          \ eos_token_added = AddedToken(eos_token, lstrip=False, rstrip=False) if\
          \ isinstance(eos_token, str) else eos_token\r\n--> 137 super().__init__(\r\
          \n    138     pad_token=pad_token_added,\r\n    139     eos_token=eos_token_added,\r\
          \n    140     add_eos_token=add_eos_token,\r\n    141     add_special_tokens=add_special_tokens,\r\
          \n    142     **kwargs,\r\n    143 )\r\n    144 self.add_eos_token = add_eos_token\r\
          \n    145 self.encoder = tiktoken_tokenizer(base=\"gpt2\", pad_token=pad_token,\
          \ add_special=add_special_tokens)\r\n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils.py:366,\
          \ in PreTrainedTokenizer.__init__(self, **kwargs)\r\n    362 self._added_tokens_encoder:\
          \ Dict[str, int] = {k.content: v for v, k in self._added_tokens_decoder.items()}\r\
          \n    364 # 4. If some of the special tokens are not part of the vocab,\
          \ we add them, at the end.\r\n    365 # the order of addition is the same\
          \ as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\r\n--> 366 self._add_tokens(self.all_special_tokens_extended,\
          \ special_tokens=True)\r\n    368 self._decode_use_source_tokenizer = False\r\
          \n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils.py:462,\
          \ in PreTrainedTokenizer._add_tokens(self, new_tokens, special_tokens)\r\
          \n    460 if new_tokens is None:\r\n    461     return added_tokens\r\n\
          --> 462 current_vocab = self.get_vocab().copy()\r\n    463 new_idx = len(current_vocab)\
          \  # only call this once, len gives the last index + 1\r\n    464 for token\
          \ in new_tokens:\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:154,\
          \ in XgenTokenizer.get_vocab(self)\r\n    152 def get_vocab(self):\r\n \
          \   153     \"\"\"Returns vocab as a dict\"\"\"\r\n--> 154     vocab = {self.encoder.decode_single_token_bytes(i):\
          \ i for i in range(self.vocab_size)}\r\n    155     return vocab\r\n\r\n\
          File ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:150,\
          \ in XgenTokenizer.vocab_size(self)\r\n    147 @property\r\n    148 def\
          \ vocab_size(self):\r\n    149     \"\"\"Returns vocab size\"\"\"\r\n-->\
          \ 150     return self.encoder.n_vocab\r\n\r\nAttributeError: 'XgenTokenizer'\
          \ object has no attribute 'encoder'"
        updatedAt: '2023-10-08T02:06:49.127Z'
      numEdits: 0
      reactions: []
    id: 65220eb9974423bd3edd0ef4
    type: comment
  author: awesomenes
  content: "---------------------------------------------------------------------------\r\
    \nAttributeError                            Traceback (most recent call last)\r\
    \nCell In[17], line 3\r\n      1 pretrained_model_name = \"./models/xgen-7b-8k-base\"\
    \r\n      2 # model = AutoModelForCausalLM.from_pretrained(pretrained_model_name,\
    \ torch_dtype=torch.bfloat16)\r\n----> 3 tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name,\
    \ trust_remote_code=True)\r\n      4 # model\r\n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:738,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    736     if os.path.isdir(pretrained_model_name_or_path):\r\n\
    \    737         tokenizer_class.register_for_auto_class()\r\n--> 738     return\
    \ tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\
    \n    739 elif config_tokenizer_class is not None:\r\n    740     tokenizer_class\
    \ = None\r\n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2045,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\r\n   2042     else:\r\n   2043         logger.info(f\"loading file\
    \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n-> 2045 return\
    \ cls._from_pretrained(\r\n   2046     resolved_vocab_files,\r\n   2047     pretrained_model_name_or_path,\r\
    \n   2048     init_configuration,\r\n   2049     *init_inputs,\r\n   2050    \
    \ token=token,\r\n   2051     cache_dir=cache_dir,\r\n   2052     local_files_only=local_files_only,\r\
    \n   2053     _commit_hash=commit_hash,\r\n   2054     _is_local=is_local,\r\n\
    \   2055     **kwargs,\r\n   2056 )\r\n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2256,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\r\n   2254 # Instantiate the tokenizer.\r\n   2255 try:\r\
    \n-> 2256     tokenizer = cls(*init_inputs, **init_kwargs)\r\n   2257 except OSError:\r\
    \n   2258     raise OSError(\r\n   2259         \"Unable to load vocabulary from\
    \ file. \"\r\n   2260         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\r\n   2261     )\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:137,\
    \ in XgenTokenizer.__init__(self, pad_token, eos_token, add_eos_token, add_special_tokens,\
    \ **kwargs)\r\n    135 pad_token_added = AddedToken(pad_token, lstrip=False, rstrip=False)\
    \ if isinstance(pad_token, str) else pad_token\r\n    136 eos_token_added = AddedToken(eos_token,\
    \ lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\r\n\
    --> 137 super().__init__(\r\n    138     pad_token=pad_token_added,\r\n    139\
    \     eos_token=eos_token_added,\r\n    140     add_eos_token=add_eos_token,\r\
    \n    141     add_special_tokens=add_special_tokens,\r\n    142     **kwargs,\r\
    \n    143 )\r\n    144 self.add_eos_token = add_eos_token\r\n    145 self.encoder\
    \ = tiktoken_tokenizer(base=\"gpt2\", pad_token=pad_token, add_special=add_special_tokens)\r\
    \n\r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils.py:366,\
    \ in PreTrainedTokenizer.__init__(self, **kwargs)\r\n    362 self._added_tokens_encoder:\
    \ Dict[str, int] = {k.content: v for v, k in self._added_tokens_decoder.items()}\r\
    \n    364 # 4. If some of the special tokens are not part of the vocab, we add\
    \ them, at the end.\r\n    365 # the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES\
    \ following `tokenizers`\r\n--> 366 self._add_tokens(self.all_special_tokens_extended,\
    \ special_tokens=True)\r\n    368 self._decode_use_source_tokenizer = False\r\n\
    \r\nFile ~/miniconda3/envs/fine-tuning/lib/python3.10/site-packages/transformers/tokenization_utils.py:462,\
    \ in PreTrainedTokenizer._add_tokens(self, new_tokens, special_tokens)\r\n   \
    \ 460 if new_tokens is None:\r\n    461     return added_tokens\r\n--> 462 current_vocab\
    \ = self.get_vocab().copy()\r\n    463 new_idx = len(current_vocab)  # only call\
    \ this once, len gives the last index + 1\r\n    464 for token in new_tokens:\r\
    \n\r\nFile ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:154,\
    \ in XgenTokenizer.get_vocab(self)\r\n    152 def get_vocab(self):\r\n    153\
    \     \"\"\"Returns vocab as a dict\"\"\"\r\n--> 154     vocab = {self.encoder.decode_single_token_bytes(i):\
    \ i for i in range(self.vocab_size)}\r\n    155     return vocab\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/xgen-7b-8k-base/tokenization_xgen.py:150,\
    \ in XgenTokenizer.vocab_size(self)\r\n    147 @property\r\n    148 def vocab_size(self):\r\
    \n    149     \"\"\"Returns vocab size\"\"\"\r\n--> 150     return self.encoder.n_vocab\r\
    \n\r\nAttributeError: 'XgenTokenizer' object has no attribute 'encoder'"
  created_at: 2023-10-08 01:06:49+00:00
  edited: false
  hidden: false
  id: 65220eb9974423bd3edd0ef4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcab9d6743bc102c5585b53ad2945982.svg
      fullname: donreen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donreen
      type: user
    createdAt: '2023-10-12T08:48:30.000Z'
    data:
      edited: false
      editors:
      - donreen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9287910461425781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcab9d6743bc102c5585b53ad2945982.svg
          fullname: donreen
          isHf: false
          isPro: false
          name: donreen
          type: user
        html: '<p>Maybe the version of transformers is too high, and  "pip install
          transformers==4.30.0" can deal with this problem.</p>

          '
        raw: Maybe the version of transformers is too high, and  "pip install transformers==4.30.0"
          can deal with this problem.
        updatedAt: '2023-10-12T08:48:30.064Z'
      numEdits: 0
      reactions: []
    id: 6527b2de8baa340969e17cf2
    type: comment
  author: donreen
  content: Maybe the version of transformers is too high, and  "pip install transformers==4.30.0"
    can deal with this problem.
  created_at: 2023-10-12 07:48:30+00:00
  edited: false
  hidden: false
  id: 6527b2de8baa340969e17cf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-12T12:56:45.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8685401082038879
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\">@<span class=\"\
          underline\">ArthurZ</span></a></span>\n\n\t</span></span> I think this is\
          \ fixed in the latest transformers indeed</p>\n<pre><code class=\"language-bash\"\
          >pip install -U transformers\n</code></pre>\n"
        raw: 'cc @ArthurZ I think this is fixed in the latest transformers indeed


          ```bash

          pip install -U transformers

          ```'
        updatedAt: '2023-10-12T12:56:45.057Z'
      numEdits: 0
      reactions: []
    id: 6527ed0dfdb72d7f54511501
    type: comment
  author: ybelkada
  content: 'cc @ArthurZ I think this is fixed in the latest transformers indeed


    ```bash

    pip install -U transformers

    ```'
  created_at: 2023-10-12 11:56:45+00:00
  edited: false
  hidden: false
  id: 6527ed0dfdb72d7f54511501
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24574113bfe000250196e935e4f7cde2.svg
      fullname: Tian Xie
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tianxie-sf
      type: user
    createdAt: '2023-10-24T17:38:59.000Z'
    data:
      edited: false
      editors:
      - tianxie-sf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9175747632980347
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24574113bfe000250196e935e4f7cde2.svg
          fullname: Tian Xie
          isHf: false
          isPro: false
          name: tianxie-sf
          type: user
        html: '<p>Hi everyone, please clear the hf cache, and retry.</p>

          '
        raw: Hi everyone, please clear the hf cache, and retry.
        updatedAt: '2023-10-24T17:38:59.276Z'
      numEdits: 0
      reactions: []
    id: 653801335ac3642f471e7476
    type: comment
  author: tianxie-sf
  content: Hi everyone, please clear the hf cache, and retry.
  created_at: 2023-10-24 16:38:59+00:00
  edited: false
  hidden: false
  id: 653801335ac3642f471e7476
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: Salesforce/xgen-7b-8k-base
repo_type: model
status: open
target_branch: null
title: 'xgen-7b-8k-base model tokenizer has a problem: AttributeError: ''XgenTokenizer''
  object has no attribute ''encoder'''
