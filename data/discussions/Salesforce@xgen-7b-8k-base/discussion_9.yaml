!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kalinasviatoslav
conflicting_files: null
created_at: 2023-06-30 07:00:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e5a0d0d2234566b6d96fb9dbd512510.svg
      fullname: Sviatoslav Kalina
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kalinasviatoslav
      type: user
    createdAt: '2023-06-30T08:00:37.000Z'
    data:
      edited: true
      editors:
      - kalinasviatoslav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9711285829544067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e5a0d0d2234566b6d96fb9dbd512510.svg
          fullname: Sviatoslav Kalina
          isHf: false
          isPro: false
          name: kalinasviatoslav
          type: user
        html: '<p>Please provide the recommended capacity of CPU, GPU, and RAM, or
          at least a description of the hardware which you use to run it. </p>

          <p>I used<br>16 GB of RAM<br>GPU 11gig (1080TI)<br>CPU i7 6800K </p>

          <p>I started model from scratch in 37 minutes( 100 mb/s internet)<br>each
          query execute very long ( even with GPU)<br>import torch<br>from transformers
          import AutoTokenizer, AutoModelForCausalLM</p>

          <p>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("Salesforce/xgen-7b-8k-base",
          trust_remote_code=True)<br>model = AutoModelForCausalLM.from_pretrained("Salesforce/xgen-7b-8k-base",
          torch_dtype=torch.bfloat16)<br>model = model.to(device)</p>

          <p>inputs = tokenizer("The world is", return_tensors="pt").to(device)<br>sample
          = model.generate(**inputs, max_length=128)<br>decoded_output = tokenizer.decode(sample[0])</p>

          <p>print(decoded_output)</p>

          <p>Please provide someone speed result which you achieve with your machine.</p>

          '
        raw: "Please provide the recommended capacity of CPU, GPU, and RAM, or at\
          \ least a description of the hardware which you use to run it. \n\nI used\n\
          16 GB of RAM \nGPU 11gig (1080TI) \nCPU i7 6800K \n\nI started model from\
          \ scratch in 37 minutes( 100 mb/s internet) \neach query execute very long\
          \ ( even with GPU) \nimport torch\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/xgen-7b-8k-base\"\
          , trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          Salesforce/xgen-7b-8k-base\", torch_dtype=torch.bfloat16)\nmodel = model.to(device)\n\
          \ninputs = tokenizer(\"The world is\", return_tensors=\"pt\").to(device)\n\
          sample = model.generate(**inputs, max_length=128)\ndecoded_output = tokenizer.decode(sample[0])\n\
          \nprint(decoded_output)\n\n\nPlease provide someone speed result which you\
          \ achieve with your machine.\n\n"
        updatedAt: '2023-06-30T10:03:08.921Z'
      numEdits: 5
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - andreP
        - Neman
        - ARMonte
    id: 649e8ba513e6f65bf6d24bab
    type: comment
  author: kalinasviatoslav
  content: "Please provide the recommended capacity of CPU, GPU, and RAM, or at least\
    \ a description of the hardware which you use to run it. \n\nI used\n16 GB of\
    \ RAM \nGPU 11gig (1080TI) \nCPU i7 6800K \n\nI started model from scratch in\
    \ 37 minutes( 100 mb/s internet) \neach query execute very long ( even with GPU)\
    \ \nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\
    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/xgen-7b-8k-base\", trust_remote_code=True)\n\
    model = AutoModelForCausalLM.from_pretrained(\"Salesforce/xgen-7b-8k-base\", torch_dtype=torch.bfloat16)\n\
    model = model.to(device)\n\ninputs = tokenizer(\"The world is\", return_tensors=\"\
    pt\").to(device)\nsample = model.generate(**inputs, max_length=128)\ndecoded_output\
    \ = tokenizer.decode(sample[0])\n\nprint(decoded_output)\n\n\nPlease provide someone\
    \ speed result which you achieve with your machine.\n\n"
  created_at: 2023-06-30 07:00:37+00:00
  edited: true
  hidden: false
  id: 649e8ba513e6f65bf6d24bab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: Salesforce/xgen-7b-8k-base
repo_type: model
status: open
target_branch: null
title: hardware  requirements
