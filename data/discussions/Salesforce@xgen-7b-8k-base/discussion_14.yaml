!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DataScienceGal
conflicting_files: null
created_at: 2023-07-03 15:20:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3c2aafec2f1f201b0f701b3413ec64e.svg
      fullname: Khushee Kapoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DataScienceGal
      type: user
    createdAt: '2023-07-03T16:20:11.000Z'
    data:
      edited: true
      editors:
      - DataScienceGal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3750167787075043
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3c2aafec2f1f201b0f701b3413ec64e.svg
          fullname: Khushee Kapoor
          isHf: false
          isPro: false
          name: DataScienceGal
          type: user
        html: '<hr>

          <p>KeyError                                  Traceback (most recent call
          last)<br>Cell In [4], line 1<br>----&gt; 1 model = AutoModelForCausalLM.from_pretrained("Salesforce/xgen-7b-8k-base",
          torch_dtype=torch.bfloat16).to("cuda")</p>

          <p>File /usr/local/lib/python3.9/dist-packages/transformers/models/auto/auto_factory.py:434,
          in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,
          *model_args, **kwargs)<br>    432 hub_kwargs = {name: kwargs.pop(name) for
          name in hub_kwargs_names if name in kwargs}<br>    433 if not isinstance(config,
          PretrainedConfig):<br>--&gt; 434     config, kwargs = AutoConfig.from_pretrained(<br>    435         pretrained_model_name_or_path,<br>    436         return_unused_kwargs=True,<br>    437         trust_remote_code=trust_remote_code,<br>    438         **hub_kwargs,<br>    439         **kwargs,<br>    440     )<br>    441
          if hasattr(config, "auto_map") and cls.<strong>name</strong> in config.auto_map:<br>    442     if
          not trust_remote_code:</p>

          <p>File /usr/local/lib/python3.9/dist-packages/transformers/models/auto/configuration_auto.py:829,
          in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>    827     return
          config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)<br>    828
          elif "model_type" in config_dict:<br>--&gt; 829     config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>    830     return
          config_class.from_dict(config_dict, **unused_kwargs)<br>    831 else:<br>    832     #
          Fallback: use pattern matching on the string.<br>    833     # We go from
          longer names to shorter names to catch roberta before bert (for instance)</p>

          <p>File /usr/local/lib/python3.9/dist-packages/transformers/models/auto/configuration_auto.py:536,
          in _LazyConfigMapping.<strong>getitem</strong>(self, key)<br>    534     return
          self._extra_content[key]<br>    535 if key not in self._mapping:<br>--&gt;
          536     raise KeyError(key)<br>    537 value = self._mapping[key]<br>    538
          module_name = model_type_to_module_name(key)</p>

          <p>KeyError: ''llama''</p>

          <p>Hi, I have used the exact same commands as in the readme, however, I
          am facing the above error in the line:<br>model = AutoModelForCausalLM.from_pretrained("Salesforce/xgen-7b-8k-base",
          torch_dtype=torch.bfloat16).to("cuda")</p>

          <p>It is able to download config.json though!</p>

          '
        raw: "---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In [4], line 1\n----> 1 model = AutoModelForCausalLM.from_pretrained(\"\
          Salesforce/xgen-7b-8k-base\", torch_dtype=torch.bfloat16).to(\"cuda\")\n\
          \nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/auto_factory.py:434,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for\
          \ name in hub_kwargs_names if name in kwargs}\n    433 if not isinstance(config,\
          \ PretrainedConfig):\n--> 434     config, kwargs = AutoConfig.from_pretrained(\n\
          \    435         pretrained_model_name_or_path,\n    436         return_unused_kwargs=True,\n\
          \    437         trust_remote_code=trust_remote_code,\n    438         **hub_kwargs,\n\
          \    439         **kwargs,\n    440     )\n    441 if hasattr(config, \"\
          auto_map\") and cls.__name__ in config.auto_map:\n    442     if not trust_remote_code:\n\
          \nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/configuration_auto.py:829,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    827     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n    828 elif \"model_type\" in config_dict:\n--> 829     config_class\
          \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    830     return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n    831 else:\n    832     # Fallback: use pattern matching\
          \ on the string.\n    833     # We go from longer names to shorter names\
          \ to catch roberta before bert (for instance)\n\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/configuration_auto.py:536,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    534     return self._extra_content[key]\n\
          \    535 if key not in self._mapping:\n--> 536     raise KeyError(key)\n\
          \    537 value = self._mapping[key]\n    538 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'llama'\n\nHi, I have used the exact same commands as in the\
          \ readme, however, I am facing the above error in the line:\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          Salesforce/xgen-7b-8k-base\", torch_dtype=torch.bfloat16).to(\"cuda\")\n\
          \nIt is able to download config.json though!"
        updatedAt: '2023-07-03T16:24:02.027Z'
      numEdits: 1
      reactions: []
    id: 64a2f53b8649c0f4aee2ab83
    type: comment
  author: DataScienceGal
  content: "---------------------------------------------------------------------------\n\
    KeyError                                  Traceback (most recent call last)\n\
    Cell In [4], line 1\n----> 1 model = AutoModelForCausalLM.from_pretrained(\"Salesforce/xgen-7b-8k-base\"\
    , torch_dtype=torch.bfloat16).to(\"cuda\")\n\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/auto_factory.py:434,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names\
    \ if name in kwargs}\n    433 if not isinstance(config, PretrainedConfig):\n-->\
    \ 434     config, kwargs = AutoConfig.from_pretrained(\n    435         pretrained_model_name_or_path,\n\
    \    436         return_unused_kwargs=True,\n    437         trust_remote_code=trust_remote_code,\n\
    \    438         **hub_kwargs,\n    439         **kwargs,\n    440     )\n   \
    \ 441 if hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map:\n\
    \    442     if not trust_remote_code:\n\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/configuration_auto.py:829,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
    \    827     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\n    828 elif \"model_type\" in config_dict:\n--> 829     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    830     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\n    831 else:\n    832     # Fallback: use pattern matching\
    \ on the string.\n    833     # We go from longer names to shorter names to catch\
    \ roberta before bert (for instance)\n\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/configuration_auto.py:536,\
    \ in _LazyConfigMapping.__getitem__(self, key)\n    534     return self._extra_content[key]\n\
    \    535 if key not in self._mapping:\n--> 536     raise KeyError(key)\n    537\
    \ value = self._mapping[key]\n    538 module_name = model_type_to_module_name(key)\n\
    \nKeyError: 'llama'\n\nHi, I have used the exact same commands as in the readme,\
    \ however, I am facing the above error in the line:\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    Salesforce/xgen-7b-8k-base\", torch_dtype=torch.bfloat16).to(\"cuda\")\n\nIt is\
    \ able to download config.json though!"
  created_at: 2023-07-03 15:20:11+00:00
  edited: true
  hidden: false
  id: 64a2f53b8649c0f4aee2ab83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648750941188-619aa4e2198a95c9b962252e.jpeg?w=200&h=200&f=face
      fullname: Hiroaki Hayashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rooa
      type: user
    createdAt: '2023-07-03T16:32:50.000Z'
    data:
      edited: false
      editors:
      - rooa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8955023288726807
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648750941188-619aa4e2198a95c9b962252e.jpeg?w=200&h=200&f=face
          fullname: Hiroaki Hayashi
          isHf: false
          isPro: false
          name: rooa
          type: user
        html: '<p>What''s your transformers version? Can you load other LLaMA models
          on the hub?</p>

          '
        raw: What's your transformers version? Can you load other LLaMA models on
          the hub?
        updatedAt: '2023-07-03T16:32:50.590Z'
      numEdits: 0
      reactions: []
    id: 64a2f832bf860f9c565799fe
    type: comment
  author: rooa
  content: What's your transformers version? Can you load other LLaMA models on the
    hub?
  created_at: 2023-07-03 15:32:50+00:00
  edited: false
  hidden: false
  id: 64a2f832bf860f9c565799fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3c2aafec2f1f201b0f701b3413ec64e.svg
      fullname: Khushee Kapoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DataScienceGal
      type: user
    createdAt: '2023-07-03T16:42:58.000Z'
    data:
      edited: false
      editors:
      - DataScienceGal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8418769240379333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3c2aafec2f1f201b0f701b3413ec64e.svg
          fullname: Khushee Kapoor
          isHf: false
          isPro: false
          name: DataScienceGal
          type: user
        html: '<p>As per Salesforce''s repo, my transformers version is transformers==4.25.1</p>

          '
        raw: As per Salesforce's repo, my transformers version is transformers==4.25.1
        updatedAt: '2023-07-03T16:42:58.541Z'
      numEdits: 0
      reactions: []
    id: 64a2fa92ca77286c24da47f2
    type: comment
  author: DataScienceGal
  content: As per Salesforce's repo, my transformers version is transformers==4.25.1
  created_at: 2023-07-03 15:42:58+00:00
  edited: false
  hidden: false
  id: 64a2fa92ca77286c24da47f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648750941188-619aa4e2198a95c9b962252e.jpeg?w=200&h=200&f=face
      fullname: Hiroaki Hayashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rooa
      type: user
    createdAt: '2023-07-03T16:46:20.000Z'
    data:
      edited: false
      editors:
      - rooa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8936098217964172
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648750941188-619aa4e2198a95c9b962252e.jpeg?w=200&h=200&f=face
          fullname: Hiroaki Hayashi
          isHf: false
          isPro: false
          name: rooa
          type: user
        html: '<p>Llama model class is not available before 4.28. Could you please
          install <code>4.29.2</code> (the version we used)?<br>We will update the
          requirements on GitHub as well.</p>

          '
        raw: "Llama model class is not available before 4.28. Could you please install\
          \ `4.29.2` (the version we used)? \nWe will update the requirements on GitHub\
          \ as well."
        updatedAt: '2023-07-03T16:46:20.565Z'
      numEdits: 0
      reactions: []
    id: 64a2fb5c657029485bb4aa0f
    type: comment
  author: rooa
  content: "Llama model class is not available before 4.28. Could you please install\
    \ `4.29.2` (the version we used)? \nWe will update the requirements on GitHub\
    \ as well."
  created_at: 2023-07-03 15:46:20+00:00
  edited: false
  hidden: false
  id: 64a2fb5c657029485bb4aa0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3c2aafec2f1f201b0f701b3413ec64e.svg
      fullname: Khushee Kapoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DataScienceGal
      type: user
    createdAt: '2023-07-03T16:48:57.000Z'
    data:
      edited: false
      editors:
      - DataScienceGal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6860408186912537
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3c2aafec2f1f201b0f701b3413ec64e.svg
          fullname: Khushee Kapoor
          isHf: false
          isPro: false
          name: DataScienceGal
          type: user
        html: '<p>Okay, thanks!</p>

          '
        raw: Okay, thanks!
        updatedAt: '2023-07-03T16:48:57.856Z'
      numEdits: 0
      reactions: []
    id: 64a2fbf9c34564e62f600370
    type: comment
  author: DataScienceGal
  content: Okay, thanks!
  created_at: 2023-07-03 15:48:57+00:00
  edited: false
  hidden: false
  id: 64a2fbf9c34564e62f600370
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648750941188-619aa4e2198a95c9b962252e.jpeg?w=200&h=200&f=face
      fullname: Hiroaki Hayashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rooa
      type: user
    createdAt: '2023-07-04T07:03:51.000Z'
    data:
      status: closed
    id: 64a3c457301712161a543c63
    type: status-change
  author: rooa
  created_at: 2023-07-04 06:03:51+00:00
  id: 64a3c457301712161a543c63
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c72891e02c03812d1684950d13d89163.svg
      fullname: varma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aanchal
      type: user
    createdAt: '2023-07-08T19:37:53.000Z'
    data:
      edited: false
      editors:
      - aanchal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.912398636341095
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c72891e02c03812d1684950d13d89163.svg
          fullname: varma
          isHf: false
          isPro: false
          name: aanchal
          type: user
        html: '<p>I tried with transformers version 4.29.2, still getting the same
          error</p>

          '
        raw: I tried with transformers version 4.29.2, still getting the same error
        updatedAt: '2023-07-08T19:37:53.814Z'
      numEdits: 0
      reactions: []
    id: 64a9bb11baf671fbebc518e8
    type: comment
  author: aanchal
  content: I tried with transformers version 4.29.2, still getting the same error
  created_at: 2023-07-08 18:37:53+00:00
  edited: false
  hidden: false
  id: 64a9bb11baf671fbebc518e8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: Salesforce/xgen-7b-8k-base
repo_type: model
status: closed
target_branch: null
title: Error in Loading Model
