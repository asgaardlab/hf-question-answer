!!python/object:huggingface_hub.community.DiscussionWithDetails
author: arkaprovob
conflicting_files: null
created_at: 2023-07-04 15:00:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a561ac8f6b33ab60cedc68293c9c610f.svg
      fullname: Arjya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arkaprovob
      type: user
    createdAt: '2023-07-04T16:00:39.000Z'
    data:
      edited: false
      editors:
      - arkaprovob
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9541631937026978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a561ac8f6b33ab60cedc68293c9c610f.svg
          fullname: Arjya
          isHf: false
          isPro: false
          name: arkaprovob
          type: user
        html: '<p>I am currently utilizing the declare-lab/flan-alpaca-xl LLM programmatically.
          My system is equipped with a 14-core processor and a GTX 3060 Mobile GPU
          with 6GB VRAM and 32 Gb of DDR5 ram.<br>I''ve noticed something unusual
          when running the model: The performance is much faster when the model runs
          solely on the CPU as opposed to using the GPU. This is despite setting the
          device_map to "auto", which should theoretically take advantage of both
          CPU and GPU resources.<br>Considering the common wisdom that GPUs should
          outperform CPUs in deep learning tasks due to their parallel processing
          capabilities, I''m puzzled as to why I''m observing the opposite.</p>

          '
        raw: "I am currently utilizing the declare-lab/flan-alpaca-xl LLM programmatically.\
          \ My system is equipped with a 14-core processor and a GTX 3060 Mobile GPU\
          \ with 6GB VRAM and 32 Gb of DDR5 ram.\r\nI've noticed something unusual\
          \ when running the model: The performance is much faster when the model\
          \ runs solely on the CPU as opposed to using the GPU. This is despite setting\
          \ the device_map to \"auto\", which should theoretically take advantage\
          \ of both CPU and GPU resources.\r\nConsidering the common wisdom that GPUs\
          \ should outperform CPUs in deep learning tasks due to their parallel processing\
          \ capabilities, I'm puzzled as to why I'm observing the opposite."
        updatedAt: '2023-07-04T16:00:39.482Z'
      numEdits: 0
      reactions: []
    id: 64a44227f7c0b86a62e4d7ea
    type: comment
  author: arkaprovob
  content: "I am currently utilizing the declare-lab/flan-alpaca-xl LLM programmatically.\
    \ My system is equipped with a 14-core processor and a GTX 3060 Mobile GPU with\
    \ 6GB VRAM and 32 Gb of DDR5 ram.\r\nI've noticed something unusual when running\
    \ the model: The performance is much faster when the model runs solely on the\
    \ CPU as opposed to using the GPU. This is despite setting the device_map to \"\
    auto\", which should theoretically take advantage of both CPU and GPU resources.\r\
    \nConsidering the common wisdom that GPUs should outperform CPUs in deep learning\
    \ tasks due to their parallel processing capabilities, I'm puzzled as to why I'm\
    \ observing the opposite."
  created_at: 2023-07-04 15:00:39+00:00
  edited: false
  hidden: false
  id: 64a44227f7c0b86a62e4d7ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: declare-lab/flan-alpaca-xl
repo_type: model
status: open
target_branch: null
title: Why does my CPU outperform my GTX 3060 Mobile GPU when running declare-lab/flan-alpaca-xl
  locally
