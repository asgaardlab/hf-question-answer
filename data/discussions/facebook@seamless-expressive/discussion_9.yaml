!!python/object:huggingface_hub.community.DiscussionWithDetails
author: M2LabOrg
conflicting_files: null
created_at: 2023-12-18 16:51:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
      fullname: Michel Mesquita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: M2LabOrg
      type: user
    createdAt: '2023-12-18T16:51:39.000Z'
    data:
      edited: false
      editors:
      - M2LabOrg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9517608880996704
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
          fullname: Michel Mesquita
          isHf: false
          isPro: false
          name: M2LabOrg
          type: user
        html: "<p>Hi there,</p>\n<p>I've duplicated the seamless-expressive space\
          \ for a private run on Nvidia T4. However, I'm facing an issue with an audio\
          \ file \u2013 it's 2 minutes and 30 seconds long, but I'm getting a message\
          \ that only the initial 10 seconds are processed. Is there a way to address\
          \ this?</p>\n<p>Thanks!</p>\n<p>Michel</p>\n"
        raw: "Hi there,\r\n\r\nI've duplicated the seamless-expressive space for a\
          \ private run on Nvidia T4. However, I'm facing an issue with an audio file\
          \ \u2013 it's 2 minutes and 30 seconds long, but I'm getting a message that\
          \ only the initial 10 seconds are processed. Is there a way to address this?\r\
          \n\r\nThanks!\r\n\r\nMichel"
        updatedAt: '2023-12-18T16:51:39.973Z'
      numEdits: 0
      reactions: []
    id: 6580789b133ca7da45beef75
    type: comment
  author: M2LabOrg
  content: "Hi there,\r\n\r\nI've duplicated the seamless-expressive space for a private\
    \ run on Nvidia T4. However, I'm facing an issue with an audio file \u2013 it's\
    \ 2 minutes and 30 seconds long, but I'm getting a message that only the initial\
    \ 10 seconds are processed. Is there a way to address this?\r\n\r\nThanks!\r\n\
    \r\nMichel"
  created_at: 2023-12-18 16:51:39+00:00
  edited: false
  hidden: false
  id: 6580789b133ca7da45beef75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
      fullname: Michel Mesquita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: M2LabOrg
      type: user
    createdAt: '2023-12-19T11:34:47.000Z'
    data:
      edited: false
      editors:
      - M2LabOrg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.690477728843689
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
          fullname: Michel Mesquita
          isHf: false
          isPro: false
          name: M2LabOrg
          type: user
        html: '<p>Hey everyone,</p>

          <p> I''ve successfully adjusted the MAX_INPUT_AUDIO_LENGTH in my HuggingFace
          space (check the steps below), but now I''m encountering an issue. After
          uploading the audio file and hitting the RUN button, I get errors in both
          the audio output and the translation text output.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/OCuYTicMokNunowka-95l.png"><img
          alt="Screenshot 2023-12-19 at 12.32.51.png" src="https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/OCuYTicMokNunowka-95l.png"></a></p>

          <p>Here are the steps I followed:</p>

          <ol>

          <li><p><strong>Clone the HuggingFace repo locally:</strong></p>

          <pre><code class="language-bash">$ brew install git-lfs

          $ git lfs install

          $ git <span class="hljs-built_in">clone</span> https://huggingface.co/spaces/&lt;username&gt;/&lt;spacename&gt;

          </code></pre>

          </li>

          <li><p><strong>Open the file app.py:</strong></p>

          <pre><code class="language-bash">$ <span class="hljs-built_in">cd</span>
          &lt;spacename&gt;

          $ vim app.py

          </code></pre>

          </li>

          <li><p><strong>Change the variable: "MAX_INPUT_AUDIO_LENGTH = 10  # in seconds"</strong>
          to the desired length in seconds.</p>

          </li>

          <li><p><strong>Commit and Push the Changes:</strong></p>

          <pre><code class="language-bash">$ git add -A

          $ git commit -m <span class="hljs-string">"Increased MAX_INPUT_AUDIO_LENGTH"</span>

          $ git push origin main

          </code></pre>

          </li>

          <li><p><strong>HuggingFace will automatically build a new copy</strong>
          incorporating the changes.</p>

          </li>

          </ol>

          <p>Now, when I try to process the data, I''m running into errors. Any insights
          or suggestions would be greatly appreciated.</p>

          <p>Thanks in advance!</p>

          <p>Michel</p>

          '
        raw: "Hey everyone,\n\n I've successfully adjusted the MAX_INPUT_AUDIO_LENGTH\
          \ in my HuggingFace space (check the steps below), but now I'm encountering\
          \ an issue. After uploading the audio file and hitting the RUN button, I\
          \ get errors in both the audio output and the translation text output.\n\
          \n\n![Screenshot 2023-12-19 at 12.32.51.png](https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/OCuYTicMokNunowka-95l.png)\n\
          \n\nHere are the steps I followed:\n\n1. **Clone the HuggingFace repo locally:**\n\
          \   ```bash\n   $ brew install git-lfs\n   $ git lfs install\n   $ git clone\
          \ https://huggingface.co/spaces/<username>/<spacename>\n   ```\n\n2. **Open\
          \ the file app.py:**\n   ```bash\n   $ cd <spacename>\n   $ vim app.py\n\
          \   ```\n\n3. **Change the variable: \"MAX_INPUT_AUDIO_LENGTH = 10  # in\
          \ seconds\"** to the desired length in seconds.\n\n4. **Commit and Push\
          \ the Changes:**\n   ```bash\n   $ git add -A\n   $ git commit -m \"Increased\
          \ MAX_INPUT_AUDIO_LENGTH\"\n   $ git push origin main\n   ```\n\n5. **HuggingFace\
          \ will automatically build a new copy** incorporating the changes.\n\nNow,\
          \ when I try to process the data, I'm running into errors. Any insights\
          \ or suggestions would be greatly appreciated.\n\nThanks in advance!\n\n\
          Michel"
        updatedAt: '2023-12-19T11:34:47.330Z'
      numEdits: 0
      reactions: []
    id: 65817fd7499fbe1b9548aba5
    type: comment
  author: M2LabOrg
  content: "Hey everyone,\n\n I've successfully adjusted the MAX_INPUT_AUDIO_LENGTH\
    \ in my HuggingFace space (check the steps below), but now I'm encountering an\
    \ issue. After uploading the audio file and hitting the RUN button, I get errors\
    \ in both the audio output and the translation text output.\n\n\n![Screenshot\
    \ 2023-12-19 at 12.32.51.png](https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/OCuYTicMokNunowka-95l.png)\n\
    \n\nHere are the steps I followed:\n\n1. **Clone the HuggingFace repo locally:**\n\
    \   ```bash\n   $ brew install git-lfs\n   $ git lfs install\n   $ git clone https://huggingface.co/spaces/<username>/<spacename>\n\
    \   ```\n\n2. **Open the file app.py:**\n   ```bash\n   $ cd <spacename>\n   $\
    \ vim app.py\n   ```\n\n3. **Change the variable: \"MAX_INPUT_AUDIO_LENGTH = 10\
    \  # in seconds\"** to the desired length in seconds.\n\n4. **Commit and Push\
    \ the Changes:**\n   ```bash\n   $ git add -A\n   $ git commit -m \"Increased\
    \ MAX_INPUT_AUDIO_LENGTH\"\n   $ git push origin main\n   ```\n\n5. **HuggingFace\
    \ will automatically build a new copy** incorporating the changes.\n\nNow, when\
    \ I try to process the data, I'm running into errors. Any insights or suggestions\
    \ would be greatly appreciated.\n\nThanks in advance!\n\nMichel"
  created_at: 2023-12-19 11:34:47+00:00
  edited: false
  hidden: false
  id: 65817fd7499fbe1b9548aba5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
      fullname: Michel Mesquita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: M2LabOrg
      type: user
    createdAt: '2023-12-19T11:58:26.000Z'
    data:
      edited: true
      editors:
      - M2LabOrg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8308125734329224
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
          fullname: Michel Mesquita
          isHf: false
          isPro: false
          name: M2LabOrg
          type: user
        html: "<p>It appears that despite upgrading to T4 Medium with increased memory,\
          \ the CUDA out-of-memory error persists. Here's an edited comment:</p>\n\
          <pre><code class=\"language-plaintext\">Seems like we're still grappling\
          \ with memory issues, even after upgrading to T4 Medium (8 vCPU, 30 GB RAM,\
          \ 16 GB VRAM). The error persists:\n</code></pre>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">return</span> torch.embedding(weight, <span\
          \ class=\"hljs-built_in\">input</span>, padding_idx, scale_grad_by_freq,\
          \ sparse)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ <span class=\"hljs-number\">6.28</span> GiB. GPU <span class=\"hljs-number\"\
          >0</span> has a total capacity of <span class=\"hljs-number\">14.58</span>\
          \ GiB, <span class=\"hljs-keyword\">with</span> <span class=\"hljs-number\"\
          >3.42</span> GiB free. Process <span class=\"hljs-number\">199325</span>\
          \ <span class=\"hljs-keyword\">is</span> using <span class=\"hljs-number\"\
          >11.15</span> GiB memory, out of which <span class=\"hljs-number\">10.94</span>\
          \ GiB <span class=\"hljs-keyword\">is</span> allocated by PyTorch, <span\
          \ class=\"hljs-keyword\">and</span> <span class=\"hljs-number\">69.17</span>\
          \ MiB <span class=\"hljs-keyword\">is</span> reserved but unallocated. If\
          \ reserved but unallocated memory <span class=\"hljs-keyword\">is</span>\
          \ substantial, consider setting max_split_size_mb to avoid fragmentation.\
          \ See the documentation <span class=\"hljs-keyword\">for</span> Memory Management\
          \ <span class=\"hljs-keyword\">and</span> PYTORCH_CUDA_ALLOC_CONF.\n\nThe\
          \ above exception caused the following exception:\n\nTraceback (most recent\
          \ call last):\n  File <span class=\"hljs-string\">\"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
          </span>, line <span class=\"hljs-number\">497</span>, <span class=\"hljs-keyword\"\
          >in</span> process_events\n    response = <span class=\"hljs-keyword\">await</span>\
          \ self.call_prediction(awake_events, batch)\n  File <span class=\"hljs-string\"\
          >\"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
          </span>, line <span class=\"hljs-number\">468</span>, <span class=\"hljs-keyword\"\
          >in</span> call_prediction\n    <span class=\"hljs-keyword\">raise</span>\
          \ Exception(<span class=\"hljs-built_in\">str</span>(error) <span class=\"\
          hljs-keyword\">if</span> show_error <span class=\"hljs-keyword\">else</span>\
          \ <span class=\"hljs-literal\">None</span>) <span class=\"hljs-keyword\"\
          >from</span> error\nException: <span class=\"hljs-literal\">None</span>\n\
          </code></pre>\n<p>Despite the hardware upgrade, it seems we're still pushing\
          \ the limits. Any suggestions or insights would be greatly appreciated.\
          \ Thanks!</p>\n"
        raw: "It appears that despite upgrading to T4 Medium with increased memory,\
          \ the CUDA out-of-memory error persists. Here's an edited comment:\n\n```plaintext\n\
          Seems like we're still grappling with memory issues, even after upgrading\
          \ to T4 Medium (8 vCPU, 30 GB RAM, 16 GB VRAM). The error persists:\n```\n\
          \n```python\nreturn torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
          \ sparse)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 6.28 GiB. GPU 0 has a total capacity of 14.58 GiB, with 3.42 GiB free.\
          \ Process 199325 is using 11.15 GiB memory, out of which 10.94 GiB is allocated\
          \ by PyTorch, and 69.17 MiB is reserved but unallocated. If reserved but\
          \ unallocated memory is substantial, consider setting max_split_size_mb\
          \ to avoid fragmentation. See the documentation for Memory Management and\
          \ PYTORCH_CUDA_ALLOC_CONF.\n\nThe above exception caused the following exception:\n\
          \nTraceback (most recent call last):\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
          , line 497, in process_events\n    response = await self.call_prediction(awake_events,\
          \ batch)\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
          , line 468, in call_prediction\n    raise Exception(str(error) if show_error\
          \ else None) from error\nException: None\n```\n\nDespite the hardware upgrade,\
          \ it seems we're still pushing the limits. Any suggestions or insights would\
          \ be greatly appreciated. Thanks!\n"
        updatedAt: '2023-12-19T12:00:06.798Z'
      numEdits: 4
      reactions: []
    id: 658185624466994ea83f26bc
    type: comment
  author: M2LabOrg
  content: "It appears that despite upgrading to T4 Medium with increased memory,\
    \ the CUDA out-of-memory error persists. Here's an edited comment:\n\n```plaintext\n\
    Seems like we're still grappling with memory issues, even after upgrading to T4\
    \ Medium (8 vCPU, 30 GB RAM, 16 GB VRAM). The error persists:\n```\n\n```python\n\
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n\
    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.28 GiB. GPU\
    \ 0 has a total capacity of 14.58 GiB, with 3.42 GiB free. Process 199325 is using\
    \ 11.15 GiB memory, out of which 10.94 GiB is allocated by PyTorch, and 69.17\
    \ MiB is reserved but unallocated. If reserved but unallocated memory is substantial,\
    \ consider setting max_split_size_mb to avoid fragmentation. See the documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF.\n\nThe above exception caused\
    \ the following exception:\n\nTraceback (most recent call last):\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
    , line 497, in process_events\n    response = await self.call_prediction(awake_events,\
    \ batch)\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
    , line 468, in call_prediction\n    raise Exception(str(error) if show_error else\
    \ None) from error\nException: None\n```\n\nDespite the hardware upgrade, it seems\
    \ we're still pushing the limits. Any suggestions or insights would be greatly\
    \ appreciated. Thanks!\n"
  created_at: 2023-12-19 11:58:26+00:00
  edited: true
  hidden: false
  id: 658185624466994ea83f26bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
      fullname: Michel Mesquita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: M2LabOrg
      type: user
    createdAt: '2023-12-20T10:15:57.000Z'
    data:
      edited: false
      editors:
      - M2LabOrg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9417123198509216
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
          fullname: Michel Mesquita
          isHf: false
          isPro: false
          name: M2LabOrg
          type: user
        html: "<p>Awesome news! After some trial and error, I figured out that the\
          \ hiccup was all about memory. The lightbulb moment happened when I split\
          \ my file into two, around one minute each. That did the trick, and now\
          \ everything's running smoothly.</p>\n<p>If anyone else is in the same memory\
          \ boat, I'm curious if tweaking some settings could help handle larger audio\
          \ files. Let me know if you discover any hacks!</p>\n<p>Happy coding, everyone!\
          \ \U0001F60A</p>\n"
        raw: "Awesome news! After some trial and error, I figured out that the hiccup\
          \ was all about memory. The lightbulb moment happened when I split my file\
          \ into two, around one minute each. That did the trick, and now everything's\
          \ running smoothly.\n\nIf anyone else is in the same memory boat, I'm curious\
          \ if tweaking some settings could help handle larger audio files. Let me\
          \ know if you discover any hacks!\n\nHappy coding, everyone! \U0001F60A"
        updatedAt: '2023-12-20T10:15:57.246Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6582bedd117b524ef3fec8b7
    id: 6582bedd117b524ef3fec8b4
    type: comment
  author: M2LabOrg
  content: "Awesome news! After some trial and error, I figured out that the hiccup\
    \ was all about memory. The lightbulb moment happened when I split my file into\
    \ two, around one minute each. That did the trick, and now everything's running\
    \ smoothly.\n\nIf anyone else is in the same memory boat, I'm curious if tweaking\
    \ some settings could help handle larger audio files. Let me know if you discover\
    \ any hacks!\n\nHappy coding, everyone! \U0001F60A"
  created_at: 2023-12-20 10:15:57+00:00
  edited: false
  hidden: false
  id: 6582bedd117b524ef3fec8b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65709c57c1b36e8e57741ea3/UOifz8mjGeGJnTJ3AOd20.jpeg?w=200&h=200&f=face
      fullname: Michel Mesquita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: M2LabOrg
      type: user
    createdAt: '2023-12-20T10:15:57.000Z'
    data:
      status: closed
    id: 6582bedd117b524ef3fec8b7
    type: status-change
  author: M2LabOrg
  created_at: 2023-12-20 10:15:57+00:00
  id: 6582bedd117b524ef3fec8b7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: facebook/seamless-expressive
repo_type: model
status: closed
target_branch: null
title: Query on Audio File Length in Duplicated Space
