!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Onix22
conflicting_files: null
created_at: 2023-07-10 18:08:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
      fullname: tp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Onix22
      type: user
    createdAt: '2023-07-10T19:08:26.000Z'
    data:
      edited: false
      editors:
      - Onix22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9295149445533752
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
          fullname: tp
          isHf: false
          isPro: false
          name: Onix22
          type: user
        html: '<p>this  model is producing nonsense<br>if temperature is decreased
          it remains stable for a bit longer but eventually it defaults to this</p>

          <p>he the story tale of Timmy''s s prankankles were were were were not only
          amused entertained but also learned lesson moremory moment experience from
          what happened outcome result effect impact influence on upon upon upon upon
          upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon
          upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon
          upon upon upon upon upon upon upon upon upon upon </p>

          '
        raw: "this  model is producing nonsense \r\nif temperature is decreased it\
          \ remains stable for a bit longer but eventually it defaults to this\r\n\
          \r\nhe the story tale of Timmy's s prankankles were were were were not only\
          \ amused entertained but also learned lesson moremory moment experience\
          \ from what happened outcome result effect impact influence on upon upon\
          \ upon upon upon upon upon upon upon upon upon upon upon upon upon upon\
          \ upon upon upon upon upon upon upon upon upon upon upon upon upon upon\
          \ upon upon upon upon upon upon upon upon upon upon upon upon upon upon "
        updatedAt: '2023-07-10T19:08:26.322Z'
      numEdits: 0
      reactions: []
    id: 64ac572acf90fe27553919a2
    type: comment
  author: Onix22
  content: "this  model is producing nonsense \r\nif temperature is decreased it remains\
    \ stable for a bit longer but eventually it defaults to this\r\n\r\nhe the story\
    \ tale of Timmy's s prankankles were were were were not only amused entertained\
    \ but also learned lesson moremory moment experience from what happened outcome\
    \ result effect impact influence on upon upon upon upon upon upon upon upon upon\
    \ upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon\
    \ upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon\
    \ upon upon upon "
  created_at: 2023-07-10 18:08:26+00:00
  edited: false
  hidden: false
  id: 64ac572acf90fe27553919a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d6443bf7a57c624d6ee581d08745edd.svg
      fullname: DB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Olbyack
      type: user
    createdAt: '2023-07-11T22:30:23.000Z'
    data:
      edited: false
      editors:
      - Olbyack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9872215390205383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d6443bf7a57c624d6ee581d08745edd.svg
          fullname: DB
          isHf: false
          isPro: false
          name: Olbyack
          type: user
        html: '<p>I had this problem because I failed to change the model loader to
          ExLlama. </p>

          '
        raw: 'I had this problem because I failed to change the model loader to ExLlama. '
        updatedAt: '2023-07-11T22:30:23.530Z'
      numEdits: 0
      reactions: []
    id: 64add7fff9f761a4c39e7b6c
    type: comment
  author: Olbyack
  content: 'I had this problem because I failed to change the model loader to ExLlama. '
  created_at: 2023-07-11 21:30:23+00:00
  edited: false
  hidden: false
  id: 64add7fff9f761a4c39e7b6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5d74938a17536d21065c969f34c822b.svg
      fullname: Dillon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SatanSoldier
      type: user
    createdAt: '2023-07-12T17:54:43.000Z'
    data:
      edited: true
      editors:
      - SatanSoldier
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8199208974838257
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5d74938a17536d21065c969f34c822b.svg
          fullname: Dillon
          isHf: false
          isPro: false
          name: SatanSoldier
          type: user
        html: '<p>With the OoobaBooga webui I got it working on a single 3090, total
          GPU memory used is 23.6/24. Generates text just fine on any of the instruct
          generation parameters presets. Ensure your ModelLoader is set to ExLlama,
          not ExLlama_hf, max_seq_len = 4096, compress_pos_emb = 2. You''ll need more
          than 24GB VRAM for 8k tokens. It seems to be riding the limit at 4k. Under
          Parameters, Truncate Prompt = 4096. Also, check your instruct template under
          chat settings, I have mine on WizardLM. It didn''t set this automatically
          for me.</p>

          '
        raw: With the OoobaBooga webui I got it working on a single 3090, total GPU
          memory used is 23.6/24. Generates text just fine on any of the instruct
          generation parameters presets. Ensure your ModelLoader is set to ExLlama,
          not ExLlama_hf, max_seq_len = 4096, compress_pos_emb = 2. You'll need more
          than 24GB VRAM for 8k tokens. It seems to be riding the limit at 4k. Under
          Parameters, Truncate Prompt = 4096. Also, check your instruct template under
          chat settings, I have mine on WizardLM. It didn't set this automatically
          for me.
        updatedAt: '2023-07-12T17:55:01.494Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jaimu
    id: 64aee8e31cefb66e87aed985
    type: comment
  author: SatanSoldier
  content: With the OoobaBooga webui I got it working on a single 3090, total GPU
    memory used is 23.6/24. Generates text just fine on any of the instruct generation
    parameters presets. Ensure your ModelLoader is set to ExLlama, not ExLlama_hf,
    max_seq_len = 4096, compress_pos_emb = 2. You'll need more than 24GB VRAM for
    8k tokens. It seems to be riding the limit at 4k. Under Parameters, Truncate Prompt
    = 4096. Also, check your instruct template under chat settings, I have mine on
    WizardLM. It didn't set this automatically for me.
  created_at: 2023-07-12 16:54:43+00:00
  edited: true
  hidden: false
  id: 64aee8e31cefb66e87aed985
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
      fullname: tp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Onix22
      type: user
    createdAt: '2023-07-12T21:03:14.000Z'
    data:
      edited: false
      editors:
      - Onix22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9628000855445862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
          fullname: tp
          isHf: false
          isPro: false
          name: Onix22
          type: user
        html: '<p>I found on other site that those models need to be set to more than
          2048 token context length  or else they don''t work at all.<br>Now everything
          seem to be working but this should be told on the model page.</p>

          <p>As for the maximum I did not hit the limit yet but looks like exlliama
          is capable of using shared GPU memory so context length is not realy limited
          by usual OOM but it may get super slow<br>also it allocates all memory in
          advance </p>

          '
        raw: "I found on other site that those models need to be set to more than\
          \ 2048 token context length  or else they don't work at all.\nNow everything\
          \ seem to be working but this should be told on the model page.\n\nAs for\
          \ the maximum I did not hit the limit yet but looks like exlliama is capable\
          \ of using shared GPU memory so context length is not realy limited by usual\
          \ OOM but it may get super slow \nalso it allocates all memory in advance "
        updatedAt: '2023-07-12T21:03:14.285Z'
      numEdits: 0
      reactions: []
    id: 64af15125c17fe25cfbb98b0
    type: comment
  author: Onix22
  content: "I found on other site that those models need to be set to more than 2048\
    \ token context length  or else they don't work at all.\nNow everything seem to\
    \ be working but this should be told on the model page.\n\nAs for the maximum\
    \ I did not hit the limit yet but looks like exlliama is capable of using shared\
    \ GPU memory so context length is not realy limited by usual OOM but it may get\
    \ super slow \nalso it allocates all memory in advance "
  created_at: 2023-07-12 20:03:14+00:00
  edited: false
  hidden: false
  id: 64af15125c17fe25cfbb98b0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ
repo_type: model
status: open
target_branch: null
title: Does it work for anyone?
