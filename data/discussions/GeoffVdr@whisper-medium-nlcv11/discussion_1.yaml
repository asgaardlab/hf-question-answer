!!python/object:huggingface_hub.community.DiscussionWithDetails
author: daniel-v-e
conflicting_files: null
created_at: 2023-01-08 02:05:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e365b78b6b58b63c617cee36d5ff0de8.svg
      fullname: Daniel von Eschwege
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniel-v-e
      type: user
    createdAt: '2023-01-08T02:05:37.000Z'
    data:
      edited: false
      editors:
      - daniel-v-e
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e365b78b6b58b63c617cee36d5ff0de8.svg
          fullname: Daniel von Eschwege
          isHf: false
          isPro: false
          name: daniel-v-e
          type: user
        html: '<p>Good day, is it maybe possible to somehow obtain the .pt model,
          similarly to the format the whisper models are in <code>~/.cache/whisper/</code>,
          when doing ''pip install whisper'' &amp; running i.e. <code>model = whisper.load_model(''medium'')</code>
          ?</p>

          <p>I ask because I''d like to perform batch transcription of longer documents,
          and running the model through huggingface only allows for up to max_new_tokens
          =  484 or something like that, requiring all input audio to be split.</p>

          <p>If there is an alternative way to perform batch transcription, that would
          also be great.</p>

          <p>Thanks!</p>

          '
        raw: "Good day, is it maybe possible to somehow obtain the .pt model, similarly\
          \ to the format the whisper models are in `~/.cache/whisper/`, when doing\
          \ 'pip install whisper' & running i.e. `model = whisper.load_model('medium')`\
          \ ?\r\n\r\nI ask because I'd like to perform batch transcription of longer\
          \ documents, and running the model through huggingface only allows for up\
          \ to max_new_tokens =  484 or something like that, requiring all input audio\
          \ to be split.\r\n\r\nIf there is an alternative way to perform batch transcription,\
          \ that would also be great.\r\n\r\nThanks!"
        updatedAt: '2023-01-08T02:05:37.990Z'
      numEdits: 0
      reactions: []
    id: 63ba24f10a9866b28caf9df2
    type: comment
  author: daniel-v-e
  content: "Good day, is it maybe possible to somehow obtain the .pt model, similarly\
    \ to the format the whisper models are in `~/.cache/whisper/`, when doing 'pip\
    \ install whisper' & running i.e. `model = whisper.load_model('medium')` ?\r\n\
    \r\nI ask because I'd like to perform batch transcription of longer documents,\
    \ and running the model through huggingface only allows for up to max_new_tokens\
    \ =  484 or something like that, requiring all input audio to be split.\r\n\r\n\
    If there is an alternative way to perform batch transcription, that would also\
    \ be great.\r\n\r\nThanks!"
  created_at: 2023-01-08 02:05:37+00:00
  edited: false
  hidden: false
  id: 63ba24f10a9866b28caf9df2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84704b17ffb17b1c982473e0a80264b2.svg
      fullname: Geoffroy Vanderreydt
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: GeoffVdr
      type: user
    createdAt: '2023-01-10T11:58:08.000Z'
    data:
      edited: false
      editors:
      - GeoffVdr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84704b17ffb17b1c982473e0a80264b2.svg
          fullname: Geoffroy Vanderreydt
          isHf: false
          isPro: false
          name: GeoffVdr
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;daniel-v-e&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daniel-v-e\"\
          >@<span class=\"underline\">daniel-v-e</span></a></span>\n\n\t</span></span>\
          \ ! Can't you use the <code>pytorch_model.bin</code> for that? I don't know,\
          \ I have never tried. Maybe <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \ or <span data-props=\"{&quot;user&quot;:&quot;vb&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vb\">@<span class=\"\
          underline\">vb</span></a></span>\n\n\t</span></span> can help you with that.</p>\n"
        raw: Hi @daniel-v-e ! Can't you use the `pytorch_model.bin` for that? I don't
          know, I have never tried. Maybe @sanchit-gandhi or @vb can help you with
          that.
        updatedAt: '2023-01-10T11:58:08.036Z'
      numEdits: 0
      reactions: []
    id: 63bd52d0bb80ec4ef14529ba
    type: comment
  author: GeoffVdr
  content: Hi @daniel-v-e ! Can't you use the `pytorch_model.bin` for that? I don't
    know, I have never tried. Maybe @sanchit-gandhi or @vb can help you with that.
  created_at: 2023-01-10 11:58:08+00:00
  edited: false
  hidden: false
  id: 63bd52d0bb80ec4ef14529ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-01-13T15:02:33.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;daniel-v-e&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daniel-v-e\"\
          >@<span class=\"underline\">daniel-v-e</span></a></span>\n\n\t</span></span>!\
          \ Here's a code snippet you can use to run 'streamed inference' with batching\
          \ for audio samples of up to arbitrary length:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> datasets <span\
          \ class=\"hljs-keyword\">import</span> load_dataset\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> pipeline\n\ndataset = load_dataset(<span\
          \ class=\"hljs-string\">\"mozilla-foundation/common_voice_11_0\"</span>,\
          \ <span class=\"hljs-string\">\"es\"</span>, split=<span class=\"hljs-string\"\
          >\"test\"</span>, streaming=<span class=\"hljs-literal\">True</span>)\n\
          <span class=\"hljs-comment\"># only for debugging, restricts the number\
          \ of rows to numeric value in brackets -&gt; remove for full testing</span>\n\
          dataset = dataset.take(<span class=\"hljs-number\">16</span>)\n\n<span class=\"\
          hljs-comment\"># change to checkpoint and language of your choice</span>\n\
          ckpt = <span class=\"hljs-string\">\"openai/whisper-tiny\"</span>\nlang\
          \ = <span class=\"hljs-string\">\"es\"</span>\ndevice = <span class=\"hljs-number\"\
          >0</span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"\
          cpu\"</span>\n\npipe = pipeline(\n    task=<span class=\"hljs-string\">\"\
          automatic-speech-recognition\"</span>,\n    model= ckpt,\n    chunk_length_s=<span\
          \ class=\"hljs-number\">30</span>,\n    device=device,\n)\n\npipe.model.config.forced_decoder_ids\
          \ = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=<span class=\"\
          hljs-string\">\"transcribe\"</span>)\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">iterate_data</span>(<span class=\"\
          hljs-params\">dataset</span>):\n    <span class=\"hljs-keyword\">for</span>\
          \ i, item <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >enumerate</span>(dataset):\n        <span class=\"hljs-keyword\">yield</span>\
          \ item[<span class=\"hljs-string\">\"audio\"</span>]\n\n<span class=\"hljs-comment\"\
          ># set the batch size in accordance to your device</span>\nBATCH_SIZE =\
          \ <span class=\"hljs-number\">16</span>\n\npredictions = []\n\n<span class=\"\
          hljs-comment\"># run streamed inference</span>\n<span class=\"hljs-keyword\"\
          >for</span> out <span class=\"hljs-keyword\">in</span> pipe(iterate_data(dataset),\
          \ batch_size=BATCH_SIZE):\n    predictions.append(out[<span class=\"hljs-string\"\
          >\"text\"</span>])\n\n<span class=\"hljs-built_in\">print</span>(predictions)\n\
          </code></pre>\n<p><strong>Print Output:</strong></p>\n<pre><code>[' Habritan\
          \ aguas poco profundas y lo cosas.', ' Opera principalmente huelo de cargotajes\
          \ y regionales de carga.', ' 3', ' le alic\xF3 los estudios primarias en\
          \ plancle para continuar en luego en espa\xF1ol', ' En los a\xF1os que siguieron\
          \ este trabajo es parte pero de un coducena de buenas jugadores.', ' propuso\
          \ un nuevo marco para los territorios de alabagu y buscan a barra y b\xED\
          cay', ' \xBFCual es cierto? \xBFSe est\xE1 tratando de recuperar su prote\xF3\
          n de as\xEDsina escanarias?', ' Estas cr\xEDticas inciden en varios aspectos.',\
          \ ' Fue se pultada en el cementario general de Santiago.', ' Si', ' Maite\
          \ Perroni no ha assistido por estar grabando una telenovela.', ' Otras propulsieron\
          \ que era una superpluma africada, la que caus\xF3 la de la pronaci\xF3\
          n del mante.', ' Es un cactus de f\xE1cil cultivo de crecimiento vigoroso\
          \ y r\xE1pido.', ' Sus principales mercados son Estados Unidos y Tal\xED\
          a, Espa\xF1a y Jap\xF3n.', ' El archetypo del enfoque art\xEDstico en situ\
          \ es el arte urbano.', ' F\xF3rdates y M de le ayudar\xE1n a Sabat\xEDn\
          \ y a componer y para decir los ni tan isis.']\n</code></pre>\n"
        raw: "Hey @daniel-v-e! Here's a code snippet you can use to run 'streamed\
          \ inference' with batching for audio samples of up to arbitrary length:\n\
          \n```python\nfrom datasets import load_dataset\nimport torch\nfrom transformers\
          \ import pipeline\n\ndataset = load_dataset(\"mozilla-foundation/common_voice_11_0\"\
          , \"es\", split=\"test\", streaming=True)\n# only for debugging, restricts\
          \ the number of rows to numeric value in brackets -> remove for full testing\n\
          dataset = dataset.take(16)\n\n# change to checkpoint and language of your\
          \ choice\nckpt = \"openai/whisper-tiny\"\nlang = \"es\"\ndevice = 0 if torch.cuda.is_available()\
          \ else \"cpu\"\n\npipe = pipeline(\n    task=\"automatic-speech-recognition\"\
          ,\n    model= ckpt,\n    chunk_length_s=30,\n    device=device,\n)\n\npipe.model.config.forced_decoder_ids\
          \ = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=\"transcribe\"\
          )\n\ndef iterate_data(dataset):\n    for i, item in enumerate(dataset):\n\
          \        yield item[\"audio\"]\n\n# set the batch size in accordance to\
          \ your device\nBATCH_SIZE = 16\n\npredictions = []\n\n# run streamed inference\n\
          for out in pipe(iterate_data(dataset), batch_size=BATCH_SIZE):\n    predictions.append(out[\"\
          text\"])\n\nprint(predictions)\n```\n\n**Print Output:**\n```\n[' Habritan\
          \ aguas poco profundas y lo cosas.', ' Opera principalmente huelo de cargotajes\
          \ y regionales de carga.', ' 3', ' le alic\xF3 los estudios primarias en\
          \ plancle para continuar en luego en espa\xF1ol', ' En los a\xF1os que siguieron\
          \ este trabajo es parte pero de un coducena de buenas jugadores.', ' propuso\
          \ un nuevo marco para los territorios de alabagu y buscan a barra y b\xED\
          cay', ' \xBFCual es cierto? \xBFSe est\xE1 tratando de recuperar su prote\xF3\
          n de as\xEDsina escanarias?', ' Estas cr\xEDticas inciden en varios aspectos.',\
          \ ' Fue se pultada en el cementario general de Santiago.', ' Si', ' Maite\
          \ Perroni no ha assistido por estar grabando una telenovela.', ' Otras propulsieron\
          \ que era una superpluma africada, la que caus\xF3 la de la pronaci\xF3\
          n del mante.', ' Es un cactus de f\xE1cil cultivo de crecimiento vigoroso\
          \ y r\xE1pido.', ' Sus principales mercados son Estados Unidos y Tal\xED\
          a, Espa\xF1a y Jap\xF3n.', ' El archetypo del enfoque art\xEDstico en situ\
          \ es el arte urbano.', ' F\xF3rdates y M de le ayudar\xE1n a Sabat\xEDn\
          \ y a componer y para decir los ni tan isis.']\n```"
        updatedAt: '2023-01-13T15:02:50.349Z'
      numEdits: 1
      reactions: []
    id: 63c1728994b28327f0e9d0a4
    type: comment
  author: sanchit-gandhi
  content: "Hey @daniel-v-e! Here's a code snippet you can use to run 'streamed inference'\
    \ with batching for audio samples of up to arbitrary length:\n\n```python\nfrom\
    \ datasets import load_dataset\nimport torch\nfrom transformers import pipeline\n\
    \ndataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"\
    test\", streaming=True)\n# only for debugging, restricts the number of rows to\
    \ numeric value in brackets -> remove for full testing\ndataset = dataset.take(16)\n\
    \n# change to checkpoint and language of your choice\nckpt = \"openai/whisper-tiny\"\
    \nlang = \"es\"\ndevice = 0 if torch.cuda.is_available() else \"cpu\"\n\npipe\
    \ = pipeline(\n    task=\"automatic-speech-recognition\",\n    model= ckpt,\n\
    \    chunk_length_s=30,\n    device=device,\n)\n\npipe.model.config.forced_decoder_ids\
    \ = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=\"transcribe\")\n\
    \ndef iterate_data(dataset):\n    for i, item in enumerate(dataset):\n       \
    \ yield item[\"audio\"]\n\n# set the batch size in accordance to your device\n\
    BATCH_SIZE = 16\n\npredictions = []\n\n# run streamed inference\nfor out in pipe(iterate_data(dataset),\
    \ batch_size=BATCH_SIZE):\n    predictions.append(out[\"text\"])\n\nprint(predictions)\n\
    ```\n\n**Print Output:**\n```\n[' Habritan aguas poco profundas y lo cosas.',\
    \ ' Opera principalmente huelo de cargotajes y regionales de carga.', ' 3', '\
    \ le alic\xF3 los estudios primarias en plancle para continuar en luego en espa\xF1\
    ol', ' En los a\xF1os que siguieron este trabajo es parte pero de un coducena\
    \ de buenas jugadores.', ' propuso un nuevo marco para los territorios de alabagu\
    \ y buscan a barra y b\xEDcay', ' \xBFCual es cierto? \xBFSe est\xE1 tratando\
    \ de recuperar su prote\xF3n de as\xEDsina escanarias?', ' Estas cr\xEDticas inciden\
    \ en varios aspectos.', ' Fue se pultada en el cementario general de Santiago.',\
    \ ' Si', ' Maite Perroni no ha assistido por estar grabando una telenovela.',\
    \ ' Otras propulsieron que era una superpluma africada, la que caus\xF3 la de\
    \ la pronaci\xF3n del mante.', ' Es un cactus de f\xE1cil cultivo de crecimiento\
    \ vigoroso y r\xE1pido.', ' Sus principales mercados son Estados Unidos y Tal\xED\
    a, Espa\xF1a y Jap\xF3n.', ' El archetypo del enfoque art\xEDstico en situ es\
    \ el arte urbano.', ' F\xF3rdates y M de le ayudar\xE1n a Sabat\xEDn y a componer\
    \ y para decir los ni tan isis.']\n```"
  created_at: 2023-01-13 15:02:33+00:00
  edited: true
  hidden: false
  id: 63c1728994b28327f0e9d0a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e365b78b6b58b63c617cee36d5ff0de8.svg
      fullname: Daniel von Eschwege
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniel-v-e
      type: user
    createdAt: '2023-01-13T21:27:15.000Z'
    data:
      edited: false
      editors:
      - daniel-v-e
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e365b78b6b58b63c617cee36d5ff0de8.svg
          fullname: Daniel von Eschwege
          isHf: false
          isPro: false
          name: daniel-v-e
          type: user
        html: '<p>Thanks, but this still requires a long audio file to be split into
          30second chunks before transcription? What I am looking for is a method
          to for example pass a 40min audio file to a fine-tuned Huggingface Whisper
          model, after following the steps in your blog post <a href="https://huggingface.co/blog/fine-tune-whisper">https://huggingface.co/blog/fine-tune-whisper</a>
          (great blog by the way, super useful!). Surely it has to be possible, since
          running whisper via python / CLI somehow automatically splits the transcript?</p>

          '
        raw: Thanks, but this still requires a long audio file to be split into 30second
          chunks before transcription? What I am looking for is a method to for example
          pass a 40min audio file to a fine-tuned Huggingface Whisper model, after
          following the steps in your blog post https://huggingface.co/blog/fine-tune-whisper
          (great blog by the way, super useful!). Surely it has to be possible, since
          running whisper via python / CLI somehow automatically splits the transcript?
        updatedAt: '2023-01-13T21:27:15.513Z'
      numEdits: 0
      reactions: []
    id: 63c1ccb3f9453420b5d9f5a5
    type: comment
  author: daniel-v-e
  content: Thanks, but this still requires a long audio file to be split into 30second
    chunks before transcription? What I am looking for is a method to for example
    pass a 40min audio file to a fine-tuned Huggingface Whisper model, after following
    the steps in your blog post https://huggingface.co/blog/fine-tune-whisper (great
    blog by the way, super useful!). Surely it has to be possible, since running whisper
    via python / CLI somehow automatically splits the transcript?
  created_at: 2023-01-13 21:27:15+00:00
  edited: false
  hidden: false
  id: 63c1ccb3f9453420b5d9f5a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-01-16T15:36:15.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;daniel-v-e&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daniel-v-e\"\
          >@<span class=\"underline\">daniel-v-e</span></a></span>\n\n\t</span></span>!\
          \ </p>\n<p>The audio samples are split into 30s chunks for two reasons:</p>\n\
          <ol>\n<li>The Whisper model is defined such that the inputs are always padded/truncated\
          \ to 30s. Consequently, the model <strong>always</strong> expects audio\
          \ samples of the same input length (30s). This is explained in more depth\
          \ in the blog post (<a href=\"https://huggingface.co/blog/fine-tune-whisper#load-whisperfeatureextractor\"\
          >https://huggingface.co/blog/fine-tune-whisper#load-whisperfeatureextractor</a>)</li>\n\
          <li>Due to the attention mechanism in the Transformer block, memory complexity\
          \ scales with input length ^ 2, so doubling the audio input length quadruples\
          \ the memory required by the encoder. The memory required quickly blows\
          \ up as we increase the length of our audio sequences, hence the need to\
          \ define a cut-off to prevent out-of-memory errors.</li>\n</ol>\n<p>The\
          \ only way really of transcribing audio samples &gt; 30s is by chunking\
          \ them into 30s samples, transcribing them individually, and stitching the\
          \ transcriptions together at the boundaries. This is the same approach that\
          \ is used by the 'official' Whisper CLI: <a rel=\"nofollow\" href=\"https://github.com/openai/whisper/blob/f82bc59f5ea234d4b97fb2860842ed38519f7e65/whisper/transcribe.py#L175\"\
          >https://github.com/openai/whisper/blob/f82bc59f5ea234d4b97fb2860842ed38519f7e65/whisper/transcribe.py#L175</a></p>\n\
          <p>Therefore, these are two equivalent approaches.</p>\n"
        raw: "Hey @daniel-v-e! \n\nThe audio samples are split into 30s chunks for\
          \ two reasons:\n1. The Whisper model is defined such that the inputs are\
          \ always padded/truncated to 30s. Consequently, the model **always** expects\
          \ audio samples of the same input length (30s). This is explained in more\
          \ depth in the blog post (https://huggingface.co/blog/fine-tune-whisper#load-whisperfeatureextractor)\n\
          2. Due to the attention mechanism in the Transformer block, memory complexity\
          \ scales with input length ^ 2, so doubling the audio input length quadruples\
          \ the memory required by the encoder. The memory required quickly blows\
          \ up as we increase the length of our audio sequences, hence the need to\
          \ define a cut-off to prevent out-of-memory errors.\n\nThe only way really\
          \ of transcribing audio samples > 30s is by chunking them into 30s samples,\
          \ transcribing them individually, and stitching the transcriptions together\
          \ at the boundaries. This is the same approach that is used by the 'official'\
          \ Whisper CLI: https://github.com/openai/whisper/blob/f82bc59f5ea234d4b97fb2860842ed38519f7e65/whisper/transcribe.py#L175\n\
          \nTherefore, these are two equivalent approaches."
        updatedAt: '2023-01-16T15:36:15.565Z'
      numEdits: 0
      reactions: []
    id: 63c56eef388e302949bac865
    type: comment
  author: sanchit-gandhi
  content: "Hey @daniel-v-e! \n\nThe audio samples are split into 30s chunks for two\
    \ reasons:\n1. The Whisper model is defined such that the inputs are always padded/truncated\
    \ to 30s. Consequently, the model **always** expects audio samples of the same\
    \ input length (30s). This is explained in more depth in the blog post (https://huggingface.co/blog/fine-tune-whisper#load-whisperfeatureextractor)\n\
    2. Due to the attention mechanism in the Transformer block, memory complexity\
    \ scales with input length ^ 2, so doubling the audio input length quadruples\
    \ the memory required by the encoder. The memory required quickly blows up as\
    \ we increase the length of our audio sequences, hence the need to define a cut-off\
    \ to prevent out-of-memory errors.\n\nThe only way really of transcribing audio\
    \ samples > 30s is by chunking them into 30s samples, transcribing them individually,\
    \ and stitching the transcriptions together at the boundaries. This is the same\
    \ approach that is used by the 'official' Whisper CLI: https://github.com/openai/whisper/blob/f82bc59f5ea234d4b97fb2860842ed38519f7e65/whisper/transcribe.py#L175\n\
    \nTherefore, these are two equivalent approaches."
  created_at: 2023-01-16 15:36:15+00:00
  edited: false
  hidden: false
  id: 63c56eef388e302949bac865
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e365b78b6b58b63c617cee36d5ff0de8.svg
      fullname: Daniel von Eschwege
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniel-v-e
      type: user
    createdAt: '2023-01-31T10:45:03.000Z'
    data:
      edited: false
      editors:
      - daniel-v-e
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e365b78b6b58b63c617cee36d5ff0de8.svg
          fullname: Daniel von Eschwege
          isHf: false
          isPro: false
          name: daniel-v-e
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Thanks a lot for the clarification! That is actually perfect, since I\
          \ can now split the audio into 30sec chunks exactly the way whisper does\
          \ it, using the code you linked to. Much appreciated!</p>\n"
        raw: 'Hey @sanchit-gandhi,


          Thanks a lot for the clarification! That is actually perfect, since I can
          now split the audio into 30sec chunks exactly the way whisper does it, using
          the code you linked to. Much appreciated!'
        updatedAt: '2023-01-31T10:45:03.247Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63d8f12fd3ea5e7669137b7e
    id: 63d8f12fd3ea5e7669137b7d
    type: comment
  author: daniel-v-e
  content: 'Hey @sanchit-gandhi,


    Thanks a lot for the clarification! That is actually perfect, since I can now
    split the audio into 30sec chunks exactly the way whisper does it, using the code
    you linked to. Much appreciated!'
  created_at: 2023-01-31 10:45:03+00:00
  edited: false
  hidden: false
  id: 63d8f12fd3ea5e7669137b7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e365b78b6b58b63c617cee36d5ff0de8.svg
      fullname: Daniel von Eschwege
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniel-v-e
      type: user
    createdAt: '2023-01-31T10:45:03.000Z'
    data:
      status: closed
    id: 63d8f12fd3ea5e7669137b7e
    type: status-change
  author: daniel-v-e
  created_at: 2023-01-31 10:45:03+00:00
  id: 63d8f12fd3ea5e7669137b7e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: GeoffVdr/whisper-medium-nlcv11
repo_type: model
status: closed
target_branch: null
title: Batch transcription
