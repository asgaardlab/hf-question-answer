!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yuguang
conflicting_files: null
created_at: 2023-06-25 01:44:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1322e673c3ab61e620902c325bbf5628.svg
      fullname: Yuguang Meng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuguang
      type: user
    createdAt: '2023-06-25T02:44:43.000Z'
    data:
      edited: true
      editors:
      - yuguang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3721546530723572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1322e673c3ab61e620902c325bbf5628.svg
          fullname: Yuguang Meng
          isHf: false
          isPro: false
          name: yuguang
          type: user
        html: "<p>import torch<br>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import\
          \ os<br>os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\
          <br>os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'<br>device = torch.device('cuda')<br>gpus=[0,1,2,3,4,5,6,7]<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"/data/ygmeng/xuanyuan\", trust_remote_code=True)<br>model\
          \ = AutoModelForCausalLM.from_pretrained(\"/data/ygmeng/xuanyuan\",trust_remote_code=True,device_map=\"\
          auto\")<br>\u4E00\u5171\u9047\u52303\u4E2A\u95EE\u9898\uFF0C\u524D\u4E24\
          \u4E2A\u5DF2\u7ECF\u89E3\u51B3\u4E86\u3002<br>1\u3001\u5982\u679C\u4E0D\u52A0\
          device_map=\"auto\"\uFF0C\u8F7D\u5165\u6A21\u578B\u7684\u65F6\u5019\uFF0C\
          \u4E0D\u76F4\u63A5\u4F7F\u7528GPU\uFF0C\u8F7D\u5165\u65F6\u95F4\u5F88\u957F\
          \uFF0C\u8FD9\u4E2A\u76EE\u524D\u5DF2\u7ECF\u89E3\u51B3\u4E86\u3002<br>2\u3001\
          \u4E0A\u9762\u7684\u8FD9\u6BB5\u4EE3\u7801\u8F7D\u5165\uFF0C\u4F1A\u62A5\
          <br>(torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 13.40 GiB (GPU 0; 79.15 GiB total capacity; 68.53 GiB already allocated;\
          \ 9.66 GiB free; 68.53 GiB reserved in total by PyTorch) If reserved memory\
          \ is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF)<br><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/t21ol3iC0CYnKUMYnIZ__.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/t21ol3iC0CYnKUMYnIZ__.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/pDL5GQ9uluhdORSj_lPmO.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/pDL5GQ9uluhdORSj_lPmO.png\"\
          ></a><br>\u7B2C\u4E00\u6B21\u8F7D\u5165\u5931\u8D25\u3002<br>\u91CD\u8BD5\
          \u4E86\u4E00\u6B21\uFF0C\u6210\u529F\u8F7D\u5165<br>3\u3001\u63A8\u7406\u901F\
          \u5EA6\u5F88\u6162\uFF0C\u6CA1\u6709\u4F7F\u7528\u5230GPU<br>model = model.eval()<br>inputs=tokenizer(\"\
          Human:\u4F60\u662F\u8C01\uFF1F\\n\\nAssistant: \",return_tensors='pt')<br>inputs.to(device)<br>output\
          \ = model.generate(**inputs, do_sample=True, temperature=0.8, top_k=50,\
          \ top_p=0.9, early_stopping=True, repetition_penalty=1.1, min_new_tokens=1,\
          \ max_new_tokens=256)<br>\u6B64\u65F6\u4F1A\u62A5warning\u4FE1\u606F\uFF1A\
          <br> You are calling .generate() with the <code>input_ids</code> being on\
          \ a device type different than your model's device. <code>input_ids</code>\
          \ is on cuda, whereas the model is on cpu. You may experience unexpected\
          \ behaviors or slower generation. Please make sure that you have put <code>input_ids</code>\
          \ to the correct device by calling for example input_ids = input_ids.to('cpu')\
          \ before running <code>.generate()</code>.<br>\u8FD9\u91CC\u8BF4inputs_id\u5728\
          gpu\uFF0C\u800C\u6A21\u578B\u5728cpu\u3002<br>\u6B64\u65F6\u5982\u679Cmodel.to(device)\uFF0C\
          \u4F1A\u62A5OOM\u3002<br>torch.cuda.OutOfMemoryError: CUDA out of memory.\
          \ Tried to allocate 2.30 GiB (GPU 0; 79.15 GiB total capacity; 77.72 GiB\
          \ already allocated; 479.62 MiB free; 77.72 GiB reserved in total by PyTorch)\
          \ If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>\u5982\
          \u679Cinputs.to(\"cpu\")\uFF0C\u63A8\u7406\u901F\u5EA6\u5F88\u6162\uFF0C\
          \u5B8C\u5168\u7528cpu\u8FDB\u884C\u63A8\u7406\u3002<br>\u8FD9\u79CD\u60C5\
          \u51B5\u600E\u4E48\u89E3\u51B3\u3002</p>\n"
        raw: "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\
          \nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\ndevice = torch.device('cuda')\n\
          gpus=[0,1,2,3,4,5,6,7]\ntokenizer = AutoTokenizer.from_pretrained(\"/data/ygmeng/xuanyuan\"\
          , trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          /data/ygmeng/xuanyuan\",trust_remote_code=True,device_map=\"auto\")\n\u4E00\
          \u5171\u9047\u52303\u4E2A\u95EE\u9898\uFF0C\u524D\u4E24\u4E2A\u5DF2\u7ECF\
          \u89E3\u51B3\u4E86\u3002\n1\u3001\u5982\u679C\u4E0D\u52A0device_map=\"auto\"\
          \uFF0C\u8F7D\u5165\u6A21\u578B\u7684\u65F6\u5019\uFF0C\u4E0D\u76F4\u63A5\
          \u4F7F\u7528GPU\uFF0C\u8F7D\u5165\u65F6\u95F4\u5F88\u957F\uFF0C\u8FD9\u4E2A\
          \u76EE\u524D\u5DF2\u7ECF\u89E3\u51B3\u4E86\u3002\n2\u3001\u4E0A\u9762\u7684\
          \u8FD9\u6BB5\u4EE3\u7801\u8F7D\u5165\uFF0C\u4F1A\u62A5\n(torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 13.40 GiB (GPU 0; 79.15 GiB total\
          \ capacity; 68.53 GiB already allocated; 9.66 GiB free; 68.53 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF)\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/t21ol3iC0CYnKUMYnIZ__.png)\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/pDL5GQ9uluhdORSj_lPmO.png)\n\
          \u7B2C\u4E00\u6B21\u8F7D\u5165\u5931\u8D25\u3002\n\u91CD\u8BD5\u4E86\u4E00\
          \u6B21\uFF0C\u6210\u529F\u8F7D\u5165\n3\u3001\u63A8\u7406\u901F\u5EA6\u5F88\
          \u6162\uFF0C\u6CA1\u6709\u4F7F\u7528\u5230GPU\nmodel = model.eval()\ninputs=tokenizer(\"\
          Human:\u4F60\u662F\u8C01\uFF1F\\n\\nAssistant: \",return_tensors='pt')\n\
          inputs.to(device)\noutput = model.generate(**inputs, do_sample=True, temperature=0.8,\
          \ top_k=50, top_p=0.9, early_stopping=True, repetition_penalty=1.1, min_new_tokens=1,\
          \ max_new_tokens=256)\n\u6B64\u65F6\u4F1A\u62A5warning\u4FE1\u606F\uFF1A\
          \n You are calling .generate() with the `input_ids` being on a device type\
          \ different than your model's device. `input_ids` is on cuda, whereas the\
          \ model is on cpu. You may experience unexpected behaviors or slower generation.\
          \ Please make sure that you have put `input_ids` to the correct device by\
          \ calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n\
          \u8FD9\u91CC\u8BF4inputs_id\u5728gpu\uFF0C\u800C\u6A21\u578B\u5728cpu\u3002\
          \n\u6B64\u65F6\u5982\u679Cmodel.to(device)\uFF0C\u4F1A\u62A5OOM\u3002\n\
          torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.30\
          \ GiB (GPU 0; 79.15 GiB total capacity; 77.72 GiB already allocated; 479.62\
          \ MiB free; 77.72 GiB reserved in total by PyTorch) If reserved memory is\
          \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          \u5982\u679Cinputs.to(\"cpu\")\uFF0C\u63A8\u7406\u901F\u5EA6\u5F88\u6162\
          \uFF0C\u5B8C\u5168\u7528cpu\u8FDB\u884C\u63A8\u7406\u3002\n\u8FD9\u79CD\u60C5\
          \u51B5\u600E\u4E48\u89E3\u51B3\u3002"
        updatedAt: '2023-06-25T04:44:47.998Z'
      numEdits: 11
      reactions: []
    id: 6497aa1bf436b85fddafe992
    type: comment
  author: yuguang
  content: "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n\
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\ndevice = torch.device('cuda')\n\
    gpus=[0,1,2,3,4,5,6,7]\ntokenizer = AutoTokenizer.from_pretrained(\"/data/ygmeng/xuanyuan\"\
    , trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"/data/ygmeng/xuanyuan\"\
    ,trust_remote_code=True,device_map=\"auto\")\n\u4E00\u5171\u9047\u52303\u4E2A\u95EE\
    \u9898\uFF0C\u524D\u4E24\u4E2A\u5DF2\u7ECF\u89E3\u51B3\u4E86\u3002\n1\u3001\u5982\
    \u679C\u4E0D\u52A0device_map=\"auto\"\uFF0C\u8F7D\u5165\u6A21\u578B\u7684\u65F6\
    \u5019\uFF0C\u4E0D\u76F4\u63A5\u4F7F\u7528GPU\uFF0C\u8F7D\u5165\u65F6\u95F4\u5F88\
    \u957F\uFF0C\u8FD9\u4E2A\u76EE\u524D\u5DF2\u7ECF\u89E3\u51B3\u4E86\u3002\n2\u3001\
    \u4E0A\u9762\u7684\u8FD9\u6BB5\u4EE3\u7801\u8F7D\u5165\uFF0C\u4F1A\u62A5\n(torch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 13.40 GiB (GPU 0; 79.15 GiB total capacity;\
    \ 68.53 GiB already allocated; 9.66 GiB free; 68.53 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF)\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/t21ol3iC0CYnKUMYnIZ__.png)\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/60e6704d370ea79efbb3f539/pDL5GQ9uluhdORSj_lPmO.png)\n\
    \u7B2C\u4E00\u6B21\u8F7D\u5165\u5931\u8D25\u3002\n\u91CD\u8BD5\u4E86\u4E00\u6B21\
    \uFF0C\u6210\u529F\u8F7D\u5165\n3\u3001\u63A8\u7406\u901F\u5EA6\u5F88\u6162\uFF0C\
    \u6CA1\u6709\u4F7F\u7528\u5230GPU\nmodel = model.eval()\ninputs=tokenizer(\"Human:\u4F60\
    \u662F\u8C01\uFF1F\\n\\nAssistant: \",return_tensors='pt')\ninputs.to(device)\n\
    output = model.generate(**inputs, do_sample=True, temperature=0.8, top_k=50, top_p=0.9,\
    \ early_stopping=True, repetition_penalty=1.1, min_new_tokens=1, max_new_tokens=256)\n\
    \u6B64\u65F6\u4F1A\u62A5warning\u4FE1\u606F\uFF1A\n You are calling .generate()\
    \ with the `input_ids` being on a device type different than your model's device.\
    \ `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected\
    \ behaviors or slower generation. Please make sure that you have put `input_ids`\
    \ to the correct device by calling for example input_ids = input_ids.to('cpu')\
    \ before running `.generate()`.\n\u8FD9\u91CC\u8BF4inputs_id\u5728gpu\uFF0C\u800C\
    \u6A21\u578B\u5728cpu\u3002\n\u6B64\u65F6\u5982\u679Cmodel.to(device)\uFF0C\u4F1A\
    \u62A5OOM\u3002\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
    \ 2.30 GiB (GPU 0; 79.15 GiB total capacity; 77.72 GiB already allocated; 479.62\
    \ MiB free; 77.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
    \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\u5982\u679Cinputs.to(\"\
    cpu\")\uFF0C\u63A8\u7406\u901F\u5EA6\u5F88\u6162\uFF0C\u5B8C\u5168\u7528cpu\u8FDB\
    \u884C\u63A8\u7406\u3002\n\u8FD9\u79CD\u60C5\u51B5\u600E\u4E48\u89E3\u51B3\u3002"
  created_at: 2023-06-25 01:44:43+00:00
  edited: true
  hidden: false
  id: 6497aa1bf436b85fddafe992
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/1322e673c3ab61e620902c325bbf5628.svg
      fullname: Yuguang Meng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuguang
      type: user
    createdAt: '2023-06-25T02:44:54.000Z'
    data:
      from: "8\u5361\u7684A100\uFF0C\u8F7D\u5165\u65F6\u63D0\u793A\u5185\u5B58\u4E0D\
        \u591F\uFF0C\u4F46\u662F\u53EA\u628A\u4E00\u5F20\u5361\u5360\u6EE1\u4E86\u5B9E\
        \u9645\u4E0A"
      to: "8\u5361\u7684A100\uFF0C\u8F7D\u5165\u65F6\u63D0\u793A\u5185\u5B58\u4E0D\
        \u591F"
    id: 6497aa26f71aaa3d59bd1910
    type: title-change
  author: yuguang
  created_at: 2023-06-25 01:44:54+00:00
  id: 6497aa26f71aaa3d59bd1910
  new_title: "8\u5361\u7684A100\uFF0C\u8F7D\u5165\u65F6\u63D0\u793A\u5185\u5B58\u4E0D\
    \u591F"
  old_title: "8\u5361\u7684A100\uFF0C\u8F7D\u5165\u65F6\u63D0\u793A\u5185\u5B58\u4E0D\
    \u591F\uFF0C\u4F46\u662F\u53EA\u628A\u4E00\u5F20\u5361\u5360\u6EE1\u4E86\u5B9E\
    \u9645\u4E0A"
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/1322e673c3ab61e620902c325bbf5628.svg
      fullname: Yuguang Meng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuguang
      type: user
    createdAt: '2023-06-25T03:24:04.000Z'
    data:
      from: "8\u5361\u7684A100\uFF0C\u8F7D\u5165\u65F6\u63D0\u793A\u5185\u5B58\u4E0D\
        \u591F"
      to: "8\u5361\u7684A100\uFF0C\u63A8\u7406\u901F\u5EA6\u8FC7\u6162"
    id: 6497b35444fc44b6dcd0b729
    type: title-change
  author: yuguang
  created_at: 2023-06-25 02:24:04+00:00
  id: 6497b35444fc44b6dcd0b729
  new_title: "8\u5361\u7684A100\uFF0C\u63A8\u7406\u901F\u5EA6\u8FC7\u6162"
  old_title: "8\u5361\u7684A100\uFF0C\u8F7D\u5165\u65F6\u63D0\u793A\u5185\u5B58\u4E0D\
    \u591F"
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d5f7852e17f76948671c338dabfeab9.svg
      fullname: Wei Han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MengAI
      type: user
    createdAt: '2023-11-14T12:09:33.000Z'
    data:
      edited: false
      editors:
      - MengAI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4536448121070862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d5f7852e17f76948671c338dabfeab9.svg
          fullname: Wei Han
          isHf: false
          isPro: false
          name: MengAI
          type: user
        html: '<p>import os<br>os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32"<br>os.environ[''CUDA_VISIBLE_DEVICES'']
          = ''0,1,2,3,4,5,6,7''</p>

          <p>import torch<br>from transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>device = torch.device(''cuda'')<br>gpus=[0,1,2,3,4,5,6,7]<br>tokenizer
          = AutoTokenizer.from_pretrained("/data/ygmeng/xuanyuan", trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained("/data/ygmeng/xuanyuan",trust_remote_code=True,device_map="auto")</p>

          '
        raw: 'import os

          os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32"

          os.environ[''CUDA_VISIBLE_DEVICES''] = ''0,1,2,3,4,5,6,7''


          import torch

          from transformers import AutoTokenizer, AutoModelForCausalLM


          device = torch.device(''cuda'')

          gpus=[0,1,2,3,4,5,6,7]

          tokenizer = AutoTokenizer.from_pretrained("/data/ygmeng/xuanyuan", trust_remote_code=True)

          model = AutoModelForCausalLM.from_pretrained("/data/ygmeng/xuanyuan",trust_remote_code=True,device_map="auto")'
        updatedAt: '2023-11-14T12:09:33.325Z'
      numEdits: 0
      reactions: []
    id: 6553637d711b1e1e111c4f3a
    type: comment
  author: MengAI
  content: 'import os

    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32"

    os.environ[''CUDA_VISIBLE_DEVICES''] = ''0,1,2,3,4,5,6,7''


    import torch

    from transformers import AutoTokenizer, AutoModelForCausalLM


    device = torch.device(''cuda'')

    gpus=[0,1,2,3,4,5,6,7]

    tokenizer = AutoTokenizer.from_pretrained("/data/ygmeng/xuanyuan", trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained("/data/ygmeng/xuanyuan",trust_remote_code=True,device_map="auto")'
  created_at: 2023-11-14 12:09:33+00:00
  edited: false
  hidden: false
  id: 6553637d711b1e1e111c4f3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec1b5c9725953e8322120a35509d35bf.svg
      fullname: lhlnlp@ai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lhlnlp
      type: user
    createdAt: '2023-12-29T06:56:06.000Z'
    data:
      edited: false
      editors:
      - lhlnlp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22069524228572845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec1b5c9725953e8322120a35509d35bf.svg
          fullname: lhlnlp@ai
          isHf: false
          isPro: false
          name: lhlnlp
          type: user
        html: "<p>mo'xin<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/625cc0e9f14e262dbb4643e7/0_vDkcI8fa4vUZSmrb0OE.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/625cc0e9f14e262dbb4643e7/0_vDkcI8fa4vUZSmrb0OE.png\"\
          ></a><br>\u6A21\u578B\u6CA1\u6709\u63A8\u7406\u8F93\u51FA\u7ED3\u679C</p>\n"
        raw: "mo'xin\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/625cc0e9f14e262dbb4643e7/0_vDkcI8fa4vUZSmrb0OE.png)\n\
          \u6A21\u578B\u6CA1\u6709\u63A8\u7406\u8F93\u51FA\u7ED3\u679C"
        updatedAt: '2023-12-29T06:56:06.039Z'
      numEdits: 0
      reactions: []
    id: 658e6d8650d39af7f4aef868
    type: comment
  author: lhlnlp
  content: "mo'xin\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/625cc0e9f14e262dbb4643e7/0_vDkcI8fa4vUZSmrb0OE.png)\n\
    \u6A21\u578B\u6CA1\u6709\u63A8\u7406\u8F93\u51FA\u7ED3\u679C"
  created_at: 2023-12-29 06:56:06+00:00
  edited: false
  hidden: false
  id: 658e6d8650d39af7f4aef868
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: xyz-nlp/XuanYuan2.0
repo_type: model
status: open
target_branch: null
title: "8\u5361\u7684A100\uFF0C\u63A8\u7406\u901F\u5EA6\u8FC7\u6162"
