!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AS1200
conflicting_files: null
created_at: 2023-12-02 10:53:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/062c16dfafe7f1d7371454934bf91527.svg
      fullname: SA2100
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AS1200
      type: user
    createdAt: '2023-12-02T10:53:03.000Z'
    data:
      edited: true
      editors:
      - AS1200
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.880086362361908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/062c16dfafe7f1d7371454934bf91527.svg
          fullname: SA2100
          isHf: false
          isPro: false
          name: AS1200
          type: user
        html: "<p>I get the error \"AttributeError llamacppmodel object has attribute\
          \ model\"</p>\n<p>This error occurs when using llama cpp and ctransformers</p>\n\
          <p>In the model description there is a link to the latest version of cpp,\
          \ but I don\u2019t understand how to install it in the oobabooga interface.\
          \ Help me please. I really want to test qwen on my local computer.</p>\n\
          <p>I'm a noob and hope to get clear instructions.</p>\n"
        raw: "I get the error \"AttributeError llamacppmodel object has attribute\
          \ model\"\n\nThis error occurs when using llama cpp and ctransformers\n\n\
          In the model description there is a link to the latest version of cpp, but\
          \ I don\u2019t understand how to install it in the oobabooga interface.\
          \ Help me please. I really want to test qwen on my local computer.\n\nI'm\
          \ a noob and hope to get clear instructions."
        updatedAt: '2023-12-02T10:55:32.398Z'
      numEdits: 1
      reactions: []
    id: 656b0c8f02a56b531a6127d3
    type: comment
  author: AS1200
  content: "I get the error \"AttributeError llamacppmodel object has attribute model\"\
    \n\nThis error occurs when using llama cpp and ctransformers\n\nIn the model description\
    \ there is a link to the latest version of cpp, but I don\u2019t understand how\
    \ to install it in the oobabooga interface. Help me please. I really want to test\
    \ qwen on my local computer.\n\nI'm a noob and hope to get clear instructions."
  created_at: 2023-12-02 10:53:03+00:00
  edited: true
  hidden: false
  id: 656b0c8f02a56b531a6127d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-12-02T11:14:24.000Z'
    data:
      edited: true
      editors:
      - JosephusCheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7149612903594971
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
          fullname: "Jos\xE9phus Cheung"
          isHf: false
          isPro: false
          name: JosephusCheung
          type: user
        html: '<p>Working on that.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/Cc-OUUCOa58YyBCZdJx76.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/Cc-OUUCOa58YyBCZdJx76.png"></a><br>Building
          new whls, while you can manually install the newest version with <a rel="nofollow"
          href="https://github.com/CausalLM/llama-cpp-python">https://github.com/CausalLM/llama-cpp-python</a><br><del>cuBLAS
          for example: <code>CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install git+https://github.com/CausalLM/llama-cpp-python</code></del><br>Sorry,
          should wait for whl build, different pkg names.</p>

          '
        raw: 'Working on that.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/Cc-OUUCOa58YyBCZdJx76.png)

          Building new whls, while you can manually install the newest version with
          https://github.com/CausalLM/llama-cpp-python

          ~cuBLAS for example: `CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install git+https://github.com/CausalLM/llama-cpp-python`~

          Sorry, should wait for whl build, different pkg names.'
        updatedAt: '2023-12-02T11:17:59.713Z'
      numEdits: 1
      reactions: []
    id: 656b11909c8778992f6b450b
    type: comment
  author: JosephusCheung
  content: 'Working on that.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/Cc-OUUCOa58YyBCZdJx76.png)

    Building new whls, while you can manually install the newest version with https://github.com/CausalLM/llama-cpp-python

    ~cuBLAS for example: `CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install git+https://github.com/CausalLM/llama-cpp-python`~

    Sorry, should wait for whl build, different pkg names.'
  created_at: 2023-12-02 11:14:24+00:00
  edited: true
  hidden: false
  id: 656b11909c8778992f6b450b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-12-02T12:44:27.000Z'
    data:
      edited: false
      editors:
      - JosephusCheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6588630676269531
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
          fullname: "Jos\xE9phus Cheung"
          isHf: false
          isPro: false
          name: JosephusCheung
          type: user
        html: '<p>updated, please follow the new readme</p>

          '
        raw: updated, please follow the new readme
        updatedAt: '2023-12-02T12:44:27.510Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656b26abd848a6683aa1f5d8
    id: 656b26abd848a6683aa1f5d2
    type: comment
  author: JosephusCheung
  content: updated, please follow the new readme
  created_at: 2023-12-02 12:44:27+00:00
  edited: false
  hidden: false
  id: 656b26abd848a6683aa1f5d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-12-02T12:44:27.000Z'
    data:
      status: closed
    id: 656b26abd848a6683aa1f5d8
    type: status-change
  author: JosephusCheung
  created_at: 2023-12-02 12:44:27+00:00
  id: 656b26abd848a6683aa1f5d8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: CausalLM/72B-preview-GGUF
repo_type: model
status: closed
target_branch: null
title: Error when trying to load into RAM
