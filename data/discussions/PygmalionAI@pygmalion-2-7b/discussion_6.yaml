!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ta-ylor
conflicting_files: null
created_at: 2023-11-27 11:00:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a089ec2a687093a08a351699f3d8b88d.svg
      fullname: Taylor Maass
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ta-ylor
      type: user
    createdAt: '2023-11-27T11:00:41.000Z'
    data:
      edited: false
      editors:
      - ta-ylor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9646910429000854
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a089ec2a687093a08a351699f3d8b88d.svg
          fullname: Taylor Maass
          isHf: false
          isPro: false
          name: ta-ylor
          type: user
        html: '<p>The model card refers to &lt;|system|&gt;, &lt;|model|&gt;, and
          &lt;|user|&gt; as tokens, but in <code>tokenizer.json</code> they don''t
          appear, and I can''t get them to be tokenized as single tokens when running
          the model myself. Is there a typo in the model card or am I misunderstanding
          something?</p>

          '
        raw: The model card refers to <|system|>, <|model|>, and <|user|> as tokens,
          but in `tokenizer.json` they don't appear, and I can't get them to be tokenized
          as single tokens when running the model myself. Is there a typo in the model
          card or am I misunderstanding something?
        updatedAt: '2023-11-27T11:00:41.404Z'
      numEdits: 0
      reactions: []
    id: 656476d9871dde8e7b5032cf
    type: comment
  author: ta-ylor
  content: The model card refers to <|system|>, <|model|>, and <|user|> as tokens,
    but in `tokenizer.json` they don't appear, and I can't get them to be tokenized
    as single tokens when running the model myself. Is there a typo in the model card
    or am I misunderstanding something?
  created_at: 2023-11-27 11:00:41+00:00
  edited: false
  hidden: false
  id: 656476d9871dde8e7b5032cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a310fc1f6197573e8c36f9ec5a5a2369.svg
      fullname: zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shane112
      type: user
    createdAt: '2023-12-08T02:46:47.000Z'
    data:
      edited: false
      editors:
      - shane112
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8403872847557068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a310fc1f6197573e8c36f9ec5a5a2369.svg
          fullname: zhao
          isHf: false
          isPro: false
          name: shane112
          type: user
        html: '<p>Can anyone teach me how to write a proper prompt?</p>

          '
        raw: 'Can anyone teach me how to write a proper prompt?

          '
        updatedAt: '2023-12-08T02:46:47.573Z'
      numEdits: 0
      reactions: []
    id: 65728397e887face3eba5eec
    type: comment
  author: shane112
  content: 'Can anyone teach me how to write a proper prompt?

    '
  created_at: 2023-12-08 02:46:47+00:00
  edited: false
  hidden: false
  id: 65728397e887face3eba5eec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-12-08T05:44:48.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9392765760421753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>Though it makes sense to have them be actual tokens, I''m not sure
          they mean token in the tokenizer.json sense, I think they mean token in
          the English sense (so it wouldn''t get tokenised to a single token). But
          hopefully someone from the Pygmalion team can confirm.</p>

          '
        raw: Though it makes sense to have them be actual tokens, I'm not sure they
          mean token in the tokenizer.json sense, I think they mean token in the English
          sense (so it wouldn't get tokenised to a single token). But hopefully someone
          from the Pygmalion team can confirm.
        updatedAt: '2023-12-08T05:44:48.483Z'
      numEdits: 0
      reactions: []
    id: 6572ad506e1ac5922ab0c605
    type: comment
  author: RazielAU
  content: Though it makes sense to have them be actual tokens, I'm not sure they
    mean token in the tokenizer.json sense, I think they mean token in the English
    sense (so it wouldn't get tokenised to a single token). But hopefully someone
    from the Pygmalion team can confirm.
  created_at: 2023-12-08 05:44:48+00:00
  edited: false
  hidden: false
  id: 6572ad506e1ac5922ab0c605
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6475f9e9f84258330f5b31b9/Su2NH3gRLckg56lXCcUQ2.jpeg?w=200&h=200&f=face
      fullname: Theseus Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheseusGrey
      type: user
    createdAt: '2024-01-13T01:21:37.000Z'
    data:
      edited: true
      editors:
      - TheseusGrey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8268288969993591
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6475f9e9f84258330f5b31b9/Su2NH3gRLckg56lXCcUQ2.jpeg?w=200&h=200&f=face
          fullname: Theseus Grey
          isHf: false
          isPro: false
          name: TheseusGrey
          type: user
        html: "<p>My understanding is they should appear as plain text in your prompts,\
          \ at least that's how I've had the most success. As an example from a project\
          \ I'm working on.</p>\n<p>First things first is taking my conversation history\
          \ and formatting it for the model:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >formatConversation</span>(<span class=\"hljs-params\">messages: <span class=\"\
          hljs-type\">List</span>[<span class=\"hljs-type\">Dict</span>[<span class=\"\
          hljs-built_in\">str</span>, <span class=\"hljs-built_in\">str</span>]]</span>):\n\
          \    <span class=\"hljs-comment\"># The f-string shows how I add the tokens\
          \ into the model (which could be entirely wrong but I've been getting some\
          \ success with it when testing)</span>\n    <span class=\"hljs-keyword\"\
          >return</span> <span class=\"hljs-string\">'\\n'</span>.join([<span class=\"\
          hljs-string\">f\"&lt;|<span class=\"hljs-subst\">{message[<span class=\"\
          hljs-string\">'role'</span>]}</span>|&gt;<span class=\"hljs-subst\">{message[<span\
          \ class=\"hljs-string\">'content'</span>]}</span>\"</span> <span class=\"\
          hljs-keyword\">for</span> message <span class=\"hljs-keyword\">in</span>\
          \ messages])\n</code></pre>\n<p>Then I just pass in my conversation history\
          \ and then append the <code>&lt;|assistant|&gt;</code> tag to the end of\
          \ the text: </p>\n<pre><code class=\"language-python\">formattedConversation\
          \ = formatConversation(conversation.messages) + <span class=\"hljs-string\"\
          >'&lt;|assistant|&gt;'</span>\n</code></pre>\n<p>Which gets passed to the\
          \ text-generation pipeline to generate responses: </p>\n<pre><code class=\"\
          language-python\">pipeline(\n        formattedConversation, <span class=\"\
          hljs-comment\"># &lt;= convo history with assistant tag appended</span>\n\
          \        do_sample=<span class=\"hljs-literal\">True</span>,\n        top_k=<span\
          \ class=\"hljs-number\">10</span>,\n        num_return_sequences=<span class=\"\
          hljs-number\">1</span>,\n        eos_token_id=tokenizer.eos_token_id,\n\
          \        max_length=<span class=\"hljs-number\">200</span>,\n    )\n</code></pre>\n\
          <p>I should probably add this is my first major LLM project so I'm still\
          \ getting my head around the various config options (the ones provided to\
          \ the <code>pipeline</code> in the snippet above I took from somewhere in\
          \ the HuggingFace docs on the Llama2 Model this one is built on top of</p>\n"
        raw: "My understanding is they should appear as plain text in your prompts,\
          \ at least that's how I've had the most success. As an example from a project\
          \ I'm working on.\n\nFirst things first is taking my conversation history\
          \ and formatting it for the model:\n```python\ndef formatConversation(messages:\
          \ List[Dict[str, str]]):\n\t# The f-string shows how I add the tokens into\
          \ the model (which could be entirely wrong but I've been getting some success\
          \ with it when testing)\n\treturn '\\n'.join([f\"<|{message['role']}|>{message['content']}\"\
          \ for message in messages])\n```\n\nThen I just pass in my conversation\
          \ history and then append the `<|assistant|>` tag to the end of the text:\
          \ \n```python\nformattedConversation = formatConversation(conversation.messages)\
          \ + '<|assistant|>'\n```\n\nWhich gets passed to the text-generation pipeline\
          \ to generate responses: \n```python\npipeline(\n\t\tformattedConversation,\
          \ # <= convo history with assistant tag appended\n\t\tdo_sample=True,\n\t\
          \ttop_k=10,\n\t\tnum_return_sequences=1,\n\t\teos_token_id=tokenizer.eos_token_id,\n\
          \t\tmax_length=200,\n\t)\n```\n\nI should probably add this is my first\
          \ major LLM project so I'm still getting my head around the various config\
          \ options (the ones provided to the `pipeline` in the snippet above I took\
          \ from somewhere in the HuggingFace docs on the Llama2 Model this one is\
          \ built on top of"
        updatedAt: '2024-01-13T01:42:54.703Z'
      numEdits: 2
      reactions: []
    id: 65a1e5a1e82b0b84907b091c
    type: comment
  author: TheseusGrey
  content: "My understanding is they should appear as plain text in your prompts,\
    \ at least that's how I've had the most success. As an example from a project\
    \ I'm working on.\n\nFirst things first is taking my conversation history and\
    \ formatting it for the model:\n```python\ndef formatConversation(messages: List[Dict[str,\
    \ str]]):\n\t# The f-string shows how I add the tokens into the model (which could\
    \ be entirely wrong but I've been getting some success with it when testing)\n\
    \treturn '\\n'.join([f\"<|{message['role']}|>{message['content']}\" for message\
    \ in messages])\n```\n\nThen I just pass in my conversation history and then append\
    \ the `<|assistant|>` tag to the end of the text: \n```python\nformattedConversation\
    \ = formatConversation(conversation.messages) + '<|assistant|>'\n```\n\nWhich\
    \ gets passed to the text-generation pipeline to generate responses: \n```python\n\
    pipeline(\n\t\tformattedConversation, # <= convo history with assistant tag appended\n\
    \t\tdo_sample=True,\n\t\ttop_k=10,\n\t\tnum_return_sequences=1,\n\t\teos_token_id=tokenizer.eos_token_id,\n\
    \t\tmax_length=200,\n\t)\n```\n\nI should probably add this is my first major\
    \ LLM project so I'm still getting my head around the various config options (the\
    \ ones provided to the `pipeline` in the snippet above I took from somewhere in\
    \ the HuggingFace docs on the Llama2 Model this one is built on top of"
  created_at: 2024-01-13 01:21:37+00:00
  edited: true
  hidden: false
  id: 65a1e5a1e82b0b84907b091c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: PygmalionAI/pygmalion-2-7b
repo_type: model
status: open
target_branch: null
title: About the <|system|>, <|model|>, and <|user|> "tokens"
