!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-12-30 13:27:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-30T13:27:59.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9252344369888306
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>What prompt template did you use? </p>

          '
        raw: 'What prompt template did you use? '
        updatedAt: '2023-12-30T13:27:59.405Z'
      numEdits: 0
      reactions: []
    id: 65901adfac02633c0d82f418
    type: comment
  author: KnutJaegersberg
  content: 'What prompt template did you use? '
  created_at: 2023-12-30 13:27:59+00:00
  edited: false
  hidden: false
  id: 65901adfac02633c0d82f418
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7af170dd135997911503527dd82aa93e.svg
      fullname: Yeonwoo Sung
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: BlackBeenie
      type: user
    createdAt: '2023-12-31T04:52:24.000Z'
    data:
      edited: false
      editors:
      - BlackBeenie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5431071519851685
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7af170dd135997911503527dd82aa93e.svg
          fullname: Yeonwoo Sung
          isHf: false
          isPro: false
          name: BlackBeenie
          type: user
        html: "<p>Below is the formatting function that I used for training:</p>\n\
          <pre><code class=\"language-python\">BASE_FSTR = <span class=\"hljs-string\"\
          >\"\"\"{0}</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"\
          >{1}</span>\n<span class=\"hljs-string\">\"\"\"</span>\n\n\n<span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">formatting_prompts_func</span>(<span\
          \ class=\"hljs-params\">example</span>):\n    output_texts = []\n    <span\
          \ class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span>\
          \ <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\"\
          >len</span>(example[<span class=\"hljs-string\">'source'</span>])):\n  \
          \      source = example[<span class=\"hljs-string\">'source'</span>][i]\n\
          \        target = example[<span class=\"hljs-string\">'target'</span>][i]\n\
          \        rationale = example[<span class=\"hljs-string\">'rationale'</span>][i]\n\
          \n        text = BASE_FSTR.<span class=\"hljs-built_in\">format</span>(source,\
          \ target)\n        output_texts.append(text)\n    <span class=\"hljs-keyword\"\
          >return</span> output_texts\n</code></pre>\n"
        raw: "Below is the formatting function that I used for training:\n\n```python\n\
          BASE_FSTR = \"\"\"{0}\n\n{1}\n\"\"\"\n\n\ndef formatting_prompts_func(example):\n\
          \    output_texts = []\n    for i in range(len(example['source'])):\n  \
          \      source = example['source'][i]\n        target = example['target'][i]\n\
          \        rationale = example['rationale'][i]\n\n        text = BASE_FSTR.format(source,\
          \ target)\n        output_texts.append(text)\n    return output_texts\n\
          ```"
        updatedAt: '2023-12-31T04:52:24.789Z'
      numEdits: 0
      reactions: []
    id: 6590f388dbdeb5bf072d5953
    type: comment
  author: BlackBeenie
  content: "Below is the formatting function that I used for training:\n\n```python\n\
    BASE_FSTR = \"\"\"{0}\n\n{1}\n\"\"\"\n\n\ndef formatting_prompts_func(example):\n\
    \    output_texts = []\n    for i in range(len(example['source'])):\n        source\
    \ = example['source'][i]\n        target = example['target'][i]\n        rationale\
    \ = example['rationale'][i]\n\n        text = BASE_FSTR.format(source, target)\n\
    \        output_texts.append(text)\n    return output_texts\n```"
  created_at: 2023-12-31 04:52:24+00:00
  edited: false
  hidden: false
  id: 6590f388dbdeb5bf072d5953
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-31T05:45:06.000Z'
    data:
      edited: true
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9489360451698303
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: "<p>not sure if I get it right, but did you include the rationale?<br>also\
          \ my interpretation of this code is there is no token sequence indicating\
          \ the alternation between instruction and response.<br>How do you prompt\
          \ your model?<br>I did a similar model that only used a few records of the\
          \ cot dataset, I formatted it like this: </p>\n<pre><code>### Instruction:\n\
          In this task, you are given a sentence with a missing word that can be an\
          \ object, a person, and/or an action. Fill in the blank with a plausible\
          \ word. Although each sentence has many correct answers, you only have to\
          \ write one answer. PersonX puts ___ on PersonY's car \n### Reasoning:\n\
          Since PersonX is putting water on PersonY's car, it could be assumed that\
          \ the car is on fire. \n### Response:\nwater\n</code></pre>\n<p>If I use\
          \ that model, I prompt it for reasoning. </p>\n"
        raw: "not sure if I get it right, but did you include the rationale? \nalso\
          \ my interpretation of this code is there is no token sequence indicating\
          \ the alternation between instruction and response. \nHow do you prompt\
          \ your model? \nI did a similar model that only used a few records of the\
          \ cot dataset, I formatted it like this: \n\n```\n### Instruction:\nIn this\
          \ task, you are given a sentence with a missing word that can be an object,\
          \ a person, and/or an action. Fill in the blank with a plausible word. Although\
          \ each sentence has many correct answers, you only have to write one answer.\
          \ PersonX puts ___ on PersonY's car \n### Reasoning:\nSince PersonX is putting\
          \ water on PersonY's car, it could be assumed that the car is on fire. \n\
          ### Response:\nwater\n\n```\n\nIf I use that model, I prompt it for reasoning. "
        updatedAt: '2023-12-31T05:46:57.654Z'
      numEdits: 2
      reactions: []
    id: 6590ffe20800e55419f2a69c
    type: comment
  author: KnutJaegersberg
  content: "not sure if I get it right, but did you include the rationale? \nalso\
    \ my interpretation of this code is there is no token sequence indicating the\
    \ alternation between instruction and response. \nHow do you prompt your model?\
    \ \nI did a similar model that only used a few records of the cot dataset, I formatted\
    \ it like this: \n\n```\n### Instruction:\nIn this task, you are given a sentence\
    \ with a missing word that can be an object, a person, and/or an action. Fill\
    \ in the blank with a plausible word. Although each sentence has many correct\
    \ answers, you only have to write one answer. PersonX puts ___ on PersonY's car\
    \ \n### Reasoning:\nSince PersonX is putting water on PersonY's car, it could\
    \ be assumed that the car is on fire. \n### Response:\nwater\n\n```\n\nIf I use\
    \ that model, I prompt it for reasoning. "
  created_at: 2023-12-31 05:45:06+00:00
  edited: true
  hidden: false
  id: 6590ffe20800e55419f2a69c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7af170dd135997911503527dd82aa93e.svg
      fullname: Yeonwoo Sung
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: BlackBeenie
      type: user
    createdAt: '2023-12-31T06:23:16.000Z'
    data:
      edited: false
      editors:
      - BlackBeenie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6284525990486145
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7af170dd135997911503527dd82aa93e.svg
          fullname: Yeonwoo Sung
          isHf: false
          isPro: false
          name: BlackBeenie
          type: user
        html: "<p>First, I did not include the rationale (sorry for the confusion).</p>\n\
          <p>For testing, I used the code below:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >get_completion</span>(<span class=\"hljs-params\">query: <span class=\"\
          hljs-built_in\">str</span>, model, tokenizer, max_new_tokens=<span class=\"\
          hljs-number\">50</span>, temperature=<span class=\"hljs-number\">0.5</span>,\
          \ top_k=<span class=\"hljs-number\">2</span></span>) -&gt; <span class=\"\
          hljs-built_in\">str</span>:\n    device = <span class=\"hljs-string\">\"\
          cuda:0\"</span>\n\n    prompt = <span class=\"hljs-string\">f\"\"\"<span\
          \ class=\"hljs-subst\">{query}</span></span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">    \"\"\"</span>\n\n    encodeds\
          \ = tokenizer(query, return_tensors=<span class=\"hljs-string\">\"pt\"</span>,\
          \ add_special_tokens=<span class=\"hljs-literal\">True</span>)\n    model_inputs\
          \ = encodeds.to(device)\n\n    <span class=\"hljs-keyword\">with</span>\
          \ torch.no_grad():\n        outputs = model.generate(\n            **model_inputs,\n\
          \            max_new_tokens=max_new_tokens,\n            do_sample=<span\
          \ class=\"hljs-literal\">True</span>,\n            temperature=temperature,\n\
          \            top_k=top_k,\n            return_dict_in_generate=<span class=\"\
          hljs-literal\">True</span>,\n            output_scores=<span class=\"hljs-literal\"\
          >True</span>,\n            pad_token_id=tokenizer.eos_token_id,\n      \
          \      repetition_penalty=<span class=\"hljs-number\">1.0</span>,\n    \
          \    )\n    output = tokenizer.decode(outputs.sequences[<span class=\"hljs-number\"\
          >0</span>], skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n\
          \    <span class=\"hljs-keyword\">return</span> output\n</code></pre>\n\
          <p>Initially, I also made some templates just like what you suggested.<br>However,\
          \ the average length of text data in the dataset that I used for training\
          \ exceeds 5000 tokens (I used 4096 for context length).<br>That is the main\
          \ reason that I used an extremely simple prompt template.</p>\n"
        raw: "First, I did not include the rationale (sorry for the confusion).\n\n\
          For testing, I used the code below:\n\n```python\ndef get_completion(query:\
          \ str, model, tokenizer, max_new_tokens=50, temperature=0.5, top_k=2) ->\
          \ str:\n    device = \"cuda:0\"\n\n    prompt = f\"\"\"{query}\n\n    \"\
          \"\"\n\n    encodeds = tokenizer(query, return_tensors=\"pt\", add_special_tokens=True)\n\
          \    model_inputs = encodeds.to(device)\n\n    with torch.no_grad():\n \
          \       outputs = model.generate(\n            **model_inputs,\n       \
          \     max_new_tokens=max_new_tokens,\n            do_sample=True,\n    \
          \        temperature=temperature,\n            top_k=top_k,\n          \
          \  return_dict_in_generate=True,\n            output_scores=True,\n    \
          \        pad_token_id=tokenizer.eos_token_id,\n            repetition_penalty=1.0,\n\
          \        )\n    output = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\
          \    return output\n```\n\nInitially, I also made some templates just like\
          \ what you suggested.\nHowever, the average length of text data in the dataset\
          \ that I used for training exceeds 5000 tokens (I used 4096 for context\
          \ length).\nThat is the main reason that I used an extremely simple prompt\
          \ template."
        updatedAt: '2023-12-31T06:23:16.936Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
    id: 659108d4bec7ad264774df09
    type: comment
  author: BlackBeenie
  content: "First, I did not include the rationale (sorry for the confusion).\n\n\
    For testing, I used the code below:\n\n```python\ndef get_completion(query: str,\
    \ model, tokenizer, max_new_tokens=50, temperature=0.5, top_k=2) -> str:\n   \
    \ device = \"cuda:0\"\n\n    prompt = f\"\"\"{query}\n\n    \"\"\"\n\n    encodeds\
    \ = tokenizer(query, return_tensors=\"pt\", add_special_tokens=True)\n    model_inputs\
    \ = encodeds.to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n\
    \            **model_inputs,\n            max_new_tokens=max_new_tokens,\n   \
    \         do_sample=True,\n            temperature=temperature,\n            top_k=top_k,\n\
    \            return_dict_in_generate=True,\n            output_scores=True,\n\
    \            pad_token_id=tokenizer.eos_token_id,\n            repetition_penalty=1.0,\n\
    \        )\n    output = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\
    \    return output\n```\n\nInitially, I also made some templates just like what\
    \ you suggested.\nHowever, the average length of text data in the dataset that\
    \ I used for training exceeds 5000 tokens (I used 4096 for context length).\n\
    That is the main reason that I used an extremely simple prompt template."
  created_at: 2023-12-31 06:23:16+00:00
  edited: false
  hidden: false
  id: 659108d4bec7ad264774df09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-31T07:38:14.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8934414386749268
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Thanks for clarifying :)</p>

          '
        raw: Thanks for clarifying :)
        updatedAt: '2023-12-31T07:38:14.874Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - BlackBeenie
      relatedEventId: 65911a6635c41262d6952452
    id: 65911a6635c41262d6952451
    type: comment
  author: KnutJaegersberg
  content: Thanks for clarifying :)
  created_at: 2023-12-31 07:38:14+00:00
  edited: false
  hidden: false
  id: 65911a6635c41262d6952451
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-31T07:38:14.000Z'
    data:
      status: closed
    id: 65911a6635c41262d6952452
    type: status-change
  author: KnutJaegersberg
  created_at: 2023-12-31 07:38:14+00:00
  id: 65911a6635c41262d6952452
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Technonia/mistral-7b-cot-neftune
repo_type: model
status: closed
target_branch: null
title: prompt template?
