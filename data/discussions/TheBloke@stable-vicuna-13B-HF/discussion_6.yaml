!!python/object:huggingface_hub.community.DiscussionWithDetails
author: techwoof
conflicting_files: null
created_at: 2023-05-03 11:19:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/882bc01113ea2a73f920d742458451c3.svg
      fullname: TechWoof
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: techwoof
      type: user
    createdAt: '2023-05-03T12:19:40.000Z'
    data:
      edited: false
      editors:
      - techwoof
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/882bc01113ea2a73f920d742458451c3.svg
          fullname: TechWoof
          isHf: false
          isPro: false
          name: techwoof
          type: user
        html: '<p>The model is very good, and i got very good results running it on
          Colab premium (and even local, 4bit quantized). But it is spitting out ChatGPT/OpenAI
          messages. The weirdest one was "too many requests in 1 hour. Try again later.",
          as well as some OpenAI messages about policies. The message changes after
          i changed my prompt to something completely different, so it has definitely
          been trained with those messages.</p>

          <p>I don''t have the prompt that resulted on this message, but i can confirm
          it happened multiple times while running it on Colab premium with ''text-generation-webui''.</p>

          <p>I suppose that is one of the drawbacks on training such models using
          ChatGPT :D</p>

          '
        raw: "The model is very good, and i got very good results running it on Colab\
          \ premium (and even local, 4bit quantized). But it is spitting out ChatGPT/OpenAI\
          \ messages. The weirdest one was \"too many requests in 1 hour. Try again\
          \ later.\", as well as some OpenAI messages about policies. The message\
          \ changes after i changed my prompt to something completely different, so\
          \ it has definitely been trained with those messages.\r\n\r\nI don't have\
          \ the prompt that resulted on this message, but i can confirm it happened\
          \ multiple times while running it on Colab premium with 'text-generation-webui'.\r\
          \n\r\nI suppose that is one of the drawbacks on training such models using\
          \ ChatGPT :D"
        updatedAt: '2023-05-03T12:19:40.656Z'
      numEdits: 0
      reactions: []
    id: 6452515c924a653471667d5d
    type: comment
  author: techwoof
  content: "The model is very good, and i got very good results running it on Colab\
    \ premium (and even local, 4bit quantized). But it is spitting out ChatGPT/OpenAI\
    \ messages. The weirdest one was \"too many requests in 1 hour. Try again later.\"\
    , as well as some OpenAI messages about policies. The message changes after i\
    \ changed my prompt to something completely different, so it has definitely been\
    \ trained with those messages.\r\n\r\nI don't have the prompt that resulted on\
    \ this message, but i can confirm it happened multiple times while running it\
    \ on Colab premium with 'text-generation-webui'.\r\n\r\nI suppose that is one\
    \ of the drawbacks on training such models using ChatGPT :D"
  created_at: 2023-05-03 11:19:40+00:00
  edited: false
  hidden: false
  id: 6452515c924a653471667d5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-03T13:35:24.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah I'm afraid so!  That's hilarious about the \"too many requests\
          \ message\", I've not seen that one before :)</p>\n<p>Whenever an organisation\
          \ or group releases a model like this, other people then try to create \"\
          unfiltered\" versions of it.  What they mean by \"unfiltered\" is actually\
          \ itself filtering - they edit the source dataset and try and remove all\
          \ the messages with stuff like \"I'm sorry, but as a Large Language Model\
          \ I can't tell you how to murder puppies\" or whatever. They aim to leave\
          \ all the usable content, but with none of the sentences where it said it\
          \ was an LLM or why it couldn't answer in full.</p>\n<p>There were several\
          \ such releases for Vicuna, and I expect before long someone will do the\
          \ same for StableVicuna. I know that at least one person is already working\
          \ on that for WizardLM for example (<span data-props=\"{&quot;user&quot;:&quot;ehartford&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ehartford\"\
          >@<span class=\"underline\">ehartford</span></a></span>\n\n\t</span></span>\
          \ )</p>\n"
        raw: 'Yeah I''m afraid so!  That''s hilarious about the "too many requests
          message", I''ve not seen that one before :)


          Whenever an organisation or group releases a model like this, other people
          then try to create "unfiltered" versions of it.  What they mean by "unfiltered"
          is actually itself filtering - they edit the source dataset and try and
          remove all the messages with stuff like "I''m sorry, but as a Large Language
          Model I can''t tell you how to murder puppies" or whatever. They aim to
          leave all the usable content, but with none of the sentences where it said
          it was an LLM or why it couldn''t answer in full.


          There were several such releases for Vicuna, and I expect before long someone
          will do the same for StableVicuna. I know that at least one person is already
          working on that for WizardLM for example (@ehartford )'
        updatedAt: '2023-05-03T13:36:18.303Z'
      numEdits: 2
      reactions: []
    id: 6452631ca0c0a664a23f5441
    type: comment
  author: TheBloke
  content: 'Yeah I''m afraid so!  That''s hilarious about the "too many requests message",
    I''ve not seen that one before :)


    Whenever an organisation or group releases a model like this, other people then
    try to create "unfiltered" versions of it.  What they mean by "unfiltered" is
    actually itself filtering - they edit the source dataset and try and remove all
    the messages with stuff like "I''m sorry, but as a Large Language Model I can''t
    tell you how to murder puppies" or whatever. They aim to leave all the usable
    content, but with none of the sentences where it said it was an LLM or why it
    couldn''t answer in full.


    There were several such releases for Vicuna, and I expect before long someone
    will do the same for StableVicuna. I know that at least one person is already
    working on that for WizardLM for example (@ehartford )'
  created_at: 2023-05-03 12:35:24+00:00
  edited: true
  hidden: false
  id: 6452631ca0c0a664a23f5441
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/stable-vicuna-13B-HF
repo_type: model
status: open
target_branch: null
title: '"Too many requests in 1 hour. Try again later."'
