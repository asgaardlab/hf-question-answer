!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hbfe
conflicting_files: null
created_at: 2023-05-23 02:27:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0184cf0b944c2a78d2608af437e60535.svg
      fullname: L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hbfe
      type: user
    createdAt: '2023-05-23T03:27:18.000Z'
    data:
      edited: true
      editors:
      - hbfe
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0184cf0b944c2a78d2608af437e60535.svg
          fullname: L
          isHf: false
          isPro: false
          name: hbfe
          type: user
        html: '<p>When I try to deploy it on the AWS sagemaker, deploy success, but
          predict failed, </p>

          <p>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint
          operation: Received client error (400) from primary with message "{<br>  "code":
          400,<br>  "type": "InternalServerException",<br>  "message": "\u0027llama\u0027"<br>}</p>

          <p>and I got this from the CloudWatch:</p>

          <p>com.amazonaws.ml.mms.wlm.workerlifecycle - keyerror: ''llama''</p>

          <p>Is it because the version of Transformers is not correct?</p>

          '
        raw: "When I try to deploy it on the AWS sagemaker, deploy success, but predict\
          \ failed, \n\nModelError: An error occurred (ModelError) when calling the\
          \ InvokeEndpoint operation: Received client error (400) from primary with\
          \ message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"\\u0027llama\\u0027\"\n}\n\nand I got this from the\
          \ CloudWatch:\n\ncom.amazonaws.ml.mms.wlm.workerlifecycle - keyerror: 'llama'\n\
          \nIs it because the version of Transformers is not correct?"
        updatedAt: '2023-05-23T03:39:31.586Z'
      numEdits: 1
      reactions: []
    id: 646c3296628e5b50b2e23924
    type: comment
  author: hbfe
  content: "When I try to deploy it on the AWS sagemaker, deploy success, but predict\
    \ failed, \n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint\
    \ operation: Received client error (400) from primary with message \"{\n  \"code\"\
    : 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027llama\\\
    u0027\"\n}\n\nand I got this from the CloudWatch:\n\ncom.amazonaws.ml.mms.wlm.workerlifecycle\
    \ - keyerror: 'llama'\n\nIs it because the version of Transformers is not correct?"
  created_at: 2023-05-23 02:27:18+00:00
  edited: true
  hidden: false
  id: 646c3296628e5b50b2e23924
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T23:12:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry I have no experience of SageMaker at all. All I can suggest
          is to google for examples of other people running Llama models on Sagemaker.</p>

          '
        raw: Sorry I have no experience of SageMaker at all. All I can suggest is
          to google for examples of other people running Llama models on Sagemaker.
        updatedAt: '2023-05-23T23:12:21.974Z'
      numEdits: 0
      reactions: []
    id: 646d48554a2db77443829bb7
    type: comment
  author: TheBloke
  content: Sorry I have no experience of SageMaker at all. All I can suggest is to
    google for examples of other people running Llama models on Sagemaker.
  created_at: 2023-05-23 22:12:21+00:00
  edited: false
  hidden: false
  id: 646d48554a2db77443829bb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X9e3J93IotXb7gbPXUSQO.png?w=200&h=200&f=face
      fullname: Pei Ran Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lipeiran
      type: user
    createdAt: '2023-06-08T05:10:35.000Z'
    data:
      edited: false
      editors:
      - lipeiran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.57938551902771
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X9e3J93IotXb7gbPXUSQO.png?w=200&h=200&f=face
          fullname: Pei Ran Li
          isHf: false
          isPro: false
          name: lipeiran
          type: user
        html: '<p>this example helped me with deploying  vicuna-13b-HF on sagemaker
          - <a rel="nofollow" href="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab10-open-llama/open-llama-7b/open_llama_7b.ipynb">https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab10-open-llama/open-llama-7b/open_llama_7b.ipynb</a></p>

          '
        raw: this example helped me with deploying  vicuna-13b-HF on sagemaker - https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab10-open-llama/open-llama-7b/open_llama_7b.ipynb
        updatedAt: '2023-06-08T05:10:35.249Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 648162cbbb25a636c9e93527
    type: comment
  author: lipeiran
  content: this example helped me with deploying  vicuna-13b-HF on sagemaker - https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab10-open-llama/open-llama-7b/open_llama_7b.ipynb
  created_at: 2023-06-08 04:10:35+00:00
  edited: false
  hidden: false
  id: 648162cbbb25a636c9e93527
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ead3b33d1512b42d00d23d99358dbda7.svg
      fullname: Eng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrick-ml
      type: user
    createdAt: '2023-09-21T13:07:21.000Z'
    data:
      edited: false
      editors:
      - patrick-ml
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7373453378677368
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ead3b33d1512b42d00d23d99358dbda7.svg
          fullname: Eng
          isHf: false
          isPro: false
          name: patrick-ml
          type: user
        html: "<p>Have you solved this issue <span data-props=\"{&quot;user&quot;:&quot;hbfe&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hbfe\"\
          >@<span class=\"underline\">hbfe</span></a></span>\n\n\t</span></span> ?\
          \ I get the same error when deploying \"meta-llama/Llama-2-7b-chat-hf\"\
          \ model using \"ml.c5.2xlarge\" instance and dlc image: \"huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04\"\
          \ - the one without GPU. The versions with GPU works fine, but I want to\
          \ test if it is possible to inference the model using only CPU instances</p>\n"
        raw: 'Have you solved this issue @hbfe ? I get the same error when deploying
          "meta-llama/Llama-2-7b-chat-hf" model using "ml.c5.2xlarge" instance and
          dlc image: "huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04"
          - the one without GPU. The versions with GPU works fine, but I want to test
          if it is possible to inference the model using only CPU instances'
        updatedAt: '2023-09-21T13:07:21.784Z'
      numEdits: 0
      reactions: []
    id: 650c40094d77a095223d02ea
    type: comment
  author: patrick-ml
  content: 'Have you solved this issue @hbfe ? I get the same error when deploying
    "meta-llama/Llama-2-7b-chat-hf" model using "ml.c5.2xlarge" instance and dlc image:
    "huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04"
    - the one without GPU. The versions with GPU works fine, but I want to test if
    it is possible to inference the model using only CPU instances'
  created_at: 2023-09-21 12:07:21+00:00
  edited: false
  hidden: false
  id: 650c40094d77a095223d02ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/stable-vicuna-13B-HF
repo_type: model
status: open
target_branch: null
title: 'keyerror: ''llama'''
