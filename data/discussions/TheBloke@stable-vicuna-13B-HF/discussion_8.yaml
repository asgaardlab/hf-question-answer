!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VSFletch3r
conflicting_files: null
created_at: 2023-05-08 04:20:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d96f22bffed44c7507bd18a2f12f848e.svg
      fullname: Grigoriou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VSFletch3r
      type: user
    createdAt: '2023-05-08T05:20:14.000Z'
    data:
      edited: false
      editors:
      - VSFletch3r
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d96f22bffed44c7507bd18a2f12f848e.svg
          fullname: Grigoriou
          isHf: false
          isPro: false
          name: VSFletch3r
          type: user
        html: '<p>Hey,</p>

          <p>Im on a 2021 macbook pro m1 max w/ 64gb. IOS 13.3.1<br>I could luanch
          the webui normally, and load the model.</p>

          <h2 id="but-whatever-i-type-in-the-chat-log-will-simply-blink-and-disappear">But
          whatever I type in, the chat log will simply blink and disappear.</h2>

          <hr>

          <p>Traceback (most recent call last):<br>  File "/Users/x/Desktop/oobabooga_macos/text-generation-webui/modules/callbacks.py",
          line 73, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "/Users/x/Desktop/oobabooga_macos/text-generation-webui/modules/text_generation.py",
          line 251, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py",
          line 2521, in sample<br>    model_inputs = self.prepare_inputs_for_generation(input_ids,
          **model_kwargs)<br>  File "/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py",
          line 736, in prepare_inputs_for_generation<br>    position_ids = attention_mask.long().cumsum(-1)
          - 1<br>RuntimeError: MPS does not support cumsum op with int64 input<br>Output
          generated in 0.19 seconds (0.00 tokens/s, 0 tokens, context 36, seed 639214276)</p>

          <hr>

          <p>I have tried to "pip3 install --pre torch torchvision torchaudio --index-url
          <a rel="nofollow" href="https://download.pytorch.org/whl/nightly/cpu&quot;">https://download.pytorch.org/whl/nightly/cpu"</a><br>It
          seems not working to fix the problem.</p>

          <p>Any idea who to fix this? I am very new to my macbook, might need help
          in details.</p>

          <p>Thanks in advance.</p>

          '
        raw: "Hey,\r\n\r\nIm on a 2021 macbook pro m1 max w/ 64gb. IOS 13.3.1\r\n\
          I could luanch the webui normally, and load the model.\r\n\r\nBut whatever\
          \ I type in, the chat log will simply blink and disappear.\r\n---------\r\
          \n\r\n---------\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/x/Desktop/oobabooga_macos/text-generation-webui/modules/callbacks.py\"\
          , line 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
          \n  File \"/Users/x/Desktop/oobabooga_macos/text-generation-webui/modules/text_generation.py\"\
          , line 251, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1485, in generate\r\n    return self.sample(\r\n  File \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2521, in sample\r\n    model_inputs = self.prepare_inputs_for_generation(input_ids,\
          \ **model_kwargs)\r\n  File \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 736, in prepare_inputs_for_generation\r\n    position_ids = attention_mask.long().cumsum(-1)\
          \ - 1\r\nRuntimeError: MPS does not support cumsum op with int64 input\r\
          \nOutput generated in 0.19 seconds (0.00 tokens/s, 0 tokens, context 36,\
          \ seed 639214276)\r\n\r\n---------\r\nI have tried to \"pip3 install --pre\
          \ torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\"\
          \r\nIt seems not working to fix the problem.\r\n\r\nAny idea who to fix\
          \ this? I am very new to my macbook, might need help in details.\r\n\r\n\
          Thanks in advance.\r\n\r\n"
        updatedAt: '2023-05-08T05:20:14.334Z'
      numEdits: 0
      reactions: []
    id: 6458868eaa426bae5e07eed3
    type: comment
  author: VSFletch3r
  content: "Hey,\r\n\r\nIm on a 2021 macbook pro m1 max w/ 64gb. IOS 13.3.1\r\nI could\
    \ luanch the webui normally, and load the model.\r\n\r\nBut whatever I type in,\
    \ the chat log will simply blink and disappear.\r\n---------\r\n\r\n---------\r\
    \n\r\nTraceback (most recent call last):\r\n  File \"/Users/x/Desktop/oobabooga_macos/text-generation-webui/modules/callbacks.py\"\
    , line 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
    \n  File \"/Users/x/Desktop/oobabooga_macos/text-generation-webui/modules/text_generation.py\"\
    , line 251, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
    \n  File \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1485, in generate\r\n    return self.sample(\r\n  File \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2521, in sample\r\n    model_inputs = self.prepare_inputs_for_generation(input_ids,\
    \ **model_kwargs)\r\n  File \"/Users/x/Desktop/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 736, in prepare_inputs_for_generation\r\n    position_ids = attention_mask.long().cumsum(-1)\
    \ - 1\r\nRuntimeError: MPS does not support cumsum op with int64 input\r\nOutput\
    \ generated in 0.19 seconds (0.00 tokens/s, 0 tokens, context 36, seed 639214276)\r\
    \n\r\n---------\r\nI have tried to \"pip3 install --pre torch torchvision torchaudio\
    \ --index-url https://download.pytorch.org/whl/nightly/cpu\"\r\nIt seems not working\
    \ to fix the problem.\r\n\r\nAny idea who to fix this? I am very new to my macbook,\
    \ might need help in details.\r\n\r\nThanks in advance.\r\n\r\n"
  created_at: 2023-05-08 04:20:14+00:00
  edited: false
  hidden: false
  id: 6458868eaa426bae5e07eed3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-08T07:30:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Here''s a thread describing the issue: <a rel="nofollow" href="https://github.com/pytorch/pytorch/issues/96610">https://github.com/pytorch/pytorch/issues/96610</a></p>

          <p> You need to make sure you''re running macOS 13.3.  If you already are
          then it might be because torch is being installed in a different version
          of Python:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/USbo6xF2czi6DRFxx5dxD.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/USbo6xF2czi6DRFxx5dxD.png"></a></p>

          '
        raw: "Here's a thread describing the issue: https://github.com/pytorch/pytorch/issues/96610\n\
          \n You need to make sure you're running macOS 13.3.  If you already are\
          \ then it might be because torch is being installed in a different version\
          \ of Python:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/USbo6xF2czi6DRFxx5dxD.png)"
        updatedAt: '2023-05-08T07:30:22.475Z'
      numEdits: 0
      reactions: []
    id: 6458a50ef92601affa28a06e
    type: comment
  author: TheBloke
  content: "Here's a thread describing the issue: https://github.com/pytorch/pytorch/issues/96610\n\
    \n You need to make sure you're running macOS 13.3.  If you already are then it\
    \ might be because torch is being installed in a different version of Python:\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/USbo6xF2czi6DRFxx5dxD.png)"
  created_at: 2023-05-08 06:30:22+00:00
  edited: false
  hidden: false
  id: 6458a50ef92601affa28a06e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0adb52b13a4f7a04ecb67e9c346e08c4.svg
      fullname: Felix Brand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cfmbrand
      type: user
    createdAt: '2023-09-10T19:16:28.000Z'
    data:
      edited: true
      editors:
      - cfmbrand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.852485716342926
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0adb52b13a4f7a04ecb67e9c346e08c4.svg
          fullname: Felix Brand
          isHf: false
          isPro: false
          name: cfmbrand
          type: user
        html: '<p>Hey, </p>

          <p>I''m getting an identical error message trying to run CodeLlama 7B on
          an M1 Pro - I''ve updated MacOS to 13.5.2, and am running python 3.10.9
          and pip for python 3.10. </p>

          <p>Not sure if this is relevant, but I am also getting the following message
          when I load the model using the WebUI (<a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a>):</p>

          <p>UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization
          are unavailable.<br>  warn("The installed version of bitsandbytes was compiled
          without GPU support. "<br>''NoneType'' object has no attribute ''cadam32bit_grad_fp32''</p>

          <p>Appreciate any thoughts on what is going wrong here. </p>

          '
        raw: "Hey, \n\nI'm getting an identical error message trying to run CodeLlama\
          \ 7B on an M1 Pro - I've updated MacOS to 13.5.2, and am running python\
          \ 3.10.9 and pip for python 3.10. \n\nNot sure if this is relevant, but\
          \ I am also getting the following message when I load the model using the\
          \ WebUI (https://github.com/oobabooga/text-generation-webui):\n\nUserWarning:\
          \ The installed version of bitsandbytes was compiled without GPU support.\
          \ 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n\
          \  warn(\"The installed version of bitsandbytes was compiled without GPU\
          \ support. \"\n'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n\
          \nAppreciate any thoughts on what is going wrong here. "
        updatedAt: '2023-09-10T19:17:28.959Z'
      numEdits: 2
      reactions: []
    id: 64fe160cc367f7b1ca243558
    type: comment
  author: cfmbrand
  content: "Hey, \n\nI'm getting an identical error message trying to run CodeLlama\
    \ 7B on an M1 Pro - I've updated MacOS to 13.5.2, and am running python 3.10.9\
    \ and pip for python 3.10. \n\nNot sure if this is relevant, but I am also getting\
    \ the following message when I load the model using the WebUI (https://github.com/oobabooga/text-generation-webui):\n\
    \nUserWarning: The installed version of bitsandbytes was compiled without GPU\
    \ support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n\
    \  warn(\"The installed version of bitsandbytes was compiled without GPU support.\
    \ \"\n'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n\nAppreciate\
    \ any thoughts on what is going wrong here. "
  created_at: 2023-09-10 18:16:28+00:00
  edited: true
  hidden: false
  id: 64fe160cc367f7b1ca243558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0adb52b13a4f7a04ecb67e9c346e08c4.svg
      fullname: Felix Brand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cfmbrand
      type: user
    createdAt: '2023-09-10T22:34:04.000Z'
    data:
      edited: true
      editors:
      - cfmbrand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9294031262397766
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0adb52b13a4f7a04ecb67e9c346e08c4.svg
          fullname: Felix Brand
          isHf: false
          isPro: false
          name: cfmbrand
          type: user
        html: '<p>This appears to have been resolved elsewhere, requiring you to install
          PyTorch nightlies- not stable release.</p>

          <p><a rel="nofollow" href="https://github.com/pytorch/pytorch/issues/96610#issuecomment-1597314364">https://github.com/pytorch/pytorch/issues/96610#issuecomment-1597314364</a></p>

          <p>But having implemented the change my inference time is still unusably
          slow at 0.02 tokens/sec. Anyone know why that might be? Thanks in advance.
          </p>

          '
        raw: "This appears to have been resolved elsewhere, requiring you to install\
          \ PyTorch nightlies- not stable release.\n\nhttps://github.com/pytorch/pytorch/issues/96610#issuecomment-1597314364\n\
          \nBut having implemented the change my inference time is still unusably\
          \ slow at 0.02 tokens/sec. Anyone know why that might be? Thanks in advance.\
          \ \n"
        updatedAt: '2023-09-11T19:29:55.229Z'
      numEdits: 1
      reactions: []
    id: 64fe445c67a8befb5c490215
    type: comment
  author: cfmbrand
  content: "This appears to have been resolved elsewhere, requiring you to install\
    \ PyTorch nightlies- not stable release.\n\nhttps://github.com/pytorch/pytorch/issues/96610#issuecomment-1597314364\n\
    \nBut having implemented the change my inference time is still unusably slow at\
    \ 0.02 tokens/sec. Anyone know why that might be? Thanks in advance. \n"
  created_at: 2023-09-10 21:34:04+00:00
  edited: true
  hidden: false
  id: 64fe445c67a8befb5c490215
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/stable-vicuna-13B-HF
repo_type: model
status: open
target_branch: null
title: No answer, chat log blink and disappear
