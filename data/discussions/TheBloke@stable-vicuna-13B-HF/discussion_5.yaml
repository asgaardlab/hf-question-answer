!!python/object:huggingface_hub.community.DiscussionWithDetails
author: papipsycho
conflicting_files: null
created_at: 2023-05-02 19:10:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59df7caeb40df25ea2a85120ec722d8e.svg
      fullname: julien romagnoli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: papipsycho
      type: user
    createdAt: '2023-05-02T20:10:07.000Z'
    data:
      edited: false
      editors:
      - papipsycho
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59df7caeb40df25ea2a85120ec722d8e.svg
          fullname: julien romagnoli
          isHf: false
          isPro: false
          name: papipsycho
          type: user
        html: '<p>Hello Guys, </p>

          <p>i were wanted to test your model on spaces, so i just ran it on space
          A10G with gradio and i left the default app.py</p>

          <p>i got the front of gradio but after 3 min still no anwser.</p>

          <p>i should take bigger spaces ? A100 ? i''m doing something wrong ? or
          i forget one step ?</p>

          <p>Best</p>

          '
        raw: "Hello Guys, \r\n\r\ni were wanted to test your model on spaces, so i\
          \ just ran it on space A10G with gradio and i left the default app.py\r\n\
          \r\ni got the front of gradio but after 3 min still no anwser.\r\n\r\ni\
          \ should take bigger spaces ? A100 ? i'm doing something wrong ? or i forget\
          \ one step ?\r\n\r\nBest"
        updatedAt: '2023-05-02T20:10:07.421Z'
      numEdits: 0
      reactions: []
    id: 64516e1fa31c8a4cbf74f2fa
    type: comment
  author: papipsycho
  content: "Hello Guys, \r\n\r\ni were wanted to test your model on spaces, so i just\
    \ ran it on space A10G with gradio and i left the default app.py\r\n\r\ni got\
    \ the front of gradio but after 3 min still no anwser.\r\n\r\ni should take bigger\
    \ spaces ? A100 ? i'm doing something wrong ? or i forget one step ?\r\n\r\nBest"
  created_at: 2023-05-02 19:10:07+00:00
  edited: false
  hidden: false
  id: 64516e1fa31c8a4cbf74f2fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-02T20:10:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What prompt did you use?</p>

          '
        raw: What prompt did you use?
        updatedAt: '2023-05-02T20:10:42.357Z'
      numEdits: 0
      reactions: []
    id: 64516e4241f3c769b914e5f9
    type: comment
  author: TheBloke
  content: What prompt did you use?
  created_at: 2023-05-02 19:10:42+00:00
  edited: false
  hidden: false
  id: 64516e4241f3c769b914e5f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59df7caeb40df25ea2a85120ec722d8e.svg
      fullname: julien romagnoli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: papipsycho
      type: user
    createdAt: '2023-05-02T20:31:12.000Z'
    data:
      edited: false
      editors:
      - papipsycho
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59df7caeb40df25ea2a85120ec722d8e.svg
          fullname: julien romagnoli
          isHf: false
          isPro: false
          name: papipsycho
          type: user
        html: "<p>Nothing i really left the default app.py which is not defined any\
          \ prompt.</p>\n<p>so i checked into this space <a href=\"https://huggingface.co/spaces/CarperAI/StableVicuna\"\
          >https://huggingface.co/spaces/CarperAI/StableVicuna</a> </p>\n<p>here there\
          \ is this :</p>\n<p>prompt_template = Template(\"\"\"\\</p>\n<h3 id=\"human-human\"\
          >Human: $human</h3>\n<h3 id=\"assistant-bot\">Assistant: $bot\\</h3>\n<p>\"\
          \"\")</p>\n<p>system_prompt = \"### Assistant: I am StableVicuna, a large\
          \ language model created by CarperAI. I am here to chat!\"<br>system_prompt_tokens\
          \ = tokenizer([f\"{system_prompt}\\n\\n\"], return_tensors=\"pt\")<br>max_sys_tokens\
          \ = system_prompt_tokens['input_ids'].size(-1)</p>\n<p>so i guess i should\
          \ change my app.py more or less like this :<br>import os<br>import gc<br>from\
          \ string import Template<br>from threading import Thread</p>\n<p>import\
          \ torch<br>import gradio as gr<br>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM, BatchEncoding, TextIteratorStreamer</p>\n<p>auth_token\
          \ = os.environ.get(\"HUGGINGFACE_TOKEN\")<br>tokenizer = AutoTokenizer.from_pretrained(<br>\
          \    \"TheBloke/stable-vicuna-13B-HF\",<br>    use_auth_token=auth_token\
          \ if auth_token else True,<br>)<br>model = AutoModelForCausalLM.from_pretrained(<br>\
          \    \"TheBloke/stable-vicuna-13B-HF\",<br>    torch_dtype=torch.float16,<br>\
          \    low_cpu_mem_usage=True,<br>    device_map=\"auto\",<br>    use_auth_token=auth_token\
          \ if auth_token else True,<br>)<br>model.eval()</p>\n<p>max_context_length\
          \ = model.config.max_position_embeddings<br>max_new_tokens = 768</p>\n<p>prompt_template\
          \ = Template(\"\"\"\\</p>\n<h3 id=\"human-human-1\">Human: $human</h3>\n\
          <h3 id=\"assistant-bot-1\">Assistant: $bot\\</h3>\n<p>\"\"\")</p>\n<p>system_prompt\
          \ = \"### Assistant: I am StableVicuna, a large language model created by\
          \ CarperAI. I am here to chat!\"<br>system_prompt_tokens = tokenizer([f\"\
          {system_prompt}\\n\\n\"], return_tensors=\"pt\")<br>max_sys_tokens = system_prompt_tokens['input_ids'].size(-1)</p>\n\
          <p>def bot(history):<br>    history = history or []</p>\n<pre><code># Inject\
          \ prompt formatting into the history\nprompt_history = []\nfor human, bot\
          \ in history:\n    if bot is not None:\n        bot = bot.replace(\"&lt;br&gt;\"\
          , \"\\n\")\n        bot = bot.rstrip()\n    prompt_history.append(\n   \
          \     prompt_template.substitute(\n            human=human, bot=bot if bot\
          \ is not None else \"\")\n    )\n\nmsg_tokens = tokenizer(\n    \"\\n\\\
          n\".join(prompt_history).strip(),\n    return_tensors=\"pt\",\n    add_special_tokens=False\
          \  # Use &lt;BOS&gt; from the system prompt\n)\n\n# Take only the most recent\
          \ context up to the max context length and prepend the\n# system prompt\
          \ with the messages\nmax_tokens = -max_context_length + max_new_tokens +\
          \ max_sys_tokens\ninputs = BatchEncoding({\n    k: torch.concat([system_prompt_tokens[k],\
          \ msg_tokens[k][:, max_tokens:]], dim=-1)\n    for k in msg_tokens\n}).to('cuda')\n\
          # Remove `token_type_ids` b/c it's not yet supported for LLaMA `transformers`\
          \ models\nif inputs.get(\"token_type_ids\", None) is not None:\n    inputs.pop(\"\
          token_type_ids\")\n\nstreamer = TextIteratorStreamer(\n    tokenizer, timeout=10.0,\
          \ skip_prompt=True, skip_special_tokens=True\n)\ngenerate_kwargs = dict(\n\
          \    inputs,\n    streamer=streamer,\n    max_new_tokens=max_new_tokens,\n\
          \    do_sample=True,\n    top_p=1.0,\n    temperature=1.0,\n)\nthread =\
          \ Thread(target=model.generate, kwargs=generate_kwargs)\nthread.start()\n\
          \npartial_text = \"\"\nfor new_text in streamer:\n    # Process out the\
          \ prompt separator\n    new_text = new_text.replace(\"&lt;br&gt;\", \"\\\
          n\")\n    if \"###\" in new_text:\n        new_text = new_text.split(\"\
          ###\")[0]\n        partial_text += new_text.strip()\n        history[-1][1]\
          \ = partial_text\n        break\n    else:\n        # Filter empty trailing\
          \ new lines\n        if new_text == \"\\n\":\n            new_text = new_text.strip()\n\
          \        partial_text += new_text\n        history[-1][1] = partial_text\n\
          \    yield history\nreturn partial_text\n</code></pre>\n<p>def user(user_message,\
          \ history):<br>    return \"\", history + [[user_message, None]]</p>\n<p>with\
          \ gr.Blocks() as demo:<br>    gr.Markdown(\"# StableVicuna by CarperAI\"\
          )<br>    gr.HTML(\"<a href=\"https://huggingface.co/CarperAI/stable-vicuna-13b-delta\"\
          ><code>CarperAI/stable-vicuna-13b-delta</code></a><code>\")<br>    gr.HTML('''</code></p><center><code><a\
          \ href=\"https://huggingface.co/spaces/CarperAI/StableVicuna?duplicate=true\"\
          ><img alt=\"Duplicate Space\" src=\"https://bit.ly/3gLdBN6\"></a>Duplicate\
          \ the Space to skip the queue and run in a private space</code></center><code>''')<p></p>\n\
          <pre><code>chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=500)\n\
          state = gr.State([])\nwith gr.Row():\n    with gr.Column():\n        msg\
          \ = gr.Textbox(\n            label=\"Send a message\",\n            placeholder=\"\
          Send a message\",\n            show_label=False\n        ).style(container=False)\n\
          \    with gr.Column():\n        with gr.Row():\n            submit = gr.Button(\"\
          Send\")\n            stop = gr.Button(\"Stop\")\n            clear = gr.Button(\"\
          Clear History\")\n\nsubmit_event = msg.submit(user, inputs=[msg, chatbot],\
          \ outputs=[msg, chatbot], queue=False).then(\n    fn=bot, inputs=[chatbot],\
          \ outputs=[chatbot], queue=True)\nsubmit_click_event = submit.click(user,\
          \ inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False).then(\n  \
          \  fn=bot, inputs=[chatbot], outputs=[chatbot], queue=True)\n\nstop.click(fn=None,\
          \ inputs=None, outputs=None, cancels=[submit_event, submit_click_event],\
          \ queue=False)\nclear.click(lambda: None, None, [chatbot], queue=True)\n\
          </code></pre>\n<p>demo.queue(max_size=32)<br>demo.launch()</p>\n</code>"
        raw: "Nothing i really left the default app.py which is not defined any prompt.\n\
          \nso i checked into this space https://huggingface.co/spaces/CarperAI/StableVicuna\
          \ \n\nhere there is this :\n\nprompt_template = Template(\"\"\"\\\n### Human:\
          \ $human\n### Assistant: $bot\\\n\"\"\")\n\n\nsystem_prompt = \"### Assistant:\
          \ I am StableVicuna, a large language model created by CarperAI. I am here\
          \ to chat!\"\nsystem_prompt_tokens = tokenizer([f\"{system_prompt}\\n\\\
          n\"], return_tensors=\"pt\")\nmax_sys_tokens = system_prompt_tokens['input_ids'].size(-1)\n\
          \n\nso i guess i should change my app.py more or less like this : \nimport\
          \ os\nimport gc\nfrom string import Template\nfrom threading import Thread\n\
          \nimport torch\nimport gradio as gr\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM, BatchEncoding, TextIteratorStreamer\n\n\nauth_token\
          \ = os.environ.get(\"HUGGINGFACE_TOKEN\")\ntokenizer = AutoTokenizer.from_pretrained(\n\
          \    \"TheBloke/stable-vicuna-13B-HF\",\n    use_auth_token=auth_token if\
          \ auth_token else True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"TheBloke/stable-vicuna-13B-HF\",\n    torch_dtype=torch.float16,\n\
          \    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    use_auth_token=auth_token\
          \ if auth_token else True,\n)\nmodel.eval()\n\n\nmax_context_length = model.config.max_position_embeddings\n\
          max_new_tokens = 768\n\n\nprompt_template = Template(\"\"\"\\\n### Human:\
          \ $human\n### Assistant: $bot\\\n\"\"\")\n\n\nsystem_prompt = \"### Assistant:\
          \ I am StableVicuna, a large language model created by CarperAI. I am here\
          \ to chat!\"\nsystem_prompt_tokens = tokenizer([f\"{system_prompt}\\n\\\
          n\"], return_tensors=\"pt\")\nmax_sys_tokens = system_prompt_tokens['input_ids'].size(-1)\n\
          \n\ndef bot(history):\n    history = history or []\n\n    # Inject prompt\
          \ formatting into the history\n    prompt_history = []\n    for human, bot\
          \ in history:\n        if bot is not None:\n            bot = bot.replace(\"\
          <br>\", \"\\n\")\n            bot = bot.rstrip()\n        prompt_history.append(\n\
          \            prompt_template.substitute(\n                human=human, bot=bot\
          \ if bot is not None else \"\")\n        )\n\n    msg_tokens = tokenizer(\n\
          \        \"\\n\\n\".join(prompt_history).strip(),\n        return_tensors=\"\
          pt\",\n        add_special_tokens=False  # Use <BOS> from the system prompt\n\
          \    )\n\n    # Take only the most recent context up to the max context\
          \ length and prepend the\n    # system prompt with the messages\n    max_tokens\
          \ = -max_context_length + max_new_tokens + max_sys_tokens\n    inputs =\
          \ BatchEncoding({\n        k: torch.concat([system_prompt_tokens[k], msg_tokens[k][:,\
          \ max_tokens:]], dim=-1)\n        for k in msg_tokens\n    }).to('cuda')\n\
          \    # Remove `token_type_ids` b/c it's not yet supported for LLaMA `transformers`\
          \ models\n    if inputs.get(\"token_type_ids\", None) is not None:\n   \
          \     inputs.pop(\"token_type_ids\")\n\n    streamer = TextIteratorStreamer(\n\
          \        tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True\n\
          \    )\n    generate_kwargs = dict(\n        inputs,\n        streamer=streamer,\n\
          \        max_new_tokens=max_new_tokens,\n        do_sample=True,\n     \
          \   top_p=1.0,\n        temperature=1.0,\n    )\n    thread = Thread(target=model.generate,\
          \ kwargs=generate_kwargs)\n    thread.start()\n\n    partial_text = \"\"\
          \n    for new_text in streamer:\n        # Process out the prompt separator\n\
          \        new_text = new_text.replace(\"<br>\", \"\\n\")\n        if \"###\"\
          \ in new_text:\n            new_text = new_text.split(\"###\")[0]\n    \
          \        partial_text += new_text.strip()\n            history[-1][1] =\
          \ partial_text\n            break\n        else:\n            # Filter empty\
          \ trailing new lines\n            if new_text == \"\\n\":\n            \
          \    new_text = new_text.strip()\n            partial_text += new_text\n\
          \            history[-1][1] = partial_text\n        yield history\n    return\
          \ partial_text\n\n\ndef user(user_message, history):\n    return \"\", history\
          \ + [[user_message, None]]\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"\
          # StableVicuna by CarperAI\")\n    gr.HTML(\"<a href='https://huggingface.co/CarperAI/stable-vicuna-13b-delta'><code>CarperAI/stable-vicuna-13b-delta</a>\"\
          )\n    gr.HTML('''<center><a href=\"https://huggingface.co/spaces/CarperAI/StableVicuna?duplicate=true\"\
          ><img src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>Duplicate\
          \ the Space to skip the queue and run in a private space</center>''')\n\n\
          \    chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=500)\n \
          \   state = gr.State([])\n    with gr.Row():\n        with gr.Column():\n\
          \            msg = gr.Textbox(\n                label=\"Send a message\"\
          ,\n                placeholder=\"Send a message\",\n                show_label=False\n\
          \            ).style(container=False)\n        with gr.Column():\n     \
          \       with gr.Row():\n                submit = gr.Button(\"Send\")\n \
          \               stop = gr.Button(\"Stop\")\n                clear = gr.Button(\"\
          Clear History\")\n\n    submit_event = msg.submit(user, inputs=[msg, chatbot],\
          \ outputs=[msg, chatbot], queue=False).then(\n        fn=bot, inputs=[chatbot],\
          \ outputs=[chatbot], queue=True)\n    submit_click_event = submit.click(user,\
          \ inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False).then(\n  \
          \      fn=bot, inputs=[chatbot], outputs=[chatbot], queue=True)\n\n    stop.click(fn=None,\
          \ inputs=None, outputs=None, cancels=[submit_event, submit_click_event],\
          \ queue=False)\n    clear.click(lambda: None, None, [chatbot], queue=True)\n\
          \ndemo.queue(max_size=32)\ndemo.launch()"
        updatedAt: '2023-05-02T20:31:12.346Z'
      numEdits: 0
      reactions: []
    id: 6451731041f3c769b9158b64
    type: comment
  author: papipsycho
  content: "Nothing i really left the default app.py which is not defined any prompt.\n\
    \nso i checked into this space https://huggingface.co/spaces/CarperAI/StableVicuna\
    \ \n\nhere there is this :\n\nprompt_template = Template(\"\"\"\\\n### Human:\
    \ $human\n### Assistant: $bot\\\n\"\"\")\n\n\nsystem_prompt = \"### Assistant:\
    \ I am StableVicuna, a large language model created by CarperAI. I am here to\
    \ chat!\"\nsystem_prompt_tokens = tokenizer([f\"{system_prompt}\\n\\n\"], return_tensors=\"\
    pt\")\nmax_sys_tokens = system_prompt_tokens['input_ids'].size(-1)\n\n\nso i guess\
    \ i should change my app.py more or less like this : \nimport os\nimport gc\n\
    from string import Template\nfrom threading import Thread\n\nimport torch\nimport\
    \ gradio as gr\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
    \ BatchEncoding, TextIteratorStreamer\n\n\nauth_token = os.environ.get(\"HUGGINGFACE_TOKEN\"\
    )\ntokenizer = AutoTokenizer.from_pretrained(\n    \"TheBloke/stable-vicuna-13B-HF\"\
    ,\n    use_auth_token=auth_token if auth_token else True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"TheBloke/stable-vicuna-13B-HF\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n\
    \    device_map=\"auto\",\n    use_auth_token=auth_token if auth_token else True,\n\
    )\nmodel.eval()\n\n\nmax_context_length = model.config.max_position_embeddings\n\
    max_new_tokens = 768\n\n\nprompt_template = Template(\"\"\"\\\n### Human: $human\n\
    ### Assistant: $bot\\\n\"\"\")\n\n\nsystem_prompt = \"### Assistant: I am StableVicuna,\
    \ a large language model created by CarperAI. I am here to chat!\"\nsystem_prompt_tokens\
    \ = tokenizer([f\"{system_prompt}\\n\\n\"], return_tensors=\"pt\")\nmax_sys_tokens\
    \ = system_prompt_tokens['input_ids'].size(-1)\n\n\ndef bot(history):\n    history\
    \ = history or []\n\n    # Inject prompt formatting into the history\n    prompt_history\
    \ = []\n    for human, bot in history:\n        if bot is not None:\n        \
    \    bot = bot.replace(\"<br>\", \"\\n\")\n            bot = bot.rstrip()\n  \
    \      prompt_history.append(\n            prompt_template.substitute(\n     \
    \           human=human, bot=bot if bot is not None else \"\")\n        )\n\n\
    \    msg_tokens = tokenizer(\n        \"\\n\\n\".join(prompt_history).strip(),\n\
    \        return_tensors=\"pt\",\n        add_special_tokens=False  # Use <BOS>\
    \ from the system prompt\n    )\n\n    # Take only the most recent context up\
    \ to the max context length and prepend the\n    # system prompt with the messages\n\
    \    max_tokens = -max_context_length + max_new_tokens + max_sys_tokens\n    inputs\
    \ = BatchEncoding({\n        k: torch.concat([system_prompt_tokens[k], msg_tokens[k][:,\
    \ max_tokens:]], dim=-1)\n        for k in msg_tokens\n    }).to('cuda')\n   \
    \ # Remove `token_type_ids` b/c it's not yet supported for LLaMA `transformers`\
    \ models\n    if inputs.get(\"token_type_ids\", None) is not None:\n        inputs.pop(\"\
    token_type_ids\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer,\
    \ timeout=10.0, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs\
    \ = dict(\n        inputs,\n        streamer=streamer,\n        max_new_tokens=max_new_tokens,\n\
    \        do_sample=True,\n        top_p=1.0,\n        temperature=1.0,\n    )\n\
    \    thread = Thread(target=model.generate, kwargs=generate_kwargs)\n    thread.start()\n\
    \n    partial_text = \"\"\n    for new_text in streamer:\n        # Process out\
    \ the prompt separator\n        new_text = new_text.replace(\"<br>\", \"\\n\"\
    )\n        if \"###\" in new_text:\n            new_text = new_text.split(\"###\"\
    )[0]\n            partial_text += new_text.strip()\n            history[-1][1]\
    \ = partial_text\n            break\n        else:\n            # Filter empty\
    \ trailing new lines\n            if new_text == \"\\n\":\n                new_text\
    \ = new_text.strip()\n            partial_text += new_text\n            history[-1][1]\
    \ = partial_text\n        yield history\n    return partial_text\n\n\ndef user(user_message,\
    \ history):\n    return \"\", history + [[user_message, None]]\n\n\nwith gr.Blocks()\
    \ as demo:\n    gr.Markdown(\"# StableVicuna by CarperAI\")\n    gr.HTML(\"<a\
    \ href='https://huggingface.co/CarperAI/stable-vicuna-13b-delta'><code>CarperAI/stable-vicuna-13b-delta</a>\"\
    )\n    gr.HTML('''<center><a href=\"https://huggingface.co/spaces/CarperAI/StableVicuna?duplicate=true\"\
    ><img src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>Duplicate the\
    \ Space to skip the queue and run in a private space</center>''')\n\n    chatbot\
    \ = gr.Chatbot([], elem_id=\"chatbot\").style(height=500)\n    state = gr.State([])\n\
    \    with gr.Row():\n        with gr.Column():\n            msg = gr.Textbox(\n\
    \                label=\"Send a message\",\n                placeholder=\"Send\
    \ a message\",\n                show_label=False\n            ).style(container=False)\n\
    \        with gr.Column():\n            with gr.Row():\n                submit\
    \ = gr.Button(\"Send\")\n                stop = gr.Button(\"Stop\")\n        \
    \        clear = gr.Button(\"Clear History\")\n\n    submit_event = msg.submit(user,\
    \ inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False).then(\n        fn=bot,\
    \ inputs=[chatbot], outputs=[chatbot], queue=True)\n    submit_click_event = submit.click(user,\
    \ inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False).then(\n        fn=bot,\
    \ inputs=[chatbot], outputs=[chatbot], queue=True)\n\n    stop.click(fn=None,\
    \ inputs=None, outputs=None, cancels=[submit_event, submit_click_event], queue=False)\n\
    \    clear.click(lambda: None, None, [chatbot], queue=True)\n\ndemo.queue(max_size=32)\n\
    demo.launch()"
  created_at: 2023-05-02 19:31:12+00:00
  edited: false
  hidden: false
  id: 6451731041f3c769b9158b64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-02T20:32:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes you have to use this prompt template:</p>

          <pre><code>### Human: user''s prompt goes here

          ### Assistant:

          </code></pre>

          <p>Else it will return nothing</p>

          '
        raw: 'Yes you have to use this prompt template:

          ```

          ### Human: user''s prompt goes here

          ### Assistant:

          ```


          Else it will return nothing'
        updatedAt: '2023-05-02T20:32:53.865Z'
      numEdits: 0
      reactions: []
    id: 64517375b3f75261a7dd0ab7
    type: comment
  author: TheBloke
  content: 'Yes you have to use this prompt template:

    ```

    ### Human: user''s prompt goes here

    ### Assistant:

    ```


    Else it will return nothing'
  created_at: 2023-05-02 19:32:53+00:00
  edited: false
  hidden: false
  id: 64517375b3f75261a7dd0ab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bc026911785c84960327b9bba2abc693.svg
      fullname: Varun Hariprasad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alchemy5
      type: user
    createdAt: '2023-06-12T16:53:56.000Z'
    data:
      edited: false
      editors:
      - Alchemy5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9721524119377136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bc026911785c84960327b9bba2abc693.svg
          fullname: Varun Hariprasad
          isHf: false
          isPro: false
          name: Alchemy5
          type: user
        html: '<p>Even after using this prompt template when fine-tuning the model
          the fine-tuned model returns nothing. However, something I noticed that
          was interesting was when I fine-tuned only for one epoch then I was getting
          outputs from the model, but after fine-tuning for &gt;3 epochs, I get nothing
          from the fine-tuned model?</p>

          '
        raw: Even after using this prompt template when fine-tuning the model the
          fine-tuned model returns nothing. However, something I noticed that was
          interesting was when I fine-tuned only for one epoch then I was getting
          outputs from the model, but after fine-tuning for >3 epochs, I get nothing
          from the fine-tuned model?
        updatedAt: '2023-06-12T16:53:56.603Z'
      numEdits: 0
      reactions: []
    id: 64874da4e9bc28eb17e8aee0
    type: comment
  author: Alchemy5
  content: Even after using this prompt template when fine-tuning the model the fine-tuned
    model returns nothing. However, something I noticed that was interesting was when
    I fine-tuned only for one epoch then I was getting outputs from the model, but
    after fine-tuning for >3 epochs, I get nothing from the fine-tuned model?
  created_at: 2023-06-12 15:53:56+00:00
  edited: false
  hidden: false
  id: 64874da4e9bc28eb17e8aee0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/stable-vicuna-13B-HF
repo_type: model
status: open
target_branch: null
title: Never get any anwser
