!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mbecuwe
conflicting_files: null
created_at: 2023-05-24 14:26:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11f721bc0ce48626d95a062b86364a66.svg
      fullname: martin becuwe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mbecuwe
      type: user
    createdAt: '2023-05-24T15:26:04.000Z'
    data:
      edited: false
      editors:
      - mbecuwe
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11f721bc0ce48626d95a062b86364a66.svg
          fullname: martin becuwe
          isHf: false
          isPro: false
          name: mbecuwe
          type: user
        html: "<p>I am trying to finetune a Vicuna model using text generation webui.<br>I\
          \ followed these steps for install as shown in the documentation: </p>\n\
          <pre><code>#&nbsp;Install miniconda\ncurl -sL \"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\"\
          \ &gt; \"Miniconda3.sh\"\nbash Miniconda3.sh\n\n#&nbsp;Create conda env\n\
          conda create -n textgen python=3.10.9\nconda activate textgen\n\n#&nbsp;Install\
          \ torch\npip3 install torch torchvision torchaudio\n\n#&nbsp;Install text\
          \ generation webui\ngit clone https://github.com/oobabooga/text-generation-webui\n\
          cd text-generation-webui\npip install -r requirements.txt\n\n#&nbsp;install\
          \ nvcc\nconda install -c conda-forge cudatoolkit-dev\n\n#&nbsp;Install GPTQ\
          \ for LLaMa\nsudo apt install build-essential\nmkdir repositories\ncd repositories\n\
          git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\ncd GPTQ-for-LLaMa\n\
          python setup_cuda.py install\n\n#&nbsp;Install monkey patch\ncd ..\ngit\
          \ clone https://github.com/johnsmith0031/alpaca_lora_4bit\npip install git+https://github.com/sterlind/GPTQ-for-LLaMa.git@eaa9955\
          \ #&nbsp;Wont work if I dont revert to this specific commit \n\n# Download\
          \ model\ncd ..\npython download-model.py TheBloke/stable-vicuna-13B-GPTQ\n\
          \n#&nbsp;Run server with monkey patch\npython server.py --model TheBloke_stable-vicuna-13B-GPTQ\
          \ --wbits 4 --groupsize 128 --model_type Llama --share --api --listen --auto-devices\
          \ --monkey-patch --no-stream\n</code></pre>\n<p>When trying to generate\
          \ from prompts in the interface, I get the error: </p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"/home/jupyter/text-generation-webui/modules/callbacks.py\"\
          , line 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
          \  File \"/home/jupyter/text-generation-webui/modules/text_generation.py\"\
          , line 277, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/amp_wrapper.py\"\
          , line 18, in autocast_generate\n    return self.model.non_autocast_generate(*args,\
          \ **kwargs)\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\n    return self.sample(\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\n    outputs = self(\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 688, in forward\n    outputs = self.model(\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 578, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 293, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 197, in forward\n    query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/autograd_4bit.py\"\
          , line 133, in forward\n    out = matmul4bit_with_backend(x, self.qweight,\
          \ self.scales,\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/autograd_4bit.py\"\
          , line 89, in matmul4bit_with_backend\n    return mm4b.matmul4bit(x, qweight,\
          \ scales, qzeros, g_idx)\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/matmul_utils_4bit.py\"\
          , line 131, in matmul4bit\n    output = _matmul4bit_v2(x, qweight, scales,\
          \ zeros, g_idx)\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/matmul_utils_4bit.py\"\
          , line 70, in _matmul4bit_v2\n    quant_cuda.vecquant4matmul_faster(x, qweight,\
          \ y, scales, zeros, g_idx, x.shape[-1] // 2)\nRuntimeError: expected scalar\
          \ type Float but found Half\n</code></pre>\n<p>Text generation will work\
          \ without the monkey patch but then I cannot finetune the model on my dataset.<br>All\
          \ my tests are using GPU Nvidia P100.<br>Would be a great help if you could\
          \ help me fix it ! </p>\n"
        raw: "I am trying to finetune a Vicuna model using text generation webui.\
          \ \r\nI followed these steps for install as shown in the documentation:\
          \ \r\n``` \r\n#\_Install miniconda\r\ncurl -sL \"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\"\
          \ > \"Miniconda3.sh\"\r\nbash Miniconda3.sh\r\n\r\n#\_Create conda env\r\
          \nconda create -n textgen python=3.10.9\r\nconda activate textgen\r\n\r\n\
          #\_Install torch\r\npip3 install torch torchvision torchaudio\r\n\r\n#\_\
          Install text generation webui\r\ngit clone https://github.com/oobabooga/text-generation-webui\r\
          \ncd text-generation-webui\r\npip install -r requirements.txt\r\n\r\n#\_\
          install nvcc\r\nconda install -c conda-forge cudatoolkit-dev\r\n\r\n#\_\
          Install GPTQ for LLaMa\r\nsudo apt install build-essential\r\nmkdir repositories\r\
          \ncd repositories\r\ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
          \ -b cuda\r\ncd GPTQ-for-LLaMa\r\npython setup_cuda.py install\r\n\r\n#\_\
          Install monkey patch\r\ncd ..\r\ngit clone https://github.com/johnsmith0031/alpaca_lora_4bit\r\
          \npip install git+https://github.com/sterlind/GPTQ-for-LLaMa.git@eaa9955\
          \ #\_Wont work if I dont revert to this specific commit \r\n\r\n# Download\
          \ model\r\ncd ..\r\npython download-model.py TheBloke/stable-vicuna-13B-GPTQ\r\
          \n\r\n#\_Run server with monkey patch\r\npython server.py --model TheBloke_stable-vicuna-13B-GPTQ\
          \ --wbits 4 --groupsize 128 --model_type Llama --share --api --listen --auto-devices\
          \ --monkey-patch --no-stream\r\n``` \r\nWhen trying to generate from prompts\
          \ in the interface, I get the error: \r\n``` \r\nTraceback (most recent\
          \ call last):\r\n  File \"/home/jupyter/text-generation-webui/modules/callbacks.py\"\
          , line 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
          \n  File \"/home/jupyter/text-generation-webui/modules/text_generation.py\"\
          , line 277, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/amp_wrapper.py\"\
          , line 18, in autocast_generate\r\n    return self.model.non_autocast_generate(*args,\
          \ **kwargs)\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\r\n    return self.sample(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\r\n    outputs = self(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 688, in forward\r\n    outputs = self.model(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 578, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"\
          /opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 293, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 197, in forward\r\n    query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/autograd_4bit.py\"\
          , line 133, in forward\r\n    out = matmul4bit_with_backend(x, self.qweight,\
          \ self.scales,\r\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/autograd_4bit.py\"\
          , line 89, in matmul4bit_with_backend\r\n    return mm4b.matmul4bit(x, qweight,\
          \ scales, qzeros, g_idx)\r\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/matmul_utils_4bit.py\"\
          , line 131, in matmul4bit\r\n    output = _matmul4bit_v2(x, qweight, scales,\
          \ zeros, g_idx)\r\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/matmul_utils_4bit.py\"\
          , line 70, in _matmul4bit_v2\r\n    quant_cuda.vecquant4matmul_faster(x,\
          \ qweight, y, scales, zeros, g_idx, x.shape[-1] // 2)\r\nRuntimeError: expected\
          \ scalar type Float but found Half\r\n``` \r\n\r\n\r\nText generation will\
          \ work without the monkey patch but then I cannot finetune the model on\
          \ my dataset. \r\nAll my tests are using GPU Nvidia P100. \r\nWould be a\
          \ great help if you could help me fix it ! "
        updatedAt: '2023-05-24T15:26:04.419Z'
      numEdits: 0
      reactions: []
    id: 646e2c8c212146121303a5f6
    type: comment
  author: mbecuwe
  content: "I am trying to finetune a Vicuna model using text generation webui. \r\
    \nI followed these steps for install as shown in the documentation: \r\n``` \r\
    \n#\_Install miniconda\r\ncurl -sL \"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\"\
    \ > \"Miniconda3.sh\"\r\nbash Miniconda3.sh\r\n\r\n#\_Create conda env\r\nconda\
    \ create -n textgen python=3.10.9\r\nconda activate textgen\r\n\r\n#\_Install\
    \ torch\r\npip3 install torch torchvision torchaudio\r\n\r\n#\_Install text generation\
    \ webui\r\ngit clone https://github.com/oobabooga/text-generation-webui\r\ncd\
    \ text-generation-webui\r\npip install -r requirements.txt\r\n\r\n#\_install nvcc\r\
    \nconda install -c conda-forge cudatoolkit-dev\r\n\r\n#\_Install GPTQ for LLaMa\r\
    \nsudo apt install build-essential\r\nmkdir repositories\r\ncd repositories\r\n\
    git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\r\ncd GPTQ-for-LLaMa\r\
    \npython setup_cuda.py install\r\n\r\n#\_Install monkey patch\r\ncd ..\r\ngit\
    \ clone https://github.com/johnsmith0031/alpaca_lora_4bit\r\npip install git+https://github.com/sterlind/GPTQ-for-LLaMa.git@eaa9955\
    \ #\_Wont work if I dont revert to this specific commit \r\n\r\n# Download model\r\
    \ncd ..\r\npython download-model.py TheBloke/stable-vicuna-13B-GPTQ\r\n\r\n#\_\
    Run server with monkey patch\r\npython server.py --model TheBloke_stable-vicuna-13B-GPTQ\
    \ --wbits 4 --groupsize 128 --model_type Llama --share --api --listen --auto-devices\
    \ --monkey-patch --no-stream\r\n``` \r\nWhen trying to generate from prompts in\
    \ the interface, I get the error: \r\n``` \r\nTraceback (most recent call last):\r\
    \n  File \"/home/jupyter/text-generation-webui/modules/callbacks.py\", line 73,\
    \ in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\n  File\
    \ \"/home/jupyter/text-generation-webui/modules/text_generation.py\", line 277,\
    \ in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\n  File \"\
    /home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/amp_wrapper.py\"\
    , line 18, in autocast_generate\r\n    return self.model.non_autocast_generate(*args,\
    \ **kwargs)\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1565, in generate\r\n    return self.sample(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2612, in sample\r\n    outputs = self(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 688, in forward\r\n    outputs = self.model(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 578, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 293, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 197, in forward\r\n    query_states = self.q_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n  File \"/opt/conda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/autograd_4bit.py\"\
    , line 133, in forward\r\n    out = matmul4bit_with_backend(x, self.qweight, self.scales,\r\
    \n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/autograd_4bit.py\"\
    , line 89, in matmul4bit_with_backend\r\n    return mm4b.matmul4bit(x, qweight,\
    \ scales, qzeros, g_idx)\r\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/matmul_utils_4bit.py\"\
    , line 131, in matmul4bit\r\n    output = _matmul4bit_v2(x, qweight, scales, zeros,\
    \ g_idx)\r\n  File \"/home/jupyter/text-generation-webui/repositories/alpaca_lora_4bit/matmul_utils_4bit.py\"\
    , line 70, in _matmul4bit_v2\r\n    quant_cuda.vecquant4matmul_faster(x, qweight,\
    \ y, scales, zeros, g_idx, x.shape[-1] // 2)\r\nRuntimeError: expected scalar\
    \ type Float but found Half\r\n``` \r\n\r\n\r\nText generation will work without\
    \ the monkey patch but then I cannot finetune the model on my dataset. \r\nAll\
    \ my tests are using GPU Nvidia P100. \r\nWould be a great help if you could help\
    \ me fix it ! "
  created_at: 2023-05-24 14:26:04+00:00
  edited: false
  hidden: false
  id: 646e2c8c212146121303a5f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:36:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9377341270446777
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry I have no experience of the monkey patch or fine tuning GPTQ
          models.</p>

          <p>AutoGPTQ is added PEFT support soon (it''s currently <a rel="nofollow"
          href="https://github.com/PanQiWei/AutoGPTQ/pull/102">in a PR</a> - you could
          try it) which will be much better, when it works.</p>

          <p>Try asking on the Github where you got the monkey patch code - is it
          Alpaca Lora 4bit?</p>

          '
        raw: 'Sorry I have no experience of the monkey patch or fine tuning GPTQ models.


          AutoGPTQ is added PEFT support soon (it''s currently [in a PR](https://github.com/PanQiWei/AutoGPTQ/pull/102)
          - you could try it) which will be much better, when it works.


          Try asking on the Github where you got the monkey patch code - is it Alpaca
          Lora 4bit?'
        updatedAt: '2023-06-05T10:36:55.461Z'
      numEdits: 0
      reactions: []
    id: 647dbac710b7a3b157fe9808
    type: comment
  author: TheBloke
  content: 'Sorry I have no experience of the monkey patch or fine tuning GPTQ models.


    AutoGPTQ is added PEFT support soon (it''s currently [in a PR](https://github.com/PanQiWei/AutoGPTQ/pull/102)
    - you could try it) which will be much better, when it works.


    Try asking on the Github where you got the monkey patch code - is it Alpaca Lora
    4bit?'
  created_at: 2023-06-05 09:36:55+00:00
  edited: false
  hidden: false
  id: 647dbac710b7a3b157fe9808
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/stable-vicuna-13B-HF
repo_type: model
status: open
target_branch: null
title: Expected scalar type Float but found Half when using Text Gen WebUI with VIcuna
  & monkey-patch
