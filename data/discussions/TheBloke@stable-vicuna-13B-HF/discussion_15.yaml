!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Andyrasika
conflicting_files: null
created_at: 2023-08-12 15:33:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
      fullname: Ankush Singal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Andyrasika
      type: user
    createdAt: '2023-08-12T16:33:43.000Z'
    data:
      edited: false
      editors:
      - Andyrasika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.31115928292274475
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
          fullname: Ankush Singal
          isHf: false
          isPro: false
          name: Andyrasika
          type: user
        html: "<p>i tried:</p>\n<pre><code>lora_config = LoraConfig(\n    r=16, \n\
          \    lora_alpha=32, \n    target_modules=[\n        \"query_key_value\"\
          ,\n        \"dense\",\n        \"dense_h_to_4h\",\n        \"dense_4h_to_h\"\
          ,\n    ],\n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"\
          CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nprint_trainable_parameters(model)\n\
          </code></pre>\n<p>and got error:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[24], line 15\n      1 lora_config = LoraConfig(\n      2     r=16,\
          \ \n      3     lora_alpha=32, \n   (...)\n     12     task_type=\"CAUSAL_LM\"\
          \n     13 )\n---&gt; 15 model = get_peft_model(model, lora_config)\n   \
          \  16 print_trainable_parameters(model)\n\nFile /opt/conda/lib/python3.10/site-packages/peft/mapping.py:98,\
          \ in get_peft_model(model, peft_config, adapter_name)\n     96 if isinstance(peft_config,\
          \ PromptLearningConfig):\n     97     peft_config = _prepare_prompt_learning_config(peft_config,\
          \ model_config)\n---&gt; 98 return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model,\
          \ peft_config, adapter_name=adapter_name)\n\nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:893,\
          \ in PeftModelForCausalLM.__init__(self, model, peft_config, adapter_name)\n\
          \    892 def __init__(self, model, peft_config: PeftConfig, adapter_name=\"\
          default\"):\n--&gt; 893     super().__init__(model, peft_config, adapter_name)\n\
          \    894     self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n\
          \nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:112, in\
          \ PeftModel.__init__(self, model, peft_config, adapter_name)\n    110 if\
          \ not isinstance(peft_config, PromptLearningConfig):\n    111     self.peft_config[adapter_name]\
          \ = peft_config\n--&gt; 112     self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\n\
          \    113         self.base_model, self.peft_config, adapter_name\n    114\
          \     )\n    115     self.set_additional_trainable_modules(peft_config,\
          \ adapter_name)\n    116 else:\n\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:180,\
          \ in LoraModel.__init__(self, model, config, adapter_name)\n    178 self.forward\
          \ = self.model.forward\n    179 self.peft_config = config\n--&gt; 180 self.add_adapter(adapter_name,\
          \ self.peft_config[adapter_name])\n    182 # transformers models have a\
          \ .config attribute, whose presence is assumed later on\n    183 if not\
          \ hasattr(self, \"config\"):\n\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:194,\
          \ in LoraModel.add_adapter(self, adapter_name, config)\n    192     config\
          \ = self._prepare_lora_config(config, model_config)\n    193     self.peft_config[adapter_name]\
          \ = config\n--&gt; 194 self._find_and_replace(adapter_name)\n    195 if\
          \ len(self.peft_config) &gt; 1 and self.peft_config[adapter_name].bias !=\
          \ \"none\":\n    196     raise ValueError(\n    197         \"LoraModel\
          \ supports only 1 adapter with bias. When using multiple adapters, set bias\
          \ to 'none' for all adapters.\"\n    198     )\n\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:356,\
          \ in LoraModel._find_and_replace(self, adapter_name)\n    353         self._replace_module(parent,\
          \ target_name, new_module, target)\n    355 if not is_target_modules_in_base_model:\n\
          --&gt; 356     raise ValueError(\n    357         f\"Target modules {lora_config.target_modules}\
          \ not found in the base model. \"\n    358         f\"Please check the target\
          \ modules and try again.\"\n    359     )\n\nValueError: Target modules\
          \ ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'] not found\
          \ in the base model. Please check the target modules and try again.\n</code></pre>\n\
          <p>Any workarond this issue?</p>\n"
        raw: "i tried:\r\n```\r\nlora_config = LoraConfig(\r\n    r=16, \r\n    lora_alpha=32,\
          \ \r\n    target_modules=[\r\n        \"query_key_value\",\r\n        \"\
          dense\",\r\n        \"dense_h_to_4h\",\r\n        \"dense_4h_to_h\",\r\n\
          \    ],\r\n    lora_dropout=0.05, \r\n    bias=\"none\", \r\n    task_type=\"\
          CAUSAL_LM\"\r\n)\r\n\r\nmodel = get_peft_model(model, lora_config)\r\nprint_trainable_parameters(model)\r\
          \n```\r\nand got error:\r\n```\r\n---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\nCell In[24], line 15\r\n      1 lora_config = LoraConfig(\r\n\
          \      2     r=16, \r\n      3     lora_alpha=32, \r\n   (...)\r\n     12\
          \     task_type=\"CAUSAL_LM\"\r\n     13 )\r\n---> 15 model = get_peft_model(model,\
          \ lora_config)\r\n     16 print_trainable_parameters(model)\r\n\r\nFile\
          \ /opt/conda/lib/python3.10/site-packages/peft/mapping.py:98, in get_peft_model(model,\
          \ peft_config, adapter_name)\r\n     96 if isinstance(peft_config, PromptLearningConfig):\r\
          \n     97     peft_config = _prepare_prompt_learning_config(peft_config,\
          \ model_config)\r\n---> 98 return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model,\
          \ peft_config, adapter_name=adapter_name)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:893,\
          \ in PeftModelForCausalLM.__init__(self, model, peft_config, adapter_name)\r\
          \n    892 def __init__(self, model, peft_config: PeftConfig, adapter_name=\"\
          default\"):\r\n--> 893     super().__init__(model, peft_config, adapter_name)\r\
          \n    894     self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\r\
          \n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:112,\
          \ in PeftModel.__init__(self, model, peft_config, adapter_name)\r\n    110\
          \ if not isinstance(peft_config, PromptLearningConfig):\r\n    111     self.peft_config[adapter_name]\
          \ = peft_config\r\n--> 112     self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\r\
          \n    113         self.base_model, self.peft_config, adapter_name\r\n  \
          \  114     )\r\n    115     self.set_additional_trainable_modules(peft_config,\
          \ adapter_name)\r\n    116 else:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:180,\
          \ in LoraModel.__init__(self, model, config, adapter_name)\r\n    178 self.forward\
          \ = self.model.forward\r\n    179 self.peft_config = config\r\n--> 180 self.add_adapter(adapter_name,\
          \ self.peft_config[adapter_name])\r\n    182 # transformers models have\
          \ a .config attribute, whose presence is assumed later on\r\n    183 if\
          \ not hasattr(self, \"config\"):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:194,\
          \ in LoraModel.add_adapter(self, adapter_name, config)\r\n    192     config\
          \ = self._prepare_lora_config(config, model_config)\r\n    193     self.peft_config[adapter_name]\
          \ = config\r\n--> 194 self._find_and_replace(adapter_name)\r\n    195 if\
          \ len(self.peft_config) > 1 and self.peft_config[adapter_name].bias != \"\
          none\":\r\n    196     raise ValueError(\r\n    197         \"LoraModel\
          \ supports only 1 adapter with bias. When using multiple adapters, set bias\
          \ to 'none' for all adapters.\"\r\n    198     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:356,\
          \ in LoraModel._find_and_replace(self, adapter_name)\r\n    353        \
          \ self._replace_module(parent, target_name, new_module, target)\r\n    355\
          \ if not is_target_modules_in_base_model:\r\n--> 356     raise ValueError(\r\
          \n    357         f\"Target modules {lora_config.target_modules} not found\
          \ in the base model. \"\r\n    358         f\"Please check the target modules\
          \ and try again.\"\r\n    359     )\r\n\r\nValueError: Target modules ['query_key_value',\
          \ 'dense', 'dense_h_to_4h', 'dense_4h_to_h'] not found in the base model.\
          \ Please check the target modules and try again.\r\n```\r\nAny workarond\
          \ this issue?"
        updatedAt: '2023-08-12T16:33:43.778Z'
      numEdits: 0
      reactions: []
    id: 64d7b467050438e3b977ecbc
    type: comment
  author: Andyrasika
  content: "i tried:\r\n```\r\nlora_config = LoraConfig(\r\n    r=16, \r\n    lora_alpha=32,\
    \ \r\n    target_modules=[\r\n        \"query_key_value\",\r\n        \"dense\"\
    ,\r\n        \"dense_h_to_4h\",\r\n        \"dense_4h_to_h\",\r\n    ],\r\n  \
    \  lora_dropout=0.05, \r\n    bias=\"none\", \r\n    task_type=\"CAUSAL_LM\"\r\
    \n)\r\n\r\nmodel = get_peft_model(model, lora_config)\r\nprint_trainable_parameters(model)\r\
    \n```\r\nand got error:\r\n```\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \nCell In[24], line 15\r\n      1 lora_config = LoraConfig(\r\n      2     r=16,\
    \ \r\n      3     lora_alpha=32, \r\n   (...)\r\n     12     task_type=\"CAUSAL_LM\"\
    \r\n     13 )\r\n---> 15 model = get_peft_model(model, lora_config)\r\n     16\
    \ print_trainable_parameters(model)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/mapping.py:98,\
    \ in get_peft_model(model, peft_config, adapter_name)\r\n     96 if isinstance(peft_config,\
    \ PromptLearningConfig):\r\n     97     peft_config = _prepare_prompt_learning_config(peft_config,\
    \ model_config)\r\n---> 98 return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model,\
    \ peft_config, adapter_name=adapter_name)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:893,\
    \ in PeftModelForCausalLM.__init__(self, model, peft_config, adapter_name)\r\n\
    \    892 def __init__(self, model, peft_config: PeftConfig, adapter_name=\"default\"\
    ):\r\n--> 893     super().__init__(model, peft_config, adapter_name)\r\n    894\
    \     self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\r\
    \n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:112, in\
    \ PeftModel.__init__(self, model, peft_config, adapter_name)\r\n    110 if not\
    \ isinstance(peft_config, PromptLearningConfig):\r\n    111     self.peft_config[adapter_name]\
    \ = peft_config\r\n--> 112     self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\r\
    \n    113         self.base_model, self.peft_config, adapter_name\r\n    114 \
    \    )\r\n    115     self.set_additional_trainable_modules(peft_config, adapter_name)\r\
    \n    116 else:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:180,\
    \ in LoraModel.__init__(self, model, config, adapter_name)\r\n    178 self.forward\
    \ = self.model.forward\r\n    179 self.peft_config = config\r\n--> 180 self.add_adapter(adapter_name,\
    \ self.peft_config[adapter_name])\r\n    182 # transformers models have a .config\
    \ attribute, whose presence is assumed later on\r\n    183 if not hasattr(self,\
    \ \"config\"):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:194,\
    \ in LoraModel.add_adapter(self, adapter_name, config)\r\n    192     config =\
    \ self._prepare_lora_config(config, model_config)\r\n    193     self.peft_config[adapter_name]\
    \ = config\r\n--> 194 self._find_and_replace(adapter_name)\r\n    195 if len(self.peft_config)\
    \ > 1 and self.peft_config[adapter_name].bias != \"none\":\r\n    196     raise\
    \ ValueError(\r\n    197         \"LoraModel supports only 1 adapter with bias.\
    \ When using multiple adapters, set bias to 'none' for all adapters.\"\r\n   \
    \ 198     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:356,\
    \ in LoraModel._find_and_replace(self, adapter_name)\r\n    353         self._replace_module(parent,\
    \ target_name, new_module, target)\r\n    355 if not is_target_modules_in_base_model:\r\
    \n--> 356     raise ValueError(\r\n    357         f\"Target modules {lora_config.target_modules}\
    \ not found in the base model. \"\r\n    358         f\"Please check the target\
    \ modules and try again.\"\r\n    359     )\r\n\r\nValueError: Target modules\
    \ ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'] not found in\
    \ the base model. Please check the target modules and try again.\r\n```\r\nAny\
    \ workarond this issue?"
  created_at: 2023-08-12 15:33:43+00:00
  edited: false
  hidden: false
  id: 64d7b467050438e3b977ecbc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/stable-vicuna-13B-HF
repo_type: model
status: open
target_branch: null
title: 'not able to add Loraconfig target modules '
