!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mklf
conflicting_files: []
created_at: 2023-09-12 02:06:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/077f18f485edd9c2542c977769453746.svg
      fullname: Frank Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mklf
      type: user
    createdAt: '2023-09-12T03:06:49.000Z'
    data:
      edited: true
      editors:
      - mklf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.373665988445282
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/077f18f485edd9c2542c977769453746.svg
          fullname: Frank Lee
          isHf: false
          isPro: false
          name: mklf
          type: user
        html: "<p>WizardCoder-Python-34B-V1.0 was trained by transformers 4.31.0.\
          \ In transformers 4.31.0, <code>rope_theta</code> was not used in initializing\
          \ <code>RotaryEmbedding</code>, so   the default value for <code>base</code>\
          \ parameter is used here. Higher version of transformers(4.33.0  is tested)\
          \ passes <code>rope_theta</code> to the <code>base</code> parameter, which\
          \ is <code>1000000</code> in the config file </p>\n<p>initialize in 4.31.0:</p>\n\
          <pre><code>    def _init_rope(self):\n        if self.config.rope_scaling\
          \ is None:\n            self.rotary_emb = LlamaRotaryEmbedding(self.head_dim,\
          \ max_position_embeddings=self.max_position_embeddings)\n        else:\n\
          \            scaling_type = self.config.rope_scaling[\"type\"]\n       \
          \     scaling_factor = self.config.rope_scaling[\"factor\"]\n          \
          \  if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n\
          \                    self.head_dim, max_position_embeddings=self.max_position_embeddings,\
          \ scaling_factor=scaling_factor\n                )\n            elif scaling_type\
          \ == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n\
          \                    self.head_dim, max_position_embeddings=self.max_position_embeddings,\
          \ scaling_factor=scaling_factor\n                )\n            else:\n\
          \                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\"\
          )\n</code></pre>\n<p>default base value is <code>10000</code>: </p>\n<pre><code>class\
          \ LlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048,\
          \ base=10000, device=None):\n            ...\n</code></pre>\n"
        raw: "WizardCoder-Python-34B-V1.0 was trained by transformers 4.31.0. In transformers\
          \ 4.31.0, `rope_theta` was not used in initializing `RotaryEmbedding`, so\
          \   the default value for `base` parameter is used here. Higher version\
          \ of transformers(4.33.0  is tested) passes `rope_theta` to the `base` parameter,\
          \ which is `1000000` in the config file \n\ninitialize in 4.31.0:\n```\n\
          \    def _init_rope(self):\n        if self.config.rope_scaling is None:\n\
          \            self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\
          \        else:\n            scaling_type = self.config.rope_scaling[\"type\"\
          ]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n \
          \           if scaling_type == \"linear\":\n                self.rotary_emb\
          \ = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\
          \ max_position_embeddings=self.max_position_embeddings, scaling_factor=scaling_factor\n\
          \                )\n            elif scaling_type == \"dynamic\":\n    \
          \            self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n\
          \                    self.head_dim, max_position_embeddings=self.max_position_embeddings,\
          \ scaling_factor=scaling_factor\n                )\n            else:\n\
          \                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\"\
          )\n```\n\n\ndefault base value is `10000`: \n```\nclass LlamaRotaryEmbedding(torch.nn.Module):\n\
          \    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n\
          \            ...\n```"
        updatedAt: '2023-09-12T03:29:19.810Z'
      numEdits: 3
      reactions: []
    id: 64ffd5c985a884a964aebc7b
    type: comment
  author: mklf
  content: "WizardCoder-Python-34B-V1.0 was trained by transformers 4.31.0. In transformers\
    \ 4.31.0, `rope_theta` was not used in initializing `RotaryEmbedding`, so   the\
    \ default value for `base` parameter is used here. Higher version of transformers(4.33.0\
    \  is tested) passes `rope_theta` to the `base` parameter, which is `1000000`\
    \ in the config file \n\ninitialize in 4.31.0:\n```\n    def _init_rope(self):\n\
    \        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(self.head_dim,\
    \ max_position_embeddings=self.max_position_embeddings)\n        else:\n     \
    \       scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor\
    \ = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\"\
    :\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n    \
    \                self.head_dim, max_position_embeddings=self.max_position_embeddings,\
    \ scaling_factor=scaling_factor\n                )\n            elif scaling_type\
    \ == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n\
    \                    self.head_dim, max_position_embeddings=self.max_position_embeddings,\
    \ scaling_factor=scaling_factor\n                )\n            else:\n      \
    \          raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n```\n\
    \n\ndefault base value is `10000`: \n```\nclass LlamaRotaryEmbedding(torch.nn.Module):\n\
    \    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n\
    \            ...\n```"
  created_at: 2023-09-12 02:06:49+00:00
  edited: true
  hidden: false
  id: 64ffd5c985a884a964aebc7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/077f18f485edd9c2542c977769453746.svg
      fullname: Frank Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mklf
      type: user
    createdAt: '2023-09-12T03:06:50.000Z'
    data:
      oid: 9201d240d938b1c89b970a119876167f32027a1a
      parents:
      - d869ce178715f8d6e8141e2ed50e6290985eedb0
      subject: Update config.json
    id: 64ffd5ca0000000000000000
    type: commit
  author: mklf
  created_at: 2023-09-12 02:06:50+00:00
  id: 64ffd5ca0000000000000000
  oid: 9201d240d938b1c89b970a119876167f32027a1a
  summary: Update config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&h=200&f=face
      fullname: WizardLM
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: WizardLM
      type: user
    createdAt: '2023-09-14T06:58:56.000Z'
    data:
      status: closed
    id: 6502af30b5d4e364c22323ee
    type: status-change
  author: WizardLM
  created_at: 2023-09-14 05:58:56+00:00
  id: 6502af30b5d4e364c22323ee
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 24
repo_id: WizardLM/WizardCoder-Python-34B-V1.0
repo_type: model
status: closed
target_branch: refs/heads/main
title: Update config.json
