!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VivekChauhan06
conflicting_files: null
created_at: 2023-09-12 17:47:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d4f0eb9a0146997241c9091e511ac37.svg
      fullname: Vivek Chauhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VivekChauhan06
      type: user
    createdAt: '2023-09-12T18:47:08.000Z'
    data:
      edited: false
      editors:
      - VivekChauhan06
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8148504495620728
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d4f0eb9a0146997241c9091e511ac37.svg
          fullname: Vivek Chauhan
          isHf: false
          isPro: false
          name: VivekChauhan06
          type: user
        html: '<p>Can anyone tell me  about the memory requirements for the WizardLM/WizardCoder-Python-7B-V1.0.</p>

          '
        raw: Can anyone tell me  about the memory requirements for the WizardLM/WizardCoder-Python-7B-V1.0.
        updatedAt: '2023-09-12T18:47:08.923Z'
      numEdits: 0
      reactions: []
    id: 6500b22c520f8d7b4d0f8159
    type: comment
  author: VivekChauhan06
  content: Can anyone tell me  about the memory requirements for the WizardLM/WizardCoder-Python-7B-V1.0.
  created_at: 2023-09-12 17:47:08+00:00
  edited: false
  hidden: false
  id: 6500b22c520f8d7b4d0f8159
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c96fec0d11a442c226ffab63cddbc26e.svg
      fullname: Joseph Pan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wzpan
      type: user
    createdAt: '2023-09-13T08:04:33.000Z'
    data:
      edited: true
      editors:
      - wzpan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8851361274719238
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c96fec0d11a442c226ffab63cddbc26e.svg
          fullname: Joseph Pan
          isHf: false
          isPro: false
          name: wzpan
          type: user
        html: "<p>Here's the result given by <a href=\"https://huggingface.co/spaces/hf-accelerate/model-memory-usage\"\
          >Model Memory Calculator</a>, which is a handy tool to calculate the memory\
          \ requirements of a certain LLM.</p>\n<div class=\"max-w-full overflow-auto\"\
          >\n\t<table>\n\t\t<thead><tr>\n<th>dtype</th>\n<th>Largest Layer or Residual\
          \ Group</th>\n<th>Total Size</th>\n<th>Training using Adam</th>\n</tr>\n\
          \n\t\t</thead><tbody><tr>\n<td>float32</td>\n<td>788.03 MB</td>\n<td>25.11\
          \ GB</td>\n<td>100.46 GB</td>\n</tr>\n<tr>\n<td>float16/bfloat16</td>\n\
          <td>394.02 MB</td>\n<td>12.56 GB</td>\n<td>50.23 GB</td>\n</tr>\n<tr>\n\
          <td>int8</td>\n<td>197.01 MB</td>\n<td>6.28 GB</td>\n<td>25.11 GB</td>\n\
          </tr>\n<tr>\n<td>int4</td>\n<td>98.5 MB</td>\n<td>3.14 GB</td>\n<td>12.56\
          \ GB</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: 'Here''s the result given by [Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage),
          which is a handy tool to calculate the memory requirements of a certain
          LLM.


          | dtype | Largest Layer or Residual Group | Total Size | Training using
          Adam |

          | --------  | ------------------- | ------------- | ------------- |

          | float32 |  788.03 MB |  25.11 GB |   100.46 GB |

          | float16/bfloat16 |  394.02 MB |  12.56 GB |  50.23 GB |

          | int8 |  197.01 MB |   6.28 GB |  25.11 GB |

          | int4 |   98.5 MB |   3.14 GB |  12.56 GB  |'
        updatedAt: '2023-09-13T08:10:52.172Z'
      numEdits: 3
      reactions: []
    id: 65016d11e3201fff8886fb78
    type: comment
  author: wzpan
  content: 'Here''s the result given by [Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage),
    which is a handy tool to calculate the memory requirements of a certain LLM.


    | dtype | Largest Layer or Residual Group | Total Size | Training using Adam |

    | --------  | ------------------- | ------------- | ------------- |

    | float32 |  788.03 MB |  25.11 GB |   100.46 GB |

    | float16/bfloat16 |  394.02 MB |  12.56 GB |  50.23 GB |

    | int8 |  197.01 MB |   6.28 GB |  25.11 GB |

    | int4 |   98.5 MB |   3.14 GB |  12.56 GB  |'
  created_at: 2023-09-13 07:04:33+00:00
  edited: true
  hidden: false
  id: 65016d11e3201fff8886fb78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d4f0eb9a0146997241c9091e511ac37.svg
      fullname: Vivek Chauhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VivekChauhan06
      type: user
    createdAt: '2023-09-13T11:12:02.000Z'
    data:
      edited: false
      editors:
      - VivekChauhan06
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7715728878974915
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d4f0eb9a0146997241c9091e511ac37.svg
          fullname: Vivek Chauhan
          isHf: false
          isPro: false
          name: VivekChauhan06
          type: user
        html: '<p>model = AutoModelForCausalLM.from_pretrained("WizardLM/WizardCoder-Python-7B-V1.0"),
          quantization_config=bnb_config, device_map={"":0})<br>While running the
          above code the model takes the whole CPU RAM but it does not take the Colab
          GPU which I am using and due to this reason  I am not able to use this model
          in Colab. I have already used the" codellama/CodeLlama-7b-Instruct-hf" model
          which works well while using the  AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Instruct-hf",
          quantization_config=bnb_config, device_map={"":0})</p>

          <p>Give me the solution which can help me to know the reason behind it.</p>

          '
        raw: 'model = AutoModelForCausalLM.from_pretrained("WizardLM/WizardCoder-Python-7B-V1.0"),
          quantization_config=bnb_config, device_map={"":0})

          While running the above code the model takes the whole CPU RAM but it does
          not take the Colab GPU which I am using and due to this reason  I am not
          able to use this model in Colab. I have already used the" codellama/CodeLlama-7b-Instruct-hf"
          model which works well while using the  AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Instruct-hf",
          quantization_config=bnb_config, device_map={"":0})


          Give me the solution which can help me to know the reason behind it.

          '
        updatedAt: '2023-09-13T11:12:02.330Z'
      numEdits: 0
      reactions: []
      relatedEventId: 650199025c0e01b2286708fa
    id: 650199025c0e01b2286708f7
    type: comment
  author: VivekChauhan06
  content: 'model = AutoModelForCausalLM.from_pretrained("WizardLM/WizardCoder-Python-7B-V1.0"),
    quantization_config=bnb_config, device_map={"":0})

    While running the above code the model takes the whole CPU RAM but it does not
    take the Colab GPU which I am using and due to this reason  I am not able to use
    this model in Colab. I have already used the" codellama/CodeLlama-7b-Instruct-hf"
    model which works well while using the  AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Instruct-hf",
    quantization_config=bnb_config, device_map={"":0})


    Give me the solution which can help me to know the reason behind it.

    '
  created_at: 2023-09-13 10:12:02+00:00
  edited: false
  hidden: false
  id: 650199025c0e01b2286708f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1d4f0eb9a0146997241c9091e511ac37.svg
      fullname: Vivek Chauhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VivekChauhan06
      type: user
    createdAt: '2023-09-13T11:12:02.000Z'
    data:
      status: closed
    id: 650199025c0e01b2286708fa
    type: status-change
  author: VivekChauhan06
  created_at: 2023-09-13 10:12:02+00:00
  id: 650199025c0e01b2286708fa
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1d4f0eb9a0146997241c9091e511ac37.svg
      fullname: Vivek Chauhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VivekChauhan06
      type: user
    createdAt: '2023-09-13T13:50:05.000Z'
    data:
      status: open
    id: 6501be0d6df841449a46b39e
    type: status-change
  author: VivekChauhan06
  created_at: 2023-09-13 12:50:05+00:00
  id: 6501be0d6df841449a46b39e
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: WizardLM/WizardCoder-Python-34B-V1.0
repo_type: model
status: open
target_branch: null
title: 'WizardLM/WizardCoder-Python-7B-V1.0 Memory Requirements '
