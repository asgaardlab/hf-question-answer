!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-09-06 05:29:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-06T06:29:08.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9039379358291626
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;WizardLM&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/WizardLM\">@<span class=\"\
          underline\">WizardLM</span></a></span>\n\n\t</span></span><br>Hello you\
          \ can call me rombodawg. I have worked for the past few months on the LosslessMegaCodetraining\
          \ datasets that you can find on my hugging face page, and have refined them\
          \ to the most peak that they have ever been to this day. I ask that you\
          \ thoroughly consider using one of (or experiment and use both then test\
          \ the results) my datasets. which i will link bellow with a brief description,\
          \ in order to either further fine tune your wizardcoder-python series of\
          \ models (7b, 13b, 34b) or combine with your existing dataset and re-finetune\
          \ the codellama models that you originally used to create the wizardcoder-python\
          \ series. </p>\n<p>Let me give you a background on why my datasets are worth\
          \ your while.  I have created the LosslessMegaCoder-llama2-13b-mini ai model\
          \ using the Version 2 of my LosslessMegaCodeTraining dataset. This model\
          \ if you look at the \"Can ai code\" leaderboard actually beats the wizardcoder-python\
          \ ai model in python code generation if this leaderboard is to be trusted.\
          \ </p>\n<p>\"Can ai code\" for reference in above statement.<br><a href=\"\
          https://huggingface.co/spaces/mike-ravkine/can-ai-code-results\">https://huggingface.co/spaces/mike-ravkine/can-ai-code-results</a></p>\n\
          <p>However, i believe the Version 2 of my dataset is actually considered\
          \ \"weak-sauce\" compared to the version 3. The reason? Bigcode's commitpackft!\
          \ I have converted bigcode's commitpackft to alpaca format, and taken much\
          \ time to remove any errors caused by the vast amount of over 250 coding\
          \ languages in the dataset when trying to train models with it in the new\
          \ format. Along with this I have also added some of the more refined non-coding\
          \ datasets such as open-platypus, and (in the mini version) only airoboros\
          \ 2.1 dataset (I will explain this in more detail bellow). </p>\n<ul>\n\
          <li>My philosophy (LosslessCoding):<br>The idea behind lossless coding is\
          \ simple, train ai with coding and non-coding knowledge at the same time,\
          \ and the dont lose reasoning, and logic abilities. The was an issue with\
          \ the old wizardcoder model (15b) as well as the NewHope model that promised\
          \ high levels of coding performance. I dont know if you follow this type\
          \ of ideals of not when training your models but I have provided multiple\
          \ datasets (both Lossless, and purely coding) for your convenience at the\
          \ bottom.</li>\n</ul>\n<p>Thats enough rambling for now, let me get to the\
          \ meat and potatoes. What are the two datasets that make up LosslessMegaCodeTrainingVersion3,\
          \ bellow you will finds brief descriptions of how they are diffrent. I highly\
          \ recommend using one of these two datasets, however if you do not follow\
          \ my philosophy of the \"LosslessCoding\" that i have mentioned above, i\
          \ will link two more datasets that are purely meant for code only, you can\
          \ combine these two and they will be only coding data without any non-coding\
          \ data. However only commitpackft has been filtered so that dataset is the\
          \ best version to use if you want pure, filtered coding data.  </p>\n<p>LosslessMegaCodeTrainingV3_1.6m_Evol<br>This\
          \ dataset is comprised of the entire Version 2 dataset, that is mentioned\
          \ above, in addition to the commipackft conversion that i made, as well\
          \ as openplatypus.</p>\n<p>link:</p>\n<ul>\n<li><a href=\"https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol\"\
          >https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol</a></li>\n\
          </ul>\n<p>LosslessMegaCodeTrainingV3_MINI<br>This dataset actually doesnt\
          \ contain any data from Version 2, its only comprised of commitpackft, openplatypus,\
          \ and airoboros 2.1 datasets. This ensures the lower quality data from Version\
          \ 2 was left out (because it has not been manually filtered nor filtered\
          \ by ai) and only uses data that has been filtered. </p>\n<p>link:</p>\n\
          <ul>\n<li><a href=\"https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI\"\
          >https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI</a></li>\n\
          </ul>\n<hr>\n<ul>\n<li>Pure coding datasets:</li>\n</ul>\n<p>2XUNCENSORED_MegaCodeTraining188k<br>The\
          \ unfiltered coding data present in Version 2 of losslessmegacodetraining.</p>\n\
          <p>link:</p>\n<ul>\n<li><a href=\"https://huggingface.co/datasets/rombodawg/2XUNCENSORED_MegaCodeTraining188k\"\
          >https://huggingface.co/datasets/rombodawg/2XUNCENSORED_MegaCodeTraining188k</a></li>\n\
          </ul>\n<p>Rombodawgs_commitpackft_Evolinstruct_Converted<br>The 100% pure,\
          \ perfect filtered data, from bigcode, converted to alpaca format.</p>\n\
          <p>link:</p>\n<ul>\n<li><a href=\"https://huggingface.co/datasets/rombodawg/Rombodawgs_commitpackft_Evolinstruct_Converted\"\
          >https://huggingface.co/datasets/rombodawg/Rombodawgs_commitpackft_Evolinstruct_Converted</a></li>\n\
          </ul>\n"
        raw: "@WizardLM \nHello you can call me rombodawg. I have worked for the past\
          \ few months on the LosslessMegaCodetraining datasets that you can find\
          \ on my hugging face page, and have refined them to the most peak that they\
          \ have ever been to this day. I ask that you thoroughly consider using one\
          \ of (or experiment and use both then test the results) my datasets. which\
          \ i will link bellow with a brief description, in order to either further\
          \ fine tune your wizardcoder-python series of models (7b, 13b, 34b) or combine\
          \ with your existing dataset and re-finetune the codellama models that you\
          \ originally used to create the wizardcoder-python series. \n\nLet me give\
          \ you a background on why my datasets are worth your while.  I have created\
          \ the LosslessMegaCoder-llama2-13b-mini ai model using the Version 2 of\
          \ my LosslessMegaCodeTraining dataset. This model if you look at the \"\
          Can ai code\" leaderboard actually beats the wizardcoder-python ai model\
          \ in python code generation if this leaderboard is to be trusted. \n\n\"\
          Can ai code\" for reference in above statement. \nhttps://huggingface.co/spaces/mike-ravkine/can-ai-code-results\n\
          \nHowever, i believe the Version 2 of my dataset is actually considered\
          \ \"weak-sauce\" compared to the version 3. The reason? Bigcode's commitpackft!\
          \ I have converted bigcode's commitpackft to alpaca format, and taken much\
          \ time to remove any errors caused by the vast amount of over 250 coding\
          \ languages in the dataset when trying to train models with it in the new\
          \ format. Along with this I have also added some of the more refined non-coding\
          \ datasets such as open-platypus, and (in the mini version) only airoboros\
          \ 2.1 dataset (I will explain this in more detail bellow). \n\n- My philosophy\
          \ (LosslessCoding):\nThe idea behind lossless coding is simple, train ai\
          \ with coding and non-coding knowledge at the same time, and the dont lose\
          \ reasoning, and logic abilities. The was an issue with the old wizardcoder\
          \ model (15b) as well as the NewHope model that promised high levels of\
          \ coding performance. I dont know if you follow this type of ideals of not\
          \ when training your models but I have provided multiple datasets (both\
          \ Lossless, and purely coding) for your convenience at the bottom. \n\n\
          Thats enough rambling for now, let me get to the meat and potatoes. What\
          \ are the two datasets that make up LosslessMegaCodeTrainingVersion3, bellow\
          \ you will finds brief descriptions of how they are diffrent. I highly recommend\
          \ using one of these two datasets, however if you do not follow my philosophy\
          \ of the \"LosslessCoding\" that i have mentioned above, i will link two\
          \ more datasets that are purely meant for code only, you can combine these\
          \ two and they will be only coding data without any non-coding data. However\
          \ only commitpackft has been filtered so that dataset is the best version\
          \ to use if you want pure, filtered coding data.  \n\n\nLosslessMegaCodeTrainingV3_1.6m_Evol\n\
          This dataset is comprised of the entire Version 2 dataset, that is mentioned\
          \ above, in addition to the commipackft conversion that i made, as well\
          \ as openplatypus.\n\nlink:\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol\n\
          \nLosslessMegaCodeTrainingV3_MINI\nThis dataset actually doesnt contain\
          \ any data from Version 2, its only comprised of commitpackft, openplatypus,\
          \ and airoboros 2.1 datasets. This ensures the lower quality data from Version\
          \ 2 was left out (because it has not been manually filtered nor filtered\
          \ by ai) and only uses data that has been filtered. \n\nlink:\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI\n\
          \n___________________________________________________________________________________________________________________\n\
          \n- Pure coding datasets:\n\n2XUNCENSORED_MegaCodeTraining188k\nThe unfiltered\
          \ coding data present in Version 2 of losslessmegacodetraining.\n\nlink:\n\
          - https://huggingface.co/datasets/rombodawg/2XUNCENSORED_MegaCodeTraining188k\n\
          \nRombodawgs_commitpackft_Evolinstruct_Converted \nThe 100% pure, perfect\
          \ filtered data, from bigcode, converted to alpaca format.\n\nlink:\n- https://huggingface.co/datasets/rombodawg/Rombodawgs_commitpackft_Evolinstruct_Converted\n"
        updatedAt: '2023-09-06T06:29:47.045Z'
      numEdits: 1
      reactions: []
    id: 64f81c34a9b6fed18c68b680
    type: comment
  author: rombodawg
  content: "@WizardLM \nHello you can call me rombodawg. I have worked for the past\
    \ few months on the LosslessMegaCodetraining datasets that you can find on my\
    \ hugging face page, and have refined them to the most peak that they have ever\
    \ been to this day. I ask that you thoroughly consider using one of (or experiment\
    \ and use both then test the results) my datasets. which i will link bellow with\
    \ a brief description, in order to either further fine tune your wizardcoder-python\
    \ series of models (7b, 13b, 34b) or combine with your existing dataset and re-finetune\
    \ the codellama models that you originally used to create the wizardcoder-python\
    \ series. \n\nLet me give you a background on why my datasets are worth your while.\
    \  I have created the LosslessMegaCoder-llama2-13b-mini ai model using the Version\
    \ 2 of my LosslessMegaCodeTraining dataset. This model if you look at the \"Can\
    \ ai code\" leaderboard actually beats the wizardcoder-python ai model in python\
    \ code generation if this leaderboard is to be trusted. \n\n\"Can ai code\" for\
    \ reference in above statement. \nhttps://huggingface.co/spaces/mike-ravkine/can-ai-code-results\n\
    \nHowever, i believe the Version 2 of my dataset is actually considered \"weak-sauce\"\
    \ compared to the version 3. The reason? Bigcode's commitpackft! I have converted\
    \ bigcode's commitpackft to alpaca format, and taken much time to remove any errors\
    \ caused by the vast amount of over 250 coding languages in the dataset when trying\
    \ to train models with it in the new format. Along with this I have also added\
    \ some of the more refined non-coding datasets such as open-platypus, and (in\
    \ the mini version) only airoboros 2.1 dataset (I will explain this in more detail\
    \ bellow). \n\n- My philosophy (LosslessCoding):\nThe idea behind lossless coding\
    \ is simple, train ai with coding and non-coding knowledge at the same time, and\
    \ the dont lose reasoning, and logic abilities. The was an issue with the old\
    \ wizardcoder model (15b) as well as the NewHope model that promised high levels\
    \ of coding performance. I dont know if you follow this type of ideals of not\
    \ when training your models but I have provided multiple datasets (both Lossless,\
    \ and purely coding) for your convenience at the bottom. \n\nThats enough rambling\
    \ for now, let me get to the meat and potatoes. What are the two datasets that\
    \ make up LosslessMegaCodeTrainingVersion3, bellow you will finds brief descriptions\
    \ of how they are diffrent. I highly recommend using one of these two datasets,\
    \ however if you do not follow my philosophy of the \"LosslessCoding\" that i\
    \ have mentioned above, i will link two more datasets that are purely meant for\
    \ code only, you can combine these two and they will be only coding data without\
    \ any non-coding data. However only commitpackft has been filtered so that dataset\
    \ is the best version to use if you want pure, filtered coding data.  \n\n\nLosslessMegaCodeTrainingV3_1.6m_Evol\n\
    This dataset is comprised of the entire Version 2 dataset, that is mentioned above,\
    \ in addition to the commipackft conversion that i made, as well as openplatypus.\n\
    \nlink:\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol\n\
    \nLosslessMegaCodeTrainingV3_MINI\nThis dataset actually doesnt contain any data\
    \ from Version 2, its only comprised of commitpackft, openplatypus, and airoboros\
    \ 2.1 datasets. This ensures the lower quality data from Version 2 was left out\
    \ (because it has not been manually filtered nor filtered by ai) and only uses\
    \ data that has been filtered. \n\nlink:\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI\n\
    \n___________________________________________________________________________________________________________________\n\
    \n- Pure coding datasets:\n\n2XUNCENSORED_MegaCodeTraining188k\nThe unfiltered\
    \ coding data present in Version 2 of losslessmegacodetraining.\n\nlink:\n- https://huggingface.co/datasets/rombodawg/2XUNCENSORED_MegaCodeTraining188k\n\
    \nRombodawgs_commitpackft_Evolinstruct_Converted \nThe 100% pure, perfect filtered\
    \ data, from bigcode, converted to alpaca format.\n\nlink:\n- https://huggingface.co/datasets/rombodawg/Rombodawgs_commitpackft_Evolinstruct_Converted\n"
  created_at: 2023-09-06 05:29:08+00:00
  edited: true
  hidden: false
  id: 64f81c34a9b6fed18c68b680
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-07T14:09:03.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9405384659767151
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Hello once again. I am pleased to inform you that I have released
          an a new dataset, the successor to Megacodetraining. It is called Limitlesscodetraining.
          Its is (as far as my knowledge) the purest, most refined and filtered coding
          dataset on hugginface. Feel free to use it to further finetune your wizard
          coder dataset, or train your model.</p>

          <p>link:</p>

          <ul>

          <li><a href="https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining">https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining</a></li>

          </ul>

          '
        raw: 'Hello once again. I am pleased to inform you that I have released an
          a new dataset, the successor to Megacodetraining. It is called Limitlesscodetraining.
          Its is (as far as my knowledge) the purest, most refined and filtered coding
          dataset on hugginface. Feel free to use it to further finetune your wizard
          coder dataset, or train your model.


          link:

          - https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining'
        updatedAt: '2023-09-07T14:09:03.757Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - rexzen
        - rooben
      - count: 1
        reaction: "\U0001F614"
        users:
        - bmorphism
    id: 64f9d97f2105ae68a0efcbef
    type: comment
  author: rombodawg
  content: 'Hello once again. I am pleased to inform you that I have released an a
    new dataset, the successor to Megacodetraining. It is called Limitlesscodetraining.
    Its is (as far as my knowledge) the purest, most refined and filtered coding dataset
    on hugginface. Feel free to use it to further finetune your wizard coder dataset,
    or train your model.


    link:

    - https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining'
  created_at: 2023-09-07 13:09:03+00:00
  edited: false
  hidden: false
  id: 64f9d97f2105ae68a0efcbef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&h=200&f=face
      fullname: WizardLM
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: WizardLM
      type: user
    createdAt: '2023-09-14T06:58:45.000Z'
    data:
      status: closed
    id: 6502af252cec4c6d94eb1a0d
    type: status-change
  author: WizardLM
  created_at: 2023-09-14 05:58:45+00:00
  id: 6502af252cec4c6d94eb1a0d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-14T18:41:33.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9373958110809326
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Can you guys reply and give me your thoughts before closing the
          discussion? Id like to know id you plan on using my datasets at all</p>

          '
        raw: Can you guys reply and give me your thoughts before closing the discussion?
          Id like to know id you plan on using my datasets at all
        updatedAt: '2023-09-14T18:41:33.606Z'
      numEdits: 0
      reactions: []
      relatedEventId: 650353dd93574a89716e16f4
    id: 650353dd93574a89716e16f1
    type: comment
  author: rombodawg
  content: Can you guys reply and give me your thoughts before closing the discussion?
    Id like to know id you plan on using my datasets at all
  created_at: 2023-09-14 17:41:33+00:00
  edited: false
  hidden: false
  id: 650353dd93574a89716e16f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-14T18:41:33.000Z'
    data:
      status: open
    id: 650353dd93574a89716e16f4
    type: status-change
  author: rombodawg
  created_at: 2023-09-14 17:41:33+00:00
  id: 650353dd93574a89716e16f4
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&h=200&f=face
      fullname: WizardLM
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: WizardLM
      type: user
    createdAt: '2023-09-15T03:45:57.000Z'
    data:
      edited: false
      editors:
      - WizardLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8498609662055969
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&h=200&f=face
          fullname: WizardLM
          isHf: false
          isPro: false
          name: WizardLM
          type: user
        html: '<p>Thanks, we will not use.</p>

          '
        raw: Thanks, we will not use.
        updatedAt: '2023-09-15T03:45:57.478Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rombodawg
    id: 6503d375da3344b5f1171e8e
    type: comment
  author: WizardLM
  content: Thanks, we will not use.
  created_at: 2023-09-15 02:45:57+00:00
  edited: false
  hidden: false
  id: 6503d375da3344b5f1171e8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&h=200&f=face
      fullname: WizardLM
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: WizardLM
      type: user
    createdAt: '2023-09-15T03:46:55.000Z'
    data:
      status: closed
    id: 6503d3af221035b9ee0971d1
    type: status-change
  author: WizardLM
  created_at: 2023-09-15 02:46:55+00:00
  id: 6503d3af221035b9ee0971d1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: WizardLM/WizardCoder-Python-34B-V1.0
repo_type: model
status: closed
target_branch: null
title: 'Please consider my toiled over coding dataset for fine tuning a 1.1 version
  of the wizard coder series. '
