!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nguyengoc
conflicting_files: null
created_at: 2023-10-04 02:04:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab8b5adc140d538164f5678ef2ea3ea0.svg
      fullname: Ngoc Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nguyengoc
      type: user
    createdAt: '2023-10-04T03:04:56.000Z'
    data:
      edited: false
      editors:
      - nguyengoc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9780833721160889
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab8b5adc140d538164f5678ef2ea3ea0.svg
          fullname: Ngoc Nguyen
          isHf: false
          isPro: false
          name: nguyengoc
          type: user
        html: '<p>What is the system requirement to run this model, and how can I
          find that?</p>

          '
        raw: "What is the system requirement to run this model, and how can I find\
          \ that?\r\n"
        updatedAt: '2023-10-04T03:04:56.027Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Vicent3
        - DanekBigLike
        - kaplinmok
    id: 651cd65809debbe627ba3cb7
    type: comment
  author: nguyengoc
  content: "What is the system requirement to run this model, and how can I find that?\r\
    \n"
  created_at: 2023-10-04 02:04:56+00:00
  edited: false
  hidden: false
  id: 651cd65809debbe627ba3cb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07aad732d47ee5feef8e8af3027531c9.svg
      fullname: Adam Shaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shavera
      type: user
    createdAt: '2023-10-22T14:03:32.000Z'
    data:
      edited: true
      editors:
      - shavera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6124676465988159
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07aad732d47ee5feef8e8af3027531c9.svg
          fullname: Adam Shaver
          isHf: false
          isPro: false
          name: shavera
          type: user
        html: "<p>From reading the config, this is a Flaot16 model, using the Model\
          \ Memory Estimator (<a href=\"https://huggingface.co/spaces/hf-accelerate/model-memory-usage\"\
          >https://huggingface.co/spaces/hf-accelerate/model-memory-usage</a>), it\
          \ provides the following specs for the wizard-coder 30B (as a llm):</p>\n\
          <p>dtype\t                    Largest Layer or Residual Group\tTotal Size\t\
          Training using Adam<br>float32\t                    2.59 GB\t          \
          \                                      125.48 GB\t501.92 GB<br>int8\t  \
          \                      664.02 MB\t                                     \
          \       31.37 GB\t    125.48 GB<br>float16/bfloat16\t1.3 GB\t          \
          \                                          62.74 GB\t    250.96 GB<br>int4\t\
          \                        332.01 MB\t                                   \
          \        15.68 GB    \t62.74 GB</p>\n<p>So, if you pull this down, you'll\
          \ need 63GB of RAM to run it. I would love to quantize this to a int8, so\
          \ it could fit on a 4090 or A6000, but don't know how right now. </p>\n"
        raw: "From reading the config, this is a Flaot16 model, using the Model Memory\
          \ Estimator (https://huggingface.co/spaces/hf-accelerate/model-memory-usage),\
          \ it provides the following specs for the wizard-coder 30B (as a llm):\n\
          \ndtype\t                    Largest Layer or Residual Group\tTotal Size\t\
          Training using Adam\nfloat32\t                    2.59 GB\t            \
          \                                    125.48 GB\t501.92 GB\nint8\t      \
          \                  664.02 MB\t                                         \
          \   31.37 GB\t    125.48 GB\nfloat16/bfloat16\t1.3 GB\t                \
          \                                    62.74 GB\t    250.96 GB\nint4\t   \
          \                     332.01 MB\t                                      \
          \     15.68 GB    \t62.74 GB\n\nSo, if you pull this down, you'll need 63GB\
          \ of RAM to run it. I would love to quantize this to a int8, so it could\
          \ fit on a 4090 or A6000, but don't know how right now. \n\n\n"
        updatedAt: '2023-10-22T14:21:46.580Z'
      numEdits: 1
      reactions: []
    id: 65352bb4d434308ba4637f71
    type: comment
  author: shavera
  content: "From reading the config, this is a Flaot16 model, using the Model Memory\
    \ Estimator (https://huggingface.co/spaces/hf-accelerate/model-memory-usage),\
    \ it provides the following specs for the wizard-coder 30B (as a llm):\n\ndtype\t\
    \                    Largest Layer or Residual Group\tTotal Size\tTraining using\
    \ Adam\nfloat32\t                    2.59 GB\t                               \
    \                 125.48 GB\t501.92 GB\nint8\t                        664.02 MB\t\
    \                                            31.37 GB\t    125.48 GB\nfloat16/bfloat16\t\
    1.3 GB\t                                                    62.74 GB\t    250.96\
    \ GB\nint4\t                        332.01 MB\t                              \
    \             15.68 GB    \t62.74 GB\n\nSo, if you pull this down, you'll need\
    \ 63GB of RAM to run it. I would love to quantize this to a int8, so it could\
    \ fit on a 4090 or A6000, but don't know how right now. \n\n\n"
  created_at: 2023-10-22 13:03:32+00:00
  edited: true
  hidden: false
  id: 65352bb4d434308ba4637f71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d095f0623308426c8f29e0fee788978.svg
      fullname: Serge Zenchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: sergeyzen
      type: user
    createdAt: '2023-10-22T18:27:59.000Z'
    data:
      edited: false
      editors:
      - sergeyzen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.31315481662750244
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d095f0623308426c8f29e0fee788978.svg
          fullname: Serge Zenchenko
          isHf: false
          isPro: true
          name: sergeyzen
          type: user
        html: '<p>I am able to run in on M1 Max 64GB. Not super fast, but it works</p>

          <pre><code class="language-llama_print_timings:">llama_print_timings:      sample
          time =  1804.24 ms /   729 runs   (    2.47 ms per token,   404.05 tokens
          per second)

          llama_print_timings: prompt eval time =  3652.04 ms /   144 tokens (   25.36
          ms per token,    39.43 tokens per second)

          llama_print_timings:        eval time = 94289.78 ms /   728 runs   (  129.52
          ms per token,     7.72 tokens per second)

          llama_print_timings:       total time = 100932.23 ms

          Output generated in 101.16 seconds (7.20 tokens/s, 728 tokens, context 144,
          seed 1690939106)

          Llama.generate: prefix-match hit


          llama_print_timings:        load time =  3652.09 ms

          llama_print_timings:      sample time =  2548.89 ms /  1024 runs   (    2.49
          ms per token,   401.74 tokens per second)

          llama_print_timings: prompt eval time = 13158.02 ms /   751 tokens (   17.52
          ms per token,    57.08 tokens per second)

          llama_print_timings:        eval time = 141916.85 ms /  1023 runs   (  138.73
          ms per token,     7.21 tokens per second)

          llama_print_timings:       total time = 159473.00 ms

          Output generated in 159.71 seconds (6.41 tokens/s, 1024 tokens, context
          886, seed 1686911609)

          Llama.generate: prefix-match hit


          llama_print_timings:        load time =  3652.09 ms

          llama_print_timings:      sample time =   694.30 ms /   276 runs   (    2.52
          ms per token,   397.52 tokens per second)

          llama_print_timings: prompt eval time = 19746.02 ms /  1023 tokens (   19.30
          ms per token,    51.81 tokens per second)

          llama_print_timings:        eval time = 43975.35 ms /   275 runs   (  159.91
          ms per token,     6.25 tokens per second)

          llama_print_timings:       total time = 64842.96 ms

          Output generated in 65.07 seconds (4.23 tokens/s, 275 tokens, context 1909,
          seed 828516400)

          </code></pre>

          '
        raw: 'I am able to run in on M1 Max 64GB. Not super fast, but it works


          ```llama_print_timings:        load time =  3652.09 ms

          llama_print_timings:      sample time =  1804.24 ms /   729 runs   (    2.47
          ms per token,   404.05 tokens per second)

          llama_print_timings: prompt eval time =  3652.04 ms /   144 tokens (   25.36
          ms per token,    39.43 tokens per second)

          llama_print_timings:        eval time = 94289.78 ms /   728 runs   (  129.52
          ms per token,     7.72 tokens per second)

          llama_print_timings:       total time = 100932.23 ms

          Output generated in 101.16 seconds (7.20 tokens/s, 728 tokens, context 144,
          seed 1690939106)

          Llama.generate: prefix-match hit


          llama_print_timings:        load time =  3652.09 ms

          llama_print_timings:      sample time =  2548.89 ms /  1024 runs   (    2.49
          ms per token,   401.74 tokens per second)

          llama_print_timings: prompt eval time = 13158.02 ms /   751 tokens (   17.52
          ms per token,    57.08 tokens per second)

          llama_print_timings:        eval time = 141916.85 ms /  1023 runs   (  138.73
          ms per token,     7.21 tokens per second)

          llama_print_timings:       total time = 159473.00 ms

          Output generated in 159.71 seconds (6.41 tokens/s, 1024 tokens, context
          886, seed 1686911609)

          Llama.generate: prefix-match hit


          llama_print_timings:        load time =  3652.09 ms

          llama_print_timings:      sample time =   694.30 ms /   276 runs   (    2.52
          ms per token,   397.52 tokens per second)

          llama_print_timings: prompt eval time = 19746.02 ms /  1023 tokens (   19.30
          ms per token,    51.81 tokens per second)

          llama_print_timings:        eval time = 43975.35 ms /   275 runs   (  159.91
          ms per token,     6.25 tokens per second)

          llama_print_timings:       total time = 64842.96 ms

          Output generated in 65.07 seconds (4.23 tokens/s, 275 tokens, context 1909,
          seed 828516400)

          ```'
        updatedAt: '2023-10-22T18:27:59.651Z'
      numEdits: 0
      reactions: []
    id: 653569af4ac2e2d605740b23
    type: comment
  author: sergeyzen
  content: 'I am able to run in on M1 Max 64GB. Not super fast, but it works


    ```llama_print_timings:        load time =  3652.09 ms

    llama_print_timings:      sample time =  1804.24 ms /   729 runs   (    2.47 ms
    per token,   404.05 tokens per second)

    llama_print_timings: prompt eval time =  3652.04 ms /   144 tokens (   25.36 ms
    per token,    39.43 tokens per second)

    llama_print_timings:        eval time = 94289.78 ms /   728 runs   (  129.52 ms
    per token,     7.72 tokens per second)

    llama_print_timings:       total time = 100932.23 ms

    Output generated in 101.16 seconds (7.20 tokens/s, 728 tokens, context 144, seed
    1690939106)

    Llama.generate: prefix-match hit


    llama_print_timings:        load time =  3652.09 ms

    llama_print_timings:      sample time =  2548.89 ms /  1024 runs   (    2.49 ms
    per token,   401.74 tokens per second)

    llama_print_timings: prompt eval time = 13158.02 ms /   751 tokens (   17.52 ms
    per token,    57.08 tokens per second)

    llama_print_timings:        eval time = 141916.85 ms /  1023 runs   (  138.73
    ms per token,     7.21 tokens per second)

    llama_print_timings:       total time = 159473.00 ms

    Output generated in 159.71 seconds (6.41 tokens/s, 1024 tokens, context 886, seed
    1686911609)

    Llama.generate: prefix-match hit


    llama_print_timings:        load time =  3652.09 ms

    llama_print_timings:      sample time =   694.30 ms /   276 runs   (    2.52 ms
    per token,   397.52 tokens per second)

    llama_print_timings: prompt eval time = 19746.02 ms /  1023 tokens (   19.30 ms
    per token,    51.81 tokens per second)

    llama_print_timings:        eval time = 43975.35 ms /   275 runs   (  159.91 ms
    per token,     6.25 tokens per second)

    llama_print_timings:       total time = 64842.96 ms

    Output generated in 65.07 seconds (4.23 tokens/s, 275 tokens, context 1909, seed
    828516400)

    ```'
  created_at: 2023-10-22 17:27:59+00:00
  edited: false
  hidden: false
  id: 653569af4ac2e2d605740b23
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: WizardLM/WizardCoder-Python-34B-V1.0
repo_type: model
status: open
target_branch: null
title: System requirement?
