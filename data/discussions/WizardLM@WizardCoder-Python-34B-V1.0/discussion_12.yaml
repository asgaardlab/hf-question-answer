!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Teddydj
conflicting_files: null
created_at: 2023-08-28 16:13:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bc0a04e0f86fb9725b5fbc06a4fcb5d.svg
      fullname: Chama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Teddydj
      type: user
    createdAt: '2023-08-28T17:13:12.000Z'
    data:
      edited: true
      editors:
      - Teddydj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9572328329086304
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bc0a04e0f86fb9725b5fbc06a4fcb5d.svg
          fullname: Chama
          isHf: false
          isPro: false
          name: Teddydj
          type: user
        html: '<p>Hello,<br>A 4090 can use WizardLM 33B but  for WizardCoder34B it
          doesn''t seems to be enough, does someone know how much VRAM WizardCoder34B
          use ?<br>i cant find the information</p>

          <p>thanks you</p>

          '
        raw: 'Hello,

          A 4090 can use WizardLM 33B but  for WizardCoder34B it doesn''t seems to
          be enough, does someone know how much VRAM WizardCoder34B use ?

          i cant find the information


          thanks you'
        updatedAt: '2023-08-28T17:15:11.224Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Norrin-Radd
    id: 64ecd5a8c571d65496f3f6f9
    type: comment
  author: Teddydj
  content: 'Hello,

    A 4090 can use WizardLM 33B but  for WizardCoder34B it doesn''t seems to be enough,
    does someone know how much VRAM WizardCoder34B use ?

    i cant find the information


    thanks you'
  created_at: 2023-08-28 16:13:12+00:00
  edited: true
  hidden: false
  id: 64ecd5a8c571d65496f3f6f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24badcc325b7a93fa4e27e20fda2ce3d.svg
      fullname: Richard Vizi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahmabuk
      type: user
    createdAt: '2023-08-28T18:12:37.000Z'
    data:
      edited: false
      editors:
      - mahmabuk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9462835192680359
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24badcc325b7a93fa4e27e20fda2ce3d.svg
          fullname: Richard Vizi
          isHf: false
          isPro: false
          name: mahmabuk
          type: user
        html: '<p>So no point to try with a 4080? Sounds very sad :)</p>

          '
        raw: So no point to try with a 4080? Sounds very sad :)
        updatedAt: '2023-08-28T18:12:37.319Z'
      numEdits: 0
      reactions: []
    id: 64ece39560b13c0dd103f5be
    type: comment
  author: mahmabuk
  content: So no point to try with a 4080? Sounds very sad :)
  created_at: 2023-08-28 17:12:37+00:00
  edited: false
  hidden: false
  id: 64ece39560b13c0dd103f5be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bc0a04e0f86fb9725b5fbc06a4fcb5d.svg
      fullname: Chama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Teddydj
      type: user
    createdAt: '2023-08-28T22:29:50.000Z'
    data:
      edited: false
      editors:
      - Teddydj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8141459226608276
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bc0a04e0f86fb9725b5fbc06a4fcb5d.svg
          fullname: Chama
          isHf: false
          isPro: false
          name: Teddydj
          type: user
        html: '<p>This repo seems to have a better version/ more info <a href="https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF">https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF</a></p>

          '
        raw: This repo seems to have a better version/ more info https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF
        updatedAt: '2023-08-28T22:29:50.819Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mirek190
    id: 64ed1fdeac5b2353a62ee11d
    type: comment
  author: Teddydj
  content: This repo seems to have a better version/ more info https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF
  created_at: 2023-08-28 21:29:50+00:00
  edited: false
  hidden: false
  id: 64ed1fdeac5b2353a62ee11d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-08-28T23:27:29.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9704530835151672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>yep I''m using GGUF q4k_m version as it fits fully in vram ( all
          layers ) and has less perplexity than any GPTQ  obsolete version.<br>With
          RTX 3090 have around 30 t/s.  </p>

          '
        raw: 'yep I''m using GGUF q4k_m version as it fits fully in vram ( all layers
          ) and has less perplexity than any GPTQ  obsolete version.

          With RTX 3090 have around 30 t/s.  '
        updatedAt: '2023-08-28T23:28:38.073Z'
      numEdits: 1
      reactions: []
    id: 64ed2d61318f088926bac22e
    type: comment
  author: mirek190
  content: 'yep I''m using GGUF q4k_m version as it fits fully in vram ( all layers
    ) and has less perplexity than any GPTQ  obsolete version.

    With RTX 3090 have around 30 t/s.  '
  created_at: 2023-08-28 22:27:29+00:00
  edited: true
  hidden: false
  id: 64ed2d61318f088926bac22e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&h=200&f=face
      fullname: WizardLM
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: WizardLM
      type: user
    createdAt: '2023-09-14T07:00:14.000Z'
    data:
      status: closed
    id: 6502af7e64b7438a023e0b9c
    type: status-change
  author: WizardLM
  created_at: 2023-09-14 06:00:14+00:00
  id: 6502af7e64b7438a023e0b9c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: WizardLM/WizardCoder-Python-34B-V1.0
repo_type: model
status: closed
target_branch: null
title: Any idea how much VRAM does this use ?
