!!python/object:huggingface_hub.community.DiscussionWithDetails
author: UphamProjects
conflicting_files: null
created_at: 2024-01-02 17:15:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a239a39084ee703be4f61b8c7687aa5.svg
      fullname: Evan Upham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: UphamProjects
      type: user
    createdAt: '2024-01-02T17:15:04.000Z'
    data:
      edited: false
      editors:
      - UphamProjects
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9508549571037292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a239a39084ee703be4f61b8c7687aa5.svg
          fullname: Evan Upham
          isHf: false
          isPro: false
          name: UphamProjects
          type: user
        html: '<p>I''m trying to train the model and I can get the script to work,
          but the training time is kind of long.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/645979895e44a749d5a4d558/pOCSoGSCD4b7UrKzHLxnW.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/645979895e44a749d5a4d558/pOCSoGSCD4b7UrKzHLxnW.png"></a></p>

          <p>I''m likely doing something wrong but any enlightenment would be appreciated.  I''m
          not planning to use that exact dataset, but I have another one I''d like
          to pass the model, but I''m trying to see how fast it can be trained.</p>

          '
        raw: "I'm trying to train the model and I can get the script to work, but\
          \ the training time is kind of long.\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/645979895e44a749d5a4d558/pOCSoGSCD4b7UrKzHLxnW.png)\r\
          \n\r\nI'm likely doing something wrong but any enlightenment would be appreciated.\
          \  I'm not planning to use that exact dataset, but I have another one I'd\
          \ like to pass the model, but I'm trying to see how fast it can be trained."
        updatedAt: '2024-01-02T17:15:04.201Z'
      numEdits: 0
      reactions: []
    id: 659444988fec845e50a742ee
    type: comment
  author: UphamProjects
  content: "I'm trying to train the model and I can get the script to work, but the\
    \ training time is kind of long.\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/645979895e44a749d5a4d558/pOCSoGSCD4b7UrKzHLxnW.png)\r\
    \n\r\nI'm likely doing something wrong but any enlightenment would be appreciated.\
    \  I'm not planning to use that exact dataset, but I have another one I'd like\
    \ to pass the model, but I'm trying to see how fast it can be trained."
  created_at: 2024-01-02 17:15:04+00:00
  edited: false
  hidden: false
  id: 659444988fec845e50a742ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2024-01-02T17:36:46.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9798781871795654
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: '<p>Hi you are right but i cant solve at the moment. Some peoples like
          you , get some issues about training time. idk why is too long. I''ll solve
          this , maybe i will change implentation.</p>

          '
        raw: Hi you are right but i cant solve at the moment. Some peoples like you
          , get some issues about training time. idk why is too long. I'll solve this
          , maybe i will change implentation.
        updatedAt: '2024-01-02T17:36:46.426Z'
      numEdits: 0
      reactions: []
    id: 659449aea78a27780359221c
    type: comment
  author: Q-bert
  content: Hi you are right but i cant solve at the moment. Some peoples like you
    , get some issues about training time. idk why is too long. I'll solve this ,
    maybe i will change implentation.
  created_at: 2024-01-02 17:36:46+00:00
  edited: false
  hidden: false
  id: 659449aea78a27780359221c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
      fullname: Ja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kabumbus
      type: user
    createdAt: '2024-01-02T18:23:00.000Z'
    data:
      edited: false
      editors:
      - Kabumbus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9716483354568481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
          fullname: Ja
          isHf: false
          isPro: false
          name: Kabumbus
          type: user
        html: "<p>How fast is it in your experience <span data-props=\"{&quot;user&quot;:&quot;Q-bert&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Q-bert\"\
          >@<span class=\"underline\">Q-bert</span></a></span>\n\n\t</span></span>?</p>\n"
        raw: How fast is it in your experience @Q-bert?
        updatedAt: '2024-01-02T18:23:00.624Z'
      numEdits: 0
      reactions: []
    id: 6594548468d0b763315eb377
    type: comment
  author: Kabumbus
  content: How fast is it in your experience @Q-bert?
  created_at: 2024-01-02 18:23:00+00:00
  edited: false
  hidden: false
  id: 6594548468d0b763315eb377
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a239a39084ee703be4f61b8c7687aa5.svg
      fullname: Evan Upham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: UphamProjects
      type: user
    createdAt: '2024-01-04T22:44:50.000Z'
    data:
      edited: false
      editors:
      - UphamProjects
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9178011417388916
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a239a39084ee703be4f61b8c7687aa5.svg
          fullname: Evan Upham
          isHf: false
          isPro: false
          name: UphamProjects
          type: user
        html: '<p>I''m trying to see the difference between your model and using the
          architecture used here <a rel="nofollow" href="https://github.com/state-spaces/mamba/tree/main">https://github.com/state-spaces/mamba/tree/main</a>.  I
          can train both but your script takes much longer to train through the state-spaces''
          implementation.   I also can''t seem to load your model into the state spaces
          architecture like I can cli-brain''s and haven-hq''s implementation.  Your
          370 is a better starting point vs <a href="https://huggingface.co/state-spaces/mamba-370m">https://huggingface.co/state-spaces/mamba-370m</a>
          for fine tuning, but the difference in train speed is not insignificant
          and I can''t figure out what you''re doing differently. </p>

          <p>Here''s the code for how I ran through a training with the 370m from
          state-spaces using your train method - <a rel="nofollow" href="https://colab.research.google.com/drive/1ShX0bE0OuDyBOR_7YzuhasSe02zzbUy-?usp=sharing">https://colab.research.google.com/drive/1ShX0bE0OuDyBOR_7YzuhasSe02zzbUy-?usp=sharing</a><br>
          It''s the difference of 8 hours for the databricks train set for the above
          and 8 days for the below (I either have imdb or databricks in there now
          but it was still a matter of days)<br>This is how I trained using yours
          - <a rel="nofollow" href="https://colab.research.google.com/drive/199DTxoqJFRwrsykIbZpuIxVd40RCP-LJ?usp=sharing">https://colab.research.google.com/drive/199DTxoqJFRwrsykIbZpuIxVd40RCP-LJ?usp=sharing</a></p>

          <p>The inference is faster for the 2.8b here <a href="https://huggingface.co/clibrain/mamba-2.8b-instruct-openhermes">https://huggingface.co/clibrain/mamba-2.8b-instruct-openhermes</a>
          if it''s done through the state-spaces architecture directly but I can''t
          load your 370m.  The 370m you put out performs well, but hangs on a sequence
          of say 2000, whereas the 2.8b I used here can tackle full email chains(which
          tend between 1.5 and 3k tokens) about 500-700 an hour with t4 gpu on colab.
          </p>

          '
        raw: "I'm trying to see the difference between your model and using the architecture\
          \ used here https://github.com/state-spaces/mamba/tree/main.  I can train\
          \ both but your script takes much longer to train through the state-spaces'\
          \ implementation.   I also can't seem to load your model into the state\
          \ spaces architecture like I can cli-brain's and haven-hq's implementation.\
          \  Your 370 is a better starting point vs https://huggingface.co/state-spaces/mamba-370m\
          \ for fine tuning, but the difference in train speed is not insignificant\
          \ and I can't figure out what you're doing differently. \n\nHere's the code\
          \ for how I ran through a training with the 370m from state-spaces using\
          \ your train method - https://colab.research.google.com/drive/1ShX0bE0OuDyBOR_7YzuhasSe02zzbUy-?usp=sharing\n\
          \ It's the difference of 8 hours for the databricks train set for the above\
          \ and 8 days for the below (I either have imdb or databricks in there now\
          \ but it was still a matter of days)\nThis is how I trained using yours\
          \ - https://colab.research.google.com/drive/199DTxoqJFRwrsykIbZpuIxVd40RCP-LJ?usp=sharing\n\
          \nThe inference is faster for the 2.8b here https://huggingface.co/clibrain/mamba-2.8b-instruct-openhermes\
          \ if it's done through the state-spaces architecture directly but I can't\
          \ load your 370m.  The 370m you put out performs well, but hangs on a sequence\
          \ of say 2000, whereas the 2.8b I used here can tackle full email chains(which\
          \ tend between 1.5 and 3k tokens) about 500-700 an hour with t4 gpu on colab. "
        updatedAt: '2024-01-04T22:44:50.510Z'
      numEdits: 0
      reactions: []
    id: 659734e2ccd854bca54c3dbb
    type: comment
  author: UphamProjects
  content: "I'm trying to see the difference between your model and using the architecture\
    \ used here https://github.com/state-spaces/mamba/tree/main.  I can train both\
    \ but your script takes much longer to train through the state-spaces' implementation.\
    \   I also can't seem to load your model into the state spaces architecture like\
    \ I can cli-brain's and haven-hq's implementation.  Your 370 is a better starting\
    \ point vs https://huggingface.co/state-spaces/mamba-370m for fine tuning, but\
    \ the difference in train speed is not insignificant and I can't figure out what\
    \ you're doing differently. \n\nHere's the code for how I ran through a training\
    \ with the 370m from state-spaces using your train method - https://colab.research.google.com/drive/1ShX0bE0OuDyBOR_7YzuhasSe02zzbUy-?usp=sharing\n\
    \ It's the difference of 8 hours for the databricks train set for the above and\
    \ 8 days for the below (I either have imdb or databricks in there now but it was\
    \ still a matter of days)\nThis is how I trained using yours - https://colab.research.google.com/drive/199DTxoqJFRwrsykIbZpuIxVd40RCP-LJ?usp=sharing\n\
    \nThe inference is faster for the 2.8b here https://huggingface.co/clibrain/mamba-2.8b-instruct-openhermes\
    \ if it's done through the state-spaces architecture directly but I can't load\
    \ your 370m.  The 370m you put out performs well, but hangs on a sequence of say\
    \ 2000, whereas the 2.8b I used here can tackle full email chains(which tend between\
    \ 1.5 and 3k tokens) about 500-700 an hour with t4 gpu on colab. "
  created_at: 2024-01-04 22:44:50+00:00
  edited: false
  hidden: false
  id: 659734e2ccd854bca54c3dbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c298e978b54f52f41c6c1c4b50f5b7c.svg
      fullname: loic fosse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loicfosse65
      type: user
    createdAt: '2024-01-18T10:15:42.000Z'
    data:
      edited: false
      editors:
      - loicfosse65
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772101640701294
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c298e978b54f52f41c6c1c4b50f5b7c.svg
          fullname: loic fosse
          isHf: false
          isPro: false
          name: loicfosse65
          type: user
        html: '<p>Hi, I''m facing the exact same issue, but when you look at the mamba
          paper, they talk about different type of memory on the GPU and in there
          code they made some serious optimization of the memory. This is why to run
          their model you need CUDA and NVCC.</p>

          '
        raw: Hi, I'm facing the exact same issue, but when you look at the mamba paper,
          they talk about different type of memory on the GPU and in there code they
          made some serious optimization of the memory. This is why to run their model
          you need CUDA and NVCC.
        updatedAt: '2024-01-18T10:15:42.622Z'
      numEdits: 0
      reactions: []
    id: 65a8fa4ea92d5908df3efba5
    type: comment
  author: loicfosse65
  content: Hi, I'm facing the exact same issue, but when you look at the mamba paper,
    they talk about different type of memory on the GPU and in there code they made
    some serious optimization of the memory. This is why to run their model you need
    CUDA and NVCC.
  created_at: 2024-01-18 10:15:42+00:00
  edited: false
  hidden: false
  id: 65a8fa4ea92d5908df3efba5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Q-bert/Mamba-370M
repo_type: model
status: open
target_branch: null
title: Train Time
