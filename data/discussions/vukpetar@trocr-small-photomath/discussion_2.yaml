!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chrisxx
conflicting_files: null
created_at: 2023-04-03 08:11:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3fb59fd054163c8afbdfbee44ee49807.svg
      fullname: Chris Wendler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chrisxx
      type: user
    createdAt: '2023-04-03T09:11:00.000Z'
    data:
      edited: false
      editors:
      - chrisxx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3fb59fd054163c8afbdfbee44ee49807.svg
          fullname: Chris Wendler
          isHf: false
          isPro: false
          name: chrisxx
          type: user
        html: '<p>Hi, </p>

          <p>I get this problem: </p>

          <p>''''''<br>ValueError                                Traceback (most recent
          call last)</p>

          <p> in &lt;cell line: 13&gt;()<br>     11<br>     12 feature_extractor =
          AutoFeatureExtractor.from_pretrained(''vukpetar/trocr-small-photomath'')<br>---&gt;
          13 tokenizer = AutoTokenizer.from_pretrained("vukpetar/trocr-small-photomath")<br>     14
          model = VisionEncoderDecoderModel.from_pretrained(''vukpetar/trocr-small-photomath'')<br>     15
          pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values</p>

          <p>4 frames</p>

          <p>/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_fast.py
          in <strong>init</strong>(self, *args, **kwargs)<br>    118             fast_tokenizer
          = convert_slow_tokenizer(slow_tokenizer)<br>    119         else:<br>--&gt;
          120             raise ValueError(<br>    121                 "Couldn''t
          instantiate the backend tokenizer from one of: \n"<br>    122                 "(1)
          a <code>tokenizers</code> library serialization file, \n"</p>

          <p>ValueError: Couldn''t instantiate the backend tokenizer from one of:<br>(1)
          a <code>tokenizers</code> library serialization file,<br>(2) a slow tokenizer
          instance to convert or<br>(3) an equivalent slow tokenizer class to instantiate
          and convert.<br>You need to have sentencepiece installed to convert a slow
          tokenizer to a fast one.<br>''''''</p>

          '
        raw: "Hi, \r\n\r\nI get this problem: \r\n\r\n'''\r\nValueError          \
          \                      Traceback (most recent call last)\r\n\r\n<ipython-input-4-c6a0088b7844>\
          \ in <cell line: 13>()\r\n     11 \r\n     12 feature_extractor = AutoFeatureExtractor.from_pretrained('vukpetar/trocr-small-photomath')\r\
          \n---> 13 tokenizer = AutoTokenizer.from_pretrained(\"vukpetar/trocr-small-photomath\"\
          )\r\n     14 model = VisionEncoderDecoderModel.from_pretrained('vukpetar/trocr-small-photomath')\r\
          \n     15 pixel_values = feature_extractor(images=image, return_tensors=\"\
          pt\").pixel_values\r\n\r\n4 frames\r\n\r\n/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_fast.py\
          \ in __init__(self, *args, **kwargs)\r\n    118             fast_tokenizer\
          \ = convert_slow_tokenizer(slow_tokenizer)\r\n    119         else:\r\n\
          --> 120             raise ValueError(\r\n    121                 \"Couldn't\
          \ instantiate the backend tokenizer from one of: \\n\"\r\n    122      \
          \           \"(1) a `tokenizers` library serialization file, \\n\"\r\n\r\
          \nValueError: Couldn't instantiate the backend tokenizer from one of: \r\
          \n(1) a `tokenizers` library serialization file, \r\n(2) a slow tokenizer\
          \ instance to convert or \r\n(3) an equivalent slow tokenizer class to instantiate\
          \ and convert. \r\nYou need to have sentencepiece installed to convert a\
          \ slow tokenizer to a fast one.\r\n'''\r\n"
        updatedAt: '2023-04-03T09:11:00.151Z'
      numEdits: 0
      reactions: []
    id: 642a9824508b7246f9d0c338
    type: comment
  author: chrisxx
  content: "Hi, \r\n\r\nI get this problem: \r\n\r\n'''\r\nValueError            \
    \                    Traceback (most recent call last)\r\n\r\n<ipython-input-4-c6a0088b7844>\
    \ in <cell line: 13>()\r\n     11 \r\n     12 feature_extractor = AutoFeatureExtractor.from_pretrained('vukpetar/trocr-small-photomath')\r\
    \n---> 13 tokenizer = AutoTokenizer.from_pretrained(\"vukpetar/trocr-small-photomath\"\
    )\r\n     14 model = VisionEncoderDecoderModel.from_pretrained('vukpetar/trocr-small-photomath')\r\
    \n     15 pixel_values = feature_extractor(images=image, return_tensors=\"pt\"\
    ).pixel_values\r\n\r\n4 frames\r\n\r\n/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_fast.py\
    \ in __init__(self, *args, **kwargs)\r\n    118             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
    \n    119         else:\r\n--> 120             raise ValueError(\r\n    121  \
    \               \"Couldn't instantiate the backend tokenizer from one of: \\n\"\
    \r\n    122                 \"(1) a `tokenizers` library serialization file, \\\
    n\"\r\n\r\nValueError: Couldn't instantiate the backend tokenizer from one of:\
    \ \r\n(1) a `tokenizers` library serialization file, \r\n(2) a slow tokenizer\
    \ instance to convert or \r\n(3) an equivalent slow tokenizer class to instantiate\
    \ and convert. \r\nYou need to have sentencepiece installed to convert a slow\
    \ tokenizer to a fast one.\r\n'''\r\n"
  created_at: 2023-04-03 08:11:00+00:00
  edited: false
  hidden: false
  id: 642a9824508b7246f9d0c338
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: vukpetar/trocr-small-photomath
repo_type: model
status: open
target_branch: null
title: Problem with tokenizer
