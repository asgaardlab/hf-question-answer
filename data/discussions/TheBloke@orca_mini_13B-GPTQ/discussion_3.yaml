!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dspyrhsu
conflicting_files: null
created_at: 2023-07-09 10:54:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
      fullname: Reinhold von Schwerin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dspyrhsu
      type: user
    createdAt: '2023-07-09T11:54:18.000Z'
    data:
      edited: true
      editors:
      - dspyrhsu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.932931125164032
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
          fullname: Reinhold von Schwerin
          isHf: false
          isPro: false
          name: dspyrhsu
          type: user
        html: "<p>For every one else who is deperately searching for a usable text\
          \ generation webui (especially the one TheBloke seems to be talking about\
          \ in his model card), this is where to find it: <a rel=\"nofollow\" href=\"\
          https://github.com/oobabooga/text-generation-webui\">https://github.com/oobabooga/text-generation-webui</a>.\
          \ After following the installation instructions there, I had an interface\
          \ with the components mentioned in the model card, so I assume this is the\
          \ one.</p>\n<p>Now to the problem: When trying to activate the model, I\
          \ get an error in modules/GPTQ_loader.py</p>\n<p>line 17, in import llama_inference_offload\
          \ ModuleNotFoundError: No module named \u2018llama_inference_offload\u2019\
          </p>\n<p>How to proceed? Any help would be appreciated!</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> maybe it would make sense to mention that directly in\
          \ your model card. searching for the term gives many results (especially\
          \ when combined with \"huggingface\") which are not helpful at all - at\
          \ least it did in my case</p>\n"
        raw: "For every one else who is deperately searching for a usable text generation\
          \ webui (especially the one TheBloke seems to be talking about in his model\
          \ card), this is where to find it: https://github.com/oobabooga/text-generation-webui.\
          \ After following the installation instructions there, I had an interface\
          \ with the components mentioned in the model card, so I assume this is the\
          \ one.\n\nNow to the problem: When trying to activate the model, I get an\
          \ error in modules/GPTQ_loader.py\n\nline 17, in import llama_inference_offload\
          \ ModuleNotFoundError: No module named \u2018llama_inference_offload\u2019\
          \n\nHow to proceed? Any help would be appreciated!\n\n@TheBloke maybe it\
          \ would make sense to mention that directly in your model card. searching\
          \ for the term gives many results (especially when combined with \"huggingface\"\
          ) which are not helpful at all - at least it did in my case"
        updatedAt: '2023-07-09T11:54:42.620Z'
      numEdits: 1
      reactions: []
    id: 64aa9fea9915c39308c93692
    type: comment
  author: dspyrhsu
  content: "For every one else who is deperately searching for a usable text generation\
    \ webui (especially the one TheBloke seems to be talking about in his model card),\
    \ this is where to find it: https://github.com/oobabooga/text-generation-webui.\
    \ After following the installation instructions there, I had an interface with\
    \ the components mentioned in the model card, so I assume this is the one.\n\n\
    Now to the problem: When trying to activate the model, I get an error in modules/GPTQ_loader.py\n\
    \nline 17, in import llama_inference_offload ModuleNotFoundError: No module named\
    \ \u2018llama_inference_offload\u2019\n\nHow to proceed? Any help would be appreciated!\n\
    \n@TheBloke maybe it would make sense to mention that directly in your model card.\
    \ searching for the term gives many results (especially when combined with \"\
    huggingface\") which are not helpful at all - at least it did in my case"
  created_at: 2023-07-09 10:54:18+00:00
  edited: true
  hidden: false
  id: 64aa9fea9915c39308c93692
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-09T12:01:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8462033271789551
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I have clarified the README thus:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/4LhLcOpFRo5ZRnvHLqkhA.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/4LhLcOpFRo5ZRnvHLqkhA.png"></a></p>

          <p>Please use the text-generation-webui one-click installer if you didn''t
          already. And then set Loader to ExLlama. I thought that was the default
          actually. It should work out of the box with the one-click installer and
          following the instructions in the README</p>

          '
        raw: 'I have clarified the README thus:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/4LhLcOpFRo5ZRnvHLqkhA.png)


          Please use the text-generation-webui one-click installer if you didn''t
          already. And then set Loader to ExLlama. I thought that was the default
          actually. It should work out of the box with the one-click installer and
          following the instructions in the README'
        updatedAt: '2023-07-09T12:01:08.975Z'
      numEdits: 0
      reactions: []
    id: 64aaa184d4a402e8dcdde2f6
    type: comment
  author: TheBloke
  content: 'I have clarified the README thus:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/4LhLcOpFRo5ZRnvHLqkhA.png)


    Please use the text-generation-webui one-click installer if you didn''t already.
    And then set Loader to ExLlama. I thought that was the default actually. It should
    work out of the box with the one-click installer and following the instructions
    in the README'
  created_at: 2023-07-09 11:01:08+00:00
  edited: false
  hidden: false
  id: 64aaa184d4a402e8dcdde2f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
      fullname: Reinhold von Schwerin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dspyrhsu
      type: user
    createdAt: '2023-07-09T12:10:33.000Z'
    data:
      edited: false
      editors:
      - dspyrhsu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9697107076644897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
          fullname: Reinhold von Schwerin
          isHf: false
          isPro: false
          name: dspyrhsu
          type: user
        html: '<p>Thanks for the quick reply. I will have to try this again, I think,
          since I used the conda (or rather mamba) install on an older machine but
          now, at least, I get a CUDA OOM message, which is better, I guess ... I
          will try this on another machine with a RTX 2070 at least and see what happens
          there ...</p>

          '
        raw: Thanks for the quick reply. I will have to try this again, I think, since
          I used the conda (or rather mamba) install on an older machine but now,
          at least, I get a CUDA OOM message, which is better, I guess ... I will
          try this on another machine with a RTX 2070 at least and see what happens
          there ...
        updatedAt: '2023-07-09T12:10:33.498Z'
      numEdits: 0
      reactions: []
    id: 64aaa3b999639cc3127ae197
    type: comment
  author: dspyrhsu
  content: Thanks for the quick reply. I will have to try this again, I think, since
    I used the conda (or rather mamba) install on an older machine but now, at least,
    I get a CUDA OOM message, which is better, I guess ... I will try this on another
    machine with a RTX 2070 at least and see what happens there ...
  created_at: 2023-07-09 11:10:33+00:00
  edited: false
  hidden: false
  id: 64aaa3b999639cc3127ae197
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-09T12:12:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9378424286842346
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Does that only have 6GB VRAM? If so,  you''re going to struggle.  You
          need 10GB minimum to load a 13B GPTQ with ExLlama. You can use text-generation-webui''s
          <code>pre_layer</code> to offload some to RAM but it will be very slow.</p>

          <p>Using a GGML might be the better option for you, as that performs much
          better when partially on GPU and partially in RAM.</p>

          '
        raw: 'Does that only have 6GB VRAM? If so,  you''re going to struggle.  You
          need 10GB minimum to load a 13B GPTQ with ExLlama. You can use text-generation-webui''s
          `pre_layer` to offload some to RAM but it will be very slow.


          Using a GGML might be the better option for you, as that performs much better
          when partially on GPU and partially in RAM.'
        updatedAt: '2023-07-09T12:12:11.915Z'
      numEdits: 0
      reactions: []
    id: 64aaa41bc7ef738e98e7919e
    type: comment
  author: TheBloke
  content: 'Does that only have 6GB VRAM? If so,  you''re going to struggle.  You
    need 10GB minimum to load a 13B GPTQ with ExLlama. You can use text-generation-webui''s
    `pre_layer` to offload some to RAM but it will be very slow.


    Using a GGML might be the better option for you, as that performs much better
    when partially on GPU and partially in RAM.'
  created_at: 2023-07-09 11:12:11+00:00
  edited: false
  hidden: false
  id: 64aaa41bc7ef738e98e7919e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
      fullname: Reinhold von Schwerin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dspyrhsu
      type: user
    createdAt: '2023-07-09T12:16:06.000Z'
    data:
      edited: false
      editors:
      - dspyrhsu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9743480086326599
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
          fullname: Reinhold von Schwerin
          isHf: false
          isPro: false
          name: dspyrhsu
          type: user
        html: '<p>Actually, it has 8 GB VRAM AFAIK (will check in a minute), but even
          that seems too little then. I have access to other machines, just not here
          at home ... Thanks for the hints, anyway!</p>

          '
        raw: Actually, it has 8 GB VRAM AFAIK (will check in a minute), but even that
          seems too little then. I have access to other machines, just not here at
          home ... Thanks for the hints, anyway!
        updatedAt: '2023-07-09T12:16:06.182Z'
      numEdits: 0
      reactions: []
    id: 64aaa50651756fb15c12a482
    type: comment
  author: dspyrhsu
  content: Actually, it has 8 GB VRAM AFAIK (will check in a minute), but even that
    seems too little then. I have access to other machines, just not here at home
    ... Thanks for the hints, anyway!
  created_at: 2023-07-09 11:16:06+00:00
  edited: false
  hidden: false
  id: 64aaa50651756fb15c12a482
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-09T12:17:09.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9649974703788757
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>If you just want to get started quickly, get a 7B model (like Orca
          Mini v2 7B GPTQ) - that will fit in 8GB and with ExLlama should fit in 6GB
          too actually now I think of it.</p>

          '
        raw: If you just want to get started quickly, get a 7B model (like Orca Mini
          v2 7B GPTQ) - that will fit in 8GB and with ExLlama should fit in 6GB too
          actually now I think of it.
        updatedAt: '2023-07-09T12:17:09.213Z'
      numEdits: 0
      reactions: []
    id: 64aaa5455fc663fee24a1e81
    type: comment
  author: TheBloke
  content: If you just want to get started quickly, get a 7B model (like Orca Mini
    v2 7B GPTQ) - that will fit in 8GB and with ExLlama should fit in 6GB too actually
    now I think of it.
  created_at: 2023-07-09 11:17:09+00:00
  edited: false
  hidden: false
  id: 64aaa5455fc663fee24a1e81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
      fullname: Reinhold von Schwerin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dspyrhsu
      type: user
    createdAt: '2023-07-09T12:20:33.000Z'
    data:
      edited: false
      editors:
      - dspyrhsu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9588655829429626
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
          fullname: Reinhold von Schwerin
          isHf: false
          isPro: false
          name: dspyrhsu
          type: user
        html: '<p>Great, yes, I am just getting started and would like to tinker around
          a bit. Found your model card for it (my hint as to the link to the text-generation-webui
          also applies there ...). Will give this a try!</p>

          '
        raw: 'Great, yes, I am just getting started and would like to tinker around
          a bit. Found your model card for it (my hint as to the link to the text-generation-webui
          also applies there ...). Will give this a try!

          '
        updatedAt: '2023-07-09T12:20:33.963Z'
      numEdits: 0
      reactions: []
    id: 64aaa611b6512b83282d08b2
    type: comment
  author: dspyrhsu
  content: 'Great, yes, I am just getting started and would like to tinker around
    a bit. Found your model card for it (my hint as to the link to the text-generation-webui
    also applies there ...). Will give this a try!

    '
  created_at: 2023-07-09 11:20:33+00:00
  edited: false
  hidden: false
  id: 64aaa611b6512b83282d08b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
      fullname: Reinhold von Schwerin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dspyrhsu
      type: user
    createdAt: '2023-07-09T14:44:57.000Z'
    data:
      edited: false
      editors:
      - dspyrhsu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9700382947921753
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666441641849-6353e105b8aa3b0b3eba4ad5.jpeg?w=200&h=200&f=face
          fullname: Reinhold von Schwerin
          isHf: false
          isPro: false
          name: dspyrhsu
          type: user
        html: '<p>Just wanted to let you know that it works like a charm (and the
          RTX 2070 has 8GB of VRAM, of which 6.2 are used when running the model).
          Asking it "How do I use HuggingFace''s textgeneration web ui?" it comes
          up with a sensible looking answer, but hallucinates the URL "<a rel="nofollow"
          href="https://text-generation.huggingface.co/&quot;">https://text-generation.huggingface.co/"</a>,
          but this looks very promising! Thanks for your work and for your immediate
          help! One thing though: maybe you could also include the hint about having
          to uses the ExLlama loader in your instructions? I would never have known
          had you not told me above.</p>

          '
        raw: 'Just wanted to let you know that it works like a charm (and the RTX
          2070 has 8GB of VRAM, of which 6.2 are used when running the model). Asking
          it "How do I use HuggingFace''s textgeneration web ui?" it comes up with
          a sensible looking answer, but hallucinates the URL "https://text-generation.huggingface.co/",
          but this looks very promising! Thanks for your work and for your immediate
          help! One thing though: maybe you could also include the hint about having
          to uses the ExLlama loader in your instructions? I would never have known
          had you not told me above.'
        updatedAt: '2023-07-09T14:44:57.148Z'
      numEdits: 0
      reactions: []
    id: 64aac7e94a702899838de405
    type: comment
  author: dspyrhsu
  content: 'Just wanted to let you know that it works like a charm (and the RTX 2070
    has 8GB of VRAM, of which 6.2 are used when running the model). Asking it "How
    do I use HuggingFace''s textgeneration web ui?" it comes up with a sensible looking
    answer, but hallucinates the URL "https://text-generation.huggingface.co/", but
    this looks very promising! Thanks for your work and for your immediate help! One
    thing though: maybe you could also include the hint about having to uses the ExLlama
    loader in your instructions? I would never have known had you not told me above.'
  created_at: 2023-07-09 13:44:57+00:00
  edited: false
  hidden: false
  id: 64aac7e94a702899838de405
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/orca_mini_13B-GPTQ
repo_type: model
status: open
target_branch: null
title: text generation webui / Error in GPTQ_loader.py
