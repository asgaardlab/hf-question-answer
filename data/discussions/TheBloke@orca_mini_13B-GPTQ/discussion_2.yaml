!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mgleize
conflicting_files: null
created_at: 2023-07-01 08:59:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/MXaGDS8oet-KWkI0G2fmb.png?w=200&h=200&f=face
      fullname: Martin Gleize
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mgleize
      type: user
    createdAt: '2023-07-01T09:59:06.000Z'
    data:
      edited: true
      editors:
      - mgleize
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9576053023338318
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/MXaGDS8oet-KWkI0G2fmb.png?w=200&h=200&f=face
          fullname: Martin Gleize
          isHf: false
          isPro: false
          name: mgleize
          type: user
        html: '<p>Hi,</p>

          <p>I noticed this model doesn''t have ".g_idx" tensors, which apparently
          my inference code is looking for (latest source version of HF''s text-generation-inference).
          Some of your other models do have them (like stable-vicuna-13B-GPTQ). I
          understand this possibly has something to do with GPTQ-for-LLaMa. Could
          anyone explain what these do at a high level? Is there a way to bypass this
          with my code or are these vital?</p>

          <p>AutoGPTQ is unfortunately not the option I''d like to retain atm (I''m
          forced to disable Triton due to hardware bugs and it''s very slow).</p>

          <p>Btw thank you (TheBloke) for all your work getting these quantized models
          out so fast, truly a boon :).</p>

          '
        raw: 'Hi,


          I noticed this model doesn''t have ".g_idx" tensors, which apparently my
          inference code is looking for (latest source version of HF''s text-generation-inference).
          Some of your other models do have them (like stable-vicuna-13B-GPTQ). I
          understand this possibly has something to do with GPTQ-for-LLaMa. Could
          anyone explain what these do at a high level? Is there a way to bypass this
          with my code or are these vital?


          AutoGPTQ is unfortunately not the option I''d like to retain atm (I''m forced
          to disable Triton due to hardware bugs and it''s very slow).


          Btw thank you (TheBloke) for all your work getting these quantized models
          out so fast, truly a boon :).'
        updatedAt: '2023-07-01T10:11:32.815Z'
      numEdits: 1
      reactions: []
    id: 649ff8ea9b6e48cd6b4f670a
    type: comment
  author: mgleize
  content: 'Hi,


    I noticed this model doesn''t have ".g_idx" tensors, which apparently my inference
    code is looking for (latest source version of HF''s text-generation-inference).
    Some of your other models do have them (like stable-vicuna-13B-GPTQ). I understand
    this possibly has something to do with GPTQ-for-LLaMa. Could anyone explain what
    these do at a high level? Is there a way to bypass this with my code or are these
    vital?


    AutoGPTQ is unfortunately not the option I''d like to retain atm (I''m forced
    to disable Triton due to hardware bugs and it''s very slow).


    Btw thank you (TheBloke) for all your work getting these quantized models out
    so fast, truly a boon :).'
  created_at: 2023-07-01 08:59:06+00:00
  edited: true
  hidden: false
  id: 649ff8ea9b6e48cd6b4f670a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-01T10:07:48.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9434083700180054
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah, somehow Text Generation Interface managed to merge a version
          of GPTQ-for-LLaMa that doesn''t support the vast majority of models currently
          on HF.</p>

          <p>I specifically use an older version of GPTQ-for-LLaMa to make my quants
          because it guaranteed compatibility with the widest range of libraries and
          UIs - AutoGPTQ, text-generation-webui, KobaldAI, ExLlama.  This meant using
          an older version of the GPTQ format, but all the clients before TGI could
          support both the older format and the newer format.  I did experiment with
          making GPTQ using the newer format, but then I would get complaints from
          certain users because their UI couldn''t load it.</p>

          <p>Now TGI has implemented a version of GPTQ-for-LLaMa that can only load
          the new format.  They could have integrated AutoGPTQ which would have automatically
          supported all formats.  Or they could have implemented the same compatibility
          code as other clients have.  But they did neither.</p>

          <p>I will re-evaluate this soon and see what I can do but I can''t make
          any promises.  </p>

          '
        raw: 'Yeah, somehow Text Generation Interface managed to merge a version of
          GPTQ-for-LLaMa that doesn''t support the vast majority of models currently
          on HF.


          I specifically use an older version of GPTQ-for-LLaMa to make my quants
          because it guaranteed compatibility with the widest range of libraries and
          UIs - AutoGPTQ, text-generation-webui, KobaldAI, ExLlama.  This meant using
          an older version of the GPTQ format, but all the clients before TGI could
          support both the older format and the newer format.  I did experiment with
          making GPTQ using the newer format, but then I would get complaints from
          certain users because their UI couldn''t load it.


          Now TGI has implemented a version of GPTQ-for-LLaMa that can only load the
          new format.  They could have integrated AutoGPTQ which would have automatically
          supported all formats.  Or they could have implemented the same compatibility
          code as other clients have.  But they did neither.


          I will re-evaluate this soon and see what I can do but I can''t make any
          promises.  '
        updatedAt: '2023-07-01T10:10:57.228Z'
      numEdits: 1
      reactions: []
    id: 649ffaf4bfd4d3ffab909025
    type: comment
  author: TheBloke
  content: 'Yeah, somehow Text Generation Interface managed to merge a version of
    GPTQ-for-LLaMa that doesn''t support the vast majority of models currently on
    HF.


    I specifically use an older version of GPTQ-for-LLaMa to make my quants because
    it guaranteed compatibility with the widest range of libraries and UIs - AutoGPTQ,
    text-generation-webui, KobaldAI, ExLlama.  This meant using an older version of
    the GPTQ format, but all the clients before TGI could support both the older format
    and the newer format.  I did experiment with making GPTQ using the newer format,
    but then I would get complaints from certain users because their UI couldn''t
    load it.


    Now TGI has implemented a version of GPTQ-for-LLaMa that can only load the new
    format.  They could have integrated AutoGPTQ which would have automatically supported
    all formats.  Or they could have implemented the same compatibility code as other
    clients have.  But they did neither.


    I will re-evaluate this soon and see what I can do but I can''t make any promises.  '
  created_at: 2023-07-01 09:07:48+00:00
  edited: true
  hidden: false
  id: 649ffaf4bfd4d3ffab909025
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/MXaGDS8oet-KWkI0G2fmb.png?w=200&h=200&f=face
      fullname: Martin Gleize
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mgleize
      type: user
    createdAt: '2023-07-01T10:26:44.000Z'
    data:
      edited: false
      editors:
      - mgleize
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47127166390419006
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/MXaGDS8oet-KWkI0G2fmb.png?w=200&h=200&f=face
          fullname: Martin Gleize
          isHf: false
          isPro: false
          name: mgleize
          type: user
        html: '<p>Very clear, thank you!</p>

          '
        raw: Very clear, thank you!
        updatedAt: '2023-07-01T10:26:44.164Z'
      numEdits: 0
      reactions: []
    id: 649fff649b6e48cd6b4fedaa
    type: comment
  author: mgleize
  content: Very clear, thank you!
  created_at: 2023-07-01 09:26:44+00:00
  edited: false
  hidden: false
  id: 649fff649b6e48cd6b4fedaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659552894710-62d6e8b04bffd85386cbd29d.jpeg?w=200&h=200&f=face
      fullname: T Airan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrAiran
      type: user
    createdAt: '2023-07-31T23:47:18.000Z'
    data:
      edited: false
      editors:
      - MrAiran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8767406344413757
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659552894710-62d6e8b04bffd85386cbd29d.jpeg?w=200&h=200&f=face
          fullname: T Airan
          isHf: false
          isPro: false
          name: MrAiran
          type: user
        html: '<p>do you know which version they update GPTQ for llama? I need to
          use TGI to make simultaneous requests via api</p>

          '
        raw: do you know which version they update GPTQ for llama? I need to use TGI
          to make simultaneous requests via api
        updatedAt: '2023-07-31T23:47:18.104Z'
      numEdits: 0
      reactions: []
    id: 64c84806cb2f1bf0e7b3faad
    type: comment
  author: MrAiran
  content: do you know which version they update GPTQ for llama? I need to use TGI
    to make simultaneous requests via api
  created_at: 2023-07-31 22:47:18+00:00
  edited: false
  hidden: false
  id: 64c84806cb2f1bf0e7b3faad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/orca_mini_13B-GPTQ
repo_type: model
status: open
target_branch: null
title: g_idx tensors?
