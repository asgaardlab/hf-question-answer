!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dannoncaffeine
conflicting_files: null
created_at: 2023-12-01 18:09:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6427532b39c7e60c4b37cc4b/XIR5R3oB4PcPIJLN9oYHD.jpeg?w=200&h=200&f=face
      fullname: "dann \U0001F3F4\u200D\u2620\uFE0F"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dannoncaffeine
      type: user
    createdAt: '2023-12-01T18:09:15.000Z'
    data:
      edited: false
      editors:
      - dannoncaffeine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8026556968688965
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6427532b39c7e60c4b37cc4b/XIR5R3oB4PcPIJLN9oYHD.jpeg?w=200&h=200&f=face
          fullname: "dann \U0001F3F4\u200D\u2620\uFE0F"
          isHf: false
          isPro: false
          name: dannoncaffeine
          type: user
        html: "<h1 id=\"gpt2-124m-wikitext-v01\">GPT2-124M-wikitext-v0.1</h1>\n<p>This\
          \ is a practical hands-on result that helped me build a foundation on  \U0001F917\
          \ Transformers and \U0001F917 Datasets. I fint-tuned GPT2 on wikitext (103-raw-v1)\
          \ on T4. It's an interesting start.</p>\n"
        raw: "# GPT2-124M-wikitext-v0.1\r\n\r\nThis is a practical hands-on result\
          \ that helped me build a foundation on  \U0001F917 Transformers and \U0001F917\
          \ Datasets. I fint-tuned GPT2 on wikitext (103-raw-v1) on T4. It's an interesting\
          \ start."
        updatedAt: '2023-12-01T18:09:15.068Z'
      numEdits: 0
      reactions: []
    id: 656a214b3eb5f0b6a9bdd82b
    type: comment
  author: dannoncaffeine
  content: "# GPT2-124M-wikitext-v0.1\r\n\r\nThis is a practical hands-on result that\
    \ helped me build a foundation on  \U0001F917 Transformers and \U0001F917 Datasets.\
    \ I fint-tuned GPT2 on wikitext (103-raw-v1) on T4. It's an interesting start."
  created_at: 2023-12-01 18:09:15+00:00
  edited: false
  hidden: false
  id: 656a214b3eb5f0b6a9bdd82b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: dannoncaffeine/GPT2-124M-wikitext-v0.1
repo_type: model
status: open
target_branch: null
title: Introduction
