!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Zenwill
conflicting_files: null
created_at: 2023-12-18 15:46:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02d06fe43256a28606c0bb63fed1a441.svg
      fullname: Zhenwei Shao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zenwill
      type: user
    createdAt: '2023-12-18T15:46:02.000Z'
    data:
      edited: false
      editors:
      - Zenwill
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9358353614807129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02d06fe43256a28606c0bb63fed1a441.svg
          fullname: Zhenwei Shao
          isHf: false
          isPro: false
          name: Zenwill
          type: user
        html: '<p>Thx for your great job!<br>I''ve noticed that you changed the torch_dtype
          of phi-2 from float16 (the default value in the official version config.json)
          to float32. Why do that? Is that a common practice in LoRA finetuning?</p>

          '
        raw: "Thx for your great job!\r\nI've noticed that you changed the torch_dtype\
          \ of phi-2 from float16 (the default value in the official version config.json)\
          \ to float32. Why do that? Is that a common practice in LoRA finetuning?"
        updatedAt: '2023-12-18T15:46:02.782Z'
      numEdits: 0
      reactions: []
    id: 6580693a6081eba8a9d6a9b6
    type: comment
  author: Zenwill
  content: "Thx for your great job!\r\nI've noticed that you changed the torch_dtype\
    \ of phi-2 from float16 (the default value in the official version config.json)\
    \ to float32. Why do that? Is that a common practice in LoRA finetuning?"
  created_at: 2023-12-18 15:46:02+00:00
  edited: false
  hidden: false
  id: 6580693a6081eba8a9d6a9b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/519796695232fe8f99aa76c180d5b755.svg
      fullname: Venkatesh Siddi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: venkycs
      type: user
    createdAt: '2023-12-18T18:25:14.000Z'
    data:
      edited: true
      editors:
      - venkycs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9407109022140503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/519796695232fe8f99aa76c180d5b755.svg
          fullname: Venkatesh Siddi
          isHf: false
          isPro: true
          name: venkycs
          type: user
        html: '<p>Hey Zenwill, My bad I first didn''t notice it by the time I realized
          its late. However, here is my notebook with code I used also updated to
          Lora f16 - <a rel="nofollow" href="https://colab.research.google.com/drive/1UxUTH7-nFDs00YoS8Rm9v44cHK1eI-Tj#scrollTo=DdZRaqEg2x5K&amp;line=6&amp;uniqifier=1">https://colab.research.google.com/drive/1UxUTH7-nFDs00YoS8Rm9v44cHK1eI-Tj#scrollTo=DdZRaqEg2x5K&amp;line=6&amp;uniqifier=1</a></p>

          <p>You better load adapter with fp16<br>model = AutoModelForCausalLM.from_pretrained(<br>    "venkycs/phi-2-instruct",<br>    trust_remote_code=True,<br>    torch_dtype=torch.float16<br>).to(device)</p>

          '
        raw: "Hey Zenwill, My bad I first didn't notice it by the time I realized\
          \ its late. However, here is my notebook with code I used also updated to\
          \ Lora f16 - https://colab.research.google.com/drive/1UxUTH7-nFDs00YoS8Rm9v44cHK1eI-Tj#scrollTo=DdZRaqEg2x5K&line=6&uniqifier=1\n\
          \nYou better load adapter with fp16\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"venkycs/phi-2-instruct\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16\n\
          ).to(device)\n"
        updatedAt: '2023-12-19T05:47:51.002Z'
      numEdits: 2
      reactions: []
    id: 65808e8ae77395a0c8484f34
    type: comment
  author: venkycs
  content: "Hey Zenwill, My bad I first didn't notice it by the time I realized its\
    \ late. However, here is my notebook with code I used also updated to Lora f16\
    \ - https://colab.research.google.com/drive/1UxUTH7-nFDs00YoS8Rm9v44cHK1eI-Tj#scrollTo=DdZRaqEg2x5K&line=6&uniqifier=1\n\
    \nYou better load adapter with fp16\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"venkycs/phi-2-instruct\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16\n\
    ).to(device)\n"
  created_at: 2023-12-18 18:25:14+00:00
  edited: true
  hidden: false
  id: 65808e8ae77395a0c8484f34
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: venkycs/phi-2-instruct
repo_type: model
status: open
target_branch: null
title: Why change the torch_dtype of phi-2 from float16 to float32?
