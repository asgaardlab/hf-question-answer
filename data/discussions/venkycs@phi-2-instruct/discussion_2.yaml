!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aaronshenhao
conflicting_files: null
created_at: 2024-01-10 20:16:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7d96e20c0642af8e402224dda4bee21d.svg
      fullname: Aaron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aaronshenhao
      type: user
    createdAt: '2024-01-10T20:16:53.000Z'
    data:
      edited: false
      editors:
      - aaronshenhao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9189482927322388
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7d96e20c0642af8e402224dda4bee21d.svg
          fullname: Aaron
          isHf: false
          isPro: false
          name: aaronshenhao
          type: user
        html: '<p>I''m not really familiar with HF''s transformer'' library, so I''m
          not sure what''s going on. But the inference demo is not using the T4 GPU
          at all, whereas it takes up 12 GB of system RAM, which is unexpected for
          such a small model. It''s taking forever to complete. The demo also tries
          to load in the base model at the same time, which crashes Colab as it uses
          all the available system RAM.</p>

          <p>Inference demo link: <a href="https://huggingface.co/venkycs/phi-2-instruct/blob/main/inference_phi_2_instruct.ipynb">https://huggingface.co/venkycs/phi-2-instruct/blob/main/inference_phi_2_instruct.ipynb</a></p>

          '
        raw: "I'm not really familiar with HF's transformer' library, so I'm not sure\
          \ what's going on. But the inference demo is not using the T4 GPU at all,\
          \ whereas it takes up 12 GB of system RAM, which is unexpected for such\
          \ a small model. It's taking forever to complete. The demo also tries to\
          \ load in the base model at the same time, which crashes Colab as it uses\
          \ all the available system RAM.\r\n\r\nInference demo link: https://huggingface.co/venkycs/phi-2-instruct/blob/main/inference_phi_2_instruct.ipynb"
        updatedAt: '2024-01-10T20:16:53.327Z'
      numEdits: 0
      reactions: []
    id: 659efb3570cf8f1cbbe15bc8
    type: comment
  author: aaronshenhao
  content: "I'm not really familiar with HF's transformer' library, so I'm not sure\
    \ what's going on. But the inference demo is not using the T4 GPU at all, whereas\
    \ it takes up 12 GB of system RAM, which is unexpected for such a small model.\
    \ It's taking forever to complete. The demo also tries to load in the base model\
    \ at the same time, which crashes Colab as it uses all the available system RAM.\r\
    \n\r\nInference demo link: https://huggingface.co/venkycs/phi-2-instruct/blob/main/inference_phi_2_instruct.ipynb"
  created_at: 2024-01-10 20:16:53+00:00
  edited: false
  hidden: false
  id: 659efb3570cf8f1cbbe15bc8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: venkycs/phi-2-instruct
repo_type: model
status: open
target_branch: null
title: Colab demo doesn't use GPU
