!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-05-11 09:05:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-11T10:05:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Hi</p>\n<p>Thanks for uploading this interesting new LoRA.</p>\n\
          <p>You didn't include <code>adapter_config.json</code> which makes it hard\
          \ to load the LoRA.</p>\n<p>Reading the description and comments, I <em>think</em>\
          \ this is correct?  Could you confirm, or push the original <code>adapter_config.json</code>?</p>\n\
          <p>Many thanks</p>\n<pre><code class=\"language-json\"><span class=\"hljs-punctuation\"\
          >{</span>\n  <span class=\"hljs-attr\">\"base_model_name_or_path\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"decapoda-research/llama-7b-hf\"\
          </span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\"\
          >\"bias\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-string\">\"none\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"enable_lora\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-literal\"><span class=\"hljs-keyword\">null</span></span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"fan_in_fan_out\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">false</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"inference_mode\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"\
          hljs-keyword\">true</span></span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"lora_alpha\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">16</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"lora_dropout\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">0.05</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"merge_weights\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">false</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"modules_to_save\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"\
          hljs-keyword\">null</span></span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"peft_type\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"LORA\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"r\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">16</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"target_modules\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \    <span class=\"hljs-string\">\"q_proj\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-string\">\"k_proj\"</span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-string\">\"v_proj\"\
          </span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-string\"\
          >\"o_proj\"</span>\n  <span class=\"hljs-punctuation\">]</span><span class=\"\
          hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"task_type\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"CAUSAL_LM\"\
          </span>\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n"
        raw: "Hi\r\n\r\nThanks for uploading this interesting new LoRA.\r\n\r\nYou\
          \ didn't include `adapter_config.json` which makes it hard to load the LoRA.\r\
          \n\r\nReading the description and comments, I *think* this is correct? \
          \ Could you confirm, or push the original `adapter_config.json`?\r\n\r\n\
          Many thanks\r\n\r\n```json\r\n{\r\n  \"base_model_name_or_path\": \"decapoda-research/llama-7b-hf\"\
          ,\r\n  \"bias\": \"none\",\r\n  \"enable_lora\": null,\r\n  \"fan_in_fan_out\"\
          : false,\r\n  \"inference_mode\": true,\r\n  \"lora_alpha\": 16,\r\n  \"\
          lora_dropout\": 0.05,\r\n  \"merge_weights\": false,\r\n  \"modules_to_save\"\
          : null,\r\n  \"peft_type\": \"LORA\",\r\n  \"r\": 16,\r\n  \"target_modules\"\
          : [\r\n    \"q_proj\",\r\n    \"k_proj\",\r\n    \"v_proj\",\r\n    \"o_proj\"\
          \r\n  ],\r\n  \"task_type\": \"CAUSAL_LM\"\r\n}\r\n```"
        updatedAt: '2023-05-11T10:05:40.382Z'
      numEdits: 0
      reactions: []
    id: 645cbdf45ebf379fd6d741ee
    type: comment
  author: TheBloke
  content: "Hi\r\n\r\nThanks for uploading this interesting new LoRA.\r\n\r\nYou didn't\
    \ include `adapter_config.json` which makes it hard to load the LoRA.\r\n\r\n\
    Reading the description and comments, I *think* this is correct?  Could you confirm,\
    \ or push the original `adapter_config.json`?\r\n\r\nMany thanks\r\n\r\n```json\r\
    \n{\r\n  \"base_model_name_or_path\": \"decapoda-research/llama-7b-hf\",\r\n \
    \ \"bias\": \"none\",\r\n  \"enable_lora\": null,\r\n  \"fan_in_fan_out\": false,\r\
    \n  \"inference_mode\": true,\r\n  \"lora_alpha\": 16,\r\n  \"lora_dropout\":\
    \ 0.05,\r\n  \"merge_weights\": false,\r\n  \"modules_to_save\": null,\r\n  \"\
    peft_type\": \"LORA\",\r\n  \"r\": 16,\r\n  \"target_modules\": [\r\n    \"q_proj\"\
    ,\r\n    \"k_proj\",\r\n    \"v_proj\",\r\n    \"o_proj\"\r\n  ],\r\n  \"task_type\"\
    : \"CAUSAL_LM\"\r\n}\r\n```"
  created_at: 2023-05-11 09:05:40+00:00
  edited: false
  hidden: false
  id: 645cbdf45ebf379fd6d741ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f320161f8dbdf1d7b847598b1eca02c9.svg
      fullname: Zhiqing Sun
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zhiqings
      type: user
    createdAt: '2023-05-17T00:07:29.000Z'
    data:
      edited: false
      editors:
      - zhiqings
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f320161f8dbdf1d7b847598b1eca02c9.svg
          fullname: Zhiqing Sun
          isHf: false
          isPro: false
          name: zhiqings
          type: user
        html: "<p>Hi, here is the <code>adapter_config.json</code> we used:</p>\n\
          <pre><code class=\"language-json\"><span class=\"hljs-punctuation\">{</span>\n\
          \  <span class=\"hljs-attr\">\"base_model_name_or_path\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-string\">\"llama-65b-hf\"\
          </span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\"\
          >\"bias\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-string\">\"none\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"enable_lora\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-literal\"><span class=\"hljs-keyword\">null</span></span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"fan_in_fan_out\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">false</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"inference_mode\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"\
          hljs-keyword\">true</span></span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"lora_alpha\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">16</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"lora_dropout\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">0.05</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"merge_weights\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">false</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"modules_to_save\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"\
          hljs-keyword\">null</span></span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"peft_type\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"LORA\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"r\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">16</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"target_modules\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \    <span class=\"hljs-string\">\"q_proj\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-string\">\"k_proj\"</span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-string\">\"v_proj\"\
          </span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-string\"\
          >\"o_proj\"</span>\n  <span class=\"hljs-punctuation\">]</span><span class=\"\
          hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"task_type\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"CAUSAL_LM\"\
          </span>\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n"
        raw: "Hi, here is the `adapter_config.json` we used:\n\n```json\n{\n  \"base_model_name_or_path\"\
          : \"llama-65b-hf\",\n  \"bias\": \"none\",\n  \"enable_lora\": null,\n \
          \ \"fan_in_fan_out\": false,\n  \"inference_mode\": true,\n  \"lora_alpha\"\
          : 16,\n  \"lora_dropout\": 0.05,\n  \"merge_weights\": false,\n  \"modules_to_save\"\
          : null,\n  \"peft_type\": \"LORA\",\n  \"r\": 16,\n  \"target_modules\"\
          : [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\"\n\
          \  ],\n  \"task_type\": \"CAUSAL_LM\"\n}\n```"
        updatedAt: '2023-05-17T00:07:29.663Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
      relatedEventId: 64641ac16dad99445dedcc3b
    id: 64641ac16dad99445dedcc3a
    type: comment
  author: zhiqings
  content: "Hi, here is the `adapter_config.json` we used:\n\n```json\n{\n  \"base_model_name_or_path\"\
    : \"llama-65b-hf\",\n  \"bias\": \"none\",\n  \"enable_lora\": null,\n  \"fan_in_fan_out\"\
    : false,\n  \"inference_mode\": true,\n  \"lora_alpha\": 16,\n  \"lora_dropout\"\
    : 0.05,\n  \"merge_weights\": false,\n  \"modules_to_save\": null,\n  \"peft_type\"\
    : \"LORA\",\n  \"r\": 16,\n  \"target_modules\": [\n    \"q_proj\",\n    \"k_proj\"\
    ,\n    \"v_proj\",\n    \"o_proj\"\n  ],\n  \"task_type\": \"CAUSAL_LM\"\n}\n\
    ```"
  created_at: 2023-05-16 23:07:29+00:00
  edited: false
  hidden: false
  id: 64641ac16dad99445dedcc3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f320161f8dbdf1d7b847598b1eca02c9.svg
      fullname: Zhiqing Sun
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zhiqings
      type: user
    createdAt: '2023-05-17T00:07:29.000Z'
    data:
      status: closed
    id: 64641ac16dad99445dedcc3b
    type: status-change
  author: zhiqings
  created_at: 2023-05-16 23:07:29+00:00
  id: 64641ac16dad99445dedcc3b
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-17T21:30:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thank you very much</p>

          '
        raw: Thank you very much
        updatedAt: '2023-05-17T21:30:26.094Z'
      numEdits: 0
      reactions: []
    id: 6465477286e668ad22e7e525
    type: comment
  author: TheBloke
  content: Thank you very much
  created_at: 2023-05-17 20:30:26+00:00
  edited: false
  hidden: false
  id: 6465477286e668ad22e7e525
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: zhiqings/dromedary-65b-lora-delta-v0
repo_type: model
status: closed
target_branch: null
title: Is this the right adapter_config.json?
