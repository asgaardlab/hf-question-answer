!!python/object:huggingface_hub.community.DiscussionWithDetails
author: md2
conflicting_files: null
created_at: 2023-08-22 22:28:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c51694bb7cf20383d211cc6093683594.svg
      fullname: MDD
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: md2
      type: user
    createdAt: '2023-08-22T23:28:09.000Z'
    data:
      edited: false
      editors:
      - md2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6327903866767883
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c51694bb7cf20383d211cc6093683594.svg
          fullname: MDD
          isHf: false
          isPro: false
          name: md2
          type: user
        html: '<p>Hello TheBloke, thanks for the great effort in pulling this up.</p>

          <p>I understood that this version should be compatible with the latest llama.cpp.
          I have updated it a few hours ago (commit 519c981f8b65ee6c87c2965539685ced0a17223b)
          and trying to use the new model as</p>

          <p>./main -m ./models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin
          -n 256 --repeat_penalty 1.0 -c 2048  --rope-freq-base 10000 --rope-freq-scale
          0.25 --color -i -r "User:" -f prompts/chat-with-bob.txt </p>

          <p>However, this is the output:</p>

          <p>main: warning: scaling RoPE frequency by 0,25 (default 1.0)<br>main:
          build = 1022 (bac6699)<br>main: seed  = 1692746800<br>gguf_init_from_file:
          invalid magic number 67676a74<br>error loading model: llama_model_loader:
          failed to load model from ./models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin</p>

          <p>llama_load_model_from_file: failed to load model<br>llama_init_from_gpt_params:
          error: failed to load model ''./models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin''<br>main:
          error: unable to load model</p>

          <p>What am I doing wrongly? Any help will be much appreciated, thanks!</p>

          '
        raw: "Hello TheBloke, thanks for the great effort in pulling this up.\r\n\r\
          \nI understood that this version should be compatible with the latest llama.cpp.\
          \ I have updated it a few hours ago (commit 519c981f8b65ee6c87c2965539685ced0a17223b)\
          \ and trying to use the new model as\r\n\r\n./main -m ./models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin\
          \ -n 256 --repeat_penalty 1.0 -c 2048  --rope-freq-base 10000 --rope-freq-scale\
          \ 0.25 --color -i -r \"User:\" -f prompts/chat-with-bob.txt \r\n\r\nHowever,\
          \ this is the output:\r\n\r\nmain: warning: scaling RoPE frequency by 0,25\
          \ (default 1.0)\r\nmain: build = 1022 (bac6699)\r\nmain: seed  = 1692746800\r\
          \ngguf_init_from_file: invalid magic number 67676a74\r\nerror loading model:\
          \ llama_model_loader: failed to load model from ./models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin\r\
          \n\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
          \ error: failed to load model './models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin'\r\
          \nmain: error: unable to load model\r\n\r\n\r\nWhat am I doing wrongly?\
          \ Any help will be much appreciated, thanks!\r\n"
        updatedAt: '2023-08-22T23:28:09.435Z'
      numEdits: 0
      reactions: []
    id: 64e54489517ecf6640d8912e
    type: comment
  author: md2
  content: "Hello TheBloke, thanks for the great effort in pulling this up.\r\n\r\n\
    I understood that this version should be compatible with the latest llama.cpp.\
    \ I have updated it a few hours ago (commit 519c981f8b65ee6c87c2965539685ced0a17223b)\
    \ and trying to use the new model as\r\n\r\n./main -m ./models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin\
    \ -n 256 --repeat_penalty 1.0 -c 2048  --rope-freq-base 10000 --rope-freq-scale\
    \ 0.25 --color -i -r \"User:\" -f prompts/chat-with-bob.txt \r\n\r\nHowever, this\
    \ is the output:\r\n\r\nmain: warning: scaling RoPE frequency by 0,25 (default\
    \ 1.0)\r\nmain: build = 1022 (bac6699)\r\nmain: seed  = 1692746800\r\ngguf_init_from_file:\
    \ invalid magic number 67676a74\r\nerror loading model: llama_model_loader: failed\
    \ to load model from ./models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin\r\
    \n\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
    \ error: failed to load model './models/13B/vicuna-13b-v1.5-16k/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin'\r\
    \nmain: error: unable to load model\r\n\r\n\r\nWhat am I doing wrongly? Any help\
    \ will be much appreciated, thanks!\r\n"
  created_at: 2023-08-22 22:28:09+00:00
  edited: false
  hidden: false
  id: 64e54489517ecf6640d8912e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
      fullname: Akarshan Biswas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akarshanbiswas
      type: user
    createdAt: '2023-08-23T03:48:13.000Z'
    data:
      edited: false
      editors:
      - akarshanbiswas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34294548630714417
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
          fullname: Akarshan Biswas
          isHf: false
          isPro: false
          name: akarshanbiswas
          type: user
        html: '<p>The new version supports gguf format. Try converting it using the
          python ggml_to_gguf in the llama.cpp repo.</p>

          '
        raw: The new version supports gguf format. Try converting it using the python
          ggml_to_gguf in the llama.cpp repo.
        updatedAt: '2023-08-23T03:48:13.376Z'
      numEdits: 0
      reactions: []
    id: 64e5817d4c20016ec9fe5754
    type: comment
  author: akarshanbiswas
  content: The new version supports gguf format. Try converting it using the python
    ggml_to_gguf in the llama.cpp repo.
  created_at: 2023-08-23 02:48:13+00:00
  edited: false
  hidden: false
  id: 64e5817d4c20016ec9fe5754
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-23T07:50:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9018465280532837
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah, latest llama.cpp is no longer compatible with GGML models.  The
          new model format, GGUF, was merged recently.  As far as llama.cpp is concerned,
          GGML is now dead - though of course many third-party clients/libraries are
          likely to continue to support it for a lot longer.  I need to update my
          GGML READMEs to mention this and will be doing this shortly.</p>

          <p>I will be providing GGUF models for all my repos in the next 2-3 days.
          I''m waiting for another PR to merge, which will add improved k-quant quantisation
          formats.</p>

          <p>For now, if you want to use llama.cpp you will need to downgrade it back
          to commit <code>dadbed99e65252d79f81101a392d0d6497b86caa</code> or earlier.  Or
          use one of the llama.cpp binary releases from before GGUF was merged.  Or
          use a third party client like KoboldCpp, LM Studio, text-generation-webui,
          etc.</p>

          <p>Look out for new <code>-GGUF</code> repos from me in the coming days.
          Or yes, you can convert them yourself using the script <code>ggml_to_gguf.py</code>
          now provided with llama.cpp.</p>

          '
        raw: 'Yeah, latest llama.cpp is no longer compatible with GGML models.  The
          new model format, GGUF, was merged recently.  As far as llama.cpp is concerned,
          GGML is now dead - though of course many third-party clients/libraries are
          likely to continue to support it for a lot longer.  I need to update my
          GGML READMEs to mention this and will be doing this shortly.


          I will be providing GGUF models for all my repos in the next 2-3 days. I''m
          waiting for another PR to merge, which will add improved k-quant quantisation
          formats.


          For now, if you want to use llama.cpp you will need to downgrade it back
          to commit `dadbed99e65252d79f81101a392d0d6497b86caa` or earlier.  Or use
          one of the llama.cpp binary releases from before GGUF was merged.  Or use
          a third party client like KoboldCpp, LM Studio, text-generation-webui, etc.


          Look out for new `-GGUF` repos from me in the coming days. Or yes, you can
          convert them yourself using the script `ggml_to_gguf.py` now provided with
          llama.cpp.

          '
        updatedAt: '2023-08-23T07:54:38.021Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - AIlchemist
        - volodvana
        - Flanua
        - teblanc
        - neoneye
    id: 64e5ba63a9a5eabaa6fd4a04
    type: comment
  author: TheBloke
  content: 'Yeah, latest llama.cpp is no longer compatible with GGML models.  The
    new model format, GGUF, was merged recently.  As far as llama.cpp is concerned,
    GGML is now dead - though of course many third-party clients/libraries are likely
    to continue to support it for a lot longer.  I need to update my GGML READMEs
    to mention this and will be doing this shortly.


    I will be providing GGUF models for all my repos in the next 2-3 days. I''m waiting
    for another PR to merge, which will add improved k-quant quantisation formats.


    For now, if you want to use llama.cpp you will need to downgrade it back to commit
    `dadbed99e65252d79f81101a392d0d6497b86caa` or earlier.  Or use one of the llama.cpp
    binary releases from before GGUF was merged.  Or use a third party client like
    KoboldCpp, LM Studio, text-generation-webui, etc.


    Look out for new `-GGUF` repos from me in the coming days. Or yes, you can convert
    them yourself using the script `ggml_to_gguf.py` now provided with llama.cpp.

    '
  created_at: 2023-08-23 06:50:59+00:00
  edited: true
  hidden: false
  id: 64e5ba63a9a5eabaa6fd4a04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c51694bb7cf20383d211cc6093683594.svg
      fullname: MDD
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: md2
      type: user
    createdAt: '2023-08-23T08:41:00.000Z'
    data:
      edited: false
      editors:
      - md2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7629388570785522
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c51694bb7cf20383d211cc6093683594.svg
          fullname: MDD
          isHf: false
          isPro: false
          name: md2
          type: user
        html: '<p>Thank you so much TheBloke, that''s a perfect summary of the current
          situation. I can confirm that using </p>

          <p>python3 convert-llama-ggmlv3-to-gguf.py --input ... --output ...</p>

          <p>does the job! </p>

          '
        raw: "Thank you so much TheBloke, that's a perfect summary of the current\
          \ situation. I can confirm that using \n\npython3 convert-llama-ggmlv3-to-gguf.py\
          \ --input ... --output ...\n\ndoes the job! \n"
        updatedAt: '2023-08-23T08:41:00.092Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mufeed
        - lincolnalbemarle
    id: 64e5c61c9d76a3de753bc29f
    type: comment
  author: md2
  content: "Thank you so much TheBloke, that's a perfect summary of the current situation.\
    \ I can confirm that using \n\npython3 convert-llama-ggmlv3-to-gguf.py --input\
    \ ... --output ...\n\ndoes the job! \n"
  created_at: 2023-08-23 07:41:00+00:00
  edited: false
  hidden: false
  id: 64e5c61c9d76a3de753bc29f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
      fullname: Akarshan Biswas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akarshanbiswas
      type: user
    createdAt: '2023-08-24T03:49:23.000Z'
    data:
      edited: false
      editors:
      - akarshanbiswas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.94943767786026
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
          fullname: Akarshan Biswas
          isHf: false
          isPro: false
          name: akarshanbiswas
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I think it's\
          \ better to ship old ggml with gguf files in the same repo for sometime\
          \ with a description of the two types of files in the README. </p>\n"
        raw: '@TheBloke I think it''s better to ship old ggml with gguf files in the
          same repo for sometime with a description of the two types of files in the
          README. '
        updatedAt: '2023-08-24T03:49:23.696Z'
      numEdits: 0
      reactions: []
    id: 64e6d3436096e592824b1bac
    type: comment
  author: akarshanbiswas
  content: '@TheBloke I think it''s better to ship old ggml with gguf files in the
    same repo for sometime with a description of the two types of files in the README. '
  created_at: 2023-08-24 02:49:23+00:00
  edited: false
  hidden: false
  id: 64e6d3436096e592824b1bac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64595c35e51abbc104d7aab3/S6IB1Vts6SpfL64nk_KN0.png?w=200&h=200&f=face
      fullname: "Nahuel Pati\xF1o"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nahuel89p
      type: user
    createdAt: '2023-09-01T23:57:35.000Z'
    data:
      edited: false
      editors:
      - nahuel89p
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8118804097175598
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64595c35e51abbc104d7aab3/S6IB1Vts6SpfL64nk_KN0.png?w=200&h=200&f=face
          fullname: "Nahuel Pati\xF1o"
          isHf: false
          isPro: false
          name: nahuel89p
          type: user
        html: "<p>Hello. Chiming in here.<br>Let's say I want to convert nous-hermes-llama2-13b.ggmlv3.q4_K_M.bin\
          \ to GGUF.</p>\n<p>Do I have to provide the metadata from the original model\
          \ (<a href=\"https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b/tree/main\"\
          >https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b/tree/main</a>)?<br>Perhaps\
          \ config.json? If so, what's the correct flag?</p>\n<p>If I understand correctly,\
          \ GGUF is GGML in the core, but with some extra metadata. \U0001F914<br>I\
          \ want to make sure the conversion is done 100% right.</p>\n"
        raw: "Hello. Chiming in here. \nLet's say I want to convert nous-hermes-llama2-13b.ggmlv3.q4_K_M.bin\
          \ to GGUF.\n\nDo I have to provide the metadata from the original model\
          \ (https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b/tree/main)?\n\
          Perhaps config.json? If so, what's the correct flag?\n\nIf I understand\
          \ correctly, GGUF is GGML in the core, but with some extra metadata. \U0001F914\
          \nI want to make sure the conversion is done 100% right."
        updatedAt: '2023-09-01T23:57:35.457Z'
      numEdits: 0
      reactions: []
    id: 64f27a6f345b9d9a6f6a00da
    type: comment
  author: nahuel89p
  content: "Hello. Chiming in here. \nLet's say I want to convert nous-hermes-llama2-13b.ggmlv3.q4_K_M.bin\
    \ to GGUF.\n\nDo I have to provide the metadata from the original model (https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b/tree/main)?\n\
    Perhaps config.json? If so, what's the correct flag?\n\nIf I understand correctly,\
    \ GGUF is GGML in the core, but with some extra metadata. \U0001F914\nI want to\
    \ make sure the conversion is done 100% right."
  created_at: 2023-09-01 22:57:35+00:00
  edited: false
  hidden: false
  id: 64f27a6f345b9d9a6f6a00da
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/vicuna-13B-v1.5-16K-GGML
repo_type: model
status: open
target_branch: null
title: can't load model with llama.cpp commit 519c981f8b65ee6c87c2965539685ced0a17223b
