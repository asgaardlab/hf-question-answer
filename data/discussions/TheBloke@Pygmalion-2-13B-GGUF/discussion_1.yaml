!!python/object:huggingface_hub.community.DiscussionWithDetails
author: markenzwieback
conflicting_files: null
created_at: 2023-09-08 12:42:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e917704975180954d9badd9668fa5370.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: markenzwieback
      type: user
    createdAt: '2023-09-08T13:42:15.000Z'
    data:
      edited: false
      editors:
      - markenzwieback
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8631207942962646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e917704975180954d9badd9668fa5370.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: markenzwieback
          type: user
        html: '<p>Greetings,</p>

          <p>i got this model set up and running without issues, according to the
          blogpost, except for the context length: the blogpost states a length of
          4k tokens, but every time i go beyond 2k tokens (while 4k are correctly
          set) with this GGUF version, i get the context window error from llama.cpp.
          (text-generation-webui &amp; SillyTavern)</p>

          <p>I wasn''t able to test out other versions yet.</p>

          <p>Does anyone happen to know if this is a limitation with this GGUF version
          or with llama in general or may the error be somewhere else?</p>

          <p>Thanks in advance.</p>

          '
        raw: "Greetings,\r\n\r\ni got this model set up and running without issues,\
          \ according to the blogpost, except for the context length: the blogpost\
          \ states a length of 4k tokens, but every time i go beyond 2k tokens (while\
          \ 4k are correctly set) with this GGUF version, i get the context window\
          \ error from llama.cpp. (text-generation-webui & SillyTavern)\r\n\r\nI wasn't\
          \ able to test out other versions yet.\r\n\r\nDoes anyone happen to know\
          \ if this is a limitation with this GGUF version or with llama in general\
          \ or may the error be somewhere else?\r\n\r\nThanks in advance."
        updatedAt: '2023-09-08T13:42:15.164Z'
      numEdits: 0
      reactions: []
    id: 64fb24b78f76d450690b3ce0
    type: comment
  author: markenzwieback
  content: "Greetings,\r\n\r\ni got this model set up and running without issues,\
    \ according to the blogpost, except for the context length: the blogpost states\
    \ a length of 4k tokens, but every time i go beyond 2k tokens (while 4k are correctly\
    \ set) with this GGUF version, i get the context window error from llama.cpp.\
    \ (text-generation-webui & SillyTavern)\r\n\r\nI wasn't able to test out other\
    \ versions yet.\r\n\r\nDoes anyone happen to know if this is a limitation with\
    \ this GGUF version or with llama in general or may the error be somewhere else?\r\
    \n\r\nThanks in advance."
  created_at: 2023-09-08 12:42:15+00:00
  edited: false
  hidden: false
  id: 64fb24b78f76d450690b3ce0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-08T13:57:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9545928835868835
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Is it an error, or just a warning?  Does it say "warning: model
          might not support context sizes greater than 2048 tokens .. expect poor
          results" ?  If so you can safely ignore that.  I''m not sure why it''s still
          in the code, but it doesn''t apply to Llama 2 models or any model which
          has extended context</p>

          '
        raw: 'Is it an error, or just a warning?  Does it say "warning: model might
          not support context sizes greater than 2048 tokens .. expect poor results"
          ?  If so you can safely ignore that.  I''m not sure why it''s still in the
          code, but it doesn''t apply to Llama 2 models or any model which has extended
          context'
        updatedAt: '2023-09-08T13:57:17.360Z'
      numEdits: 0
      reactions: []
    id: 64fb283d304b8cb412b18fee
    type: comment
  author: TheBloke
  content: 'Is it an error, or just a warning?  Does it say "warning: model might
    not support context sizes greater than 2048 tokens .. expect poor results" ?  If
    so you can safely ignore that.  I''m not sure why it''s still in the code, but
    it doesn''t apply to Llama 2 models or any model which has extended context'
  created_at: 2023-09-08 12:57:17+00:00
  edited: false
  hidden: false
  id: 64fb283d304b8cb412b18fee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e917704975180954d9badd9668fa5370.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: markenzwieback
      type: user
    createdAt: '2023-09-08T14:11:01.000Z'
    data:
      edited: true
      editors:
      - markenzwieback
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9663466811180115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e917704975180954d9badd9668fa5370.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: markenzwieback
          type: user
        html: "<p>Unfortunately, it's the error and therefore it's not generating\
          \ any output. It happens using SillyTavern+tgw and tgw alone.</p>\n<p>In\
          \ both instances i did verify that the context length was set to 4k and\
          \ llama.cpp was used for loading the model.</p>\n<p>I get the feeling that\
          \ it's an error with tgw on my end \U0001F914</p>\n"
        raw: "Unfortunately, it's the error and therefore it's not generating any\
          \ output. It happens using SillyTavern+tgw and tgw alone.\n\nIn both instances\
          \ i did verify that the context length was set to 4k and llama.cpp was used\
          \ for loading the model.\n\n\nI get the feeling that it's an error with\
          \ tgw on my end \U0001F914"
        updatedAt: '2023-09-08T14:12:12.607Z'
      numEdits: 1
      reactions: []
    id: 64fb2b755ca946a0108cf766
    type: comment
  author: markenzwieback
  content: "Unfortunately, it's the error and therefore it's not generating any output.\
    \ It happens using SillyTavern+tgw and tgw alone.\n\nIn both instances i did verify\
    \ that the context length was set to 4k and llama.cpp was used for loading the\
    \ model.\n\n\nI get the feeling that it's an error with tgw on my end \U0001F914"
  created_at: 2023-09-08 13:11:01+00:00
  edited: true
  hidden: false
  id: 64fb2b755ca946a0108cf766
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68481d2a01b0a01c187f7f187454ea68.svg
      fullname: Voyajer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Voyajer
      type: user
    createdAt: '2023-09-09T02:46:14.000Z'
    data:
      edited: false
      editors:
      - Voyajer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.969176173210144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68481d2a01b0a01c187f7f187454ea68.svg
          fullname: Voyajer
          isHf: false
          isPro: false
          name: Voyajer
          type: user
        html: '<p>After setting context length in tgw, remember to reload the model.
          I was getting "llama_tokenize_with_model: too many tokens" in the terminal
          until I did that.</p>

          '
        raw: 'After setting context length in tgw, remember to reload the model. I
          was getting "llama_tokenize_with_model: too many tokens" in the terminal
          until I did that.'
        updatedAt: '2023-09-09T02:46:14.711Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - markenzwieback
    id: 64fbdc764010eccccc26e375
    type: comment
  author: Voyajer
  content: 'After setting context length in tgw, remember to reload the model. I was
    getting "llama_tokenize_with_model: too many tokens" in the terminal until I did
    that.'
  created_at: 2023-09-09 01:46:14+00:00
  edited: false
  hidden: false
  id: 64fbdc764010eccccc26e375
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Pygmalion-2-13B-GGUF
repo_type: model
status: open
target_branch: null
title: context length
