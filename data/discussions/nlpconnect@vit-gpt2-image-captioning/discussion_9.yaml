!!python/object:huggingface_hub.community.DiscussionWithDetails
author: prabeshkhadka
conflicting_files: null
created_at: 2022-11-30 15:57:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f9cd08d8fad21c7e8433e2007518113.svg
      fullname: Prabesh Khadka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prabeshkhadka
      type: user
    createdAt: '2022-11-30T15:57:40.000Z'
    data:
      edited: true
      editors:
      - prabeshkhadka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f9cd08d8fad21c7e8433e2007518113.svg
          fullname: Prabesh Khadka
          isHf: false
          isPro: false
          name: prabeshkhadka
          type: user
        html: '<p>Hi,<br>I would like to run the model, featureExtractor and tokenizer
          in c++.<br>So i am looking to convert it to torchscript , i load them with
          the parameter torchscript=true as below.</p>

          <p>model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning",
          torchscript=True)<br>feature_extractor = ViTFeatureExtractor.from_pretrained("nlpconnect/vit-gpt2-image-captioning",
          torchscript=True)<br>tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning",
          torchscript=True)</p>

          <p>But i cant seem to figure out what parameters to pass to the trace function.<br>traced_model
          = torch.jit.trace(model, WHAT_INPUT_TO_PASS))</p>

          <p>I did try passing pixel_values generated by feature_extractor and a random
          tensor or shape (1, 16). But the model that is traced seems to be incorrect.</p>

          <p>I tried converting it to ONNX but "Export a custom model for an unsupported
          architecture." seemed very confusing.</p>

          <p>Any guidance will be deeply appreciated.</p>

          <p>Regards,<br>Prabesh Khadka</p>

          '
        raw: "Hi,\nI would like to run the model, featureExtractor and tokenizer in\
          \ c++. \nSo i am looking to convert it to torchscript , i load them with\
          \ the parameter torchscript=true as below.\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"\
          nlpconnect/vit-gpt2-image-captioning\", torchscript=True)\nfeature_extractor\
          \ = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\"\
          , torchscript=True)\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\"\
          , torchscript=True)\n\n\nBut i cant seem to figure out what parameters to\
          \ pass to the trace function.\ntraced_model = torch.jit.trace(model, WHAT_INPUT_TO_PASS))\n\
          \nI did try passing pixel_values generated by feature_extractor and a random\
          \ tensor or shape (1, 16). But the model that is traced seems to be incorrect.\n\
          \nI tried converting it to ONNX but \"Export a custom model for an unsupported\
          \ architecture.\" seemed very confusing.\n\nAny guidance will be deeply\
          \ appreciated.\n\nRegards,\nPrabesh Khadka"
        updatedAt: '2022-11-30T19:10:31.989Z'
      numEdits: 1
      reactions: []
    id: 63877d745c68cf2713b97c74
    type: comment
  author: prabeshkhadka
  content: "Hi,\nI would like to run the model, featureExtractor and tokenizer in\
    \ c++. \nSo i am looking to convert it to torchscript , i load them with the parameter\
    \ torchscript=true as below.\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"\
    nlpconnect/vit-gpt2-image-captioning\", torchscript=True)\nfeature_extractor =\
    \ ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\"\
    , torchscript=True)\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\"\
    , torchscript=True)\n\n\nBut i cant seem to figure out what parameters to pass\
    \ to the trace function.\ntraced_model = torch.jit.trace(model, WHAT_INPUT_TO_PASS))\n\
    \nI did try passing pixel_values generated by feature_extractor and a random tensor\
    \ or shape (1, 16). But the model that is traced seems to be incorrect.\n\nI tried\
    \ converting it to ONNX but \"Export a custom model for an unsupported architecture.\"\
    \ seemed very confusing.\n\nAny guidance will be deeply appreciated.\n\nRegards,\n\
    Prabesh Khadka"
  created_at: 2022-11-30 15:57:40+00:00
  edited: true
  hidden: false
  id: 63877d745c68cf2713b97c74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599133184264-noauth.jpeg?w=200&h=200&f=face
      fullname: Ankur Singh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ankur310794
      type: user
    createdAt: '2022-12-01T04:23:00.000Z'
    data:
      edited: false
      editors:
      - ankur310794
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599133184264-noauth.jpeg?w=200&h=200&f=face
          fullname: Ankur Singh
          isHf: false
          isPro: false
          name: ankur310794
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;prabeshkhadka&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/prabeshkhadka\"\
          >@<span class=\"underline\">prabeshkhadka</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>First of all my suggestion would be go to this blog: <a rel=\"\
          nofollow\" href=\"https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\"\
          >https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/</a><br>It\
          \ will make you understand more about training and inference of vision encoder\
          \ decoder based models.</p>\n<p>This is how you can do it.</p>\n<pre><code\
          \ class=\"language-python\">url = <span class=\"hljs-string\">\"http://images.cocodataset.org/val2017/000000039769.jpg\"\
          </span>\nimage = Image.<span class=\"hljs-built_in\">open</span>(requests.get(url,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw)\npixel_values =\
          \ feature_extractor(image, return_tensors=<span class=\"hljs-string\">\"\
          pt\"</span>).pixel_values\n\nlabels = tokenizer(\n     <span class=\"hljs-string\"\
          >\"an image of two cats chilling on a couch\"</span>,\n     return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>,\n ).input_ids\n\ntraced_model = torch.jit.trace(model,\
          \ [pixel_values, labels])\ntorch.jit.save(traced_model, <span class=\"hljs-string\"\
          >\"traced_vit-gpt2-image-captioning.pt\"</span>)\n\n<span class=\"hljs-comment\"\
          ># load model</span>\nloaded_model = torch.jit.load(<span class=\"hljs-string\"\
          >\"traced_vit-gpt2-image-captioning.pt\"</span>)\n</code></pre>\n<p>you\
          \ may see running colab notebook: <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1a96pgxfpqfsJ6OCOwE6i0sGWmkhnjwv8?usp=sharing\"\
          >https://colab.research.google.com/drive/1a96pgxfpqfsJ6OCOwE6i0sGWmkhnjwv8?usp=sharing</a></p>\n"
        raw: "Hi @prabeshkhadka \n\nFirst of all my suggestion would be go to this\
          \ blog: https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\n\
          It will make you understand more about training and inference of vision\
          \ encoder decoder based models.\n\nThis is how you can do it.\n\n```python\n\
          url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage\
          \ = Image.open(requests.get(url, stream=True).raw)\npixel_values = feature_extractor(image,\
          \ return_tensors=\"pt\").pixel_values\n\nlabels = tokenizer(\n     \"an\
          \ image of two cats chilling on a couch\",\n     return_tensors=\"pt\",\n\
          \ ).input_ids\n\ntraced_model = torch.jit.trace(model, [pixel_values, labels])\n\
          torch.jit.save(traced_model, \"traced_vit-gpt2-image-captioning.pt\")\n\n\
          # load model\nloaded_model = torch.jit.load(\"traced_vit-gpt2-image-captioning.pt\"\
          )\n\n```\nyou may see running colab notebook: https://colab.research.google.com/drive/1a96pgxfpqfsJ6OCOwE6i0sGWmkhnjwv8?usp=sharing"
        updatedAt: '2022-12-01T04:23:00.835Z'
      numEdits: 0
      reactions: []
    id: 63882c241901766b88078497
    type: comment
  author: ankur310794
  content: "Hi @prabeshkhadka \n\nFirst of all my suggestion would be go to this blog:\
    \ https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\n\
    It will make you understand more about training and inference of vision encoder\
    \ decoder based models.\n\nThis is how you can do it.\n\n```python\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\
    \nimage = Image.open(requests.get(url, stream=True).raw)\npixel_values = feature_extractor(image,\
    \ return_tensors=\"pt\").pixel_values\n\nlabels = tokenizer(\n     \"an image\
    \ of two cats chilling on a couch\",\n     return_tensors=\"pt\",\n ).input_ids\n\
    \ntraced_model = torch.jit.trace(model, [pixel_values, labels])\ntorch.jit.save(traced_model,\
    \ \"traced_vit-gpt2-image-captioning.pt\")\n\n# load model\nloaded_model = torch.jit.load(\"\
    traced_vit-gpt2-image-captioning.pt\")\n\n```\nyou may see running colab notebook:\
    \ https://colab.research.google.com/drive/1a96pgxfpqfsJ6OCOwE6i0sGWmkhnjwv8?usp=sharing"
  created_at: 2022-12-01 04:23:00+00:00
  edited: false
  hidden: false
  id: 63882c241901766b88078497
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f9cd08d8fad21c7e8433e2007518113.svg
      fullname: Prabesh Khadka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prabeshkhadka
      type: user
    createdAt: '2022-12-01T13:01:38.000Z'
    data:
      edited: false
      editors:
      - prabeshkhadka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f9cd08d8fad21c7e8433e2007518113.svg
          fullname: Prabesh Khadka
          isHf: false
          isPro: false
          name: prabeshkhadka
          type: user
        html: '<p>Thank you so much. This works like a charm.</p>

          '
        raw: Thank you so much. This works like a charm.
        updatedAt: '2022-12-01T13:01:38.772Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ankur310794
    id: 6388a5b2ec1f539adc0889e5
    type: comment
  author: prabeshkhadka
  content: Thank you so much. This works like a charm.
  created_at: 2022-12-01 13:01:38+00:00
  edited: false
  hidden: false
  id: 6388a5b2ec1f539adc0889e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599133184264-noauth.jpeg?w=200&h=200&f=face
      fullname: Ankur Singh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ankur310794
      type: user
    createdAt: '2022-12-01T13:33:47.000Z'
    data:
      status: closed
    id: 6388ad3b43d8b0797a079f31
    type: status-change
  author: ankur310794
  created_at: 2022-12-01 13:33:47+00:00
  id: 6388ad3b43d8b0797a079f31
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f9cd08d8fad21c7e8433e2007518113.svg
      fullname: Prabesh Khadka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prabeshkhadka
      type: user
    createdAt: '2022-12-01T13:51:57.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/5f9cd08d8fad21c7e8433e2007518113.svg
          fullname: Prabesh Khadka
          isHf: false
          isPro: false
          name: prabeshkhadka
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-12-01T15:50:27.734Z'
      numEdits: 0
      reactions: []
    id: 6388b17d80134ba508ce8535
    type: comment
  author: prabeshkhadka
  content: This comment has been hidden
  created_at: 2022-12-01 13:51:57+00:00
  edited: true
  hidden: true
  id: 6388b17d80134ba508ce8535
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: nlpconnect/vit-gpt2-image-captioning
repo_type: model
status: closed
target_branch: null
title: Convert the model to torchscript or ONNX
