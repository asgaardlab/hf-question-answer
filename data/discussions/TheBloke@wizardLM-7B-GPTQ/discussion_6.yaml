!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ddaattaa
conflicting_files: null
created_at: 2023-04-27 23:59:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8136389eeb3e82201ff977b74b9bf1e4.svg
      fullname: Data
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ddaattaa
      type: user
    createdAt: '2023-04-28T00:59:26.000Z'
    data:
      edited: true
      editors:
      - ddaattaa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8136389eeb3e82201ff977b74b9bf1e4.svg
          fullname: Data
          isHf: false
          isPro: false
          name: ddaattaa
          type: user
        html: '<p>Hello, friends. For those of you who are experiencing significant
          slowdown when it comes to inferencing with the textgenerationwebui (including
          myself), I have what I hope is an adequate enough solution. It''s possible
          to load WizardLM-7B through KoboldAI, in which it performs much faster than
          it would''ve in textgen for whatever reason. This example output took roughly
          six seconds to generate:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6338ecc2171879571962339a/f_1iSPj1rjz6wkTQ1ChRm.png"><img
          alt="Screenshot_2023_04_27-9.png" src="https://cdn-uploads.huggingface.co/production/uploads/6338ecc2171879571962339a/f_1iSPj1rjz6wkTQ1ChRm.png"></a></p>

          <p>In order to use KoboldAI w/ WizardLM-7B, the following needs to be done:</p>

          <ol>

          <li>Download and install this fork of KoboldAI that supports loading models
          in 4bit quantization: <a rel="nofollow" href="https://github.com/0cc4m/KoboldAI">https://github.com/0cc4m/KoboldAI</a>.</li>

          <li>Once installed, launch it by running the play.bat file. It should take
          you to the KoboldAI webpage. </li>

          <li>Click the "Try New UI" button, this is the only way to use 4bit quantization.</li>

          <li>Once it takes you to the New UI, click the "Interface" button. </li>

          <li>Expand the "UI" dropdown menu and turn on the "Experimental UI" feature,
          this is what enables you to use 4bit quantization.</li>

          <li>Put your preferred model, in this case WizardLM-7B, in the KoboldAI\models
          folder.</li>

          <li>Trying to use the safetensors file won''t work (at least, in my case
          it didn''t.), you''ll have to use one of the .pt files instead. They were
          recently removed from the HF repo, so I''ve uploaded them here: <a rel="nofollow"
          href="https://mega.nz/folder/IeUgUbaZ#C8Ng-81-DAV_qfWqbVMoEw">https://mega.nz/folder/IeUgUbaZ#C8Ng-81-DAV_qfWqbVMoEw</a>.</li>

          <li>Rename one of the .pt files to "4bit-128g.pt", this is how KoboldAI
          is able to detect which quantization and group size it should load the model
          in.</li>

          <li>Click load model, select the WizardLM-7B folder, and make sure "Use
          4bit mode" is turned on.</li>

          <li>You should be able to load the model just fine and start generating.
          Have fun!</li>

          </ol>

          '
        raw: "Hello, friends. For those of you who are experiencing significant slowdown\
          \ when it comes to inferencing with the textgenerationwebui (including myself),\
          \ I have what I hope is an adequate enough solution. It's possible to load\
          \ WizardLM-7B through KoboldAI, in which it performs much faster than it\
          \ would've in textgen for whatever reason. This example output took roughly\
          \ six seconds to generate:\n![Screenshot_2023_04_27-9.png](https://cdn-uploads.huggingface.co/production/uploads/6338ecc2171879571962339a/f_1iSPj1rjz6wkTQ1ChRm.png)\n\
          \nIn order to use KoboldAI w/ WizardLM-7B, the following needs to be done:\n\
          1. Download and install this fork of KoboldAI that supports loading models\
          \ in 4bit quantization: https://github.com/0cc4m/KoboldAI.\n2. Once installed,\
          \ launch it by running the play.bat file. It should take you to the KoboldAI\
          \ webpage. \n3. Click the \"Try New UI\" button, this is the only way to\
          \ use 4bit quantization.\n4. Once it takes you to the New UI, click the\
          \ \"Interface\" button. \n5. Expand the \"UI\" dropdown menu and turn on\
          \ the \"Experimental UI\" feature, this is what enables you to use 4bit\
          \ quantization.\n6. Put your preferred model, in this case WizardLM-7B,\
          \ in the KoboldAI\\models folder.\n7. Trying to use the safetensors file\
          \ won't work (at least, in my case it didn't.), you'll have to use one of\
          \ the .pt files instead. They were recently removed from the HF repo, so\
          \ I've uploaded them here: https://mega.nz/folder/IeUgUbaZ#C8Ng-81-DAV_qfWqbVMoEw.\n\
          8. Rename one of the .pt files to \"4bit-128g.pt\", this is how KoboldAI\
          \ is able to detect which quantization and group size it should load the\
          \ model in.\n9. Click load model, select the WizardLM-7B folder, and make\
          \ sure \"Use 4bit mode\" is turned on.\n10. You should be able to load the\
          \ model just fine and start generating. Have fun!"
        updatedAt: '2023-04-28T01:12:04.131Z'
      numEdits: 1
      reactions: []
    id: 644b1a6e3a619fe72b17af1e
    type: comment
  author: ddaattaa
  content: "Hello, friends. For those of you who are experiencing significant slowdown\
    \ when it comes to inferencing with the textgenerationwebui (including myself),\
    \ I have what I hope is an adequate enough solution. It's possible to load WizardLM-7B\
    \ through KoboldAI, in which it performs much faster than it would've in textgen\
    \ for whatever reason. This example output took roughly six seconds to generate:\n\
    ![Screenshot_2023_04_27-9.png](https://cdn-uploads.huggingface.co/production/uploads/6338ecc2171879571962339a/f_1iSPj1rjz6wkTQ1ChRm.png)\n\
    \nIn order to use KoboldAI w/ WizardLM-7B, the following needs to be done:\n1.\
    \ Download and install this fork of KoboldAI that supports loading models in 4bit\
    \ quantization: https://github.com/0cc4m/KoboldAI.\n2. Once installed, launch\
    \ it by running the play.bat file. It should take you to the KoboldAI webpage.\
    \ \n3. Click the \"Try New UI\" button, this is the only way to use 4bit quantization.\n\
    4. Once it takes you to the New UI, click the \"Interface\" button. \n5. Expand\
    \ the \"UI\" dropdown menu and turn on the \"Experimental UI\" feature, this is\
    \ what enables you to use 4bit quantization.\n6. Put your preferred model, in\
    \ this case WizardLM-7B, in the KoboldAI\\models folder.\n7. Trying to use the\
    \ safetensors file won't work (at least, in my case it didn't.), you'll have to\
    \ use one of the .pt files instead. They were recently removed from the HF repo,\
    \ so I've uploaded them here: https://mega.nz/folder/IeUgUbaZ#C8Ng-81-DAV_qfWqbVMoEw.\n\
    8. Rename one of the .pt files to \"4bit-128g.pt\", this is how KoboldAI is able\
    \ to detect which quantization and group size it should load the model in.\n9.\
    \ Click load model, select the WizardLM-7B folder, and make sure \"Use 4bit mode\"\
    \ is turned on.\n10. You should be able to load the model just fine and start\
    \ generating. Have fun!"
  created_at: 2023-04-27 23:59:26+00:00
  edited: true
  hidden: false
  id: 644b1a6e3a619fe72b17af1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/361d1120df3e107f2d5623a6a9127a00.svg
      fullname: Matthew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IonizedTexasMan
      type: user
    createdAt: '2023-04-28T04:57:25.000Z'
    data:
      edited: false
      editors:
      - IonizedTexasMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/361d1120df3e107f2d5623a6a9127a00.svg
          fullname: Matthew
          isHf: false
          isPro: false
          name: IonizedTexasMan
          type: user
        html: '<p>I was able to get it to run faster on my system. In the config.json
          file, I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s</p>

          <p>Running a 2080 Ti on windows.</p>

          '
        raw: 'I was able to get it to run faster on my system. In the config.json
          file, I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s


          Running a 2080 Ti on windows.'
        updatedAt: '2023-04-28T04:57:25.293Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - BazsiBazsi
        - ddaattaa
        - JoeySalmons
      - count: 2
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - KrisKale45
      - count: 2
        reaction: "\U0001F91D"
        users:
        - TheBloke
        - BazsiBazsi
      - count: 1
        reaction: "\U0001F614"
        users:
        - TheBloke
    id: 644b5235cb45734dfd50feb5
    type: comment
  author: IonizedTexasMan
  content: 'I was able to get it to run faster on my system. In the config.json file,
    I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s


    Running a 2080 Ti on windows.'
  created_at: 2023-04-28 03:57:25+00:00
  edited: false
  hidden: false
  id: 644b5235cb45734dfd50feb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8136389eeb3e82201ff977b74b9bf1e4.svg
      fullname: Data
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ddaattaa
      type: user
    createdAt: '2023-04-28T05:06:22.000Z'
    data:
      edited: false
      editors:
      - ddaattaa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8136389eeb3e82201ff977b74b9bf1e4.svg
          fullname: Data
          isHf: false
          isPro: false
          name: ddaattaa
          type: user
        html: '<blockquote>

          <p>I was able to get it to run faster on my system. In the config.json file,
          I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s</p>

          <p>Running a 2080 Ti on windows.</p>

          </blockquote>

          <p>Just tried it out, you were not kidding. The difference is night and
          day, this may very well have been the initial problem.</p>

          '
        raw: "> I was able to get it to run faster on my system. In the config.json\
          \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
          > \n> Running a 2080 Ti on windows.\n\nJust tried it out, you were not kidding.\
          \ The difference is night and day, this may very well have been the initial\
          \ problem."
        updatedAt: '2023-04-28T05:06:22.237Z'
      numEdits: 0
      reactions: []
    id: 644b544ed4483bfaa079b67f
    type: comment
  author: ddaattaa
  content: "> I was able to get it to run faster on my system. In the config.json\
    \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
    > \n> Running a 2080 Ti on windows.\n\nJust tried it out, you were not kidding.\
    \ The difference is night and day, this may very well have been the initial problem."
  created_at: 2023-04-28 04:06:22+00:00
  edited: false
  hidden: false
  id: 644b544ed4483bfaa079b67f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T07:13:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I was able to get it to run faster on my system. In the config.json file,
          I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s</p>

          <p>Running a 2080 Ti on windows.</p>

          </blockquote>

          <p>You beauty! That was it!</p>

          <p>Thank you!  I will inform everyone the problem is solved. And know to
          look for that in future!!</p>

          '
        raw: "> I was able to get it to run faster on my system. In the config.json\
          \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
          > \n> Running a 2080 Ti on windows.\n\nYou beauty! That was it!\n\nThank\
          \ you!  I will inform everyone the problem is solved. And know to look for\
          \ that in future!!"
        updatedAt: '2023-04-28T07:13:59.105Z'
      numEdits: 0
      reactions: []
    id: 644b7237840601ec7f16e040
    type: comment
  author: TheBloke
  content: "> I was able to get it to run faster on my system. In the config.json\
    \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
    > \n> Running a 2080 Ti on windows.\n\nYou beauty! That was it!\n\nThank you!\
    \  I will inform everyone the problem is solved. And know to look for that in\
    \ future!!"
  created_at: 2023-04-28 06:13:59+00:00
  edited: false
  hidden: false
  id: 644b7237840601ec7f16e040
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/wizardLM-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: '[TemporarySolution] - Using KoboldAI to inference with WizardLM-7B'
