!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ElvisM
conflicting_files: null
created_at: 2023-04-26 10:17:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
      fullname: Leonel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElvisM
      type: user
    createdAt: '2023-04-26T11:17:13.000Z'
    data:
      edited: false
      editors:
      - ElvisM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
          fullname: Leonel
          isHf: false
          isPro: false
          name: ElvisM
          type: user
        html: "<p>Traceback (most recent call last):<br>File \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 442,\
          \ in load_state_dict<br>return torch.load(checkpoint_file, map_location=\u201C\
          cpu\u201D)<br>File \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 791, in load<br>with _open_file_like(f,\
          \ \u2018rb\u2019) as opened_file:<br>File \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\torch\\serialization.py\u201D, line 271, in _open_file_like<br>return\
          \ _open_file(name_or_buffer, mode)<br>File \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\torch\\serialization.py\u201D, line 252, in init<br>super().init(open(name,\
          \ mode))<br>FileNotFoundError: [Errno 2] No such file or directory: \u2018\
          models\\TheBloke_wizardLM-7B-GPTQ\\pytorch_model-00001-of-00003.bin\u2019\
          </p>\n<p>During handling of the above exception, another exception occurred:</p>\n\
          <p>Traceback (most recent call last):<br>File \u201CD:\\Chatbot\\text-generation-webui\\\
          server.py\u201D, line 101, in load_model_wrapper<br>shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)<br>File \u201CD:\\Chatbot\\text-generation-webui\\\
          modules\\models.py\u201D, line 207, in load_model<br>model = LoaderClass.from_pretrained(checkpoint,\
          \ **params)<br>File \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained<br>return\
          \ model_class.from_pretrained(<br>File \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2795,\
          \ in from_pretrained<br>) = cls._load_pretrained_model(<br>File \u201CD:\\\
          Chatbot\\installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 3109, in _load_pretrained_model<br>state_dict = load_state_dict(shard_file)<br>File\
          \ \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 445, in load_state_dict<br>with open(checkpoint_file)\
          \ as f:<br>FileNotFoundError: [Errno 2] No such file or directory: \u2018\
          models\\TheBloke_wizardLM-7B-GPTQ\\pytorch_model-00001-of-00003.bin\u2019\
          </p>\n"
        raw: "Traceback (most recent call last):\r\nFile \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 442,\
          \ in load_state_dict\r\nreturn torch.load(checkpoint_file, map_location=\u201C\
          cpu\u201D)\r\nFile \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 791, in load\r\nwith _open_file_like(f,\
          \ \u2018rb\u2019) as opened_file:\r\nFile \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\torch\\serialization.py\u201D, line 271, in _open_file_like\r\
          \nreturn _open_file(name_or_buffer, mode)\r\nFile \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\torch\\serialization.py\u201D, line 252, in init\r\
          \nsuper().init(open(name, mode))\r\nFileNotFoundError: [Errno 2] No such\
          \ file or directory: \u2018models\\TheBloke_wizardLM-7B-GPTQ\\pytorch_model-00001-of-00003.bin\u2019\
          \r\n\r\nDuring handling of the above exception, another exception occurred:\r\
          \n\r\nTraceback (most recent call last):\r\nFile \u201CD:\\Chatbot\\text-generation-webui\\\
          server.py\u201D, line 101, in load_model_wrapper\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\r\nFile \u201CD:\\Chatbot\\text-generation-webui\\\
          modules\\models.py\u201D, line 207, in load_model\r\nmodel = LoaderClass.from_pretrained(checkpoint,\
          \ **params)\r\nFile \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\r\
          \nreturn model_class.from_pretrained(\r\nFile \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2795,\
          \ in from_pretrained\r\n) = cls._load_pretrained_model(\r\nFile \u201CD:\\\
          Chatbot\\installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 3109, in _load_pretrained_model\r\nstate_dict = load_state_dict(shard_file)\r\
          \nFile \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 445, in load_state_dict\r\nwith open(checkpoint_file)\
          \ as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: \u2018\
          models\\TheBloke_wizardLM-7B-GPTQ\\pytorch_model-00001-of-00003.bin\u2019"
        updatedAt: '2023-04-26T11:17:13.533Z'
      numEdits: 0
      reactions: []
    id: 64490839011671fb7bdc5f96
    type: comment
  author: ElvisM
  content: "Traceback (most recent call last):\r\nFile \u201CD:\\Chatbot\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 442, in load_state_dict\r\
    \nreturn torch.load(checkpoint_file, map_location=\u201Ccpu\u201D)\r\nFile \u201C\
    D:\\Chatbot\\installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
    , line 791, in load\r\nwith _open_file_like(f, \u2018rb\u2019) as opened_file:\r\
    \nFile \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
    , line 271, in _open_file_like\r\nreturn _open_file(name_or_buffer, mode)\r\n\
    File \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
    , line 252, in init\r\nsuper().init(open(name, mode))\r\nFileNotFoundError: [Errno\
    \ 2] No such file or directory: \u2018models\\TheBloke_wizardLM-7B-GPTQ\\pytorch_model-00001-of-00003.bin\u2019\
    \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\
    \r\nTraceback (most recent call last):\r\nFile \u201CD:\\Chatbot\\text-generation-webui\\\
    server.py\u201D, line 101, in load_model_wrapper\r\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\r\nFile \u201CD:\\Chatbot\\text-generation-webui\\\
    modules\\models.py\u201D, line 207, in load_model\r\nmodel = LoaderClass.from_pretrained(checkpoint,\
    \ **params)\r\nFile \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\r\
    \nreturn model_class.from_pretrained(\r\nFile \u201CD:\\Chatbot\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2795, in\
    \ from_pretrained\r\n) = cls._load_pretrained_model(\r\nFile \u201CD:\\Chatbot\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
    , line 3109, in _load_pretrained_model\r\nstate_dict = load_state_dict(shard_file)\r\
    \nFile \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\u201D, line 445, in load_state_dict\r\nwith open(checkpoint_file)\
    \ as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: \u2018models\\\
    TheBloke_wizardLM-7B-GPTQ\\pytorch_model-00001-of-00003.bin\u2019"
  created_at: 2023-04-26 10:17:13+00:00
  edited: false
  hidden: false
  id: 64490839011671fb7bdc5f96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T11:19:49.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Firstly, please delete file <code>pytorch_model.bin.index.json</code>.
          That file shouldn''t have been there and I''ve deleted it from the repo
          now.</p>

          <p>However I think you getting this message means you''re not loading with
          GPTQ-for-LLaMa code. </p>

          <p>Is this text-generation-webui? If not then it''s probably not going to
          be able to load this model.  These models are GPTQ quantised and require
          GPTQ-for-LLaMa''s <code>llama_inference.py</code> to read them.</p>

          <p>That''s available as a plugin for text-generation-webui but I don''t
          know if other UIs support it yet.</p>

          '
        raw: "Firstly, please delete file `pytorch_model.bin.index.json`. That file\
          \ shouldn't have been there and I've deleted it from the repo now.\n\nHowever\
          \ I think you getting this message means you're not loading with GPTQ-for-LLaMa\
          \ code. \n\nIs this text-generation-webui? If not then it's probably not\
          \ going to be able to load this model.  These models are GPTQ quantised\
          \ and require GPTQ-for-LLaMa's `llama_inference.py` to read them.\n\nThat's\
          \ available as a plugin for text-generation-webui but I don't know if other\
          \ UIs support it yet."
        updatedAt: '2023-04-26T11:19:57.612Z'
      numEdits: 1
      reactions: []
    id: 644908d5f88f1495f0961979
    type: comment
  author: TheBloke
  content: "Firstly, please delete file `pytorch_model.bin.index.json`. That file\
    \ shouldn't have been there and I've deleted it from the repo now.\n\nHowever\
    \ I think you getting this message means you're not loading with GPTQ-for-LLaMa\
    \ code. \n\nIs this text-generation-webui? If not then it's probably not going\
    \ to be able to load this model.  These models are GPTQ quantised and require\
    \ GPTQ-for-LLaMa's `llama_inference.py` to read them.\n\nThat's available as a\
    \ plugin for text-generation-webui but I don't know if other UIs support it yet."
  created_at: 2023-04-26 10:19:49+00:00
  edited: true
  hidden: false
  id: 644908d5f88f1495f0961979
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
      fullname: Leonel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElvisM
      type: user
    createdAt: '2023-04-26T11:22:45.000Z'
    data:
      edited: true
      editors:
      - ElvisM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
          fullname: Leonel
          isHf: false
          isPro: false
          name: ElvisM
          type: user
        html: "<blockquote>\n<p>Firstly, please delete file <code>pytorch_model.bin.index.json</code>.\
          \ That file shouldn't have been there and I've deleted it from the repo\
          \ now.</p>\n<p>However I think you getting this message means you're not\
          \ loading with GPTQ-for-LLaMa code. </p>\n<p>Is this text-generation-webui?\
          \ If not then it's probably not going to be able to load this model.  These\
          \ models are GPTQ quantised and require GPTQ-for-LLaMa's <code>llama_inference.py</code>\
          \ to read them.</p>\n<p>That's available as a plugin for text-generation-webui\
          \ but I don't know if other UIs support it yet.</p>\n</blockquote>\n<p>I\
          \ deleted the file and now got a different error when trying to load  the\
          \ model (and yes, I'm using the text-generation-webui):</p>\n<p>Traceback\
          \ (most recent call last):<br>File \u201CD:\\Chatbot\\text-generation-webui\\\
          server.py\u201D, line 101, in load_model_wrapper<br>shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)<br>File \u201CD:\\Chatbot\\text-generation-webui\\\
          modules\\models.py\u201D, line 207, in load_model<br>model = LoaderClass.from_pretrained(checkpoint,\
          \ **params)<br>File \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained<br>return\
          \ model_class.from_pretrained(<br>File \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2405,\
          \ in from_pretrained<br>raise EnvironmentError(<br>OSError: Error no file\
          \ named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TheBloke_wizardLM-7B-GPTQ.</p>\n"
        raw: "> Firstly, please delete file `pytorch_model.bin.index.json`. That file\
          \ shouldn't have been there and I've deleted it from the repo now.\n> \n\
          > However I think you getting this message means you're not loading with\
          \ GPTQ-for-LLaMa code. \n> \n> Is this text-generation-webui? If not then\
          \ it's probably not going to be able to load this model.  These models are\
          \ GPTQ quantised and require GPTQ-for-LLaMa's `llama_inference.py` to read\
          \ them.\n> \n> That's available as a plugin for text-generation-webui but\
          \ I don't know if other UIs support it yet.\n\nI deleted the file and now\
          \ got a different error when trying to load  the model (and yes, I'm using\
          \ the text-generation-webui):\n\nTraceback (most recent call last):\nFile\
          \ \u201CD:\\Chatbot\\text-generation-webui\\server.py\u201D, line 101, in\
          \ load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
          File \u201CD:\\Chatbot\\text-generation-webui\\modules\\models.py\u201D\
          , line 207, in load_model\nmodel = LoaderClass.from_pretrained(checkpoint,\
          \ **params)\nFile \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\n\
          return model_class.from_pretrained(\nFile \u201CD:\\Chatbot\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2405,\
          \ in from_pretrained\nraise EnvironmentError(\nOSError: Error no file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TheBloke_wizardLM-7B-GPTQ."
        updatedAt: '2023-04-26T11:23:48.668Z'
      numEdits: 1
      reactions: []
    id: 64490985f88f1495f0962c3a
    type: comment
  author: ElvisM
  content: "> Firstly, please delete file `pytorch_model.bin.index.json`. That file\
    \ shouldn't have been there and I've deleted it from the repo now.\n> \n> However\
    \ I think you getting this message means you're not loading with GPTQ-for-LLaMa\
    \ code. \n> \n> Is this text-generation-webui? If not then it's probably not going\
    \ to be able to load this model.  These models are GPTQ quantised and require\
    \ GPTQ-for-LLaMa's `llama_inference.py` to read them.\n> \n> That's available\
    \ as a plugin for text-generation-webui but I don't know if other UIs support\
    \ it yet.\n\nI deleted the file and now got a different error when trying to load\
    \  the model (and yes, I'm using the text-generation-webui):\n\nTraceback (most\
    \ recent call last):\nFile \u201CD:\\Chatbot\\text-generation-webui\\server.py\u201D\
    , line 101, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
    File \u201CD:\\Chatbot\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 207, in load_model\nmodel = LoaderClass.from_pretrained(checkpoint, **params)\n\
    File \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\nreturn model_class.from_pretrained(\n\
    File \u201CD:\\Chatbot\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\u201D, line 2405, in from_pretrained\nraise EnvironmentError(\n\
    OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index\
    \ or flax_model.msgpack found in directory models\\TheBloke_wizardLM-7B-GPTQ."
  created_at: 2023-04-26 10:22:45+00:00
  edited: true
  hidden: false
  id: 64490985f88f1495f0962c3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T11:23:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>In which case you''re not running text-gen-ui with the right command
          line arguments.  From the README:</p>

          <pre><code>cd text-generation-webui

          python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type
          Llama # add any other command line args you want

          </code></pre>

          '
        raw: 'In which case you''re not running text-gen-ui with the right command
          line arguments.  From the README:


          ```

          cd text-generation-webui

          python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type
          Llama # add any other command line args you want

          ```'
        updatedAt: '2023-04-26T11:23:52.019Z'
      numEdits: 0
      reactions: []
    id: 644909c8ab5abd92785fffc9
    type: comment
  author: TheBloke
  content: 'In which case you''re not running text-gen-ui with the right command line
    arguments.  From the README:


    ```

    cd text-generation-webui

    python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type
    Llama # add any other command line args you want

    ```'
  created_at: 2023-04-26 10:23:52+00:00
  edited: false
  hidden: false
  id: 644909c8ab5abd92785fffc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
      fullname: Leonel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElvisM
      type: user
    createdAt: '2023-04-26T11:35:49.000Z'
    data:
      edited: false
      editors:
      - ElvisM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
          fullname: Leonel
          isHf: false
          isPro: false
          name: ElvisM
          type: user
        html: '<blockquote>

          <p>In which case you''re not running text-gen-ui with the right command
          line arguments.  From the README:</p>

          <pre><code>cd text-generation-webui

          python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type
          Llama # add any other command line args you want

          </code></pre>

          </blockquote>

          <p>That did it. Thanks. But the model seems to be running slow on my RTX
          2060 6GB VRAM.</p>

          '
        raw: "> In which case you're not running text-gen-ui with the right command\
          \ line arguments.  From the README:\n> \n> ```\n> cd text-generation-webui\n\
          > python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type\
          \ Llama # add any other command line args you want\n> ```\n\nThat did it.\
          \ Thanks. But the model seems to be running slow on my RTX 2060 6GB VRAM."
        updatedAt: '2023-04-26T11:35:49.397Z'
      numEdits: 0
      reactions: []
    id: 64490c95d16a70c01591758e
    type: comment
  author: ElvisM
  content: "> In which case you're not running text-gen-ui with the right command\
    \ line arguments.  From the README:\n> \n> ```\n> cd text-generation-webui\n>\
    \ python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type\
    \ Llama # add any other command line args you want\n> ```\n\nThat did it. Thanks.\
    \ But the model seems to be running slow on my RTX 2060 6GB VRAM."
  created_at: 2023-04-26 10:35:49+00:00
  edited: false
  hidden: false
  id: 64490c95d16a70c01591758e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T11:37:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I''ve had reports of this. It seems to run slowly for people
          using ooba''s fork of GPTQ-for-LLaMa. </p>

          <p>I''m making a new model to see if that''s quicker. I''ll ping you when
          it''s ready to try</p>

          '
        raw: "Yeah I've had reports of this. It seems to run slowly for people using\
          \ ooba's fork of GPTQ-for-LLaMa. \n\nI'm making a new model to see if that's\
          \ quicker. I'll ping you when it's ready to try"
        updatedAt: '2023-04-26T11:37:08.298Z'
      numEdits: 0
      reactions: []
    id: 64490ce4ab5abd927860554f
    type: comment
  author: TheBloke
  content: "Yeah I've had reports of this. It seems to run slowly for people using\
    \ ooba's fork of GPTQ-for-LLaMa. \n\nI'm making a new model to see if that's quicker.\
    \ I'll ping you when it's ready to try"
  created_at: 2023-04-26 10:37:08+00:00
  edited: false
  hidden: false
  id: 64490ce4ab5abd927860554f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
      fullname: Leonel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElvisM
      type: user
    createdAt: '2023-04-26T11:57:11.000Z'
    data:
      edited: false
      editors:
      - ElvisM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
          fullname: Leonel
          isHf: false
          isPro: false
          name: ElvisM
          type: user
        html: '<blockquote>

          <p>Yeah I''ve had reports of this. It seems to run slowly for people using
          ooba''s fork of GPTQ-for-LLaMa. </p>

          <p>I''m making a new model to see if that''s quicker. I''ll ping you when
          it''s ready to try</p>

          </blockquote>

          <p>Thanks!</p>

          '
        raw: "> Yeah I've had reports of this. It seems to run slowly for people using\
          \ ooba's fork of GPTQ-for-LLaMa. \n> \n> I'm making a new model to see if\
          \ that's quicker. I'll ping you when it's ready to try\n\nThanks!"
        updatedAt: '2023-04-26T11:57:11.806Z'
      numEdits: 0
      reactions: []
    id: 64491197f88f1495f096fad1
    type: comment
  author: ElvisM
  content: "> Yeah I've had reports of this. It seems to run slowly for people using\
    \ ooba's fork of GPTQ-for-LLaMa. \n> \n> I'm making a new model to see if that's\
    \ quicker. I'll ping you when it's ready to try\n\nThanks!"
  created_at: 2023-04-26 10:57:11+00:00
  edited: false
  hidden: false
  id: 64491197f88f1495f096fad1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T08:15:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Performance issues are resolved now. Re-download <code>config.json</code></p>

          '
        raw: Performance issues are resolved now. Re-download `config.json`
        updatedAt: '2023-04-28T08:15:56.982Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ElvisM
      relatedEventId: 644b80bc840601ec7f189c61
    id: 644b80bc840601ec7f189c60
    type: comment
  author: TheBloke
  content: Performance issues are resolved now. Re-download `config.json`
  created_at: 2023-04-28 07:15:56+00:00
  edited: false
  hidden: false
  id: 644b80bc840601ec7f189c60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T08:15:56.000Z'
    data:
      status: closed
    id: 644b80bc840601ec7f189c61
    type: status-change
  author: TheBloke
  created_at: 2023-04-28 07:15:56+00:00
  id: 644b80bc840601ec7f189c61
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/wizardLM-7B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Gives me the following error
