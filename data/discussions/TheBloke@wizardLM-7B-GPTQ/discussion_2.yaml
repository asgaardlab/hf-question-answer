!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CyberTimon
conflicting_files: null
created_at: 2023-04-26 09:55:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T10:55:14.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Hello! Just downloaded this model. I started the webui with "python3
          server.py --model wizardlm-7b-4bit-128g --gpu-memory 12 --wbits 4 --groupsize
          128 --model_type llama --listen-host 0.0.0.0 --listen --extensions api --xformers
          --listen-port 21129"</p>

          <p>But I get only 2-3 Tokens / sec. With vicuna 1.1 13b 4bit I get 30 tokens
          per second.  (I use the same command)<br>I have a rtx 3060 12gb. </p>

          '
        raw: "Hello! Just downloaded this model. I started the webui with \"python3\
          \ server.py --model wizardlm-7b-4bit-128g --gpu-memory 12 --wbits 4 --groupsize\
          \ 128 --model_type llama --listen-host 0.0.0.0 --listen --extensions api\
          \ --xformers --listen-port 21129\"\r\n\r\nBut I get only 2-3 Tokens / sec.\
          \ With vicuna 1.1 13b 4bit I get 30 tokens per second.  (I use the same\
          \ command)\r\nI have a rtx 3060 12gb. "
        updatedAt: '2023-04-26T10:55:14.026Z'
      numEdits: 0
      reactions: []
    id: 644903123adf50d8640827a8
    type: comment
  author: CyberTimon
  content: "Hello! Just downloaded this model. I started the webui with \"python3\
    \ server.py --model wizardlm-7b-4bit-128g --gpu-memory 12 --wbits 4 --groupsize\
    \ 128 --model_type llama --listen-host 0.0.0.0 --listen --extensions api --xformers\
    \ --listen-port 21129\"\r\n\r\nBut I get only 2-3 Tokens / sec. With vicuna 1.1\
    \ 13b 4bit I get 30 tokens per second.  (I use the same command)\r\nI have a rtx\
    \ 3060 12gb. "
  created_at: 2023-04-26 09:55:14+00:00
  edited: false
  hidden: false
  id: 644903123adf50d8640827a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T11:00:58.000Z'
    data:
      edited: true
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>GPU util is 100% on both models and I have the oobabooga gpt q</p>

          '
        raw: GPU util is 100% on both models and I have the oobabooga gpt q
        updatedAt: '2023-04-26T11:03:35.578Z'
      numEdits: 1
      reactions: []
    id: 6449046ad16a70c0159098e0
    type: comment
  author: CyberTimon
  content: GPU util is 100% on both models and I have the oobabooga gpt q
  created_at: 2023-04-26 10:00:58+00:00
  edited: true
  hidden: false
  id: 6449046ad16a70c0159098e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
      fullname: Leonel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElvisM
      type: user
    createdAt: '2023-04-26T11:34:16.000Z'
    data:
      edited: false
      editors:
      - ElvisM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
          fullname: Leonel
          isHf: false
          isPro: false
          name: ElvisM
          type: user
        html: '<p>Same here.</p>

          '
        raw: Same here.
        updatedAt: '2023-04-26T11:34:16.640Z'
      numEdits: 0
      reactions: []
    id: 64490c38d5d86def91cd8d7b
    type: comment
  author: ElvisM
  content: Same here.
  created_at: 2023-04-26 10:34:16+00:00
  edited: false
  hidden: false
  id: 64490c38d5d86def91cd8d7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T11:37:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m making a new model, created with ooba''s old fork of GPTQ-for-LLaMa.
          I''ll ping you when it''s ready for testing.</p>

          '
        raw: I'm making a new model, created with ooba's old fork of GPTQ-for-LLaMa.
          I'll ping you when it's ready for testing.
        updatedAt: '2023-04-26T11:37:56.160Z'
      numEdits: 0
      reactions: []
    id: 64490d14d5d86def91cda25f
    type: comment
  author: TheBloke
  content: I'm making a new model, created with ooba's old fork of GPTQ-for-LLaMa.
    I'll ping you when it's ready for testing.
  created_at: 2023-04-26 10:37:56+00:00
  edited: false
  hidden: false
  id: 64490d14d5d86def91cda25f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T11:46:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\">@<span class=\"\
          underline\">CyberTimon</span></a></span>\n\n\t</span></span> and <span data-props=\"\
          {&quot;user&quot;:&quot;ElvisM&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/ElvisM\">@<span class=\"underline\">ElvisM</span></a></span>\n\
          \n\t</span></span> can you please try new file: <a href=\"https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors\"\
          >https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors</a></p>\n\
          <p>Make sure it's the only .safetensors file in your existing wizardLM-7B-GPTQ\
          \ directory and then run text-generation-webui as before</p>\n<p>Let me\
          \ know how the speed is.</p>\n"
        raw: '@CyberTimon and @ElvisM can you please try new file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors


          Make sure it''s the only .safetensors file in your existing wizardLM-7B-GPTQ
          directory and then run text-generation-webui as before


          Let me know how the speed is.'
        updatedAt: '2023-04-26T11:46:51.771Z'
      numEdits: 0
      reactions: []
    id: 64490f2bd5d86def91cdd5ac
    type: comment
  author: TheBloke
  content: '@CyberTimon and @ElvisM can you please try new file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors


    Make sure it''s the only .safetensors file in your existing wizardLM-7B-GPTQ directory
    and then run text-generation-webui as before


    Let me know how the speed is.'
  created_at: 2023-04-26 10:46:51+00:00
  edited: false
  hidden: false
  id: 64490f2bd5d86def91cdd5ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T11:50:40.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Thank you I test it!</p>

          '
        raw: Thank you I test it!
        updatedAt: '2023-04-26T11:50:40.133Z'
      numEdits: 0
      reactions: []
    id: 64491010d16a70c01591cb75
    type: comment
  author: CyberTimon
  content: Thank you I test it!
  created_at: 2023-04-26 10:50:40+00:00
  edited: false
  hidden: false
  id: 64491010d16a70c01591cb75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
      fullname: Leonel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElvisM
      type: user
    createdAt: '2023-04-26T12:08:58.000Z'
    data:
      edited: false
      editors:
      - ElvisM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
          fullname: Leonel
          isHf: false
          isPro: false
          name: ElvisM
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\"\
          >@<span class=\"underline\">CyberTimon</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;ElvisM&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ElvisM\">@<span class=\"\
          underline\">ElvisM</span></a></span>\n\n\t</span></span> can you please\
          \ try new file: <a href=\"https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors\"\
          >https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors</a></p>\n\
          <p>Make sure it's the only .safetensors file in your existing wizardLM-7B-GPTQ\
          \ directory and then run text-generation-webui as before</p>\n<p>Let me\
          \ know how the speed is.</p>\n</blockquote>\n<p>Still no luck here for me.\
          \ That's really odd because all the other 7b 4bit models work fine for me.</p>\n\
          <p>Output generated in 8.12 seconds (1.35 tokens/s, 11 tokens, context 39,\
          \ seed 1913998095)<br>Output generated in 11.47 seconds (0.96 tokens/s,\
          \ 11 tokens, context 66, seed 627847145)</p>\n"
        raw: "> @CyberTimon and @ElvisM can you please try new file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors\n\
          > \n> Make sure it's the only .safetensors file in your existing wizardLM-7B-GPTQ\
          \ directory and then run text-generation-webui as before\n> \n> Let me know\
          \ how the speed is.\n\nStill no luck here for me. That's really odd because\
          \ all the other 7b 4bit models work fine for me.\n\nOutput generated in\
          \ 8.12 seconds (1.35 tokens/s, 11 tokens, context 39, seed 1913998095)\n\
          Output generated in 11.47 seconds (0.96 tokens/s, 11 tokens, context 66,\
          \ seed 627847145)"
        updatedAt: '2023-04-26T12:08:58.789Z'
      numEdits: 0
      reactions: []
    id: 6449145ad16a70c0159237f6
    type: comment
  author: ElvisM
  content: "> @CyberTimon and @ElvisM can you please try new file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors\n\
    > \n> Make sure it's the only .safetensors file in your existing wizardLM-7B-GPTQ\
    \ directory and then run text-generation-webui as before\n> \n> Let me know how\
    \ the speed is.\n\nStill no luck here for me. That's really odd because all the\
    \ other 7b 4bit models work fine for me.\n\nOutput generated in 8.12 seconds (1.35\
    \ tokens/s, 11 tokens, context 39, seed 1913998095)\nOutput generated in 11.47\
    \ seconds (0.96 tokens/s, 11 tokens, context 66, seed 627847145)"
  created_at: 2023-04-26 11:08:58+00:00
  edited: false
  hidden: false
  id: 6449145ad16a70c0159237f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T12:10:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Can I ask - those "other 7b 4bit" models, are any of them safetensors
          files? Or are they all .pt files?</p>

          <p>OK I''m trying one last thing - making a .pt file instead of safetensors.
          It''ll be uploaded in a minute and I''ll ping you when ready.</p>

          '
        raw: 'Can I ask - those "other 7b 4bit" models, are any of them safetensors
          files? Or are they all .pt files?


          OK I''m trying one last thing - making a .pt file instead of safetensors.
          It''ll be uploaded in a minute and I''ll ping you when ready.'
        updatedAt: '2023-04-26T12:10:03.208Z'
      numEdits: 0
      reactions: []
    id: 6449149bd5d86def91ce5f7e
    type: comment
  author: TheBloke
  content: 'Can I ask - those "other 7b 4bit" models, are any of them safetensors
    files? Or are they all .pt files?


    OK I''m trying one last thing - making a .pt file instead of safetensors. It''ll
    be uploaded in a minute and I''ll ping you when ready.'
  created_at: 2023-04-26 11:10:03+00:00
  edited: false
  hidden: false
  id: 6449149bd5d86def91ce5f7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T12:11:59.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Thank you!<br>llava-13b-4bit-128g is also safetensors. But yes,
          every other one is .pt</p>

          '
        raw: "Thank you! \nllava-13b-4bit-128g is also safetensors. But yes, every\
          \ other one is .pt"
        updatedAt: '2023-04-26T12:11:59.889Z'
      numEdits: 0
      reactions: []
    id: 6449150ff88f1495f097568a
    type: comment
  author: CyberTimon
  content: "Thank you! \nllava-13b-4bit-128g is also safetensors. But yes, every other\
    \ one is .pt"
  created_at: 2023-04-26 11:11:59+00:00
  edited: false
  hidden: false
  id: 6449150ff88f1495f097568a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T12:12:36.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>I converted a model like this: python llama.py ./chimera-7b c4 --wbits
          4 --true-sequential --groupsize 128 --save chimera7b-4bit-128g.pt</p>

          '
        raw: 'I converted a model like this: python llama.py ./chimera-7b c4 --wbits
          4 --true-sequential --groupsize 128 --save chimera7b-4bit-128g.pt'
        updatedAt: '2023-04-26T12:12:36.983Z'
      numEdits: 0
      reactions: []
    id: 64491534f88f1495f0975a44
    type: comment
  author: CyberTimon
  content: 'I converted a model like this: python llama.py ./chimera-7b c4 --wbits
    4 --true-sequential --groupsize 128 --save chimera7b-4bit-128g.pt'
  created_at: 2023-04-26 11:12:36+00:00
  edited: false
  hidden: false
  id: 64491534f88f1495f0975a44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T12:20:56.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\"\
          >@<span class=\"underline\">CyberTimon</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;ElvisM&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ElvisM\">@<span class=\"\
          underline\">ElvisM</span></a></span>\n\n\t</span></span> can you please\
          \ try new file: <a href=\"https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors\"\
          >https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors</a></p>\n\
          <p>Make sure it's the only .safetensors file in your existing wizardLM-7B-GPTQ\
          \ directory and then run text-generation-webui as before</p>\n<p>Let me\
          \ know how the speed is.</p>\n</blockquote>\n<p>Yeah still slow for me too</p>\n"
        raw: "> @CyberTimon and @ElvisM can you please try new file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors\n\
          > \n> Make sure it's the only .safetensors file in your existing wizardLM-7B-GPTQ\
          \ directory and then run text-generation-webui as before\n> \n> Let me know\
          \ how the speed is.\n\nYeah still slow for me too"
        updatedAt: '2023-04-26T12:20:56.444Z'
      numEdits: 0
      reactions: []
    id: 64491728d5d86def91cea37a
    type: comment
  author: CyberTimon
  content: "> @CyberTimon and @ElvisM can you please try new file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.safetensors\n\
    > \n> Make sure it's the only .safetensors file in your existing wizardLM-7B-GPTQ\
    \ directory and then run text-generation-webui as before\n> \n> Let me know how\
    \ the speed is.\n\nYeah still slow for me too"
  created_at: 2023-04-26 11:20:56+00:00
  edited: false
  hidden: false
  id: 64491728d5d86def91cea37a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T12:21:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\">@<span class=\"\
          underline\">CyberTimon</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;ElvisM&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/ElvisM\">@<span class=\"underline\">ElvisM</span></a></span>\n\
          \n\t</span></span> OK please try this file: <a href=\"https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.pt\"\
          >https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.pt</a></p>\n\
          <p>As before, make sure it's the only model file in the wizardLM-7B-GPTQ\
          \ directory</p>\n"
        raw: '@CyberTimon @ElvisM OK please try this file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.pt


          As before, make sure it''s the only model file in the wizardLM-7B-GPTQ directory'
        updatedAt: '2023-04-26T12:21:27.874Z'
      numEdits: 0
      reactions: []
    id: 64491747d5d86def91cea651
    type: comment
  author: TheBloke
  content: '@CyberTimon @ElvisM OK please try this file: https://huggingface.co/TheBloke/wizardLM-7B-GPTQ/blob/main/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.pt


    As before, make sure it''s the only model file in the wizardLM-7B-GPTQ directory'
  created_at: 2023-04-26 11:21:27+00:00
  edited: false
  hidden: false
  id: 64491747d5d86def91cea651
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T12:31:28.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Ahh it''s still extremly slow. Btw I need to have the configs and
          tokenizer models etc in the directory, otherwise oobabooga gives an error</p>

          '
        raw: Ahh it's still extremly slow. Btw I need to have the configs and tokenizer
          models etc in the directory, otherwise oobabooga gives an error
        updatedAt: '2023-04-26T12:31:28.833Z'
      numEdits: 0
      reactions: []
    id: 644919a0d5d86def91cedd27
    type: comment
  author: CyberTimon
  content: Ahh it's still extremly slow. Btw I need to have the configs and tokenizer
    models etc in the directory, otherwise oobabooga gives an error
  created_at: 2023-04-26 11:31:28+00:00
  edited: false
  hidden: false
  id: 644919a0d5d86def91cedd27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T12:33:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah you need all the json files and tokenizer.model, I just meant
          don''t have any other .pt or .safetensors files.</p>

          <p>In which case sorry, I''m out of ideas. Don''t know what else to try,
          or why it would be so slow for you guys but fine for me on Linux. I don''t
          know of any reason for that to be the case that would vary by model.</p>

          <p>As a sanity check: have you recently run one of the other 7B models,
          and confirmed it''s still running at a normal speed?</p>

          '
        raw: 'Yeah you need all the json files and tokenizer.model, I just meant don''t
          have any other .pt or .safetensors files.


          In which case sorry, I''m out of ideas. Don''t know what else to try, or
          why it would be so slow for you guys but fine for me on Linux. I don''t
          know of any reason for that to be the case that would vary by model.


          As a sanity check: have you recently run one of the other 7B models, and
          confirmed it''s still running at a normal speed?'
        updatedAt: '2023-04-26T12:33:26.610Z'
      numEdits: 0
      reactions: []
    id: 64491a16f88f1495f097d193
    type: comment
  author: TheBloke
  content: 'Yeah you need all the json files and tokenizer.model, I just meant don''t
    have any other .pt or .safetensors files.


    In which case sorry, I''m out of ideas. Don''t know what else to try, or why it
    would be so slow for you guys but fine for me on Linux. I don''t know of any reason
    for that to be the case that would vary by model.


    As a sanity check: have you recently run one of the other 7B models, and confirmed
    it''s still running at a normal speed?'
  created_at: 2023-04-26 11:33:26+00:00
  edited: false
  hidden: false
  id: 64491a16f88f1495f097d193
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T12:40:07.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Yes I have. Other 7b and 13b models. All work very very fast. No
          idea why this generates so much trouble</p>

          '
        raw: Yes I have. Other 7b and 13b models. All work very very fast. No idea
          why this generates so much trouble
        updatedAt: '2023-04-26T12:40:07.332Z'
      numEdits: 0
      reactions: []
    id: 64491ba7f88f1495f097f60b
    type: comment
  author: CyberTimon
  content: Yes I have. Other 7b and 13b models. All work very very fast. No idea why
    this generates so much trouble
  created_at: 2023-04-26 11:40:07+00:00
  edited: false
  hidden: false
  id: 64491ba7f88f1495f097f60b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T12:48:25.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>I quantize it myself - if it still makes issues, then I think it
          has something to do with the model delta''s provided by the wizardlm autors</p>

          '
        raw: I quantize it myself - if it still makes issues, then I think it has
          something to do with the model delta's provided by the wizardlm autors
        updatedAt: '2023-04-26T12:48:25.432Z'
      numEdits: 0
      reactions: []
    id: 64491d99d5d86def91cf3cdb
    type: comment
  author: CyberTimon
  content: I quantize it myself - if it still makes issues, then I think it has something
    to do with the model delta's provided by the wizardlm autors
  created_at: 2023-04-26 11:48:25+00:00
  edited: false
  hidden: false
  id: 64491d99d5d86def91cf3cdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T12:55:11.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah this makes no sense to me.  Here is me doing inference on the\
          \ .pt file I just uploaded, from the command line using ooba's fork of GPTQ-for-LLaMa:</p>\n\
          <pre><code>root@c14ca38d7f61:~/ooba-gptq-llama# time CUDA_VISIBLE_DEVICES=0\
          \ python llama_inference.py --wbits 4 --groupsize 128 --load /workspace/wizard-gptq/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.pt\
          \ --text \"Llamas are \" --min_length 100 --max_length 250 --temperature\
          \ 0.7 /workspace/wizard-gptq\nLoading model ...\nDone.\n&lt;s&gt; Llamas\
          \ are 36-inch tall creatures that are native to South America. They are\
          \ known for their long, luxuriant coats, which can be woven into cloth.\n\
          Llamas are social animals that live in groups called herds. They are grazers\
          \ and spend most of their days eating grass. Llamas have a keen sense of\
          \ smell, which they use to detect predators.\nLlamas are domesticated animals\
          \ that have been used for their wool for thousands of years. They are often\
          \ raised for their meat as well.\nLlamas are known for their calm, gentle\
          \ nature and can be trained as pack animals. They are often used in therapeutic\
          \ programs for people with disabilities.&lt;/s&gt;\n\nreal\t0m28.305s\n\
          user\t0m27.235s\nsys\t0m14.359s\n</code></pre>\n<p>This should be the exact\
          \ same code you're using. And as you see, it's fine. I don't know what that\
          \ is in tokens/s but it's 111 words so that's at least 5 tokens/s, probably\
          \ more given that time above includes the time to load the model, not just\
          \ inference.</p>\n<p>Can you try the same test , from the command line/Powershell?</p>\n\
          <p>I'm also making one more .pt file. I had a problem with the CUDA kernel\
          \ on my ooba GPTQ-for-LLaMa. I don't think it affected the quantisation\
          \ (as you see it infers fine above), but I'm re-doing the file just to be\
          \ sure.</p>\n"
        raw: "Yeah this makes no sense to me.  Here is me doing inference on the .pt\
          \ file I just uploaded, from the command line using ooba's fork of GPTQ-for-LLaMa:\n\
          ```\nroot@c14ca38d7f61:~/ooba-gptq-llama# time CUDA_VISIBLE_DEVICES=0 python\
          \ llama_inference.py --wbits 4 --groupsize 128 --load /workspace/wizard-gptq/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.pt\
          \ --text \"Llamas are \" --min_length 100 --max_length 250 --temperature\
          \ 0.7 /workspace/wizard-gptq\nLoading model ...\nDone.\n<s> Llamas are 36-inch\
          \ tall creatures that are native to South America. They are known for their\
          \ long, luxuriant coats, which can be woven into cloth.\nLlamas are social\
          \ animals that live in groups called herds. They are grazers and spend most\
          \ of their days eating grass. Llamas have a keen sense of smell, which they\
          \ use to detect predators.\nLlamas are domesticated animals that have been\
          \ used for their wool for thousands of years. They are often raised for\
          \ their meat as well.\nLlamas are known for their calm, gentle nature and\
          \ can be trained as pack animals. They are often used in therapeutic programs\
          \ for people with disabilities.</s>\n\nreal\t0m28.305s\nuser\t0m27.235s\n\
          sys\t0m14.359s\n```\n\nThis should be the exact same code you're using.\
          \ And as you see, it's fine. I don't know what that is in tokens/s but it's\
          \ 111 words so that's at least 5 tokens/s, probably more given that time\
          \ above includes the time to load the model, not just inference.\n\nCan\
          \ you try the same test , from the command line/Powershell?\n\nI'm also\
          \ making one more .pt file. I had a problem with the CUDA kernel on my ooba\
          \ GPTQ-for-LLaMa. I don't think it affected the quantisation (as you see\
          \ it infers fine above), but I'm re-doing the file just to be sure."
        updatedAt: '2023-04-26T12:55:25.872Z'
      numEdits: 1
      reactions: []
    id: 64491f2fd16a70c015934386
    type: comment
  author: TheBloke
  content: "Yeah this makes no sense to me.  Here is me doing inference on the .pt\
    \ file I just uploaded, from the command line using ooba's fork of GPTQ-for-LLaMa:\n\
    ```\nroot@c14ca38d7f61:~/ooba-gptq-llama# time CUDA_VISIBLE_DEVICES=0 python llama_inference.py\
    \ --wbits 4 --groupsize 128 --load /workspace/wizard-gptq/wizardLM-7B-GPTQ-4bit-128g.ooba.no-act-order.pt\
    \ --text \"Llamas are \" --min_length 100 --max_length 250 --temperature 0.7 /workspace/wizard-gptq\n\
    Loading model ...\nDone.\n<s> Llamas are 36-inch tall creatures that are native\
    \ to South America. They are known for their long, luxuriant coats, which can\
    \ be woven into cloth.\nLlamas are social animals that live in groups called herds.\
    \ They are grazers and spend most of their days eating grass. Llamas have a keen\
    \ sense of smell, which they use to detect predators.\nLlamas are domesticated\
    \ animals that have been used for their wool for thousands of years. They are\
    \ often raised for their meat as well.\nLlamas are known for their calm, gentle\
    \ nature and can be trained as pack animals. They are often used in therapeutic\
    \ programs for people with disabilities.</s>\n\nreal\t0m28.305s\nuser\t0m27.235s\n\
    sys\t0m14.359s\n```\n\nThis should be the exact same code you're using. And as\
    \ you see, it's fine. I don't know what that is in tokens/s but it's 111 words\
    \ so that's at least 5 tokens/s, probably more given that time above includes\
    \ the time to load the model, not just inference.\n\nCan you try the same test\
    \ , from the command line/Powershell?\n\nI'm also making one more .pt file. I\
    \ had a problem with the CUDA kernel on my ooba GPTQ-for-LLaMa. I don't think\
    \ it affected the quantisation (as you see it infers fine above), but I'm re-doing\
    \ the file just to be sure."
  created_at: 2023-04-26 11:55:11+00:00
  edited: true
  hidden: false
  id: 64491f2fd16a70c015934386
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T12:55:32.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>I somehow can''t quantize it myself. It uses more than 12gb vram.
          I successfully quantized other 7b llama models so I more and more think
          this model have some problems</p>

          '
        raw: I somehow can't quantize it myself. It uses more than 12gb vram. I successfully
          quantized other 7b llama models so I more and more think this model have
          some problems
        updatedAt: '2023-04-26T12:55:32.952Z'
      numEdits: 0
      reactions: []
    id: 64491f44d5d86def91cf6638
    type: comment
  author: CyberTimon
  content: I somehow can't quantize it myself. It uses more than 12gb vram. I successfully
    quantized other 7b llama models so I more and more think this model have some
    problems
  created_at: 2023-04-26 11:55:32+00:00
  edited: false
  hidden: false
  id: 64491f44d5d86def91cf6638
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
      fullname: Leonel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElvisM
      type: user
    createdAt: '2023-04-26T13:00:26.000Z'
    data:
      edited: false
      editors:
      - ElvisM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60174b38538d1b0ab03b511aa575698.svg
          fullname: Leonel
          isHf: false
          isPro: false
          name: ElvisM
          type: user
        html: '<blockquote>

          <p>Can I ask - those "other 7b 4bit" models, are any of them safetensors
          files? Or are they all .pt files?</p>

          <p>OK I''m trying one last thing - making a .pt file instead of safetensors.
          It''ll be uploaded in a minute and I''ll ping you when ready.</p>

          </blockquote>

          <p>Other than the WizardLM model, I have two models using the safetensors
          format and two in .pt. They both still work fine (I just tested them). Maybe
          it''s something to do with GPTQ since it''s been a while that textgen-webui
          is still not using the version which allows Vicuna in safetensors to work
          properly (it outputs gibberish). I don''t know why oobaboga still didn''t
          update the installer so that it uses the latest GPTQ.</p>

          '
        raw: "> Can I ask - those \"other 7b 4bit\" models, are any of them safetensors\
          \ files? Or are they all .pt files?\n> \n> OK I'm trying one last thing\
          \ - making a .pt file instead of safetensors. It'll be uploaded in a minute\
          \ and I'll ping you when ready.\n\nOther than the WizardLM model, I have\
          \ two models using the safetensors format and two in .pt. They both still\
          \ work fine (I just tested them). Maybe it's something to do with GPTQ since\
          \ it's been a while that textgen-webui is still not using the version which\
          \ allows Vicuna in safetensors to work properly (it outputs gibberish).\
          \ I don't know why oobaboga still didn't update the installer so that it\
          \ uses the latest GPTQ."
        updatedAt: '2023-04-26T13:00:26.470Z'
      numEdits: 0
      reactions: []
    id: 6449206ad5d86def91cf8909
    type: comment
  author: ElvisM
  content: "> Can I ask - those \"other 7b 4bit\" models, are any of them safetensors\
    \ files? Or are they all .pt files?\n> \n> OK I'm trying one last thing - making\
    \ a .pt file instead of safetensors. It'll be uploaded in a minute and I'll ping\
    \ you when ready.\n\nOther than the WizardLM model, I have two models using the\
    \ safetensors format and two in .pt. They both still work fine (I just tested\
    \ them). Maybe it's something to do with GPTQ since it's been a while that textgen-webui\
    \ is still not using the version which allows Vicuna in safetensors to work properly\
    \ (it outputs gibberish). I don't know why oobaboga still didn't update the installer\
    \ so that it uses the latest GPTQ."
  created_at: 2023-04-26 12:00:26+00:00
  edited: false
  hidden: false
  id: 6449206ad5d86def91cf8909
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T13:01:29.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I somehow can''t quantize it myself. It uses more than 12gb vram. I successfully
          quantized other 7b llama models so I more and more think this model have
          some problems</p>

          </blockquote>

          <p>Yeah it needs a tiny bit over 12GB - 12347 MiB is the highest I''ve seen
          so far (12.058GB)</p>

          <p>This is while re-running GPTQ to a .pt file with ooba-gptq-llama:</p>

          <pre><code>timestamp, name, pci.bus_id, driver_version, pstate, pcie.link.gen.max,
          pcie.link.gen.current, temperature.gpu, utilization.gpu [%], utilization.memory
          [%], memory.total [MiB], memory.free [MiB], memory.used [MiB]

          2023/04/26 13:00:02.777, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 70, 100 %, 56 %, 24564 MiB, 12489 MiB, 11727 MiB

          2023/04/26 13:00:07.779, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 44, 14 %, 0 %, 24564 MiB, 12489 MiB, 11727 MiB

          2023/04/26 13:00:12.780, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 69, 100 %, 55 %, 24564 MiB, 13169 MiB, 11047 MiB

          2023/04/26 13:00:17.781, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 46, 13 %, 1 %, 24564 MiB, 14131 MiB, 10085 MiB

          2023/04/26 13:00:22.783, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 70, 100 %, 59 %, 24564 MiB, 13295 MiB, 10921 MiB

          2023/04/26 13:00:27.784, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 70, 100 %, 57 %, 24564 MiB, 12845 MiB, 11371 MiB

          </code></pre>

          '
        raw: '> I somehow can''t quantize it myself. It uses more than 12gb vram.
          I successfully quantized other 7b llama models so I more and more think
          this model have some problems


          Yeah it needs a tiny bit over 12GB - 12347 MiB is the highest I''ve seen
          so far (12.058GB)


          This is while re-running GPTQ to a .pt file with ooba-gptq-llama:

          ```

          timestamp, name, pci.bus_id, driver_version, pstate, pcie.link.gen.max,
          pcie.link.gen.current, temperature.gpu, utilization.gpu [%], utilization.memory
          [%], memory.total [MiB], memory.free [MiB], memory.used [MiB]

          2023/04/26 13:00:02.777, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 70, 100 %, 56 %, 24564 MiB, 12489 MiB, 11727 MiB

          2023/04/26 13:00:07.779, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 44, 14 %, 0 %, 24564 MiB, 12489 MiB, 11727 MiB

          2023/04/26 13:00:12.780, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 69, 100 %, 55 %, 24564 MiB, 13169 MiB, 11047 MiB

          2023/04/26 13:00:17.781, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 46, 13 %, 1 %, 24564 MiB, 14131 MiB, 10085 MiB

          2023/04/26 13:00:22.783, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 70, 100 %, 59 %, 24564 MiB, 13295 MiB, 10921 MiB

          2023/04/26 13:00:27.784, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
          P2, 4, 4, 70, 100 %, 57 %, 24564 MiB, 12845 MiB, 11371 MiB

          ```'
        updatedAt: '2023-04-26T13:01:50.323Z'
      numEdits: 1
      reactions: []
    id: 644920a9f88f1495f09878df
    type: comment
  author: TheBloke
  content: '> I somehow can''t quantize it myself. It uses more than 12gb vram. I
    successfully quantized other 7b llama models so I more and more think this model
    have some problems


    Yeah it needs a tiny bit over 12GB - 12347 MiB is the highest I''ve seen so far
    (12.058GB)


    This is while re-running GPTQ to a .pt file with ooba-gptq-llama:

    ```

    timestamp, name, pci.bus_id, driver_version, pstate, pcie.link.gen.max, pcie.link.gen.current,
    temperature.gpu, utilization.gpu [%], utilization.memory [%], memory.total [MiB],
    memory.free [MiB], memory.used [MiB]

    2023/04/26 13:00:02.777, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
    P2, 4, 4, 70, 100 %, 56 %, 24564 MiB, 12489 MiB, 11727 MiB

    2023/04/26 13:00:07.779, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
    P2, 4, 4, 44, 14 %, 0 %, 24564 MiB, 12489 MiB, 11727 MiB

    2023/04/26 13:00:12.780, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
    P2, 4, 4, 69, 100 %, 55 %, 24564 MiB, 13169 MiB, 11047 MiB

    2023/04/26 13:00:17.781, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
    P2, 4, 4, 46, 13 %, 1 %, 24564 MiB, 14131 MiB, 10085 MiB

    2023/04/26 13:00:22.783, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
    P2, 4, 4, 70, 100 %, 59 %, 24564 MiB, 13295 MiB, 10921 MiB

    2023/04/26 13:00:27.784, NVIDIA GeForce RTX 4090, 00000000:C1:00.0, 525.78.01,
    P2, 4, 4, 70, 100 %, 57 %, 24564 MiB, 12845 MiB, 11371 MiB

    ```'
  created_at: 2023-04-26 12:01:29+00:00
  edited: true
  hidden: false
  id: 644920a9f88f1495f09878df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T13:03:40.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I don''t think there''s anything to suggest that there''s anything
          "wrong" with the model. It''s just different in some way.</p>

          <p>It''s working perfectly fine (and doing very well for a 7B) in HF, GGML
          and GPTQ formats for me. There''s just something unusual/different causing
          it not to work for you guys as a GPTQ on Windows.</p>

          '
        raw: 'I don''t think there''s anything to suggest that there''s anything "wrong"
          with the model. It''s just different in some way.


          It''s working perfectly fine (and doing very well for a 7B) in HF, GGML
          and GPTQ formats for me. There''s just something unusual/different causing
          it not to work for you guys as a GPTQ on Windows.'
        updatedAt: '2023-04-26T13:03:54.910Z'
      numEdits: 1
      reactions: []
    id: 6449212cd16a70c01593789f
    type: comment
  author: TheBloke
  content: 'I don''t think there''s anything to suggest that there''s anything "wrong"
    with the model. It''s just different in some way.


    It''s working perfectly fine (and doing very well for a 7B) in HF, GGML and GPTQ
    formats for me. There''s just something unusual/different causing it not to work
    for you guys as a GPTQ on Windows.'
  created_at: 2023-04-26 12:03:40+00:00
  edited: true
  hidden: false
  id: 6449212cd16a70c01593789f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T13:04:33.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p><code>(base) cybertimon@server:~/Repositorys/text-generation-webui/repositories/GPTQ-for-LLaMa$
          python3 llama_inference.py --wbits 4 --groupsize 128 --load /home/cybertimon/Repositorys/text-generation-webui/models/wizardlm-7b-4bit/pytorch_model.pt
          --text "Llamas are " --min_length 100 --max_length 250 --temperature 0.7
          /home/cybertimon/Repositorys/text-generation-webui/models/wizardlm-7b-4bit
          Loading model ... Done. &lt;s&gt; Llamas are 3-4 feet tall at the shoulder
          and can weigh up to 200 pounds. They have long, curved necks and short,
          sturdy legs. They have a coat of fur that can be brown, black, or white,
          with the males being larger than the females. Llamas are social animals
          and live in groups called herds. They communicate with each other through
          a variety of sounds, including bleats, bellows, and growls. Llamas are primarily
          grazers and can eat up to 60 pounds of vegetation per day. They have a digestive
          system similar to humans, with a cecum (a pouch at the beginning of the
          large intestine) that helps break down plant material. Llamas are native
          to the Andean Mountains of South America and were domesticated by the Incas
          over 5,000 years ago. Today, they are primarily raised for their wool, which
          is soft and warm, and is used in clothing, insulation, and other products.&lt;/s&gt;</code></p>

          <p>It didn''t print the time to execute but it was like 3-4 min for this
          text with your .pt file</p>

          '
        raw: '```(base) cybertimon@server:~/Repositorys/text-generation-webui/repositories/GPTQ-for-LLaMa$
          python3 llama_inference.py --wbits 4 --groupsize 128 --load /home/cybertimon/Repositorys/text-generation-webui/models/wizardlm-7b-4bit/pytorch_model.pt
          --text "Llamas are " --min_length 100 --max_length 250 --temperature 0.7
          /home/cybertimon/Repositorys/text-generation-webui/models/wizardlm-7b-4bit

          Loading model ...

          Done.

          <s> Llamas are 3-4 feet tall at the shoulder and can weigh up to 200 pounds.
          They have long, curved necks and short, sturdy legs. They have a coat of
          fur that can be brown, black, or white, with the males being larger than
          the females. Llamas are social animals and live in groups called herds.
          They communicate with each other through a variety of sounds, including
          bleats, bellows, and growls. Llamas are primarily grazers and can eat up
          to 60 pounds of vegetation per day. They have a digestive system similar
          to humans, with a cecum (a pouch at the beginning of the large intestine)
          that helps break down plant material. Llamas are native to the Andean Mountains
          of South America and were domesticated by the Incas over 5,000 years ago.
          Today, they are primarily raised for their wool, which is soft and warm,
          and is used in clothing, insulation, and other products.</s>```


          It didn''t print the time to execute but it was like 3-4 min for this text
          with your .pt file'
        updatedAt: '2023-04-26T13:04:33.998Z'
      numEdits: 0
      reactions: []
    id: 64492161d16a70c015937de7
    type: comment
  author: CyberTimon
  content: '```(base) cybertimon@server:~/Repositorys/text-generation-webui/repositories/GPTQ-for-LLaMa$
    python3 llama_inference.py --wbits 4 --groupsize 128 --load /home/cybertimon/Repositorys/text-generation-webui/models/wizardlm-7b-4bit/pytorch_model.pt
    --text "Llamas are " --min_length 100 --max_length 250 --temperature 0.7 /home/cybertimon/Repositorys/text-generation-webui/models/wizardlm-7b-4bit

    Loading model ...

    Done.

    <s> Llamas are 3-4 feet tall at the shoulder and can weigh up to 200 pounds. They
    have long, curved necks and short, sturdy legs. They have a coat of fur that can
    be brown, black, or white, with the males being larger than the females. Llamas
    are social animals and live in groups called herds. They communicate with each
    other through a variety of sounds, including bleats, bellows, and growls. Llamas
    are primarily grazers and can eat up to 60 pounds of vegetation per day. They
    have a digestive system similar to humans, with a cecum (a pouch at the beginning
    of the large intestine) that helps break down plant material. Llamas are native
    to the Andean Mountains of South America and were domesticated by the Incas over
    5,000 years ago. Today, they are primarily raised for their wool, which is soft
    and warm, and is used in clothing, insulation, and other products.</s>```


    It didn''t print the time to execute but it was like 3-4 min for this text with
    your .pt file'
  created_at: 2023-04-26 12:04:33+00:00
  edited: false
  hidden: false
  id: 64492161d16a70c015937de7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T13:05:07.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<blockquote>

          <p>as a GPTQ on Windows.</p>

          </blockquote>

          <p>I''m on linux 20.04 btw</p>

          '
        raw: '> as a GPTQ on Windows.


          I''m on linux 20.04 btw'
        updatedAt: '2023-04-26T13:05:07.679Z'
      numEdits: 0
      reactions: []
    id: 64492183d5d86def91cfa472
    type: comment
  author: CyberTimon
  content: '> as a GPTQ on Windows.


    I''m on linux 20.04 btw'
  created_at: 2023-04-26 12:05:07+00:00
  edited: false
  hidden: false
  id: 64492183d5d86def91cfa472
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T13:06:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <blockquote>

          <p>as a GPTQ on Windows.</p>

          </blockquote>

          <p>I''m on linux 20.04 btw</p>

          </blockquote>

          <p>Oh! Well then I''m even more confused </p>

          <p>Are you using the latest Triton GPTQ-for-LLaMa code?  <code>git clone
          https://github.com/qwopqwop200/GPTQ-for-LLaMa</code> in <code>text-generation-webui/repositories</code>
          ?</p>

          '
        raw: "> > as a GPTQ on Windows.\n> \n> I'm on linux 20.04 btw\n\nOh! Well\
          \ then I'm even more confused \n\nAre you using the latest Triton GPTQ-for-LLaMa\
          \ code?  `git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa` in `text-generation-webui/repositories`\
          \ ?"
        updatedAt: '2023-04-26T13:06:43.091Z'
      numEdits: 0
      reactions: []
    id: 644921e3d5d86def91cfacee
    type: comment
  author: TheBloke
  content: "> > as a GPTQ on Windows.\n> \n> I'm on linux 20.04 btw\n\nOh! Well then\
    \ I'm even more confused \n\nAre you using the latest Triton GPTQ-for-LLaMa code?\
    \  `git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa` in `text-generation-webui/repositories`\
    \ ?"
  created_at: 2023-04-26 12:06:43+00:00
  edited: false
  hidden: false
  id: 644921e3d5d86def91cfacee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T13:08:08.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>No I''m using the oobabooga fork as the triton was slower than the
          oobabooga''s one. Also I''m on 22.04 not 20.04 as I wrote wrong before.</p>

          '
        raw: No I'm using the oobabooga fork as the triton was slower than the oobabooga's
          one. Also I'm on 22.04 not 20.04 as I wrote wrong before.
        updatedAt: '2023-04-26T13:08:08.785Z'
      numEdits: 0
      reactions: []
    id: 64492238d5d86def91cfb4a0
    type: comment
  author: CyberTimon
  content: No I'm using the oobabooga fork as the triton was slower than the oobabooga's
    one. Also I'm on 22.04 not 20.04 as I wrote wrong before.
  created_at: 2023-04-26 12:08:08+00:00
  edited: false
  hidden: false
  id: 64492238d5d86def91cfb4a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T13:09:23.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>With llama/vicuna 7b 4bit I get incredible fast 41 tokens/s on a
          rtx 3060 12gb.<br><code>Output generated in 8.27 seconds (41.23 tokens/s,
          341 tokens, context 10, seed 928579911)</code></p>

          '
        raw: 'With llama/vicuna 7b 4bit I get incredible fast 41 tokens/s on a rtx
          3060 12gb.

          `Output generated in 8.27 seconds (41.23 tokens/s, 341 tokens, context 10,
          seed 928579911)`'
        updatedAt: '2023-04-26T13:09:23.658Z'
      numEdits: 0
      reactions: []
    id: 64492283d5d86def91cfbc7f
    type: comment
  author: CyberTimon
  content: 'With llama/vicuna 7b 4bit I get incredible fast 41 tokens/s on a rtx 3060
    12gb.

    `Output generated in 8.27 seconds (41.23 tokens/s, 341 tokens, context 10, seed
    928579911)`'
  created_at: 2023-04-26 12:09:23+00:00
  edited: false
  hidden: false
  id: 64492283d5d86def91cfbc7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T13:09:31.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>No I''m using the oobabooga fork as the triton was slower than the oobabooga''s
          one. Also I''m on 22.04 not 20.04 as I wrote wrong before.</p>

          </blockquote>

          <p>Please try the Triton branch and let me know:</p>

          <pre><code>cd text-generation-webui/repositories

          mv GPTQ-for-LLaMa ../ooba-gptq-llama

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

          </code></pre>

          <p>If it''s still slow then this I suppose this must be a GPU-specific issue,
          and not as I thought OS/installation specific.</p>

          '
        raw: '> No I''m using the oobabooga fork as the triton was slower than the
          oobabooga''s one. Also I''m on 22.04 not 20.04 as I wrote wrong before.


          Please try the Triton branch and let me know:

          ```

          cd text-generation-webui/repositories

          mv GPTQ-for-LLaMa ../ooba-gptq-llama

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

          ```


          If it''s still slow then this I suppose this must be a GPU-specific issue,
          and not as I thought OS/installation specific.'
        updatedAt: '2023-04-26T13:12:32.298Z'
      numEdits: 2
      reactions: []
    id: 6449228bab5abd9278627254
    type: comment
  author: TheBloke
  content: '> No I''m using the oobabooga fork as the triton was slower than the oobabooga''s
    one. Also I''m on 22.04 not 20.04 as I wrote wrong before.


    Please try the Triton branch and let me know:

    ```

    cd text-generation-webui/repositories

    mv GPTQ-for-LLaMa ../ooba-gptq-llama

    git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

    ```


    If it''s still slow then this I suppose this must be a GPU-specific issue, and
    not as I thought OS/installation specific.'
  created_at: 2023-04-26 12:09:31+00:00
  edited: true
  hidden: false
  id: 6449228bab5abd9278627254
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T13:19:23.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Still slow + every other model is now also just 10 tokens / sec
          instead of 40 tokens / sec so I stay with ooba''s fork</p>

          '
        raw: Still slow + every other model is now also just 10 tokens / sec instead
          of 40 tokens / sec so I stay with ooba's fork
        updatedAt: '2023-04-26T13:19:23.047Z'
      numEdits: 0
      reactions: []
    id: 644924dbd5d86def91cfff04
    type: comment
  author: CyberTimon
  content: Still slow + every other model is now also just 10 tokens / sec instead
    of 40 tokens / sec so I stay with ooba's fork
  created_at: 2023-04-26 12:19:23+00:00
  edited: false
  hidden: false
  id: 644924dbd5d86def91cfff04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T13:40:54.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah OK I see what you mean now. Vicuna 7B for example is way faster
          and has significantly lower GPU usage %.</p>

          <p><strong>CUDA ooba GPTQ-for-LlaMa - Vicuna 7B no-act-order.pt:</strong><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/r6eVSlHLYajm-hJf4fCyO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/r6eVSlHLYajm-hJf4fCyO.png"></a><br><code>Output
          generated in 33.70 seconds (15.16 tokens/s, 511 tokens, context 44, seed
          1738265307)</code></p>

          <p><strong>CUDA ooba GPTQ-for-LlaMa - WizardLM 7B no-act-order.pt:</strong><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/WWBy_yGrm3XSDNTal-g-z.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/WWBy_yGrm3XSDNTal-g-z.png"></a><br><code>Output
          generated in 113.65 seconds (4.50 tokens/s, 511 tokens, context 44, seed
          2135854730)</code></p>

          <p><strong>Triton QwopQwop GPTQ-for-LlaMa - WizardLM 7B no-act-order.pt:</strong><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Ja0b5GwX1mt-7dJQG22M-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Ja0b5GwX1mt-7dJQG22M-.png"></a><br><code>Output
          generated in 46.00 seconds (11.11 tokens/s, 511 tokens, context 44, seed
          532347497)</code></p>

          <p>VRAM usage is pretty similar. 5243 MiB for Vicuna, 5663 MiB for WizardLM.</p>

          <p>This is really weird. I''m not sure what it could possibly be. I might
          raise an issue with the GPTQ devs and see if they have thoughts.</p>

          <p>(As an aside, I''m amazed you''re getting 40 token/s on a 3060 when I''m
          peaking at 15 token/s on a 4090! I guess the cloud system I''m using must
          be bottlenecked on something else.)</p>

          '
        raw: 'Yeah OK I see what you mean now. Vicuna 7B for example is way faster
          and has significantly lower GPU usage %.


          **CUDA ooba GPTQ-for-LlaMa - Vicuna 7B no-act-order.pt:**

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/r6eVSlHLYajm-hJf4fCyO.png)

          `Output generated in 33.70 seconds (15.16 tokens/s, 511 tokens, context
          44, seed 1738265307)`


          **CUDA ooba GPTQ-for-LlaMa - WizardLM 7B no-act-order.pt:**

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/WWBy_yGrm3XSDNTal-g-z.png)

          `Output generated in 113.65 seconds (4.50 tokens/s, 511 tokens, context
          44, seed 2135854730)`


          **Triton QwopQwop GPTQ-for-LlaMa - WizardLM 7B no-act-order.pt:**

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Ja0b5GwX1mt-7dJQG22M-.png)

          `Output generated in 46.00 seconds (11.11 tokens/s, 511 tokens, context
          44, seed 532347497)`


          VRAM usage is pretty similar. 5243 MiB for Vicuna, 5663 MiB for WizardLM.


          This is really weird. I''m not sure what it could possibly be. I might raise
          an issue with the GPTQ devs and see if they have thoughts.


          (As an aside, I''m amazed you''re getting 40 token/s on a 3060 when I''m
          peaking at 15 token/s on a 4090! I guess the cloud system I''m using must
          be bottlenecked on something else.)'
        updatedAt: '2023-04-26T13:41:24.251Z'
      numEdits: 2
      reactions: []
    id: 644929e6d16a70c015948cf4
    type: comment
  author: TheBloke
  content: 'Yeah OK I see what you mean now. Vicuna 7B for example is way faster and
    has significantly lower GPU usage %.


    **CUDA ooba GPTQ-for-LlaMa - Vicuna 7B no-act-order.pt:**

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/r6eVSlHLYajm-hJf4fCyO.png)

    `Output generated in 33.70 seconds (15.16 tokens/s, 511 tokens, context 44, seed
    1738265307)`


    **CUDA ooba GPTQ-for-LlaMa - WizardLM 7B no-act-order.pt:**

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/WWBy_yGrm3XSDNTal-g-z.png)

    `Output generated in 113.65 seconds (4.50 tokens/s, 511 tokens, context 44, seed
    2135854730)`


    **Triton QwopQwop GPTQ-for-LlaMa - WizardLM 7B no-act-order.pt:**

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/Ja0b5GwX1mt-7dJQG22M-.png)

    `Output generated in 46.00 seconds (11.11 tokens/s, 511 tokens, context 44, seed
    532347497)`


    VRAM usage is pretty similar. 5243 MiB for Vicuna, 5663 MiB for WizardLM.


    This is really weird. I''m not sure what it could possibly be. I might raise an
    issue with the GPTQ devs and see if they have thoughts.


    (As an aside, I''m amazed you''re getting 40 token/s on a 3060 when I''m peaking
    at 15 token/s on a 4090! I guess the cloud system I''m using must be bottlenecked
    on something else.)'
  created_at: 2023-04-26 12:40:54+00:00
  edited: true
  hidden: false
  id: 644929e6d16a70c015948cf4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T13:46:57.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Good to know that you also have performance issues. Yes I think
          it would be nice if the gpt q devs would be informed.</p>

          <blockquote>

          <p>(As an aside, I''m amazed you''re getting 40 token/s on a 3060 when I''m
          peaking at 15 token/s on a 4090! I guess the cloud system I''m using must
          be bottlenecked on something else.)</p>

          </blockquote>

          <p>Yes it''s very nice but also a bit strange. dolly-3b (Yes just the 3b
          model) in full fp16  precision gives only 14 tokens / sec while llama 7b
          4bit gives 41 tokens / sec and 13b 4bit gives 25 tokens / sec.<br>Have you
          tried to use oobabooga''s gpt q fork as only in this fork I get 41 tokens
          / sec?<br>I don''t have a display connected to the gpu btw.</p>

          '
        raw: "Good to know that you also have performance issues. Yes I think it would\
          \ be nice if the gpt q devs would be informed.\n> (As an aside, I'm amazed\
          \ you're getting 40 token/s on a 3060 when I'm peaking at 15 token/s on\
          \ a 4090! I guess the cloud system I'm using must be bottlenecked on something\
          \ else.)\n>\nYes it's very nice but also a bit strange. dolly-3b (Yes just\
          \ the 3b model) in full fp16  precision gives only 14 tokens / sec while\
          \ llama 7b 4bit gives 41 tokens / sec and 13b 4bit gives 25 tokens / sec.\
          \ \nHave you tried to use oobabooga's gpt q fork as only in this fork I\
          \ get 41 tokens / sec?\nI don't have a display connected to the gpu btw."
        updatedAt: '2023-04-26T13:46:57.209Z'
      numEdits: 0
      reactions: []
    id: 64492b51d16a70c01594bae3
    type: comment
  author: CyberTimon
  content: "Good to know that you also have performance issues. Yes I think it would\
    \ be nice if the gpt q devs would be informed.\n> (As an aside, I'm amazed you're\
    \ getting 40 token/s on a 3060 when I'm peaking at 15 token/s on a 4090! I guess\
    \ the cloud system I'm using must be bottlenecked on something else.)\n>\nYes\
    \ it's very nice but also a bit strange. dolly-3b (Yes just the 3b model) in full\
    \ fp16  precision gives only 14 tokens / sec while llama 7b 4bit gives 41 tokens\
    \ / sec and 13b 4bit gives 25 tokens / sec. \nHave you tried to use oobabooga's\
    \ gpt q fork as only in this fork I get 41 tokens / sec?\nI don't have a display\
    \ connected to the gpu btw."
  created_at: 2023-04-26 12:46:57+00:00
  edited: false
  hidden: false
  id: 64492b51d16a70c01594bae3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T13:50:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Have you tried to use oobabooga''s gpt q fork as only in this fork I
          get 41 tokens / sec?<br>I don''t have a display connected to the gpu btw.</p>

          </blockquote>

          <p>Yeah those figures I described above as ''CUDA ooba GPTQ-for-LLaMa''
          are using ooba''s fork, on a 4090 24GB. And this is a cloud system so it
          wouldn''t have a display connected either.</p>

          <p>It is possible it''s CPU bottlenecked or something like that. It''s a
          cloud pod so the host is shared amongst multiple users. The GPU is dedicated
          to me but I think it''s possible that CPU and RAM performance can be impacted
          by other users. </p>

          <p>I will do some testing on another system and see what I find there.</p>

          '
        raw: "> Have you tried to use oobabooga's gpt q fork as only in this fork\
          \ I get 41 tokens / sec?\n> I don't have a display connected to the gpu\
          \ btw.\n\nYeah those figures I described above as 'CUDA ooba GPTQ-for-LLaMa'\
          \ are using ooba's fork, on a 4090 24GB. And this is a cloud system so it\
          \ wouldn't have a display connected either.\n\nIt is possible it's CPU bottlenecked\
          \ or something like that. It's a cloud pod so the host is shared amongst\
          \ multiple users. The GPU is dedicated to me but I think it's possible that\
          \ CPU and RAM performance can be impacted by other users. \n\nI will do\
          \ some testing on another system and see what I find there."
        updatedAt: '2023-04-26T13:50:47.337Z'
      numEdits: 0
      reactions: []
    id: 64492c37d5d86def91d0fec3
    type: comment
  author: TheBloke
  content: "> Have you tried to use oobabooga's gpt q fork as only in this fork I\
    \ get 41 tokens / sec?\n> I don't have a display connected to the gpu btw.\n\n\
    Yeah those figures I described above as 'CUDA ooba GPTQ-for-LLaMa' are using ooba's\
    \ fork, on a 4090 24GB. And this is a cloud system so it wouldn't have a display\
    \ connected either.\n\nIt is possible it's CPU bottlenecked or something like\
    \ that. It's a cloud pod so the host is shared amongst multiple users. The GPU\
    \ is dedicated to me but I think it's possible that CPU and RAM performance can\
    \ be impacted by other users. \n\nI will do some testing on another system and\
    \ see what I find there."
  created_at: 2023-04-26 12:50:47+00:00
  edited: false
  hidden: false
  id: 64492c37d5d86def91d0fec3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-26T14:02:33.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Okay nice! I have a i5 13600kf btw</p>

          '
        raw: Okay nice! I have a i5 13600kf btw
        updatedAt: '2023-04-26T14:02:33.104Z'
      numEdits: 0
      reactions: []
    id: 64492ef9d16a70c0159543dc
    type: comment
  author: CyberTimon
  content: Okay nice! I have a i5 13600kf btw
  created_at: 2023-04-26 13:02:33+00:00
  edited: false
  hidden: false
  id: 64492ef9d16a70c0159543dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
      fullname: Mikael Bohlin Oja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cyperium
      type: user
    createdAt: '2023-04-27T21:47:14.000Z'
    data:
      edited: false
      editors:
      - Cyperium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
          fullname: Mikael Bohlin Oja
          isHf: false
          isPro: false
          name: Cyperium
          type: user
        html: '<p>Just to add my two cents, I have the same slowness problem on my
          3060, I run Windows.</p>

          '
        raw: Just to add my two cents, I have the same slowness problem on my 3060,
          I run Windows.
        updatedAt: '2023-04-27T21:47:14.391Z'
      numEdits: 0
      reactions: []
    id: 644aed62f9f1b0cd3d91f3c8
    type: comment
  author: Cyperium
  content: Just to add my two cents, I have the same slowness problem on my 3060,
    I run Windows.
  created_at: 2023-04-27 20:47:14+00:00
  edited: false
  hidden: false
  id: 644aed62f9f1b0cd3d91f3c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/361d1120df3e107f2d5623a6a9127a00.svg
      fullname: Matthew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IonizedTexasMan
      type: user
    createdAt: '2023-04-28T04:41:20.000Z'
    data:
      edited: false
      editors:
      - IonizedTexasMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/361d1120df3e107f2d5623a6a9127a00.svg
          fullname: Matthew
          isHf: false
          isPro: false
          name: IonizedTexasMan
          type: user
        html: '<p>I was able to get it to run faster on my system. In the config.json
          file, I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s</p>

          <p>Running a 2080 Ti on windows.</p>

          '
        raw: 'I was able to get it to run faster on my system. In the config.json
          file, I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s


          Running a 2080 Ti on windows.'
        updatedAt: '2023-04-28T04:41:20.886Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - id523a
        - ZombieMonkey
        - Yhyu13
        - nguyenviet3057
      - count: 3
        reaction: "\U0001F91D"
        users:
        - TheEnthusiast
        - Cyperium
        - Yhyu13
    id: 644b4e70af97dfd24c1aeb1b
    type: comment
  author: IonizedTexasMan
  content: 'I was able to get it to run faster on my system. In the config.json file,
    I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s


    Running a 2080 Ti on windows.'
  created_at: 2023-04-28 03:41:20+00:00
  edited: false
  hidden: false
  id: 644b4e70af97dfd24c1aeb1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3203f5863b070e894775bc2717adee6a.svg
      fullname: Axodus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheEnthusiast
      type: user
    createdAt: '2023-04-28T05:49:35.000Z'
    data:
      edited: true
      editors:
      - TheEnthusiast
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3203f5863b070e894775bc2717adee6a.svg
          fullname: Axodus
          isHf: false
          isPro: false
          name: TheEnthusiast
          type: user
        html: '<blockquote>

          <p>I was able to get it to run faster on my system. In the config.json file,
          I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s</p>

          <p>Running a 2080 Ti on windows.</p>

          </blockquote>

          <p>Setting "use_cache" to true fixed it for me, nothing else worked.</p>

          <p>I''m running a 1080 Ti on windows.</p>

          '
        raw: "> I was able to get it to run faster on my system. In the config.json\
          \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
          > \n> Running a 2080 Ti on windows.\n\nSetting \"use_cache\" to true fixed\
          \ it for me, nothing else worked.\n\nI'm running a 1080 Ti on windows."
        updatedAt: '2023-04-28T05:49:52.407Z'
      numEdits: 1
      reactions: []
    id: 644b5e6fcb45734dfd51fcf8
    type: comment
  author: TheEnthusiast
  content: "> I was able to get it to run faster on my system. In the config.json\
    \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
    > \n> Running a 2080 Ti on windows.\n\nSetting \"use_cache\" to true fixed it\
    \ for me, nothing else worked.\n\nI'm running a 1080 Ti on windows."
  created_at: 2023-04-28 04:49:35+00:00
  edited: true
  hidden: false
  id: 644b5e6fcb45734dfd51fcf8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
      fullname: Mikael Bohlin Oja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cyperium
      type: user
    createdAt: '2023-04-28T06:27:07.000Z'
    data:
      edited: false
      editors:
      - Cyperium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
          fullname: Mikael Bohlin Oja
          isHf: false
          isPro: false
          name: Cyperium
          type: user
        html: '<blockquote>

          <p>I was able to get it to run faster on my system. In the config.json file,
          I set "use_cache" to true. Went from 1.8 tokens/s to 11.5 tokens/s</p>

          <p>Running a 2080 Ti on windows.</p>

          </blockquote>

          <p>Worked for me too! Thanks!</p>

          '
        raw: "> I was able to get it to run faster on my system. In the config.json\
          \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
          > \n> Running a 2080 Ti on windows.\n\nWorked for me too! Thanks!"
        updatedAt: '2023-04-28T06:27:07.834Z'
      numEdits: 0
      reactions: []
    id: 644b673bcc032814c010cef9
    type: comment
  author: Cyperium
  content: "> I was able to get it to run faster on my system. In the config.json\
    \ file, I set \"use_cache\" to true. Went from 1.8 tokens/s to 11.5 tokens/s\n\
    > \n> Running a 2080 Ti on windows.\n\nWorked for me too! Thanks!"
  created_at: 2023-04-28 05:27:07+00:00
  edited: false
  hidden: false
  id: 644b673bcc032814c010cef9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T07:15:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>This was the solution! Thanks so much to <span data-props=\"{&quot;user&quot;:&quot;IonizedTexasMan&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/IonizedTexasMan\"\
          >@<span class=\"underline\">IonizedTexasMan</span></a></span>\n\n\t</span></span>\
          \ for figuring this out</p>\n<p>I've updated <code>config.json</code> in\
          \ the repo and am making a note in the README for anyone who previously\
          \ downloaded it.</p>\n"
        raw: 'This was the solution! Thanks so much to @IonizedTexasMan for figuring
          this out


          I''ve updated `config.json` in the repo and am making a note in the README
          for anyone who previously downloaded it.'
        updatedAt: '2023-04-28T07:15:08.952Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - TheEnthusiast
        - Cyperium
        - ZombieMonkey
        - Yhyu13
    id: 644b727ccc032814c011f511
    type: comment
  author: TheBloke
  content: 'This was the solution! Thanks so much to @IonizedTexasMan for figuring
    this out


    I''ve updated `config.json` in the repo and am making a note in the README for
    anyone who previously downloaded it.'
  created_at: 2023-04-28 06:15:08+00:00
  edited: false
  hidden: false
  id: 644b727ccc032814c011f511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed75d4866cf1226d05a8f8fd17a90ddd.svg
      fullname: Bazsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BazsiBazsi
      type: user
    createdAt: '2023-04-28T08:06:24.000Z'
    data:
      edited: false
      editors:
      - BazsiBazsi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed75d4866cf1226d05a8f8fd17a90ddd.svg
          fullname: Bazsi
          isHf: false
          isPro: false
          name: BazsiBazsi
          type: user
        html: '<blockquote>

          <p>With llama/vicuna 7b 4bit I get incredible fast 41 tokens/s on a rtx
          3060 12gb.<br><code>Output generated in 8.27 seconds (41.23 tokens/s, 341
          tokens, context 10, seed 928579911)</code></p>

          </blockquote>

          <p>This is incredibly fast, I never achieved anything above 15 it/s on a
          3080ti. I tested with vicuna 7b also. Would you mind detailing your setup?
          Did you get Wizard 7b running at this speed?</p>

          '
        raw: '> With llama/vicuna 7b 4bit I get incredible fast 41 tokens/s on a rtx
          3060 12gb.

          > `Output generated in 8.27 seconds (41.23 tokens/s, 341 tokens, context
          10, seed 928579911)`


          This is incredibly fast, I never achieved anything above 15 it/s on a 3080ti.
          I tested with vicuna 7b also. Would you mind detailing your setup? Did you
          get Wizard 7b running at this speed?'
        updatedAt: '2023-04-28T08:06:24.945Z'
      numEdits: 0
      reactions: []
    id: 644b7e80d873cbc8cc28bcb1
    type: comment
  author: BazsiBazsi
  content: '> With llama/vicuna 7b 4bit I get incredible fast 41 tokens/s on a rtx
    3060 12gb.

    > `Output generated in 8.27 seconds (41.23 tokens/s, 341 tokens, context 10, seed
    928579911)`


    This is incredibly fast, I never achieved anything above 15 it/s on a 3080ti.
    I tested with vicuna 7b also. Would you mind detailing your setup? Did you get
    Wizard 7b running at this speed?'
  created_at: 2023-04-28 07:06:24+00:00
  edited: false
  hidden: false
  id: 644b7e80d873cbc8cc28bcb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-28T14:40:18.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: "<p>Thank you! Back on my: <code>Output generated in 2.24 seconds (41.48\
          \ tokens/s, 93 tokens, context 3, seed 1232882170)</code></p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;BazsiBazsi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/BazsiBazsi\">@<span class=\"\
          underline\">BazsiBazsi</span></a></span>\n\n\t</span></span> , I run it\
          \ on linux ubuntu 22.04. I use --xformers and run it with the original oobabooga\
          \ gpt q branch. The latest one is slow for me.</p>\n"
        raw: 'Thank you! Back on my: `Output generated in 2.24 seconds (41.48 tokens/s,
          93 tokens, context 3, seed 1232882170)`


          @BazsiBazsi , I run it on linux ubuntu 22.04. I use --xformers and run it
          with the original oobabooga gpt q branch. The latest one is slow for me.'
        updatedAt: '2023-04-28T14:40:18.211Z'
      numEdits: 0
      reactions: []
    id: 644bdad20ce4f8fb5164afe5
    type: comment
  author: CyberTimon
  content: 'Thank you! Back on my: `Output generated in 2.24 seconds (41.48 tokens/s,
    93 tokens, context 3, seed 1232882170)`


    @BazsiBazsi , I run it on linux ubuntu 22.04. I use --xformers and run it with
    the original oobabooga gpt q branch. The latest one is slow for me.'
  created_at: 2023-04-28 13:40:18+00:00
  edited: false
  hidden: false
  id: 644bdad20ce4f8fb5164afe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47a57327d60ececbbf5d99f96f183c4f.svg
      fullname: Shubham Samant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shubhamsamant
      type: user
    createdAt: '2023-04-29T22:08:13.000Z'
    data:
      edited: true
      editors:
      - shubhamsamant
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47a57327d60ececbbf5d99f96f183c4f.svg
          fullname: Shubham Samant
          isHf: false
          isPro: false
          name: shubhamsamant
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\"\
          >@<span class=\"underline\">CyberTimon</span></a></span>\n\n\t</span></span><br>I\
          \ have tried alot of permutations &amp; combinations, with multiple 4bit-7B\
          \ models, configs.json params, switch back and forth between the latest\
          \ and ooba's fork of GPTQ, but i cant get more than  12-15 tokens/sec. I\
          \ even scrapped everything and did it all from scratch. No luck, max 12-15\
          \ tokens/sec. If you are getting 3-4X better performance, may i request\
          \ you to write a step-by-step guide with exact version and commands to replicate\
          \ this performance?</p>\n<p>I'm on ubuntu 22.04, with intel i7-11370H, 16GB\
          \ RAM &amp; RTX-3060 6GB laptop<br>Thanks!</p>\n"
        raw: "Hi @CyberTimon \nI have tried alot of permutations & combinations, with\
          \ multiple 4bit-7B models, configs.json params, switch back and forth between\
          \ the latest and ooba's fork of GPTQ, but i cant get more than  12-15 tokens/sec.\
          \ I even scrapped everything and did it all from scratch. No luck, max 12-15\
          \ tokens/sec. If you are getting 3-4X better performance, may i request\
          \ you to write a step-by-step guide with exact version and commands to replicate\
          \ this performance?\n\nI'm on ubuntu 22.04, with intel i7-11370H, 16GB RAM\
          \ & RTX-3060 6GB laptop\nThanks!"
        updatedAt: '2023-04-29T22:10:25.294Z'
      numEdits: 1
      reactions: []
    id: 644d954d97a3b0904a5a39db
    type: comment
  author: shubhamsamant
  content: "Hi @CyberTimon \nI have tried alot of permutations & combinations, with\
    \ multiple 4bit-7B models, configs.json params, switch back and forth between\
    \ the latest and ooba's fork of GPTQ, but i cant get more than  12-15 tokens/sec.\
    \ I even scrapped everything and did it all from scratch. No luck, max 12-15 tokens/sec.\
    \ If you are getting 3-4X better performance, may i request you to write a step-by-step\
    \ guide with exact version and commands to replicate this performance?\n\nI'm\
    \ on ubuntu 22.04, with intel i7-11370H, 16GB RAM & RTX-3060 6GB laptop\nThanks!"
  created_at: 2023-04-29 21:08:13+00:00
  edited: true
  hidden: false
  id: 644d954d97a3b0904a5a39db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T22:09:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I must say I am still extremely confused as to how CyberTimon
          gets 40 token/s on a 3060 when I get max 16 token/s on a 4090, and others
          I''ve spoken to are in the same ballpark</p>

          <p>What have you done to unlock this power!? :)</p>

          '
        raw: 'Yeah I must say I am still extremely confused as to how CyberTimon gets
          40 token/s on a 3060 when I get max 16 token/s on a 4090, and others I''ve
          spoken to are in the same ballpark


          What have you done to unlock this power!? :)'
        updatedAt: '2023-04-29T22:09:55.844Z'
      numEdits: 0
      reactions: []
    id: 644d95b30dc952d245a82c6e
    type: comment
  author: TheBloke
  content: 'Yeah I must say I am still extremely confused as to how CyberTimon gets
    40 token/s on a 3060 when I get max 16 token/s on a 4090, and others I''ve spoken
    to are in the same ballpark


    What have you done to unlock this power!? :)'
  created_at: 2023-04-29 21:09:55+00:00
  edited: false
  hidden: false
  id: 644d95b30dc952d245a82c6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-30T08:28:00.000Z'
    data:
      edited: true
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>My gpu has superpowers!<br>No for real - no idea. Here is how I
          set up everything:</p>

          <ol>

          <li>Built my pc (used as a headless server) with 2x rtx 3060 12gb (1 running
          stable diffusion, the other one oobabooga)</li>

          <li>Installed a clean ubuntu 22.04.</li>

          <li>Installed the latest linux nvidia drivers <em>perhaps this is the fix?</em></li>

          <li>Download Jupyter Lab as this is how I controll the server</li>

          <li>Install the latest oobabooga and quant cuda. (To get gpt q working)</li>

          <li>Download any llama based 7b or 13b model. (Without act-order but with
          groupsize 128)</li>

          <li>Open text generation webui from my laptop which i started with --xformers
          and --gpu-memory 12</li>

          <li>Profit (40 tokens / sec with 7b and 25 tokens / sec with 13b model)<br>Can
          you name your setup ? Are you running wsl or native?</li>

          </ol>

          <p>In addition I have a i5 13600kf and 64gb ddr4 ram. The model is on a
          1tb kingston ssd</p>

          '
        raw: 'My gpu has superpowers!

          No for real - no idea. Here is how I set up everything:

          1. Built my pc (used as a headless server) with 2x rtx 3060 12gb (1 running
          stable diffusion, the other one oobabooga)

          2. Installed a clean ubuntu 22.04.

          3. Installed the latest linux nvidia drivers *perhaps this is the fix?*

          4. Download Jupyter Lab as this is how I controll the server

          5. Install the latest oobabooga and quant cuda. (To get gpt q working)

          6. Download any llama based 7b or 13b model. (Without act-order but with
          groupsize 128)

          7. Open text generation webui from my laptop which i started with --xformers
          and --gpu-memory 12

          8. Profit (40 tokens / sec with 7b and 25 tokens / sec with 13b model)

          Can you name your setup ? Are you running wsl or native?


          In addition I have a i5 13600kf and 64gb ddr4 ram. The model is on a 1tb
          kingston ssd'
        updatedAt: '2023-04-30T08:30:29.280Z'
      numEdits: 1
      reactions: []
    id: 644e2690cf72e60a5b730656
    type: comment
  author: CyberTimon
  content: 'My gpu has superpowers!

    No for real - no idea. Here is how I set up everything:

    1. Built my pc (used as a headless server) with 2x rtx 3060 12gb (1 running stable
    diffusion, the other one oobabooga)

    2. Installed a clean ubuntu 22.04.

    3. Installed the latest linux nvidia drivers *perhaps this is the fix?*

    4. Download Jupyter Lab as this is how I controll the server

    5. Install the latest oobabooga and quant cuda. (To get gpt q working)

    6. Download any llama based 7b or 13b model. (Without act-order but with groupsize
    128)

    7. Open text generation webui from my laptop which i started with --xformers and
    --gpu-memory 12

    8. Profit (40 tokens / sec with 7b and 25 tokens / sec with 13b model)

    Can you name your setup ? Are you running wsl or native?


    In addition I have a i5 13600kf and 64gb ddr4 ram. The model is on a 1tb kingston
    ssd'
  created_at: 2023-04-30 07:28:00+00:00
  edited: true
  hidden: false
  id: 644e2690cf72e60a5b730656
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-04-30T08:29:01.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>When I''m back home I can also make a video and share my cuda /
          driver versions.</p>

          '
        raw: When I'm back home I can also make a video and share my cuda / driver
          versions.
        updatedAt: '2023-04-30T08:29:01.923Z'
      numEdits: 0
      reactions: []
    id: 644e26cdcf72e60a5b730ad6
    type: comment
  author: CyberTimon
  content: When I'm back home I can also make a video and share my cuda / driver versions.
  created_at: 2023-04-30 07:29:01+00:00
  edited: false
  hidden: false
  id: 644e26cdcf72e60a5b730ad6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed75d4866cf1226d05a8f8fd17a90ddd.svg
      fullname: Bazsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BazsiBazsi
      type: user
    createdAt: '2023-06-03T21:22:06.000Z'
    data:
      edited: false
      editors:
      - BazsiBazsi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8264317512512207
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed75d4866cf1226d05a8f8fd17a90ddd.svg
          fullname: Bazsi
          isHf: false
          isPro: false
          name: BazsiBazsi
          type: user
        html: '<blockquote>

          <p>My gpu has superpowers!<br>No for real - no idea. Here is how I set up
          everything:</p>

          <ol>

          <li>Built my pc (used as a headless server) with 2x rtx 3060 12gb (1 running
          stable diffusion, the other one oobabooga)</li>

          <li>Installed a clean ubuntu 22.04.</li>

          <li>Installed the latest linux nvidia drivers <em>perhaps this is the fix?</em></li>

          <li>Download Jupyter Lab as this is how I controll the server</li>

          <li>Install the latest oobabooga and quant cuda. (To get gpt q working)</li>

          <li>Download any llama based 7b or 13b model. (Without act-order but with
          groupsize 128)</li>

          <li>Open text generation webui from my laptop which i started with --xformers
          and --gpu-memory 12</li>

          <li>Profit (40 tokens / sec with 7b and 25 tokens / sec with 13b model)<br>Can
          you name your setup ? Are you running wsl or native?</li>

          </ol>

          <p>In addition I have a i5 13600kf and 64gb ddr4 ram. The model is on a
          1tb kingston ssd</p>

          </blockquote>

          <p>Hey it''s a bit late but i figured it out. You gotta launch ooba with
          --autogptq. I''m getting above 20 tokens / s with a 3080ti. Ran on linux.</p>

          '
        raw: "> My gpu has superpowers!\n> No for real - no idea. Here is how I set\
          \ up everything:\n> 1. Built my pc (used as a headless server) with 2x rtx\
          \ 3060 12gb (1 running stable diffusion, the other one oobabooga)\n> 2.\
          \ Installed a clean ubuntu 22.04.\n> 3. Installed the latest linux nvidia\
          \ drivers *perhaps this is the fix?*\n> 4. Download Jupyter Lab as this\
          \ is how I controll the server\n> 5. Install the latest oobabooga and quant\
          \ cuda. (To get gpt q working)\n> 6. Download any llama based 7b or 13b\
          \ model. (Without act-order but with groupsize 128)\n> 7. Open text generation\
          \ webui from my laptop which i started with --xformers and --gpu-memory\
          \ 12\n> 8. Profit (40 tokens / sec with 7b and 25 tokens / sec with 13b\
          \ model)\n> Can you name your setup ? Are you running wsl or native?\n>\
          \ \n> In addition I have a i5 13600kf and 64gb ddr4 ram. The model is on\
          \ a 1tb kingston ssd\n\nHey it's a bit late but i figured it out. You gotta\
          \ launch ooba with --autogptq. I'm getting above 20 tokens / s with a 3080ti.\
          \ Ran on linux."
        updatedAt: '2023-06-03T21:22:06.683Z'
      numEdits: 0
      reactions: []
    id: 647baefee8b7333058b018b8
    type: comment
  author: BazsiBazsi
  content: "> My gpu has superpowers!\n> No for real - no idea. Here is how I set\
    \ up everything:\n> 1. Built my pc (used as a headless server) with 2x rtx 3060\
    \ 12gb (1 running stable diffusion, the other one oobabooga)\n> 2. Installed a\
    \ clean ubuntu 22.04.\n> 3. Installed the latest linux nvidia drivers *perhaps\
    \ this is the fix?*\n> 4. Download Jupyter Lab as this is how I controll the server\n\
    > 5. Install the latest oobabooga and quant cuda. (To get gpt q working)\n> 6.\
    \ Download any llama based 7b or 13b model. (Without act-order but with groupsize\
    \ 128)\n> 7. Open text generation webui from my laptop which i started with --xformers\
    \ and --gpu-memory 12\n> 8. Profit (40 tokens / sec with 7b and 25 tokens / sec\
    \ with 13b model)\n> Can you name your setup ? Are you running wsl or native?\n\
    > \n> In addition I have a i5 13600kf and 64gb ddr4 ram. The model is on a 1tb\
    \ kingston ssd\n\nHey it's a bit late but i figured it out. You gotta launch ooba\
    \ with --autogptq. I'm getting above 20 tokens / s with a 3080ti. Ran on linux."
  created_at: 2023-06-03 20:22:06+00:00
  edited: false
  hidden: false
  id: 647baefee8b7333058b018b8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/wizardLM-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Very slow speed with oobabooga
