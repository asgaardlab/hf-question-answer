!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MortySmith
conflicting_files: null
created_at: 2023-04-27 17:15:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662733916708-noauth.jpeg?w=200&h=200&f=face
      fullname: Leon Bieber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MortySmith
      type: user
    createdAt: '2023-04-27T18:15:59.000Z'
    data:
      edited: false
      editors:
      - MortySmith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662733916708-noauth.jpeg?w=200&h=200&f=face
          fullname: Leon Bieber
          isHf: false
          isPro: false
          name: MortySmith
          type: user
        html: '<p>I have installed this model with the one click install to my oogabooga
          webui, but can''t get it to run. When I use the default start-windows.bat,
          it throws this error: OSError: Error no file named pytorch_model.bin, tf_model.h5,
          model.ckpt.index or flax_model.msgpack found in directory models\TheBloke_wizardLM-7B-GPTQ.
          Now, I have read in your repo, that I should use this command line command
          to launch it (cd text-generation-webui<br>python server.py --model wizardLM-7B-GPTQ
          --wbits 4 --groupsize 128 --model_type Llama # add any other command line
          args you want), but I am not really shure, where to run it, so I tried to
          run it from the text generation webui folder. It kind of worked, but it
          threw this error:   File "C:\AI\oobabooga_windows\oobabooga_windows\text-generation-webui\server.py",
          line 17, in <br>    import gradio as gr<br>ModuleNotFoundError: No module
          named ''gradio''. This is kind of driving me nuts, does anyone know a fix?</p>

          '
        raw: "I have installed this model with the one click install to my oogabooga\
          \ webui, but can't get it to run. When I use the default start-windows.bat,\
          \ it throws this error: OSError: Error no file named pytorch_model.bin,\
          \ tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory\
          \ models\\TheBloke_wizardLM-7B-GPTQ. Now, I have read in your repo, that\
          \ I should use this command line command to launch it (cd text-generation-webui\r\
          \npython server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type\
          \ Llama # add any other command line args you want), but I am not really\
          \ shure, where to run it, so I tried to run it from the text generation\
          \ webui folder. It kind of worked, but it threw this error:   File \"C:\\\
          AI\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\server.py\"\
          , line 17, in <module>\r\n    import gradio as gr\r\nModuleNotFoundError:\
          \ No module named 'gradio'. This is kind of driving me nuts, does anyone\
          \ know a fix?"
        updatedAt: '2023-04-27T18:15:59.122Z'
      numEdits: 0
      reactions: []
    id: 644abbdff434a6a63b187511
    type: comment
  author: MortySmith
  content: "I have installed this model with the one click install to my oogabooga\
    \ webui, but can't get it to run. When I use the default start-windows.bat, it\
    \ throws this error: OSError: Error no file named pytorch_model.bin, tf_model.h5,\
    \ model.ckpt.index or flax_model.msgpack found in directory models\\TheBloke_wizardLM-7B-GPTQ.\
    \ Now, I have read in your repo, that I should use this command line command to\
    \ launch it (cd text-generation-webui\r\npython server.py --model wizardLM-7B-GPTQ\
    \ --wbits 4 --groupsize 128 --model_type Llama # add any other command line args\
    \ you want), but I am not really shure, where to run it, so I tried to run it\
    \ from the text generation webui folder. It kind of worked, but it threw this\
    \ error:   File \"C:\\AI\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    server.py\", line 17, in <module>\r\n    import gradio as gr\r\nModuleNotFoundError:\
    \ No module named 'gradio'. This is kind of driving me nuts, does anyone know\
    \ a fix?"
  created_at: 2023-04-27 17:15:59+00:00
  edited: false
  hidden: false
  id: 644abbdff434a6a63b187511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6ef464d22e0167e7f3398e155a26b51.svg
      fullname: M H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bonsaihorn
      type: user
    createdAt: '2023-04-27T18:56:19.000Z'
    data:
      edited: false
      editors:
      - bonsaihorn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6ef464d22e0167e7f3398e155a26b51.svg
          fullname: M H
          isHf: false
          isPro: false
          name: bonsaihorn
          type: user
        html: '<p>You can convert the safetensors model, there is a ckpt2safetensors
          program that claims you can go back to ckpt but can''t confirm that.</p>

          <p><a rel="nofollow" href="https://www.reddit.com/r/StableDiffusion/comments/zhozkz/safe_stable_ckpt2safetensors_conversion_toolgui/">https://www.reddit.com/r/StableDiffusion/comments/zhozkz/safe_stable_ckpt2safetensors_conversion_toolgui/</a></p>

          '
        raw: 'You can convert the safetensors model, there is a ckpt2safetensors program
          that claims you can go back to ckpt but can''t confirm that.


          https://www.reddit.com/r/StableDiffusion/comments/zhozkz/safe_stable_ckpt2safetensors_conversion_toolgui/'
        updatedAt: '2023-04-27T18:56:19.262Z'
      numEdits: 0
      reactions: []
    id: 644ac553cb45734dfd467044
    type: comment
  author: bonsaihorn
  content: 'You can convert the safetensors model, there is a ckpt2safetensors program
    that claims you can go back to ckpt but can''t confirm that.


    https://www.reddit.com/r/StableDiffusion/comments/zhozkz/safe_stable_ckpt2safetensors_conversion_toolgui/'
  created_at: 2023-04-27 17:56:19+00:00
  edited: false
  hidden: false
  id: 644ac553cb45734dfd467044
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-27T20:26:39.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I have installed this model with the one click install to my oogabooga
          webui, but can''t get it to run. When I use the default start-windows.bat,
          it throws this error: OSError: Error no file named pytorch_model.bin, tf_model.h5,
          model.ckpt.index or flax_model.msgpack found in directory models\TheBloke_wizardLM-7B-GPTQ.
          Now, I have read in your repo, that I should use this command line command
          to launch it (cd text-generation-webui<br>python server.py --model wizardLM-7B-GPTQ
          --wbits 4 --groupsize 128 --model_type Llama # add any other command line
          args you want), but I am not really shure, where to run it, so I tried to
          run it from the text generation webui folder. It kind of worked, but it
          threw this error:   File "C:\AI\oobabooga_windows\oobabooga_windows\text-generation-webui\server.py",
          line 17, in <br>    import gradio as gr<br>ModuleNotFoundError: No module
          named ''gradio''. This is kind of driving me nuts, does anyone know a fix?</p>

          </blockquote>

          <p>It should definitely be possible to load during the one-click-installer,
          as many other people are using this.  But yes you must specify the command
          line arguments <code>--wbits 4 --groupsize 128 --model_type llama</code></p>

          <p>I''m afraid I don''t know how you''re meant to specify that for the one-click-install.</p>

          <p>The error you got trying to run it manually suggested that it couldn''t
          find one of the Python dependencies, gradio.  You must have that installed
          if text-generation-webui works in general, so that suggests that you haven''t
          found the right way of executing server.py.</p>

          <p>How do you launch text-generation-webui at the moment? Do you have a
          shortcut or something? If so, try right-clicking on it and viewing its properties,
          see if you can see some command line arguments in there. Show me a screenshot
          if you like and I can tell you if it''s the right place to add <code>--wbits
          4 --groupsize 128 --model_type llama</code></p>

          <p>I don''t use Windows and I can''t really test it so other than that I''m
          not sure what to suggest. But it''s definitely possible to get it working.</p>

          <blockquote>

          <p>You can convert the safetensors model, there is a ckpt2safetensors program
          that claims you can go back to ckpt but can''t confirm that.</p>

          </blockquote>

          <p>It''s not related to it being a safetensors file.  That will load fine
          if the correct command line arguments are given to text-generation-webui</p>

          '
        raw: '> I have installed this model with the one click install to my oogabooga
          webui, but can''t get it to run. When I use the default start-windows.bat,
          it throws this error: OSError: Error no file named pytorch_model.bin, tf_model.h5,
          model.ckpt.index or flax_model.msgpack found in directory models\TheBloke_wizardLM-7B-GPTQ.
          Now, I have read in your repo, that I should use this command line command
          to launch it (cd text-generation-webui

          > python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type
          Llama # add any other command line args you want), but I am not really shure,
          where to run it, so I tried to run it from the text generation webui folder.
          It kind of worked, but it threw this error:   File "C:\AI\oobabooga_windows\oobabooga_windows\text-generation-webui\server.py",
          line 17, in <module>

          >     import gradio as gr

          > ModuleNotFoundError: No module named ''gradio''. This is kind of driving
          me nuts, does anyone know a fix?


          It should definitely be possible to load during the one-click-installer,
          as many other people are using this.  But yes you must specify the command
          line arguments `--wbits 4 --groupsize 128 --model_type llama`


          I''m afraid I don''t know how you''re meant to specify that for the one-click-install.


          The error you got trying to run it manually suggested that it couldn''t
          find one of the Python dependencies, gradio.  You must have that installed
          if text-generation-webui works in general, so that suggests that you haven''t
          found the right way of executing server.py.


          How do you launch text-generation-webui at the moment? Do you have a shortcut
          or something? If so, try right-clicking on it and viewing its properties,
          see if you can see some command line arguments in there. Show me a screenshot
          if you like and I can tell you if it''s the right place to add `--wbits
          4 --groupsize 128 --model_type llama`


          I don''t use Windows and I can''t really test it so other than that I''m
          not sure what to suggest. But it''s definitely possible to get it working.


          > You can convert the safetensors model, there is a ckpt2safetensors program
          that claims you can go back to ckpt but can''t confirm that.


          It''s not related to it being a safetensors file.  That will load fine if
          the correct command line arguments are given to text-generation-webui'
        updatedAt: '2023-04-27T20:30:53.546Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - RalphX1
        - MortySmith
    id: 644ada7fd4483bfaa070bbfa
    type: comment
  author: TheBloke
  content: '> I have installed this model with the one click install to my oogabooga
    webui, but can''t get it to run. When I use the default start-windows.bat, it
    throws this error: OSError: Error no file named pytorch_model.bin, tf_model.h5,
    model.ckpt.index or flax_model.msgpack found in directory models\TheBloke_wizardLM-7B-GPTQ.
    Now, I have read in your repo, that I should use this command line command to
    launch it (cd text-generation-webui

    > python server.py --model wizardLM-7B-GPTQ --wbits 4 --groupsize 128 --model_type
    Llama # add any other command line args you want), but I am not really shure,
    where to run it, so I tried to run it from the text generation webui folder. It
    kind of worked, but it threw this error:   File "C:\AI\oobabooga_windows\oobabooga_windows\text-generation-webui\server.py",
    line 17, in <module>

    >     import gradio as gr

    > ModuleNotFoundError: No module named ''gradio''. This is kind of driving me
    nuts, does anyone know a fix?


    It should definitely be possible to load during the one-click-installer, as many
    other people are using this.  But yes you must specify the command line arguments
    `--wbits 4 --groupsize 128 --model_type llama`


    I''m afraid I don''t know how you''re meant to specify that for the one-click-install.


    The error you got trying to run it manually suggested that it couldn''t find one
    of the Python dependencies, gradio.  You must have that installed if text-generation-webui
    works in general, so that suggests that you haven''t found the right way of executing
    server.py.


    How do you launch text-generation-webui at the moment? Do you have a shortcut
    or something? If so, try right-clicking on it and viewing its properties, see
    if you can see some command line arguments in there. Show me a screenshot if you
    like and I can tell you if it''s the right place to add `--wbits 4 --groupsize
    128 --model_type llama`


    I don''t use Windows and I can''t really test it so other than that I''m not sure
    what to suggest. But it''s definitely possible to get it working.


    > You can convert the safetensors model, there is a ckpt2safetensors program that
    claims you can go back to ckpt but can''t confirm that.


    It''s not related to it being a safetensors file.  That will load fine if the
    correct command line arguments are given to text-generation-webui'
  created_at: 2023-04-27 19:26:39+00:00
  edited: true
  hidden: false
  id: 644ada7fd4483bfaa070bbfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313867d263d02341a8b4c40005d0fa7d.svg
      fullname: Sameer Adhikari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anthrax247
      type: user
    createdAt: '2023-04-29T09:01:24.000Z'
    data:
      edited: false
      editors:
      - Anthrax247
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313867d263d02341a8b4c40005d0fa7d.svg
          fullname: Sameer Adhikari
          isHf: false
          isPro: false
          name: Anthrax247
          type: user
        html: '<p>To add "--wbits 4 --groupsize 128 --model_type llama", edit line
          146 of "webui.py" file in oobabooga folder.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/1DSDLNbW_jqxIj1o_Vld5.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/1DSDLNbW_jqxIj1o_Vld5.png"></a></p>

          <p>After using CUDA branch of GPTQ-for-LLaMa,  even the act-order model
          is working perfectly on oobabooga on windows.</p>

          '
        raw: 'To add "--wbits 4 --groupsize 128 --model_type llama", edit line 146
          of "webui.py" file in oobabooga folder.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/1DSDLNbW_jqxIj1o_Vld5.png)


          After using CUDA branch of GPTQ-for-LLaMa,  even the act-order model is
          working perfectly on oobabooga on windows.'
        updatedAt: '2023-04-29T09:01:24.249Z'
      numEdits: 0
      reactions: []
    id: 644cdce497a3b0904a49e558
    type: comment
  author: Anthrax247
  content: 'To add "--wbits 4 --groupsize 128 --model_type llama", edit line 146 of
    "webui.py" file in oobabooga folder.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/1DSDLNbW_jqxIj1o_Vld5.png)


    After using CUDA branch of GPTQ-for-LLaMa,  even the act-order model is working
    perfectly on oobabooga on windows.'
  created_at: 2023-04-29 08:01:24+00:00
  edited: false
  hidden: false
  id: 644cdce497a3b0904a49e558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T09:03:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great, thanks for the details Anthrax</p>

          <p>Just so I''m clear - to get the act-order file working on Windows, you
          downloaded and self-compiled the CUDA branch of GPTQ-for-LLaMa?  It still
          doesn''t work with the pre-compiled ooba fork in the one-click-installers?</p>

          '
        raw: 'Great, thanks for the details Anthrax


          Just so I''m clear - to get the act-order file working on Windows, you downloaded
          and self-compiled the CUDA branch of GPTQ-for-LLaMa?  It still doesn''t
          work with the pre-compiled ooba fork in the one-click-installers?'
        updatedAt: '2023-04-29T09:03:32.827Z'
      numEdits: 0
      reactions: []
    id: 644cdd6497a3b0904a49f18e
    type: comment
  author: TheBloke
  content: 'Great, thanks for the details Anthrax


    Just so I''m clear - to get the act-order file working on Windows, you downloaded
    and self-compiled the CUDA branch of GPTQ-for-LLaMa?  It still doesn''t work with
    the pre-compiled ooba fork in the one-click-installers?'
  created_at: 2023-04-29 08:03:32+00:00
  edited: false
  hidden: false
  id: 644cdd6497a3b0904a49f18e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662733916708-noauth.jpeg?w=200&h=200&f=face
      fullname: Leon Bieber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MortySmith
      type: user
    createdAt: '2023-04-29T09:18:17.000Z'
    data:
      edited: false
      editors:
      - MortySmith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662733916708-noauth.jpeg?w=200&h=200&f=face
          fullname: Leon Bieber
          isHf: false
          isPro: false
          name: MortySmith
          type: user
        html: '<p>Thanks so much for all the helpfull comments. I solved it now by
          just reinstalling with the guide from Aitrepreneur.</p>

          '
        raw: Thanks so much for all the helpfull comments. I solved it now by just
          reinstalling with the guide from Aitrepreneur.
        updatedAt: '2023-04-29T09:18:17.852Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 644ce0d997a3b0904a4a46ef
    type: comment
  author: MortySmith
  content: Thanks so much for all the helpfull comments. I solved it now by just reinstalling
    with the guide from Aitrepreneur.
  created_at: 2023-04-29 08:18:17+00:00
  edited: false
  hidden: false
  id: 644ce0d997a3b0904a4a46ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313867d263d02341a8b4c40005d0fa7d.svg
      fullname: Sameer Adhikari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anthrax247
      type: user
    createdAt: '2023-04-29T09:23:45.000Z'
    data:
      edited: false
      editors:
      - Anthrax247
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313867d263d02341a8b4c40005d0fa7d.svg
          fullname: Sameer Adhikari
          isHf: false
          isPro: false
          name: Anthrax247
          type: user
        html: '<p>I just followed instructions from your vicuna-7B-1.1-GPTQ-4bit-128g
          page, and it works now.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/dZgK7Tq4e0KhpX6FpFTDy.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/dZgK7Tq4e0KhpX6FpFTDy.png"></a></p>

          <p>The one that ooba installed automatically still gives random characters
          for me.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/PKzqTHeeMyrflmEjXnK1q.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/PKzqTHeeMyrflmEjXnK1q.png"></a></p>

          '
        raw: 'I just followed instructions from your vicuna-7B-1.1-GPTQ-4bit-128g
          page, and it works now.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/dZgK7Tq4e0KhpX6FpFTDy.png)


          The one that ooba installed automatically still gives random characters
          for me.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/PKzqTHeeMyrflmEjXnK1q.png)'
        updatedAt: '2023-04-29T09:23:45.745Z'
      numEdits: 0
      reactions: []
    id: 644ce22197a3b0904a4a659e
    type: comment
  author: Anthrax247
  content: 'I just followed instructions from your vicuna-7B-1.1-GPTQ-4bit-128g page,
    and it works now.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/dZgK7Tq4e0KhpX6FpFTDy.png)


    The one that ooba installed automatically still gives random characters for me.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cd7d4ce3065fb76be7cc2/PKzqTHeeMyrflmEjXnK1q.png)'
  created_at: 2023-04-29 08:23:45+00:00
  edited: false
  hidden: false
  id: 644ce22197a3b0904a4a659e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T09:32:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah sorry, until a few minutes ago ooba would automatically install
          the file that requires the latest GPTQ-for-LLaMa code.</p>

          <p>I''ve fixed that now by renaming the files so the no-act-order file will
          always be loaded in preference to the act-order file, And I''ve put these
          instructions for easy installation in the README:</p>

          <h2 id="how-to-easily-download-and-use-this-model-in-text-generation-webui">How
          to easily download and use this model in text-generation-webui</h2>

          <p>Load text-generation-webui as you normally do.</p>

          <ol>

          <li>Click the <strong>Model tab</strong>.</li>

          <li>Under <strong>Download custom model or LoRA</strong>, enter this repo
          name: <code>TheBloke/stable-vicuna-13B-GPTQ</code>.</li>

          <li>Click <strong>Download</strong>.</li>

          <li>Wait until it says it''s finished downloading.</li>

          <li>As this is a GPTQ model, fill in the <code>GPTQ parameters</code> on
          the right: <code>Bits = 4</code>, <code>Groupsize = 128</code>, <code>model_type
          = Llama</code></li>

          <li>Now click the <strong>Refresh</strong> icon next to <strong>Model</strong>
          in the top left.</li>

          <li>In the <strong>Model drop-down</strong>: choose this model: <code>stable-vicuna-13B-GPTQ</code>.</li>

          <li>Click <strong>Reload the Model</strong> in the top right.</li>

          <li>Once it says it''s loaded, click the <strong>Text Generation tab</strong>
          and enter a prompt!</li>

          </ol>

          '
        raw: 'Yeah sorry, until a few minutes ago ooba would automatically install
          the file that requires the latest GPTQ-for-LLaMa code.


          I''ve fixed that now by renaming the files so the no-act-order file will
          always be loaded in preference to the act-order file, And I''ve put these
          instructions for easy installation in the README:


          ## How to easily download and use this model in text-generation-webui


          Load text-generation-webui as you normally do.


          1. Click the **Model tab**.

          2. Under **Download custom model or LoRA**, enter this repo name: `TheBloke/stable-vicuna-13B-GPTQ`.

          3. Click **Download**.

          4. Wait until it says it''s finished downloading.

          5. As this is a GPTQ model, fill in the `GPTQ parameters` on the right:
          `Bits = 4`, `Groupsize = 128`, `model_type = Llama`

          6. Now click the **Refresh** icon next to **Model** in the top left.

          7. In the **Model drop-down**: choose this model: `stable-vicuna-13B-GPTQ`.

          8. Click **Reload the Model** in the top right.

          9. Once it says it''s loaded, click the **Text Generation tab** and enter
          a prompt!'
        updatedAt: '2023-04-29T09:32:27.801Z'
      numEdits: 0
      reactions: []
    id: 644ce42bfa94e93b0ebcdc36
    type: comment
  author: TheBloke
  content: 'Yeah sorry, until a few minutes ago ooba would automatically install the
    file that requires the latest GPTQ-for-LLaMa code.


    I''ve fixed that now by renaming the files so the no-act-order file will always
    be loaded in preference to the act-order file, And I''ve put these instructions
    for easy installation in the README:


    ## How to easily download and use this model in text-generation-webui


    Load text-generation-webui as you normally do.


    1. Click the **Model tab**.

    2. Under **Download custom model or LoRA**, enter this repo name: `TheBloke/stable-vicuna-13B-GPTQ`.

    3. Click **Download**.

    4. Wait until it says it''s finished downloading.

    5. As this is a GPTQ model, fill in the `GPTQ parameters` on the right: `Bits
    = 4`, `Groupsize = 128`, `model_type = Llama`

    6. Now click the **Refresh** icon next to **Model** in the top left.

    7. In the **Model drop-down**: choose this model: `stable-vicuna-13B-GPTQ`.

    8. Click **Reload the Model** in the top right.

    9. Once it says it''s loaded, click the **Text Generation tab** and enter a prompt!'
  created_at: 2023-04-29 08:32:27+00:00
  edited: false
  hidden: false
  id: 644ce42bfa94e93b0ebcdc36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
      fullname: Mikael Bohlin Oja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cyperium
      type: user
    createdAt: '2023-04-29T22:49:50.000Z'
    data:
      edited: false
      editors:
      - Cyperium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
          fullname: Mikael Bohlin Oja
          isHf: false
          isPro: false
          name: Cyperium
          type: user
        html: '<p>Is it possible to start the WebUI without a model if there is an
          error loading a model? That would be better instead of just ending the program,
          then the user can load a model and set the parameters in the WebUI instead.</p>

          '
        raw: Is it possible to start the WebUI without a model if there is an error
          loading a model? That would be better instead of just ending the program,
          then the user can load a model and set the parameters in the WebUI instead.
        updatedAt: '2023-04-29T22:49:50.106Z'
      numEdits: 0
      reactions: []
    id: 644d9f0e328c1aa30e4efa6a
    type: comment
  author: Cyperium
  content: Is it possible to start the WebUI without a model if there is an error
    loading a model? That would be better instead of just ending the program, then
    the user can load a model and set the parameters in the WebUI instead.
  created_at: 2023-04-29 21:49:50+00:00
  edited: false
  hidden: false
  id: 644d9f0e328c1aa30e4efa6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
      fullname: Hypersniper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hypersniper
      type: user
    createdAt: '2023-04-30T01:53:24.000Z'
    data:
      edited: false
      editors:
      - Hypersniper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
          fullname: Hypersniper
          isHf: false
          isPro: false
          name: Hypersniper
          type: user
        html: '<p>Modify the webui.py , specifically this line :<br>      run_cmd("python
          server.py --chat --model-menu")  # put your flags here!<br>to this:<br>     run_cmd("python
          server.py --chat")  # put your flags here!</p>

          '
        raw: "Modify the webui.py , specifically this line :\n      run_cmd(\"python\
          \ server.py --chat --model-menu\")  # put your flags here!\nto this:\n \
          \    run_cmd(\"python server.py --chat\")  # put your flags here!"
        updatedAt: '2023-04-30T01:53:24.199Z'
      numEdits: 0
      reactions: []
    id: 644dca14328c1aa30e521215
    type: comment
  author: Hypersniper
  content: "Modify the webui.py , specifically this line :\n      run_cmd(\"python\
    \ server.py --chat --model-menu\")  # put your flags here!\nto this:\n     run_cmd(\"\
    python server.py --chat\")  # put your flags here!"
  created_at: 2023-04-30 00:53:24+00:00
  edited: false
  hidden: false
  id: 644dca14328c1aa30e521215
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
      fullname: Hypersniper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hypersniper
      type: user
    createdAt: '2023-04-30T02:05:14.000Z'
    data:
      edited: false
      editors:
      - Hypersniper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
          fullname: Hypersniper
          isHf: false
          isPro: false
          name: Hypersniper
          type: user
        html: '<p>use this one instead:</p>

          <p>run_cmd("python server.py --chat --auto-device")  # put your flags here!</p>

          '
        raw: 'use this one instead:


          run_cmd("python server.py --chat --auto-device")  # put your flags here!'
        updatedAt: '2023-04-30T02:05:14.920Z'
      numEdits: 0
      reactions: []
    id: 644dccda0dc952d245ac3518
    type: comment
  author: Hypersniper
  content: 'use this one instead:


    run_cmd("python server.py --chat --auto-device")  # put your flags here!'
  created_at: 2023-04-30 01:05:14+00:00
  edited: false
  hidden: false
  id: 644dccda0dc952d245ac3518
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
      fullname: Mikael Bohlin Oja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cyperium
      type: user
    createdAt: '2023-04-30T02:08:44.000Z'
    data:
      edited: false
      editors:
      - Cyperium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c205cb3ebd67e9e492195807348f054.svg
          fullname: Mikael Bohlin Oja
          isHf: false
          isPro: false
          name: Cyperium
          type: user
        html: '<p>Ok, thanks. I would hope that they could make a change to the error
          routine so that it doesn''t end the program as failing to load a model isn''t
          a critical error (the WebUI works without any model loaded).</p>

          '
        raw: Ok, thanks. I would hope that they could make a change to the error routine
          so that it doesn't end the program as failing to load a model isn't a critical
          error (the WebUI works without any model loaded).
        updatedAt: '2023-04-30T02:08:44.932Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Hypersniper
    id: 644dcdac0dc952d245ac4497
    type: comment
  author: Cyperium
  content: Ok, thanks. I would hope that they could make a change to the error routine
    so that it doesn't end the program as failing to load a model isn't a critical
    error (the WebUI works without any model loaded).
  created_at: 2023-04-30 01:08:44+00:00
  edited: false
  hidden: false
  id: 644dcdac0dc952d245ac4497
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313867d263d02341a8b4c40005d0fa7d.svg
      fullname: Sameer Adhikari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anthrax247
      type: user
    createdAt: '2023-04-30T13:18:22.000Z'
    data:
      edited: false
      editors:
      - Anthrax247
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313867d263d02341a8b4c40005d0fa7d.svg
          fullname: Sameer Adhikari
          isHf: false
          isPro: false
          name: Anthrax247
          type: user
        html: '<p>I was experimenting with oobabooga and noticed something weird.<br>It
          downloads compat.no-act-order file first as expected.<br>But if both safetensors
          files are downloaded, it loads the one that comes last alphabetically, loading
          latest.act-order file.<br>Sorry for all the renaming trouble. Seems like
          deleting the latest.act-order file is the easiest solution for oobabooga
          users currently.</p>

          '
        raw: 'I was experimenting with oobabooga and noticed something weird.

          It downloads compat.no-act-order file first as expected.

          But if both safetensors files are downloaded, it loads the one that comes
          last alphabetically, loading latest.act-order file.

          Sorry for all the renaming trouble. Seems like deleting the latest.act-order
          file is the easiest solution for oobabooga users currently.'
        updatedAt: '2023-04-30T13:18:22.540Z'
      numEdits: 0
      reactions: []
    id: 644e6a9ecf72e60a5b78cd43
    type: comment
  author: Anthrax247
  content: 'I was experimenting with oobabooga and noticed something weird.

    It downloads compat.no-act-order file first as expected.

    But if both safetensors files are downloaded, it loads the one that comes last
    alphabetically, loading latest.act-order file.

    Sorry for all the renaming trouble. Seems like deleting the latest.act-order file
    is the easiest solution for oobabooga users currently.'
  created_at: 2023-04-30 12:18:22+00:00
  edited: false
  hidden: false
  id: 644e6a9ecf72e60a5b78cd43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T13:21:39.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh you''re kidding me! I renamed them all because I thought it loaded
          the first!  I suppose I should have actually tested it :D</p>

          <p>OK thanks, I will rename them all back I guess..</p>

          <p>On my newer repos I''m going for a more sophisticated system where there''s
          only one model per repo, and other versions are available via branches.
          I put in a PR with ooba to enable auto download from different branches,
          eg with <code>TheBloke/stable-vicuna-13B-GPTQ:latest</code>.  Not been merged
          yet though</p>

          '
        raw: 'Oh you''re kidding me! I renamed them all because I thought it loaded
          the first!  I suppose I should have actually tested it :D


          OK thanks, I will rename them all back I guess..


          On my newer repos I''m going for a more sophisticated system where there''s
          only one model per repo, and other versions are available via branches.
          I put in a PR with ooba to enable auto download from different branches,
          eg with `TheBloke/stable-vicuna-13B-GPTQ:latest`.  Not been merged yet though'
        updatedAt: '2023-04-30T13:21:39.462Z'
      numEdits: 0
      reactions: []
    id: 644e6b63ddf20748b0565061
    type: comment
  author: TheBloke
  content: 'Oh you''re kidding me! I renamed them all because I thought it loaded
    the first!  I suppose I should have actually tested it :D


    OK thanks, I will rename them all back I guess..


    On my newer repos I''m going for a more sophisticated system where there''s only
    one model per repo, and other versions are available via branches. I put in a
    PR with ooba to enable auto download from different branches, eg with `TheBloke/stable-vicuna-13B-GPTQ:latest`.  Not
    been merged yet though'
  created_at: 2023-04-30 12:21:39+00:00
  edited: false
  hidden: false
  id: 644e6b63ddf20748b0565061
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/wizardLM-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: I am getting errors, when I am trying to launch this in text generation webui
